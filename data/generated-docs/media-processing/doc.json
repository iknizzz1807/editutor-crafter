{"html":"<h1 id=\"media-processing-pipeline-design-document\">Media Processing Pipeline: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This system builds a scalable media processing service that handles image resizing, video transcoding, and thumbnail generation through an asynchronous job queue. The key architectural challenge is efficiently processing large media files while providing real-time progress tracking and reliable error recovery across distributed worker processes.</p>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as this foundational understanding applies throughout the system</p>\n</blockquote>\n<p>The explosive growth of digital media content creation and consumption has created an unprecedented demand for scalable media processing services. Modern applications routinely handle thousands of user-uploaded images and videos daily, each requiring transformation into multiple formats, resolutions, and quality levels to support diverse devices and network conditions. This section establishes the real-world drivers behind media processing systems and examines the core technical challenges that make building such systems complex.</p>\n<h3 id=\"mental-model-digital-photo-lab\">Mental Model: Digital Photo Lab</h3>\n<blockquote>\n<p><strong>Mental Model</strong>: Think of this system as a modern digital photo lab that never closes and can process thousands of orders simultaneously.</p>\n</blockquote>\n<p>In the era of film photography, a photo lab operated as a specialized facility where customers dropped off rolls of film and returned later to collect developed prints. The lab employed skilled technicians who understood the chemistry of different film types, the characteristics of various paper stocks, and the precise timing required for each processing step. Customers could request different print sizes, finishes, and quantities, with each order requiring specific handling procedures.</p>\n<p>Our media processing pipeline operates on remarkably similar principles, but at digital scale. Users submit &quot;orders&quot; by uploading raw media files through an API gateway, much like dropping off film canisters at a lab counter. Each upload triggers the creation of a <strong>processing job</strong> that contains detailed instructions about desired output formats, resolutions, and quality settings—analogous to the order forms customers filled out for their prints.</p>\n<p>The job queue functions as the lab&#39;s work order system, organizing incoming requests by priority and distributing them to available technicians. In our digital lab, these technicians are <strong>worker processes</strong> that specialize in different types of media transformation. Just as a photo lab might have separate stations for developing, printing, and finishing, our workers are equipped with specialized tools: Pillow for image manipulation, FFmpeg for video transcoding, and custom progress tracking systems.</p>\n<p>The most crucial similarity lies in the concept of <strong>batch processing with progress tracking</strong>. Traditional photo labs provided customers with claim tickets and estimated completion times. Our system maintains detailed progress records for each job, sending webhook notifications that function like the phone calls labs made when orders were ready for pickup. Both systems must handle the reality that some processes take much longer than others—a simple wallet-sized print versus a large format enlargement in the traditional lab, or thumbnail generation versus 4K video transcoding in our digital system.</p>\n<p>This analogy helps clarify why certain architectural decisions are essential: job queues prevent the system from being overwhelmed during peak times (like the Christmas photo rush), worker specialization ensures optimal resource utilization, and progress tracking provides transparency that builds user trust in a process they cannot directly observe.</p>\n<h3 id=\"existing-solutions-comparison\">Existing Solutions Comparison</h3>\n<p>The media processing landscape offers three primary architectural approaches, each with distinct trade-offs that influence system design decisions. Understanding these alternatives provides context for the architectural choices made in this implementation.</p>\n<p><strong>Cloud Service Integration Approach</strong></p>\n<p>Cloud-based solutions like AWS MediaConvert, Google Cloud Video Intelligence, and Azure Media Services represent the &quot;software-as-a-service&quot; model for media processing. These services handle infrastructure scaling automatically and provide pre-optimized processing pipelines with minimal configuration requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Cloud Services</th>\n<th>Assessment</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Setup Complexity</strong></td>\n<td>Minimal - API integration only</td>\n<td>Fastest time to market</td>\n</tr>\n<tr>\n<td><strong>Scaling</strong></td>\n<td>Automatic horizontal scaling</td>\n<td>Handles traffic spikes seamlessly</td>\n</tr>\n<tr>\n<td><strong>Cost Model</strong></td>\n<td>Pay-per-operation pricing</td>\n<td>Can become expensive at scale</td>\n</tr>\n<tr>\n<td><strong>Customization</strong></td>\n<td>Limited to provided parameters</td>\n<td>Insufficient for specialized workflows</td>\n</tr>\n<tr>\n<td><strong>Latency</strong></td>\n<td>Network round-trip overhead</td>\n<td>Adds 200-500ms per operation</td>\n</tr>\n<tr>\n<td><strong>Data Residency</strong></td>\n<td>Files transmitted to cloud</td>\n<td>Potential privacy/compliance issues</td>\n</tr>\n<tr>\n<td><strong>Vendor Lock-in</strong></td>\n<td>Tight coupling to provider APIs</td>\n<td>Migration complexity increases over time</td>\n</tr>\n</tbody></table>\n<p>Cloud services excel for applications with straightforward processing requirements and variable usage patterns. However, they become prohibitively expensive for high-volume applications and offer insufficient control for specialized processing workflows that require custom algorithms or unusual format support.</p>\n<p><strong>On-Premise Processing Solutions</strong></p>\n<p>Self-hosted media processing represents the traditional approach where organizations deploy and manage their own processing infrastructure. This includes both custom-built solutions using libraries like FFmpeg and Pillow, as well as commercial software packages designed for media workflow automation.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>On-Premise</th>\n<th>Assessment</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Infrastructure Control</strong></td>\n<td>Complete hardware/software control</td>\n<td>Optimal for specialized requirements</td>\n</tr>\n<tr>\n<td><strong>Processing Latency</strong></td>\n<td>Local processing - minimal network overhead</td>\n<td>Best performance for real-time needs</td>\n</tr>\n<tr>\n<td><strong>Customization</strong></td>\n<td>Unlimited - full source code access</td>\n<td>Supports any processing algorithm</td>\n</tr>\n<tr>\n<td><strong>Scaling Complexity</strong></td>\n<td>Manual capacity planning required</td>\n<td>Requires infrastructure expertise</td>\n</tr>\n<tr>\n<td><strong>Operational Overhead</strong></td>\n<td>Full system administration burden</td>\n<td>Significant ongoing maintenance costs</td>\n</tr>\n<tr>\n<td><strong>Initial Investment</strong></td>\n<td>High upfront hardware/software costs</td>\n<td>Substantial capital expenditure</td>\n</tr>\n<tr>\n<td><strong>Reliability</strong></td>\n<td>Single points of failure without redundancy</td>\n<td>Requires sophisticated disaster recovery</td>\n</tr>\n</tbody></table>\n<p>On-premise solutions provide maximum control and can achieve the lowest per-operation costs at high volumes. They are essential for organizations with strict data residency requirements or highly specialized processing needs. However, they demand significant infrastructure expertise and cannot easily handle unpredictable traffic patterns.</p>\n<p><strong>Hybrid Processing Architecture</strong></p>\n<p>The hybrid approach combines self-hosted processing capabilities with cloud service integration, allowing systems to optimize for both control and scalability. This architecture typically handles routine processing operations locally while leveraging cloud services for overflow capacity or specialized operations.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Hybrid Approach</th>\n<th>Assessment</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Cost Optimization</strong></td>\n<td>Local processing for base load, cloud for peaks</td>\n<td>Balances fixed and variable costs</td>\n</tr>\n<tr>\n<td><strong>Flexibility</strong></td>\n<td>Custom algorithms locally, cloud for standard operations</td>\n<td>Best of both worlds approach</td>\n</tr>\n<tr>\n<td><strong>Complexity</strong></td>\n<td>Requires orchestration between multiple systems</td>\n<td>Significant architectural complexity</td>\n</tr>\n<tr>\n<td><strong>Reliability</strong></td>\n<td>Cloud failover for local system outages</td>\n<td>Enhanced disaster recovery options</td>\n</tr>\n<tr>\n<td><strong>Data Management</strong></td>\n<td>Intelligent routing based on content sensitivity</td>\n<td>Supports compliance requirements</td>\n</tr>\n<tr>\n<td><strong>Scaling Strategy</strong></td>\n<td>Predictable local capacity with elastic overflow</td>\n<td>Handles both steady state and spikes</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Self-Hosted Processing with Cloud-Ready Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: This implementation targets organizations that need control over processing algorithms, data residency, and per-operation costs, while maintaining the flexibility to integrate cloud services in the future</li>\n<li><strong>Options Considered</strong>: Pure cloud integration, pure on-premise deployment, hybrid architecture from the start</li>\n<li><strong>Decision</strong>: Build a self-hosted system with modular components that can integrate with cloud services later</li>\n<li><strong>Rationale</strong>: Self-hosted provides learning value for understanding media processing fundamentals, offers maximum customization for specialized requirements, and delivers the lowest long-term operational costs for high-volume scenarios. The modular architecture ensures cloud integration remains possible without major refactoring.</li>\n<li><strong>Consequences</strong>: Higher initial development complexity and operational overhead, but provides deep understanding of media processing challenges and maximum future flexibility</li>\n</ul>\n</blockquote>\n<h3 id=\"core-technical-challenges\">Core Technical Challenges</h3>\n<p>Building a scalable media processing pipeline presents several interconnected technical challenges that differentiate it from typical web application development. Each challenge requires specific architectural decisions and implementation strategies that influence the entire system design.</p>\n<p><strong>Memory Management and Resource Utilization</strong></p>\n<p>Media processing operations are inherently memory-intensive, with resource requirements that scale dramatically based on content characteristics. A single 4K video file can require several gigabytes of RAM during transcoding operations, while high-resolution image processing may need temporary buffers that exceed available system memory.</p>\n<p>The fundamental challenge lies in the <strong>unpredictable nature of resource requirements</strong>. Unlike traditional web services where request processing consumes predictable amounts of memory for predictable durations, media processing operations vary enormously based on input characteristics. A 30-second 1080p video might require 500MB of working memory, while a 2-hour 4K video could need 8GB or more.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Challenge</th>\n<th>Impact</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Memory Usage</strong></td>\n<td>Unpredictable working set sizes</td>\n<td>Worker process crashes, system instability</td>\n<td>Pre-flight content analysis, memory-based job routing</td>\n</tr>\n<tr>\n<td><strong>Disk I/O</strong></td>\n<td>Large temporary file requirements</td>\n<td>Storage exhaustion, I/O bottlenecks</td>\n<td>Streaming processing, temporary file cleanup</td>\n</tr>\n<tr>\n<td><strong>CPU Utilization</strong></td>\n<td>CPU-intensive encoding operations</td>\n<td>Worker starvation, system responsiveness</td>\n<td>Process isolation, CPU quota enforcement</td>\n</tr>\n<tr>\n<td><strong>Network Bandwidth</strong></td>\n<td>Large file transfers during processing</td>\n<td>Network congestion, timeout issues</td>\n<td>Local processing, chunked transfers</td>\n</tr>\n</tbody></table>\n<p>The memory management challenge extends beyond simple allocation tracking to include <strong>temporal memory usage patterns</strong>. Video transcoding typically follows a sawtooth pattern where memory usage builds during frame buffering, spikes during encoding operations, then drops during I/O operations. Image processing may require sudden allocation of large contiguous buffers for pixel manipulation operations.</p>\n<p>Effective memory management requires implementing <strong>resource-aware job scheduling</strong> that considers both current system utilization and estimated resource requirements for queued jobs. This involves analyzing input media characteristics before processing begins and routing jobs to workers with appropriate resource availability.</p>\n<p><strong>Format Diversity and Compatibility</strong></p>\n<p>The media processing ecosystem encompasses dozens of container formats, codecs, and encoding parameters, each with unique characteristics and compatibility requirements. Modern applications must support legacy formats for backward compatibility while adopting emerging formats like WebP, AVIF, and AV1 for optimal efficiency.</p>\n<p>The complexity extends beyond simple format conversion to include <strong>parameter optimization for different use cases</strong>. A single input image might need transformation into JPEG for broad compatibility, WebP for size optimization, and AVIF for cutting-edge browsers, each with different quality settings optimized for the intended display context.</p>\n<table>\n<thead>\n<tr>\n<th>Format Category</th>\n<th>Complexity Factor</th>\n<th>Technical Challenge</th>\n<th>Implementation Requirement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Image Formats</strong></td>\n<td>Color space variations (RGB, CMYK, YUV)</td>\n<td>Color accuracy preservation</td>\n<td>Comprehensive color management</td>\n</tr>\n<tr>\n<td><strong>Video Containers</strong></td>\n<td>Codec/container compatibility matrices</td>\n<td>Format selection optimization</td>\n<td>Intelligent codec mapping</td>\n</tr>\n<tr>\n<td><strong>Encoding Parameters</strong></td>\n<td>Quality vs. size trade-offs</td>\n<td>Context-aware optimization</td>\n<td>Adaptive parameter selection</td>\n</tr>\n<tr>\n<td><strong>Metadata Handling</strong></td>\n<td>Format-specific metadata standards</td>\n<td>Information preservation/privacy</td>\n<td>Configurable metadata policies</td>\n</tr>\n</tbody></table>\n<p>The challenge intensifies when considering <strong>cross-format metadata preservation</strong>. EXIF data in JPEG images contains orientation, camera settings, and potentially sensitive location information. Video files include complex metadata about encoding parameters, timestamps, and technical characteristics that may need preservation or stripping based on privacy policies.</p>\n<p>Format diversity also introduces <strong>version compatibility issues</strong>. The WebP format supports both lossy and lossless compression with different browser support timelines. H.265 video encoding provides superior compression but requires hardware acceleration for practical performance and has complex licensing requirements.</p>\n<p><strong>Processing Time Estimation and Progress Tracking</strong></p>\n<p>Unlike typical web service operations that complete in milliseconds, media processing operations can require minutes or hours depending on content characteristics and output requirements. Users expect accurate progress estimates and reliable completion notifications, but media processing progress is notoriously difficult to predict accurately.</p>\n<p>The fundamental challenge stems from the <strong>non-linear nature of media processing operations</strong>. Video transcoding progress varies dramatically based on scene complexity—static scenes encode quickly while high-motion sequences require significantly more processing time per frame. Image processing operations may complete in stages with vastly different durations for each phase.</p>\n<p>Traditional percentage-based progress indicators prove inadequate for media processing workflows. A more effective approach involves <strong>stage-based progress tracking</strong> that provides users with meaningful information about current processing phases rather than potentially inaccurate time estimates.</p>\n<table>\n<thead>\n<tr>\n<th>Processing Stage</th>\n<th>Progress Characteristics</th>\n<th>Estimation Challenge</th>\n<th>Tracking Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Content Analysis</strong></td>\n<td>Fast, predictable duration</td>\n<td>Minimal - file I/O bound</td>\n<td>Simple percentage based on bytes read</td>\n</tr>\n<tr>\n<td><strong>Format Conversion</strong></td>\n<td>Highly variable based on complexity</td>\n<td>Significant - depends on content characteristics</td>\n<td>Stage completion milestones</td>\n</tr>\n<tr>\n<td><strong>Quality Optimization</strong></td>\n<td>Iterative with unknown convergence</td>\n<td>Extreme - optimization algorithms vary</td>\n<td>Iteration count with quality targets</td>\n</tr>\n<tr>\n<td><strong>Output Generation</strong></td>\n<td>Predictable but format-dependent</td>\n<td>Moderate - based on output size estimates</td>\n<td>Bytes written vs. estimated output size</td>\n</tr>\n</tbody></table>\n<p>Progress tracking complexity increases with <strong>multi-output processing scenarios</strong>. A single input video might generate multiple resolution variants, each requiring separate transcoding operations with different complexity characteristics. Users need visibility into overall job progress rather than individual operation progress.</p>\n<p>The challenge extends to <strong>failure recovery and retry scenarios</strong>. When processing operations fail partway through completion, the system must determine whether to restart from the beginning or resume from intermediate checkpoints. This decision impacts both progress accuracy and resource utilization efficiency.</p>\n<p><strong>Worker Coordination and Distributed Processing</strong></p>\n<p>Media processing systems require sophisticated coordination between multiple worker processes to achieve optimal throughput while avoiding resource conflicts and ensuring reliable job completion. Unlike stateless web service workers, media processing workers maintain significant state during long-running operations and require careful coordination to prevent resource exhaustion.</p>\n<p>The primary coordination challenge involves <strong>load balancing with resource awareness</strong>. Traditional round-robin or random load balancing proves inadequate when workers have different resource profiles and jobs have varying resource requirements. A worker processing a large video file may be unavailable for new jobs for hours, while a worker handling image resizing might complete dozens of operations in the same timeframe.</p>\n<table>\n<thead>\n<tr>\n<th>Coordination Aspect</th>\n<th>Challenge</th>\n<th>Impact</th>\n<th>Solution Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Resource-Aware Scheduling</strong></td>\n<td>Matching job requirements with worker capabilities</td>\n<td>Suboptimal resource utilization, job failures</td>\n<td>Dynamic worker capability advertising</td>\n</tr>\n<tr>\n<td><strong>Failure Detection</strong></td>\n<td>Long-running operations mask worker failures</td>\n<td>Jobs stuck in processing state indefinitely</td>\n<td>Heartbeat monitoring with timeout escalation</td>\n</tr>\n<tr>\n<td><strong>State Management</strong></td>\n<td>Workers maintain processing state across operations</td>\n<td>Inconsistent state after failures</td>\n<td>Checkpoint-based recovery mechanisms</td>\n</tr>\n<tr>\n<td><strong>Priority Handling</strong></td>\n<td>High-priority jobs blocked by long-running operations</td>\n<td>Poor user experience for urgent requests</td>\n<td>Preemptive scheduling with job priority queues</td>\n</tr>\n</tbody></table>\n<p>Worker coordination must also handle <strong>graceful degradation scenarios</strong> where individual workers become unavailable due to resource exhaustion, process failures, or system maintenance. The coordination system needs mechanisms to redistribute work from failed workers while preserving processing progress and avoiding duplicate work.</p>\n<p>The challenge extends to <strong>dynamic scaling scenarios</strong> where worker capacity needs to increase or decrease based on current load patterns. Adding new workers requires capability discovery and job redistribution, while removing workers requires graceful job migration and state preservation.</p>\n<blockquote>\n<p><strong>Architecture Insight</strong>: The combination of these four core challenges—memory management, format diversity, progress estimation, and worker coordination—creates a system complexity profile that differs significantly from typical web applications. Success requires architectural patterns that embrace asynchronous processing, resource-aware scheduling, and comprehensive error recovery mechanisms.</p>\n</blockquote>\n<p>These technical challenges inform every major architectural decision in the media processing pipeline, from the choice of message queue technology to the design of progress tracking APIs. Understanding their interconnected nature helps explain why media processing systems require specialized patterns and cannot simply apply traditional web application architectures.</p>\n<p>The following sections will detail how each system component addresses these challenges through specific design decisions, implementation patterns, and architectural trade-offs that provide reliable, scalable media processing capabilities.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical technology choices and starter code to address the core challenges identified in the problem analysis. The recommendations balance learning value with production readiness, focusing on Python-based solutions that provide clear visibility into media processing concepts.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Message Queue</strong></td>\n<td>Redis with <code>rq</code> library</td>\n<td>RabbitMQ with Celery</td>\n<td>Redis offers simpler setup for learning; RabbitMQ provides better production reliability</td>\n</tr>\n<tr>\n<td><strong>Image Processing</strong></td>\n<td>Pillow (PIL fork)</td>\n<td>OpenCV with Pillow fallback</td>\n<td>Pillow handles most use cases; OpenCV adds computer vision capabilities</td>\n</tr>\n<tr>\n<td><strong>Video Processing</strong></td>\n<td>subprocess calls to FFmpeg</td>\n<td>python-ffmpeg wrapper</td>\n<td>Direct subprocess gives full control; wrapper adds convenience</td>\n</tr>\n<tr>\n<td><strong>Progress Storage</strong></td>\n<td>Redis key-value store</td>\n<td>PostgreSQL with real-time updates</td>\n<td>Redis provides fast updates; PostgreSQL offers ACID guarantees</td>\n</tr>\n<tr>\n<td><strong>File Storage</strong></td>\n<td>Local filesystem</td>\n<td>AWS S3 with local caching</td>\n<td>Local files simplify development; S3 enables production scaling</td>\n</tr>\n<tr>\n<td><strong>Worker Management</strong></td>\n<td>Simple process pool</td>\n<td>Kubernetes job scheduling</td>\n<td>Process pool suitable for single-machine; K8s enables distributed workers</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Project Structure</strong></p>\n<p>The project structure separates core media processing logic from infrastructure concerns, enabling focused development on each component while maintaining clear boundaries for testing and maintenance.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media_pipeline/\n├── src/\n│   ├── media_pipeline/\n│   │   ├── __init__.py\n│   │   ├── api/                    # REST API and job submission\n│   │   │   ├── __init__.py\n│   │   │   ├── handlers.py         # Request handlers\n│   │   │   └── schemas.py          # Request/response models\n│   │   ├── core/                   # Core business logic\n│   │   │   ├── __init__.py\n│   │   │   ├── job_manager.py      # Job lifecycle management\n│   │   │   ├── progress_tracker.py # Progress monitoring\n│   │   │   └── webhook_sender.py   # Notification delivery\n│   │   ├── processors/             # Media processing components\n│   │   │   ├── __init__.py\n│   │   │   ├── image_processor.py  # Image operations\n│   │   │   ├── video_processor.py  # Video transcoding\n│   │   │   └── base_processor.py   # Common processing interface\n│   │   ├── queue/                  # Job queue implementation\n│   │   │   ├── __init__.py\n│   │   │   ├── redis_queue.py      # Redis-based queue\n│   │   │   ├── worker.py           # Worker process logic\n│   │   │   └── scheduler.py        # Job scheduling\n│   │   ├── storage/                # File and metadata storage\n│   │   │   ├── __init__.py\n│   │   │   ├── file_manager.py     # File I/O operations\n│   │   │   └── metadata_store.py   # Job and progress storage\n│   │   └── utils/                  # Common utilities\n│   │       ├── __init__.py\n│   │       ├── config.py           # Configuration management\n│   │       ├── logging.py          # Logging setup\n│   │       └── errors.py           # Custom exception types\n├── tests/                          # Test files mirror src structure\n│   ├── unit/\n│   ├── integration/\n│   └── fixtures/                   # Sample media files\n├── scripts/                        # Operational scripts\n│   ├── start_workers.py            # Worker process management\n│   └── health_check.py             # System monitoring\n├── config/                         # Configuration files\n│   ├── development.yaml\n│   └── production.yaml\n├── requirements/                   # Dependency specifications\n│   ├── base.txt                    # Core dependencies\n│   ├── dev.txt                     # Development tools\n│   └── prod.txt                    # Production additions\n└── docker/                        # Container definitions\n    ├── Dockerfile.api\n    └── Dockerfile.worker</code></pre></div>\n\n<p><strong>Infrastructure Starter Code</strong></p>\n<p>The following components provide complete, production-ready infrastructure that supports the core learning objectives without requiring deep implementation of supporting systems.</p>\n<p><strong>Configuration Management System</strong> (<code>src/media_pipeline/utils/config.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Configuration management for media processing pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Handles environment-specific settings and validation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    host: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"localhost\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    port: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6379</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    db: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    password: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    base_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"./storage\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temp_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"./temp\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_file_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\">  # 100MB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProcessingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#6A737D\">  # 1 hour</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhook_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AppConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis: RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage: StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    processing: ProcessingConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debug: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_yaml</span><span style=\"color:#E1E4E8\">(cls, config_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'AppConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load configuration from YAML file with environment variable substitution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(config_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            config_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> yaml.safe_load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Substitute environment variables</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._substitute_env_vars(config_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            redis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RedisConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config_data.get(</span><span style=\"color:#9ECBFF\">'redis'</span><span style=\"color:#E1E4E8\">, {})),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            storage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">StorageConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config_data.get(</span><span style=\"color:#9ECBFF\">'storage'</span><span style=\"color:#E1E4E8\">, {})),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            processing</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">ProcessingConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config_data.get(</span><span style=\"color:#9ECBFF\">'processing'</span><span style=\"color:#E1E4E8\">, {})),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            debug</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config_data.get(</span><span style=\"color:#9ECBFF\">'debug'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _substitute_env_vars</span><span style=\"color:#E1E4E8\">(data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Recursively substitute environment variables in configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {k: AppConfig._substitute_env_vars(v) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data.items()}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> data.startswith(</span><span style=\"color:#9ECBFF\">'${'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> data.endswith(</span><span style=\"color:#9ECBFF\">'}'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            env_var </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data[</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> os.getenv(env_var, data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global configuration instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config: Optional[AppConfig] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> init_config</span><span style=\"color:#E1E4E8\">(config_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> AppConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize global configuration from file or environment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    global</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config_path </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        env </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> os.getenv(</span><span style=\"color:#9ECBFF\">'ENVIRONMENT'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'development'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"config/</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">env</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">.yaml\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AppConfig.from_yaml(config_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_config</span><span style=\"color:#E1E4E8\">() -> AppConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Get global configuration, initializing if necessary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    global</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> init_config()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> config</span></span></code></pre></div>\n\n<p><strong>Logging Infrastructure</strong> (<code>src/media_pipeline/utils/logging.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Centralized logging configuration for media processing pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides structured logging with correlation IDs for job tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging.config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobContextFilter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">logging</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Filter</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Add job context information to log records.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> filter</span><span style=\"color:#E1E4E8\">(self, record):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add job_id from context if available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        record.job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'no-job'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        record.correlation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'correlation_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JSONFormatter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">logging</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Formatter</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Format log records as JSON for structured logging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> format</span><span style=\"color:#E1E4E8\">(self, record):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log_entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'level'</span><span style=\"color:#E1E4E8\">: record.levelname,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'message'</span><span style=\"color:#E1E4E8\">: record.getMessage(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'module'</span><span style=\"color:#E1E4E8\">: record.module,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'function'</span><span style=\"color:#E1E4E8\">: record.funcName,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'line'</span><span style=\"color:#E1E4E8\">: record.lineno,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'job_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">getattr</span><span style=\"color:#E1E4E8\">(record, </span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'correlation_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">getattr</span><span style=\"color:#E1E4E8\">(record, </span><span style=\"color:#9ECBFF\">'correlation_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> record.exc_info:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log_entry[</span><span style=\"color:#9ECBFF\">'exception'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.formatException(record.exc_info)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> json.dumps(log_entry)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Logging configuration</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">LOGGING_CONFIG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'version'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'disable_existing_loggers'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'formatters'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'standard'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'format'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">%(asctime)s</span><span style=\"color:#9ECBFF\"> [</span><span style=\"color:#79B8FF\">%(levelname)s</span><span style=\"color:#9ECBFF\">] </span><span style=\"color:#79B8FF\">%(name)s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%(message)s</span><span style=\"color:#9ECBFF\"> (job=</span><span style=\"color:#79B8FF\">%(job_id)s</span><span style=\"color:#9ECBFF\">)'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'json'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '()'</span><span style=\"color:#E1E4E8\">: JSONFormatter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'filters'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'job_context'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '()'</span><span style=\"color:#E1E4E8\">: JobContextFilter,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'handlers'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'console'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'class'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'logging.StreamHandler'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'level'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'INFO'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'formatter'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'standard'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'filters'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'job_context'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'stream'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'ext://sys.stdout'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'file'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'class'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'logging.handlers.RotatingFileHandler'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'level'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'DEBUG'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'formatter'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'json'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'filters'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'job_context'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'filename'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'logs/media_pipeline.log'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'maxBytes'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">10485760</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># 10MB</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'backupCount'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'root'</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'level'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'INFO'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'handlers'</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'console'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'file'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> setup_logging</span><span style=\"color:#E1E4E8\">(debug: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize logging configuration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> debug:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        LOGGING_CONFIG</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'root'</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'level'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'DEBUG'</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        LOGGING_CONFIG</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'handlers'</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'console'</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'level'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'DEBUG'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.config.dictConfig(</span><span style=\"color:#79B8FF\">LOGGING_CONFIG</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> job_logging_context</span><span style=\"color:#E1E4E8\">(job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, correlation_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager that adds job information to all log records.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> correlation_id </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        correlation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(uuid.uuid4())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Get the job context filter and set context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_filter </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> logger.handlers:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> filter_obj </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> handler.filters:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(filter_obj, JobContextFilter):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                job_filter </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> filter_obj</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> job_filter:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        old_job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(job_filter, </span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        old_correlation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(job_filter, </span><span style=\"color:#9ECBFF\">'correlation_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_filter.job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_filter.correlation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> correlation_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_filter.job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_job_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_filter.correlation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_correlation_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code</strong></p>\n<p>The following skeletons provide the structure for core components that learners should implement themselves, with detailed TODO comments mapping to the architectural concepts discussed above.</p>\n<p><strong>Job Management Core</strong> (<code>src/media_pipeline/core/job_manager.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Core job management functionality for media processing pipeline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Handles job lifecycle, state transitions, and resource coordination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROCESSING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"processing\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCELLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cancelled\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobPriority</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOW</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NORMAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HIGH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    URGENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 20</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProcessingJob</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a media processing job with all required metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_file_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_specifications: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: JobPriority </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobPriority.</span><span style=\"color:#79B8FF\">NORMAL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: JobStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">PENDING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.utcnow)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhook_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    progress_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_duration: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Manages the complete lifecycle of media processing jobs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Coordinates between job queue, progress tracking, and worker processes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, queue_backend, progress_store, webhook_sender):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_backend</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.progress_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> progress_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.webhook_sender </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> webhook_sender</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active_jobs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ProcessingJob] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> submit_job</span><span style=\"color:#E1E4E8\">(self, input_file: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_specs: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   priority: JobPriority </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobPriority.</span><span style=\"color:#79B8FF\">NORMAL</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   webhook_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ProcessingJob:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Submit a new processing job to the queue.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            input_file: Path to input media file</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            output_specs: List of output format specifications</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            priority: Job processing priority</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            webhook_url: Optional webhook for status notifications</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ProcessingJob instance with assigned job_id</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create ProcessingJob instance with unique job_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input file exists and is readable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate output specifications contain required fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Estimate processing duration based on input file characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store job metadata in progress store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Submit job to queue with priority handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Send initial webhook notification if webhook_url provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return created job instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_job_progress</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, progress_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, details: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Update job progress and notify interested parties.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Unique job identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            progress_percentage: Completion percentage (0.0 - 100.0)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stage: Current processing stage description</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            details: Optional additional progress details</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate job_id exists in active jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update job progress in memory and persistent storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate estimated time remaining based on progress rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for progress milestone thresholds (25%, 50%, 75%)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Send webhook notifications for milestone progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update job status if progress indicates completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Progress updates can arrive out of order - handle gracefully</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_completed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_files: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Mark job as completed and perform cleanup operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Unique job identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            output_files: List of generated output file paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate job exists and is in PROCESSING state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update job status to COMPLETED with completion timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store output file paths in job metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Remove job from active jobs tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Send completion webhook notification with output file details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Schedule cleanup of temporary files after retention period</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_failed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       retry_eligible: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handle job failure with retry logic and error reporting.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Unique job identifier  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            error_message: Detailed error description</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            retry_eligible: Whether job can be retried</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve job and increment retry_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if retry_count exceeds maximum retry attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If retries available and retry_eligible, requeue job with exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If no retries remaining, mark job as permanently FAILED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Send failure webhook notification with error details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Clean up any partial output files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Log failure details for debugging and monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Exponential backoff delays should be: 1min, 2min, 4min, 8min...</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Implementation Hints</strong></p>\n<p><strong>Python Media Processing Essentials:</strong></p>\n<ul>\n<li>Use <code>Pillow.Image.MAX_IMAGE_PIXELS = None</code> to handle large images, but implement custom size limits to prevent memory exhaustion</li>\n<li>Install FFmpeg system package: <code>apt-get install ffmpeg</code> (Ubuntu) or <code>brew install ffmpeg</code> (macOS)  </li>\n<li>Use <code>subprocess.run()</code> with <code>capture_output=True</code> and <code>text=True</code> for FFmpeg integration</li>\n<li>Implement timeout handling with <code>subprocess.run(timeout=seconds)</code> to prevent hung processes</li>\n<li>Use <code>pathlib.Path</code> for cross-platform file path handling instead of string concatenation</li>\n<li>Redis connection pooling: <code>redis.ConnectionPool(max_connections=20)</code> prevents connection exhaustion</li>\n</ul>\n<p><strong>Memory Management Patterns:</strong></p>\n<ul>\n<li>Process images in chunks for large files: read → process → write → release memory</li>\n<li>Use <code>gc.collect()</code> after processing large files to force garbage collection</li>\n<li>Monitor memory usage with <code>psutil.Process().memory_info().rss</code> and fail fast when limits exceeded  </li>\n<li>Implement file streaming: <code>with open(file, &#39;rb&#39;) as f: for chunk in iter(lambda: f.read(8192), b&#39;&#39;):</code></li>\n</ul>\n<p><strong>Error Handling Strategies:</strong></p>\n<ul>\n<li>Catch <code>PIL.Image.DecompressionBombError</code> for oversized images</li>\n<li>Handle <code>subprocess.CalledProcessError</code> for FFmpeg failures and parse stderr for specific error types</li>\n<li>Use <code>try/except/finally</code> blocks to ensure temporary file cleanup even when processing fails</li>\n<li>Implement circuit breaker pattern for external services (webhook delivery, storage APIs)</li>\n</ul>\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the core infrastructure, verify the following behaviors:</p>\n<p><strong>Configuration and Logging Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test configuration loading</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"from media_pipeline.utils.config import init_config; print(init_config('config/development.yaml'))\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify logging output includes job context</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from media_pipeline.utils.logging import setup_logging, job_logging_context</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import logging</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">setup_logging(debug=True)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">with job_logging_context('test-job-123'):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    logging.info('Test message with job context')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Expected Output Indicators:</strong></p>\n<ul>\n<li>Configuration loads without errors and displays all nested settings</li>\n<li>Log messages include job_id and correlation_id in structured format</li>\n<li>No import errors when loading core modules</li>\n</ul>\n<p><strong>Common Setup Issues:</strong></p>\n<ul>\n<li><strong>ModuleNotFoundError</strong>: Ensure <code>PYTHONPATH</code> includes <code>src/</code> directory or install package in development mode</li>\n<li><strong>FFmpeg not found</strong>: Install system FFmpeg package, verify with <code>ffmpeg -version</code></li>\n<li><strong>Redis connection errors</strong>: Start Redis server with <code>redis-server</code> or use Docker container</li>\n<li><strong>Permission errors</strong>: Ensure write access to configured storage and log directories</li>\n</ul>\n<p>The infrastructure code provides a solid foundation that handles configuration management, logging, and error handling patterns. This allows learners to focus on implementing the core media processing algorithms and job coordination logic without getting distracted by supporting infrastructure concerns.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as these requirements drive the entire system design and implementation scope</p>\n</blockquote>\n<h3 id=\"mental-model-restaurant-menu-planning\">Mental Model: Restaurant Menu Planning</h3>\n<p>Think of defining system goals like planning a restaurant menu. A successful restaurant doesn&#39;t try to serve every possible dish—instead, it carefully selects a focused set of offerings that it can execute exceptionally well with its available kitchen staff, equipment, and ingredients. The chef explicitly decides what goes on the menu (functional requirements), sets quality standards for each dish (non-functional requirements), and importantly, decides what NOT to serve (non-goals) to maintain focus and quality.</p>\n<p>Our media processing pipeline follows this same principle. We must clearly define what we will build, how well it must perform, and what we intentionally exclude to ensure we can deliver a high-quality, maintainable system within our resource constraints.</p>\n<h3 id=\"functional-requirements\">Functional Requirements</h3>\n<p>The <strong>functional requirements</strong> define the core capabilities our media processing pipeline must deliver. These represent the essential features that users directly interact with and depend upon for their media processing workflows.</p>\n<blockquote>\n<p><strong>Decision: Core Media Processing Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Users need to process various media types with different output requirements, from simple image resizing to complex video transcoding workflows</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Image-only processing system with simple resize operations</li>\n<li>Video-only transcoding service focusing on streaming formats</li>\n<li>Comprehensive media pipeline supporting both images and videos with advanced features</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Comprehensive media pipeline with full image and video processing capabilities</li>\n<li><strong>Rationale</strong>: Modern applications require both image and video processing, and building separate systems would create operational complexity and code duplication. A unified pipeline provides better resource utilization and simpler client integration.</li>\n<li><strong>Consequences</strong>: Increased system complexity requiring careful resource management, but provides complete media processing solution that scales with user needs</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Capability Category</th>\n<th>Specific Requirements</th>\n<th>Input Formats</th>\n<th>Output Formats</th>\n<th>Configuration Options</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Image Processing</strong></td>\n<td>Resize with aspect ratio preservation</td>\n<td>JPEG, PNG, WebP, GIF</td>\n<td>JPEG, PNG, WebP, AVIF</td>\n<td>Target dimensions, interpolation algorithm, quality settings</td>\n</tr>\n<tr>\n<td></td>\n<td>Format conversion with quality control</td>\n<td></td>\n<td></td>\n<td>Compression level, progressive encoding, color space</td>\n</tr>\n<tr>\n<td></td>\n<td>Thumbnail generation with smart cropping</td>\n<td></td>\n<td></td>\n<td>Standard sizes, center-crop vs smart-crop, preview quality</td>\n</tr>\n<tr>\n<td></td>\n<td>EXIF metadata handling</td>\n<td></td>\n<td></td>\n<td>Preserve, strip, or selectively copy metadata fields</td>\n</tr>\n<tr>\n<td><strong>Video Processing</strong></td>\n<td>Multi-format transcoding</td>\n<td>MP4, AVI, MOV, WebM, MKV</td>\n<td>MP4, WebM, HLS, DASH</td>\n<td>Codec selection, bitrate, resolution, frame rate</td>\n</tr>\n<tr>\n<td></td>\n<td>Adaptive bitrate variant generation</td>\n<td></td>\n<td></td>\n<td>Multiple quality levels, resolution ladder, segment duration</td>\n</tr>\n<tr>\n<td></td>\n<td>Video thumbnail extraction</td>\n<td></td>\n<td>JPEG, PNG, WebP</td>\n<td>Time offset, frame selection, thumbnail size</td>\n</tr>\n<tr>\n<td></td>\n<td>Audio track handling</td>\n<td></td>\n<td></td>\n<td>Audio codec, bitrate, channel configuration, synchronization</td>\n</tr>\n<tr>\n<td><strong>Job Management</strong></td>\n<td>Asynchronous processing with priority queuing</td>\n<td>All supported formats</td>\n<td>All output formats</td>\n<td>Priority levels, processing timeout, resource allocation</td>\n</tr>\n<tr>\n<td></td>\n<td>Progress tracking and status reporting</td>\n<td></td>\n<td></td>\n<td>Real-time updates, stage-based progress, time estimation</td>\n</tr>\n<tr>\n<td></td>\n<td>Webhook notifications for status changes</td>\n<td></td>\n<td></td>\n<td>Completion, failure, progress milestones, retry events</td>\n</tr>\n<tr>\n<td></td>\n<td>Error handling with automatic retry logic</td>\n<td></td>\n<td></td>\n<td>Exponential backoff, retry limits, dead letter handling</td>\n</tr>\n</tbody></table>\n<p>The system must support <strong>batch processing operations</strong> where multiple output variants are generated from a single input file. For example, a single uploaded image might produce thumbnails at three different sizes, convert to WebP format for web display, and generate a high-quality JPEG for download—all as part of one <code>ProcessingJob</code> with multiple output specifications.</p>\n<p><strong>Resource-aware scheduling</strong> ensures that memory-intensive video transcoding jobs don&#39;t overwhelm the system while lightweight image resizing operations continue processing. The job queue must intelligently distribute work based on worker process capabilities and current system resource utilization.</p>\n<blockquote>\n<p><strong>Critical Design Insight</strong>: The functional requirements prioritize <strong>flexibility over simplicity</strong>. Rather than building separate specialized tools, we create a unified pipeline that handles diverse media processing scenarios through configurable job specifications. This approach serves both simple use cases (single image resize) and complex workflows (multi-variant video transcoding with thumbnails).</p>\n</blockquote>\n<h3 id=\"non-functional-requirements\">Non-Functional Requirements</h3>\n<p>The <strong>non-functional requirements</strong> establish the quality standards and operational characteristics that make our media processing pipeline suitable for production environments. These requirements directly impact architecture decisions and implementation strategies.</p>\n<blockquote>\n<p><strong>Decision: Performance and Scalability Targets</strong></p>\n<ul>\n<li><strong>Context</strong>: Media processing is computationally intensive with highly variable workloads, from small profile images to large video files requiring hours of processing time</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single-node processing with simple scaling through instance size increases</li>\n<li>Horizontally scalable worker pool with shared job queue</li>\n<li>Serverless processing using cloud functions with automatic scaling</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Horizontally scalable worker pool architecture with intelligent job distribution</li>\n<li><strong>Rationale</strong>: Provides predictable costs, allows optimization for different media types, and enables resource-aware scheduling that serverless cannot provide. Single-node scaling hits memory and I/O limits with large video files.</li>\n<li><strong>Consequences</strong>: Requires distributed system complexity but provides linear scaling and cost control</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Requirement Category</th>\n<th>Specification</th>\n<th>Measurement Method</th>\n<th>Business Justification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Processing Performance</strong></td>\n<td>Image resize operations complete within 5 seconds for files up to 50MB</td>\n<td>End-to-end job completion time</td>\n<td>User experience for real-time workflows</td>\n</tr>\n<tr>\n<td></td>\n<td>Video transcoding processes at 2x real-time speed for 1080p content</td>\n<td>FFmpeg processing rate measurement</td>\n<td>Acceptable wait times for uploaded content</td>\n</tr>\n<tr>\n<td></td>\n<td>System processes 1000 concurrent image jobs</td>\n<td>Active job queue depth monitoring</td>\n<td>Peak load handling for content publishing</td>\n</tr>\n<tr>\n<td></td>\n<td>95th percentile job completion time under 30 seconds for standard operations</td>\n<td>Job duration histogram analysis</td>\n<td>Predictable processing times for SLA commitments</td>\n</tr>\n<tr>\n<td><strong>Reliability and Availability</strong></td>\n<td>99.5% job success rate excluding invalid input files</td>\n<td>Success/failure ratio tracking</td>\n<td>Minimize user frustration from processing failures</td>\n</tr>\n<tr>\n<td></td>\n<td>Failed jobs automatically retry with exponential backoff</td>\n<td>Retry attempt logging and outcome tracking</td>\n<td>Handles transient failures without manual intervention</td>\n</tr>\n<tr>\n<td></td>\n<td>System recovers from worker crashes within 30 seconds</td>\n<td>Worker health monitoring and replacement time</td>\n<td>Maintains processing capacity during failures</td>\n</tr>\n<tr>\n<td></td>\n<td>Zero data loss for accepted processing jobs</td>\n<td>Job persistence and completion verification</td>\n<td>Protects user content and maintains service trust</td>\n</tr>\n<tr>\n<td><strong>Scalability Characteristics</strong></td>\n<td>Linear scaling to 50 worker processes per node</td>\n<td>Throughput measurement across worker counts</td>\n<td>Handles traffic growth through horizontal scaling</td>\n</tr>\n<tr>\n<td></td>\n<td>Supports 10,000 queued jobs without performance degradation</td>\n<td>Queue depth vs processing latency correlation</td>\n<td>Manages traffic spikes and batch processing loads</td>\n</tr>\n<tr>\n<td></td>\n<td>Worker processes scale up/down based on queue depth</td>\n<td>Auto-scaling trigger timing and effectiveness</td>\n<td>Cost optimization during variable load periods</td>\n</tr>\n<tr>\n<td></td>\n<td>Storage scales to 100TB of processing temp files</td>\n<td>Disk usage monitoring and cleanup verification</td>\n<td>Supports large-scale video processing operations</td>\n</tr>\n<tr>\n<td><strong>Resource Management</strong></td>\n<td>Individual jobs limited to 8GB memory usage</td>\n<td>Process memory monitoring and enforcement</td>\n<td>Prevents single large jobs from destabilizing system</td>\n</tr>\n<tr>\n<td></td>\n<td>Temp file cleanup within 1 hour of job completion</td>\n<td>File system monitoring and cleanup verification</td>\n<td>Maintains disk space availability for ongoing operations</td>\n</tr>\n<tr>\n<td></td>\n<td>CPU utilization averaged across workers stays below 80%</td>\n<td>System resource monitoring and alert thresholds</td>\n<td>Maintains responsive performance during peak loads</td>\n</tr>\n<tr>\n<td></td>\n<td>Network bandwidth utilization for job coordination under 100Mbps</td>\n<td>Inter-component traffic measurement</td>\n<td>Ensures job queue doesn&#39;t become network bottleneck</td>\n</tr>\n</tbody></table>\n<p><strong>Progress tracking accuracy</strong> represents a particularly challenging non-functional requirement. Video transcoding progress estimation requires stage-based reporting rather than simple percentage completion, since FFmpeg progress varies significantly based on content complexity and encoding parameters. The system must provide meaningful progress updates that help users understand processing status without making overly precise time commitments.</p>\n<blockquote>\n<p><strong>Decision: Consistency vs Availability Trade-offs</strong></p>\n<ul>\n<li><strong>Context</strong>: Media processing jobs represent significant computational work that shouldn&#39;t be lost, but the system must remain responsive during partial failures</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Strict consistency requiring all components to acknowledge job state changes</li>\n<li>Eventual consistency with job state synchronization through message queue</li>\n<li>Hybrid approach with strong consistency for job submission, eventual consistency for progress updates</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid consistency model with durable job submission and eventually consistent progress reporting</li>\n<li><strong>Rationale</strong>: Job submission must be durable to prevent lost work, but progress updates can be eventually consistent since they&#39;re informational rather than critical for system correctness</li>\n<li><strong>Consequences</strong>: Simplified client integration with reliable job submission, but progress updates may occasionally show stale information</li>\n</ul>\n</blockquote>\n<p><strong>Security and compliance requirements</strong> focus on protecting user content and metadata privacy. The system must provide configurable EXIF metadata stripping to remove potentially sensitive location and device information from processed images. All temporary files must be securely deleted after processing completion, and webhook notifications must use signature verification to prevent spoofing attacks.</p>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>The <strong>explicit non-goals</strong> define functionality that we intentionally exclude from this media processing pipeline implementation. These boundaries prevent scope creep and ensure we can deliver high-quality core functionality rather than attempting to solve every possible media processing challenge.</p>\n<blockquote>\n<p><strong>Decision: Content Storage and Management Exclusion</strong></p>\n<ul>\n<li><strong>Context</strong>: Many media processing systems also provide content storage, CDN integration, and digital asset management features</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Full-featured media management platform with storage, metadata, and delivery</li>\n<li>Processing-only service that integrates with external storage systems</li>\n<li>Hybrid approach with basic storage and advanced processing capabilities</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Processing-only service with clear storage integration points</li>\n<li><strong>Rationale</strong>: Content storage, CDN management, and digital asset management represent separate domain expertise areas with different scaling characteristics and operational requirements</li>\n<li><strong>Consequences</strong>: Simpler system design focused on processing excellence, but requires clients to manage their own storage and delivery infrastructure</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Non-Goal Category</th>\n<th>Specific Exclusions</th>\n<th>Rationale</th>\n<th>Alternative Solutions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Content Management</strong></td>\n<td>Long-term storage of original or processed media files</td>\n<td>Storage requirements vary dramatically by use case and compliance needs</td>\n<td>Integrate with S3, Google Cloud Storage, or local file systems</td>\n</tr>\n<tr>\n<td></td>\n<td>Digital asset management with metadata search and organization</td>\n<td>Requires different expertise in search indexing and content categorization</td>\n<td>Use dedicated DAM systems like Adobe Experience Manager or custom solutions</td>\n</tr>\n<tr>\n<td></td>\n<td>Content delivery network integration and optimization</td>\n<td>CDN management requires separate operational expertise and provider relationships</td>\n<td>Integrate processed outputs with CloudFlare, AWS CloudFront, or similar</td>\n</tr>\n<tr>\n<td></td>\n<td>User authentication and access control for media files</td>\n<td>Authentication adds complexity unrelated to media processing core competency</td>\n<td>Handle auth at API gateway or application layer before job submission</td>\n</tr>\n<tr>\n<td><strong>Advanced Processing Features</strong></td>\n<td>AI-powered content analysis and automatic tagging</td>\n<td>Machine learning model management requires different infrastructure and expertise</td>\n<td>Integrate with cloud AI services or specialized content analysis tools</td>\n</tr>\n<tr>\n<td></td>\n<td>Real-time live streaming and broadcast processing</td>\n<td>Live streaming has different latency, resource, and reliability requirements than batch processing</td>\n<td>Use dedicated streaming platforms like Wowza, OBS, or cloud streaming services</td>\n</tr>\n<tr>\n<td></td>\n<td>Advanced video editing operations beyond transcoding</td>\n<td>Complex editing requires timeline management, effects processing, and interactive workflows</td>\n<td>Integrate with video editing APIs or desktop applications for advanced editing</td>\n</tr>\n<tr>\n<td></td>\n<td>Content moderation and inappropriate content detection</td>\n<td>Moderation requires specialized models, human review workflows, and policy management</td>\n<td>Use dedicated moderation services like AWS Rekognition or manual review processes</td>\n</tr>\n<tr>\n<td><strong>Specialized Format Support</strong></td>\n<td>Legacy or proprietary media formats with complex licensing requirements</td>\n<td>Specialized formats require additional dependencies and potentially expensive licensing</td>\n<td>Handle conversion to standard formats before submitting to processing pipeline</td>\n</tr>\n<tr>\n<td></td>\n<td>Professional broadcast formats and workflows</td>\n<td>Broadcast requirements include specialized metadata, color spaces, and workflow integration</td>\n<td>Use professional broadcast tools and integrate outputs with standard pipeline</td>\n</tr>\n<tr>\n<td></td>\n<td>3D media processing, VR/AR content optimization</td>\n<td>Immersive media has different processing requirements and specialized libraries</td>\n<td>Develop separate processing pipeline optimized for 3D workflows</td>\n</tr>\n<tr>\n<td></td>\n<td>Medical or scientific imaging format support</td>\n<td>Specialized domains require DICOM, microscopy format expertise</td>\n<td>Use domain-specific processing tools and convert outputs to standard formats</td>\n</tr>\n<tr>\n<td><strong>Infrastructure and Operations</strong></td>\n<td>Built-in monitoring, alerting, and observability dashboards</td>\n<td>Monitoring requirements vary by deployment environment and operational preferences</td>\n<td>Integrate with existing monitoring stack using structured logging and metrics</td>\n</tr>\n<tr>\n<td></td>\n<td>Automatic cloud provider provisioning and infrastructure management</td>\n<td>Infrastructure automation requires deep cloud-specific knowledge and operational procedures</td>\n<td>Use existing infrastructure-as-code tools like Terraform or cloud-native auto-scaling</td>\n</tr>\n<tr>\n<td></td>\n<td>Multi-tenant processing with resource isolation and billing</td>\n<td>Multi-tenancy adds complexity in resource accounting, security isolation, and billing integration</td>\n<td>Deploy separate instances per tenant or add tenancy features in future versions</td>\n</tr>\n<tr>\n<td></td>\n<td>Geographic distribution and edge processing capabilities</td>\n<td>Edge deployment requires different architectural patterns and content synchronization strategies</td>\n<td>Deploy multiple regional instances or integrate with edge computing platforms</td>\n</tr>\n</tbody></table>\n<p><strong>Real-time processing requirements</strong> represent a significant non-goal that affects architecture decisions. While the system provides progress tracking and webhook notifications, it does not attempt to provide sub-second processing latency or real-time streaming capabilities. Batch processing optimization allows for better resource utilization and more reliable error recovery compared to real-time processing constraints.</p>\n<p><strong>Custom processing algorithm development</strong> falls outside our scope. The system integrates with proven libraries like Pillow for image processing and FFmpeg for video transcoding rather than implementing custom algorithms. This decision ensures reliability and compatibility while focusing development effort on job orchestration, progress tracking, and error recovery—the areas where custom logic provides the most value.</p>\n<blockquote>\n<p><strong>Critical Boundary Decision</strong>: We explicitly exclude <strong>content-aware processing features</strong> like automatic cropping based on face detection, intelligent compression based on content analysis, or quality optimization based on viewing context. These features require machine learning integration and significantly more complex configuration management. Clients needing content-aware features should implement them as separate preprocessing steps or integrate with specialized AI services.</p>\n</blockquote>\n<p>The non-goals serve as architectural constraints that simplify system design and clarify integration points. By excluding storage management, we avoid the complexity of data durability guarantees and focus on processing reliability. By excluding real-time requirements, we can optimize for throughput and implement simpler retry mechanisms. By excluding content analysis, we avoid the operational complexity of managing machine learning models and their dependencies.</p>\n<p>These boundaries are not permanent limitations but represent conscious decisions about the current implementation scope. Future versions might incorporate some excluded features as optional components or extension points, but the initial system focuses on delivering excellent core media processing capabilities with clear integration patterns for external systems to provide the excluded functionality.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical direction for translating the goals and requirements into working code structure and development milestones.</p>\n<p><strong>A. Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Job Queue</strong></td>\n<td>Redis with simple pub/sub</td>\n<td>Celery with RabbitMQ broker</td>\n</tr>\n<tr>\n<td><strong>Progress Tracking</strong></td>\n<td>Redis hash with periodic updates</td>\n<td>WebSocket server with real-time updates</td>\n</tr>\n<tr>\n<td><strong>Webhook Delivery</strong></td>\n<td>Python requests with basic retry</td>\n<td>Task queue with exponential backoff</td>\n</tr>\n<tr>\n<td><strong>Configuration Management</strong></td>\n<td>JSON config files</td>\n<td>Environment-based config with validation</td>\n</tr>\n<tr>\n<td><strong>Logging</strong></td>\n<td>Python logging to files</td>\n<td>Structured logging with correlation IDs</td>\n</tr>\n<tr>\n<td><strong>Metrics</strong></td>\n<td>Simple counters in Redis</td>\n<td>Prometheus metrics with custom collectors</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">media_pipeline</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── config</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── app_config.py          </span><span style=\"color:#6A737D\"># AppConfig, RedisConfig, StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── settings.py            </span><span style=\"color:#6A737D\"># Configuration loading and validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── core</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── job_manager.py         </span><span style=\"color:#6A737D\"># ProcessingJob operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── progress_tracker.py    </span><span style=\"color:#6A737D\"># Progress reporting and webhook delivery</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── types.py              </span><span style=\"color:#6A737D\"># JobStatus, JobPriority enums</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── workers</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── base_worker.py        </span><span style=\"color:#6A737D\"># Abstract worker interface</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── image_worker.py       </span><span style=\"color:#6A737D\"># Image processing implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── video_worker.py       </span><span style=\"color:#6A737D\"># Video transcoding implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── api</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── handlers.py           </span><span style=\"color:#6A737D\"># HTTP request handlers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── validation.py         </span><span style=\"color:#6A737D\"># Input validation and sanitization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">├── utils</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── </span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">.py</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   ├── logging.py            </span><span style=\"color:#6A737D\"># Structured logging setup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">│   └── storage.py            </span><span style=\"color:#6A737D\"># File system utilities</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">└── tests</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ├── integration</span><span style=\"color:#F97583\">/</span><span style=\"color:#6A737D\">          # End-to-end job processing tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ├── unit</span><span style=\"color:#F97583\">/</span><span style=\"color:#6A737D\">                 # Component unit tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    └── fixtures</span><span style=\"color:#F97583\">/</span><span style=\"color:#6A737D\">             # Sample media files for testing</span></span></code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># config/app_config.py - Complete configuration management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    host: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"localhost\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    port: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6379</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    db: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    password: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> connection_url</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.password:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"redis://:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.password</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">@</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.host</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.port</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.db</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"redis://</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.host</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.port</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.db</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    base_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"/tmp/media_processing\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temp_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"/tmp/media_processing/temp\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_file_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\">  # 1GB</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        os.makedirs(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.base_path, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        os.makedirs(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.temp_path, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProcessingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#6A737D\">  # 1 hour</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhook_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AppConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis: RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage: StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    processing: ProcessingConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debug: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> init_config</span><span style=\"color:#E1E4E8\">(config_path: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> AppConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize application configuration from file or environment variables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load config from JSON file if config_path provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Override with environment variables where present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate configuration values and ranges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize storage directories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return complete AppConfig instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># utils/logging.py - Complete logging infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JSONFormatter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">logging</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Formatter</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"JSON formatter for structured logging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> format</span><span style=\"color:#E1E4E8\">(self, record):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log_entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'level'</span><span style=\"color:#E1E4E8\">: record.levelname,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'message'</span><span style=\"color:#E1E4E8\">: record.getMessage(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'module'</span><span style=\"color:#E1E4E8\">: record.module,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'function'</span><span style=\"color:#E1E4E8\">: record.funcName,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'line'</span><span style=\"color:#E1E4E8\">: record.lineno</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add job context if present</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(record, </span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log_entry[</span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> record.job_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(record, </span><span style=\"color:#9ECBFF\">'correlation_id'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log_entry[</span><span style=\"color:#9ECBFF\">'correlation_id'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> record.correlation_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> json.dumps(log_entry)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> setup_logging</span><span style=\"color:#E1E4E8\">(debug: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configure structured logging system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    level </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">DEBUG</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> debug </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">INFO</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.StreamHandler()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    handler.setFormatter(JSONFormatter())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    root_logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    root_logger.setLevel(level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    root_logger.addHandler(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Suppress noisy third-party logs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.getLogger(</span><span style=\"color:#9ECBFF\">'urllib3'</span><span style=\"color:#E1E4E8\">).setLevel(logging.</span><span style=\"color:#79B8FF\">WARNING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.getLogger(</span><span style=\"color:#9ECBFF\">'requests'</span><span style=\"color:#E1E4E8\">).setLevel(logging.</span><span style=\"color:#79B8FF\">WARNING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> job_logging_context</span><span style=\"color:#E1E4E8\">(job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, correlation_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for job-aware logging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current logger</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create logging adapter with job_id and correlation_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Yield adapter for use in job processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Clean up context on exit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># core/types.py - Core data types</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROCESSING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"processing\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobPriority</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOW</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NORMAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HIGH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    URGENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 20</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProcessingJob</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_file_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_specifications: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: JobPriority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: JobStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">PENDING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.utcnow)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhook_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    progress_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_duration: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># core/job_manager.py - Job lifecycle management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ProcessingJob, JobStatus, JobPriority</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages processing job lifecycle and state transitions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client, storage_config):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage_config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> submit_job</span><span style=\"color:#E1E4E8\">(self, input_file: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_specs: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   priority: JobPriority, webhook_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ProcessingJob:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create and queue new processing job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate unique job_id using uuid4</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input_file exists and is readable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate output_specs format and required fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create ProcessingJob instance with provided parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Serialize job to Redis with job:{job_id} key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add job to priority queue based on priority level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return created ProcessingJob instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_job_progress</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, progress_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update job progress and send notifications.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve job from Redis using job_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update progress_percentage and add stage information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate estimated_duration based on progress rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Save updated job state to Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Send webhook notification if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Log progress update with job context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_completed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_files: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Finalize completed job and cleanup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve job from Redis and validate current status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update job status to COMPLETED with completion timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store output_files list in job record</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Send completion webhook notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Schedule cleanup of temporary files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update job completion metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_failed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, retry_eligible: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle job failure with retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve job from Redis and increment retry_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if retry_eligible and retry_count &#x3C; max_retry_attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If retryable, requeue job with exponential backoff delay</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If not retryable, mark job as FAILED permanently</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store error_message and send failure webhook notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Clean up temporary files for failed job</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li><strong>Redis Integration</strong>: Use <code>redis-py</code> library with connection pooling for job queue operations. Store jobs as JSON-serialized strings with TTL for automatic cleanup.</li>\n<li><strong>File Processing</strong>: Use <code>pathlib.Path</code> for cross-platform file path handling. Always use context managers (<code>with open()</code>) for file operations to ensure proper cleanup.</li>\n<li><strong>Process Management</strong>: Use <code>subprocess.run()</code> with timeout parameters for FFmpeg integration. Capture stdout/stderr for progress parsing and error reporting.</li>\n<li><strong>Error Handling</strong>: Create custom exception classes for different failure types (transient vs permanent). Use try/except blocks with specific exception types rather than bare <code>except:</code> clauses.</li>\n<li><strong>Async Operations</strong>: Consider <code>asyncio</code> for webhook delivery and I/O operations, but keep media processing in separate worker processes to avoid blocking the event loop.</li>\n</ul>\n<p><strong>F. Milestone Checkpoints:</strong></p>\n<p><strong>After Requirements Definition:</strong></p>\n<ul>\n<li>Run <code>python -m pytest tests/test_requirements.py -v</code> to verify requirement validation logic</li>\n<li>Expected: All functional requirement validation passes, non-functional requirement measurements return reasonable defaults</li>\n<li>Manual verification: Create sample <code>ProcessingJob</code> instances with different priorities and validate serialization/deserialization</li>\n</ul>\n<p><strong>After Goal Validation Implementation:</strong></p>\n<ul>\n<li>Test command: <code>python scripts/validate_goals.py --config config/test.json</code></li>\n<li>Expected output: &quot;✓ All functional requirements validated&quot;, &quot;✓ Non-functional requirements initialized&quot;, &quot;✓ Non-goals properly excluded&quot;</li>\n<li>Manual verification: Attempt to submit job with excluded feature (should raise clear error), verify job priority ordering in queue</li>\n</ul>\n<p><strong>G. Common Implementation Pitfalls:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pitfall</th>\n<th>Symptoms</th>\n<th>Root Cause</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Requirement Validation Missing</strong></td>\n<td>Jobs fail mysteriously during processing</td>\n<td>No upfront validation of input parameters</td>\n<td>Add comprehensive validation in <code>submit_job()</code> before queuing</td>\n</tr>\n<tr>\n<td><strong>Priority Queue Not Working</strong></td>\n<td>High priority jobs wait behind low priority</td>\n<td>Using simple FIFO queue instead of priority queue</td>\n<td>Implement Redis sorted sets with priority scores for job ordering</td>\n</tr>\n<tr>\n<td><strong>Non-Functional Requirements Not Measured</strong></td>\n<td>Performance degrades without notice</td>\n<td>No monitoring of actual vs required performance</td>\n<td>Add metrics collection for all non-functional requirements</td>\n</tr>\n<tr>\n<td><strong>Goal Scope Creep</strong></td>\n<td>System becomes complex and unstable</td>\n<td>Adding features that should be non-goals</td>\n<td>Regularly review and reject features not in functional requirements</td>\n</tr>\n</tbody></table>\n<p>The implementation should start with requirement validation and configuration management before building any processing capabilities. This foundation ensures all subsequent components operate within the defined goals and can be tested against concrete acceptance criteria.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as this architectural foundation supports image processing, video transcoding, and job queue components</p>\n</blockquote>\n<h3 id=\"mental-model-modern-manufacturing-assembly-line\">Mental Model: Modern Manufacturing Assembly Line</h3>\n<p>Think of our media processing pipeline as a modern manufacturing assembly line, but instead of building cars or electronics, we&#39;re transforming raw media files into optimized, web-ready content. Just like Toyota&#39;s production system revolutionized manufacturing with just-in-time delivery and quality control at every stage, our pipeline processes media files through specialized stations with real-time monitoring and automatic error recovery.</p>\n<p>The <strong>API gateway</strong> acts as the customer service desk where orders (processing jobs) are received, validated, and entered into the system with detailed specifications. The <strong>job queue</strong> functions as the production control system that schedules work orders and routes them to the appropriate assembly stations based on priority and worker availability. Each <strong>worker process</strong> represents a specialized assembly station equipped with specific tools—some stations excel at image manipulation using precision instruments (Pillow), while others handle complex video assembly using industrial-grade equipment (FFmpeg). The <strong>storage layer</strong> serves as both the incoming materials warehouse and the finished goods depot, organizing raw inputs and polished outputs with proper inventory tracking.</p>\n<p>What makes this assembly line particularly sophisticated is its ability to handle rush orders (high-priority jobs), automatically retry failed operations when a station encounters problems, and provide real-time progress updates to customers waiting for their custom media products. Unlike traditional assembly lines that process identical widgets, our pipeline handles diverse media formats and transforms them according to unique specifications for each job.</p>\n<h3 id=\"component-overview\">Component Overview</h3>\n<p>The media processing pipeline consists of four primary architectural layers, each with distinct responsibilities and clear interfaces for maximum modularity and testability.</p>\n<h4 id=\"api-gateway-layer\">API Gateway Layer</h4>\n<p>The <strong>API Gateway</strong> serves as the system&#39;s primary entry point, handling all external client interactions and providing a clean REST interface for job submission and status monitoring. This component owns the responsibility for request validation, authentication, rate limiting, and translating external API contracts into internal job specifications.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Primary Responsibility</th>\n<th>Key Operations</th>\n<th>External Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>REST API Server</td>\n<td>HTTP request handling and response formatting</td>\n<td>Job submission, status queries, file uploads</td>\n<td>Flask/FastAPI framework</td>\n</tr>\n<tr>\n<td>Request Validator</td>\n<td>Input sanitization and schema validation</td>\n<td>Media file type checking, parameter validation</td>\n<td>JSON schema libraries</td>\n</tr>\n<tr>\n<td>Authentication Handler</td>\n<td>API key and webhook signature verification</td>\n<td>Token validation, signature generation</td>\n<td>JWT libraries</td>\n</tr>\n<tr>\n<td>Rate Limiter</td>\n<td>Traffic control and abuse prevention</td>\n<td>Request counting, backoff enforcement</td>\n<td>Redis for distributed counting</td>\n</tr>\n</tbody></table>\n<p>The gateway operates stateless by design, allowing horizontal scaling through simple load balancing. All persistent state lives in the job queue and storage layers, while the gateway focuses purely on protocol translation and input validation.</p>\n<h4 id=\"job-queue-layer\">Job Queue Layer</h4>\n<p>The <strong>Job Queue</strong> implements the core orchestration logic, managing the lifecycle of processing jobs from submission through completion. This layer abstracts away the complexities of distributed task scheduling and provides reliable message delivery semantics with exactly-once processing guarantees.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Primary Responsibility</th>\n<th>Key Operations</th>\n<th>Storage Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Manager</td>\n<td>Job lifecycle coordination</td>\n<td>Job creation, state transitions, cleanup</td>\n<td>Job metadata persistence</td>\n</tr>\n<tr>\n<td>Priority Scheduler</td>\n<td>Work distribution based on priority and resources</td>\n<td>Queue ordering, worker assignment</td>\n<td>Priority queue implementation</td>\n</tr>\n<tr>\n<td>Progress Tracker</td>\n<td>Real-time status monitoring and reporting</td>\n<td>Progress updates, time estimation</td>\n<td>Progress state storage</td>\n</tr>\n<tr>\n<td>Webhook Dispatcher</td>\n<td>External notification delivery</td>\n<td>HTTP callbacks, retry logic</td>\n<td>Webhook delivery logs</td>\n</tr>\n</tbody></table>\n<p>The queue layer maintains strong consistency for job state while providing eventual consistency for progress updates. This design ensures that jobs never get lost while allowing for high-throughput progress reporting without blocking core operations.</p>\n<blockquote>\n<p><strong>Decision: Redis vs RabbitMQ for Message Queuing</strong></p>\n<ul>\n<li><strong>Context</strong>: Need reliable message delivery with priority support and progress tracking</li>\n<li><strong>Options Considered</strong>: Redis with sorted sets, RabbitMQ with priority queues, Amazon SQS with message attributes</li>\n<li><strong>Decision</strong>: Redis with sorted sets for primary queue, RabbitMQ for complex routing scenarios</li>\n<li><strong>Rationale</strong>: Redis provides atomic operations for priority management and excellent performance for frequent progress updates. RabbitMQ adds complexity but offers superior durability guarantees for critical jobs.</li>\n<li><strong>Consequences</strong>: Enables sub-second job scheduling with atomic priority updates but requires Redis persistence configuration and monitoring</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Queue Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Use Case Fit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Sorted Sets</td>\n<td>Atomic priority operations, excellent performance, built-in progress storage</td>\n<td>Less durable than dedicated message brokers, memory-based</td>\n<td>High-throughput processing with frequent priority changes</td>\n</tr>\n<tr>\n<td>RabbitMQ Priority Queues</td>\n<td>Strong durability, mature ecosystem, complex routing</td>\n<td>Higher latency for simple operations, more operational overhead</td>\n<td>Mission-critical jobs requiring guaranteed delivery</td>\n</tr>\n<tr>\n<td>Amazon SQS</td>\n<td>Fully managed, infinite scale, integrated with AWS</td>\n<td>Vendor lock-in, limited priority levels, higher per-message cost</td>\n<td>Cloud-native deployments with moderate priority requirements</td>\n</tr>\n</tbody></table>\n<h4 id=\"worker-process-layer\">Worker Process Layer</h4>\n<p><strong>Worker Processes</strong> execute the actual media transformation logic, providing isolated execution environments with resource monitoring and automatic recovery capabilities. Each worker specializes in specific media types while sharing common infrastructure for job acquisition, progress reporting, and error handling.</p>\n<table>\n<thead>\n<tr>\n<th>Worker Type</th>\n<th>Specialization</th>\n<th>Resource Requirements</th>\n<th>Typical Processing Time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image Worker</td>\n<td>JPEG/PNG/WebP processing, thumbnail generation</td>\n<td>512MB RAM, moderate CPU</td>\n<td>1-30 seconds per job</td>\n</tr>\n<tr>\n<td>Video Worker</td>\n<td>FFmpeg-based transcoding, segment generation</td>\n<td>2GB+ RAM, high CPU/disk I/O</td>\n<td>30 seconds to 2+ hours per job</td>\n</tr>\n<tr>\n<td>Batch Worker</td>\n<td>Multi-file operations, archive processing</td>\n<td>Variable based on batch size</td>\n<td>Minutes to hours</td>\n</tr>\n</tbody></table>\n<p>Workers implement a polling model with intelligent backoff, claiming jobs from the priority queue based on their capabilities and current resource utilization. Each worker process runs in isolation with configurable memory and CPU limits to prevent resource exhaustion from affecting other jobs.</p>\n<h4 id=\"storage-layer\">Storage Layer</h4>\n<p>The <strong>Storage Layer</strong> provides persistent storage for input files, intermediate processing artifacts, and final output files with proper lifecycle management and cleanup policies. This layer abstracts storage implementation details while providing consistent interfaces for file operations across different backend systems.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Component</th>\n<th>Purpose</th>\n<th>Consistency Requirements</th>\n<th>Cleanup Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input File Storage</td>\n<td>Raw uploaded media files</td>\n<td>Strong consistency for uploads</td>\n<td>Retain until processing complete</td>\n</tr>\n<tr>\n<td>Work Directory Manager</td>\n<td>Temporary processing workspace</td>\n<td>Local consistency per worker</td>\n<td>Immediate cleanup after job completion</td>\n</tr>\n<tr>\n<td>Output File Storage</td>\n<td>Processed media deliverables</td>\n<td>Strong consistency for results</td>\n<td>Configurable retention policy</td>\n</tr>\n<tr>\n<td>Metadata Database</td>\n<td>Job state and processing logs</td>\n<td>Strong consistency for job state</td>\n<td>Archive after job expiration</td>\n</tr>\n</tbody></table>\n<h3 id=\"request-processing-flow\">Request Processing Flow</h3>\n<p>The end-to-end processing flow orchestrates multiple components to transform uploaded media files into optimized outputs while providing comprehensive monitoring and error recovery.</p>\n<h4 id=\"job-submission-phase\">Job Submission Phase</h4>\n<p>When a client submits a new processing job, the request flows through several validation and preparation stages before entering the execution queue:</p>\n<ol>\n<li><p><strong>Request Reception</strong>: The API gateway receives an HTTP POST request containing the input media file, output specifications, priority level, and optional webhook URL for notifications.</p>\n</li>\n<li><p><strong>Input Validation</strong>: The request validator examines the uploaded file to determine media type, validates format support, checks file size limits, and ensures output specifications are achievable given the input characteristics.</p>\n</li>\n<li><p><strong>Job Creation</strong>: The job manager generates a unique job identifier, creates a <code>ProcessingJob</code> record with status <code>JobStatus.PENDING</code>, and calculates initial processing time estimates based on file size and requested operations.</p>\n</li>\n<li><p><strong>Queue Insertion</strong>: The priority scheduler inserts the job into the appropriate queue based on the requested <code>JobPriority</code> level and current system load, using atomic Redis operations to maintain queue consistency.</p>\n</li>\n<li><p><strong>Client Response</strong>: The API gateway returns the job identifier and initial status to the client, allowing them to track progress through subsequent status queries.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Duration</th>\n<th>Failure Modes</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Upload</td>\n<td>1-60 seconds</td>\n<td>Network interruption, file corruption</td>\n<td>Client retry with resumable uploads</td>\n</tr>\n<tr>\n<td>Validation</td>\n<td>&lt;1 second</td>\n<td>Invalid format, unsupported operations</td>\n<td>Immediate error response with specific details</td>\n</tr>\n<tr>\n<td>Job Creation</td>\n<td>&lt;100ms</td>\n<td>Database connectivity, resource exhaustion</td>\n<td>Automatic retry with exponential backoff</td>\n</tr>\n<tr>\n<td>Queue Insertion</td>\n<td>&lt;50ms</td>\n<td>Redis connectivity, memory limits</td>\n<td>Failover to secondary queue or delayed retry</td>\n</tr>\n</tbody></table>\n<h4 id=\"job-execution-phase\">Job Execution Phase</h4>\n<p>Once queued, jobs wait for available worker processes that match their resource requirements and processing capabilities:</p>\n<ol>\n<li><p><strong>Worker Job Acquisition</strong>: Workers poll the priority queue using Redis ZPOPMIN operations to atomically claim the highest-priority job matching their capabilities, updating job status to <code>JobStatus.PROCESSING</code>.</p>\n</li>\n<li><p><strong>Resource Preparation</strong>: The assigned worker creates an isolated working directory, downloads the input file from storage, and initializes processing tools (Pillow for images, FFmpeg for videos) with job-specific configuration parameters.</p>\n</li>\n<li><p><strong>Processing Execution</strong>: The worker executes the media transformation pipeline, reporting progress updates at regular intervals using <code>update_job_progress()</code> calls that broadcast status changes to monitoring systems and trigger webhook notifications.</p>\n</li>\n<li><p><strong>Output Generation</strong>: Completed processing artifacts are uploaded to the output storage layer with proper naming conventions and metadata tags, ensuring atomic replacement of any existing files.</p>\n</li>\n<li><p><strong>Job Completion</strong>: The worker calls <code>mark_job_completed()</code> to update the job status, trigger final webhook notifications, and schedule cleanup of temporary resources.</p>\n</li>\n</ol>\n<p>The execution phase implements comprehensive error handling with automatic retries for transient failures and clear failure classification for permanent errors that require human intervention.</p>\n<h4 id=\"progress-reporting-and-notification-flow\">Progress Reporting and Notification Flow</h4>\n<p>Throughout processing, the system maintains real-time visibility into job status and estimated completion times:</p>\n<ol>\n<li><p><strong>Stage-Based Progress</strong>: Workers report progress using discrete stages rather than time-based percentages, providing more accurate estimates for complex operations like video transcoding where processing speed varies significantly across different content sections.</p>\n</li>\n<li><p><strong>Webhook Delivery</strong>: The webhook dispatcher sends HTTP POST notifications to client-provided URLs for all significant job state changes, implementing retry logic with exponential backoff for failed delivery attempts.</p>\n</li>\n<li><p><strong>Status Queries</strong>: Clients can query job status at any time through the API gateway, receiving current progress percentage, estimated completion time, and detailed stage information without impacting processing performance.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Notification Type</th>\n<th>Trigger Event</th>\n<th>Payload Contents</th>\n<th>Retry Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Started</td>\n<td>Status change to PROCESSING</td>\n<td>Job ID, worker assignment, estimated duration</td>\n<td>3 retries over 15 minutes</td>\n</tr>\n<tr>\n<td>Progress Update</td>\n<td>Configurable percentage milestones</td>\n<td>Current progress, stage description, time remaining</td>\n<td>2 retries over 5 minutes</td>\n</tr>\n<tr>\n<td>Job Completed</td>\n<td>Successful processing finish</td>\n<td>Job ID, output file URLs, final metrics</td>\n<td>5 retries over 1 hour</td>\n</tr>\n<tr>\n<td>Job Failed</td>\n<td>Permanent processing failure</td>\n<td>Job ID, error details, retry eligibility</td>\n<td>5 retries over 1 hour</td>\n</tr>\n</tbody></table>\n<h3 id=\"recommended-project-structure\">Recommended Project Structure</h3>\n<p>The codebase organization reflects the architectural layers while promoting clear separation of concerns and enabling independent testing of each component.</p>\n<h4 id=\"top-level-directory-organization\">Top-Level Directory Organization</h4>\n<p>The project structure follows Python packaging best practices with clear separation between application logic, configuration, and external interfaces:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media-processing-pipeline/\n├── src/\n│   └── media_processor/           # Main application package\n│       ├── __init__.py\n│       ├── api/                   # API Gateway Layer\n│       ├── queue/                 # Job Queue Layer\n│       ├── workers/               # Worker Process Layer\n│       ├── storage/               # Storage Layer\n│       ├── models/                # Shared data models\n│       └── common/                # Cross-cutting utilities\n├── config/                        # Configuration files\n│   ├── development.yaml\n│   ├── production.yaml\n│   └── testing.yaml\n├── migrations/                    # Database schema migrations\n├── tests/                         # Test organization mirrors src/\n│   ├── unit/\n│   ├── integration/\n│   └── fixtures/                  # Sample media files\n├── scripts/                       # Deployment and maintenance scripts\n├── docker/                        # Container configurations\n└── docs/                          # Architecture and API documentation</code></pre></div>\n\n<h4 id=\"api-gateway-module-structure\">API Gateway Module Structure</h4>\n<p>The API layer implements a clean separation between HTTP handling, business logic, and external integrations:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/media_processor/api/\n├── __init__.py\n├── app.py                         # FastAPI application factory\n├── routes/\n│   ├── __init__.py\n│   ├── jobs.py                    # Job submission and status endpoints\n│   ├── health.py                  # System health and monitoring\n│   └── webhooks.py                # Webhook management endpoints\n├── middleware/\n│   ├── __init__.py\n│   ├── authentication.py         # API key validation\n│   ├── rate_limiting.py          # Request throttling\n│   └── request_logging.py        # Structured request/response logging\n├── schemas/\n│   ├── __init__.py\n│   ├── request_models.py         # Input validation schemas\n│   └── response_models.py        # Output serialization schemas\n└── dependencies.py               # FastAPI dependency injection setup</code></pre></div>\n\n<h4 id=\"job-queue-module-structure\">Job Queue Module Structure</h4>\n<p>The queue layer encapsulates all aspects of job lifecycle management and worker coordination:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/media_processor/queue/\n├── __init__.py\n├── job_manager.py                # Core job lifecycle operations\n├── priority_scheduler.py         # Queue ordering and worker assignment\n├── progress_tracker.py          # Real-time progress monitoring\n├── webhook_dispatcher.py        # External notification delivery\n├── backends/                     # Queue implementation adapters\n│   ├── __init__.py\n│   ├── redis_queue.py           # Redis-based queue operations\n│   └── rabbitmq_queue.py        # RabbitMQ integration (optional)\n└── serializers.py               # Job data serialization formats</code></pre></div>\n\n<h4 id=\"worker-process-module-structure\">Worker Process Module Structure</h4>\n<p>The worker layer organizes media processing capabilities into specialized modules with shared infrastructure:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/media_processor/workers/\n├── __init__.py\n├── base_worker.py               # Common worker infrastructure\n├── worker_manager.py            # Worker process lifecycle management\n├── image_worker.py              # Image processing specialization\n├── video_worker.py              # Video transcoding specialization\n├── processors/                  # Media processing implementations\n│   ├── __init__.py\n│   ├── image_processor.py       # Pillow-based image operations\n│   ├── video_processor.py       # FFmpeg integration layer\n│   └── thumbnail_generator.py   # Cross-format thumbnail creation\n└── resource_monitor.py          # Memory and CPU usage tracking</code></pre></div>\n\n<h4 id=\"shared-models-and-utilities-structure\">Shared Models and Utilities Structure</h4>\n<p>Common code shared across all layers resides in dedicated modules to prevent circular dependencies:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>src/media_processor/models/\n├── __init__.py\n├── job_models.py                # ProcessingJob and related entities\n├── config_models.py            # AppConfig and component configurations\n└── enums.py                     # JobStatus, JobPriority, and other constants\n\nsrc/media_processor/common/\n├── __init__.py\n├── logging_setup.py             # Structured logging configuration\n├── config_loader.py            # Configuration file parsing\n├── exceptions.py               # Custom exception hierarchy\n└── utils.py                    # Generic utility functions</code></pre></div>\n\n<blockquote>\n<p><strong>Decision: Package Organization Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance modularity with import simplicity while preventing circular dependencies</li>\n<li><strong>Options Considered</strong>: Flat structure with all modules at top level, strict layered packages, domain-driven module grouping</li>\n<li><strong>Decision</strong>: Layered packages with shared models and utilities at the same level</li>\n<li><strong>Rationale</strong>: Reflects architectural boundaries while making dependencies explicit. Shared models prevent import cycles between layers.</li>\n<li><strong>Consequences</strong>: Enables independent testing of each layer and clear dependency direction from API → Queue → Workers → Storage</li>\n</ul>\n</blockquote>\n<h4 id=\"configuration-and-deployment-structure\">Configuration and Deployment Structure</h4>\n<p>Environment-specific configuration and deployment artifacts maintain clear separation to support multiple deployment targets:</p>\n<table>\n<thead>\n<tr>\n<th>Configuration File</th>\n<th>Purpose</th>\n<th>Contains</th>\n<th>Environment</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>config/development.yaml</code></td>\n<td>Local development settings</td>\n<td>Debug logging, local Redis, relaxed timeouts</td>\n<td>Developer workstations</td>\n</tr>\n<tr>\n<td><code>config/testing.yaml</code></td>\n<td>Automated test configuration</td>\n<td>In-memory queues, mock external services</td>\n<td>CI/CD pipelines</td>\n</tr>\n<tr>\n<td><code>config/production.yaml</code></td>\n<td>Production deployment settings</td>\n<td>Cluster Redis, strict timeouts, monitoring</td>\n<td>Production clusters</td>\n</tr>\n<tr>\n<td><code>docker/worker.Dockerfile</code></td>\n<td>Worker container image</td>\n<td>Media processing tools, Python runtime</td>\n<td>All containerized environments</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Circular Import Dependencies</strong></p>\n<p>A common mistake in Python projects is creating circular imports between layers, especially when the API layer needs to import job models that also reference API-specific types. This manifests as <code>ImportError: cannot import name</code> exceptions during module loading.</p>\n<p><strong>Why it&#39;s wrong</strong>: Python cannot resolve module dependencies when they form cycles, leading to runtime import failures that only appear when specific code paths are executed.</p>\n<p><strong>How to fix</strong>: Place all shared data models (<code>ProcessingJob</code>, <code>AppConfig</code>, etc.) in a dedicated <code>models</code> package that doesn&#39;t import from other application layers. Use dependency injection at the application boundary to provide layer-specific implementations to shared models.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Web Framework</td>\n<td>Flask with Flask-RESTful</td>\n<td>FastAPI with async support</td>\n</tr>\n<tr>\n<td>Message Queue</td>\n<td>Redis with manual job management</td>\n<td>Celery with Redis/RabbitMQ backend</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Python configparser with INI files</td>\n<td>Pydantic with YAML configuration</td>\n</tr>\n<tr>\n<td>Database</td>\n<td>SQLite with simple schema</td>\n<td>PostgreSQL with SQLAlchemy ORM</td>\n</tr>\n<tr>\n<td>File Storage</td>\n<td>Local filesystem with organized directories</td>\n<td>AWS S3 with boto3 integration</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Python logging with file rotation</td>\n<td>Structured logging with JSON output</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>unittest with manual mocks</td>\n<td>pytest with pytest-asyncio and fixtures</td>\n</tr>\n<tr>\n<td>Containerization</td>\n<td>Single Docker container</td>\n<td>Multi-stage builds with Alpine Linux</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media-processing-pipeline/\n├── src/\n│   └── media_processor/\n│       ├── __init__.py\n│       ├── main.py                    # Application entry point\n│       ├── config.py                  # Configuration management\n│       ├── models/\n│       │   ├── __init__.py\n│       │   ├── job_models.py          # ProcessingJob, JobStatus, JobPriority\n│       │   └── config_models.py       # AppConfig, RedisConfig, etc.\n│       ├── api/\n│       │   ├── __init__.py\n│       │   ├── app.py                 # FastAPI application setup\n│       │   └── routes/\n│       │       ├── __init__.py\n│       │       └── jobs.py            # Job submission endpoints\n│       ├── queue/\n│       │   ├── __init__.py\n│       │   ├── job_manager.py         # Core job operations\n│       │   └── redis_queue.py         # Redis integration\n│       ├── workers/\n│       │   ├── __init__.py\n│       │   ├── base_worker.py         # Worker base class\n│       │   ├── image_worker.py        # Image processing worker\n│       │   └── video_worker.py        # Video processing worker\n│       └── storage/\n│           ├── __init__.py\n│           └── file_manager.py        # File operations\n├── config/\n│   ├── development.yaml\n│   └── production.yaml\n├── requirements.txt\n├── requirements-dev.txt\n└── tests/\n    ├── __init__.py\n    ├── test_job_manager.py\n    └── fixtures/\n        ├── sample.jpg\n        └── sample.mp4</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Configuration Management (src/media_processor/config.py)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    host: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"localhost\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    port: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6379</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    db: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    password: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    base_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"./storage\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temp_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"./temp\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_file_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\">  # 100MB</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProcessingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#6A737D\">  # 1 hour</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhook_timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AppConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis: RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage: StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    processing: ProcessingConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debug: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> init_config</span><span style=\"color:#E1E4E8\">(config_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> AppConfig:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize application configuration from YAML file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config_file </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(config_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> config_file.exists():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> FileNotFoundError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Configuration file not found: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">config_path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(config_file, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> yaml.safe_load(f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> AppConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        redis</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RedisConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config_data.get(</span><span style=\"color:#9ECBFF\">'redis'</span><span style=\"color:#E1E4E8\">, {})),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        storage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">StorageConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config_data.get(</span><span style=\"color:#9ECBFF\">'storage'</span><span style=\"color:#E1E4E8\">, {})),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        processing</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">ProcessingConfig(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">config_data.get(</span><span style=\"color:#9ECBFF\">'processing'</span><span style=\"color:#E1E4E8\">, {})),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        debug</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config_data.get(</span><span style=\"color:#9ECBFF\">'debug'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n\n<p><strong>Logging Setup (src/media_processor/common/logging_setup.py)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> setup_logging</span><span style=\"color:#E1E4E8\">(debug: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configure structured logging for the application.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log_level </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">DEBUG</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> debug </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">INFO</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log_format </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"</span><span style=\"color:#79B8FF\">%(asctime)s</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">%(name)s</span><span style=\"color:#9ECBFF\"> - </span><span style=\"color:#79B8FF\">%(levelname)s</span><span style=\"color:#9ECBFF\"> - \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"</span><span style=\"color:#79B8FF\">%(message)s</span><span style=\"color:#9ECBFF\"> [</span><span style=\"color:#79B8FF\">%(filename)s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%(lineno)d</span><span style=\"color:#9ECBFF\">]\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.basicConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        level</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">log_level,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        format</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">log_format,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        handlers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.StreamHandler(sys.stdout),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logging.FileHandler(</span><span style=\"color:#9ECBFF\">'media_processor.log'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Reduce noise from third-party libraries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.getLogger(</span><span style=\"color:#9ECBFF\">'urllib3'</span><span style=\"color:#E1E4E8\">).setLevel(logging.</span><span style=\"color:#79B8FF\">WARNING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.getLogger(</span><span style=\"color:#9ECBFF\">'redis'</span><span style=\"color:#E1E4E8\">).setLevel(logging.</span><span style=\"color:#79B8FF\">WARNING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> job_logging_context</span><span style=\"color:#E1E4E8\">(job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, correlation_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for job-aware logging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    old_factory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogRecordFactory()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_factory</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        record </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_factory(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        record.job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        record.correlation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> correlation_id </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> job_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> record</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.setLogRecordFactory(record_factory)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span><span style=\"color:#E1E4E8\"> logger</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logging.setLogRecordFactory(old_factory)</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Job Manager Core (src/media_processor/queue/job_manager.py)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..models.job_models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ProcessingJob, JobStatus, JobPriority</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages job lifecycle from creation to completion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client, storage_manager):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage_manager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> submit_job</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input_file: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_specs: List[</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        priority: JobPriority, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        webhook_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> ProcessingJob:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create and queue new processing job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate unique job_id using uuid.uuid4()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate input_file exists in storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create ProcessingJob instance with PENDING status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store job metadata in Redis hash</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add job to priority queue using Redis ZADD with priority score</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Log job creation with structured metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use job priority value as Redis sorted set score for automatic ordering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_job_progress</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        progress_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        details: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update job progress and send notifications.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve current job from Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate job exists and is in PROCESSING status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update progress_percentage and stage in job record</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store updated job back to Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Trigger webhook notification if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Publish progress update to monitoring channels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use Redis HSET for atomic field updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_completed</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_files: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Finalize completed job and cleanup.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Update job status to COMPLETED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set completed_at timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store output file references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Remove job from active processing queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Send completion webhook notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Schedule temporary file cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use Redis transaction (MULTI/EXEC) for atomic state updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_failed</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        retry_eligible: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle job failure with retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve current job and increment retry_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if retry_count exceeds max_retry_attempts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If retry eligible and under limit, requeue with exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Otherwise, mark as permanently FAILED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Send failure webhook notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Log detailed error information for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Calculate backoff delay as 2^retry_count * base_delay seconds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>API Application Setup (src/media_processor/api/app.py)</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> fastapi </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> FastAPI, HTTPException, Depends</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> fastapi.middleware.cors </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CORSMiddleware</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AppConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..queue.job_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..storage.file_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> FileManager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_app</span><span style=\"color:#E1E4E8\">(config: AppConfig) -> FastAPI:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"FastAPI application factory.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    app </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FastAPI(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        title</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Media Processing Pipeline\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        description</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Scalable media processing with job queue\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        version</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"1.0.0\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Configure CORS for web client access</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    app.add_middleware(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CORSMiddleware,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        allow_origins</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> config.debug </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">\"https://yourdomain.com\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        allow_credentials</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        allow_methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"POST\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        allow_headers</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Initialize core services</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        host</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.redis.host,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        port</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.redis.port,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        db</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.redis.db,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        password</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.redis.password,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FileManager(config.storage)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobManager(redis_client, storage_manager)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Dependency injection for route handlers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_job_manager</span><span style=\"color:#E1E4E8\">() -> JobManager:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> job_manager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Import and include route modules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # from .routes.jobs import router as jobs_router</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # app.include_router(jobs_router, prefix=\"/api/v1\")</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> app</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Python-Specific Considerations:</strong></p>\n<ul>\n<li>Use <code>redis-py</code> client with connection pooling for high-throughput job operations</li>\n<li>Configure <code>multiprocessing.Process</code> for worker isolation with proper signal handling</li>\n<li>Use <code>pathlib.Path</code> instead of <code>os.path</code> for cross-platform file operations</li>\n<li>Implement proper exception handling with custom exception hierarchy in <code>common/exceptions.py</code></li>\n<li>Use <code>dataclasses</code> or <code>pydantic</code> models for type safety and automatic validation</li>\n<li>Configure proper virtual environment with <code>requirements.txt</code> and <code>requirements-dev.txt</code></li>\n</ul>\n<p><strong>Redis Integration Tips:</strong></p>\n<ul>\n<li>Use Redis hash structures (<code>HSET</code>/<code>HGET</code>) for job metadata storage</li>\n<li>Implement atomic operations with Redis transactions (<code>MULTI</code>/<code>EXEC</code>) for job state changes</li>\n<li>Use sorted sets (<code>ZADD</code>/<code>ZPOPMIN</code>) for priority queue implementation</li>\n<li>Configure Redis persistence (AOF + RDB) for job durability in production</li>\n<li>Set appropriate Redis memory policies for handling job queue growth</li>\n</ul>\n<p><strong>File Operation Best Practices:</strong></p>\n<ul>\n<li>Create temporary directories using <code>tempfile.mkdtemp()</code> for isolated processing</li>\n<li>Use context managers (<code>with</code> statements) for automatic file handle cleanup</li>\n<li>Implement atomic file operations by writing to temporary files and renaming</li>\n<li>Configure proper file permissions (0o600) for sensitive temporary files</li>\n<li>Use <code>shutil.move()</code> for atomic cross-filesystem file operations</li>\n</ul>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the high-level architecture foundation:</p>\n<p><strong>What to verify manually:</strong></p>\n<ol>\n<li>Start the application with <code>python -m media_processor.main --config config/development.yaml</code></li>\n<li>Submit a test job via curl: <code>curl -X POST http://localhost:8000/api/v1/jobs -F &quot;file=@test.jpg&quot;</code></li>\n<li>Check Redis for job data: <code>redis-cli HGETALL job:[job_id]</code></li>\n<li>Verify job appears in priority queue: <code>redis-cli ZRANGE jobs:pending 0 -1 WITHSCORES</code></li>\n</ol>\n<p><strong>Expected behavior:</strong></p>\n<ul>\n<li>Application starts without import errors</li>\n<li>Configuration loads successfully from YAML file</li>\n<li>Redis connection establishes and accepts job data</li>\n<li>File uploads save to configured storage directory</li>\n<li>Job IDs are properly formatted UUIDs</li>\n<li>Priority queue maintains correct score ordering</li>\n</ul>\n<p><strong>Signs something is wrong:</strong></p>\n<ul>\n<li>Import errors: Check Python path and package <code>__init__.py</code> files</li>\n<li>Redis connection failures: Verify Redis server is running and configuration matches</li>\n<li>File permission errors: Check storage directory exists and is writable</li>\n<li>Configuration errors: Validate YAML syntax and required fields are present</li>\n</ul>\n<h2 id=\"data-model-and-types\">Data Model and Types</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as these core data structures underpin image processing, video transcoding, and job queue operations</p>\n</blockquote>\n<h3 id=\"mental-model-blueprint-library\">Mental Model: Blueprint Library</h3>\n<p>Think of our data model as a comprehensive blueprint library for a construction company. Just as architects need detailed blueprints that specify every dimension, material, and connection point before breaking ground, our media processing system needs precise data structures that define every job parameter, metadata field, and configuration option before processing begins. Each blueprint (data type) serves a specific purpose: job blueprints define what work needs to be done, media metadata blueprints capture the current state of materials, and configuration blueprints establish the standards and limits for all operations. Without these detailed specifications, workers would make inconsistent decisions, leading to structural failures (processing errors) and cost overruns (resource waste).</p>\n<p>The power of a well-designed blueprint library lies in its ability to handle complexity through standardization. A residential blueprint looks different from a commercial one, just as image processing jobs have different data requirements than video transcoding jobs. However, both follow the same foundational patterns for dimensions, materials, and safety requirements. Similarly, our data structures establish common patterns for job identification, progress tracking, and error handling while allowing specialization for different media types and processing operations.</p>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fdata-model.svg\" alt=\"Data Model Relationships\"></p>\n<h3 id=\"job-and-task-entities\">Job and Task Entities</h3>\n<p>The job and task entities form the backbone of our asynchronous processing system, representing units of work from initial submission through final completion. These structures must capture not only the processing requirements but also the complete lifecycle state, error conditions, and progress information needed for reliable distributed processing.</p>\n<p>A <strong>processing job</strong> represents a single media processing request from a client, containing the input file, desired output specifications, and all metadata needed for execution and tracking. Each job moves through a well-defined lifecycle managed by the job queue system, with state transitions triggered by worker processes and external events. The job entity serves as the primary coordination point between the client-facing API, the job queue infrastructure, and the worker processes that perform actual media processing.</p>\n<p><strong>Processing jobs</strong> contain both the specification of work to be performed and the runtime state of that work as it progresses through the system. This dual nature requires careful design to separate immutable job parameters (input file, output specifications) from mutable runtime state (current status, progress percentage, error conditions). The job entity must be serializable for storage in the job queue and database, while providing efficient access patterns for both job submission and progress tracking operations.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job_id</code></td>\n<td><code>str</code></td>\n<td>Unique identifier generated at job creation, used for all tracking and reference operations</td>\n</tr>\n<tr>\n<td><code>input_file_path</code></td>\n<td><code>str</code></td>\n<td>Absolute path to the source media file in the storage system, validated at submission time</td>\n</tr>\n<tr>\n<td><code>output_specifications</code></td>\n<td><code>List[OutputSpec]</code></td>\n<td>Collection of desired output formats, resolutions, and quality settings for this job</td>\n</tr>\n<tr>\n<td><code>priority</code></td>\n<td><code>JobPriority</code></td>\n<td>Processing priority level determining queue position and resource allocation preferences</td>\n</tr>\n<tr>\n<td><code>status</code></td>\n<td><code>JobStatus</code></td>\n<td>Current lifecycle state of the job, updated atomically by worker processes and queue system</td>\n</tr>\n<tr>\n<td><code>created_at</code></td>\n<td><code>datetime</code></td>\n<td>UTC timestamp when job was first submitted to the system</td>\n</tr>\n<tr>\n<td><code>started_at</code></td>\n<td><code>datetime</code></td>\n<td>UTC timestamp when worker process began processing, null if not yet started</td>\n</tr>\n<tr>\n<td><code>completed_at</code></td>\n<td><code>datetime</code></td>\n<td>UTC timestamp when processing finished (success or permanent failure)</td>\n</tr>\n<tr>\n<td><code>error_message</code></td>\n<td><code>str</code></td>\n<td>Human-readable error description for failed jobs, null for successful or pending jobs</td>\n</tr>\n<tr>\n<td><code>retry_count</code></td>\n<td><code>int</code></td>\n<td>Number of processing attempts made so far, incremented before each retry</td>\n</tr>\n<tr>\n<td><code>webhook_url</code></td>\n<td><code>str</code></td>\n<td>Optional HTTP endpoint for status notifications, validated as proper URL at submission</td>\n</tr>\n<tr>\n<td><code>progress_percentage</code></td>\n<td><code>float</code></td>\n<td>Current completion percentage (0.0-100.0) updated by worker during processing</td>\n</tr>\n<tr>\n<td><code>estimated_duration</code></td>\n<td><code>int</code></td>\n<td>Estimated total processing time in seconds, calculated based on file size and type</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The <code>ProcessingJob</code> structure intentionally separates immutable job specification (input file, output specs, webhook URL) from mutable runtime state (status, progress, timestamps). This separation enables safe concurrent access patterns where multiple processes can read the job specification while only the assigned worker updates the runtime state.</p>\n</blockquote>\n<p>The job priority system uses numerical values to enable flexible scheduling while maintaining intuitive semantic meaning. Higher numerical values represent higher priority, allowing the job queue to use simple numerical comparison for ordering operations.</p>\n<table>\n<thead>\n<tr>\n<th>Priority Level</th>\n<th>Numeric Value</th>\n<th>Use Case</th>\n<th>Typical Processing Time</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>JobPriority.LOW</code></td>\n<td>1</td>\n<td>Batch processing, background tasks</td>\n<td>No specific SLA</td>\n</tr>\n<tr>\n<td><code>JobPriority.NORMAL</code></td>\n<td>5</td>\n<td>Standard user uploads, scheduled processing</td>\n<td>Process within 1 hour</td>\n</tr>\n<tr>\n<td><code>JobPriority.HIGH</code></td>\n<td>10</td>\n<td>Interactive user requests, preview generation</td>\n<td>Process within 15 minutes</td>\n</tr>\n<tr>\n<td><code>JobPriority.URGENT</code></td>\n<td>20</td>\n<td>Critical system operations, error recovery</td>\n<td>Process immediately</td>\n</tr>\n</tbody></table>\n<p>The job status enumeration defines a finite state machine that governs job lifecycle management and ensures consistent state transitions across the distributed system.</p>\n<table>\n<thead>\n<tr>\n<th>Status Value</th>\n<th>Description</th>\n<th>Valid Transitions</th>\n<th>Worker Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>JobStatus.PENDING</code></td>\n<td>Job queued awaiting worker assignment</td>\n<td>→ PROCESSING</td>\n<td>Worker claims job and begins processing</td>\n</tr>\n<tr>\n<td><code>JobStatus.PROCESSING</code></td>\n<td>Job actively being processed by worker</td>\n<td>→ COMPLETED, FAILED</td>\n<td>Worker updates progress and handles completion</td>\n</tr>\n<tr>\n<td><code>JobStatus.COMPLETED</code></td>\n<td>Job finished successfully with outputs</td>\n<td>None (terminal state)</td>\n<td>Worker publishes results and cleans up</td>\n</tr>\n<tr>\n<td><code>JobStatus.FAILED</code></td>\n<td>Job failed permanently after all retries</td>\n<td>None (terminal state)</td>\n<td>Worker logs error and triggers notifications</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Single Job Entity vs Separate Task Breakdown</strong></p>\n<ul>\n<li><strong>Context</strong>: Complex processing jobs (like video transcoding) involve multiple discrete steps that could be modeled as separate task entities</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Single <code>ProcessingJob</code> entity with stage-based progress</li>\n<li><code>ProcessingJob</code> with separate <code>Task</code> entities for each processing step</li>\n<li>Hierarchical job structure with parent/child relationships</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Single <code>ProcessingJob</code> entity with stage-based progress tracking</li>\n<li><strong>Rationale</strong>: Simplifies data model and reduces coordination complexity while still providing detailed progress information through stage reporting. Most media processing operations are sequential pipelines that don&#39;t benefit from independent task scheduling.</li>\n<li><strong>Consequences</strong>: Enables simpler queue management and progress tracking, but limits ability to pause/resume individual processing steps or optimize resource allocation per step</li>\n</ul>\n</blockquote>\n<h3 id=\"media-metadata-structures\">Media Metadata Structures</h3>\n<p>Media metadata structures capture the essential characteristics of input files and processed outputs, enabling format-aware processing decisions and preserving important file attributes through the processing pipeline. These structures must handle the diversity of media formats while providing a consistent interface for processing components.</p>\n<p><strong>Media metadata</strong> serves two critical functions in our processing pipeline: informing processing decisions based on source file characteristics, and preserving important metadata through format conversions. Different media types require different metadata structures, but all follow common patterns for basic file information, format specifications, and technical parameters that affect processing operations.</p>\n<p>The metadata extraction process occurs early in job processing, with results cached to avoid repeated parsing of the same source files. This approach enables processing components to make informed decisions about optimal algorithms, quality settings, and output formats based on source file characteristics rather than relying on generic processing parameters.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>file_path</code></td>\n<td><code>str</code></td>\n<td>Absolute path to the media file in storage</td>\n</tr>\n<tr>\n<td><code>file_size</code></td>\n<td><code>int</code></td>\n<td>File size in bytes for resource planning</td>\n</tr>\n<tr>\n<td><code>mime_type</code></td>\n<td><code>str</code></td>\n<td>MIME type determined by content inspection</td>\n</tr>\n<tr>\n<td><code>format_name</code></td>\n<td><code>str</code></td>\n<td>Human-readable format name (JPEG, PNG, MP4, etc.)</td>\n</tr>\n<tr>\n<td><code>created_at</code></td>\n<td><code>datetime</code></td>\n<td>File creation timestamp from filesystem</td>\n</tr>\n<tr>\n<td><code>checksum</code></td>\n<td><code>str</code></td>\n<td>SHA-256 hash for integrity verification</td>\n</tr>\n<tr>\n<td><code>width</code></td>\n<td><code>int</code></td>\n<td>Media width in pixels (images) or video resolution</td>\n</tr>\n<tr>\n<td><code>height</code></td>\n<td><code>int</code></td>\n<td>Media height in pixels (images) or video resolution</td>\n</tr>\n</tbody></table>\n<p><strong>Image metadata</strong> extends the base media metadata with photography-specific information including color profiles, compression settings, and embedded metadata that affects processing quality and client compatibility.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>color_mode</code></td>\n<td><code>str</code></td>\n<td>Color space representation (RGB, CMYK, Grayscale, Palette)</td>\n</tr>\n<tr>\n<td><code>bit_depth</code></td>\n<td><code>int</code></td>\n<td>Bits per channel for color precision (8, 16, 32)</td>\n</tr>\n<tr>\n<td><code>compression</code></td>\n<td><code>str</code></td>\n<td>Compression algorithm used in source file</td>\n</tr>\n<tr>\n<td><code>quality</code></td>\n<td><code>int</code></td>\n<td>Original quality setting for lossy formats (1-100)</td>\n</tr>\n<tr>\n<td><code>has_transparency</code></td>\n<td><code>bool</code></td>\n<td>Whether image contains alpha channel information</td>\n</tr>\n<tr>\n<td><code>orientation</code></td>\n<td><code>int</code></td>\n<td>EXIF orientation value for rotation correction</td>\n</tr>\n<tr>\n<td><code>dpi</code></td>\n<td><code>Tuple[int, int]</code></td>\n<td>Horizontal and vertical dots per inch</td>\n</tr>\n<tr>\n<td><code>icc_profile</code></td>\n<td><code>bytes</code></td>\n<td>Embedded ICC color profile data</td>\n</tr>\n<tr>\n<td><code>exif_data</code></td>\n<td><code>Dict[str, Any]</code></td>\n<td>Complete EXIF metadata dictionary</td>\n</tr>\n</tbody></table>\n<p><strong>Video metadata</strong> captures motion picture characteristics including codecs, frame rates, and stream information necessary for transcoding decisions and quality preservation.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>duration</code></td>\n<td><code>float</code></td>\n<td>Video length in seconds with millisecond precision</td>\n</tr>\n<tr>\n<td><code>frame_rate</code></td>\n<td><code>float</code></td>\n<td>Frames per second as decimal value</td>\n</tr>\n<tr>\n<td><code>video_codec</code></td>\n<td><code>str</code></td>\n<td>Video compression codec (H.264, H.265, VP9, etc.)</td>\n</tr>\n<tr>\n<td><code>audio_codec</code></td>\n<td><code>str</code></td>\n<td>Audio compression codec (AAC, MP3, Opus, etc.)</td>\n</tr>\n<tr>\n<td><code>bitrate</code></td>\n<td><code>int</code></td>\n<td>Average bitrate in bits per second</td>\n</tr>\n<tr>\n<td><code>keyframe_interval</code></td>\n<td><code>int</code></td>\n<td>GOP size in frames for seeking optimization</td>\n</tr>\n<tr>\n<td><code>pixel_format</code></td>\n<td><code>str</code></td>\n<td>Pixel format specification (yuv420p, rgb24, etc.)</td>\n</tr>\n<tr>\n<td><code>aspect_ratio</code></td>\n<td><code>str</code></td>\n<td>Display aspect ratio (16:9, 4:3, etc.)</td>\n</tr>\n<tr>\n<td><code>audio_channels</code></td>\n<td><code>int</code></td>\n<td>Number of audio channels (1=mono, 2=stereo, etc.)</td>\n</tr>\n<tr>\n<td><code>audio_sample_rate</code></td>\n<td><code>int</code></td>\n<td>Audio sample rate in Hz (44100, 48000, etc.)</td>\n</tr>\n<tr>\n<td><code>subtitle_tracks</code></td>\n<td><code>List[str]</code></td>\n<td>Available subtitle languages and formats</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Embedded EXIF vs Separate EXIF Entity</strong></p>\n<ul>\n<li><strong>Context</strong>: EXIF metadata can be extensive and includes nested structures like GPS coordinates, camera settings, and thumbnail images</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Store complete EXIF as JSON blob in image metadata</li>\n<li>Create separate EXIF entity with structured fields</li>\n<li>Extract only commonly-used EXIF fields into image metadata</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Store complete EXIF as JSON dictionary with extracted key fields in image metadata</li>\n<li><strong>Rationale</strong>: Preserves complete EXIF data for applications that need it while providing fast access to commonly-used fields like orientation and GPS coordinates. JSON storage handles the nested and variable structure of EXIF data.</li>\n<li><strong>Consequences</strong>: Enables flexible EXIF handling and preservation while maintaining query performance for common fields, but requires JSON parsing for detailed EXIF access</li>\n</ul>\n</blockquote>\n<h3 id=\"processing-configuration-types\">Processing Configuration Types</h3>\n<p>Processing configuration structures define the parameters and constraints that control media processing operations, enabling fine-tuned control over quality, performance, and output characteristics. These configurations must balance processing quality with resource consumption while providing sensible defaults for common use cases.</p>\n<p><strong>Processing configurations</strong> serve as the bridge between high-level client requirements (resize to thumbnail, transcode for web playback) and low-level processing parameters (interpolation algorithms, codec settings, bitrate targets). The configuration system enables both simple preset-based processing and detailed parameter control for advanced use cases.</p>\n<p>Configuration structures follow a hierarchical pattern where general processing parameters are inherited by specific operation types, allowing shared settings like quality levels and resource limits while enabling specialized parameters for image resizing, video transcoding, and thumbnail generation operations.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>output_path</code></td>\n<td><code>str</code></td>\n<td>Target file path for processed output</td>\n</tr>\n<tr>\n<td><code>format</code></td>\n<td><code>str</code></td>\n<td>Output format specification (JPEG, PNG, WebP, MP4, WebM)</td>\n</tr>\n<tr>\n<td><code>quality</code></td>\n<td><code>int</code></td>\n<td>Quality level from 1-100 for lossy compression formats</td>\n</tr>\n<tr>\n<td><code>overwrite_existing</code></td>\n<td><code>bool</code></td>\n<td>Whether to replace existing files at output path</td>\n</tr>\n<tr>\n<td><code>preserve_metadata</code></td>\n<td><code>bool</code></td>\n<td>Whether to copy source metadata to output file</td>\n</tr>\n<tr>\n<td><code>strip_private_data</code></td>\n<td><code>bool</code></td>\n<td>Whether to remove GPS and personal information from metadata</td>\n</tr>\n</tbody></table>\n<p><strong>Image processing configurations</strong> specify parameters for resize operations, format conversions, and optimization techniques that balance output quality with file size and processing performance.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>target_width</code></td>\n<td><code>int</code></td>\n<td>Desired output width in pixels, null to preserve aspect ratio</td>\n</tr>\n<tr>\n<td><code>target_height</code></td>\n<td><code>int</code></td>\n<td>Desired output height in pixels, null to preserve aspect ratio</td>\n</tr>\n<tr>\n<td><code>resize_mode</code></td>\n<td><code>str</code></td>\n<td>Scaling behavior: &#39;fit&#39;, &#39;fill&#39;, &#39;crop&#39;, &#39;stretch&#39;</td>\n</tr>\n<tr>\n<td><code>interpolation</code></td>\n<td><code>str</code></td>\n<td>Resampling algorithm: &#39;nearest&#39;, &#39;bilinear&#39;, &#39;bicubic&#39;, &#39;lanczos&#39;</td>\n</tr>\n<tr>\n<td><code>crop_position</code></td>\n<td><code>str</code></td>\n<td>Crop anchor point: &#39;center&#39;, &#39;top&#39;, &#39;bottom&#39;, &#39;left&#39;, &#39;right&#39;</td>\n</tr>\n<tr>\n<td><code>background_color</code></td>\n<td><code>str</code></td>\n<td>Fill color for letterboxing in hex format (#FFFFFF)</td>\n</tr>\n<tr>\n<td><code>sharpen_amount</code></td>\n<td><code>float</code></td>\n<td>Post-resize sharpening intensity (0.0-2.0)</td>\n</tr>\n<tr>\n<td><code>optimize_for_web</code></td>\n<td><code>bool</code></td>\n<td>Apply web-specific optimizations (progressive JPEG, etc.)</td>\n</tr>\n<tr>\n<td><code>max_colors</code></td>\n<td><code>int</code></td>\n<td>Color palette size for PNG optimization</td>\n</tr>\n</tbody></table>\n<p><strong>Video transcoding configurations</strong> control codec selection, quality settings, and streaming optimizations for different playback scenarios and device capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>video_codec</code></td>\n<td><code>str</code></td>\n<td>Output video codec: &#39;h264&#39;, &#39;h265&#39;, &#39;vp9&#39;, &#39;av1&#39;</td>\n</tr>\n<tr>\n<td><code>audio_codec</code></td>\n<td><code>str</code></td>\n<td>Output audio codec: &#39;aac&#39;, &#39;mp3&#39;, &#39;opus&#39;, &#39;vorbis&#39;</td>\n</tr>\n<tr>\n<td><code>target_bitrate</code></td>\n<td><code>int</code></td>\n<td>Average bitrate in kbps, null for CRF encoding</td>\n</tr>\n<tr>\n<td><code>crf_value</code></td>\n<td><code>int</code></td>\n<td>Constant Rate Factor for quality-based encoding (18-28)</td>\n</tr>\n<tr>\n<td><code>frame_rate</code></td>\n<td><code>float</code></td>\n<td>Output frame rate, null to preserve source frame rate</td>\n</tr>\n<tr>\n<td><code>keyframe_interval</code></td>\n<td><code>int</code></td>\n<td>GOP size in seconds for streaming optimization</td>\n</tr>\n<tr>\n<td><code>preset</code></td>\n<td><code>str</code></td>\n<td>Encoding speed preset: &#39;ultrafast&#39;, &#39;fast&#39;, &#39;medium&#39;, &#39;slow&#39;</td>\n</tr>\n<tr>\n<td><code>profile</code></td>\n<td><code>str</code></td>\n<td>Codec profile: &#39;baseline&#39;, &#39;main&#39;, &#39;high&#39; for H.264 compatibility</td>\n</tr>\n<tr>\n<td><code>level</code></td>\n<td><code>str</code></td>\n<td>Codec level constraint for device compatibility</td>\n</tr>\n<tr>\n<td><code>two_pass_encoding</code></td>\n<td><code>bool</code></td>\n<td>Use two-pass encoding for optimal bitrate distribution</td>\n</tr>\n</tbody></table>\n<p><strong>Adaptive bitrate configurations</strong> define multiple quality variants for streaming applications, enabling clients to select appropriate quality based on network conditions and device capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>variant_name</code></td>\n<td><code>str</code></td>\n<td>Human-readable quality identifier (&#39;720p&#39;, &#39;1080p&#39;, etc.)</td>\n</tr>\n<tr>\n<td><code>width</code></td>\n<td><code>int</code></td>\n<td>Video width for this quality variant</td>\n</tr>\n<tr>\n<td><code>height</code></td>\n<td><code>int</code></td>\n<td>Video height for this quality variant</td>\n</tr>\n<tr>\n<td><code>bitrate</code></td>\n<td><code>int</code></td>\n<td>Target bitrate in kbps for this variant</td>\n</tr>\n<tr>\n<td><code>max_bitrate</code></td>\n<td><code>int</code></td>\n<td>Maximum bitrate ceiling in kbps</td>\n</tr>\n<tr>\n<td><code>buffer_size</code></td>\n<td><code>int</code></td>\n<td>Rate control buffer size in kbits</td>\n</tr>\n<tr>\n<td><code>audio_bitrate</code></td>\n<td><code>int</code></td>\n<td>Audio bitrate in kbps for this variant</td>\n</tr>\n<tr>\n<td><code>segment_duration</code></td>\n<td><code>int</code></td>\n<td>HLS/DASH segment length in seconds</td>\n</tr>\n</tbody></table>\n<p>The configuration system uses a preset-based approach for common scenarios while allowing parameter overrides for specialized requirements. This design enables simple integration for typical use cases while maintaining flexibility for advanced applications.</p>\n<table>\n<thead>\n<tr>\n<th>Preset Name</th>\n<th>Use Case</th>\n<th>Default Parameters</th>\n<th>Typical Output Size</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>thumbnail</code></td>\n<td>Small preview images</td>\n<td>150x150 crop, JPEG quality 80</td>\n<td>5-15 KB</td>\n</tr>\n<tr>\n<td><code>web_optimized</code></td>\n<td>Website display images</td>\n<td>Max 1920px, WebP quality 85</td>\n<td>50-200 KB</td>\n</tr>\n<tr>\n<td><code>mobile_video</code></td>\n<td>Mobile streaming</td>\n<td>720p H.264, 1.5 Mbps</td>\n<td>10-20% of source</td>\n</tr>\n<tr>\n<td><code>web_video</code></td>\n<td>Desktop streaming</td>\n<td>1080p H.264, 3 Mbps</td>\n<td>20-30% of source</td>\n</tr>\n<tr>\n<td><code>archive_quality</code></td>\n<td>Long-term storage</td>\n<td>Lossless or very high quality</td>\n<td>80-95% of source</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Preset-Based vs Parameter-Based Configuration</strong></p>\n<ul>\n<li><strong>Context</strong>: Clients need both simple configuration for common cases and detailed control for specialized requirements</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Pure preset system with fixed parameters</li>\n<li>Pure parameter system requiring all settings</li>\n<li>Hybrid system with presets and parameter overrides</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid system with presets as starting points and parameter overrides</li>\n<li><strong>Rationale</strong>: Provides ease of use for common scenarios while maintaining flexibility for advanced requirements. Presets encode best practices and tested parameter combinations.</li>\n<li><strong>Consequences</strong>: Enables rapid integration and consistent results for typical use cases while supporting advanced customization, but requires careful preset design and parameter validation</li>\n</ul>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Quality Parameter Confusion</strong></p>\n<p>A common mistake is assuming that quality parameters work the same way across all formats and codecs. JPEG quality of 85 produces very different file sizes and visual quality compared to WebP quality 85 or H.264 CRF 23. Each format has its own quality scale, compression characteristics, and optimal parameter ranges.</p>\n<p><strong>Why this is wrong</strong>: Using the same quality value across different formats can result in unnecessarily large files (over-encoding) or poor visual quality (under-encoding). For example, WebP typically achieves the same visual quality as JPEG at 15-20% lower quality settings.</p>\n<p><strong>How to fix it</strong>: Use format-specific quality mapping in your configuration system. Define quality levels semantically (&#39;low&#39;, &#39;medium&#39;, &#39;high&#39;, &#39;maximum&#39;) and map them to appropriate numerical values for each format. Provide format-specific presets that encode best practices for each codec and container combination.</p>\n<p>⚠️ <strong>Pitfall: Missing Resource Constraints</strong></p>\n<p>Processing configurations often omit resource limits, leading to worker processes consuming excessive memory or CPU time when processing large files or using computationally expensive settings like very slow encoding presets or high-quality interpolation algorithms.</p>\n<p><strong>Why this is wrong</strong>: Without resource constraints, a single large video transcoding job can consume all available system memory, causing the worker process to crash or the entire system to become unresponsive. High-quality processing settings can extend job execution time from minutes to hours.</p>\n<p><strong>How to fix it</strong>: Include resource constraint fields in your configuration types: <code>max_memory_mb</code>, <code>max_processing_time_seconds</code>, <code>cpu_threads</code>. Implement resource monitoring in your worker processes to enforce these limits and fail jobs gracefully when constraints are exceeded.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The data model implementation focuses on creating clean, serializable data structures that support both SQL database storage and Redis job queue operations. Python&#39;s dataclasses provide an excellent foundation for these structures while maintaining compatibility with JSON serialization for API communication and queue message formats.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Validation</td>\n<td>Python dataclasses with type hints</td>\n<td>Pydantic models with validation</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>Built-in <code>json</code> module</td>\n<td><code>msgpack</code> for binary efficiency</td>\n</tr>\n<tr>\n<td>Configuration Loading</td>\n<td>YAML with <code>pyyaml</code></td>\n<td>TOML with <code>tomli</code> for type safety</td>\n</tr>\n<tr>\n<td>Enum Management</td>\n<td>Python <code>enum.Enum</code></td>\n<td><code>enum.IntEnum</code> for database compatibility</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media_processing/\n  models/\n    __init__.py                 ← export all public types\n    base.py                     ← base classes and common fields\n    jobs.py                     ← ProcessingJob and related types  \n    media.py                    ← media metadata structures\n    config.py                   ← processing configuration types\n    enums.py                    ← status, priority, format enums\n  serializers/\n    __init__.py\n    json_serializer.py          ← JSON serialization helpers\n    queue_serializer.py         ← queue message format handling</code></pre></div>\n\n<p><strong>Base Data Structure Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># models/base.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseModel</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all data models with common serialization support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert model to dictionary with datetime serialization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use dataclasses.asdict to get base dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Iterate through values and convert datetime objects to ISO strings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Convert enums to their string/int values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle None values appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return clean dictionary ready for JSON serialization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create model instance from dictionary with type conversion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get class field annotations for type information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse datetime strings back to datetime objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Convert enum string/int values back to enum instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle optional fields with None defaults</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create and return class instance with converted values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_json</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize model to JSON string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> json.dumps(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.to_dict(), </span><span style=\"color:#FFAB70\">separators</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">','</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">':'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_json</span><span style=\"color:#E1E4E8\">(cls, json_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deserialize model from JSON string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">.from_dict(json.loads(json_str))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_job_id</span><span style=\"color:#E1E4E8\">() -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate unique job identifier with timestamp prefix for sorting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.utcnow().strftime(</span><span style=\"color:#9ECBFF\">'%Y%m</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">_%H%M%S'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    unique_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(uuid.uuid4())[:</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">timestamp</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">unique_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Enumeration Definitions:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># models/enums.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum, IntEnum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Job lifecycle status enumeration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROCESSING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"processing\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_terminal</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if status represents a finished job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> (JobStatus.</span><span style=\"color:#79B8FF\">COMPLETED</span><span style=\"color:#E1E4E8\">, JobStatus.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> can_transition_to</span><span style=\"color:#E1E4E8\">(self, new_status: </span><span style=\"color:#9ECBFF\">'JobStatus'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate state transition rules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Define valid transitions from current status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return True only for legal state changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Prevent transitions from terminal states</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobPriority</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">IntEnum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Job priority levels with numeric values for queue ordering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOW</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NORMAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HIGH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    URGENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 20</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_string</span><span style=\"color:#E1E4E8\">(cls, priority_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'JobPriority'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Parse priority from string representation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Handle case-insensitive string matching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return appropriate JobPriority enum value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Raise ValueError for invalid priority strings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MediaFormat</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Supported media format enumeration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JPEG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"jpeg\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PNG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"png\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WEBP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"webp\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AVIF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"avif\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MP4</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"mp4\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WEBM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"webm\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GIF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"gif\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_image_format</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if format is for still images.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> (MediaFormat.</span><span style=\"color:#79B8FF\">JPEG</span><span style=\"color:#E1E4E8\">, MediaFormat.</span><span style=\"color:#79B8FF\">PNG</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       MediaFormat.</span><span style=\"color:#79B8FF\">WEBP</span><span style=\"color:#E1E4E8\">, MediaFormat.</span><span style=\"color:#79B8FF\">AVIF</span><span style=\"color:#E1E4E8\">, MediaFormat.</span><span style=\"color:#79B8FF\">GIF</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> is_video_format</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if format is for video content.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> (MediaFormat.</span><span style=\"color:#79B8FF\">MP4</span><span style=\"color:#E1E4E8\">, MediaFormat.</span><span style=\"color:#79B8FF\">WEBM</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Core Job Entity Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># models/jobs.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .base </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> BaseModel, generate_job_id</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .enums </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobStatus, JobPriority</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OutputSpecification</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseModel</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Specification for a single output file from processing job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    format</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    width: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    height: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    quality: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add additional format-specific parameters as needed</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProcessingJob</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseModel</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Core job entity representing a media processing request.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">generate_job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_file_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_specifications: List[OutputSpecification] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: JobPriority </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobPriority.</span><span style=\"color:#79B8FF\">NORMAL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: JobStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">PENDING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.utcnow)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completed_at: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhook_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    progress_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_duration: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_started</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark job as started and update status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set started_at to current UTC time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Change status to PROCESSING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate that job was in PENDING status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_progress</span><span style=\"color:#E1E4E8\">(self, percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update job progress with validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate percentage is between 0.0 and 100.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Ensure progress only increases (no going backwards)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update progress_percentage field</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log progress update with stage information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_completed</span><span style=\"color:#E1E4E8\">(self, output_files: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark job as successfully completed.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set completed_at to current UTC time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Change status to COMPLETED  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set progress_percentage to 100.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate that output files exist and are readable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_failed</span><span style=\"color:#E1E4E8\">(self, error_msg: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, retryable: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark job as failed with error details.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set error_message field</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set completed_at to current UTC time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Increment retry_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set status to FAILED only if max retries exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Keep status as PENDING if retryable and under retry limit</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the data model structures:</p>\n<ol>\n<li><strong>Run the data model tests:</strong> <code>python -m pytest tests/models/ -v</code></li>\n<li><strong>Expected output:</strong> All serialization and validation tests should pass</li>\n<li><strong>Manual verification:</strong> Create a <code>ProcessingJob</code> instance, serialize it to JSON, deserialize it back, and verify all fields match</li>\n<li><strong>Test job state transitions:</strong> Verify that status transitions follow the defined state machine rules</li>\n<li><strong>Signs of problems:</strong> <ul>\n<li>Serialization errors indicate missing type conversion handling</li>\n<li>State transition failures suggest incomplete validation logic</li>\n<li>Datetime handling issues point to timezone or format problems</li>\n</ul>\n</li>\n</ol>\n<p><strong>Language-Specific Python Tips:</strong></p>\n<ul>\n<li>Use <code>dataclasses.field(default_factory=list)</code> for mutable default values to avoid shared state bugs</li>\n<li>Implement <code>__post_init__</code> method for complex validation that requires multiple fields</li>\n<li>Use <code>typing.Optional[T]</code> for nullable fields and handle None values in serialization</li>\n<li>Consider <code>pydantic</code> models instead of dataclasses for production systems requiring extensive validation</li>\n<li>Use <code>datetime.utcnow()</code> consistently for all timestamps to avoid timezone confusion</li>\n<li>Implement custom JSON serialization for complex types like <code>datetime</code> and enums</li>\n</ul>\n<h2 id=\"image-processing-component\">Image Processing Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 (Image Processing) - this section covers the core image manipulation, format conversion, and metadata handling capabilities</p>\n</blockquote>\n<h3 id=\"mental-model-digital-darkroom\">Mental Model: Digital Darkroom</h3>\n<p>Think of the image processing component as a sophisticated <strong>digital darkroom</strong> with automated equipment. In a traditional film darkroom, photographers would manipulate exposure, contrast, and cropping using enlargers, filters, and chemical baths. Our digital darkroom operates on the same principles but with pixel-perfect precision and unlimited repeatability.</p>\n<p>Just as a darkroom technician follows a recipe card specifying exposure time, paper grade, and chemical concentrations, our image processor follows an <code>ImageProcessingConfig</code> that specifies target dimensions, quality settings, and output formats. The &quot;enlarger&quot; is our resize algorithm that can scale images up or down with various interpolation methods. The &quot;cropping easel&quot; becomes our intelligent crop positioning that can center-crop, smart-crop, or preserve aspect ratios. The &quot;chemical baths&quot; for different paper types become our format converters that transform JPEG to WebP or PNG to AVIF while optimizing for web delivery.</p>\n<p>The critical difference is that our digital darkroom can process hundreds of images simultaneously across multiple &quot;stations&quot; (worker processes) and automatically handle the complex chemistry of color spaces, bit depths, and compression algorithms that would require expert knowledge in the physical world. Each processing job represents a batch of prints from a single negative, where we might produce thumbnails (contact sheet), medium-resolution previews (work prints), and high-quality finals (exhibition prints) all from the same source image.</p>\n<h3 id=\"core-image-operations\">Core Image Operations</h3>\n<p>The image processing component implements four fundamental operations that mirror real-world image manipulation workflows but leverage computational precision for optimal results.</p>\n<p><strong>Image Loading and Format Detection</strong></p>\n<p>The image loader serves as the component&#39;s entry point, responsible for reading diverse image formats into a standardized in-memory representation. This process involves more complexity than simply opening a file because images arrive with varying color spaces, bit depths, compression schemes, and metadata structures.</p>\n<p>The loader first performs <strong>format detection</strong> by examining file headers rather than relying on file extensions, which can be misleading or absent. JPEG files begin with the hex signature <code>FFD8FF</code>, PNG files start with <code>89504E47</code>, and WebP files contain <code>52494646</code> followed by <code>57454250</code>. This signature-based detection prevents processing failures when files have incorrect extensions or no extensions at all.</p>\n<p>Once the format is identified, the loader delegates to format-specific decoders that handle the intricacies of each format. JPEG decoding involves Huffman decompression and DCT (Discrete Cosine Transform) coefficient reconstruction. PNG decoding requires LZ77 decompression and filter row processing. WebP can contain either lossy VP8 or lossless VP8L data, requiring different decoder paths.</p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Input</th>\n<th>Output</th>\n<th>Critical Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Format Detection</td>\n<td>Raw file bytes</td>\n<td>Format enum + confidence</td>\n<td>Must handle corrupted headers, ambiguous signatures</td>\n</tr>\n<tr>\n<td>JPEG Decoding</td>\n<td>JPEG bitstream</td>\n<td>RGB pixel array</td>\n<td>EXIF orientation, CMYK→RGB conversion, progressive vs baseline</td>\n</tr>\n<tr>\n<td>PNG Decoding</td>\n<td>PNG bitstream</td>\n<td>RGBA pixel array</td>\n<td>Transparency handling, gamma correction, color profile application</td>\n</tr>\n<tr>\n<td>WebP Decoding</td>\n<td>WebP container</td>\n<td>RGB/RGBA array</td>\n<td>Lossy vs lossless detection, animation frame extraction</td>\n</tr>\n<tr>\n<td>GIF Decoding</td>\n<td>GIF bitstream</td>\n<td>RGB array + palette</td>\n<td>First frame extraction, transparency index handling</td>\n</tr>\n</tbody></table>\n<p><strong>Resize Algorithms and Interpolation</strong></p>\n<p>Image resizing is mathematically equivalent to <strong>resampling a discrete signal</strong> in two dimensions. When scaling an image, we must calculate new pixel values at positions that don&#39;t correspond exactly to original pixel locations. The interpolation algorithm determines how we estimate these new values, directly impacting visual quality and processing time.</p>\n<p><strong>Nearest-neighbor interpolation</strong> simply copies the value of the closest original pixel. This preserves sharp edges and is computationally fastest, making it suitable for pixel art or when preserving exact color values is critical. However, it produces jaggy, blocky results for photographic content.</p>\n<p><strong>Bilinear interpolation</strong> computes each new pixel as a weighted average of the four nearest original pixels. The weights are based on distance, creating smooth gradients but potentially causing slight blurriness. This strikes a good balance for general-purpose resizing.</p>\n<p><strong>Bicubic interpolation</strong> considers a 4×4 grid of surrounding pixels, using cubic polynomial curves to estimate new values. This produces sharper results than bilinear but requires significantly more computation. Bicubic is particularly effective for upscaling operations where detail preservation is paramount.</p>\n<p><strong>Lanczos resampling</strong> applies a sophisticated windowed sinc function that minimizes aliasing artifacts while preserving edge sharpness. This is the gold standard for high-quality downscaling, especially when reducing images to small thumbnail sizes where detail loss would otherwise be severe.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The optimal interpolation algorithm depends on the scaling direction and ratio. Lanczos excels for downscaling by factors of 2× or more, while bicubic is preferred for modest upscaling. For real-time previews, bilinear provides acceptable quality with minimal latency.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Quality Score</th>\n<th>Speed Score</th>\n<th>Best Use Cases</th>\n<th>Computational Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Nearest-Neighbor</td>\n<td>2/10</td>\n<td>10/10</td>\n<td>Pixel art, exact color preservation</td>\n<td>O(1) per pixel</td>\n</tr>\n<tr>\n<td>Bilinear</td>\n<td>6/10</td>\n<td>8/10</td>\n<td>General-purpose, real-time previews</td>\n<td>O(4) per pixel</td>\n</tr>\n<tr>\n<td>Bicubic</td>\n<td>8/10</td>\n<td>5/10</td>\n<td>High-quality upscaling, print preparation</td>\n<td>O(16) per pixel</td>\n</tr>\n<tr>\n<td>Lanczos</td>\n<td>9/10</td>\n<td>3/10</td>\n<td>Thumbnail generation, dramatic downscaling</td>\n<td>O(64) per pixel</td>\n</tr>\n</tbody></table>\n<p><strong>Aspect Ratio Handling and Smart Cropping</strong></p>\n<p>When target dimensions don&#39;t match the source image&#39;s aspect ratio, the system must choose between <strong>distortion, letterboxing, or cropping</strong>. Each approach serves different use cases and requires careful implementation to avoid common pitfalls.</p>\n<p><strong>Preserve aspect ratio</strong> scaling calculates the maximum scale factor that fits the image within target bounds without distortion. For a 1600×1200 source targeting 800×400, the limiting factor is height (1200→400 = 3×), so the result becomes 800×267 with potential letterboxing to reach exactly 800×400.</p>\n<p><strong>Center cropping</strong> first scales the image so its smallest dimension matches the target, then crops from the center. This ensures the target dimensions are met exactly but may remove important content from image edges. The algorithm scales to fill the larger target dimension, then crops the excess from the perpendicular dimension.</p>\n<p><strong>Smart cropping</strong> attempts to identify the most visually interesting region before cropping. Simple implementations use edge detection to find high-contrast areas. More sophisticated versions analyze face detection, rule-of-thirds positioning, or entropy maps to preserve the most important content.</p>\n<blockquote>\n<p><strong>Decision: Smart Cropping Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: Users upload images with varying compositions, and center cropping often removes important subjects</li>\n<li><strong>Options Considered</strong>: Center crop only, edge-detection based smart crop, ML-based composition analysis</li>\n<li><strong>Decision</strong>: Implement entropy-based smart cropping with center crop fallback</li>\n<li><strong>Rationale</strong>: Entropy-based cropping identifies visually complex regions without requiring ML models, providing better results than center crop while remaining computationally efficient</li>\n<li><strong>Consequences</strong>: Enables better automatic thumbnails but requires additional processing time; some images may still benefit from manual crop positioning</li>\n</ul>\n</blockquote>\n<p><strong>Format Conversion and Quality Optimization</strong></p>\n<p>Format conversion involves more than changing file extensions—it requires understanding the capabilities and limitations of each target format, then optimizing encoding parameters for the intended use case.</p>\n<p><strong>JPEG optimization</strong> centers on the quality parameter (1-100) which controls quantization table scaling. Quality 85-95 provides excellent visual results for most photographic content while achieving significant compression. The <code>optimize</code> flag enables Huffman table optimization, typically reducing file size by 5-10% with minimal processing overhead.</p>\n<p><strong>PNG optimization</strong> focuses on compression level (0-9) and filter selection. PNG uses DEFLATE compression after applying prediction filters to reduce entropy. Filter selection can be automatic (Pillow chooses per-row) or fixed. For images with large solid areas, higher compression levels (7-9) provide substantial size reduction. For images with high-frequency detail, the computational cost of level 9 may not justify the modest size improvement.</p>\n<p><strong>WebP encoding</strong> offers both lossy and lossless modes. Lossy WebP typically achieves 25-30% smaller file sizes than equivalent-quality JPEG while supporting alpha transparency. The quality parameter (0-100) behaves similarly to JPEG. Lossless WebP compresses better than PNG for photographic content but worse for simple graphics with few colors.</p>\n<table>\n<thead>\n<tr>\n<th>Format</th>\n<th>Compression Type</th>\n<th>Alpha Support</th>\n<th>Animation Support</th>\n<th>Browser Support</th>\n<th>Typical Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JPEG</td>\n<td>Lossy only</td>\n<td>No</td>\n<td>No</td>\n<td>Universal</td>\n<td>Photographs, complex images</td>\n</tr>\n<tr>\n<td>PNG</td>\n<td>Lossless only</td>\n<td>Yes</td>\n<td>No</td>\n<td>Universal</td>\n<td>Graphics, logos, screenshots</td>\n</tr>\n<tr>\n<td>WebP</td>\n<td>Lossy + Lossless</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Modern browsers</td>\n<td>Web-optimized photos and graphics</td>\n</tr>\n<tr>\n<td>AVIF</td>\n<td>Lossy + Lossless</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Very modern</td>\n<td>Next-gen web images, highest compression</td>\n</tr>\n</tbody></table>\n<h3 id=\"exif-and-metadata-management\">EXIF and Metadata Management</h3>\n<p>Image metadata encompasses technical camera settings, geolocation data, copyright information, and processing history. Proper metadata handling is crucial for legal compliance, user privacy, and image display accuracy.</p>\n<p><strong>EXIF Data Structure and Extraction</strong></p>\n<p>EXIF (Exchangeable Image File Format) data is embedded within JPEG and TIFF files as a series of <strong>IFD (Image File Directory) entries</strong>. Each entry contains a tag ID, data type, count, and value or value offset. The structure is hierarchical, with primary IFD containing basic image information and sub-IFDs containing specialized data like GPS coordinates or camera-specific settings.</p>\n<p>The most critical EXIF tag for image processing is <strong>Orientation</strong> (tag 0x0112), which indicates how the camera was rotated when the photo was taken. Digital cameras often save images in their sensor&#39;s native orientation, then record the intended viewing orientation in EXIF. Failing to handle this correctly results in sideways or upside-down images after processing.</p>\n<table>\n<thead>\n<tr>\n<th>EXIF Tag</th>\n<th>Tag ID</th>\n<th>Data Type</th>\n<th>Critical for Processing</th>\n<th>Privacy Implications</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Orientation</td>\n<td>0x0112</td>\n<td>SHORT</td>\n<td>Yes - affects display rotation</td>\n<td>None</td>\n</tr>\n<tr>\n<td>DateTime</td>\n<td>0x0132</td>\n<td>ASCII</td>\n<td>No - metadata only</td>\n<td>Medium - reveals when photo taken</td>\n</tr>\n<tr>\n<td>GPS Latitude</td>\n<td>0x0002</td>\n<td>RATIONAL</td>\n<td>No - metadata only</td>\n<td>High - reveals location</td>\n</tr>\n<tr>\n<td>GPS Longitude</td>\n<td>0x0004</td>\n<td>RATIONAL</td>\n<td>No - metadata only</td>\n<td>High - reveals location</td>\n</tr>\n<tr>\n<td>Camera Make</td>\n<td>0x010F</td>\n<td>ASCII</td>\n<td>No - metadata only</td>\n<td>Low - device information</td>\n</tr>\n<tr>\n<td>Camera Model</td>\n<td>0x0110</td>\n<td>ASCII</td>\n<td>No - metadata only</td>\n<td>Low - device information</td>\n</tr>\n<tr>\n<td>Software</td>\n<td>0x0131</td>\n<td>ASCII</td>\n<td>No - metadata only</td>\n<td>Low - processing software</td>\n</tr>\n</tbody></table>\n<p><strong>Orientation Handling and Rotation</strong></p>\n<p>EXIF orientation values range from 1-8, representing different combinations of rotation and mirroring. The standard defines these transformations relative to the top-left corner of the image as stored in the file.</p>\n<table>\n<thead>\n<tr>\n<th>Orientation Value</th>\n<th>Transformation Required</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>None</td>\n<td>Normal (top-left)</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Horizontal flip</td>\n<td>Mirrored</td>\n</tr>\n<tr>\n<td>3</td>\n<td>180° rotation</td>\n<td>Upside down</td>\n</tr>\n<tr>\n<td>4</td>\n<td>Vertical flip</td>\n<td>Mirrored upside down</td>\n</tr>\n<tr>\n<td>5</td>\n<td>90° CCW + horizontal flip</td>\n<td>Mirrored + rotated left</td>\n</tr>\n<tr>\n<td>6</td>\n<td>90° CW</td>\n<td>Rotated right</td>\n</tr>\n<tr>\n<td>7</td>\n<td>90° CW + horizontal flip</td>\n<td>Mirrored + rotated right</td>\n</tr>\n<tr>\n<td>8</td>\n<td>90° CCW</td>\n<td>Rotated left</td>\n</tr>\n</tbody></table>\n<p>The critical processing step is applying the orientation transformation <strong>before</strong> any resize or crop operations. This ensures that user-specified dimensions refer to the correctly oriented image. After applying the transformation, the orientation tag should be reset to 1 (normal) or removed entirely to prevent downstream applications from re-applying the correction.</p>\n<p><strong>Privacy and Metadata Stripping</strong></p>\n<p>For web-facing applications, metadata stripping is essential for user privacy protection. GPS coordinates can reveal home addresses, timestamps can establish presence at specific locations, and camera serial numbers can link images to specific devices across platforms.</p>\n<p>The system implements <strong>configurable metadata preservation</strong> based on output usage. Internal processing might preserve technical metadata for quality analysis, while public-facing outputs strip all potentially sensitive information.</p>\n<blockquote>\n<p><strong>Decision: Selective Metadata Preservation</strong></p>\n<ul>\n<li><strong>Context</strong>: Different use cases require different metadata handling—internal analysis needs technical data while public sharing requires privacy protection</li>\n<li><strong>Options Considered</strong>: Strip all metadata, preserve all metadata, configurable per-output-type</li>\n<li><strong>Decision</strong>: Implement three metadata modes: preserve_all, preserve_technical, strip_all</li>\n<li><strong>Rationale</strong>: Provides flexibility for different use cases while defaulting to privacy-safe stripping for web outputs</li>\n<li><strong>Consequences</strong>: Enables compliance with privacy regulations but requires careful configuration management to prevent accidental data exposure</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Metadata Category</th>\n<th>preserve_all</th>\n<th>preserve_technical</th>\n<th>strip_all</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Camera settings (ISO, aperture, etc.)</td>\n<td>✓</td>\n<td>✓</td>\n<td>✗</td>\n</tr>\n<tr>\n<td>GPS coordinates</td>\n<td>✓</td>\n<td>✗</td>\n<td>✗</td>\n</tr>\n<tr>\n<td>Timestamps</td>\n<td>✓</td>\n<td>✗</td>\n<td>✗</td>\n</tr>\n<tr>\n<td>Device serial numbers</td>\n<td>✓</td>\n<td>✗</td>\n<td>✗</td>\n</tr>\n<tr>\n<td>Color profile (ICC)</td>\n<td>✓</td>\n<td>✓</td>\n<td>✗</td>\n</tr>\n<tr>\n<td>Processing software</td>\n<td>✓</td>\n<td>✓</td>\n<td>✗</td>\n</tr>\n</tbody></table>\n<h3 id=\"image-processing-architecture-decisions\">Image Processing Architecture Decisions</h3>\n<p>The image processing component&#39;s architecture must balance performance, quality, memory usage, and format compatibility while remaining maintainable and extensible.</p>\n<blockquote>\n<p><strong>Decision: Pillow vs OpenCV vs ImageIO</strong></p>\n<ul>\n<li><strong>Context</strong>: Python offers multiple image processing libraries with different performance characteristics and feature sets</li>\n<li><strong>Options Considered</strong>: Pillow (PIL fork), OpenCV, ImageIO + scikit-image</li>\n<li><strong>Decision</strong>: Use Pillow as primary library with ImageIO for AVIF support</li>\n<li><strong>Rationale</strong>: Pillow provides excellent format support, straightforward API, and robust EXIF handling. OpenCV is overkill for basic operations and has complex deployment requirements. ImageIO fills format gaps.</li>\n<li><strong>Consequences</strong>: Enables rapid development and broad format support but may require performance optimization for high-throughput scenarios</li>\n</ul>\n</blockquote>\n<p><strong>Memory Management Strategy</strong></p>\n<p>Large images can quickly exhaust available memory, especially when processing multiple images concurrently. A 24-megapixel image (6000×4000 pixels) requires approximately 72MB of RAM when loaded as uncompressed RGB data (6000 × 4000 × 3 bytes). Processing operations often require additional temporary buffers, potentially doubling or tripling memory usage.</p>\n<p>The system implements <strong>streaming processing</strong> for operations that support it, processing image tiles rather than loading entire images into memory. For operations requiring global image access (like smart cropping), the system enforces <strong>memory limits per worker process</strong> and queues large images for processing by workers with sufficient available memory.</p>\n<table>\n<thead>\n<tr>\n<th>Image Size</th>\n<th>Uncompressed RGB</th>\n<th>Peak Processing Memory</th>\n<th>Recommended Worker Limit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1MP (1024×1024)</td>\n<td>3MB</td>\n<td>9MB</td>\n<td>16 workers per 1GB RAM</td>\n</tr>\n<tr>\n<td>6MP (3000×2000)</td>\n<td>18MB</td>\n<td>54MB</td>\n<td>4 workers per 1GB RAM</td>\n</tr>\n<tr>\n<td>24MP (6000×4000)</td>\n<td>72MB</td>\n<td>216MB</td>\n<td>1 worker per 1GB RAM</td>\n</tr>\n<tr>\n<td>50MP+ (8000×6000)</td>\n<td>144MB+</td>\n<td>432MB+</td>\n<td>Queue for dedicated high-memory workers</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Tile-Based vs Full-Image Processing</strong></p>\n<ul>\n<li><strong>Context</strong>: Large images can exhaust worker process memory and impact overall system stability</li>\n<li><strong>Options Considered</strong>: Always load full image, tile-based streaming, hybrid approach based on image size</li>\n<li><strong>Decision</strong>: Implement hybrid approach—tile-based for resize/conversion, full-image for smart cropping</li>\n<li><strong>Rationale</strong>: Resize and format conversion can be done on tiles, reducing memory usage. Smart cropping requires global image analysis.</li>\n<li><strong>Consequences</strong>: Reduces memory footprint for large images but adds complexity for operations requiring global analysis</li>\n</ul>\n</blockquote>\n<p><strong>Color Space and Profile Management</strong></p>\n<p>Digital images can exist in various color spaces (RGB, CMYK, LAB, etc.) and may include ICC color profiles that define how colors should be interpreted for display or printing. Naive processing can result in significant color shifts or incorrect color reproduction.</p>\n<p>The system standardizes on <strong>sRGB as the working color space</strong> for web delivery, as it&#39;s supported universally by browsers and provides predictable color reproduction on most displays. Images with embedded ICC profiles undergo color space conversion during loading to ensure consistent color handling.</p>\n<p>CMYK images (common in print workflows) require special handling because they cannot be directly displayed on RGB monitors. The conversion process involves ICC profile-based transformation or, when profiles are unavailable, approximation using standard CMYK-to-RGB conversion matrices.</p>\n<p><strong>Thread Safety and Concurrent Processing</strong></p>\n<p>Pillow&#39;s underlying image objects are <strong>not thread-safe</strong>, meaning the same image instance cannot be safely accessed from multiple threads simultaneously. However, separate image instances can be processed concurrently without issues.</p>\n<p>The system architecture ensures thread safety by creating <strong>separate image instances per processing task</strong> and never sharing image objects between worker threads. Each processing job operates on its own memory space and file handles, eliminating race conditions and allowing full parallel processing.</p>\n<h3 id=\"common-image-processing-pitfalls\">Common Image Processing Pitfalls</h3>\n<p>Understanding and avoiding these common mistakes is crucial for building a robust image processing system that handles real-world image diversity.</p>\n<p>⚠️ <strong>Pitfall: Ignoring EXIF Orientation</strong></p>\n<p>Many developers load images and immediately begin processing without checking EXIF orientation data. This results in images that appear correctly in photo viewers (which honor EXIF orientation) but display incorrectly after processing (when EXIF orientation is lost or ignored).</p>\n<p><strong>Why it&#39;s wrong</strong>: Modern smartphones and digital cameras often store images in their sensor&#39;s native orientation (landscape) and record the intended viewing orientation in EXIF metadata. Processing the raw pixel data without applying the orientation transformation produces sideways or upside-down results.</p>\n<p><strong>How to fix</strong>: Always read and apply EXIF orientation before any other processing operations. Use Pillow&#39;s <code>ImageOps.exif_transpose()</code> function which automatically handles all 8 orientation cases. After applying the transformation, either remove the orientation EXIF tag or set it to 1 (normal orientation).</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Correct approach - handle orientation first</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">image </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Image.open(input_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">image </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ImageOps.exif_transpose(image)  </span><span style=\"color:#6A737D\"># Apply EXIF rotation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Now proceed with resize, crop, format conversion</span></span></code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Inappropriate Interpolation Algorithm Selection</strong></p>\n<p>Using the same interpolation algorithm for all resize operations leads to suboptimal quality. Many developers default to bilinear interpolation for everything, missing opportunities for better quality or faster processing.</p>\n<p><strong>Why it&#39;s wrong</strong>: Different scaling factors and image types benefit from different algorithms. Nearest-neighbor preserves sharp pixel boundaries for sprites and pixel art. Lanczos minimizes aliasing for dramatic downscaling. Bicubic provides better upscaling quality than bilinear.</p>\n<p><strong>How to fix</strong>: Implement algorithm selection logic based on scaling factor and image characteristics. Use Lanczos for downscaling by 2× or more, bicubic for upscaling, and bilinear for real-time previews or modest scaling.</p>\n<p>⚠️ <strong>Pitfall: Memory Exhaustion with Large Images</strong></p>\n<p>Loading very large images (50+ megapixels) into memory without size checks can crash worker processes or exhaust system memory, affecting other concurrent operations.</p>\n<p><strong>Why it&#39;s wrong</strong>: A 100-megapixel image requires 300MB just to store the RGB pixel data, before any processing operations that might require additional temporary buffers. Multiple workers processing large images simultaneously can quickly exhaust available RAM.</p>\n<p><strong>How to fix</strong>: Implement image dimension checks before loading. For images exceeding memory thresholds, either reject them with appropriate error messages, queue them for specialized high-memory workers, or implement tile-based processing for supported operations.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Color Space Conversion</strong></p>\n<p>Processing CMYK images as if they were RGB, or failing to handle embedded ICC color profiles, results in dramatic color shifts and unprofessional output quality.</p>\n<p><strong>Why it&#39;s wrong</strong>: CMYK uses subtractive color mixing (like printing inks) while RGB uses additive color mixing (like display pixels). Direct interpretation of CMYK values as RGB produces inverted, muddy colors. Similarly, images with wide-gamut color profiles may appear oversaturated or color-shifted when viewed in sRGB.</p>\n<p><strong>How to fix</strong>: Always check the image&#39;s color mode and convert to RGB using proper color management. Use ICC profile-based conversion when profiles are available, or standard conversion matrices as fallback. Pillow&#39;s <code>Image.convert(&#39;RGB&#39;)</code> handles basic cases, but professional workflows may require more sophisticated color management.</p>\n<p>⚠️ <strong>Pitfall: Lossy Recompression Cascades</strong></p>\n<p>Repeatedly loading and saving JPEG images during multi-step processing operations accumulates compression artifacts and degrades image quality with each iteration.</p>\n<p><strong>Why it&#39;s wrong</strong>: JPEG uses lossy compression, meaning some image information is permanently discarded during each save operation. Processing workflows that load JPEG → process → save JPEG → load JPEG → process → save JPEG accumulate generational loss, similar to making photocopies of photocopies.</p>\n<p><strong>How to fix</strong>: Maintain images in uncompressed format (PNG or in-memory bitmap) throughout multi-step processing pipelines. Only apply final compression when generating the ultimate output files. If intermediate files must be saved, use lossless PNG for temporary storage.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Quality Parameter Interpretation</strong></p>\n<p>Different image formats interpret quality parameters differently, leading to inconsistent file sizes and visual quality when converting between formats.</p>\n<p><strong>Why it&#39;s wrong</strong>: JPEG quality 85 and WebP quality 85 produce different visual quality levels and file sizes because they use different compression algorithms and quality scales. Users expecting consistent results across formats may be surprised by significant variations.</p>\n<p><strong>How to fix</strong>: Implement format-specific quality mapping or target file size algorithms. Document quality parameter behavior clearly, or provide high-level quality settings (like &quot;web_optimized&quot;, &quot;high_quality&quot;, &quot;maximum_compression&quot;) that map to appropriate format-specific parameters.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations-table\">Technology Recommendations Table</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image Library</td>\n<td>Pillow (PIL) with basic operations</td>\n<td>Pillow + ImageIO + pyheif for extended format support</td>\n</tr>\n<tr>\n<td>EXIF Handling</td>\n<td>Pillow&#39;s built-in EXIF support</td>\n<td>ExifRead library for detailed metadata parsing</td>\n</tr>\n<tr>\n<td>Color Management</td>\n<td>Pillow&#39;s basic color conversion</td>\n<td>colour-science library for professional color workflows</td>\n</tr>\n<tr>\n<td>Performance Optimization</td>\n<td>Sequential processing with memory limits</td>\n<td>Wand (ImageMagick binding) for high-performance operations</td>\n</tr>\n<tr>\n<td>Format Support</td>\n<td>JPEG, PNG, WebP via Pillow</td>\n<td>Add AVIF, HEIF via ImageIO plugins</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-filemodule-structure\">Recommended File/Module Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media_processing/\n  image_processing/\n    __init__.py                    ← component exports\n    core/\n      processor.py                 ← ImageProcessor main class\n      operations.py                ← resize, crop, convert functions\n      metadata.py                  ← EXIF handling and metadata management\n      formats.py                   ← format detection and loader registry\n    config/\n      processing_config.py         ← ImageProcessingConfig and related types\n      quality_profiles.py          ← predefined quality settings\n    utils/\n      color_space.py              ← color conversion utilities\n      interpolation.py            ← algorithm selection logic\n      memory.py                   ← memory management helpers\n    tests/\n      test_processor.py           ← main processor tests\n      test_operations.py          ← operation-specific tests\n      fixtures/                   ← test images in various formats\n        sample.jpg\n        sample.png\n        sample_with_exif.jpg</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Format Detection and Image Loader</strong> (complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># image_processing/core/formats.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> io</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Tuple, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#79B8FF\"> PIL</span><span style=\"color:#F97583\"> import</span><span style=\"color:#E1E4E8\"> Image, ExifTags</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#79B8FF\"> PIL</span><span style=\"color:#E1E4E8\">.ExifTags </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ORIENTATION</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SupportedFormat</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JPEG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"JPEG\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PNG</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"PNG\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    WEBP</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"WEBP\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GIF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"GIF\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AVIF</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"AVIF\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FormatDetector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Detects image formats by file signature rather than extension.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SIGNATURES</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        b</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\xff\\xd8\\xff</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">: SupportedFormat.</span><span style=\"color:#79B8FF\">JPEG</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        b</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\x89</span><span style=\"color:#9ECBFF\">PNG</span><span style=\"color:#79B8FF\">\\r\\n\\x1a\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">: SupportedFormat.</span><span style=\"color:#79B8FF\">PNG</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        b</span><span style=\"color:#9ECBFF\">'RIFF'</span><span style=\"color:#E1E4E8\">: SupportedFormat.</span><span style=\"color:#79B8FF\">WEBP</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Needs additional validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        b</span><span style=\"color:#9ECBFF\">'GIF87a'</span><span style=\"color:#E1E4E8\">: SupportedFormat.</span><span style=\"color:#79B8FF\">GIF</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        b</span><span style=\"color:#9ECBFF\">'GIF89a'</span><span style=\"color:#E1E4E8\">: SupportedFormat.</span><span style=\"color:#79B8FF\">GIF</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_format</span><span style=\"color:#E1E4E8\">(cls, file_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[SupportedFormat]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect image format by reading file signature.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(file_path, </span><span style=\"color:#9ECBFF\">'rb'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                header </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f.read(</span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Check standard signatures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> sig, fmt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">SIGNATURES</span><span style=\"color:#E1E4E8\">.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> header.startswith(sig):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> fmt </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> SupportedFormat.</span><span style=\"color:#79B8FF\">WEBP</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # Verify WebP by checking for WEBP signature at offset 8</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        return</span><span style=\"color:#E1E4E8\"> fmt </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> header[</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#F97583\"> b</span><span style=\"color:#9ECBFF\">'WEBP'</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> fmt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Check for AVIF (more complex signature)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">._is_avif(header):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> SupportedFormat.</span><span style=\"color:#79B8FF\">AVIF</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">IOError</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">IndexError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _is_avif</span><span style=\"color:#E1E4E8\">(header: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check for AVIF format signature.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(header) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 12</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                header[</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#F97583\"> b</span><span style=\"color:#9ECBFF\">'ftyp'</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                header[</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">:</span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">'avif'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">'avis'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ImageLoader</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Loads images from various formats into standardized PIL Image objects.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.detector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FormatDetector()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_image</span><span style=\"color:#E1E4E8\">(self, file_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple[Image.Image, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load image and extract metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Detect format</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        detected_format </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.detector.detect_format(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> detected_format:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unsupported image format: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">file_path</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Load with PIL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        image </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Image.open(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._extract_metadata(image, detected_format)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> image, metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _extract_metadata</span><span style=\"color:#E1E4E8\">(self, image: Image.Image, fmt: SupportedFormat) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract comprehensive metadata from loaded image.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'format'</span><span style=\"color:#E1E4E8\">: fmt.value,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'size'</span><span style=\"color:#E1E4E8\">: image.size,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'mode'</span><span style=\"color:#E1E4E8\">: image.mode,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'has_transparency'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._has_transparency(image),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract EXIF if available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(image, </span><span style=\"color:#9ECBFF\">'_getexif'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> image._getexif():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exif_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exif </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> image._getexif()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> tag_id, value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> exif.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tag </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ExifTags.</span><span style=\"color:#79B8FF\">TAGS</span><span style=\"color:#E1E4E8\">.get(tag_id, tag_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                exif_dict[tag] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metadata[</span><span style=\"color:#9ECBFF\">'exif'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exif_dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Extract orientation specifically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> ORIENTATION</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> exif:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                metadata[</span><span style=\"color:#9ECBFF\">'orientation'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exif[</span><span style=\"color:#79B8FF\">ORIENTATION</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _has_transparency</span><span style=\"color:#E1E4E8\">(image: Image.Image) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if image has transparency channel.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (image.mode </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">'RGBA'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'LA'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'transparency'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> image.info)</span></span></code></pre></div>\n\n<p><strong>Memory Management Helper</strong> (complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># image_processing/utils/memory.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#79B8FF\"> PIL</span><span style=\"color:#F97583\"> import</span><span style=\"color:#E1E4E8\"> Image</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages memory usage for image processing operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_memory_mb: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_memory_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_memory_mb</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_memory_bytes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_memory_mb </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_image_memory_requirements</span><span style=\"color:#E1E4E8\">(self, width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, height: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                      channels: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate memory required for image in bytes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Base memory for pixel data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pixel_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> width </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> height </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> channels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add overhead for processing (temporary buffers, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Typical operations need 2-3x the base memory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pixel_memory </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> total_memory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> can_process_image</span><span style=\"color:#E1E4E8\">(self, image_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if image can be safely processed within memory limits.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Get image dimensions without fully loading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> Image.open(image_path) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> img:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                width, height </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> img.size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                channels </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(img.getbands())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            required_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.check_image_memory_requirements(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                width, height, channels)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> required_memory </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_memory_bytes:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Image requires </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">required_memory </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> //</span><span style=\"color:#79B8FF\"> 1024}</span><span style=\"color:#9ECBFF\">MB, limit is </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.max_memory_mb</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">MB\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Check current system memory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            available_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.virtual_memory().available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> required_memory </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> available_memory </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># Leave 20% buffer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Insufficient system memory available\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Cannot analyze image: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_optimal_tile_size</span><span style=\"color:#E1E4E8\">(self, image_width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, image_height: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             target_memory_mb: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 64</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate optimal tile size for streaming processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        target_pixels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (target_memory_mb </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">//</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># RGB + overhead</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Try to maintain aspect ratio while staying under memory limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        aspect_ratio </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> image_width </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> image_height</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> target_pixels </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> image_width </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> image_height:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> image_width, image_height  </span><span style=\"color:#6A737D\"># Can process whole image</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate tile dimensions maintaining aspect ratio</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tile_height </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">((target_pixels </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> aspect_ratio) </span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tile_width </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(tile_height </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> aspect_ratio)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Ensure minimum tile size for quality</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tile_width </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(tile_width, </span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tile_height </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(tile_height, </span><span style=\"color:#79B8FF\">256</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> tile_width, tile_height</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Main Image Processor</strong> (signatures + TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># image_processing/core/processor.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#79B8FF\"> PIL</span><span style=\"color:#F97583\"> import</span><span style=\"color:#E1E4E8\"> Image, ImageOps</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..config.processing_config </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ImageProcessingConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..utils.memory </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MemoryManager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ImageProcessor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main image processing coordinator that orchestrates operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, memory_limit_mb: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 512</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MemoryManager(memory_limit_mb)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.format_loader </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ImageLoader()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> process_image</span><span style=\"color:#E1E4E8\">(self, input_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_specs: List[OutputSpecification]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Process a single image according to multiple output specifications.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns list of generated output file paths.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load and validate input image using ImageLoader</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check memory requirements using MemoryManager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply EXIF orientation correction using ImageOps.exif_transpose()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each output specification:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4a: Calculate resize parameters (target size, interpolation method)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4b: Apply resize operation with appropriate algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4c: Apply cropping if aspect ratios don't match</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4d: Convert to target format with quality settings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4e: Handle metadata according to privacy settings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4f: Save optimized output file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return list of successfully generated output paths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use try/except around each output spec to continue processing others on failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> resize_image</span><span style=\"color:#E1E4E8\">(self, image: Image.Image, target_width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, target_height: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    interpolation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"lanczos\"</span><span style=\"color:#E1E4E8\">, resize_mode: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"fit\"</span><span style=\"color:#E1E4E8\">) -> Image.Image:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Resize image to target dimensions with specified algorithm and mode.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            image: Source PIL Image</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            target_width, target_height: Target dimensions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            interpolation: Algorithm (\"nearest\", \"bilinear\", \"bicubic\", \"lanczos\")  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            resize_mode: How to handle aspect ratio (\"fit\", \"fill\", \"crop\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Select PIL resampling filter based on interpolation parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate scaling factors for width and height</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If resize_mode is \"fit\": scale to fit within bounds, preserve aspect ratio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If resize_mode is \"fill\": scale to fill bounds exactly (may distort)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If resize_mode is \"crop\": scale to fill bounds, crop excess</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply the resize operation using PIL Image.resize()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return resized image</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use Image.Resampling.LANCZOS etc for PIL resampling constants</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> convert_format</span><span style=\"color:#E1E4E8\">(self, image: Image.Image, target_format: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, quality: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 85</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      optimize: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Convert image to target format with specified quality settings.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns the converted image as bytes.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate target format against supported formats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle color mode conversions (RGBA->RGB for JPEG, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set format-specific parameters (quality for JPEG/WebP, compression for PNG)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create BytesIO buffer for output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Save image to buffer with format and parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return buffer contents as bytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: JPEG doesn't support transparency, convert RGBA to RGB with background</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_thumbnail</span><span style=\"color:#E1E4E8\">(self, image: Image.Image, size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 256</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          crop_mode: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"center\"</span><span style=\"color:#E1E4E8\">) -> Image.Image:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate square thumbnail with smart cropping.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            image: Source PIL Image</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            size: Target thumbnail size (square)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            crop_mode: Cropping strategy (\"center\", \"smart\", \"top\", \"bottom\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate current aspect ratio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If crop_mode is \"center\": crop from center to square aspect ratio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If crop_mode is \"smart\": use entropy-based cropping to find interesting region</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If crop_mode is \"top\"/\"bottom\": crop from specified edge</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Resize the cropped square image to target size using appropriate interpolation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply sharpening filter if thumbnail is significantly smaller than source</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return thumbnail image</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: For smart cropping, analyze image entropy in sliding windows</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> extract_and_handle_metadata</span><span style=\"color:#E1E4E8\">(self, image: Image.Image, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   metadata_mode: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"strip_all\"</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Extract metadata and filter according to privacy settings.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract EXIF data using PIL image.getexif()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Parse GPS coordinates if present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract camera information (make, model, settings)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Based on metadata_mode, filter what to preserve:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - \"preserve_all\": keep everything</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - \"preserve_technical\": keep camera settings, strip GPS/timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #   - \"strip_all\": remove all metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return filtered metadata dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use ExifTags.TAGS to convert numeric tag IDs to readable names</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the image processing component, verify functionality with these tests:</p>\n<p><strong>Expected Behavior After Implementation:</strong></p>\n<ol>\n<li><strong>Format Detection</strong>: Run <code>python -m image_processing.test_formats</code> - should correctly identify JPEG, PNG, WebP files by signature</li>\n<li><strong>EXIF Orientation</strong>: Process a rotated smartphone photo - output should be correctly oriented even if input appears sideways</li>\n<li><strong>Memory Management</strong>: Attempt to process a very large image (&gt;100MB) - should either process successfully or fail gracefully with memory limit message</li>\n<li><strong>Quality Settings</strong>: Convert the same image to JPEG at quality 50, 85, and 95 - file sizes should vary significantly with visible quality differences</li>\n<li><strong>Thumbnail Generation</strong>: Generate thumbnails from landscape and portrait images - all should be properly cropped squares at specified size</li>\n</ol>\n<p><strong>Commands to Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test basic functionality</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from image_processing.core.processor import ImageProcessor</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">processor = ImageProcessor()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">outputs = processor.process_image('test.jpg', [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    OutputSpecification('thumb.jpg', 'JPEG', 256, 256, 85),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    OutputSpecification('web.webp', 'WEBP', 1200, 800, 80)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Generated: {outputs}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test memory limits</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from image_processing.utils.memory import MemoryManager</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">mem = MemoryManager(max_memory_mb=100)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">can_process, reason = mem.can_process_image('large_image.jpg') </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Can process: {can_process}, Reason: {reason}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Signs Something Is Wrong:</strong></p>\n<ul>\n<li>Images appear rotated after processing → Check EXIF orientation handling</li>\n<li>Process crashes with large images → Verify memory management implementation</li>\n<li>Poor thumbnail quality → Check interpolation algorithm selection</li>\n<li>Wrong colors in output → Verify color space conversion (CMYK→RGB)</li>\n<li>Metadata leaking in outputs → Check metadata stripping implementation</li>\n</ul>\n<h2 id=\"video-transcoding-component\">Video Transcoding Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 (Video Transcoding) - this section covers video format conversion, adaptive bitrate encoding, thumbnail extraction, and FFmpeg integration</p>\n</blockquote>\n<h3 id=\"mental-model-film-studio-pipeline\">Mental Model: Film Studio Pipeline</h3>\n<p>Think of the video transcoding component as a <strong>modern film studio&#39;s post-production pipeline</strong>. Just as a film studio receives raw footage from cameras and transforms it into multiple distribution formats for theaters, streaming services, and home video, our video transcoding component takes input video files and produces multiple optimized versions for different playback scenarios.</p>\n<p>In a film studio, the workflow follows a structured pipeline. First, the <strong>dailies</strong> (raw footage) are ingested and cataloged with metadata about format, resolution, and quality. Then, the <strong>color correction</strong> and <strong>audio sync</strong> departments work their magic to optimize the content. Next, the <strong>mastering</strong> department creates different versions - a high-quality theatrical print, compressed versions for streaming, and lower-bitrate versions for mobile devices. Finally, the <strong>quality control</strong> department validates each output before distribution.</p>\n<p>Our video transcoding component mirrors this workflow exactly. The <strong>FFmpeg integration layer</strong> acts as our sophisticated film processing equipment - it can handle virtually any input format and codec, just like professional film equipment can work with different film stocks and digital formats. The <strong>adaptive bitrate streaming</strong> component functions as our distribution mastering department, creating multiple quality variants from a single source. The <strong>progress tracking</strong> system serves as our production coordinator, keeping stakeholders informed about which films are in what stage of processing.</p>\n<p>This mental model is crucial because video transcoding involves the same core challenges as film production: <strong>quality versus file size trade-offs</strong>, <strong>format compatibility</strong> across different playback devices, <strong>processing time management</strong> for large files, and <strong>workflow coordination</strong> across multiple processing stages. Understanding this parallel helps developers appreciate why video transcoding requires sophisticated orchestration rather than simple format conversion.</p>\n<h3 id=\"ffmpeg-integration-layer\">FFmpeg Integration Layer</h3>\n<p>The <strong>FFmpeg integration layer</strong> serves as the foundational component that wraps the powerful FFmpeg command-line tools in a Python-friendly interface. FFmpeg is the industry-standard Swiss Army knife for multimedia processing, capable of handling hundreds of video codecs, container formats, and processing operations. However, integrating FFmpeg into a production system requires careful handling of process management, progress parsing, error interpretation, and resource monitoring.</p>\n<p>The integration layer abstracts FFmpeg&#39;s complexity behind clean Python interfaces while preserving access to its full capabilities. This approach allows the media processing pipeline to leverage FFmpeg&#39;s battle-tested encoding algorithms and format support without requiring developers to become experts in FFmpeg&#39;s extensive command-line syntax.</p>\n<p><strong>Core FFmpeg Wrapper Architecture</strong></p>\n<p>The wrapper architecture centers around a <code>FFmpegProcessor</code> class that manages FFmpeg subprocess execution with sophisticated error handling and progress monitoring. The processor maintains a <strong>command builder pattern</strong> that constructs FFmpeg commands from high-level configuration objects, ensuring type safety and preventing common command-line construction errors.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Key Methods</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>FFmpegProcessor</code></td>\n<td>Process lifecycle management</td>\n<td><code>execute_command</code>, <code>parse_progress</code>, <code>handle_errors</code></td>\n</tr>\n<tr>\n<td><code>CommandBuilder</code></td>\n<td>FFmpeg command construction</td>\n<td><code>build_transcode_command</code>, <code>build_thumbnail_command</code>, <code>add_codec_options</code></td>\n</tr>\n<tr>\n<td><code>ProgressParser</code></td>\n<td>Real-time progress extraction</td>\n<td><code>parse_ffmpeg_output</code>, <code>calculate_percentage</code>, <code>estimate_remaining_time</code></td>\n</tr>\n<tr>\n<td><code>ErrorHandler</code></td>\n<td>FFmpeg error interpretation</td>\n<td><code>classify_error_type</code>, <code>suggest_recovery_action</code>, <code>extract_error_details</code></td>\n</tr>\n</tbody></table>\n<p>The <code>FFmpegProcessor</code> implements a <strong>non-blocking execution model</strong> that spawns FFmpeg as a subprocess while continuously monitoring its stderr output for progress updates and error conditions. This design prevents long-running video transcoding operations from blocking the worker process and enables real-time progress reporting to end users.</p>\n<p><strong>Command Construction and Validation</strong></p>\n<p>The command builder component translates high-level <code>VideoTranscodingConfig</code> objects into properly formatted FFmpeg command lines. This translation layer is critical because FFmpeg&#39;s command-line interface is extremely flexible but also prone to subtle errors that can result in failed transcoding jobs or suboptimal output quality.</p>\n<table>\n<thead>\n<tr>\n<th>FFmpeg Parameter Category</th>\n<th>Configuration Fields</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Handling</td>\n<td><code>input_file_path</code>, <code>input_format</code></td>\n<td>File existence, format detection, stream analysis</td>\n</tr>\n<tr>\n<td>Video Encoding</td>\n<td><code>video_codec</code>, <code>crf_value</code>, <code>preset</code>, <code>profile</code></td>\n<td>Codec compatibility, CRF range (0-51), preset validation</td>\n</tr>\n<tr>\n<td>Audio Processing</td>\n<td><code>audio_codec</code>, <code>audio_bitrate</code>, <code>audio_channels</code></td>\n<td>Channel count limits, bitrate ranges, codec support</td>\n</tr>\n<tr>\n<td>Output Options</td>\n<td><code>output_path</code>, <code>container_format</code>, <code>optimize_flags</code></td>\n<td>Path writability, format/codec compatibility</td>\n</tr>\n<tr>\n<td>Advanced Settings</td>\n<td><code>two_pass_encoding</code>, <code>keyframe_interval</code>, <code>pixel_format</code></td>\n<td>GOP structure validation, pixel format support</td>\n</tr>\n</tbody></table>\n<p>The command builder implements <strong>defensive validation</strong> at multiple levels. First, it validates individual parameter values against known ranges and supported options. Then, it performs <strong>cross-parameter validation</strong> to ensure codec and container format compatibility. Finally, it adds <strong>optimization flags</strong> specific to the target use case, such as web streaming optimization or file size minimization.</p>\n<p><strong>Progress Parsing and Monitoring</strong></p>\n<p>FFmpeg outputs detailed progress information to stderr in a semi-structured format that requires careful parsing to extract meaningful progress metrics. The progress parser component implements a <strong>state machine</strong> that tracks FFmpeg&#39;s processing phases and translates raw output into standardized progress updates.</p>\n<table>\n<thead>\n<tr>\n<th>Processing Phase</th>\n<th>FFmpeg Output Indicators</th>\n<th>Progress Calculation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Analysis</td>\n<td>&quot;Input #0&quot;, stream detection</td>\n<td>5% of total progress</td>\n</tr>\n<tr>\n<td>Video Processing</td>\n<td>frame count, time position</td>\n<td>(current_time / total_duration) * 0.85</td>\n</tr>\n<tr>\n<td>Audio Processing</td>\n<td>audio frame processing</td>\n<td>Combined with video progress</td>\n</tr>\n<tr>\n<td>Output Finalization</td>\n<td>muxing, index writing</td>\n<td>Final 10% of progress</td>\n</tr>\n</tbody></table>\n<p>The progress calculation uses <strong>duration-based estimation</strong> rather than frame-based counting because frame processing rates vary significantly based on video complexity, encoding settings, and system performance. The parser maintains a <strong>rolling average</strong> of processing speed to provide increasingly accurate time estimates as transcoding progresses.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: FFmpeg progress parsing is inherently imprecise because encoding complexity varies throughout a video file. Scene changes, motion complexity, and audio dynamics all affect processing speed. The progress parser focuses on providing directionally accurate estimates rather than precise percentages.</p>\n</blockquote>\n<p><strong>Error Classification and Recovery</strong></p>\n<p>FFmpeg can fail for dozens of different reasons, from unsupported codecs to insufficient disk space to corrupted input files. The error handler component implements a <strong>hierarchical error classification system</strong> that categorizes failures by their root cause and suggests appropriate recovery strategies.</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Example Causes</th>\n<th>Recovery Strategy</th>\n<th>Retry Eligible</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Errors</td>\n<td>Corrupted file, unsupported format</td>\n<td>Validate input, suggest format conversion</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Configuration Errors</td>\n<td>Invalid codec combination, bad parameters</td>\n<td>Adjust encoding settings, use fallback profile</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>Resource Errors</td>\n<td>Insufficient memory, disk full</td>\n<td>Wait and retry, reduce quality settings</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>System Errors</td>\n<td>FFmpeg binary missing, permission denied</td>\n<td>Check system configuration, adjust paths</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>The error classification logic uses <strong>pattern matching</strong> against FFmpeg&#39;s stderr output combined with <strong>exit code analysis</strong> to determine the failure category. This dual approach is necessary because FFmpeg&#39;s error reporting is inconsistent - some errors produce specific exit codes while others require parsing text output for diagnostic information.</p>\n<blockquote>\n<p><strong>Decision: Process Isolation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: FFmpeg processing can consume significant system resources and occasionally crash or hang, potentially affecting other jobs</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Run FFmpeg in same process with threading</li>\n<li>Spawn FFmpeg as subprocess with resource limits</li>\n<li>Use containerized FFmpeg execution</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Subprocess execution with resource monitoring and timeouts</li>\n<li><strong>Rationale</strong>: Provides process isolation while maintaining simplicity. Resource limits prevent runaway processes, and subprocess cleanup ensures system stability</li>\n<li><strong>Consequences</strong>: Enables robust error handling and resource management at the cost of slightly increased system overhead</li>\n</ul>\n</blockquote>\n<h3 id=\"adaptive-bitrate-streaming\">Adaptive Bitrate Streaming</h3>\n<p><strong>Adaptive Bitrate Streaming (ABR)</strong> represents one of the most sophisticated aspects of modern video delivery. ABR enables video players to dynamically adjust quality based on network conditions, device capabilities, and user preferences by providing multiple quality variants of the same content. The video transcoding component must generate these variants efficiently while ensuring smooth transitions between quality levels during playback.</p>\n<p>The ABR implementation centers around creating <strong>quality ladders</strong> - sets of video renditions at different resolutions and bitrates that cover the full spectrum from mobile networks to high-speed broadband. Each quality ladder must be carefully designed to provide meaningful quality improvements while avoiding wasteful bandwidth usage.</p>\n<p><strong>Quality Ladder Generation</strong></p>\n<p>The quality ladder generation algorithm analyzes the source video&#39;s characteristics to determine optimal encoding targets. This analysis considers <strong>source resolution</strong>, <strong>content complexity</strong>, <strong>motion characteristics</strong>, and <strong>target delivery scenarios</strong> to create a customized quality ladder rather than applying a one-size-fits-all approach.</p>\n<table>\n<thead>\n<tr>\n<th>Resolution Tier</th>\n<th>Typical Use Case</th>\n<th>Bitrate Range</th>\n<th>Audio Bitrate</th>\n<th>Target Devices</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>240p (426×240)</td>\n<td>Very low bandwidth</td>\n<td>300-500 kbps</td>\n<td>64 kbps</td>\n<td>Feature phones, poor connections</td>\n</tr>\n<tr>\n<td>360p (640×360)</td>\n<td>Mobile networks</td>\n<td>500-800 kbps</td>\n<td>96 kbps</td>\n<td>Smartphones, tablet cellular</td>\n</tr>\n<tr>\n<td>480p (854×480)</td>\n<td>Standard definition</td>\n<td>800-1200 kbps</td>\n<td>128 kbps</td>\n<td>Laptops, tablets, smart TVs</td>\n</tr>\n<tr>\n<td>720p (1280×720)</td>\n<td>High definition</td>\n<td>1500-3000 kbps</td>\n<td>128 kbps</td>\n<td>Desktop, modern mobile devices</td>\n</tr>\n<tr>\n<td>1080p (1920×1080)</td>\n<td>Full HD</td>\n<td>3000-6000 kbps</td>\n<td>192 kbps</td>\n<td>High-end devices, good connections</td>\n</tr>\n<tr>\n<td>1440p (2560×1440)</td>\n<td>2K/QHD</td>\n<td>6000-12000 kbps</td>\n<td>256 kbps</td>\n<td>Premium displays, excellent bandwidth</td>\n</tr>\n</tbody></table>\n<p>The quality ladder algorithm implements <strong>content-aware encoding</strong> that adjusts bitrate targets based on video complexity analysis. Simple content like presentations or screencasts can achieve acceptable quality at lower bitrates, while complex content with rapid motion requires higher bitrates to maintain visual fidelity.</p>\n<p>The encoding process follows a <strong>parallel generation strategy</strong> where multiple quality variants are produced simultaneously rather than sequentially. This approach reduces total processing time by leveraging multiple CPU cores and optimizing I/O patterns across the encoding pipeline.</p>\n<p><strong>HLS (HTTP Live Streaming) Implementation</strong></p>\n<p>HLS represents Apple&#39;s adaptive streaming standard that segments video content into small chunks (typically 6-10 seconds) and creates manifest files that describe available quality variants. The HLS implementation must generate properly formatted segments and manifests that ensure compatibility across a wide range of devices and players.</p>\n<p>The HLS generation process involves several critical steps that must be executed with precise timing and format adherence:</p>\n<ol>\n<li><strong>Segment Duration Calculation</strong>: Analyze source video keyframe intervals to determine optimal segment boundaries that align with GOP (Group of Pictures) structures</li>\n<li><strong>Multi-Variant Playlist Creation</strong>: Generate the master manifest that lists all available quality variants with their technical specifications</li>\n<li><strong>Individual Stream Processing</strong>: Create separate media playlists for each quality variant with segment references and timing information</li>\n<li><strong>Encryption Key Management</strong>: Generate and rotate AES-128 encryption keys for content protection when required</li>\n<li><strong>Subtitle and Audio Track Handling</strong>: Process additional streams for multi-language support and accessibility features</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>HLS Component</th>\n<th>File Extension</th>\n<th>Purpose</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Master Playlist</td>\n<td><code>.m3u8</code></td>\n<td>Lists all variant streams</td>\n<td>Static after creation</td>\n</tr>\n<tr>\n<td>Media Playlist</td>\n<td><code>.m3u8</code></td>\n<td>Segment list for specific quality</td>\n<td>Dynamic for live streams</td>\n</tr>\n<tr>\n<td>Video Segments</td>\n<td><code>.ts</code> or <code>.m4s</code></td>\n<td>Actual video content chunks</td>\n<td>Static after creation</td>\n</tr>\n<tr>\n<td>Key Files</td>\n<td><code>.key</code></td>\n<td>Encryption keys for protected content</td>\n<td>Rotated periodically</td>\n</tr>\n</tbody></table>\n<p>The HLS encoder implements <strong>segment alignment</strong> across quality variants to ensure smooth switching between streams. This alignment requires careful keyframe placement and GOP structure coordination across all encoding passes.</p>\n<p><strong>DASH (Dynamic Adaptive Streaming over HTTP) Support</strong></p>\n<p>DASH provides an alternative adaptive streaming standard that offers more flexibility than HLS through its XML-based manifest format and support for multiple container formats. The DASH implementation complements the HLS support to ensure broad client compatibility across different platforms and players.</p>\n<p>DASH manifest generation requires creating <strong>Media Presentation Description (MPD)</strong> files that describe the temporal structure of content, available representations, and addressing schemes for segment retrieval. The MPD format is more complex than HLS playlists but provides greater expressiveness for advanced streaming scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>DASH Element</th>\n<th>XML Structure</th>\n<th>Content Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Period</td>\n<td><code>&lt;Period&gt;</code></td>\n<td>Temporal division of content</td>\n</tr>\n<tr>\n<td>AdaptationSet</td>\n<td><code>&lt;AdaptationSet&gt;</code></td>\n<td>Group of representations with same content</td>\n</tr>\n<tr>\n<td>Representation</td>\n<td><code>&lt;Representation&gt;</code></td>\n<td>Specific encoding variant</td>\n</tr>\n<tr>\n<td>SegmentTemplate</td>\n<td><code>&lt;SegmentTemplate&gt;</code></td>\n<td>URL pattern for segment addressing</td>\n</tr>\n</tbody></table>\n<p>The DASH encoder supports both <strong>SegmentTemplate</strong> and <strong>SegmentList</strong> addressing modes, with template-based addressing preferred for its efficiency and scalability. Template addressing allows clients to construct segment URLs mathematically rather than downloading explicit segment lists.</p>\n<blockquote>\n<p><strong>Decision: Dual ABR Format Support</strong></p>\n<ul>\n<li><strong>Context</strong>: Different platforms and clients prefer different adaptive streaming formats (HLS vs DASH)</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>HLS only (Apple/Safari focus)</li>\n<li>DASH only (open standard focus)</li>\n<li>Dual format support with shared segments</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Generate both HLS and DASH manifests from common segment files</li>\n<li><strong>Rationale</strong>: Maximizes client compatibility while minimizing storage overhead by reusing segment files across formats</li>\n<li><strong>Consequences</strong>: Increased processing complexity but broader device support and reduced storage costs</li>\n</ul>\n</blockquote>\n<h3 id=\"video-transcoding-architecture-decisions\">Video Transcoding Architecture Decisions</h3>\n<p>The video transcoding component requires numerous architectural decisions that significantly impact performance, quality, compatibility, and resource utilization. These decisions form the foundation for reliable, scalable video processing that meets diverse client requirements while maintaining operational efficiency.</p>\n<p><strong>Codec Selection Strategy</strong></p>\n<p>Video codec selection represents one of the most impactful architectural decisions because it directly affects output quality, file sizes, encoding performance, and client compatibility. The transcoding component implements a <strong>multi-codec strategy</strong> that selects optimal codecs based on content characteristics, target platforms, and performance requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Codec</th>\n<th>Primary Use Case</th>\n<th>Encoding Speed</th>\n<th>Quality Efficiency</th>\n<th>Browser Support</th>\n<th>Hardware Acceleration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>H.264 (AVC)</td>\n<td>Universal compatibility</td>\n<td>Fast</td>\n<td>Good</td>\n<td>Excellent (99%+)</td>\n<td>Widely available</td>\n</tr>\n<tr>\n<td>H.265 (HEVC)</td>\n<td>High efficiency, 4K content</td>\n<td>Slow</td>\n<td>Excellent</td>\n<td>Limited (60%)</td>\n<td>Growing support</td>\n</tr>\n<tr>\n<td>VP9</td>\n<td>Open source, web focus</td>\n<td>Medium</td>\n<td>Very Good</td>\n<td>Good (80%)</td>\n<td>Limited hardware support</td>\n</tr>\n<tr>\n<td>AV1</td>\n<td>Future standard, efficiency</td>\n<td>Very Slow</td>\n<td>Excellent</td>\n<td>Growing (40%)</td>\n<td>Emerging hardware support</td>\n</tr>\n</tbody></table>\n<p>The codec selection algorithm implements <strong>content analysis heuristics</strong> that examine source video characteristics to determine the most appropriate encoding approach. High-resolution content with complex motion benefits from advanced codecs like H.265 or VP9, while simple content prioritizes H.264 for universal compatibility.</p>\n<p>The system supports <strong>fallback codec chains</strong> where encoding attempts with advanced codecs automatically fall back to more compatible options if hardware acceleration is unavailable or encoding times exceed acceptable thresholds. This approach ensures job completion while optimizing for available resources.</p>\n<p><strong>Encoding Parameter Optimization</strong></p>\n<p>Video encoding involves dozens of parameters that control the trade-offs between quality, file size, and processing time. The transcoding component implements <strong>profile-based parameter management</strong> that groups related settings into coherent configurations optimized for specific use cases.</p>\n<table>\n<thead>\n<tr>\n<th>Encoding Profile</th>\n<th>Target Scenario</th>\n<th>CRF Range</th>\n<th>Preset</th>\n<th>Special Flags</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Web Optimized</td>\n<td>Streaming delivery</td>\n<td>20-24</td>\n<td>medium</td>\n<td><code>-movflags +faststart</code></td>\n</tr>\n<tr>\n<td>High Quality</td>\n<td>Archive/premium content</td>\n<td>16-20</td>\n<td>slow</td>\n<td><code>-tune film</code></td>\n</tr>\n<tr>\n<td>Mobile Optimized</td>\n<td>Cellular networks</td>\n<td>24-28</td>\n<td>fast</td>\n<td><code>-profile:v baseline</code></td>\n</tr>\n<tr>\n<td>Low Latency</td>\n<td>Live streaming</td>\n<td>22-26</td>\n<td>ultrafast</td>\n<td><code>-tune zerolatency</code></td>\n</tr>\n</tbody></table>\n<p>The <strong>Constant Rate Factor (CRF)</strong> approach provides superior quality control compared to target bitrate encoding because it maintains consistent perceptual quality across varying content complexity. The system automatically adjusts CRF values based on source resolution and target quality requirements.</p>\n<p><strong>Two-pass encoding</strong> is selectively applied for scenarios where precise bitrate control is required, such as broadcast delivery or bandwidth-constrained environments. The first pass analyzes content complexity while the second pass optimizes bit allocation based on the analysis results.</p>\n<blockquote>\n<p><strong>Decision: CRF-Based Quality Control</strong></p>\n<ul>\n<li><strong>Context</strong>: Need consistent quality across diverse content while maintaining reasonable file sizes</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Target bitrate encoding (CBR/VBR)</li>\n<li>Constant Rate Factor (CRF) encoding</li>\n<li>Constrained Quality (CQ) encoding</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: CRF as primary method with bitrate constraints for ABR</li>\n<li><strong>Rationale</strong>: CRF provides consistent perceptual quality regardless of content complexity, resulting in better user experience and more predictable quality</li>\n<li><strong>Consequences</strong>: File sizes vary based on content complexity, but quality remains consistent across all content types</li>\n</ul>\n</blockquote>\n<p><strong>Container Format and Streaming Optimization</strong></p>\n<p>Container format selection affects compatibility, streaming performance, and metadata handling capabilities. The transcoding component supports multiple container formats with optimization flags tailored to specific delivery scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Container Format</th>\n<th>Primary Use Case</th>\n<th>Streaming Support</th>\n<th>Metadata Capacity</th>\n<th>Browser Compatibility</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MP4</td>\n<td>Web delivery, mobile</td>\n<td>Excellent</td>\n<td>Good</td>\n<td>Universal</td>\n</tr>\n<tr>\n<td>WebM</td>\n<td>Open web standards</td>\n<td>Good</td>\n<td>Limited</td>\n<td>Modern browsers</td>\n</tr>\n<tr>\n<td>MKV</td>\n<td>High-quality archival</td>\n<td>Limited</td>\n<td>Extensive</td>\n<td>Desktop players</td>\n</tr>\n<tr>\n<td>HLS/TS</td>\n<td>Adaptive streaming</td>\n<td>Native</td>\n<td>Limited</td>\n<td>iOS, modern browsers</td>\n</tr>\n</tbody></table>\n<p>The MP4 container receives <strong>web streaming optimizations</strong> including the <code>faststart</code> flag that relocates metadata to the file beginning, enabling progressive download and immediate playback initiation. This optimization is crucial for user experience in web-based video players.</p>\n<p><strong>Hardware Acceleration Integration</strong></p>\n<p>Modern systems provide hardware-accelerated video encoding through GPU-based encoders like NVIDIA&#39;s NVENC, Intel&#39;s Quick Sync, and AMD&#39;s VCE. The transcoding component implements <strong>intelligent hardware acceleration</strong> that automatically detects available acceleration capabilities and falls back gracefully to software encoding when hardware acceleration is unavailable or insufficient.</p>\n<table>\n<thead>\n<tr>\n<th>Hardware Platform</th>\n<th>Acceleration Technology</th>\n<th>Supported Codecs</th>\n<th>Performance Gain</th>\n<th>Quality Trade-off</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>NVIDIA GPU</td>\n<td>NVENC</td>\n<td>H.264, H.265</td>\n<td>5-10x faster</td>\n<td>Slight quality reduction</td>\n</tr>\n<tr>\n<td>Intel CPU</td>\n<td>Quick Sync</td>\n<td>H.264, H.265, AV1</td>\n<td>3-5x faster</td>\n<td>Minimal quality impact</td>\n</tr>\n<tr>\n<td>AMD GPU</td>\n<td>VCE/AMF</td>\n<td>H.264, H.265</td>\n<td>4-8x faster</td>\n<td>Slight quality reduction</td>\n</tr>\n<tr>\n<td>Apple Silicon</td>\n<td>VideoToolbox</td>\n<td>H.264, H.265, ProRes</td>\n<td>6-12x faster</td>\n<td>High quality maintained</td>\n</tr>\n</tbody></table>\n<p>The hardware acceleration detection implements <strong>capability probing</strong> during system initialization to build a matrix of available acceleration options. This probe includes performance benchmarking to ensure hardware encoders provide meaningful speed improvements without unacceptable quality degradation.</p>\n<p>Hardware encoding failures trigger <strong>automatic fallback</strong> to software encoding to ensure job completion. The system logs hardware failures for monitoring and analysis to identify patterns that might indicate driver issues or hardware problems.</p>\n<h3 id=\"common-video-processing-pitfalls\">Common Video Processing Pitfalls</h3>\n<p>Video processing introduces numerous opportunities for subtle bugs and performance issues that can severely impact system reliability and user experience. Understanding these common pitfalls helps developers build robust video transcoding systems that handle edge cases gracefully and maintain consistent performance under diverse conditions.</p>\n<p>⚠️ <strong>Pitfall: Memory Exhaustion with High-Resolution Video</strong></p>\n<p>Video processing requires substantial memory for frame buffering, especially when handling 4K or 8K content. A single uncompressed 4K frame requires approximately 33MB of memory (3840×2160×4 bytes per pixel), and FFmpeg typically buffers multiple frames simultaneously for encoding efficiency.</p>\n<p>The primary mistake developers make is <strong>not implementing memory monitoring</strong> during video processing operations. Long-running transcoding jobs can gradually consume system memory until they trigger out-of-memory conditions that crash worker processes or destabilize the entire system.</p>\n<p><strong>Detection</strong>: Monitor process memory usage during transcoding operations. Memory consumption that grows continuously rather than stabilizing indicates a potential memory leak or insufficient memory availability for the current job.</p>\n<p><strong>Prevention</strong>: Implement <strong>resource-aware job scheduling</strong> that estimates memory requirements based on input resolution and rejects jobs that would exceed available system memory. Use FFmpeg&#39;s buffer size controls to limit memory usage during processing.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Memory estimation for job scheduling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> estimate_memory_requirements</span><span style=\"color:#E1E4E8\">(video_metadata):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    width </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> video_metadata.width</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    height </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> video_metadata.height</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Estimate memory for frame buffers (assuming 4 bytes per pixel, 8 frame buffer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    frame_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> width </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> height </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 4</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 8</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Add overhead for FFmpeg processing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> frame_memory </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1.5</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(total_memory)</span></span></code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Inaccurate Progress Estimation</strong></p>\n<p>Video transcoding progress is notoriously difficult to estimate accurately because encoding complexity varies dramatically throughout a video file. Scene changes, motion complexity, and content characteristics all affect processing speed in unpredictable ways.</p>\n<p>Many developers implement <strong>linear progress estimation</strong> based on processed time or frame count, leading to progress indicators that jump around unpredictably or stall for extended periods. This creates poor user experience and makes it difficult to detect genuinely stalled processing jobs.</p>\n<p><strong>Detection</strong>: Progress updates that move backward, remain stalled for extended periods, or jump by large amounts indicate flawed progress calculation logic.</p>\n<p><strong>Solution</strong>: Implement <strong>stage-based progress reporting</strong> that divides transcoding into distinct phases with predictable duration ranges. Use historical processing data to improve time estimates for similar content types.</p>\n<table>\n<thead>\n<tr>\n<th>Processing Stage</th>\n<th>Typical Duration %</th>\n<th>Progress Range</th>\n<th>Key Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Analysis</td>\n<td>2-5%</td>\n<td>0-5%</td>\n<td>Stream detection, metadata extraction</td>\n</tr>\n<tr>\n<td>Video Transcoding</td>\n<td>80-90%</td>\n<td>5-90%</td>\n<td>Frame processing, bitrate varies by content</td>\n</tr>\n<tr>\n<td>Audio Processing</td>\n<td>5-10%</td>\n<td>90-95%</td>\n<td>Audio encoding, typically consistent speed</td>\n</tr>\n<tr>\n<td>Output Finalization</td>\n<td>2-5%</td>\n<td>95-100%</td>\n<td>Container muxing, index generation</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Codec Compatibility Issues</strong></p>\n<p>Different combinations of video codecs, audio codecs, and container formats create complex compatibility matrices that can result in files that encode successfully but fail to play on certain devices or browsers. This is particularly problematic for web delivery where broad compatibility is essential.</p>\n<p>The most common mistake is <strong>not validating codec/container combinations</strong> before starting transcoding operations. Invalid combinations can waste significant processing time and produce unusable output files.</p>\n<p><strong>Detection</strong>: Files that encode without errors but fail validation testing on target playback platforms indicate codec compatibility problems.</p>\n<p><strong>Prevention</strong>: Implement <strong>compatibility validation matrices</strong> that verify codec and container combinations before job execution. Test output files on representative target platforms during development.</p>\n<table>\n<thead>\n<tr>\n<th>Target Platform</th>\n<th>Recommended Video Codec</th>\n<th>Audio Codec</th>\n<th>Container</th>\n<th>Profile Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>iOS Safari</td>\n<td>H.264</td>\n<td>AAC</td>\n<td>MP4</td>\n<td>Baseline/Main profile, Level 4.0 max</td>\n</tr>\n<tr>\n<td>Android Chrome</td>\n<td>H.264/VP9</td>\n<td>AAC/Opus</td>\n<td>MP4/WebM</td>\n<td>High profile acceptable</td>\n</tr>\n<tr>\n<td>Desktop Browsers</td>\n<td>H.264/VP9/AV1</td>\n<td>AAC/Opus</td>\n<td>MP4/WebM</td>\n<td>All profiles supported</td>\n</tr>\n<tr>\n<td>Smart TV</td>\n<td>H.264</td>\n<td>AAC</td>\n<td>MP4</td>\n<td>Main profile, conservative levels</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Keyframe Alignment in ABR Streams</strong></p>\n<p>Adaptive bitrate streaming requires keyframes to be aligned across all quality variants to enable smooth switching between streams during playback. Misaligned keyframes cause visual artifacts, rebuffering, or playback failures when clients attempt to switch quality levels.</p>\n<p>Many implementations <strong>generate ABR variants independently</strong> without coordinating keyframe placement, resulting in keyframes that occur at different timestamps across quality levels. This breaks the fundamental requirement for seamless quality switching.</p>\n<p><strong>Detection</strong>: ABR streams with playback stuttering or visual artifacts during quality changes indicate keyframe alignment problems. Analysis tools can detect timestamp mismatches in segment boundaries.</p>\n<p><strong>Solution</strong>: Use <strong>synchronized encoding</strong> with fixed GOP (Group of Pictures) structure across all quality variants. Force keyframes at identical timestamps for all encoding passes.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Keyframe synchronization for ABR encoding</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_abr_variants</span><span style=\"color:#E1E4E8\">(input_file, quality_configs):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Calculate keyframe intervals based on segment duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    segment_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">  # seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keyframe_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> segment_duration </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> frame_rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    common_flags </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '-force_key_frames'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'expr:gte(t,n_forced*</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">segment_duration</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '-g'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(keyframe_interval),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        '-keyint_min'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(keyframe_interval),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Apply common keyframe settings to all variants</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> quality_configs:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.encoding_flags.extend(common_flags)</span></span></code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Temporary File Management and Cleanup</strong></p>\n<p>Video processing generates numerous temporary files including intermediate encode passes, segment files, and working directories. Poor temporary file management leads to disk space exhaustion, permission issues, and security vulnerabilities from abandoned sensitive content.</p>\n<p>The most critical mistake is <strong>not implementing comprehensive cleanup logic</strong> that handles both successful completion and error scenarios. Temporary files can accumulate rapidly during high-volume processing, eventually exhausting available disk space.</p>\n<p><strong>Detection</strong>: Disk usage that grows continuously over time, even after job completion, indicates temporary file cleanup problems. Monitor temporary directory sizes and file ages.</p>\n<p><strong>Prevention</strong>: Implement <strong>context managers</strong> for temporary file handling that guarantee cleanup regardless of how processing completes. Use unique temporary directories for each job to prevent file conflicts.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tempfile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> shutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> processing_workspace</span><span style=\"color:#E1E4E8\">(job_id):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create and cleanup temporary workspace for video processing\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    temp_dir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tempfile.mkdtemp(</span><span style=\"color:#FFAB70\">prefix</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"video_job_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span><span style=\"color:#E1E4E8\"> temp_dir</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Cleanup regardless of success or failure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        shutil.rmtree(temp_dir, </span><span style=\"color:#FFAB70\">ignore_errors</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Usage ensures cleanup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">with</span><span style=\"color:#E1E4E8\"> processing_workspace(job.job_id) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> workspace:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # All temporary files created in workspace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    process_video(input_file, workspace)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Automatic cleanup when context exits</span></span></code></pre></div>\n\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fvideo-processing-sequence.svg\" alt=\"Video Transcoding Sequence\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>FFmpeg Integration</td>\n<td><code>subprocess</code> with <code>Popen</code></td>\n<td><code>asyncio.subprocess</code> with process pools</td>\n</tr>\n<tr>\n<td>Progress Parsing</td>\n<td>Regex pattern matching</td>\n<td>State machine with tokenizer</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Exit code checking</td>\n<td>ML-based error classification</td>\n</tr>\n<tr>\n<td>Temporary Files</td>\n<td><code>tempfile.mkdtemp()</code></td>\n<td>Memory-mapped temporary storage</td>\n</tr>\n<tr>\n<td>Resource Monitoring</td>\n<td><code>psutil</code> memory checking</td>\n<td>cgroup-based resource limits</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Module Structure</strong></p>\n<p>The video transcoding component integrates into the larger media processing pipeline through well-defined interfaces and module boundaries:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media_processing/\n  video/\n    __init__.py                 ← Public interface exports\n    transcoder.py              ← Main VideoTranscoder class\n    ffmpeg_wrapper.py          ← FFmpeg process management\n    progress_parser.py         ← Progress monitoring logic  \n    quality_ladder.py          ← ABR variant generation\n    codecs.py                  ← Codec compatibility matrices\n    containers.py              ← Container format handling\n    errors.py                  ← Video-specific error types\n  tests/\n    test_transcoder.py         ← Core transcoding tests\n    test_ffmpeg_wrapper.py     ← FFmpeg integration tests\n    fixtures/                  ← Sample video files for testing\n      sample_video.mp4\n      sample_audio.wav</code></pre></div>\n\n<p><strong>Complete FFmpeg Wrapper Implementation</strong></p>\n<p>This wrapper provides production-ready FFmpeg integration that handles process management, progress parsing, and error recovery:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> subprocess</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> signal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FFmpegError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"FFmpeg processing error with detailed context\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, exit_code: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, stderr: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(message)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.exit_code </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exit_code</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stderr </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stderr</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FFmpegProgressParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Parses FFmpeg stderr output for progress information\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.duration_pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">Duration: </span><span style=\"color:#79B8FF\">(\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#DBEDFF\">:</span><span style=\"color:#79B8FF\">(\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#DBEDFF\">:</span><span style=\"color:#79B8FF\">(\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.progress_pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">time=</span><span style=\"color:#79B8FF\">(\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#DBEDFF\">:</span><span style=\"color:#79B8FF\">(\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#DBEDFF\">:</span><span style=\"color:#79B8FF\">(\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(\\d</span><span style=\"color:#F97583\">{2}</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.total_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_duration</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract total duration from FFmpeg output\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        match </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.duration_pattern.search(line)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> match:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            hours, minutes, seconds, centiseconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> map</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, match.groups())</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.total_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hours </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> minutes </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> seconds </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> centiseconds </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.total_duration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse_progress</span><span style=\"color:#E1E4E8\">(self, line: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract current progress from FFmpeg output\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        match </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.progress_pattern.search(line)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> match </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.total_duration </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            hours, minutes, seconds, centiseconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> map</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, match.groups())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hours </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> minutes </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> seconds </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> centiseconds </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(current_time </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.total_duration, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FFmpegWrapper</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Production-ready FFmpeg process wrapper\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, ffmpeg_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"ffmpeg\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.ffmpeg_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ffmpeg_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> execute_command</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        command: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        progress_callback: Optional[Callable[[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> subprocess.CompletedProcess:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute FFmpeg command with progress monitoring and timeout\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        full_command </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.ffmpeg_path] </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> command</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Executing FFmpeg: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">.join(full_command)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> FFmpegProgressParser()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        process </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Start FFmpeg process with stderr capture</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            process </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> asyncio.create_subprocess_exec(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                *</span><span style=\"color:#E1E4E8\">full_command,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                stdout</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">subprocess.</span><span style=\"color:#79B8FF\">PIPE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                stderr</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">subprocess.</span><span style=\"color:#79B8FF\">PIPE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                preexec_fn</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> os.name </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'nt'</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> os.setsid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Monitor progress in real-time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            stderr_lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    line </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> asyncio.wait_for(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        process.stderr.readline(), </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30.0</span><span style=\"color:#6A737D\">  # Line timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> line:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        break</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    line_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> line.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">errors</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'ignore'</span><span style=\"color:#E1E4E8\">).strip()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    stderr_lines.append(line_str)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Parse duration on first encounter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    parser.parse_duration(line_str)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Report progress updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> progress_callback:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        progress </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse_progress(line_str)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        if</span><span style=\"color:#E1E4E8\"> progress </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            progress_callback(progress)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                except</span><span style=\"color:#E1E4E8\"> asyncio.TimeoutError:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Check if process is still alive</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> process.returncode </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        break</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#9ECBFF\">\"FFmpeg output timeout, continuing...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait for process completion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> asyncio.wait_for(process.wait(), </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timeout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            stderr_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">.join(stderr_lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> process.returncode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#E1E4E8\"> FFmpegError(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"FFmpeg failed with exit code </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">process.returncode</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    exit_code</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">process.returncode,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    stderr</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">stderr_output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> subprocess.CompletedProcess(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                args</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">full_command,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                returncode</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">process.returncode,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                stdout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># We don't capture stdout</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                stderr</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">stderr_output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> asyncio.TimeoutError:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> process:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._terminate_process(process)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#E1E4E8\"> FFmpegError(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"FFmpeg timed out after </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">timeout</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> seconds\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> process:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._terminate_process(process)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#E1E4E8\"> FFmpegError(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"FFmpeg execution failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _terminate_process</span><span style=\"color:#E1E4E8\">(self, process: asyncio.subprocess.Process):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Gracefully terminate FFmpeg process\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> os.name </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> 'nt'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Send SIGTERM to process group</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                os.killpg(os.getpgid(process.pid), signal.</span><span style=\"color:#79B8FF\">SIGTERM</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                process.terminate()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> asyncio.wait_for(process.wait(), </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> asyncio.TimeoutError:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Force kill if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> os.name </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> 'nt'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    os.killpg(os.getpgid(process.pid), signal.</span><span style=\"color:#79B8FF\">SIGKILL</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    process.kill()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> process.wait()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">ProcessLookupError</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">OSError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span><span style=\"color:#6A737D\">  # Process already terminated</span></span></code></pre></div>\n\n<p><strong>Core Video Transcoder Skeleton</strong></p>\n<p>This skeleton provides the main transcoding interface that builds on the FFmpeg wrapper:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> VideoTranscodingConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for video transcoding operation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    video_codec: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"libx264\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    audio_codec: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"aac\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    crf_value: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 23</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    preset: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"medium\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    profile: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"high\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    level: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"4.0\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_bitrate: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_resolution: Optional[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    segment_duration: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # For HLS/DASH</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    two_pass: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> VideoTranscoder</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main video transcoding orchestrator\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, ffmpeg_wrapper: FFmpegWrapper):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.ffmpeg </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ffmpeg_wrapper</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> transcode_video</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config: VideoTranscodingConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        progress_callback: Optional[Callable[[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">any</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Transcode video file according to configuration.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            input_path: Source video file path</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            output_path: Destination file path</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            config: Transcoding parameters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            progress_callback: Function called with (progress_percentage, stage_name)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary with transcoding results and metadata</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input file exists and is readable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Analyze input video to extract metadata (resolution, duration, codecs)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Build FFmpeg command based on config and input characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute transcoding with progress monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate output file was created successfully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Extract output metadata and return results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _build_transcode_command</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config: VideoTranscodingConfig,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input_metadata: Dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Build FFmpeg command for transcoding operation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This method constructs the complete FFmpeg command line based on:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - Input file characteristics</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - Target configuration parameters  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - Container format requirements</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        - Web optimization flags</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start with basic input/output file parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add video codec and encoding parameters (CRF, preset, profile)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add audio codec and quality settings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add resolution scaling if target_resolution specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add container-specific optimization flags (faststart for MP4)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Add any advanced encoding options (two-pass, keyframe intervals)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use list concatenation to build command parts: cmd += ['-c:v', config.video_codec]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> generate_abr_variants</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_dir: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        variant_configs: List[AdaptiveBitrateConfig],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        progress_callback: Optional[Callable[[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate multiple quality variants for adaptive bitrate streaming.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dictionary mapping variant names to lists of segment files</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create output directory structure for variants</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Process variants in parallel using asyncio.gather()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure keyframe alignment across all variants</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate HLS and/or DASH manifests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate all segments were created successfully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use asyncio.gather(*tasks) to process multiple variants concurrently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> extract_thumbnail</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timestamp: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        resolution: </span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">320</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">180</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Extract thumbnail frame from video at specified timestamp.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            timestamp: Time in seconds to extract frame</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            resolution: Target thumbnail resolution (width, height)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if thumbnail was extracted successfully</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build FFmpeg command for single frame extraction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add timestamp seeking and resolution scaling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute extraction command</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify output image file was created</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use -ss for seeking, -vframes 1 for single frame, -s for resolution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the video transcoding component, verify the following behaviors:</p>\n<ol>\n<li><strong>Basic Transcoding</strong>: Convert a sample MP4 file to different resolutions and formats</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> media_processing.video.test_basic_transcoding</span></span></code></pre></div>\n<p>   Expected: Output files created with correct resolutions and playable in VLC</p>\n<ol start=\"2\">\n<li><p><strong>Progress Monitoring</strong>: Watch progress updates during a long transcoding job<br>Expected: Progress increases from 0% to 100% with reasonable time estimates</p>\n</li>\n<li><p><strong>Error Handling</strong>: Attempt to process a corrupted or missing video file\nExpected: Clear error message with appropriate error classification</p>\n</li>\n<li><p><strong>ABR Generation</strong>: Create HLS segments from a sample video\nExpected: Master playlist and variant playlists with properly segmented content</p>\n</li>\n<li><p><strong>Resource Management</strong>: Monitor memory usage during 4K video processing\nExpected: Memory usage remains stable without continuous growth</p>\n</li>\n</ol>\n<p><strong>Performance Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Method</th>\n<th>Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transcoding hangs indefinitely</td>\n<td>FFmpeg process deadlock</td>\n<td>Check process tree, stderr output</td>\n<td>Add timeout, restart worker</td>\n</tr>\n<tr>\n<td>Memory usage grows continuously</td>\n<td>Frame buffer leaks</td>\n<td>Monitor RSS memory over time</td>\n<td>Implement buffer limits, restart workers</td>\n</tr>\n<tr>\n<td>Poor output quality</td>\n<td>Wrong CRF/bitrate settings</td>\n<td>Compare source vs output bitrates</td>\n<td>Adjust quality parameters</td>\n</tr>\n<tr>\n<td>Slow encoding performance</td>\n<td>Software encoding on server</td>\n<td>Check for hardware acceleration</td>\n<td>Enable NVENC/QSV if available</td>\n</tr>\n<tr>\n<td>Segment alignment issues</td>\n<td>Keyframe misalignment</td>\n<td>Analyze GOP structure in outputs</td>\n<td>Force keyframes at segment boundaries</td>\n</tr>\n</tbody></table>\n<h2 id=\"job-queue-and-scheduling-component\">Job Queue and Scheduling Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (Processing Queue &amp; Progress) - this section covers asynchronous job processing, priority queuing, worker management, and distributed task execution</p>\n</blockquote>\n<h3 id=\"mental-model-restaurant-kitchen\">Mental Model: Restaurant Kitchen</h3>\n<p>Think of the media processing pipeline as a busy restaurant kitchen during dinner rush. When customers place orders (submit processing jobs), those orders don&#39;t go directly to the chefs—they go to an <strong>order management system</strong> that prioritizes them, assigns them to available cooks based on specialization and workload, and tracks progress from preparation through completion.</p>\n<p>The <strong>order tickets</strong> represent processing jobs, each containing specific instructions: &quot;Table 12 wants a medium steak with truffle sauce, allergic to shellfish.&quot; Similarly, a processing job contains the input file path, desired output specifications, priority level, and callback information for notifications.</p>\n<p>The <strong>kitchen manager</strong> acts like our job queue system, deciding which orders get processed first. Rush orders (high priority) jump ahead of regular dinner orders (normal priority), and simple appetizers (image resizing) might be handled by different stations than complex entrees requiring specialized equipment (video transcoding with FFmpeg).</p>\n<p>Each <strong>cooking station</strong> represents a worker process—some specialize in grilling (image processing), others handle sauce preparation (video encoding), and some can do general prep work. The kitchen manager knows each station&#39;s current workload and capabilities, distributing orders accordingly. When a dish is ready, the expediter (webhook notification system) alerts the wait staff and updates the order status.</p>\n<p>Just as restaurants handle kitchen fires, equipment breakdowns, and ingredient shortages without losing orders, our job queue system must gracefully handle worker crashes, resource exhaustion, and processing failures while ensuring no jobs disappear into the void.</p>\n<h3 id=\"queue-operations-and-priority\">Queue Operations and Priority</h3>\n<p>The job queue serves as the central coordination hub for all media processing operations, managing the entire lifecycle from job submission through completion notification. At its core, the queue implements a <strong>priority-based work distribution system</strong> that ensures time-sensitive jobs receive immediate attention while maintaining fair processing of routine work.</p>\n<p>When a client submits a processing job through the <code>submit_job</code> function, the system first validates the request parameters and generates a unique job identifier using <code>generate_job_id()</code>. This identifier includes a timestamp prefix to enable chronological sorting and debugging. The job gets wrapped in a <code>ProcessingJob</code> structure containing all necessary execution metadata: input file path, output specifications, priority level, webhook notification URL, and tracking timestamps.</p>\n<blockquote>\n<p><strong>Decision: Priority Queue with Redis Sorted Sets</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to handle jobs with different urgency levels while maintaining FIFO order within each priority band</li>\n<li><strong>Options Considered</strong>: Simple FIFO queue, weighted round-robin, Redis sorted sets with priority scores</li>\n<li><strong>Decision</strong>: Redis sorted sets with numeric priority values</li>\n<li><strong>Rationale</strong>: Sorted sets provide atomic priority insertion, efficient range queries for worker polling, and built-in deduplication. Priority scores allow fine-grained control over job ordering.</li>\n<li><strong>Consequences</strong>: Enables responsive handling of urgent jobs while preventing starvation of lower-priority work. Requires careful priority score design to avoid edge cases.</li>\n</ul>\n</blockquote>\n<p>The priority system uses enumerated levels that map to numeric scores for queue ordering. The priority-to-score mapping ensures that urgent jobs always process before high-priority jobs, which process before normal and low-priority jobs respectively.</p>\n<table>\n<thead>\n<tr>\n<th>Priority Level</th>\n<th>Numeric Score</th>\n<th>Use Case</th>\n<th>Typical SLA</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>JobPriority.URGENT</code></td>\n<td>20</td>\n<td>Real-time processing requests, critical system operations</td>\n<td>&lt; 30 seconds</td>\n</tr>\n<tr>\n<td><code>JobPriority.HIGH</code></td>\n<td>10</td>\n<td>User-facing operations, thumbnail generation</td>\n<td>&lt; 2 minutes</td>\n</tr>\n<tr>\n<td><code>JobPriority.NORMAL</code></td>\n<td>5</td>\n<td>Standard processing requests, background transcoding</td>\n<td>&lt; 10 minutes</td>\n</tr>\n<tr>\n<td><code>JobPriority.LOW</code></td>\n<td>1</td>\n<td>Batch operations, archive processing, non-critical tasks</td>\n<td>Best effort</td>\n</tr>\n</tbody></table>\n<p>Worker processes continuously poll the queue using blocking pop operations that respect priority ordering. The polling mechanism implements a <strong>hybrid pull model</strong> where workers request jobs based on their current capacity and specialization. This approach prevents overwhelming individual workers while ensuring efficient resource utilization across the worker pool.</p>\n<p>The job distribution algorithm considers several factors beyond simple priority ordering. Workers maintain capability flags indicating which media formats and processing types they support. For example, a worker running on a GPU-enabled machine might advertise video transcoding capabilities, while CPU-only workers focus on image processing tasks. The queue system matches job requirements to worker capabilities during the assignment process.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Job priority affects scheduling order, but it doesn&#39;t preempt running jobs. Once a worker begins processing a job, it continues to completion regardless of higher-priority jobs arriving in the queue. This design prevents resource waste from partially completed work while still providing prioritization benefits for queued jobs.</p>\n</blockquote>\n<p>The queue implements several advanced features to handle real-world operational challenges. <strong>Job deduplication</strong> prevents duplicate processing when clients accidentally submit the same request multiple times. The system generates a content-based hash from the input file checksum and output specifications, rejecting duplicate submissions within a configurable time window.</p>\n<p><strong>Batch job submission</strong> allows clients to submit multiple related jobs atomically, ensuring they&#39;re processed as a cohesive unit. This feature proves particularly valuable for generating multiple video quality variants or processing image galleries where partial completion would be problematic.</p>\n<p>The queue system maintains comprehensive metrics and monitoring data to support operational visibility. Key metrics include queue depth by priority level, average job processing time, worker utilization rates, and failure rates categorized by error type. These metrics feed into alerting systems that notify operators of queue backup, worker failures, or other operational issues requiring attention.</p>\n<h3 id=\"worker-process-management\">Worker Process Management</h3>\n<p>Worker process management represents one of the most complex aspects of the job queue system, requiring careful coordination of resource allocation, process isolation, health monitoring, and failure recovery. Each worker operates as an independent process with clearly defined responsibilities and resource constraints.</p>\n<p>The <strong>worker lifecycle</strong> follows a standardized pattern designed to ensure reliable operation under various failure scenarios. During startup, each worker registers with the job queue system, advertising its capabilities, resource limits, and processing specializations. This registration process includes a health check sequence that verifies the worker can access required dependencies like FFmpeg, storage systems, and temporary workspace directories.</p>\n<table>\n<thead>\n<tr>\n<th>Worker State</th>\n<th>Description</th>\n<th>Actions Permitted</th>\n<th>Monitoring</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>STARTING</code></td>\n<td>Worker initializing, loading configuration</td>\n<td>Registration, dependency checks</td>\n<td>Startup timeout</td>\n</tr>\n<tr>\n<td><code>IDLE</code></td>\n<td>Worker available, polling for jobs</td>\n<td>Accept jobs, health checks</td>\n<td>Heartbeat monitoring</td>\n</tr>\n<tr>\n<td><code>PROCESSING</code></td>\n<td>Worker executing job</td>\n<td>Progress updates, resource monitoring</td>\n<td>Job timeout, memory limits</td>\n</tr>\n<tr>\n<td><code>DRAINING</code></td>\n<td>Worker finishing current job before shutdown</td>\n<td>Complete current job only</td>\n<td>Graceful shutdown timer</td>\n</tr>\n<tr>\n<td><code>FAILED</code></td>\n<td>Worker encountered unrecoverable error</td>\n<td>Cleanup, restart procedures</td>\n<td>Error reporting</td>\n</tr>\n</tbody></table>\n<p>Each worker maintains a <strong>resource budget</strong> that prevents individual jobs from consuming excessive system resources. The budget includes memory limits, processing time constraints, temporary disk space allocation, and network bandwidth quotas. Before accepting a job, workers verify they have sufficient resources to complete the processing without exceeding their allocated limits.</p>\n<p>Memory management requires particular attention given the potentially large size of media files. Workers implement a <strong>staged processing approach</strong> where they estimate memory requirements before loading media files into memory. For video processing, this might involve analyzing file headers to determine resolution and codec information, then calculating expected memory usage based on frame buffer requirements and transcoding parameters.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Memory Estimation Process:\n1. Analyze input file headers to extract resolution and format information\n2. Calculate raw frame buffer size: width × height × channels × bit_depth\n3. Estimate transcoding memory overhead based on codec complexity\n4. Add safety margin for temporary buffers and processing overhead\n5. Compare total estimate to available worker memory budget\n6. Reject job if memory requirements exceed budget, otherwise proceed</code></pre></div>\n\n<p>The worker pool implements <strong>horizontal scaling</strong> through a coordinator process that monitors overall system load and can spawn additional workers when queue depth exceeds configurable thresholds. Conversely, during low-demand periods, workers can be gracefully terminated to reduce resource consumption. This auto-scaling mechanism responds to both queue depth metrics and historical load patterns to anticipate demand fluctuations.</p>\n<p><strong>Process isolation</strong> ensures that failures in one worker don&#39;t affect others or compromise system stability. Each worker runs in a separate process with restricted file system access, limited network permissions, and resource quotas enforced at the operating system level. Workers communicate with the job queue and storage systems through well-defined APIs that validate all inputs and sanitize file paths to prevent directory traversal attacks.</p>\n<p>Health monitoring operates at multiple levels to detect various failure modes. <strong>Heartbeat monitoring</strong> requires workers to send periodic status updates to the queue system, including current job progress, resource utilization, and processing stage information. Workers that fail to send heartbeats within the configured timeout period are marked as failed and their current jobs are rescheduled for processing by healthy workers.</p>\n<blockquote>\n<p><strong>Decision: Process-Based Worker Isolation</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to isolate media processing operations to prevent failures from affecting other jobs</li>\n<li><strong>Options Considered</strong>: Thread-based workers, process-based workers, containerized workers</li>\n<li><strong>Decision</strong>: Separate processes with resource limits</li>\n<li><strong>Rationale</strong>: Processes provide strong isolation boundaries, prevent memory leaks from affecting other jobs, and enable independent restart of failed workers. Operating system process limits provide reliable resource enforcement.</li>\n<li><strong>Consequences</strong>: Higher memory overhead compared to threads, but significantly improved fault tolerance and resource management. Simplified debugging since each worker has independent memory space.</li>\n</ul>\n</blockquote>\n<p>Worker coordination handles several challenging scenarios that arise in distributed processing environments. <strong>Split-brain prevention</strong> ensures that only one worker processes a given job, even during network partitions or queue system failures. This protection uses distributed locking with expiration timeouts to prevent jobs from being permanently orphaned.</p>\n<p>The system implements <strong>graceful degradation</strong> when worker capacity becomes insufficient for demand. Rather than rejecting new jobs outright, the queue system can temporarily increase job timeout limits, reduce quality settings for non-critical processing, or defer low-priority jobs until capacity improves. These degradation strategies allow the system to continue operating during peak load periods without complete service disruption.</p>\n<h3 id=\"job-queue-architecture-decisions\">Job Queue Architecture Decisions</h3>\n<p>The architecture of the job queue system required careful evaluation of multiple design alternatives, each with significant implications for scalability, reliability, and operational complexity. These decisions form the foundation that enables the entire media processing pipeline to operate reliably under production workloads.</p>\n<p><strong>Message Broker Selection</strong> represents the most fundamental architectural choice, as it determines the queue&#39;s performance characteristics, durability guarantees, and operational requirements. The decision between Redis, RabbitMQ, Apache Kafka, and cloud-native solutions like AWS SQS required analysis of both technical capabilities and operational constraints.</p>\n<blockquote>\n<p><strong>Decision: Redis with Persistence for Message Broker</strong></p>\n<ul>\n<li><strong>Context</strong>: Need reliable job queuing with priority support, reasonable throughput, and operational simplicity</li>\n<li><strong>Options Considered</strong>: Redis (in-memory), RabbitMQ (AMQP), Apache Kafka (distributed log), AWS SQS (managed service)</li>\n<li><strong>Decision</strong>: Redis with AOF persistence enabled</li>\n<li><strong>Rationale</strong>: Redis sorted sets provide native priority queue functionality with atomic operations. AOF persistence ensures job durability across restarts. Simpler operational model than RabbitMQ or Kafka for media processing workloads. Single-node deployment sufficient for moderate scale.</li>\n<li><strong>Consequences</strong>: Excellent performance for job queuing patterns, but requires careful memory management for large job volumes. Limited horizontal scaling compared to Kafka. Simplified deployment and monitoring.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Verdict</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Redis</strong></td>\n<td>Native priority queues, atomic operations, simple deployment</td>\n<td>Memory-limited storage, single-node bottleneck</td>\n<td>✅ <strong>Chosen</strong></td>\n</tr>\n<tr>\n<td><strong>RabbitMQ</strong></td>\n<td>Mature AMQP implementation, flexible routing, persistent queues</td>\n<td>Complex configuration, requires queue management</td>\n<td>Considered</td>\n</tr>\n<tr>\n<td><strong>Apache Kafka</strong></td>\n<td>High throughput, horizontal scaling, replay capability</td>\n<td>Over-engineered for job queuing, complex operations</td>\n<td>Rejected</td>\n</tr>\n<tr>\n<td><strong>AWS SQS</strong></td>\n<td>Fully managed, automatic scaling, pay-per-use</td>\n<td>Vendor lock-in, limited priority support, higher latency</td>\n<td>Rejected</td>\n</tr>\n</tbody></table>\n<p><strong>Serialization Format</strong> affects both performance and system evolution capability. The choice between JSON, MessagePack, Protocol Buffers, and other serialization approaches required balancing human readability, parsing performance, schema evolution support, and cross-language compatibility.</p>\n<blockquote>\n<p><strong>Decision: JSON with Schema Validation</strong></p>\n<ul>\n<li><strong>Context</strong>: Need human-readable job data for debugging with reasonable parsing performance</li>\n<li><strong>Options Considered</strong>: JSON (human-readable), MessagePack (compact binary), Protocol Buffers (schema evolution)</li>\n<li><strong>Decision</strong>: JSON with JSON Schema validation for job structures</li>\n<li><strong>Rationale</strong>: Human readability crucial for debugging media processing issues. JSON parsing performance adequate for job queue throughput. Schema validation prevents malformed job data. Wide language support for future extensions.</li>\n<li><strong>Consequences</strong>: Larger message sizes than binary formats, but acceptable for job queue usage patterns. Easier debugging and system monitoring. Simplified client integration.</li>\n</ul>\n</blockquote>\n<p>The <strong>concurrency model</strong> determines how the system handles multiple simultaneous jobs and coordinates access to shared resources. The design choice between actor-based concurrency, shared-memory threading, and process-based isolation has far-reaching implications for system reliability and debugging complexity.</p>\n<p>Process-based concurrency was selected over threading or actor models primarily due to the resource-intensive nature of media processing operations. FFmpeg and image processing libraries can consume significant memory and CPU resources, making process isolation essential for preventing resource leaks from affecting other jobs. Additionally, many media processing libraries have complex memory management patterns that are difficult to coordinate safely in multi-threaded environments.</p>\n<p><strong>Job Persistence Strategy</strong> required balancing durability guarantees with system performance. The choice between storing complete job data in the message queue versus using lightweight job references with external storage affects both system complexity and failure recovery capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Persistence Approach</th>\n<th>Description</th>\n<th>Durability</th>\n<th>Performance</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>In-Queue Job Data</strong></td>\n<td>Complete job specifications stored in Redis</td>\n<td>High</td>\n<td>Medium</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Reference + Database</strong></td>\n<td>Job IDs in queue, full data in PostgreSQL</td>\n<td>Very High</td>\n<td>Lower</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Hybrid Approach</strong></td>\n<td>Essential data in queue, full specs in storage</td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n</tbody></table>\n<p>The system implements a <strong>hybrid persistence approach</strong> where the job queue contains essential execution metadata (job ID, priority, worker assignment) while complete job specifications and processing history are stored in external persistent storage. This design provides the performance benefits of in-memory job queuing while ensuring that job data survives system restarts and can be analyzed for historical reporting.</p>\n<p><strong>Worker Discovery and Load Balancing</strong> mechanisms determine how jobs are distributed across available workers and how the system adapts to changing worker capacity. The choice between push-based job assignment and pull-based worker polling affects system responsiveness and load distribution fairness.</p>\n<p>The implemented pull-based model allows workers to request jobs based on their current capacity and capabilities, preventing overloading of slower workers while ensuring that fast workers remain fully utilized. Workers include capability advertisements in their job requests, allowing the queue system to match job requirements with worker specializations automatically.</p>\n<p><strong>Error Handling and Recovery Architecture</strong> establishes how the system responds to various failure scenarios, from temporary network issues to permanent worker failures. The decision between immediate retry, exponential backoff, dead letter queues, and manual intervention affects both system reliability and operational burden.</p>\n<p>The multi-tiered error handling approach categorizes failures based on their likelihood of recovery and implements appropriate response strategies for each category. Transient failures (network timeouts, temporary resource exhaustion) trigger automatic retry with exponential backoff. Persistent failures (corrupted input files, unsupported formats) are moved to dead letter queues for manual investigation. System failures (worker crashes, storage unavailability) trigger immediate job rescheduling to healthy workers.</p>\n<h3 id=\"common-queue-implementation-pitfalls\">Common Queue Implementation Pitfalls</h3>\n<p>Implementing a robust job queue system involves navigating numerous subtle pitfalls that can compromise reliability, performance, or data consistency. Understanding these common mistakes helps avoid production incidents and ensures the system operates correctly under all conditions.</p>\n<p>⚠️ <strong>Pitfall: Job Deduplication Race Conditions</strong></p>\n<p>Many implementations fail to properly handle the race condition where multiple workers might pick up duplicate jobs before the deduplication check completes. This occurs when clients submit the same job multiple times in rapid succession, and the queue system hasn&#39;t completed processing the first submission before subsequent duplicates arrive.</p>\n<p>The incorrect approach uses a simple &quot;check then insert&quot; pattern that&#39;s vulnerable to race conditions:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>1. Check if job with same hash exists in queue\n2. If not found, insert new job\n3. Return job ID</code></pre></div>\n\n<p>Between steps 1 and 2, another thread might insert the same job, resulting in duplicate processing. The correct implementation uses atomic operations provided by Redis to ensure deduplication works even under high concurrency. The system generates a content-based hash from the input file and output specifications, then uses Redis&#39;s SET with NX (not exists) flag to atomically insert the job only if no duplicate exists.</p>\n<p>⚠️ <strong>Pitfall: Improper Dead Letter Queue Management</strong></p>\n<p>A common mistake is implementing dead letter queues without proper categorization of failure types, leading to either excessive manual intervention or silent data loss. Not all job failures should go to the dead letter queue—some represent permanent errors that should never be retried, while others might be temporary issues that resolve automatically.</p>\n<p>The system should categorize failures into distinct types with appropriate handling strategies:</p>\n<ul>\n<li><strong>Transient failures</strong> (network timeouts, temporary resource exhaustion): Retry with exponential backoff up to maximum retry limit</li>\n<li><strong>Permanent failures</strong> (corrupted input files, unsupported formats): Move to dead letter queue with detailed error information for manual review</li>\n<li><strong>System failures</strong> (worker crashes, storage unavailability): Reschedule on different worker without counting against retry limit</li>\n</ul>\n<p>Additionally, dead letter queues require active monitoring and periodic cleanup to prevent indefinite accumulation of failed jobs. The system should implement automated alerts when dead letter queue depth exceeds thresholds and provide tools for bulk job analysis and cleanup.</p>\n<p>⚠️ <strong>Pitfall: Resource Cleanup on Worker Failure</strong></p>\n<p>Worker processes that crash or are forcibly terminated often leave behind temporary files, partially processed media, and locked resources. Without proper cleanup mechanisms, these artifacts accumulate over time and can exhaust disk space or cause resource conflicts for subsequent jobs.</p>\n<p>The solution requires implementing cleanup procedures at multiple levels:</p>\n<ol>\n<li><strong>Graceful shutdown handlers</strong> that cleanup temporary files when workers receive termination signals</li>\n<li><strong>Periodic cleanup processes</strong> that identify and remove orphaned temporary files based on age and worker heartbeat status</li>\n<li><strong>Job reschedule logic</strong> that ensures crashed jobs are properly reset and rescheduled rather than left in a permanent &quot;processing&quot; state</li>\n<li><strong>Resource lock recovery</strong> that releases file locks and other resources held by failed workers</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Progress Estimation Accuracy</strong></p>\n<p>Many implementations provide progress estimates that are wildly inaccurate, either showing 90% completion for hours or jumping from 10% to 100% instantly. This occurs because developers base progress estimates on simple metrics like file size processed rather than actual processing complexity.</p>\n<p>Media processing operations have highly variable computational requirements depending on content characteristics. A one-hour video with static scenes processes much faster than a one-hour video with rapid motion and complex visual effects, even though both files might be similar in size. The system should implement <strong>stage-based progress reporting</strong> rather than attempting to provide precise percentage estimates.</p>\n<p>The correct approach divides processing into discrete stages (validation, preprocessing, encoding, postprocessing) and reports progress as stage completion rather than attempting granular percentage tracking. This provides users with meaningful progress information while avoiding the false precision of inaccurate time estimates.</p>\n<p>⚠️ <strong>Pitfall: Worker Capacity Estimation</strong></p>\n<p>Incorrectly estimating worker processing capacity leads to either resource over-allocation (causing memory exhaustion and crashes) or under-utilization (leaving workers idle while jobs queue up). The challenge lies in predicting resource requirements for media processing jobs before loading the actual media files.</p>\n<p>The system should implement <strong>conservative capacity estimation</strong> that analyzes input file metadata to predict memory and CPU requirements. For images, this involves calculating raw pixel buffer size based on resolution and bit depth. For videos, estimation includes frame buffer requirements for the target resolution plus transcoding overhead for the selected codec.</p>\n<p>Workers should maintain resource budgets that account for peak memory usage during processing, not just average usage. A safety margin prevents edge cases from causing worker crashes, and jobs that exceed estimated resource requirements should be gracefully rejected rather than causing worker failures.</p>\n<p>⚠️ <strong>Pitfall: Queue Monitoring Blind Spots</strong></p>\n<p>Production issues often arise from monitoring gaps that miss critical failure modes. Common blind spots include gradual memory leaks in workers, increasing job processing times due to resource contention, and cascade failures where problems in one component affect others.</p>\n<p>Comprehensive monitoring should track both technical metrics (queue depth, processing times, error rates) and business metrics (job completion rates, customer-facing latency, output quality measures). Alert thresholds should account for normal operational variance while detecting genuine issues early enough for proactive intervention.</p>\n<p>The monitoring system should implement <strong>trend analysis</strong> that detects gradual degradation in addition to sudden failures. For example, slowly increasing job processing times might indicate resource exhaustion or performance regression that requires attention before it becomes a customer-impacting outage.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The job queue and scheduling component serves as the coordination backbone for the entire media processing system, managing job distribution, worker coordination, and progress tracking across the processing pipeline. This implementation guidance provides the infrastructure foundation and core logic structure needed to build a production-ready job queue system.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Message Broker</strong></td>\n<td>Redis with <code>redis-py</code> (in-memory sorted sets)</td>\n<td>Redis Cluster with <code>redis-py-cluster</code> (distributed)</td>\n</tr>\n<tr>\n<td><strong>Worker Framework</strong></td>\n<td>Custom worker processes with multiprocessing</td>\n<td>Celery with Redis broker (full-featured task queue)</td>\n</tr>\n<tr>\n<td><strong>Job Persistence</strong></td>\n<td>Redis with AOF persistence</td>\n<td>PostgreSQL with Redis cache layer</td>\n</tr>\n<tr>\n<td><strong>Process Monitoring</strong></td>\n<td>Python multiprocessing with custom health checks</td>\n<td>Supervisor process management with monitoring</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>YAML files with <code>pyyaml</code></td>\n<td>Consul/etcd distributed configuration</td>\n</tr>\n<tr>\n<td><strong>Logging</strong></td>\n<td>Python <code>logging</code> with structured JSON output</td>\n<td>ELK stack (Elasticsearch, Logstash, Kibana)</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media_pipeline/\n  queue/\n    __init__.py\n    models.py                 ← Job, Priority, Status data structures\n    queue_manager.py          ← Job submission, priority handling\n    worker_pool.py           ← Worker process management and coordination\n    progress_tracker.py      ← Progress updates and webhook notifications\n    redis_backend.py         ← Redis operations and connection management\n    worker_process.py        ← Individual worker implementation\n  config/\n    queue_config.py          ← Queue configuration and validation\n    worker_config.py         ← Worker resource limits and capabilities\n  tests/\n    test_queue_operations.py ← Queue submission and priority tests\n    test_worker_management.py ← Worker lifecycle and coordination tests\n    integration/\n      test_job_processing.py ← End-to-end job processing tests\n  scripts/\n    start_workers.py         ← Worker startup script\n    queue_monitor.py         ← Queue monitoring and alerting\n    cleanup_jobs.py          ← Maintenance and cleanup utilities</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Redis Backend Connection Management</strong> (<code>redis_backend.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisBackend</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Redis backend for job queue with connection pooling and error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: RedisConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.ConnectionPool(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            host</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.host,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            port</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.port,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            db</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.db,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            password</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">config.password,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            max_connections</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">20</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis(</span><span style=\"color:#FFAB70\">connection_pool</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pool)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Queue and status key patterns</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.job_queue_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"media:jobs:queue\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.job_data_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"media:jobs:data:</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.worker_status_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"media:workers:status\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.progress_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"media:jobs:progress:</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> health_check</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify Redis connection and basic operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.ping()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis health check failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> submit_job_atomic</span><span style=\"color:#E1E4E8\">(self, job: ProcessingJob) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Atomically submit job with deduplication check.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Use job content hash for deduplication</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            dedup_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"media:jobs:dedup:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job.content_hash()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.job_data_key.format(job.job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Watch deduplication key for atomic operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.watch(dedup_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> pipe.get(dedup_key):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Duplicate job detected for hash </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job.content_hash()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Atomic job submission</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.multi()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.setex(dedup_key, timedelta(</span><span style=\"color:#FFAB70\">hours</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">), job.job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.hset(job_data_key, </span><span style=\"color:#FFAB70\">mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job.to_dict())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.zadd(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.job_queue_key, {job.job_id: job.priority.value})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.execute()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job.job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> submitted successfully\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.WatchError:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.warning(</span><span style=\"color:#9ECBFF\">\"Job submission failed due to concurrent modification\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis error during job submission: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.reset()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pop_highest_priority_job</span><span style=\"color:#E1E4E8\">(self, worker_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> Optional[ProcessingJob]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Blocking pop of highest priority job with worker assignment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Use blocking pop on sorted set (highest score first)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.bzpopmax(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.job_queue_key, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">timeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> result:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            _, job_id, priority </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.job_data_key.format(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Atomically assign worker and update status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pipe.hgetall(job_data_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.hset(job_data_key, </span><span style=\"color:#9ECBFF\">'assigned_worker'</span><span style=\"color:#E1E4E8\">, worker_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.hset(job_data_key, </span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">, JobStatus.</span><span style=\"color:#79B8FF\">PROCESSING</span><span style=\"color:#E1E4E8\">.value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.hset(job_data_key, </span><span style=\"color:#9ECBFF\">'started_at'</span><span style=\"color:#E1E4E8\">, datetime.utcnow().isoformat())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.execute()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> job_data:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ProcessingJob.from_dict(job_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> assigned to worker </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> job</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job data not found for job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Redis error during job pop: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_job_progress</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, progress_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update job progress and timestamp.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            progress_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'job_id'</span><span style=\"color:#E1E4E8\">: job_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'progress_percentage'</span><span style=\"color:#E1E4E8\">: progress_percentage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'stage'</span><span style=\"color:#E1E4E8\">: stage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'updated_at'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'details'</span><span style=\"color:#E1E4E8\">: json.dumps(details </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            progress_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.progress_key.format(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.job_data_key.format(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.hset(progress_key, </span><span style=\"color:#FFAB70\">mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">progress_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.hset(job_data_key, </span><span style=\"color:#9ECBFF\">'progress_percentage'</span><span style=\"color:#E1E4E8\">, progress_percentage)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.expire(progress_key, timedelta(</span><span style=\"color:#FFAB70\">days</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">))  </span><span style=\"color:#6A737D\"># Auto-expire progress data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.execute()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to update progress for job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_completed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_files: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark job as completed and store output file paths.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            completion_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'status'</span><span style=\"color:#E1E4E8\">: JobStatus.</span><span style=\"color:#79B8FF\">COMPLETED</span><span style=\"color:#E1E4E8\">.value,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'completed_at'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'output_files'</span><span style=\"color:#E1E4E8\">: json.dumps(output_files),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'progress_percentage'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.job_data_key.format(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.redis.hset(job_data_key, </span><span style=\"color:#FFAB70\">mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">completion_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> marked as completed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to mark job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> as completed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_failed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, retry_eligible: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark job as failed and handle retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.job_data_key.format(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.hgetall(job_data_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> current_data:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job data not found for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            retry_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(current_data.get(</span><span style=\"color:#9ECBFF\">'retry_count'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            max_retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">  # Should come from config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            failure_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'status'</span><span style=\"color:#E1E4E8\">: JobStatus.</span><span style=\"color:#79B8FF\">FAILED</span><span style=\"color:#E1E4E8\">.value,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'error_message'</span><span style=\"color:#E1E4E8\">: error_message,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'failed_at'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'retry_count'</span><span style=\"color:#E1E4E8\">: retry_count </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.hset(job_data_key, </span><span style=\"color:#FFAB70\">mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">failure_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Retry logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> retry_eligible </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> retry_count </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> max_retries:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Calculate exponential backoff delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                delay_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> **</span><span style=\"color:#E1E4E8\"> retry_count))  </span><span style=\"color:#6A737D\"># Cap at 5 minutes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Reschedule job with delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                retry_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.utcnow() </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> timedelta(</span><span style=\"color:#FFAB70\">seconds</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">delay_seconds)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                pipe.zadd(</span><span style=\"color:#9ECBFF\">\"media:jobs:retry\"</span><span style=\"color:#E1E4E8\">, {job_id: retry_time.timestamp()})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> scheduled for retry in </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">delay_seconds</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> seconds\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Move to dead letter queue for manual review</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                pipe.lpush(</span><span style=\"color:#9ECBFF\">\"media:jobs:dead_letter\"</span><span style=\"color:#E1E4E8\">, job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> moved to dead letter queue after </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">retry_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> retries\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.execute()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.RedisError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to mark job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> as failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong>Worker Process Infrastructure</strong> (<code>worker_process.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> signal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> multiprocessing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WorkerProcess</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual worker process with resource management and health monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, worker_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, config: ProcessingConfig, redis_backend: RedisBackend):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.worker_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> worker_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_backend </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_backend</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_job: Optional[ProcessingJob] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.utcnow()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_shutting_down </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.resource_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ResourceMonitor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Set up signal handlers for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        signal.signal(signal.</span><span style=\"color:#79B8FF\">SIGTERM</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._handle_shutdown_signal)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        signal.signal(signal.</span><span style=\"color:#79B8FF\">SIGINT</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._handle_shutdown_signal)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main worker loop with job processing and health monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Worker </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> starting up\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._register_worker()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.is_shutting_down:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Send heartbeat with current status and resource usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if worker should shutdown (maintenance mode, resource limits)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Poll for available job with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If job received, validate worker can process it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Execute job processing with progress callbacks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle job completion or failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Cleanup temporary resources after job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Brief sleep to prevent busy polling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Worker </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> encountered fatal error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._emergency_cleanup()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._unregister_worker()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Worker </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> shutdown complete\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _register_worker</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register worker with queue system and advertise capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create worker status record with capabilities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Register in Redis worker registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Send initial heartbeat</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Include worker_id, start_time, capabilities, resource_limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _send_heartbeat</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Send worker status update to queue system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            status_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'worker_id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.worker_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'status'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'processing'</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_job </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> 'idle'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'current_job'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.current_job.job_id </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_job </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'heartbeat_time'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'cpu_usage'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.resource_monitor.get_cpu_usage(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'memory_usage'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.resource_monitor.get_memory_usage(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'uptime_seconds'</span><span style=\"color:#E1E4E8\">: (datetime.utcnow() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_time).total_seconds()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Store heartbeat with expiration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"media:workers:heartbeat:</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.redis_backend.redis.setex(key, timedelta(</span><span style=\"color:#FFAB70\">minutes</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">), status_data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to send heartbeat: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_shutdown_signal</span><span style=\"color:#E1E4E8\">(self, signum, frame):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle graceful shutdown signal.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Worker </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> received shutdown signal </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">signum</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.is_shutting_down </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.current_job:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Completing current job </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.current_job.job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> before shutdown\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Allow current job to complete but don't accept new jobs</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ResourceMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Monitor worker resource usage and enforce limits.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.process </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.Process()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peak_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.utcnow()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_cpu_usage</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current CPU usage percentage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.process.cpu_percent()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_memory_usage</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get detailed memory usage information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.process.memory_info()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_percent </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.process.memory_percent()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Track peak memory usage</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.peak_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peak_memory, memory_info.rss)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'rss_mb'</span><span style=\"color:#E1E4E8\">: memory_info.rss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'vms_mb'</span><span style=\"color:#E1E4E8\">: memory_info.vms </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'percent'</span><span style=\"color:#E1E4E8\">: memory_percent,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'peak_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.peak_memory </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_resource_limits</span><span style=\"color:#E1E4E8\">(self, limits: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if current resource usage exceeds configured limits.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compare current memory usage to limits.max_memory_mb</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compare current CPU usage to limits.max_cpu_percent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check disk usage in temporary directories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return dict with limit_exceeded: bool for each resource</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This helps prevent workers from overwhelming the system</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Job Queue Manager</strong> (<code>queue_manager.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages job submission, priority queuing, and worker coordination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_backend: RedisBackend, config: AppConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_backend </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_backend</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.job_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JobStatistics()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> submit_job</span><span style=\"color:#E1E4E8\">(self, input_file: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_specs: List[OutputSpecification], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   priority: JobPriority, webhook_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ProcessingJob:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Submit new processing job to queue with priority handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate input_file exists and is readable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate output_specs have valid formats and parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate unique job_id using generate_job_id()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create ProcessingJob instance with all parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Submit job atomically using redis_backend.submit_job_atomic()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If submission successful, send webhook notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Update job statistics and metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return ProcessingJob instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use try/except to handle validation and submission errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_queue_statistics</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current queue depth and processing statistics by priority.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Query Redis for total jobs in queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Count jobs by priority level using ZRANGEBYSCORE</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Get count of active workers from heartbeat keys</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate average job processing time from recent completions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Get failed job count from dead letter queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return comprehensive statistics dict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use Redis pipeline for efficient multi-query operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_expired_jobs</span><span style=\"color:#E1E4E8\">(self, max_age_hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove old job data and cleanup temporary resources.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Find jobs older than max_age_hours in COMPLETED/FAILED status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Archive job data to long-term storage if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Remove job data keys from Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Cleanup temporary files associated with old jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update cleanup metrics and log results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use SCAN to iterate through job keys efficiently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WorkerCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates worker processes and manages worker pool scaling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_backend: RedisBackend, config: ProcessingConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_backend </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_backend</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.worker_processes: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, multiprocessing.Process] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.target_worker_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.max_workers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_worker_pool</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start configured number of worker processes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate optimal worker count based on CPU cores and memory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create worker processes using multiprocessing.Process</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Start each worker process and track PIDs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Register signal handlers for pool shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Start monitoring thread for worker health checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Store worker PIDs for graceful shutdown management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scale_worker_pool</span><span style=\"color:#E1E4E8\">(self, target_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Dynamically scale worker pool up or down.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compare target_count to current active workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If scaling up, start additional worker processes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If scaling down, signal workers to shutdown gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Wait for workers to complete current jobs before termination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update worker pool tracking and metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Always allow current jobs to complete before shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> monitor_worker_health</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Monitor worker heartbeats and restart failed workers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check heartbeat timestamps for all registered workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify workers with stale heartbeats (likely crashed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Restart failed workers and reassign their jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Send alerts for worker failures and restarts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update worker health metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use heartbeat timeout from config to detect failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Checkpoint 1: Basic Queue Operations</strong>\nAfter implementing the Redis backend and basic job submission:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_queue_operations.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p>Expected behavior:</p>\n<ul>\n<li>Jobs submitted with different priorities are retrieved in correct order</li>\n<li>Duplicate job submission is properly rejected with deduplication</li>\n<li>Job status transitions work correctly (PENDING → PROCESSING → COMPLETED)</li>\n<li>Redis connection handling works with proper error recovery</li>\n</ul>\n<p>Test manually:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> queue.queue_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> queue.models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ProcessingJob, JobPriority</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Submit test job</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QueueManager(redis_backend, config)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> manager.submit_job(</span><span style=\"color:#9ECBFF\">\"test.jpg\"</span><span style=\"color:#E1E4E8\">, [output_spec], JobPriority.</span><span style=\"color:#79B8FF\">HIGH</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Job submitted: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job.job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check queue statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> manager.get_queue_statistics()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Queue depth: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">stats[</span><span style=\"color:#9ECBFF\">'total_jobs'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Checkpoint 2: Worker Process Management</strong>\nAfter implementing worker processes and coordination:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/start_workers.py</span><span style=\"color:#79B8FF\"> --worker-count</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_worker_management.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p>Expected behavior:</p>\n<ul>\n<li>Worker processes start and register with queue system</li>\n<li>Heartbeat monitoring detects healthy and failed workers</li>\n<li>Jobs are distributed to available workers based on capabilities</li>\n<li>Graceful shutdown completes current jobs before terminating</li>\n</ul>\n<p><strong>Checkpoint 3: End-to-End Job Processing</strong>\nAfter implementing complete job processing pipeline:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_job_processing.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p>Expected behavior:</p>\n<ul>\n<li>Jobs flow from submission through processing to completion</li>\n<li>Progress updates are tracked and reported correctly</li>\n<li>Failed jobs are retried with exponential backoff</li>\n<li>Webhook notifications are sent at appropriate stages</li>\n<li>Resource cleanup occurs after job completion</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Jobs stuck in PENDING</strong></td>\n<td>No available workers or worker capability mismatch</td>\n<td>Check worker heartbeats in Redis, verify worker capabilities</td>\n<td>Start workers or check worker capability registration</td>\n</tr>\n<tr>\n<td><strong>Workers not picking up jobs</strong></td>\n<td>Polling timeout too short or Redis connection issues</td>\n<td>Check worker logs for Redis errors, verify queue key names</td>\n<td>Increase polling timeout, verify Redis connectivity</td>\n</tr>\n<tr>\n<td><strong>Memory exhaustion crashes</strong></td>\n<td>Worker memory limits not enforced or estimation incorrect</td>\n<td>Monitor worker memory usage, check job size estimation</td>\n<td>Implement proper memory limits, improve size estimation</td>\n</tr>\n<tr>\n<td><strong>Jobs processed multiple times</strong></td>\n<td>Duplicate job submission or improper deduplication</td>\n<td>Check job deduplication keys in Redis, verify atomic submission</td>\n<td>Fix deduplication hash calculation, ensure atomic operations</td>\n</tr>\n<tr>\n<td><strong>Progress updates missing</strong></td>\n<td>Worker not sending updates or Redis key expiration</td>\n<td>Check progress update calls in worker, verify Redis keys</td>\n<td>Add progress update calls, adjust Redis key expiration</td>\n</tr>\n<tr>\n<td><strong>Dead letter queue growing</strong></td>\n<td>Permanent failures not categorized properly</td>\n<td>Analyze dead letter queue jobs, check error categorization</td>\n<td>Improve error classification, add manual job review process</td>\n</tr>\n</tbody></table>\n<h2 id=\"progress-tracking-and-notification-component\">Progress Tracking and Notification Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 (Processing Queue &amp; Progress) - this section covers real-time progress updates, webhook notifications, and job status management across the processing pipeline</p>\n</blockquote>\n<h3 id=\"mental-model-package-delivery-tracking\">Mental Model: Package Delivery Tracking</h3>\n<p>Think of progress tracking like a modern package delivery service such as FedEx or UPS. When you ship a package, you receive a tracking number that lets you monitor its journey from pickup to delivery. The package moves through distinct stages: &quot;Package picked up&quot;, &quot;Arrived at sorting facility&quot;, &quot;Out for delivery&quot;, and &quot;Delivered&quot;. Each stage transition triggers an update to the tracking system, and you can receive notifications via SMS, email, or app push notifications when important milestones occur.</p>\n<p>Similarly, our media processing pipeline treats each processing job like a package moving through a fulfillment system. The job progresses through well-defined stages: &quot;Job queued&quot;, &quot;Processing started&quot;, &quot;Image resizing complete&quot;, &quot;Format conversion in progress&quot;, &quot;Thumbnail generation&quot;, and &quot;Processing complete&quot;. Just as package tracking provides estimated delivery times based on historical data and current conditions, our system estimates completion times based on file size, processing complexity, and current worker load.</p>\n<p>The notification system works like delivery alerts - when significant events occur (job completion, failure, or major progress milestones), the system sends webhook notifications to registered callback URLs, similar to how delivery services send status updates to your phone. If a notification fails to deliver (like when your phone is offline), the system retries with exponential backoff, ensuring important updates eventually reach their destination.</p>\n<p>This mental model helps us understand three key principles: <strong>stage-based progress</strong> (discrete milestones rather than continuous percentages), <strong>event-driven notifications</strong> (updates triggered by state changes), and <strong>reliable delivery</strong> (guaranteed notification delivery with retry mechanisms).</p>\n<h3 id=\"progress-calculation-strategies\">Progress Calculation Strategies</h3>\n<p>Progress tracking in media processing presents unique challenges compared to simple file operations. Unlike copying files where progress correlates linearly with bytes transferred, media processing involves multiple distinct phases with varying computational complexity and duration. Our system employs <strong>stage-based progress</strong> rather than time-based estimates, providing more accurate and meaningful progress updates to users.</p>\n<h4 id=\"stage-based-progress-architecture\">Stage-Based Progress Architecture</h4>\n<p>The foundation of our progress system divides each processing job into discrete, measurable stages. Each stage represents a significant computational milestone with clearly defined inputs, outputs, and completion criteria. This approach provides several advantages over percentage-based progress: stages are deterministic (a job always progresses through the same sequence), measurable (each stage has concrete completion criteria), and meaningful to users (stage names communicate actual processing activities).</p>\n<table>\n<thead>\n<tr>\n<th>Stage Name</th>\n<th>Description</th>\n<th>Percentage Weight</th>\n<th>Duration Factors</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>VALIDATION</code></td>\n<td>File format detection and metadata extraction</td>\n<td>5%</td>\n<td>File size, format complexity</td>\n</tr>\n<tr>\n<td><code>PREPROCESSING</code></td>\n<td>EXIF handling, orientation correction</td>\n<td>10%</td>\n<td>Metadata density, rotation needed</td>\n</tr>\n<tr>\n<td><code>RESIZE_OPERATIONS</code></td>\n<td>Primary resizing and cropping operations</td>\n<td>40%</td>\n<td>Output count, resolution changes</td>\n</tr>\n<tr>\n<td><code>FORMAT_CONVERSION</code></td>\n<td>Transcoding between image/video formats</td>\n<td>35%</td>\n<td>Codec complexity, quality settings</td>\n</tr>\n<tr>\n<td><code>OPTIMIZATION</code></td>\n<td>Compression and web optimization</td>\n<td>8%</td>\n<td>Quality targets, advanced features</td>\n</tr>\n<tr>\n<td><code>FINALIZATION</code></td>\n<td>File writing and cleanup operations</td>\n<td>2%</td>\n<td>Storage latency, temp file cleanup</td>\n</tr>\n</tbody></table>\n<p>Each stage maintains its own progress tracking mechanism. For example, during <code>RESIZE_OPERATIONS</code>, progress increments as each output specification completes processing. If a job requires generating five different image sizes, progress within this stage advances by 20% for each completed resize operation. This granular tracking provides meaningful feedback even for complex jobs with dozens of output variants.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Stage-based progress solves the estimation problem that plagues traditional progress bars. Instead of guessing &quot;45% complete in 3.7 minutes&quot;, we can accurately report &quot;Format conversion in progress - 3 of 5 outputs complete&quot;, which provides actionable information to users.</p>\n</blockquote>\n<h4 id=\"time-estimation-algorithms\">Time Estimation Algorithms</h4>\n<p>While stage-based progress forms our primary tracking mechanism, users still desire time estimates for planning purposes. Our system combines historical performance data with real-time job characteristics to generate dynamic estimates that improve in accuracy as processing progresses.</p>\n<p>The estimation algorithm operates in three phases: <strong>initial estimate</strong> (based on file size and historical averages), <strong>refinement phase</strong> (adjusts estimates as early stages complete), and <strong>final phase</strong> (provides high-accuracy estimates for remaining work). Initial estimates use a regression model trained on historical job completion times, factoring in input file size, output specification count, and current worker load.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Initial Estimate Calculation:\n1. Extract job characteristics: file_size, output_count, format_complexity_score\n2. Query historical database for similar jobs (±20% file size, same format family)\n3. Calculate baseline duration using weighted average of historical completion times\n4. Apply load factor adjustment based on current queue depth and worker availability\n5. Add safety margin (typically 15-25%) to account for estimation uncertainty</code></pre></div>\n\n<p>As processing progresses, the system refines estimates using actual stage completion times. If the <code>VALIDATION</code> stage completes faster than expected, the algorithm proportionally adjusts estimates for remaining stages. Conversely, if early stages take longer than predicted, the system increases estimates for subsequent work to maintain accuracy.</p>\n<blockquote>\n<p><strong>Decision: Dynamic Estimation Refinement</strong></p>\n<ul>\n<li><strong>Context</strong>: Static time estimates become inaccurate as processing reveals actual complexity</li>\n<li><strong>Options Considered</strong>: Fixed estimates, linear interpolation, machine learning models</li>\n<li><strong>Decision</strong>: Weighted refinement algorithm that adjusts estimates based on completed stage performance</li>\n<li><strong>Rationale</strong>: Balances simplicity with accuracy, provides meaningful updates without complex ML infrastructure</li>\n<li><strong>Consequences</strong>: Estimates improve in accuracy over time but may initially fluctuate as refinements apply</li>\n</ul>\n</blockquote>\n<h4 id=\"progress-update-mechanisms\">Progress Update Mechanisms</h4>\n<p>Progress updates must balance accuracy with performance overhead. Our system implements a <strong>hierarchical update strategy</strong> that provides frequent updates for user-facing operations while minimizing database write load and notification spam. The strategy employs different update frequencies based on the operation type and user visibility requirements.</p>\n<p>For long-running operations like video transcoding, the system reports progress updates at regular time intervals (every 30 seconds) rather than after every processed frame. This prevents overwhelming the notification system while maintaining responsive user feedback. Short-duration operations like image resizing trigger updates only at stage boundaries to minimize overhead while still providing meaningful progress visibility.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Update Frequency</th>\n<th>Trigger Events</th>\n<th>Notification Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image Processing</td>\n<td>Stage boundaries</td>\n<td>Stage complete, error encountered</td>\n<td>25% progress increments</td>\n</tr>\n<tr>\n<td>Video Transcoding</td>\n<td>30-second intervals</td>\n<td>Time elapsed, segment complete</td>\n<td>20% progress increments</td>\n</tr>\n<tr>\n<td>Batch Operations</td>\n<td>Per-item completion</td>\n<td>Individual job complete</td>\n<td>Per-job completion</td>\n</tr>\n<tr>\n<td>Format Conversion</td>\n<td>Quality milestone</td>\n<td>Encoding pass complete</td>\n<td>Major milestone only</td>\n</tr>\n</tbody></table>\n<p>The progress update pipeline employs <strong>atomic state transitions</strong> to prevent race conditions between multiple worker processes updating the same job. Each progress update includes a sequence number and timestamp to ensure updates apply in correct order and detect potential concurrency issues.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Atomic Progress Update Procedure:\n1. Acquire advisory lock on job record using job_id as lock key\n2. Read current job state and validate update is newer than stored progress\n3. Calculate new progress percentage based on stage completion and weights\n4. Update job record with new progress, stage, timestamp, and sequence number\n5. Determine if notification threshold reached since last webhook delivery\n6. Release advisory lock and trigger async notification if threshold met\n7. Log progress update with correlation_id for debugging and audit trails</code></pre></div>\n\n<h3 id=\"webhook-notification-system\">Webhook Notification System</h3>\n<p>The webhook notification system provides reliable, real-time communication between the media processing pipeline and external applications. Unlike simple HTTP callbacks, our implementation handles the complexities of distributed systems: network failures, service outages, duplicate detection, and security verification. The system guarantees <strong>at-least-once delivery</strong> for critical notifications while providing <strong>exactly-once semantics</strong> for idempotent operations.</p>\n<h4 id=\"webhook-event-types-and-payloads\">Webhook Event Types and Payloads</h4>\n<p>The notification system supports multiple event types, each with standardized payload structures optimized for different integration patterns. Events follow a consistent schema that includes correlation metadata, security signatures, and extensible payload sections for future enhancement without breaking existing integrations.</p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Trigger Condition</th>\n<th>Payload Contents</th>\n<th>Retry Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job.started</code></td>\n<td>Job processing begins</td>\n<td>Job metadata, input file info</td>\n<td>Standard retry</td>\n</tr>\n<tr>\n<td><code>job.progress</code></td>\n<td>Major progress milestone</td>\n<td>Current stage, percentage, ETA</td>\n<td>No retry (informational)</td>\n</tr>\n<tr>\n<td><code>job.completed</code></td>\n<td>Processing successfully finishes</td>\n<td>Output file URLs, metadata</td>\n<td>Aggressive retry</td>\n</tr>\n<tr>\n<td><code>job.failed</code></td>\n<td>Permanent failure occurs</td>\n<td>Error details, retry eligibility</td>\n<td>Aggressive retry</td>\n</tr>\n<tr>\n<td><code>job.retry</code></td>\n<td>Job scheduled for retry</td>\n<td>Retry count, next attempt time</td>\n<td>Standard retry</td>\n</tr>\n</tbody></table>\n<p>Each webhook payload includes a comprehensive event envelope that provides recipients with sufficient context for processing and debugging. The envelope structure promotes consistency across event types while allowing type-specific data in the payload section.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Webhook Payload Structure:\n{\n  &quot;event_id&quot;: &quot;unique identifier for deduplication&quot;,\n  &quot;event_type&quot;: &quot;job.completed&quot;,\n  &quot;event_timestamp&quot;: &quot;ISO 8601 timestamp of event occurrence&quot;, \n  &quot;correlation_id&quot;: &quot;request tracking identifier&quot;,\n  &quot;api_version&quot;: &quot;v1&quot;,\n  &quot;signature&quot;: &quot;HMAC-SHA256 signature for verification&quot;,\n  &quot;job&quot;: {\n    &quot;job_id&quot;: &quot;processing job identifier&quot;,\n    &quot;status&quot;: &quot;current job status enum value&quot;,\n    &quot;progress_percentage&quot;: &quot;current completion percentage&quot;,\n    &quot;created_at&quot;: &quot;job creation timestamp&quot;,\n    &quot;started_at&quot;: &quot;processing start timestamp&quot;, \n    &quot;completed_at&quot;: &quot;completion timestamp if applicable&quot;\n  },\n  &quot;payload&quot;: {\n    // Event-specific data varies by event_type\n    &quot;output_files&quot;: [&quot;array of generated file URLs&quot;],\n    &quot;processing_duration&quot;: &quot;total processing time in seconds&quot;,\n    &quot;metadata&quot;: {&quot;extracted media metadata&quot;}\n  }\n}</code></pre></div>\n\n<p>The signature field contains an HMAC-SHA256 hash computed over the entire payload body using a shared secret configured per webhook endpoint. This prevents tampering and allows recipients to verify message authenticity and integrity. The signature calculation includes the raw JSON payload to prevent parsing-related security vulnerabilities.</p>\n<h4 id=\"reliable-delivery-implementation\">Reliable Delivery Implementation</h4>\n<p>Webhook delivery reliability requires sophisticated retry logic that balances prompt notification delivery with respectful behavior toward recipient services. Our implementation employs <strong>exponential backoff with jitter</strong> to prevent thundering herd effects while ensuring important notifications eventually reach their destinations even during extended outages.</p>\n<p>The retry system categorizes delivery failures into <strong>transient</strong> (network timeouts, 5xx responses), <strong>permanent</strong> (4xx client errors, invalid URLs), and <strong>throttling</strong> (429 rate limit responses) categories, each with appropriate retry behavior. Transient failures trigger exponential backoff retries, permanent failures immediately move to dead letter storage, and throttling failures use the <code>Retry-After</code> header when provided by the recipient.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>HTTP Status Codes</th>\n<th>Retry Behavior</th>\n<th>Max Attempts</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transient</td>\n<td>408, 500, 502, 503, 504</td>\n<td>Exponential backoff (1, 2, 4, 8, 16 min)</td>\n<td>5 attempts</td>\n</tr>\n<tr>\n<td>Permanent</td>\n<td>400, 401, 403, 404, 410</td>\n<td>No retry, dead letter</td>\n<td>1 attempt</td>\n</tr>\n<tr>\n<td>Rate Limiting</td>\n<td>429</td>\n<td>Honor Retry-After header</td>\n<td>3 attempts</td>\n</tr>\n<tr>\n<td>Network Error</td>\n<td>Connection timeout, DNS failure</td>\n<td>Exponential backoff</td>\n<td>5 attempts</td>\n</tr>\n</tbody></table>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Webhook Delivery Algorithm:\n1. Construct webhook payload with current job state and event-specific data\n2. Generate HMAC-SHA256 signature using configured webhook secret\n3. Set HTTP headers: Content-Type, User-Agent, X-Webhook-Signature, X-Event-Type\n4. Attempt HTTP POST delivery with 30-second timeout\n5. Classify response: success (2xx), transient error, permanent error, rate limit\n6. For transient errors: schedule retry with exponential backoff plus random jitter\n7. For permanent errors: log failure and move to dead letter queue  \n8. For rate limits: respect Retry-After header or use default backoff\n9. Update delivery attempt metrics for monitoring and alerting</code></pre></div>\n\n<p>The delivery system maintains <strong>idempotency</strong> through unique event identifiers that allow recipients to detect and discard duplicate deliveries. This enables aggressive retry policies without fear of causing duplicate processing in downstream systems.</p>\n<blockquote>\n<p><strong>Decision: At-Least-Once Delivery Guarantee</strong></p>\n<ul>\n<li><strong>Context</strong>: Network failures and service outages require robust notification delivery</li>\n<li><strong>Options Considered</strong>: At-most-once, at-least-once, exactly-once delivery semantics</li>\n<li><strong>Decision</strong>: At-least-once delivery with idempotency keys for duplicate detection</li>\n<li><strong>Rationale</strong>: Balances reliability with complexity - easier to detect duplicates than recover lost messages</li>\n<li><strong>Consequences</strong>: Recipients must implement idempotency checking but gain strong delivery guarantees</li>\n</ul>\n</blockquote>\n<h4 id=\"security-and-authentication\">Security and Authentication</h4>\n<p>Webhook security protects against message tampering, replay attacks, and unauthorized webhook injection. Our implementation provides <strong>cryptographic verification</strong> through HMAC signatures, <strong>timestamp validation</strong> to prevent replay attacks, and <strong>endpoint validation</strong> to ensure webhook URLs point to expected domains.</p>\n<p>The signature verification process uses HMAC-SHA256 with webhook-specific secrets that rotate periodically for enhanced security. Recipients verify signatures by computing the HMAC over the raw request body and comparing it to the provided signature header. Mismatched signatures result in rejection and security logging for investigation.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Signature Verification Process:\n1. Extract X-Webhook-Signature header from incoming request\n2. Retrieve webhook secret associated with the endpoint\n3. Compute HMAC-SHA256(secret, raw_request_body) \n4. Compare computed signature with provided signature using constant-time comparison\n5. Verify timestamp is within acceptable window (default: 5 minutes)\n6. Accept message if signature valid and timestamp fresh, otherwise reject</code></pre></div>\n\n<p>Timestamp validation prevents replay attacks where attackers capture valid webhook payloads and retransmit them later. The system includes the current timestamp in the signed payload and rejects messages older than a configurable threshold (typically 5-10 minutes).</p>\n<h3 id=\"progress-tracking-architecture-decisions\">Progress Tracking Architecture Decisions</h3>\n<p>The progress tracking system requires careful architectural decisions that balance real-time responsiveness, storage efficiency, and system reliability. These decisions shape how the system handles concurrent updates, stores progress data, and provides query interfaces for both internal components and external integrations.</p>\n<h4 id=\"storage-architecture-for-progress-data\">Storage Architecture for Progress Data</h4>\n<p>Progress tracking data exhibits unique characteristics that influence storage design: <strong>high write frequency</strong> (frequent progress updates), <strong>temporal access patterns</strong> (recent data accessed most often), <strong>moderate retention requirements</strong> (historical progress needed for debugging), and <strong>real-time query needs</strong> (dashboard and API access). These characteristics favor a hybrid storage approach combining in-memory caching for active jobs with persistent storage for historical data.</p>\n<blockquote>\n<p><strong>Decision: Redis + PostgreSQL Hybrid Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Progress data requires high-frequency writes with real-time read access</li>\n<li><strong>Options Considered</strong>: PostgreSQL only, Redis only, hybrid Redis+PostgreSQL approach</li>\n<li><strong>Decision</strong>: Active job progress in Redis with periodic persistence to PostgreSQL</li>\n<li><strong>Rationale</strong>: Redis provides sub-millisecond updates for active jobs, PostgreSQL ensures durability and complex queries</li>\n<li><strong>Consequences</strong>: Adds storage complexity but enables real-time performance with data durability</li>\n</ul>\n</blockquote>\n<p>The hybrid approach stores <strong>active job progress</strong> in Redis using optimized data structures for fast updates and queries. Each active job maintains a Redis hash containing current progress state, stage information, and update timestamps. Redis persistence occurs through periodic snapshots to PostgreSQL, ensuring progress data survives system restarts while maintaining real-time performance.</p>\n<table>\n<thead>\n<tr>\n<th>Data Category</th>\n<th>Primary Storage</th>\n<th>Persistence Layer</th>\n<th>Access Pattern</th>\n<th>Retention</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Active Progress</td>\n<td>Redis Hash</td>\n<td>PostgreSQL sync every 30s</td>\n<td>High frequency read/write</td>\n<td>Until job complete</td>\n</tr>\n<tr>\n<td>Completed Jobs</td>\n<td>PostgreSQL</td>\n<td>Archive after 90 days</td>\n<td>Occasional queries</td>\n<td>90 days active</td>\n</tr>\n<tr>\n<td>Progress History</td>\n<td>PostgreSQL</td>\n<td>Compressed storage</td>\n<td>Analytics queries</td>\n<td>1 year archived</td>\n</tr>\n<tr>\n<td>Notification Log</td>\n<td>PostgreSQL</td>\n<td>Time-series partitioning</td>\n<td>Debugging access</td>\n<td>30 days</td>\n</tr>\n</tbody></table>\n<p>Redis data structures optimize for common access patterns. Job progress uses Redis hashes for atomic field updates, while progress history uses sorted sets ordered by timestamp for efficient range queries. The notification system employs Redis lists for webhook delivery queues with automatic expiration for failed deliveries.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Redis Data Structure Design:\nprogress:job:{job_id} = Hash {\n  &quot;percentage&quot;: &quot;current completion percentage&quot;,\n  &quot;stage&quot;: &quot;current processing stage name&quot;, \n  &quot;stage_progress&quot;: &quot;progress within current stage&quot;,\n  &quot;updated_at&quot;: &quot;timestamp of last update&quot;,\n  &quot;estimated_completion&quot;: &quot;ETA timestamp&quot;,\n  &quot;notification_sent&quot;: &quot;last notification percentage&quot;\n}\n\nprogress:history:{job_id} = Sorted Set {\n  &quot;timestamp1&quot;: &quot;stage:percentage:details&quot;,\n  &quot;timestamp2&quot;: &quot;stage:percentage:details&quot; \n}\n\nwebhooks:pending = List [\n  &quot;webhook_delivery_request_1&quot;,\n  &quot;webhook_delivery_request_2&quot;\n]</code></pre></div>\n\n<h4 id=\"real-time-update-mechanisms\">Real-Time Update Mechanisms</h4>\n<p>Real-time progress updates serve multiple consumers with different latency requirements: <strong>user dashboards</strong> (sub-second updates), <strong>API clients</strong> (5-10 second intervals), <strong>webhook notifications</strong> (event-driven), and <strong>monitoring systems</strong> (1-minute aggregates). The system employs a <strong>publish-subscribe pattern</strong> with Redis Pub/Sub to efficiently distribute updates to all interested consumers without coupling update generation to consumption.</p>\n<p>Progress updates flow through a <strong>multi-tier notification system</strong> that provides different update frequencies and delivery guarantees based on consumer requirements. Real-time dashboard connections receive immediate updates via WebSocket connections, while API clients can poll for updates at configurable intervals. Webhook notifications trigger only when significant thresholds are crossed to prevent notification spam.</p>\n<table>\n<thead>\n<tr>\n<th>Consumer Type</th>\n<th>Update Frequency</th>\n<th>Delivery Method</th>\n<th>Filtering</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>WebSocket Dashboard</td>\n<td>Immediate</td>\n<td>Redis Pub/Sub → WebSocket</td>\n<td>All progress changes</td>\n</tr>\n<tr>\n<td>REST API Clients</td>\n<td>On-demand poll</td>\n<td>Direct Redis query</td>\n<td>Current state only</td>\n</tr>\n<tr>\n<td>Webhook Notifications</td>\n<td>Threshold-based</td>\n<td>Async delivery queue</td>\n<td>Major milestones</td>\n</tr>\n<tr>\n<td>Monitoring Systems</td>\n<td>1-minute aggregates</td>\n<td>Background collection</td>\n<td>Statistical summaries</td>\n</tr>\n</tbody></table>\n<p>The pub/sub architecture decouples progress generation from consumption, allowing the system to scale notification consumers independently of progress update frequency. Workers publish progress updates to Redis channels without waiting for acknowledgment, ensuring update operations remain fast and don&#39;t block processing.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Real-Time Update Flow:\n1. Worker process calculates new progress percentage and stage information\n2. Worker atomically updates Redis progress hash with new values\n3. Worker publishes update message to Redis channel: progress:updates\n4. WebSocket service subscribed to channel receives update immediately\n5. WebSocket service filters updates and forwards to connected dashboard clients\n6. Background process periodically persists Redis state to PostgreSQL\n7. Webhook service checks notification thresholds and queues deliveries as needed</code></pre></div>\n\n<h4 id=\"concurrency-control-and-race-conditions\">Concurrency Control and Race Conditions</h4>\n<p>Multiple worker processes may attempt simultaneous progress updates for the same job, particularly during batch operations or parallel processing stages. The system prevents race conditions through <strong>optimistic concurrency control</strong> using Redis atomic operations and sequence numbers that detect out-of-order updates.</p>\n<p>Each progress update includes a <strong>monotonically increasing sequence number</strong> generated by the worker process performing the update. Redis WATCH/MULTI/EXEC transactions ensure progress updates apply atomically and reject out-of-order updates that could cause progress to move backward incorrectly.</p>\n<blockquote>\n<p><strong>Decision: Optimistic Concurrency with Sequence Numbers</strong></p>\n<ul>\n<li><strong>Context</strong>: Multiple workers may update progress for the same job simultaneously</li>\n<li><strong>Options Considered</strong>: Pessimistic locking, optimistic concurrency, last-writer-wins</li>\n<li><strong>Decision</strong>: Optimistic concurrency control with sequence numbers and Redis atomic operations</li>\n<li><strong>Rationale</strong>: Avoids lock contention while preventing race conditions and progress reversals</li>\n<li><strong>Consequences</strong>: Requires retry logic in workers but provides better scalability than locking</li>\n</ul>\n</blockquote>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Atomic Progress Update Implementation:\n1. Worker generates sequence number: current_timestamp_microseconds\n2. Redis WATCH command monitors the progress hash for concurrent modifications  \n3. Worker reads current sequence number from progress hash\n4. If new sequence number &lt;= current sequence, abort update (out of order)\n5. Begin Redis MULTI transaction for atomic updates\n6. Update progress hash fields: percentage, stage, sequence, timestamp\n7. Publish update message to progress channel\n8. EXEC transaction - succeeds only if no concurrent modifications detected\n9. If transaction fails, retry with exponential backoff up to 3 attempts</code></pre></div>\n\n<p>Sequence number comparison prevents older updates from overwriting newer progress data, which could occur due to network delays or worker process variations in timing. Workers that detect rejected updates log the event for debugging but continue processing since another worker has already reported more recent progress.</p>\n<h3 id=\"common-progress-tracking-pitfalls\">Common Progress Tracking Pitfalls</h3>\n<p>Progress tracking systems encounter predictable failure modes that can significantly impact user experience and system reliability. Understanding these pitfalls and their solutions helps avoid common implementation mistakes that lead to inaccurate progress reporting, notification failures, and poor system observability.</p>\n<h4 id=\"-pitfall-progress-reversals-and-non-monotonic-updates\">⚠️ <strong>Pitfall: Progress Reversals and Non-Monotonic Updates</strong></h4>\n<p>One of the most confusing user experiences occurs when progress appears to move backward, showing 75% complete followed by 60% complete. This typically happens when multiple worker processes or processing stages report progress concurrently without proper coordination, or when different estimation algorithms provide conflicting progress calculations.</p>\n<p>Progress reversals commonly occur during <strong>parallel processing scenarios</strong> where multiple workers handle different output specifications for the same job. Worker A might complete 3 of 4 image resize operations (reporting 75% stage progress), while Worker B starts format conversion and reports early conversion progress (30% of final stage), causing the overall job progress to appear to decrease when calculated incorrectly.</p>\n<p><strong>Why this breaks user trust</strong>: Users expect progress to monotonically increase toward completion. Backward movement suggests system errors or inaccurate estimates, damaging confidence in the processing pipeline. Dashboard interfaces may show confusing progress bar animations that move backward, creating a broken user experience.</p>\n<p><strong>Prevention and fixes</strong>:</p>\n<ul>\n<li>Implement sequence numbers for all progress updates to detect and reject out-of-order updates</li>\n<li>Use atomic Redis operations to ensure progress updates apply consistently without race conditions  </li>\n<li>Design stage-based progress weights that sum to 100% and validate updates maintain monotonic progression</li>\n<li>Add progress validation logic that rejects updates showing backward movement unless explicitly resetting job state</li>\n<li>Log progress reversals as warnings for debugging while maintaining the last valid monotonic progress value</li>\n</ul>\n<h4 id=\"-pitfall-webhook-delivery-storms-and-rate-limiting\">⚠️ <strong>Pitfall: Webhook Delivery Storms and Rate Limiting</strong></h4>\n<p>Aggressive progress reporting can overwhelm webhook endpoints with excessive notification traffic, particularly during batch processing operations or when multiple jobs complete simultaneously. This leads to rate limiting responses from recipient services, failed deliveries, and potential service degradation for webhook consumers.</p>\n<p>Webhook storms typically occur when the system sends notifications for every progress increment rather than meaningful milestones. A video transcoding job might generate hundreds of progress updates per minute, causing the notification system to attempt hundreds of webhook deliveries that quickly exceed recipient rate limits and cause cascading failures.</p>\n<p><strong>Why this causes problems</strong>: Recipient services implement rate limiting to protect against abuse, and webhook storms can trigger these protections. Failed deliveries require retry logic that can compound the problem. Excessive webhook traffic also increases processing overhead and can impact system performance.</p>\n<p><strong>Prevention and fixes</strong>:</p>\n<ul>\n<li>Implement notification thresholds that trigger webhooks only for significant progress changes (typically 10-25% increments)</li>\n<li>Use webhook consolidation that batches multiple progress updates into single notifications when appropriate</li>\n<li>Add per-endpoint rate limiting in the webhook delivery system to respect recipient service limits</li>\n<li>Implement backoff strategies that honor <code>Retry-After</code> headers and reduce delivery frequency for rate-limited endpoints</li>\n<li>Provide webhook filtering configuration that lets recipients specify which event types and progress thresholds they want to receive</li>\n</ul>\n<h4 id=\"-pitfall-memory-leaks-in-progress-storage\">⚠️ <strong>Pitfall: Memory Leaks in Progress Storage</strong></h4>\n<p>Long-running systems accumulate progress tracking data for completed jobs, leading to memory exhaustion if not properly managed. Redis instances storing active job progress can grow unbounded when cleanup processes fail to remove completed job data, eventually causing out-of-memory errors and system instability.</p>\n<p>Memory leaks often occur because progress cleanup depends on external job completion notifications that may be lost due to system failures or race conditions. Completed jobs leave behind Redis hashes, sorted sets, and pub/sub subscriptions that continue consuming memory without providing value.</p>\n<p><strong>Why this causes system failures</strong>: Redis operates primarily in memory, and unbounded growth leads to memory exhaustion. System administrators may not notice gradual memory increases until sudden spikes in job volume cause out-of-memory crashes. Recovery requires manual intervention and potential data loss.</p>\n<p><strong>Prevention and fixes</strong>:</p>\n<ul>\n<li>Implement automatic cleanup processes with Redis TTL (Time To Live) settings on all progress data structures</li>\n<li>Use separate cleanup workers that periodically scan for completed jobs and remove associated progress data</li>\n<li>Add memory monitoring and alerting for Redis instances to detect gradual memory growth trends</li>\n<li>Implement circuit breaker patterns that stop accepting new jobs when Redis memory usage exceeds safe thresholds</li>\n<li>Design progress data structures with explicit expiration policies rather than relying solely on cleanup processes</li>\n</ul>\n<h4 id=\"-pitfall-inaccurate-time-estimates-and-user-expectations\">⚠️ <strong>Pitfall: Inaccurate Time Estimates and User Expectations</strong></h4>\n<p>Time estimation algorithms often provide wildly inaccurate completion estimates, particularly for complex media processing operations where actual processing time varies significantly based on content characteristics, system load, and processing complexity that&#39;s difficult to predict from input file metadata.</p>\n<p>Estimation errors frequently occur when algorithms rely solely on file size without considering content complexity factors like video codec efficiency, image compression characteristics, or the computational cost of specific processing operations. A large but simply-encoded video may process faster than a smaller file with complex motion that requires intensive encoding.</p>\n<p><strong>Why this damages user experience</strong>: Inaccurate estimates lead to user frustration and poor planning. Estimates that consistently overshoot completion times cause users to abandon jobs they believe are stuck. Underestimated completion times cause users to wait expecting imminent completion.</p>\n<p><strong>Prevention and fixes</strong>:</p>\n<ul>\n<li>Use conservative estimation algorithms that tend to overestimate rather than underestimate completion times</li>\n<li>Implement estimation confidence intervals that communicate uncertainty to users (&quot;15-25 minutes remaining&quot;)</li>\n<li>Refine estimates dynamically as processing progresses and actual performance data becomes available</li>\n<li>Provide stage-based progress communication (&quot;Converting video format - 3 of 5 quality variants complete&quot;) that gives meaningful information without specific time commitments</li>\n<li>Track estimation accuracy over time and adjust algorithms based on historical performance data</li>\n</ul>\n<h4 id=\"-pitfall-race-conditions-in-progress-percentage-calculations\">⚠️ <strong>Pitfall: Race Conditions in Progress Percentage Calculations</strong></h4>\n<p>Complex jobs involving multiple parallel processing stages can produce race conditions when calculating overall progress percentages, leading to inconsistent or impossible progress values (such as percentages exceeding 100% or stage progress that doesn&#39;t align with overall job progress).</p>\n<p>Race conditions typically emerge when different processing threads update stage-specific progress simultaneously while another thread calculates overall job progress. The calculation may read partially updated stage progress values, producing incorrect overall percentages that don&#39;t reflect actual processing state.</p>\n<p><strong>Why this causes confusion</strong>: Inconsistent progress reporting makes the system appear unreliable and can cause client applications to malfunction if they depend on progress values for logic decisions. Users see confusing progress displays that don&#39;t match the actual processing state.</p>\n<p><strong>Prevention and fixes</strong>:</p>\n<ul>\n<li>Use atomic read operations when calculating progress percentages to ensure consistent snapshots of all stage progress values</li>\n<li>Implement progress calculation locks or atomic transactions that prevent concurrent modification during percentage calculation</li>\n<li>Add validation logic that verifies calculated progress percentages fall within expected ranges (0-100%) and stage consistency</li>\n<li>Design progress data structures that support atomic updates of multiple related values</li>\n<li>Use eventual consistency patterns where progress displays may lag slightly but always show consistent, valid values</li>\n</ul>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fwebhook-notification-flow.svg\" alt=\"Webhook Notification Flow\"></p>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fvideo-processing-sequence.svg\" alt=\"Video Processing Sequence\"></p>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fworker-coordination.svg\" alt=\"Worker Process Coordination\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The progress tracking and notification system requires careful orchestration of real-time updates, persistent storage, and reliable webhook delivery. This implementation guidance provides complete starter code for the infrastructure components while identifying the core logic areas where learners should focus their implementation efforts.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Progress Storage</td>\n<td>SQLite with in-memory caching</td>\n<td>Redis + PostgreSQL hybrid</td>\n</tr>\n<tr>\n<td>Real-time Updates</td>\n<td>HTTP polling with 5-second intervals</td>\n<td>WebSocket connections with Redis Pub/Sub</td>\n</tr>\n<tr>\n<td>Webhook Delivery</td>\n<td>Simple HTTP POST with basic retry</td>\n<td>Async queue with exponential backoff</td>\n</tr>\n<tr>\n<td>Progress Calculation</td>\n<td>Fixed percentage increments per stage</td>\n<td>Dynamic stage weighting with time estimates</td>\n</tr>\n<tr>\n<td>Notification Filtering</td>\n<td>All progress changes trigger webhooks</td>\n<td>Threshold-based filtering with batching</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>The progress tracking component integrates with job queue operations and external webhook delivery, requiring clean separation between progress calculation, storage management, and notification delivery concerns.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media_processing/\n  progress/\n    __init__.py                 ← component exports\n    tracker.py                  ← core ProgressTracker class\n    calculator.py               ← stage-based progress calculation\n    storage.py                  ← Redis/PostgreSQL hybrid storage\n    notifications.py            ← webhook notification system  \n    models.py                   ← progress data structures\n    webhooks/\n      __init__.py\n      delivery.py               ← reliable webhook delivery\n      retry.py                  ← exponential backoff logic\n      security.py               ← signature generation/verification\n    tests/\n      test_tracker.py           ← progress tracking tests\n      test_webhooks.py          ← webhook delivery tests\n      test_storage.py           ← storage integration tests\n  queue/\n    worker.py                   ← imports progress.tracker\n  main.py                       ← progress tracking configuration</code></pre></div>\n\n<h4 id=\"progress-storage-infrastructure-complete-implementation\">Progress Storage Infrastructure (Complete Implementation)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># progress/storage.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timezone</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psycopg2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> psycopg2.extras </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RealDictCursor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobProgress, ProgressUpdate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProgressStorage</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Hybrid Redis + PostgreSQL storage for job progress tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Redis provides real-time performance for active jobs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    PostgreSQL ensures durability and supports complex queries.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: redis.Redis, pg_conn_string: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pg_conn_string </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pg_conn_string</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._setup_schema()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _setup_schema</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize PostgreSQL schema for progress persistence.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pg_conn_string) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> conn.cursor() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cur:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    CREATE TABLE IF NOT EXISTS job_progress (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        job_id VARCHAR(255) PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        percentage FLOAT NOT NULL DEFAULT 0.0,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        stage VARCHAR(100) NOT NULL DEFAULT 'pending',</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        stage_progress FLOAT NOT NULL DEFAULT 0.0,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        updated_at TIMESTAMP WITH TIME ZONE NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        estimated_completion TIMESTAMP WITH TIME ZONE,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        sequence_number BIGINT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        details JSONB</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    );</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    CREATE TABLE IF NOT EXISTS progress_history (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        id SERIAL PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        job_id VARCHAR(255) NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        percentage FLOAT NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        stage VARCHAR(100) NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        recorded_at TIMESTAMP WITH TIME ZONE NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        details JSONB</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    );</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    CREATE INDEX IF NOT EXISTS idx_progress_history_job_time </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        ON progress_history(job_id, recorded_at);</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                conn.commit()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_progress</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, progress: JobProgress) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Atomically update job progress in Redis with PostgreSQL backup.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if update succeeded, False if rejected (out of order).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sequence_num </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(time.time() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000000</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># microsecond timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Atomic update with sequence number validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe.watch(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> pipe.hgetall(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_seq </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(current_data.get(</span><span style=\"color:#9ECBFF\">'sequence_number'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> sequence_num </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> current_seq:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Out of order progress update for job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                             f</span><span style=\"color:#9ECBFF\">\"new=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">sequence_num</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, current=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">current_seq</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.multi()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            progress_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asdict(progress)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            progress_dict[</span><span style=\"color:#9ECBFF\">'sequence_number'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sequence_num</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            progress_dict[</span><span style=\"color:#9ECBFF\">'updated_at'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now(timezone.utc).isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">progress_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.zadd(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:history:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     {json.dumps(progress_dict): sequence_num})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.expire(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">86400</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># 24 hour TTL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.expire(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:history:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">86400</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.publish(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:updates\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        json.dumps({</span><span style=\"color:#9ECBFF\">\"job_id\"</span><span style=\"color:#E1E4E8\">: job_id, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">progress_dict}))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.execute()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Async backup to PostgreSQL every 10% progress</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> progress.percentage </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._backup_to_postgres(job_id, progress, sequence_num)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> redis.WatchError:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Concurrent update detected for job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, retrying\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            pipe.reset()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_progress</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[JobProgress]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve current progress for job from Redis or PostgreSQL.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Try Redis first for active jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.hgetall(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> data:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> JobProgress(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                job_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                percentage</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(data[</span><span style=\"color:#9ECBFF\">'percentage'</span><span style=\"color:#E1E4E8\">]),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                stage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'stage'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                stage_progress</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(data[</span><span style=\"color:#9ECBFF\">'stage_progress'</span><span style=\"color:#E1E4E8\">]),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">json.loads(data.get(</span><span style=\"color:#9ECBFF\">'details'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Fallback to PostgreSQL for completed jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pg_conn_string) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> conn.cursor(</span><span style=\"color:#FFAB70\">cursor_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RealDictCursor) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cur:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    SELECT percentage, stage, stage_progress, details, updated_at</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    FROM job_progress WHERE job_id = </span><span style=\"color:#79B8FF\">%s</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">, (job_id,))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                row </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cur.fetchone()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> row:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> JobProgress(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        job_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        percentage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">row[</span><span style=\"color:#9ECBFF\">'percentage'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        stage</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">row[</span><span style=\"color:#9ECBFF\">'stage'</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        stage_progress</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">row[</span><span style=\"color:#9ECBFF\">'stage_progress'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                        details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">row[</span><span style=\"color:#9ECBFF\">'details'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _backup_to_postgres</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, progress: JobProgress, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           sequence_num: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Asynchronously backup progress to PostgreSQL.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.pg_conn_string) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                with</span><span style=\"color:#E1E4E8\"> conn.cursor() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cur:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        INSERT INTO job_progress </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        (job_id, percentage, stage, stage_progress, updated_at, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                         sequence_number, details)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        VALUES (</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        ON CONFLICT (job_id) DO UPDATE SET</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            percentage = EXCLUDED.percentage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            stage = EXCLUDED.stage,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            stage_progress = EXCLUDED.stage_progress,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            updated_at = EXCLUDED.updated_at,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            sequence_number = EXCLUDED.sequence_number,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            details = EXCLUDED.details</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"\"\"</span><span style=\"color:#E1E4E8\">, (job_id, progress.percentage, progress.stage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          progress.stage_progress, datetime.now(timezone.utc),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          sequence_num, json.dumps(progress.details)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Archive progress milestone to history</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        INSERT INTO progress_history </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        (job_id, percentage, stage, recorded_at, details)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        VALUES (</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"\"\"</span><span style=\"color:#E1E4E8\">, (job_id, progress.percentage, progress.stage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          datetime.now(timezone.utc), json.dumps(progress.details)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                conn.commit()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to backup progress for job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> cleanup_completed_job</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove Redis data for completed job to prevent memory leaks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.pipeline()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe.delete(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe.delete(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"progress:history:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pipe.execute()</span></span></code></pre></div>\n\n<h4 id=\"webhook-security-and-signature-verification-complete-implementation\">Webhook Security and Signature Verification (Complete Implementation)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># progress/webhooks/security.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hmac</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> WebhookSecurity</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides HMAC signature generation and verification for webhook security.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Prevents tampering and replay attacks through cryptographic validation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_signature</span><span style=\"color:#E1E4E8\">(payload: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, secret: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate HMAC-SHA256 signature for webhook payload.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            payload: Raw JSON payload string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            secret: Webhook endpoint secret key</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Hex-encoded signature for X-Webhook-Signature header</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        signature </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hmac.new(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            secret.encode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            payload.encode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            hashlib.sha256</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ).hexdigest()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"sha256=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">signature</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_signature</span><span style=\"color:#E1E4E8\">(payload: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, signature: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, secret: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        max_age: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Verify webhook signature and timestamp to prevent replay attacks.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            payload: Raw request body string</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            signature: X-Webhook-Signature header value</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            secret: Webhook endpoint secret</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_age: Maximum message age in seconds (default 5 minutes)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if signature valid and timestamp fresh</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> signature.startswith(</span><span style=\"color:#9ECBFF\">'sha256='</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected_signature </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> WebhookSecurity.generate_signature(payload, secret)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Constant-time comparison prevents timing attacks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> hmac.compare_digest(signature, expected_signature):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Verify timestamp to prevent replay attacks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(payload)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            event_timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> data.get(</span><span style=\"color:#9ECBFF\">'event_timestamp'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> event_timestamp:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                event_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.mktime(time.strptime(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    event_timestamp, </span><span style=\"color:#9ECBFF\">'%Y-%m-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">T%H:%M:%S.</span><span style=\"color:#79B8FF\">%f</span><span style=\"color:#9ECBFF\">Z'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> event_time </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> max_age:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> (json.JSONDecodeError, </span><span style=\"color:#79B8FF\">ValueError</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">KeyError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_webhook_payload</span><span style=\"color:#E1E4E8\">(event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, job_data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              payload_data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Create standardized webhook payload with security metadata.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            event_type: Type of event (job.started, job.completed, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_data: Job information dictionary  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            payload_data: Event-specific payload data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Complete webhook payload ready for signing and delivery</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"event_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"event_type\"</span><span style=\"color:#E1E4E8\">: event_type,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"event_timestamp\"</span><span style=\"color:#E1E4E8\">: time.strftime(</span><span style=\"color:#9ECBFF\">'%Y-%m-</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">T%H:%M:%S.</span><span style=\"color:#79B8FF\">%f</span><span style=\"color:#9ECBFF\">Z'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"api_version\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"v1\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"job\"</span><span style=\"color:#E1E4E8\">: job_data,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"payload\"</span><span style=\"color:#E1E4E8\">: payload_data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<h4 id=\"core-progress-tracking-logic-implementation-skeleton\">Core Progress Tracking Logic (Implementation Skeleton)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># progress/tracker.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .storage </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ProgressStorage  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .calculator </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ProgressCalculator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .notifications </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> WebhookNotifier</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobProgress, ProcessingStage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProgressTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Central progress tracking coordinator that manages stage-based progress</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    calculation, storage persistence, and webhook notification delivery.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, storage: ProgressStorage, calculator: ProgressCalculator,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 notifier: WebhookNotifier):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.calculator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> calculator</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.notifier </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> notifier</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.notification_thresholds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">25.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">50.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">75.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">90.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100.0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> initialize_job_progress</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, total_stages: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               webhook_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize progress tracking for new processing job.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Unique job identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            total_stages: List of processing stages in order</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            webhook_url: Optional webhook URL for notifications</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if initialization successful</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create initial JobProgress object with 0% completion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set stage to first stage in total_stages list  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize stage_progress to 0.0 and details to empty dict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store initial progress using self.storage.update_progress()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Register webhook URL with notifier if provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Send job.started webhook notification if webhook_url configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.calculator.initialize_stage_weights(total_stages)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_stage_progress</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             stage_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             details: Optional[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Update progress within current processing stage.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Job identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stage: Current processing stage name</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stage_percentage: Completion percentage within stage (0-100)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            details: Optional additional progress details</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if update successful and within monotonic constraints</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate stage_percentage is between 0.0 and 100.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get current progress from storage to check stage consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use calculator to compute overall job percentage from stage progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate new percentage >= current percentage (monotonic progression)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create updated JobProgress object with new values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store updated progress atomically using storage layer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Check if notification threshold crossed since last webhook</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Trigger async webhook delivery if threshold met</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: self.calculator.calculate_overall_progress(stage, stage_percentage)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> advance_to_next_stage</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, next_stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             completion_details: Optional[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Transition job to next processing stage with 100% completion of current stage.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Job identifier  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            next_stage: Name of next processing stage</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            completion_details: Optional details about completed stage</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if stage transition successful</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current progress and validate job exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Mark current stage as 100% complete with stage completion details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate next_stage is valid in the configured stage sequence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate new overall percentage with current stage complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create JobProgress for new stage with 0% stage progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update storage with new stage and overall progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Check for notification thresholds and send webhooks if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.calculator.get_stage_transition_percentage(next_stage)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_completed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_files: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          processing_duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Mark job as 100% complete and send final webhook notification.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Job identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            output_files: List of generated output file paths</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            processing_duration: Total processing time in seconds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if completion marking successful</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set job progress to 100% with 'completed' stage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Include output_files and processing_duration in progress details  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store final progress state with completion timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Send job.completed webhook with output file information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Clean up Redis progress data to prevent memory leaks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Log completion event with duration and output file count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: self.storage.cleanup_completed_job(job_id) for memory management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_failed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       retry_eligible: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Mark job as permanently failed or eligible for retry.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job_id: Job identifier</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            error_message: Detailed error description</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            retry_eligible: Whether job can be retried automatically</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if failure marking successful</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current progress to preserve stage and percentage information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create failed JobProgress with error details and retry status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store failure state with error message and timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Send job.failed webhook with error details and retry information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Clean up Redis data if failure is permanent (not retryable)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Log failure event with error classification for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Include current stage in failure details for debugging context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"stage-based-progress-calculation-implementation-skeleton\">Stage-Based Progress Calculation (Implementation Skeleton)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># progress/calculator.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProcessingStage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Standard processing stages with predefined weight allocations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VALIDATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"validation\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PREPROCESSING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"preprocessing\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10.0</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RESIZE_OPERATIONS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"resize_operations\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">40.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FORMAT_CONVERSION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"format_conversion\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">35.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPTIMIZATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"optimization\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">8.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FINALIZATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"finalization\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProgressCalculator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Calculates overall job progress based on stage completion and weights.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Ensures monotonic progress and accurate percentage computation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stage_weights: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stage_order: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> initialize_stage_weights</span><span style=\"color:#E1E4E8\">(self, stages: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                custom_weights: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Configure stage weights for progress calculation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stages: Ordered list of processing stages</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            custom_weights: Optional custom weight allocation per stage</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if weights total 100% and are valid</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use custom_weights if provided, otherwise use default ProcessingStage weights</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate that all stages in list have corresponding weight values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify total weights sum to exactly 100.0 (within floating point tolerance)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store stage_order list to track stage progression sequence  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store stage_weights dict for percentage calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return False if weight validation fails, True on success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: sum(weights.values()) should equal 100.0 ± 0.01 for floating point comparison</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_overall_progress</span><span style=\"color:#E1E4E8\">(self, current_stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  stage_progress: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate overall job progress percentage based on current stage position.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            current_stage: Name of currently executing stage</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            stage_progress: Completion percentage within current stage (0-100)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Overall job completion percentage (0-100)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate current_stage exists in configured stage_order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate completed percentage from all previous stages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add weighted progress from current stage: (stage_weight * stage_progress / 100)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Ensure result stays within 0-100 bounds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return total percentage with appropriate precision (1 decimal place)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.stage_order.index(current_stage) to find position</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_stage_transition_percentage</span><span style=\"color:#E1E4E8\">(self, next_stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate progress percentage when transitioning to next stage.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            next_stage: Stage being transitioned to</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Progress percentage with all previous stages 100% complete</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Find index of next_stage in stage_order list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Sum weights of all stages before next_stage (these are 100% complete)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return cumulative percentage for completed stages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate result is reasonable (should be &#x3C; 100% unless final stage)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This shows progress at start of next_stage with previous stages done</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_remaining_time</span><span style=\"color:#E1E4E8\">(self, current_progress: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               elapsed_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Estimate remaining processing time based on current progress rate.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            current_progress: Current job completion percentage  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            elapsed_time: Time elapsed since job started (seconds)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Estimated remaining time in seconds, or None if insufficient data</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate current_progress > 0 to avoid division by zero</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate processing rate: elapsed_time / current_progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Estimate total time: processing_rate * 100.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate remaining time: total_time - elapsed_time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return None if estimate seems unreasonable (negative or > 24 hours)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Add safety margin (15-25%) to account for varying stage complexity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing the progress tracking system, verify functionality through these checkpoints:</p>\n<p><strong>Checkpoint 1: Basic Progress Updates</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test progress tracking with sample job</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> progress/tests/test_tracker.py::test_stage_progression</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Job initializes with 0% progress at first stage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Stage progress updates increment overall percentage correctly  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Stage transitions advance to next stage with proper percentage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Progress updates maintain monotonic progression (never go backward)</span></span></code></pre></div>\n\n<p><strong>Checkpoint 2: Webhook Notification Delivery</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start webhook test server to receive notifications</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> progress/tests/webhook_test_server.py</span><span style=\"color:#79B8FF\"> --port</span><span style=\"color:#79B8FF\"> 8080</span><span style=\"color:#E1E4E8\"> &#x26;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run webhook delivery tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> progress/tests/test_webhooks.py::test_delivery_with_retry</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify webhook server receives:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - job.started notification when job begins</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - job.progress notifications at 25%, 50%, 75% thresholds  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - job.completed notification with output file URLs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Valid HMAC signatures on all webhook payloads</span></span></code></pre></div>\n\n<p><strong>Checkpoint 3: Concurrent Progress Updates</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test concurrent worker progress updates</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> progress/tests/test_concurrency.py</span><span style=\"color:#79B8FF\"> --workers</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#79B8FF\"> --job-count</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Multiple workers update same job without race conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Progress values remain monotonic despite concurrent updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - No progress reversals or inconsistent percentages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Redis sequence numbers prevent out-of-order updates</span></span></code></pre></div>\n\n<p><strong>Signs of Implementation Issues:</strong></p>\n<ul>\n<li><strong>Progress jumps backward</strong>: Check sequence number implementation and Redis atomic operations</li>\n<li><strong>Webhook delivery failures</strong>: Verify signature generation and retry logic configuration</li>\n<li><strong>Memory usage growth</strong>: Confirm cleanup processes remove completed job data from Redis</li>\n<li><strong>Inaccurate percentages</strong>: Validate stage weight calculations sum to exactly 100%</li>\n</ul>\n<h2 id=\"component-interactions-and-data-flow\">Component Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as this section details the communication patterns between image processing, video transcoding, job queue, and progress tracking components</p>\n</blockquote>\n<h3 id=\"mental-model-orchestra-performance\">Mental Model: Orchestra Performance</h3>\n<p>Think of the media processing pipeline as a symphony orchestra performing a complex musical piece. The <strong>API Gateway</strong> acts as the conductor, receiving requests from the audience (clients) and coordinating the entire performance. The <strong>job queue</strong> serves as the sheet music distribution system, ensuring each musician (worker process) knows what to play and when. Individual <strong>worker processes</strong> are like specialized musicians - some excel at string instruments (image processing), others at brass (video transcoding), each following their part while contributing to the overall performance.</p>\n<p>The <strong>progress tracking system</strong> functions like the concert program that audience members follow, providing real-time updates about which movement is currently playing and how much remains. <strong>Webhook notifications</strong> are like the applause cues in the program - they signal important moments to the audience at precisely the right time. Just as musicians must stay synchronized through visual cues and timing, our components communicate through well-defined message formats and sequenced interactions to produce a harmonious result.</p>\n<p>This orchestration requires precise timing, clear communication channels, and graceful recovery when a musician (component) encounters difficulties. The conductor doesn&#39;t need to know how to play every instrument, but must understand how each section contributes to the whole performance and coordinate their interactions seamlessly.</p>\n<h3 id=\"api-interfaces-and-contracts\">API Interfaces and Contracts</h3>\n<p>The API layer serves as the primary interface between external clients and the internal media processing system, providing REST endpoints that abstract the complexity of job submission, status monitoring, and result retrieval. These interfaces follow RESTful principles while accommodating the asynchronous nature of media processing operations.</p>\n<h4 id=\"job-submission-endpoint\">Job Submission Endpoint</h4>\n<p>The job submission endpoint receives media processing requests and returns immediately with a job identifier, enabling clients to track processing asynchronously. This endpoint accepts multipart form data containing the input file and processing specifications, validating inputs before queue submission.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Endpoint</th>\n<th>Request Format</th>\n<th>Response Format</th>\n<th>Status Codes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>POST</td>\n<td><code>/jobs</code></td>\n<td><code>multipart/form-data</code> with file and JSON config</td>\n<td>JSON with <code>job_id</code>, <code>status</code>, <code>estimated_duration</code></td>\n<td>201 Created, 400 Bad Request, 413 Payload Too Large, 422 Unprocessable Entity</td>\n</tr>\n<tr>\n<td>GET</td>\n<td><code>/jobs/{job_id}</code></td>\n<td>None</td>\n<td>JSON with <code>ProcessingJob</code> details</td>\n<td>200 OK, 404 Not Found</td>\n</tr>\n<tr>\n<td>GET</td>\n<td><code>/jobs/{job_id}/progress</code></td>\n<td>None</td>\n<td>JSON with <code>JobProgress</code> details</td>\n<td>200 OK, 404 Not Found</td>\n</tr>\n<tr>\n<td>DELETE</td>\n<td><code>/jobs/{job_id}</code></td>\n<td>None</td>\n<td>JSON confirmation</td>\n<td>200 OK, 404 Not Found, 409 Conflict if processing</td>\n</tr>\n</tbody></table>\n<p>The request payload structure accommodates multiple output specifications within a single job, allowing clients to request multiple formats, resolutions, or quality variants in one submission:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Required</th>\n<th>Description</th>\n<th>Validation Rules</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>input_file</code></td>\n<td>File</td>\n<td>Yes</td>\n<td>Media file to process</td>\n<td>Max 500MB, supported MIME types only</td>\n</tr>\n<tr>\n<td><code>output_specifications</code></td>\n<td>JSON Array</td>\n<td>Yes</td>\n<td>List of desired outputs</td>\n<td>1-10 specifications per job</td>\n</tr>\n<tr>\n<td><code>priority</code></td>\n<td>String</td>\n<td>No</td>\n<td>Job priority level</td>\n<td>One of: low, normal, high, urgent</td>\n</tr>\n<tr>\n<td><code>webhook_url</code></td>\n<td>URL</td>\n<td>No</td>\n<td>Callback URL for notifications</td>\n<td>Must be HTTPS, reachable endpoint</td>\n</tr>\n<tr>\n<td><code>metadata_handling</code></td>\n<td>String</td>\n<td>No</td>\n<td>EXIF/metadata preservation mode</td>\n<td>strip, preserve, selective</td>\n</tr>\n<tr>\n<td><code>processing_options</code></td>\n<td>JSON Object</td>\n<td>No</td>\n<td>Advanced processing parameters</td>\n<td>Format-specific validation</td>\n</tr>\n</tbody></table>\n<h4 id=\"response-data-structures\">Response Data Structures</h4>\n<p>API responses provide comprehensive job information while maintaining consistency across different endpoint variations. The response format adapts to include relevant details based on the endpoint and job state.</p>\n<table>\n<thead>\n<tr>\n<th>Response Field</th>\n<th>Type</th>\n<th>Always Present</th>\n<th>Description</th>\n<th>Example Values</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job_id</code></td>\n<td>String</td>\n<td>Yes</td>\n<td>Unique job identifier</td>\n<td><code>img_20231215_143022_abc123</code></td>\n</tr>\n<tr>\n<td><code>status</code></td>\n<td>Enum</td>\n<td>Yes</td>\n<td>Current job status</td>\n<td><code>pending</code>, <code>processing</code>, <code>completed</code>, <code>failed</code></td>\n</tr>\n<tr>\n<td><code>created_at</code></td>\n<td>ISO8601</td>\n<td>Yes</td>\n<td>Job submission timestamp</td>\n<td><code>2023-12-15T14:30:22Z</code></td>\n</tr>\n<tr>\n<td><code>started_at</code></td>\n<td>ISO8601</td>\n<td>If started</td>\n<td>Processing start time</td>\n<td><code>2023-12-15T14:30:25Z</code></td>\n</tr>\n<tr>\n<td><code>completed_at</code></td>\n<td>ISO8601</td>\n<td>If terminal</td>\n<td>Processing completion time</td>\n<td><code>2023-12-15T14:32:18Z</code></td>\n</tr>\n<tr>\n<td><code>progress_percentage</code></td>\n<td>Float</td>\n<td>If processing</td>\n<td>Overall completion percentage</td>\n<td><code>67.5</code></td>\n</tr>\n<tr>\n<td><code>current_stage</code></td>\n<td>String</td>\n<td>If processing</td>\n<td>Current processing phase</td>\n<td><code>resize_operations</code></td>\n</tr>\n<tr>\n<td><code>estimated_duration</code></td>\n<td>Integer</td>\n<td>If processing</td>\n<td>Remaining seconds estimate</td>\n<td><code>45</code></td>\n</tr>\n<tr>\n<td><code>output_files</code></td>\n<td>Array</td>\n<td>If completed</td>\n<td>Generated file download URLs</td>\n<td><code>[&quot;/downloads/thumb.jpg&quot;]</code></td>\n</tr>\n<tr>\n<td><code>error_message</code></td>\n<td>String</td>\n<td>If failed</td>\n<td>Human-readable error description</td>\n<td><code>Unsupported codec in input video</code></td>\n</tr>\n<tr>\n<td><code>retry_count</code></td>\n<td>Integer</td>\n<td>Yes</td>\n<td>Number of retry attempts made</td>\n<td><code>2</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The API maintains idempotency by accepting optional <code>idempotency_key</code> headers. If a client submits the same key within 24 hours, the system returns the existing job rather than creating a duplicate. This prevents accidental duplicate processing when clients retry failed requests.</p>\n</blockquote>\n<h4 id=\"content-negotiation-and-format-support\">Content Negotiation and Format Support</h4>\n<p>The API supports multiple response formats and content negotiation to accommodate different client requirements and integration patterns. Clients can request JSON, XML, or abbreviated formats based on their capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Accept Header</th>\n<th>Response Format</th>\n<th>Use Case</th>\n<th>Content Type</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>application/json</code></td>\n<td>Full JSON with all fields</td>\n<td>Web applications, modern APIs</td>\n<td><code>application/json; charset=utf-8</code></td>\n</tr>\n<tr>\n<td><code>application/json; compact=true</code></td>\n<td>Minimal JSON with essential fields</td>\n<td>Mobile apps, bandwidth-limited</td>\n<td><code>application/json; charset=utf-8</code></td>\n</tr>\n<tr>\n<td><code>application/xml</code></td>\n<td>XML format for legacy systems</td>\n<td>Enterprise integration</td>\n<td><code>application/xml; charset=utf-8</code></td>\n</tr>\n<tr>\n<td><code>text/plain</code></td>\n<td>Simple status text</td>\n<td>Command-line tools, monitoring</td>\n<td><code>text/plain; charset=utf-8</code></td>\n</tr>\n</tbody></table>\n<p><strong>Decision: REST over GraphQL for API Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose API paradigm for client communication with varying complexity requirements</li>\n<li><strong>Options Considered</strong>: REST with JSON, GraphQL, gRPC with Protocol Buffers</li>\n<li><strong>Decision</strong>: REST with JSON as primary interface</li>\n<li><strong>Rationale</strong>: Media processing APIs have simple, resource-oriented operations (submit job, check status, retrieve results). REST provides excellent caching, is universally supported, and matches the async processing model naturally. GraphQL adds complexity without significant benefit for this domain.</li>\n<li><strong>Consequences</strong>: Enables standard HTTP caching, simple client integration, but requires multiple requests for complex queries. Webhook notifications complement REST&#39;s limitations for real-time updates.</li>\n</ul>\n<h4 id=\"error-response-format\">Error Response Format</h4>\n<p>Error responses follow RFC 7807 Problem Details format, providing structured error information that clients can programmatically handle while remaining human-readable for debugging purposes.</p>\n<table>\n<thead>\n<tr>\n<th>Error Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>type</code></td>\n<td>URI</td>\n<td>Problem type identifier</td>\n<td><code>/errors/unsupported-format</code></td>\n</tr>\n<tr>\n<td><code>title</code></td>\n<td>String</td>\n<td>Short error summary</td>\n<td><code>Unsupported Media Format</code></td>\n</tr>\n<tr>\n<td><code>status</code></td>\n<td>Integer</td>\n<td>HTTP status code</td>\n<td><code>422</code></td>\n</tr>\n<tr>\n<td><code>detail</code></td>\n<td>String</td>\n<td>Detailed error explanation</td>\n<td><code>WebM container with AV1 codec not supported</code></td>\n</tr>\n<tr>\n<td><code>instance</code></td>\n<td>URI</td>\n<td>Specific error instance</td>\n<td><code>/jobs/video_20231215_143022_xyz789</code></td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td>ISO8601</td>\n<td>Error occurrence time</td>\n<td><code>2023-12-15T14:30:23Z</code></td>\n</tr>\n<tr>\n<td><code>request_id</code></td>\n<td>String</td>\n<td>Request correlation ID</td>\n<td><code>req_abc123def456</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"internal-message-formats\">Internal Message Formats</h3>\n<p>Internal communication between components uses structured message formats that ensure reliable delivery, enable monitoring, and support system evolution. These messages flow through Redis-based queues with JSON serialization for cross-language compatibility.</p>\n<h4 id=\"job-queue-message-structure\">Job Queue Message Structure</h4>\n<p>Job queue messages contain all information necessary for worker processes to execute media processing tasks independently. The message format balances completeness with serialization efficiency, supporting both immediate processing and delayed execution scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Message Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Serialization Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job_id</code></td>\n<td>String</td>\n<td>Unique job identifier</td>\n<td>Used for correlation across all systems</td>\n</tr>\n<tr>\n<td><code>message_type</code></td>\n<td>Enum</td>\n<td>Message type discriminator</td>\n<td><code>PROCESS_MEDIA</code>, <code>CANCEL_JOB</code>, <code>HEARTBEAT</code></td>\n</tr>\n<tr>\n<td><code>priority</code></td>\n<td>Integer</td>\n<td>Numeric priority for queue ordering</td>\n<td>Higher numbers processed first</td>\n</tr>\n<tr>\n<td><code>input_file_path</code></td>\n<td>String</td>\n<td>Absolute path to input media file</td>\n<td>Must be accessible to all workers</td>\n</tr>\n<tr>\n<td><code>output_specifications</code></td>\n<td>Array</td>\n<td>List of <code>OutputSpecification</code> objects</td>\n<td>Serialized as nested JSON</td>\n</tr>\n<tr>\n<td><code>processing_config</code></td>\n<td>Object</td>\n<td>Processing parameters and constraints</td>\n<td>Contains format-specific settings</td>\n</tr>\n<tr>\n<td><code>webhook_url</code></td>\n<td>String</td>\n<td>Callback URL for progress notifications</td>\n<td>Optional, null if no notifications</td>\n</tr>\n<tr>\n<td><code>metadata_options</code></td>\n<td>Object</td>\n<td>EXIF and metadata handling preferences</td>\n<td>Controls privacy and compatibility</td>\n</tr>\n<tr>\n<td><code>retry_policy</code></td>\n<td>Object</td>\n<td>Retry behavior configuration</td>\n<td>Max attempts, backoff strategy</td>\n</tr>\n<tr>\n<td><code>resource_constraints</code></td>\n<td>Object</td>\n<td>Memory and CPU limits for processing</td>\n<td>Helps with worker assignment</td>\n</tr>\n<tr>\n<td><code>correlation_id</code></td>\n<td>String</td>\n<td>Request tracking across components</td>\n<td>Links API request to internal operations</td>\n</tr>\n<tr>\n<td><code>created_at</code></td>\n<td>Integer</td>\n<td>Unix timestamp of message creation</td>\n<td>Used for TTL and aging policies</td>\n</tr>\n<tr>\n<td><code>timeout_at</code></td>\n<td>Integer</td>\n<td>Unix timestamp when job expires</td>\n<td>Prevents infinite queue retention</td>\n</tr>\n</tbody></table>\n<p>The <code>OutputSpecification</code> objects within job messages contain format-specific parameters that workers use to configure processing operations:</p>\n<table>\n<thead>\n<tr>\n<th>Output Spec Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Format Applicability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>output_path</code></td>\n<td>String</td>\n<td>Target file path for generated output</td>\n<td>All formats</td>\n</tr>\n<tr>\n<td><code>format</code></td>\n<td>String</td>\n<td>Target format identifier</td>\n<td><code>jpeg</code>, <code>png</code>, <code>webp</code>, <code>mp4</code>, <code>webm</code></td>\n</tr>\n<tr>\n<td><code>width</code></td>\n<td>Integer</td>\n<td>Target width in pixels</td>\n<td>Images and videos</td>\n</tr>\n<tr>\n<td><code>height</code></td>\n<td>Integer</td>\n<td>Target height in pixels</td>\n<td>Images and videos</td>\n</tr>\n<tr>\n<td><code>quality</code></td>\n<td>Integer</td>\n<td>Quality level (1-100)</td>\n<td>Lossy formats only</td>\n</tr>\n<tr>\n<td><code>optimization_level</code></td>\n<td>Integer</td>\n<td>Processing effort vs speed tradeoff</td>\n<td>Format-dependent scale</td>\n</tr>\n<tr>\n<td><code>codec_settings</code></td>\n<td>Object</td>\n<td>Format-specific encoding parameters</td>\n<td>Videos: bitrate, CRF, preset</td>\n</tr>\n<tr>\n<td><code>color_profile</code></td>\n<td>String</td>\n<td>ICC color profile to apply</td>\n<td>Images with color management</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision</strong>: Messages include complete processing specifications rather than references to external configuration. This design ensures workers can process jobs independently without additional database lookups, improving reliability and reducing latency. The tradeoff is larger message sizes, but media processing jobs are inherently heavy operations where message overhead is negligible.</p>\n</blockquote>\n<h4 id=\"progress-update-messages\">Progress Update Messages</h4>\n<p>Progress update messages flow from worker processes to the progress tracking system, providing real-time visibility into job execution. These messages support both incremental updates within processing stages and major stage transitions.</p>\n<table>\n<thead>\n<tr>\n<th>Progress Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Update Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job_id</code></td>\n<td>String</td>\n<td>Job identifier for correlation</td>\n<td>Every message</td>\n</tr>\n<tr>\n<td><code>message_type</code></td>\n<td>String</td>\n<td>Update type discriminator</td>\n<td><code>STAGE_PROGRESS</code>, <code>STAGE_COMPLETE</code>, <code>JOB_COMPLETE</code></td>\n</tr>\n<tr>\n<td><code>overall_percentage</code></td>\n<td>Float</td>\n<td>Total job completion percentage (0-100)</td>\n<td>Every update</td>\n</tr>\n<tr>\n<td><code>current_stage</code></td>\n<td>String</td>\n<td>Processing stage name</td>\n<td>Changes on stage transitions</td>\n</tr>\n<tr>\n<td><code>stage_percentage</code></td>\n<td>Float</td>\n<td>Completion within current stage (0-100)</td>\n<td>Frequent updates during processing</td>\n</tr>\n<tr>\n<td><code>stage_details</code></td>\n<td>Object</td>\n<td>Stage-specific progress information</td>\n<td>Contains operation-specific data</td>\n</tr>\n<tr>\n<td><code>estimated_remaining</code></td>\n<td>Integer</td>\n<td>Estimated seconds to completion</td>\n<td>Recalculated on each update</td>\n</tr>\n<tr>\n<td><code>worker_id</code></td>\n<td>String</td>\n<td>Identifier of processing worker</td>\n<td>For debugging and load analysis</td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td>Integer</td>\n<td>Unix timestamp of progress measurement</td>\n<td>Enables progress rate calculation</td>\n</tr>\n<tr>\n<td><code>sequence_number</code></td>\n<td>Integer</td>\n<td>Monotonically increasing sequence</td>\n<td>Prevents out-of-order updates</td>\n</tr>\n<tr>\n<td><code>resource_usage</code></td>\n<td>Object</td>\n<td>Current memory and CPU utilization</td>\n<td>Optional, for monitoring</td>\n</tr>\n</tbody></table>\n<p>The <code>stage_details</code> object provides operation-specific progress information that varies by processing type:</p>\n<table>\n<thead>\n<tr>\n<th>Stage Detail Field</th>\n<th>Type</th>\n<th>Image Processing</th>\n<th>Video Transcoding</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>current_operation</code></td>\n<td>String</td>\n<td><code>resize</code>, <code>format_convert</code></td>\n<td><code>encode_video</code>, <code>extract_audio</code></td>\n</tr>\n<tr>\n<td><code>processed_items</code></td>\n<td>Integer</td>\n<td>Number of output variants completed</td>\n<td>Number of segments processed</td>\n</tr>\n<tr>\n<td><code>total_items</code></td>\n<td>Integer</td>\n<td>Total output variants requested</td>\n<td>Total segments in video</td>\n</tr>\n<tr>\n<td><code>current_resolution</code></td>\n<td>String</td>\n<td><code>1920x1080</code></td>\n<td><code>1280x720</code></td>\n</tr>\n<tr>\n<td><code>bitrate_kbps</code></td>\n<td>Integer</td>\n<td>N/A (images)</td>\n<td>Current encoding bitrate</td>\n</tr>\n<tr>\n<td><code>fps_processed</code></td>\n<td>Float</td>\n<td>N/A (images)</td>\n<td>Frames processed per second</td>\n</tr>\n<tr>\n<td><code>temp_file_size</code></td>\n<td>Integer</td>\n<td>Intermediate file size in bytes</td>\n<td>Current output file size</td>\n</tr>\n</tbody></table>\n<h4 id=\"error-and-notification-messages\">Error and Notification Messages</h4>\n<p>Error messages provide detailed failure information for debugging and recovery decisions. The message format distinguishes between transient errors (suitable for retry) and permanent failures (requiring human intervention or job cancellation).</p>\n<table>\n<thead>\n<tr>\n<th>Error Message Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Recovery Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job_id</code></td>\n<td>String</td>\n<td>Failed job identifier</td>\n<td>Links error to specific job</td>\n</tr>\n<tr>\n<td><code>error_type</code></td>\n<td>String</td>\n<td>Error category</td>\n<td><code>INPUT_ERROR</code>, <code>PROCESSING_ERROR</code>, <code>RESOURCE_ERROR</code></td>\n</tr>\n<tr>\n<td><code>error_code</code></td>\n<td>String</td>\n<td>Specific error identifier</td>\n<td><code>UNSUPPORTED_CODEC</code>, <code>MEMORY_LIMIT_EXCEEDED</code></td>\n</tr>\n<tr>\n<td><code>error_message</code></td>\n<td>String</td>\n<td>Human-readable error description</td>\n<td>Displayed to end users</td>\n</tr>\n<tr>\n<td><code>technical_details</code></td>\n<td>String</td>\n<td>Detailed technical information</td>\n<td>For debugging and troubleshooting</td>\n</tr>\n<tr>\n<td><code>retry_eligible</code></td>\n<td>Boolean</td>\n<td>Whether error is potentially transient</td>\n<td>Controls automatic retry behavior</td>\n</tr>\n<tr>\n<td><code>failed_stage</code></td>\n<td>String</td>\n<td>Processing stage where error occurred</td>\n<td>Helps isolate problem area</td>\n</tr>\n<tr>\n<td><code>worker_id</code></td>\n<td>String</td>\n<td>Worker that encountered the error</td>\n<td>For worker health monitoring</td>\n</tr>\n<tr>\n<td><code>input_file_info</code></td>\n<td>Object</td>\n<td>Information about input file</td>\n<td>Helps identify problematic inputs</td>\n</tr>\n<tr>\n<td><code>resource_state</code></td>\n<td>Object</td>\n<td>System resource state at failure</td>\n<td>Memory, disk space, CPU load</td>\n</tr>\n<tr>\n<td><code>stack_trace</code></td>\n<td>String</td>\n<td>Exception stack trace if available</td>\n<td>Development and debugging</td>\n</tr>\n<tr>\n<td><code>suggested_action</code></td>\n<td>String</td>\n<td>Recommended resolution steps</td>\n<td>Guides manual intervention</td>\n</tr>\n</tbody></table>\n<p><strong>Decision: JSON Message Serialization</strong></p>\n<ul>\n<li><strong>Context</strong>: Need serialization format for inter-component communication supporting multiple languages</li>\n<li><strong>Options Considered</strong>: JSON, Protocol Buffers, MessagePack, Apache Avro</li>\n<li><strong>Decision</strong>: JSON with schema validation</li>\n<li><strong>Rationale</strong>: JSON provides human readability for debugging, universal language support, and flexible schema evolution. Media processing messages are relatively infrequent compared to processing time, so serialization performance is not critical. Schema validation catches format errors early.</li>\n<li><strong>Consequences</strong>: Enables easy debugging and monitoring, but larger message sizes than binary formats. Schema evolution requires careful compatibility management.</li>\n</ul>\n<h3 id=\"processing-sequence-scenarios\">Processing Sequence Scenarios</h3>\n<p>The media processing pipeline orchestrates complex sequences of operations across multiple components, with different workflows for image processing, video transcoding, and batch operations. These scenarios demonstrate how components interact and coordinate through the message passing and state management systems.</p>\n<h4 id=\"image-processing-workflow-sequence\">Image Processing Workflow Sequence</h4>\n<p>Image processing jobs follow a predictable sequence from submission through completion, with progress tracking and error handling at each stage. This workflow optimizes for quality while maintaining reasonable processing times for typical web and mobile image requirements.</p>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fimage-processing-flow.svg\" alt=\"Image Processing Workflow\"></p>\n<p><strong>Stage 1: Job Submission and Validation (5% of total progress)</strong></p>\n<ol>\n<li><strong>Client submits</strong> image processing request to API Gateway with input file and output specifications</li>\n<li><strong>API Gateway validates</strong> file format, size limits, and output specification parameters</li>\n<li><strong>Job identifier generation</strong> creates unique job ID using timestamp and random suffix format</li>\n<li><strong>Input file storage</strong> moves uploaded file to processing storage location with secure naming</li>\n<li><strong>Job record creation</strong> initializes <code>ProcessingJob</code> with status <code>PENDING</code> in Redis and PostgreSQL</li>\n<li><strong>Queue submission</strong> publishes job message to high-priority image processing queue</li>\n<li><strong>Response return</strong> sends job ID and initial status to client immediately</li>\n<li><strong>Progress initialization</strong> sets up progress tracking with stage weights and webhook configuration</li>\n</ol>\n<p><strong>Stage 2: Worker Assignment and Preprocessing (10% of total progress)</strong></p>\n<ol>\n<li><strong>Worker selection</strong> occurs when available image processing worker pops job from queue</li>\n<li><strong>Resource reservation</strong> worker claims memory and CPU resources based on job requirements</li>\n<li><strong>Input validation</strong> worker verifies file accessibility, format support, and corruption checks</li>\n<li><strong>Metadata extraction</strong> reads EXIF data, color profiles, dimensions, and technical specifications</li>\n<li><strong>Processing plan</strong> creation determines optimal resize algorithms, format conversion steps, and quality settings</li>\n<li><strong>Progress notification</strong> updates job status to <code>PROCESSING</code> and notifies webhook if configured</li>\n<li><strong>Temporary workspace</strong> allocation creates isolated directory for intermediate processing files</li>\n</ol>\n<p><strong>Stage 3: Resize Operations (40% of total progress)</strong></p>\n<ol>\n<li><strong>Image loading</strong> into memory with automatic EXIF orientation correction and color space handling</li>\n<li><strong>Memory optimization</strong> calculates processing chunks if image exceeds available memory limits</li>\n<li><strong>Interpolation algorithm</strong> selection based on resize ratio (upscaling vs downscaling requirements)</li>\n<li><strong>Resize execution</strong> for each output specification with aspect ratio preservation or cropping</li>\n<li><strong>Quality assessment</strong> validates output dimensions and visual quality against requirements</li>\n<li><strong>Progress updates</strong> sent incrementally as each resize variant completes processing</li>\n<li><strong>Intermediate storage</strong> saves resized images to temporary files before format conversion</li>\n</ol>\n<p><strong>Stage 4: Format Conversion and Optimization (35% of total progress)</strong></p>\n<ol>\n<li><strong>Format compatibility</strong> checking ensures target formats support required features (transparency, color depth)</li>\n<li><strong>Compression optimization</strong> applies format-specific encoding with quality vs file size balancing</li>\n<li><strong>Color profile</strong> management preserves or converts ICC profiles based on target format capabilities</li>\n<li><strong>Metadata handling</strong> strips or preserves EXIF data according to privacy and compatibility requirements</li>\n<li><strong>Quality validation</strong> verifies output files meet specified quality levels and size constraints</li>\n<li><strong>Progressive encoding</strong> setup for JPEG files to enable progressive download capabilities</li>\n<li><strong>Format-specific optimization</strong> applies WebP/AVIF advanced features or PNG palette optimization</li>\n</ol>\n<p><strong>Stage 5: Finalization and Cleanup (10% of total progress)</strong></p>\n<ol>\n<li><strong>Output validation</strong> confirms all requested variants were generated successfully with correct specifications</li>\n<li><strong>File verification</strong> performs integrity checks and validates that files can be opened by standard readers</li>\n<li><strong>Secure storage</strong> moves completed files to final output location with proper permissions and access controls</li>\n<li><strong>Database updates</strong> marks job as <code>COMPLETED</code> with output file paths and processing statistics</li>\n<li><strong>Progress completion</strong> sets overall percentage to 100% and sends final webhook notification</li>\n<li><strong>Resource cleanup</strong> removes temporary files, releases memory, and frees worker for next job</li>\n<li><strong>Metrics recording</strong> logs processing duration, resource usage, and quality metrics for monitoring</li>\n</ol>\n<blockquote>\n<p><strong>Critical Timing Consideration</strong>: Image processing typically completes within 5-30 seconds depending on input size and output variants. Progress updates occur every 2-3 seconds during active processing stages, balancing real-time visibility with system overhead. Workers batch multiple small resize operations to minimize progress update frequency.</p>\n</blockquote>\n<h4 id=\"video-transcoding-workflow-sequence\">Video Transcoding Workflow Sequence</h4>\n<p>Video transcoding represents the most complex and resource-intensive processing scenario, requiring careful coordination between FFmpeg processes, adaptive bitrate variant generation, and long-running progress tracking. This workflow can span minutes to hours depending on video duration and output requirements.</p>\n<p><strong>Stage 1: Video Analysis and Planning (5% of total progress)</strong></p>\n<ol>\n<li><strong>Input validation</strong> verifies video file accessibility, format support, and basic integrity through FFmpeg probe</li>\n<li><strong>Metadata extraction</strong> analyzes video streams, audio tracks, subtitle tracks, and container properties</li>\n<li><strong>Codec compatibility</strong> assessment determines optimal transcoding paths and identifies unsupported features</li>\n<li><strong>Resource estimation</strong> calculates memory requirements, processing time, and temporary storage needs</li>\n<li><strong>Quality ladder</strong> generation creates adaptive bitrate variant specifications based on input resolution</li>\n<li><strong>Processing strategy</strong> selection chooses single-pass vs two-pass encoding based on quality requirements</li>\n<li><strong>Worker assignment</strong> reserves video processing worker with sufficient resources for estimated job duration</li>\n</ol>\n<p><strong>Stage 2: Audio Stream Processing (15% of total progress)</strong></p>\n<ol>\n<li><strong>Audio stream</strong> extraction from input video using FFmpeg with format preservation or conversion</li>\n<li><strong>Audio normalization</strong> applies volume leveling and dynamic range compression for consistent playback</li>\n<li><strong>Multi-bitrate</strong> audio encoding generates variants at different quality levels for adaptive streaming</li>\n<li><strong>Audio sync</strong> verification ensures audio timing alignment is maintained throughout processing</li>\n<li><strong>Codec optimization</strong> applies audio codec-specific settings for bandwidth efficiency and compatibility</li>\n<li><strong>Quality validation</strong> verifies audio output meets bitrate and quality specifications through automated analysis</li>\n</ol>\n<p><strong>Stage 3: Video Stream Transcoding (65% of total progress)</strong></p>\n<ol>\n<li><strong>Keyframe analysis</strong> identifies scene changes and optimal keyframe placement for streaming compatibility</li>\n<li><strong>Encoding initialization</strong> starts FFmpeg processes for each required video quality variant simultaneously</li>\n<li><strong>Progress parsing</strong> extracts completion percentage from FFmpeg output streams using regex pattern matching</li>\n<li><strong>Frame processing</strong> monitoring tracks encoding speed and adjusts resource allocation dynamically</li>\n<li><strong>Quality control</strong> sampling periodically validates output quality against CRF settings and bitrate targets</li>\n<li><strong>Segment generation</strong> for HLS/DASH creates properly aligned segments across all quality variants</li>\n<li><strong>Parallel processing</strong> coordination ensures keyframe alignment across multiple quality streams</li>\n</ol>\n<p><strong>Stage 4: Manifest Generation and Packaging (10% of total progress)</strong></p>\n<ol>\n<li><strong>Stream analysis</strong> examines completed video and audio variants to extract technical specifications</li>\n<li><strong>HLS manifest</strong> creation generates M3U8 playlists with proper segment references and bandwidth declarations</li>\n<li><strong>DASH manifest</strong> generation creates MPD files with adaptation sets and representation metadata</li>\n<li><strong>Segment validation</strong> verifies all media segments are properly formatted and accessible</li>\n<li><strong>Bandwidth testing</strong> confirms actual bitrates match declared values in manifests</li>\n<li><strong>Compatibility verification</strong> ensures manifests work with standard players and CDN requirements</li>\n</ol>\n<p><strong>Stage 5: Thumbnail Extraction and Cleanup (5% of total progress)</strong></p>\n<ol>\n<li><strong>Thumbnail timestamps</strong> calculation identifies optimal frames representing video content variety</li>\n<li><strong>Frame extraction</strong> uses FFmpeg to generate high-quality still images at specified time offsets</li>\n<li><strong>Thumbnail optimization</strong> applies image processing techniques for consistent sizing and quality</li>\n<li><strong>Output verification</strong> confirms all transcoded variants, manifests, and thumbnails are accessible</li>\n<li><strong>Cleanup operations</strong> remove temporary files while preserving final outputs in secure storage</li>\n<li><strong>Completion notification</strong> sends webhook with comprehensive job results and download URLs</li>\n</ol>\n<blockquote>\n<p><strong>Resource Management</strong>: Video transcoding can consume 2-8 GB RAM and utilize multiple CPU cores for hours. Workers implement memory monitoring and can pause/resume processing if system resources become constrained. Progress updates occur every 10-15 seconds due to the longer processing duration.</p>\n</blockquote>\n<h4 id=\"batch-processing-coordination-scenario\">Batch Processing Coordination Scenario</h4>\n<p>Batch processing handles multiple related media files as a single logical operation, requiring coordination across multiple workers while maintaining consistent progress reporting and error handling. This scenario is common for photo album processing or video playlist transcoding.</p>\n<p><strong>Stage 1: Batch Job Decomposition</strong></p>\n<ol>\n<li><strong>Batch validation</strong> confirms all input files are accessible and supported formats</li>\n<li><strong>Dependency analysis</strong> identifies files that can be processed in parallel vs those requiring sequential processing  </li>\n<li><strong>Resource planning</strong> estimates total processing time and memory requirements across all batch items</li>\n<li><strong>Work distribution</strong> creates individual processing jobs for each file while maintaining batch coordination</li>\n<li><strong>Progress aggregation</strong> setup configures batch-level progress calculation from individual job progress</li>\n<li><strong>Error handling</strong> strategy defines batch behavior when individual items fail (fail-fast vs continue)</li>\n</ol>\n<p><strong>Stage 2: Parallel Processing Coordination</strong></p>\n<ol>\n<li><strong>Worker allocation</strong> distributes batch items across available workers based on file types and resource requirements</li>\n<li><strong>Progress consolidation</strong> aggregates progress from individual jobs into overall batch completion percentage</li>\n<li><strong>Error monitoring</strong> tracks failed items and makes retry decisions based on batch error tolerance policy</li>\n<li><strong>Resource balancing</strong> dynamically reassigns work if some workers complete tasks significantly faster than others</li>\n<li><strong>Checkpoint creation</strong> periodically saves batch progress state to enable recovery from system failures</li>\n<li><strong>Completion detection</strong> recognizes when all batch items reach terminal states (completed or permanently failed)</li>\n</ol>\n<p><strong>Stage 3: Batch Finalization</strong></p>\n<ol>\n<li><strong>Results aggregation</strong> collects output files from all successful processing jobs into batch results</li>\n<li><strong>Error reporting</strong> compiles detailed failure information for any items that could not be processed</li>\n<li><strong>Consistency verification</strong> ensures all related outputs (thumbnails, variants) are properly linked</li>\n<li><strong>Batch manifest</strong> generation creates index files linking all generated outputs for easy client consumption</li>\n<li><strong>Cleanup coordination</strong> removes temporary files from all workers that participated in batch processing</li>\n<li><strong>Final notification</strong> delivers comprehensive batch results including partial success scenarios</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Race Conditions in Progress Aggregation</strong>\nBatch processing can create race conditions when multiple workers update progress simultaneously. Without proper synchronization, progress percentages may appear to move backwards or skip values. Implement sequence numbers in progress updates and use atomic Redis operations for progress consolidation. Each individual job maintains its own progress sequence, and the batch coordinator uses the latest sequence number from each job to calculate accurate batch progress.</p>\n<p>⚠️ <strong>Pitfall: Memory Exhaustion During Video Processing</strong>\nVideo transcoding jobs can easily consume all available system memory, especially when processing multiple large videos simultaneously. Workers must implement memory monitoring and refuse new jobs when memory usage exceeds safe thresholds (typically 80% of available RAM). Use FFmpeg memory limits and temporary file streaming to prevent out-of-memory crashes that corrupt partially processed outputs.</p>\n<p>⚠️ <strong>Pitfall: Webhook Delivery Failures</strong>\nWebhook notifications can fail due to network issues, endpoint unavailability, or client-side errors. Without proper retry logic, clients lose visibility into job progress and completion. Implement exponential backoff for webhook retries (1s, 2s, 4s, 8s intervals) with circuit breaker patterns. Store failed webhooks in a dead letter queue for manual review, and provide alternative mechanisms like polling for clients with unreliable webhook endpoints.</p>\n<p>⚠️ <strong>Pitfall: Progress Estimation Inaccuracy</strong>\nVideo processing progress is notoriously difficult to estimate accurately because encoding complexity varies dramatically based on video content (static scenes encode faster than high-motion scenes). Avoid time-based progress estimates and instead use stage-based progress with frame counts when possible. For FFmpeg integration, parse the total frame count from initial analysis and calculate progress based on frames processed rather than elapsed time.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>API Framework</td>\n<td>Flask with Flask-RESTful</td>\n<td>FastAPI with async/await</td>\n</tr>\n<tr>\n<td>Message Queue</td>\n<td>Redis with basic pub/sub</td>\n<td>Redis with Celery task queue</td>\n</tr>\n<tr>\n<td>HTTP Client</td>\n<td>requests library</td>\n<td>aiohttp for async requests</td>\n</tr>\n<tr>\n<td>JSON Validation</td>\n<td>jsonschema library</td>\n<td>Pydantic models with validation</td>\n</tr>\n<tr>\n<td>File Upload</td>\n<td>werkzeug FileStorage</td>\n<td>streaming upload with progress</td>\n</tr>\n<tr>\n<td>Authentication</td>\n<td>API key headers</td>\n<td>JWT tokens with scope validation</td>\n</tr>\n<tr>\n<td>Rate Limiting</td>\n<td>Flask-Limiter</td>\n<td>Redis-based sliding window</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Basic logging</td>\n<td>Prometheus metrics + Grafana</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media-processor/\n  api/\n    __init__.py\n    routes.py              ← REST endpoint definitions\n    models.py              ← Request/response models\n    validation.py          ← Input validation logic\n    middleware.py          ← Authentication, rate limiting\n  messaging/\n    __init__.py\n    queue_manager.py       ← Job queue operations\n    message_formats.py     ← Message serialization\n    progress_notifier.py   ← Webhook delivery system\n  workers/\n    __init__.py\n    base_worker.py         ← Common worker functionality\n    image_worker.py        ← Image processing worker\n    video_worker.py        ← Video transcoding worker\n  storage/\n    __init__.py\n    file_manager.py        ← File I/O and cleanup\n    metadata_store.py      ← Job status persistence\n  config/\n    __init__.py\n    settings.py            ← Configuration management\n    logging.py             ← Logging setup\n  tests/\n    api/                   ← API endpoint tests\n    workers/               ← Worker process tests\n    integration/           ← End-to-end tests\n  docker/\n    api.Dockerfile         ← API server container\n    worker.Dockerfile      ← Worker process container\n  requirements.txt\n  main.py                  ← Application entry point</code></pre></div>\n\n<h4 id=\"api-server-infrastructure-code\">API Server Infrastructure Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># api/routes.py - Complete REST API implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> flask </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Flask, request, jsonify, send_file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> werkzeug.utils </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> secure_filename</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> messaging.queue_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> storage.metadata_store </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MetadataStore</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> config.settings </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> AppConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">app </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Flask(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> AppConfig()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">queue_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QueueManager(config.redis)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">metadata_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MetadataStore(config.storage)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/jobs'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'POST'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> submit_job</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Submit new media processing job with file upload and specifications.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate multipart form data contains required fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check file size against MAX_FILE_SIZE limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate file format using magic number detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Parse and validate output_specifications JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate unique job_id using generate_job_id()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store uploaded file in secure storage location</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Create ProcessingJob object with validated parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Submit job to appropriate queue based on media type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Initialize progress tracking for the new job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return job details with 201 Created status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/jobs/&#x3C;job_id>'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'GET'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_job_status</span><span style=\"color:#E1E4E8\">(job_id):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Retrieve current job status and details.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate job_id format and existence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Load ProcessingJob from metadata store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Get current progress from progress tracker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Format response with job details and progress</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return 404 if job not found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/jobs/&#x3C;job_id>/progress'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'GET'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_job_progress</span><span style=\"color:#E1E4E8\">(job_id):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Get detailed progress information for active job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Verify job exists and is in processing state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Retrieve JobProgress from progress tracking system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate stage-specific progress details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Include estimated remaining time if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Format response with progress breakdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/jobs/&#x3C;job_id>'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'DELETE'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> cancel_job</span><span style=\"color:#E1E4E8\">(job_id):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Cancel pending or processing job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load job and verify it can be cancelled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Send cancellation message to worker if processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update job status to cancelled in metadata store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Clean up any temporary files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return confirmation or error if cannot cancel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.errorhandler</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">413</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> file_too_large</span><span style=\"color:#E1E4E8\">(error):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handle file upload size limit exceeded.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> jsonify({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'type'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'/errors/file-too-large'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'title'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'File Too Large'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'status'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">413</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'detail'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'File size exceeds maximum limit of </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">config.storage.max_file_size</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> bytes'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }), </span><span style=\"color:#79B8FF\">413</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@app.errorhandler</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">422</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validation_error</span><span style=\"color:#E1E4E8\">(error):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handle request validation failures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> jsonify({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'type'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'/errors/validation-failed'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'title'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'Validation Error'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'status'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">422</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'detail'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(error),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'timestamp'</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }), </span><span style=\"color:#79B8FF\">422</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># messaging/message_formats.py - Message serialization utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MessageType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROCESS_MEDIA</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"process_media\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROGRESS_UPDATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"progress_update\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JOB_COMPLETE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"job_complete\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JOB_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"job_failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CANCEL_JOB</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cancel_job\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobMessage</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete job message format for queue communication.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message_type: MessageType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_file_path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_specifications: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    processing_config: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhook_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata_options: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_policy: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_constraints: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlation_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_at: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_json</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize message to JSON string for queue storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Convert dataclass to dictionary with enum handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Format timestamps as Unix timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle None values appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return JSON string with consistent formatting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_json</span><span style=\"color:#E1E4E8\">(cls, json_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'JobMessage'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deserialize message from JSON string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse JSON string to dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert string enums back to enum objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle timestamp conversion from Unix format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create and return JobMessage instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProgressMessage</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Progress update message format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message_type: MessageType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    overall_percentage: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_stage: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage_percentage: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage_details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_remaining: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    worker_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sequence_number: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_usage: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_json</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize progress message to JSON.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement similar to JobMessage.to_json()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_json</span><span style=\"color:#E1E4E8\">(cls, json_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'ProgressMessage'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deserialize progress message from JSON.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement similar to JobMessage.from_json()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"queue-management-infrastructure-code\">Queue Management Infrastructure Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># messaging/queue_manager.py - Redis-based job queue implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> config.settings </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RedisConfig</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> messaging.message_formats </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobMessage, MessageType</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueueManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages job submission and worker coordination through Redis queues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_config: RedisConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            host</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">redis_config.host,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            port</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">redis_config.port,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            db</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">redis_config.db,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            password</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">redis_config.password,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.job_queue_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"media_jobs\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.progress_channel </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"job_progress\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.worker_heartbeat_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"worker_heartbeats\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> submit_job</span><span style=\"color:#E1E4E8\">(self, job_message: JobMessage) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Submit job to priority queue with deduplication check.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check for duplicate job_id in active jobs set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Serialize job_message to JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add job to priority queue using ZADD with priority score</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add job_id to active jobs set for deduplication</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set job TTL based on timeout_at value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return True if successful, False if duplicate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_next_job</span><span style=\"color:#E1E4E8\">(self, worker_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">) -> Optional[JobMessage]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Blocking pop highest priority job from queue.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use BZPOPMAX to get highest priority job with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If job received, parse JSON to JobMessage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Register worker assignment in Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update worker heartbeat timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return JobMessage or None if timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mark_job_completed</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, worker_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark job as completed and clean up queue state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Remove job from active jobs set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Remove worker assignment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update completion statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Clean up any job-specific temporary data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return success status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> publish_progress</span><span style=\"color:#E1E4E8\">(self, progress_message) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Publish progress update to subscribers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Serialize progress_message to JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Publish to progress channel using PUBLISH</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store latest progress in Redis for polling clients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return publication success status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_queue_stats</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current queue statistics for monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Count jobs in priority queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Count active worker assignments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Get average job processing time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return comprehensive statistics dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-processing-logic-skeleton\">Core Processing Logic Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># workers/base_worker.py - Common worker functionality</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> signal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> messaging.queue_manager </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> QueueManager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> messaging.message_formats </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobMessage, ProgressMessage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> storage.metadata_store </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MetadataStore</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> BaseWorker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for all media processing workers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, worker_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, queue_manager: QueueManager, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 metadata_store: MetadataStore):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.worker_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> worker_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.queue_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue_manager</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metadata_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.shutdown_requested </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.current_job </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"worker.</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">worker_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main worker loop - gets jobs and processes them.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set up signal handlers for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start heartbeat thread for worker health monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Enter main processing loop until shutdown requested</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Get next job from queue with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Process job using subclass implementation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle job completion or failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Clean up resources and update worker status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_progress</span><span style=\"color:#E1E4E8\">(self, percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       stage_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Send progress update for current job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate progress values (0-100, stage exists)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create ProgressMessage with current job details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Include worker resource usage information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Publish progress message to queue manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Log progress update for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> process_job</span><span style=\"color:#E1E4E8\">(self, job: JobMessage) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Process specific job type - implemented by subclasses.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Subclasses implement specific processing logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_job_error</span><span style=\"color:#E1E4E8\">(self, job: JobMessage, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Handle job processing errors with retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine if error is transient or permanent</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check retry count against policy limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply exponential backoff for transient errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Mark job as failed if retry limit exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Clean up any partial processing results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return True if job should be retried</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After API Implementation:</strong></p>\n<ul>\n<li>Run <code>python -m pytest tests/api/</code> - all endpoint tests should pass</li>\n<li>Start API server: <code>python main.py</code></li>\n<li>Test job submission: <code>curl -X POST -F &quot;file=@test.jpg&quot; -F &quot;output_specs=[{\\&quot;format\\&quot;:\\&quot;webp\\&quot;,\\&quot;width\\&quot;:800}]&quot; http://localhost:5000/jobs</code></li>\n<li>Verify response contains job_id and status &quot;pending&quot;</li>\n<li>Test status endpoint: <code>curl http://localhost:5000/jobs/{job_id}</code></li>\n</ul>\n<p><strong>After Queue Implementation:</strong></p>\n<ul>\n<li>Run Redis server: <code>redis-server</code></li>\n<li>Test queue operations: <code>python -c &quot;from messaging.queue_manager import QueueManager; q=QueueManager(); print(&#39;Queue connected&#39;)&quot;</code></li>\n<li>Submit test job and verify it appears in Redis: <code>redis-cli ZRANGE media_jobs 0 -1 WITHSCORES</code></li>\n<li>Check job message format: <code>redis-cli ZRANGE media_jobs 0 0</code> should show valid JSON</li>\n</ul>\n<p><strong>After Worker Implementation:</strong></p>\n<ul>\n<li>Start worker process: <code>python -m workers.image_worker</code></li>\n<li>Worker should connect to queue and wait for jobs</li>\n<li>Submit job through API - worker should pick it up and process</li>\n<li>Check progress updates in Redis: <code>redis-cli SUBSCRIBE job_progress</code></li>\n<li>Verify completed job produces output files in storage directory</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Jobs stuck in pending</td>\n<td>Worker not running or crashed</td>\n<td>Check worker process status, Redis connection</td>\n<td>Restart workers, verify Redis connectivity</td>\n</tr>\n<tr>\n<td>Progress updates missing</td>\n<td>Worker not publishing or Redis pub/sub issue</td>\n<td>Monitor Redis PUBSUB channels, check worker logs</td>\n<td>Verify Redis PUBLISH permissions, restart progress tracking</td>\n</tr>\n<tr>\n<td>File upload fails</td>\n<td>Storage permissions or disk space</td>\n<td>Check file system permissions, disk usage</td>\n<td>Fix directory permissions, free up storage space</td>\n</tr>\n<tr>\n<td>Memory errors during processing</td>\n<td>Large files exceeding worker memory limits</td>\n<td>Monitor worker memory usage during processing</td>\n<td>Implement streaming processing, increase worker memory</td>\n</tr>\n<tr>\n<td>Webhook delivery fails</td>\n<td>Network issues or client endpoint problems</td>\n<td>Check webhook endpoint availability, review retry logs</td>\n<td>Implement circuit breaker, verify endpoint certificates</td>\n</tr>\n<tr>\n<td>Race conditions in progress</td>\n<td>Multiple workers updating same job</td>\n<td>Check for duplicate job assignments in Redis</td>\n<td>Implement atomic job assignment with Redis locks</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as robust error handling underlies image processing, video transcoding, and job queue reliability across the entire system</p>\n</blockquote>\n<h3 id=\"mental-model-hospital-emergency-response-system\">Mental Model: Hospital Emergency Response System</h3>\n<p>Think of error handling in our media processing pipeline like a hospital&#39;s emergency response system. Just as a hospital has protocols for different types of medical emergencies — cardiac arrest, trauma, poisoning — each requiring different response procedures, our media processing system must handle different types of failures with appropriate recovery strategies.</p>\n<p>In a hospital, triage nurses quickly categorize incoming patients by severity: critical (immediate attention), urgent (can wait briefly), less urgent (stable for now). Similarly, our system must rapidly classify errors as transient (retry immediately), resource-constrained (retry with backoff), or permanent failures (move to dead letter queue). The hospital maintains detailed logs of every emergency response to improve future care, just as our system logs every failure and recovery attempt to enable debugging and system improvement.</p>\n<p>The hospital&#39;s emergency protocols ensure that even when multiple crises occur simultaneously, resources are allocated appropriately, staff coordinate effectively, and no patient is forgotten. Our error handling system provides the same guarantee: even when multiple jobs fail across different worker processes, failures are detected quickly, resources are cleaned up properly, and no job is lost or left in an inconsistent state.</p>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fjob-state-machine.svg\" alt=\"Job Lifecycle State Machine\"></p>\n<h2 id=\"failure-modes-and-detection\">Failure Modes and Detection</h2>\n<p>Comprehensive error detection requires understanding the different categories of failures that can occur in a distributed media processing system. Each failure mode has distinct characteristics, detection methods, and recovery strategies that must be implemented systematically.</p>\n<h3 id=\"mental-model-weather-monitoring-system\">Mental Model: Weather Monitoring System</h3>\n<p>Think of failure detection like a weather monitoring system with multiple sensors detecting different types of atmospheric disturbances. Just as meteorologists use temperature sensors, pressure gauges, humidity detectors, and wind speed meters to identify incoming storms, our system deploys multiple detection mechanisms to identify different failure patterns before they cascade into larger system outages.</p>\n<p>Weather systems provide early warning alerts with different severity levels — watches, warnings, and emergencies — each triggering different response protocols. Our failure detection system similarly provides graduated alert levels that trigger increasingly aggressive recovery measures as failure severity escalates.</p>\n<h3 id=\"system-level-failure-categories\">System-Level Failure Categories</h3>\n<p>The media processing pipeline encounters failures across multiple system layers, each requiring specialized detection and handling approaches.</p>\n<p><strong>Infrastructure Failures</strong> represent the foundational layer of potential system disruption. Redis connection failures manifest when the job queue becomes unreachable due to network partitions, Redis server crashes, or configuration errors. These failures are detected through connection timeout exceptions and failed ping operations during health checks. Storage system failures occur when temporary file writes fail due to disk space exhaustion, permission errors, or filesystem corruption. Worker process crashes happen when FFmpeg consumes excessive memory, encounters segmentation faults, or is terminated by the operating system&#39;s out-of-memory killer.</p>\n<p><strong>Resource Constraint Failures</strong> emerge when system resources become insufficient to process jobs effectively. Memory exhaustion occurs when large video files or high-resolution images exceed available RAM during processing operations. CPU throttling manifests as dramatically increased processing times when system load exceeds capacity. Disk space exhaustion prevents temporary file creation and output file writing. Network bandwidth limitations cause timeout failures when downloading input files or uploading processed results to external storage systems.</p>\n<p><strong>Application Logic Failures</strong> stem from processing-specific errors and business rule violations. Invalid input files include corrupted media files, unsupported formats, or files that claim to be one format but contain different data. Configuration errors occur when output specifications request impossible operations like upscaling to extreme resolutions or using incompatible codec combinations. Processing timeout failures happen when jobs exceed configured time limits due to unexpectedly large input files or complex processing requirements.</p>\n<p><strong>External Dependency Failures</strong> involve third-party systems and services that the processing pipeline relies upon. Webhook delivery failures occur when recipient endpoints are unreachable, return error status codes, or fail to respond within timeout windows. Cloud storage API failures prevent input file downloads or processed output uploads. FFmpeg execution failures happen when the underlying binary encounters unsupported codec combinations, corrupted input streams, or internal processing errors.</p>\n<h3 id=\"failure-detection-mechanisms\">Failure Detection Mechanisms</h3>\n<p>Early and accurate failure detection requires implementing multiple monitoring layers that work together to provide comprehensive system visibility.</p>\n<table>\n<thead>\n<tr>\n<th>Detection Method</th>\n<th>Monitors</th>\n<th>Detection Time</th>\n<th>Granularity</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Health Check Probes</td>\n<td>Service availability</td>\n<td>30-60 seconds</td>\n<td>Component level</td>\n<td>Redis ping, filesystem write test</td>\n</tr>\n<tr>\n<td>Process Monitoring</td>\n<td>Worker process status</td>\n<td>5-10 seconds</td>\n<td>Process level</td>\n<td>PID monitoring, heartbeat signals</td>\n</tr>\n<tr>\n<td>Resource Monitoring</td>\n<td>CPU, memory, disk usage</td>\n<td>10-30 seconds</td>\n<td>System level</td>\n<td>Resource usage thresholds</td>\n</tr>\n<tr>\n<td>Job Timeout Detection</td>\n<td>Processing duration</td>\n<td>Real-time</td>\n<td>Job level</td>\n<td>Timer-based cancellation</td>\n</tr>\n<tr>\n<td>Progress Stall Detection</td>\n<td>Progress update frequency</td>\n<td>60-120 seconds</td>\n<td>Job level</td>\n<td>Progress timestamp monitoring</td>\n</tr>\n<tr>\n<td>Exception Handling</td>\n<td>Application errors</td>\n<td>Immediate</td>\n<td>Operation level</td>\n<td>Try-catch blocks with logging</td>\n</tr>\n<tr>\n<td>Log Analysis</td>\n<td>Error patterns</td>\n<td>1-5 minutes</td>\n<td>System level</td>\n<td>Log aggregation and pattern matching</td>\n</tr>\n<tr>\n<td>External Service Monitoring</td>\n<td>API response codes</td>\n<td>Real-time</td>\n<td>Request level</td>\n<td>HTTP status code analysis</td>\n</tr>\n</tbody></table>\n<p>The <code>ResourceMonitor</code> component implements proactive resource constraint detection by continuously tracking system utilization and predicting when thresholds will be exceeded.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Warning Threshold</th>\n<th>Critical Threshold</th>\n<th>Detection Method</th>\n<th>Response Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Usage</td>\n<td>80% of available RAM</td>\n<td>90% of available RAM</td>\n<td>Process memory monitoring</td>\n<td>Pause new job acceptance</td>\n</tr>\n<tr>\n<td>Disk Space</td>\n<td>85% of temp directory</td>\n<td>95% of temp directory</td>\n<td>Filesystem usage check</td>\n<td>Clean up old temporary files</td>\n</tr>\n<tr>\n<td>CPU Load</td>\n<td>80% sustained over 5 minutes</td>\n<td>95% sustained over 2 minutes</td>\n<td>Load average monitoring</td>\n<td>Scale down concurrent jobs</td>\n</tr>\n<tr>\n<td>Network I/O</td>\n<td>80% of bandwidth limit</td>\n<td>90% of bandwidth limit</td>\n<td>Network traffic analysis</td>\n<td>Queue bandwidth-heavy jobs</td>\n</tr>\n</tbody></table>\n<p>Progress stall detection identifies jobs that appear to be running but have stopped making meaningful progress. This situation occurs when FFmpeg processes hang, infinite loops occur in image processing algorithms, or worker processes become unresponsive while maintaining their connection to the job queue.</p>\n<p><strong>Progress Stall Detection Algorithm:</strong></p>\n<ol>\n<li>The <code>ProgressTracker</code> maintains a timestamp for each job&#39;s most recent progress update</li>\n<li>A background monitor process scans all active jobs every 60 seconds</li>\n<li>For each job, calculate the time since the last progress update</li>\n<li>If the stall duration exceeds the job type&#39;s expected maximum interval, mark the job as potentially stalled</li>\n<li>Send a health check request to the worker process handling the stalled job</li>\n<li>If the worker fails to respond within 30 seconds, initiate job recovery procedures</li>\n<li>Log the stall event with job context for debugging analysis</li>\n</ol>\n<h3 id=\"error-classification-and-triage\">Error Classification and Triage</h3>\n<p>Once failures are detected, they must be rapidly classified to determine the appropriate recovery strategy. This classification process mirrors medical triage protocols where patients are quickly categorized by severity and treatment urgency.</p>\n<p><strong>Transient Errors</strong> are temporary failures that typically resolve themselves or can be resolved through immediate retry. Network connection timeouts to Redis often succeed on retry due to temporary network congestion. File system busy errors during temporary file operations usually succeed when retried after a brief delay. FFmpeg initialization failures sometimes resolve when retried due to transient resource contention.</p>\n<p><strong>Resource Errors</strong> indicate system resource constraints that require managed backoff and resource cleanup before retry attempts. Memory allocation failures for large images require waiting for other jobs to complete and free memory. Disk space exhaustion requires cleanup of temporary files before retry. CPU throttling requires reducing concurrent job processing to allow system recovery.</p>\n<p><strong>Permanent Errors</strong> represent fundamental problems that will not resolve through retry attempts and require human intervention or job rejection. Corrupted input files will never process successfully regardless of retry count. Invalid codec combinations in video transcoding specifications cannot be resolved automatically. Malformed webhook URLs will always fail delivery attempts.</p>\n<p><strong>Dependency Errors</strong> involve external system failures that may be transient or permanent depending on the root cause. Database connection failures to PostgreSQL may be temporary network issues or permanent service outages. Cloud storage API errors might be temporary rate limiting or permanent authentication problems. Webhook delivery failures could be temporary recipient downtime or permanent endpoint changes.</p>\n<p>The error classification algorithm evaluates multiple factors to determine the appropriate error category:</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Detection Criteria</th>\n<th>Retry Eligible</th>\n<th>Backoff Strategy</th>\n<th>Max Retries</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network Timeout</td>\n<td>Connection timeout exception</td>\n<td>Yes</td>\n<td>Exponential with jitter</td>\n<td>5 attempts</td>\n</tr>\n<tr>\n<td>File I/O Error</td>\n<td>Permission denied, disk full</td>\n<td>Depends on error code</td>\n<td>Linear backoff</td>\n<td>3 attempts</td>\n</tr>\n<tr>\n<td>Memory Error</td>\n<td>Out of memory exception</td>\n<td>Yes</td>\n<td>Resource-aware backoff</td>\n<td>2 attempts</td>\n</tr>\n<tr>\n<td>Format Error</td>\n<td>Invalid file format</td>\n<td>No</td>\n<td>None</td>\n<td>0 attempts</td>\n</tr>\n<tr>\n<td>Configuration Error</td>\n<td>Invalid parameters</td>\n<td>No</td>\n<td>None</td>\n<td>0 attempts</td>\n</tr>\n<tr>\n<td>FFmpeg Error</td>\n<td>Exit code analysis</td>\n<td>Depends on exit code</td>\n<td>Job-specific backoff</td>\n<td>3 attempts</td>\n</tr>\n<tr>\n<td>Webhook Failure</td>\n<td>HTTP status code</td>\n<td>Depends on status code</td>\n<td>Exponential</td>\n<td>7 attempts</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Error Classification Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Failures need rapid classification to determine retry eligibility and recovery strategies without human intervention</li>\n<li><strong>Options Considered</strong>: Rule-based classification, machine learning classification, hybrid approach</li>\n<li><strong>Decision</strong>: Rule-based classification with exit code and exception type analysis</li>\n<li><strong>Rationale</strong>: Rule-based systems provide deterministic behavior, easier debugging, and immediate classification without training data requirements</li>\n<li><strong>Consequences</strong>: Enables automated retry decisions but requires manual rule updates as new error patterns emerge</li>\n</ul>\n</blockquote>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Ferror-recovery-paths.svg\" alt=\"Error Recovery Decision Tree\"></p>\n<h2 id=\"retry-and-backoff-strategies\">Retry and Backoff Strategies</h2>\n<p>Effective retry mechanisms must balance rapid error recovery with system stability, preventing retry storms that can overwhelm already-stressed system components. The retry strategy varies significantly based on error type, system load, and historical failure patterns.</p>\n<h3 id=\"mental-model-air-traffic-control-during-storms\">Mental Model: Air Traffic Control During Storms</h3>\n<p>Think of retry strategies like air traffic control managing flight delays during severe weather. When storms hit an airport, controllers don&#39;t immediately reroute all flights to the same alternate airport — that would overwhelm the backup facility. Instead, they implement graduated delays: some flights wait briefly for weather to clear, others are sent to different airports with spacing to prevent congestion, and some are cancelled entirely when conditions are too dangerous.</p>\n<p>Similarly, our retry system implements intelligent backoff that prevents overwhelming system components during failure scenarios. When Redis becomes unavailable, we don&#39;t immediately retry all queued operations — instead, we implement exponential backoff with jitter so that retry attempts are distributed over time, giving the system opportunity to recover without being bombarded with retry traffic.</p>\n<p>Air traffic controllers also prioritize based on urgency: emergency landings get immediate priority, while routine flights can be delayed longer. Our retry system similarly prioritizes high-priority jobs for earlier retry attempts while allowing lower-priority jobs to wait longer between retries.</p>\n<h3 id=\"exponential-backoff-implementation\">Exponential Backoff Implementation</h3>\n<p>Exponential backoff prevents retry storms by progressively increasing delays between retry attempts, giving system components time to recover from failure conditions.</p>\n<p>The base retry calculation uses the formula: <code>delay = base_delay * (2^attempt_number) + random_jitter</code> where jitter prevents the thundering herd problem when multiple jobs fail simultaneously.</p>\n<p><strong>Exponential Backoff Configuration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Base Delay</th>\n<th>Max Delay</th>\n<th>Jitter Range</th>\n<th>Max Attempts</th>\n<th>Backoff Factor</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network Timeout</td>\n<td>1 second</td>\n<td>300 seconds</td>\n<td>±20%</td>\n<td>5</td>\n<td>2.0</td>\n</tr>\n<tr>\n<td>Resource Constraint</td>\n<td>5 seconds</td>\n<td>900 seconds</td>\n<td>±30%</td>\n<td>3</td>\n<td>2.5</td>\n</tr>\n<tr>\n<td>External API Error</td>\n<td>2 seconds</td>\n<td>600 seconds</td>\n<td>±25%</td>\n<td>7</td>\n<td>2.0</td>\n</tr>\n<tr>\n<td>File I/O Error</td>\n<td>0.5 seconds</td>\n<td>60 seconds</td>\n<td>±10%</td>\n<td>3</td>\n<td>1.5</td>\n</tr>\n<tr>\n<td>FFmpeg Error</td>\n<td>3 seconds</td>\n<td>180 seconds</td>\n<td>±15%</td>\n<td>3</td>\n<td>2.0</td>\n</tr>\n</tbody></table>\n<p>The jitter component prevents synchronized retry attempts that can overwhelm recovering systems. Random jitter is calculated as: <code>jitter = delay * (1.0 + random(-jitter_range, +jitter_range))</code> where random generates values within the specified range.</p>\n<p><strong>Priority-Based Retry Scheduling</strong> ensures that high-priority jobs receive preferential retry treatment while preventing lower-priority jobs from being starved indefinitely.</p>\n<table>\n<thead>\n<tr>\n<th>Job Priority</th>\n<th>Retry Delay Multiplier</th>\n<th>Queue Position</th>\n<th>Max Concurrent Retries</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>JobPriority.URGENT</code></td>\n<td>0.5x</td>\n<td>Front of retry queue</td>\n<td>10</td>\n</tr>\n<tr>\n<td><code>JobPriority.HIGH</code></td>\n<td>0.75x</td>\n<td>Priority section</td>\n<td>7</td>\n</tr>\n<tr>\n<td><code>JobPriority.NORMAL</code></td>\n<td>1.0x</td>\n<td>Standard section</td>\n<td>5</td>\n</tr>\n<tr>\n<td><code>JobPriority.LOW</code></td>\n<td>1.5x</td>\n<td>End of retry queue</td>\n<td>2</td>\n</tr>\n</tbody></table>\n<h3 id=\"circuit-breaker-pattern-for-external-dependencies\">Circuit Breaker Pattern for External Dependencies</h3>\n<p>Circuit breakers protect the system from cascade failures when external dependencies become unreliable. Like electrical circuit breakers that prevent electrical fires by cutting power during overload conditions, software circuit breakers prevent retry storms by temporarily stopping calls to failing services.</p>\n<p>The circuit breaker maintains three states: <strong>Closed</strong> (normal operation), <strong>Open</strong> (blocking calls to failed service), and <strong>Half-Open</strong> (testing if service has recovered).</p>\n<p><strong>Circuit Breaker State Transitions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Condition</th>\n<th>Next State</th>\n<th>Action Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>Failure rate &lt; threshold</td>\n<td>Closed</td>\n<td>Allow all requests</td>\n</tr>\n<tr>\n<td>Closed</td>\n<td>Failure rate ≥ threshold</td>\n<td>Open</td>\n<td>Block all requests, start timeout</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>Timeout not expired</td>\n<td>Open</td>\n<td>Continue blocking requests</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>Timeout expired</td>\n<td>Half-Open</td>\n<td>Allow single test request</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Test request succeeds</td>\n<td>Closed</td>\n<td>Resume normal operation</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Test request fails</td>\n<td>Open</td>\n<td>Reset timeout, continue blocking</td>\n</tr>\n</tbody></table>\n<p>Circuit breaker configuration varies based on the criticality and typical failure patterns of different external dependencies:</p>\n<table>\n<thead>\n<tr>\n<th>Dependency Type</th>\n<th>Failure Threshold</th>\n<th>Timeout Duration</th>\n<th>Test Interval</th>\n<th>Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Redis Job Queue</td>\n<td>5 failures in 60s</td>\n<td>30 seconds</td>\n<td>10 seconds</td>\n<td>Job processing stops</td>\n</tr>\n<tr>\n<td>Webhook Delivery</td>\n<td>10 failures in 300s</td>\n<td>120 seconds</td>\n<td>30 seconds</td>\n<td>Notifications delayed</td>\n</tr>\n<tr>\n<td>Cloud Storage</td>\n<td>3 failures in 30s</td>\n<td>60 seconds</td>\n<td>15 seconds</td>\n<td>Upload/download blocked</td>\n</tr>\n<tr>\n<td>PostgreSQL Progress</td>\n<td>3 failures in 45s</td>\n<td>45 seconds</td>\n<td>15 seconds</td>\n<td>Progress tracking disabled</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Circuit Breaker vs Simple Retry</strong></p>\n<ul>\n<li><strong>Context</strong>: External dependencies like webhooks and cloud storage can become unreliable, causing job failures to cascade</li>\n<li><strong>Options Considered</strong>: Simple retry with exponential backoff, circuit breaker pattern, hybrid approach with circuit breaker for critical paths</li>\n<li><strong>Decision</strong>: Circuit breaker for external dependencies, simple retry for internal operations</li>\n<li><strong>Rationale</strong>: Circuit breakers prevent cascade failures and retry storms during external service outages, while simple retry is sufficient for transient internal errors</li>\n<li><strong>Consequences</strong>: Adds complexity but prevents system-wide outages when external services fail, improves overall system stability</li>\n</ul>\n</blockquote>\n<h3 id=\"retry-queue-management\">Retry Queue Management</h3>\n<p>Failed jobs requiring retry must be managed separately from the main processing queue to prevent blocking new job processing during system recovery periods. The retry queue implements priority-based scheduling and resource-aware retry timing.</p>\n<p>The <code>QueueManager</code> maintains separate Redis lists for different retry categories, allowing independent processing and monitoring of retry workloads:</p>\n<table>\n<thead>\n<tr>\n<th>Queue Name</th>\n<th>Purpose</th>\n<th>Priority</th>\n<th>Worker Pool</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>retry:immediate</code></td>\n<td>Network timeouts, transient errors</td>\n<td>High</td>\n<td>Dedicated retry workers</td>\n</tr>\n<tr>\n<td><code>retry:resource</code></td>\n<td>Memory/CPU constraint errors</td>\n<td>Medium</td>\n<td>Resource-aware workers</td>\n</tr>\n<tr>\n<td><code>retry:dependency</code></td>\n<td>External API, webhook failures</td>\n<td>Low</td>\n<td>Limited concurrency workers</td>\n</tr>\n<tr>\n<td><code>dead_letter</code></td>\n<td>Permanently failed jobs</td>\n<td>N/A</td>\n<td>Manual review queue</td>\n</tr>\n</tbody></table>\n<p><strong>Retry Scheduling Algorithm:</strong></p>\n<ol>\n<li>Failed jobs are classified by error type and assigned to appropriate retry queue</li>\n<li>Retry delay is calculated based on error type, attempt number, job priority, and current system load</li>\n<li>Jobs are scheduled for retry using Redis sorted sets with retry timestamp as score</li>\n<li>Background scheduler process scans sorted sets every 10 seconds for jobs ready to retry</li>\n<li>Ready jobs are moved back to main processing queue with retry context preserved</li>\n<li>Retry metrics are updated for monitoring and alerting</li>\n</ol>\n<p>Resource-aware retry scheduling monitors system health before processing retry jobs to prevent overwhelming an already-stressed system:</p>\n<table>\n<thead>\n<tr>\n<th>System Condition</th>\n<th>Retry Processing</th>\n<th>Queue Behavior</th>\n<th>Worker Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Normal load (&lt;70%)</td>\n<td>Process all retry types</td>\n<td>Standard scheduling</td>\n<td>Full retry processing</td>\n</tr>\n<tr>\n<td>High load (70-85%)</td>\n<td>Pause resource-intensive retries</td>\n<td>Delay resource retries</td>\n<td>Process only network retries</td>\n</tr>\n<tr>\n<td>Critical load (&gt;85%)</td>\n<td>Pause all retries</td>\n<td>Queue all retries</td>\n<td>Focus on current jobs</td>\n</tr>\n<tr>\n<td>Recovery mode</td>\n<td>Gradual retry resumption</td>\n<td>Priority-based restart</td>\n<td>Slowly increase retry rate</td>\n</tr>\n</tbody></table>\n<h3 id=\"dead-letter-queue-management\">Dead Letter Queue Management</h3>\n<p>Jobs that exceed maximum retry attempts or encounter permanent errors are moved to a dead letter queue for manual investigation and potential reprocessing. The dead letter queue serves as both a safety net preventing data loss and a debugging resource for system improvement.</p>\n<p><strong>Dead Letter Queue Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>original_job</code></td>\n<td><code>ProcessingJob</code></td>\n<td>Complete original job specification</td>\n</tr>\n<tr>\n<td><code>failure_history</code></td>\n<td><code>List[FailureRecord]</code></td>\n<td>Chronological list of all retry attempts</td>\n</tr>\n<tr>\n<td><code>final_error</code></td>\n<td><code>str</code></td>\n<td>Final error message that caused permanent failure</td>\n</tr>\n<tr>\n<td><code>classification</code></td>\n<td><code>str</code></td>\n<td>Error classification (permanent, retry_exhausted, configuration)</td>\n</tr>\n<tr>\n<td><code>moved_at</code></td>\n<td><code>datetime</code></td>\n<td>Timestamp when moved to dead letter queue</td>\n</tr>\n<tr>\n<td><code>investigation_notes</code></td>\n<td><code>str</code></td>\n<td>Manual notes from debugging investigation</td>\n</tr>\n<tr>\n<td><code>resolution_action</code></td>\n<td><code>str</code></td>\n<td>Action taken (reprocessed, cancelled, fixed)</td>\n</tr>\n</tbody></table>\n<p>Dead letter queue monitoring provides insights into system reliability and helps identify recurring failure patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Calculation</th>\n<th>Alert Threshold</th>\n<th>Action Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Dead letter rate</td>\n<td>Dead letter jobs / total jobs</td>\n<td>&gt; 2%</td>\n<td>Investigate error patterns</td>\n</tr>\n<tr>\n<td>Error pattern frequency</td>\n<td>Count by error classification</td>\n<td>&gt; 10 same errors/hour</td>\n<td>Fix underlying issue</td>\n</tr>\n<tr>\n<td>Resolution time</td>\n<td>Time from dead letter to resolution</td>\n<td>&gt; 24 hours</td>\n<td>Improve support process</td>\n</tr>\n<tr>\n<td>Reprocess success rate</td>\n<td>Successful reprocessed jobs / total reprocessed</td>\n<td>&lt; 80%</td>\n<td>Review error classification</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Retry Storm Prevention</strong></p>\n<p><strong>The Problem:</strong> When a shared dependency like Redis fails, all worker processes simultaneously retry their operations, creating a retry storm that prevents the dependency from recovering and may cause additional system failures.</p>\n<p><strong>Why It&#39;s Wrong:</strong> Synchronized retries can overwhelm recovering systems, extend outage duration, and cause cascade failures to other system components.</p>\n<p><strong>How to Fix:</strong> Implement jittered exponential backoff with circuit breakers, spread retry attempts over time, and monitor retry rates to detect storm conditions before they cause damage.</p>\n<h2 id=\"resource-cleanup-and-recovery\">Resource Cleanup and Recovery</h2>\n<p>Comprehensive resource cleanup ensures that failed jobs do not consume system resources indefinitely and that worker processes can recover gracefully from various failure scenarios. Resource management must handle both normal cleanup during successful job completion and emergency cleanup during various failure modes.</p>\n<h3 id=\"mental-model-restaurant-kitchen-cleanup\">Mental Model: Restaurant Kitchen Cleanup</h3>\n<p>Think of resource cleanup like the closing procedures at a busy restaurant kitchen. Throughout the day, cooks use various workstations, utensils, and ingredients. When a dish is completed successfully, the cook cleans their station, puts away ingredients, and washes utensils. But when something goes wrong — a pot burns, a sauce spills, or a cook gets sick mid-service — there are emergency cleanup protocols.</p>\n<p>The kitchen manager doesn&#39;t just abandon the messy workstation; they have systematic procedures: contain any spills to prevent spread, safely dispose of any ruined food, sanitize contaminated surfaces, and return the workstation to usable condition. Similarly, our media processing system must clean up temporary files, release memory allocations, terminate background processes, and reset worker state when jobs fail.</p>\n<p>Just as restaurants have health inspectors who ensure cleanup procedures are followed correctly, our system includes resource monitors that verify cleanup completion and alert operators when resources are not properly released.</p>\n<h3 id=\"temporary-file-management\">Temporary File Management</h3>\n<p>Media processing operations create numerous temporary files during processing: extracted video frames, intermediate processing results, format conversion buffers, and FFmpeg output segments. These temporary files must be tracked and cleaned up reliably even when processing fails unexpectedly.</p>\n<p>The <code>ResourceManager</code> maintains a registry of all temporary files associated with each processing job, enabling comprehensive cleanup regardless of how processing terminates.</p>\n<p><strong>Temporary File Lifecycle Management:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>File Types Created</th>\n<th>Cleanup Trigger</th>\n<th>Cleanup Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Initialization</td>\n<td>Working directory, lock files</td>\n<td>Job start failure</td>\n<td>Immediate deletion</td>\n</tr>\n<tr>\n<td>Image Processing</td>\n<td>Resized variants, format conversions</td>\n<td>Processing completion/failure</td>\n<td>Staged cleanup</td>\n</tr>\n<tr>\n<td>Video Transcoding</td>\n<td>FFmpeg segments, audio extracts</td>\n<td>Transcoding completion/failure</td>\n<td>Background cleanup</td>\n</tr>\n<tr>\n<td>Output Generation</td>\n<td>Final outputs, compressed archives</td>\n<td>Job completion/timeout</td>\n<td>Retention policy cleanup</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Debug dumps, partial results</td>\n<td>Recovery completion</td>\n<td>Delayed cleanup</td>\n</tr>\n</tbody></table>\n<p>The temporary file registry tracks file metadata and cleanup requirements for each job:</p>\n<table>\n<thead>\n<tr>\n<th>Registry Field</th>\n<th>Type</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>job_id</code></td>\n<td><code>str</code></td>\n<td>Associate files with processing job</td>\n</tr>\n<tr>\n<td><code>file_path</code></td>\n<td><code>str</code></td>\n<td>Absolute path to temporary file</td>\n</tr>\n<tr>\n<td><code>file_type</code></td>\n<td><code>str</code></td>\n<td>Category for cleanup prioritization</td>\n</tr>\n<tr>\n<td><code>size_bytes</code></td>\n<td><code>int</code></td>\n<td>File size for cleanup planning</td>\n</tr>\n<tr>\n<td><code>created_at</code></td>\n<td><code>datetime</code></td>\n<td>Creation timestamp for age-based cleanup</td>\n</tr>\n<tr>\n<td><code>cleanup_priority</code></td>\n<td><code>int</code></td>\n<td>Cleanup order priority (1=immediate, 5=delayed)</td>\n</tr>\n<tr>\n<td><code>retention_hours</code></td>\n<td><code>int</code></td>\n<td>How long to keep file after job completion</td>\n</tr>\n</tbody></table>\n<p><strong>Cleanup Priority Algorithm:</strong></p>\n<ol>\n<li><strong>Immediate Priority (1)</strong>: Lock files, PID files, and job control structures that prevent worker restart</li>\n<li><strong>High Priority (2)</strong>: Large temporary files, intermediate processing results consuming significant disk space  </li>\n<li><strong>Medium Priority (3)</strong>: Debug output files, processing logs, and diagnostic information</li>\n<li><strong>Low Priority (4)</strong>: Backup files, cached intermediate results that might be useful for retry</li>\n<li><strong>Delayed Priority (5)</strong>: Output files, final results that may need manual verification</li>\n</ol>\n<p>Background cleanup processes run every 60 seconds to handle orphaned temporary files from crashed worker processes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Cleanup Algorithm:\n1. Scan temporary directory for files older than configured age threshold\n2. Cross-reference found files against active job registry\n3. Identify orphaned files not associated with running jobs\n4. Group files by cleanup priority for efficient processing\n5. Delete immediate priority files first, then work down priority levels\n6. Log cleanup statistics for monitoring and alerting\n7. Update disk space metrics after cleanup completion</code></pre></div>\n\n<h3 id=\"memory-management-and-resource-limits\">Memory Management and Resource Limits</h3>\n<p>Image and video processing operations can consume substantial memory, particularly when handling high-resolution content or multiple concurrent jobs. Effective memory management prevents out-of-memory conditions and ensures fair resource allocation across worker processes.</p>\n<p><strong>Memory Allocation Tracking:</strong></p>\n<p>The <code>ResourceMonitor</code> tracks memory usage at multiple levels to provide comprehensive memory management:</p>\n<table>\n<thead>\n<tr>\n<th>Memory Category</th>\n<th>Tracking Method</th>\n<th>Limit Enforcement</th>\n<th>Cleanup Trigger</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Process Memory</td>\n<td>RSS monitoring</td>\n<td>Process limits</td>\n<td>Memory threshold exceeded</td>\n</tr>\n<tr>\n<td>Image Buffers</td>\n<td>Allocation tracking</td>\n<td>Buffer size limits</td>\n<td>Processing completion</td>\n</tr>\n<tr>\n<td>Video Frames</td>\n<td>Frame buffer counting</td>\n<td>Frame limit enforcement</td>\n<td>Frame processing completion</td>\n</tr>\n<tr>\n<td>FFmpeg Processes</td>\n<td>Child process monitoring</td>\n<td>Process memory limits</td>\n<td>FFmpeg completion/timeout</td>\n</tr>\n<tr>\n<td>Cache Memory</td>\n<td>Cache size tracking</td>\n<td>LRU eviction</td>\n<td>Cache size limits</td>\n</tr>\n</tbody></table>\n<p>Pre-processing memory estimation prevents jobs from starting when insufficient memory is available:</p>\n<p><strong>Memory Estimation Algorithm:</strong></p>\n<ol>\n<li>Analyze input file metadata to estimate processing requirements</li>\n<li>Calculate memory needed for image processing: <code>width * height * channels * bytes_per_channel * processing_factor</code></li>\n<li>Estimate video processing memory: <code>frame_width * frame_height * fps * processing_duration * memory_factor</code></li>\n<li>Add overhead estimates for format conversion, compression, and temporary buffers</li>\n<li>Compare total estimate against available system memory minus safety margin</li>\n<li>Reject job submission if estimated memory exceeds available resources</li>\n<li>Queue job with resource requirements for later processing when memory becomes available</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Content Type</th>\n<th>Base Memory Formula</th>\n<th>Processing Factor</th>\n<th>Safety Margin</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JPEG Images</td>\n<td>width × height × 3 bytes</td>\n<td>2.5x (for processing buffers)</td>\n<td>20% of available RAM</td>\n</tr>\n<tr>\n<td>PNG Images</td>\n<td>width × height × 4 bytes</td>\n<td>3.0x (for transparency handling)</td>\n<td>20% of available RAM</td>\n</tr>\n<tr>\n<td>4K Video</td>\n<td>3840 × 2160 × 3 × fps</td>\n<td>5.0x (for frame buffers)</td>\n<td>30% of available RAM</td>\n</tr>\n<tr>\n<td>HD Video</td>\n<td>1920 × 1080 × 3 × fps</td>\n<td>4.0x (for processing)</td>\n<td>25% of available RAM</td>\n</tr>\n</tbody></table>\n<h3 id=\"worker-process-recovery\">Worker Process Recovery</h3>\n<p>Worker processes may fail due to memory exhaustion, segmentation faults, infinite loops, or external process termination. The worker recovery system detects failed processes and restarts them while preserving job queue integrity.</p>\n<p><strong>Worker Health Monitoring:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Health Check Type</th>\n<th>Frequency</th>\n<th>Timeout</th>\n<th>Failure Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Heartbeat Signal</td>\n<td>30 seconds</td>\n<td>45 seconds</td>\n<td>Mark worker unhealthy</td>\n</tr>\n<tr>\n<td>Job Progress Update</td>\n<td>Based on job type</td>\n<td>2x expected duration</td>\n<td>Investigate job status</td>\n</tr>\n<tr>\n<td>Memory Usage Check</td>\n<td>60 seconds</td>\n<td>N/A</td>\n<td>Warn if approaching limits</td>\n</tr>\n<tr>\n<td>Process Existence</td>\n<td>15 seconds</td>\n<td>N/A</td>\n<td>Restart if process missing</td>\n</tr>\n<tr>\n<td>Queue Connection</td>\n<td>60 seconds</td>\n<td>10 seconds</td>\n<td>Restart worker</td>\n</tr>\n</tbody></table>\n<p>The <code>WorkerCoordinator</code> manages worker lifecycle and implements recovery procedures:</p>\n<p><strong>Worker Recovery Procedure:</strong></p>\n<ol>\n<li><strong>Failure Detection</strong>: Monitor detects worker process has failed or become unresponsive</li>\n<li><strong>Job Status Assessment</strong>: Determine status of job being processed by failed worker</li>\n<li><strong>Resource Cleanup</strong>: Clean up temporary files, release memory, terminate child processes</li>\n<li><strong>Job State Recovery</strong>: Mark in-progress job as failed and eligible for retry if appropriate</li>\n<li><strong>Worker Process Restart</strong>: Launch new worker process with same configuration</li>\n<li><strong>Queue Reconnection</strong>: Re-establish Redis connection and resume job processing</li>\n<li><strong>Health Verification</strong>: Verify new worker process is healthy before accepting jobs</li>\n<li><strong>Monitoring Update</strong>: Update worker registry and notify monitoring systems</li>\n</ol>\n<p><strong>Job Recovery State Machine:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Job State at Worker Failure</th>\n<th>Recovery Action</th>\n<th>Job Disposition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>JobStatus.PENDING</code></td>\n<td>No action needed</td>\n<td>Remains in queue for other worker</td>\n</tr>\n<tr>\n<td><code>JobStatus.PROCESSING</code> (&lt; 10% complete)</td>\n<td>Mark as failed</td>\n<td>Eligible for immediate retry</td>\n</tr>\n<tr>\n<td><code>JobStatus.PROCESSING</code> (10-90% complete)</td>\n<td>Mark as failed with progress</td>\n<td>Eligible for retry with backoff</td>\n</tr>\n<tr>\n<td><code>JobStatus.PROCESSING</code> (&gt; 90% complete)</td>\n<td>Attempt recovery</td>\n<td>Check for partial outputs</td>\n</tr>\n<tr>\n<td>Partially completed</td>\n<td>Validate outputs</td>\n<td>Complete if outputs valid, retry if not</td>\n</tr>\n</tbody></table>\n<h3 id=\"database-transaction-cleanup\">Database Transaction Cleanup</h3>\n<p>Progress tracking and job status updates use database transactions that must be properly cleaned up when worker processes fail unexpectedly. Abandoned transactions can lock database resources and prevent other workers from updating job status.</p>\n<p>The <code>MetadataStore</code> implements transaction cleanup procedures:</p>\n<p><strong>Transaction Management:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Transaction Type</th>\n<th>Timeout</th>\n<th>Cleanup Method</th>\n<th>Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Status Update</td>\n<td>30 seconds</td>\n<td>Auto-rollback</td>\n<td>Retry with fresh transaction</td>\n</tr>\n<tr>\n<td>Progress Update</td>\n<td>15 seconds</td>\n<td>Auto-rollback</td>\n<td>Use cached progress value</td>\n</tr>\n<tr>\n<td>Batch Progress Update</td>\n<td>60 seconds</td>\n<td>Partial commit</td>\n<td>Commit successful updates</td>\n</tr>\n<tr>\n<td>Job Completion</td>\n<td>45 seconds</td>\n<td>Manual cleanup</td>\n<td>Verify final state</td>\n</tr>\n</tbody></table>\n<p>Connection pool management prevents resource exhaustion from failed transactions:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Transaction Cleanup Algorithm:\n1. Monitor active database connections for each worker process\n2. Track transaction duration and identify long-running transactions\n3. When worker process fails, identify orphaned database connections\n4. Rollback any active transactions associated with failed worker\n5. Close database connections cleanly to return them to pool\n6. Update connection pool statistics and alert on connection leaks\n7. Verify database consistency after cleanup completion</code></pre></div>\n\n<p>⚠️ <strong>Pitfall: Incomplete Resource Cleanup</strong></p>\n<p><strong>The Problem:</strong> When worker processes crash or are terminated forcefully, temporary files, database connections, and child processes may not be cleaned up properly, leading to resource exhaustion and system instability.</p>\n<p><strong>Why It&#39;s Wrong:</strong> Accumulated resource leaks can cause disk space exhaustion, database connection pool exhaustion, zombie processes, and memory leaks that degrade system performance over time.</p>\n<p><strong>How to Fix:</strong> Implement comprehensive resource tracking with cleanup verification, use signal handlers for graceful shutdown, implement resource monitors that detect and clean up orphaned resources, and test cleanup procedures under various failure scenarios.</p>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Fwebhook-notification-flow.svg\" alt=\"Webhook Notification Flow\"></p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical implementation details for building robust error handling and recovery mechanisms in the media processing pipeline.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Classification</td>\n<td>Exception type matching</td>\n<td>Rule engine with configurable decision trees</td>\n</tr>\n<tr>\n<td>Retry Management</td>\n<td>Redis sorted sets</td>\n<td>Dedicated retry service with complex scheduling</td>\n</tr>\n<tr>\n<td>Circuit Breaker</td>\n<td>Simple state tracking</td>\n<td>Library like <code>pybreaker</code> with metrics</td>\n</tr>\n<tr>\n<td>Resource Monitoring</td>\n<td>Basic system calls</td>\n<td>APM tools like Prometheus with custom metrics</td>\n</tr>\n<tr>\n<td>Cleanup Scheduling</td>\n<td>Cron-based cleanup</td>\n<td>Event-driven cleanup with dependency tracking</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  error_handling/\n    __init__.py              ← Error handling exports\n    error_classifier.py      ← Error classification logic  \n    retry_manager.py         ← Retry and backoff implementation\n    circuit_breaker.py       ← Circuit breaker for external deps\n    resource_cleanup.py      ← Cleanup and recovery procedures\n    recovery_coordinator.py  ← Overall recovery orchestration\n  monitoring/\n    resource_monitor.py      ← System resource monitoring\n    health_checker.py        ← Worker and service health checks\n  storage/\n    temp_file_manager.py     ← Temporary file lifecycle management\n    cleanup_scheduler.py     ← Background cleanup processes</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<p>Here&#39;s a complete error classification system that categorizes failures for appropriate retry handling:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> subprocess</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorCategory</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TRANSIENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"transient\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RESOURCE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"resource\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PERMANENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"permanent\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEPENDENCY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"dependency\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorClassification</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    category: ErrorCategory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_eligible: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_retries: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_factor: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ErrorClassifier</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Classifies errors into categories for appropriate retry handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Uses exception types, error codes, and message patterns.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._init_classification_rules()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _init_classification_rules</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize error classification rules.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Network and connectivity errors</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.network_errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'ConnectionError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'TimeoutError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'ConnectionResetError'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'redis.exceptions.ConnectionError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'requests.exceptions.Timeout'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Resource constraint errors  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.resource_errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'MemoryError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'OSError'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'IOError'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # FFmpeg error code patterns</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.ffmpeg_exit_codes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Transient errors</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            1</span><span style=\"color:#E1E4E8\">: ErrorClassification(ErrorCategory.</span><span style=\"color:#79B8FF\">RESOURCE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Generic FFmpeg error\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Permanent errors  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            69</span><span style=\"color:#E1E4E8\">: ErrorClassification(ErrorCategory.</span><span style=\"color:#79B8FF\">PERMANENT</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Invalid input format\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Add more as needed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Error message patterns</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.error_patterns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">disk</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#DBEDFF\">full</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">no space left</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, re.I), ErrorCategory.</span><span style=\"color:#79B8FF\">RESOURCE</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">permission denied</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">access denied</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, re.I), ErrorCategory.</span><span style=\"color:#79B8FF\">PERMANENT</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">invalid</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#DBEDFF\">format</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">corrupt</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#DBEDFF\">file</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, re.I), ErrorCategory.</span><span style=\"color:#79B8FF\">PERMANENT</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            (re.compile(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">network</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#DBEDFF\">unreachable</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">connection</span><span style=\"color:#79B8FF\">.</span><span style=\"color:#F97583\">*</span><span style=\"color:#DBEDFF\">refused</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, re.I), ErrorCategory.</span><span style=\"color:#79B8FF\">DEPENDENCY</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> classify_error</span><span style=\"color:#E1E4E8\">(self, exception: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">, context: Dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ErrorClassification:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Classify an error for retry handling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            exception: The exception that occurred</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            context: Additional context (exit codes, job info, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ErrorClassification with retry strategy</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        exception_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> type</span><span style=\"color:#E1E4E8\">(exception).</span><span style=\"color:#79B8FF\">__name__</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        error_message </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(exception)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for network/connectivity errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> exception_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.network_errors:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ErrorClassification(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ErrorCategory.</span><span style=\"color:#79B8FF\">TRANSIENT</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"Network connectivity error\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check for resource errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> exception_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.resource_errors:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._classify_resource_error(exception, error_message)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check FFmpeg exit codes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> context </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> 'exit_code'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> context:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> context[</span><span style=\"color:#9ECBFF\">'exit_code'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.ffmpeg_exit_codes:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.ffmpeg_exit_codes[context[</span><span style=\"color:#9ECBFF\">'exit_code'</span><span style=\"color:#E1E4E8\">]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check error message patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> pattern, category </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.error_patterns:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> pattern.search(error_message):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._get_default_classification(category)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Default to transient with limited retries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrorClassification(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ErrorCategory.</span><span style=\"color:#79B8FF\">TRANSIENT</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1.5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"Unclassified error - limited retry\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _classify_resource_error</span><span style=\"color:#E1E4E8\">(self, exception: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> ErrorClassification:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Classify resource-related errors more specifically.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#9ECBFF\"> 'memory'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> message.lower() </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> isinstance</span><span style=\"color:#E1E4E8\">(exception, </span><span style=\"color:#79B8FF\">MemoryError</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ErrorClassification(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ErrorCategory.</span><span style=\"color:#79B8FF\">RESOURCE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"Memory exhaustion error\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#9ECBFF\"> 'disk'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> message.lower() </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> 'space'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> message.lower():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ErrorClassification(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ErrorCategory.</span><span style=\"color:#79B8FF\">RESOURCE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"Disk space error\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> ErrorClassification(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ErrorCategory.</span><span style=\"color:#79B8FF\">RESOURCE</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"General resource error\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_default_classification</span><span style=\"color:#E1E4E8\">(self, category: ErrorCategory) -> ErrorClassification:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get default classification for error category.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        defaults </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ErrorCategory.</span><span style=\"color:#79B8FF\">TRANSIENT</span><span style=\"color:#E1E4E8\">: ErrorClassification(category, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Transient error\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ErrorCategory.</span><span style=\"color:#79B8FF\">RESOURCE</span><span style=\"color:#E1E4E8\">: ErrorClassification(category, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Resource error\"</span><span style=\"color:#E1E4E8\">), </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ErrorCategory.</span><span style=\"color:#79B8FF\">PERMANENT</span><span style=\"color:#E1E4E8\">: ErrorClassification(category, </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Permanent error\"</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ErrorCategory.</span><span style=\"color:#79B8FF\">DEPENDENCY</span><span style=\"color:#E1E4E8\">: ErrorClassification(category, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">7</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"External dependency error\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> defaults[category]</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code:</strong></p>\n<p>Here&#39;s the main retry manager that needs to be implemented:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Callable, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryContext</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempt_number: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_error: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    classification: ErrorClassification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scheduled_at: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Manages job retries with exponential backoff and priority scheduling.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Implements circuit breaker pattern for external dependencies.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: redis.Redis, config: ProcessingConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.error_classifier </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ErrorClassifier()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Circuit breaker state tracking</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.circuit_breakers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> schedule_retry</span><span style=\"color:#E1E4E8\">(self, job: ProcessingJob, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">, context: Dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Schedule a job for retry based on error classification.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            job: The failed processing job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            error: Exception that caused failure  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            context: Additional error context</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if retry scheduled, False if job moved to dead letter queue</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use error_classifier to classify the error and get retry strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if job has exceeded maximum retry attempts for its error type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If retry limit exceeded, move job to dead letter queue and return False</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate retry delay using exponential backoff with jitter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create RetryContext with job info, attempt number, and scheduled time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store retry context in Redis sorted set with schedule time as score  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Update job status to indicate retry scheduled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Log retry scheduling with job context and delay information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Update retry metrics for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return True to indicate retry was scheduled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        classification </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.error_classifier.classify_error(error, context)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> classification.retry_eligible:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._move_to_dead_letter_queue(job, error, </span><span style=\"color:#9ECBFF\">\"Non-retryable error\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> job.retry_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> classification.max_retries:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._move_to_dead_letter_queue(job, error, </span><span style=\"color:#9ECBFF\">\"Retry limit exceeded\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Implementation continues...</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_retry_delay</span><span style=\"color:#E1E4E8\">(self, attempt: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, base_delay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, backoff_factor: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            max_delay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, jitter_range: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Calculate retry delay with exponential backoff and jitter.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            attempt: Current retry attempt number (1-based)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            base_delay: Base delay in seconds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            backoff_factor: Exponential backoff multiplier  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            max_delay: Maximum delay cap in seconds</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            jitter_range: Jitter as fraction of delay (0.0-1.0)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Calculated delay in seconds with jitter applied</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate exponential delay: base_delay * (backoff_factor ** (attempt - 1))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply maximum delay cap to prevent excessive wait times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate jitter amount: delay * random value in [-jitter_range, +jitter_range]  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add jitter to delay to prevent thundering herd</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Ensure final delay is not negative</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Log calculated delay for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return final delay value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> process_retry_queue</span><span style=\"color:#E1E4E8\">(self, worker_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[ProcessingJob]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check retry queue for jobs ready to process.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            worker_id: ID of worker requesting retry job</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Job ready for retry processing, or None if no jobs ready</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current timestamp for comparing against scheduled retry times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Query Redis sorted set for jobs with score &#x3C;= current time (ready to retry)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use ZPOPMIN to atomically get and remove highest priority retry job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If no retry jobs ready, return None</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Deserialize retry context from Redis data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Load original ProcessingJob from retry context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Increment job retry count and update retry context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Mark job status as processing and assign to worker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 9: Log retry job assignment with worker and attempt information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 10: Return ProcessingJob ready for retry processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _move_to_dead_letter_queue</span><span style=\"color:#E1E4E8\">(self, job: ProcessingJob, error: </span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                       reason: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Move job to dead letter queue for manual review.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create dead letter record with job, error history, and failure reason</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store record in dead letter queue (Redis list or database)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update job status to FAILED with reason</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Send webhook notification about job failure if webhook_url configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Log dead letter queue movement with job ID and reason</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update dead letter queue metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Clean up any retry queue entries for this job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return True to indicate successful dead letter processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_circuit_breaker_key</span><span style=\"color:#E1E4E8\">(self, service: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate Redis key for circuit breaker state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"circuit_breaker:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">service</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> check_circuit_breaker</span><span style=\"color:#E1E4E8\">(self, service: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check if circuit breaker allows requests to service.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            service: Service name (e.g., 'webhooks', 'cloud_storage')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if requests allowed, False if circuit is open</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get circuit breaker state from Redis for the service</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If no state exists, initialize as CLOSED (allow requests)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If state is CLOSED, allow request and return True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If state is OPEN, check if timeout period has expired</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If timeout expired, transition to HALF_OPEN and allow single test request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If still in timeout period, block request and return False  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Log circuit breaker decision for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return decision (True = allow, False = block)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> record_service_result</span><span style=\"color:#E1E4E8\">(self, service: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, success: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record result of service call for circuit breaker logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current circuit breaker state for service</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If state is HALF_OPEN and request succeeded, transition to CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If state is HALF_OPEN and request failed, transition back to OPEN  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If state is CLOSED, track failure rate over time window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If failure rate exceeds threshold, transition to OPEN</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Update circuit breaker metrics and state in Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Log state transitions for monitoring and alerting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the error handling system:</p>\n<ol>\n<li><strong>Test Error Classification</strong>: Run <code>python -m pytest tests/test_error_classifier.py -v</code> to verify error classification logic</li>\n<li><strong>Test Retry Logic</strong>: Simulate job failures and verify exponential backoff calculations</li>\n<li><strong>Test Circuit Breaker</strong>: Simulate external service failures and verify circuit breaker state transitions  </li>\n<li><strong>Test Resource Cleanup</strong>: Create temporary files, kill worker processes, verify cleanup completion</li>\n<li><strong>Manual Verification</strong>: Submit a job that will fail (invalid input file), verify it moves through retry attempts and eventually reaches dead letter queue</li>\n</ol>\n<p><strong>Expected Behaviors:</strong></p>\n<ul>\n<li>Transient network errors should retry immediately with exponential backoff</li>\n<li>Resource errors should delay retry until system resources are available  </li>\n<li>Permanent errors (invalid file format) should move directly to dead letter queue</li>\n<li>Circuit breakers should prevent retry storms when external services fail</li>\n<li>Temporary files should be cleaned up within 60 seconds of job failure</li>\n<li>Worker processes should restart automatically and resume processing jobs</li>\n</ul>\n<h2 id=\"testing-strategy-and-milestones\">Testing Strategy and Milestones</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as comprehensive testing strategy ensures reliable image processing, video transcoding, and job queue functionality across the entire media processing pipeline</p>\n</blockquote>\n<h3 id=\"mental-model-quality-assurance-laboratory\">Mental Model: Quality Assurance Laboratory</h3>\n<p>Think of our testing strategy as a comprehensive quality assurance laboratory for a manufacturing facility. Just as a QA lab tests components individually (unit testing), validates the entire assembly line with real products (integration testing), and maintains strict checkpoints at each production stage (milestone validation), our media processing pipeline requires the same systematic approach to ensure reliability.</p>\n<p>In a manufacturing QA lab, you have three distinct testing phases: component testing with synthetic materials to verify individual part specifications, full production line testing with actual products to validate end-to-end workflows, and stage-gate reviews at each manufacturing milestone to ensure quality before proceeding to the next phase. Our media processing testing strategy follows this exact pattern, ensuring each component works correctly in isolation, the entire pipeline processes real media files reliably, and each development milestone delivers measurable functionality.</p>\n<p>The critical insight here is that media processing testing requires specialized approaches because we&#39;re dealing with binary data, time-consuming operations, and external dependencies. Unlike simple web applications where you can mock everything, media processing demands testing with actual image and video files, realistic processing times, and real queue behavior to catch the subtle bugs that only emerge under production conditions.</p>\n<h3 id=\"unit-testing-approach\">Unit Testing Approach</h3>\n<p><strong>Component Isolation Strategy</strong></p>\n<p>Unit testing in media processing systems requires a sophisticated approach to isolating components while maintaining realistic test conditions. Each processing component—image manipulation, video transcoding, job queue management, and progress tracking—must be testable independently without requiring external dependencies or lengthy processing operations.</p>\n<p>The fundamental challenge lies in creating test conditions that mirror production behavior without the overhead of processing large media files or waiting for complex transcoding operations. This demands carefully crafted test fixtures, mock implementations that preserve behavioral accuracy, and testing strategies that validate both happy path operations and edge case scenarios.</p>\n<blockquote>\n<p><strong>Decision: Synthetic Test Media Generation</strong></p>\n<ul>\n<li><strong>Context</strong>: Unit tests need realistic media files but cannot rely on large binary fixtures or external file dependencies that slow down test execution and complicate CI/CD pipelines</li>\n<li><strong>Options Considered</strong>: Real media file fixtures, procedurally generated test media, mocked media objects with metadata simulation</li>\n<li><strong>Decision</strong>: Implement procedurally generated test media with configurable characteristics</li>\n<li><strong>Rationale</strong>: Generated media provides precise control over test conditions (specific resolutions, formats, corruption patterns) while maintaining fast test execution and avoiding binary repository bloat</li>\n<li><strong>Consequences</strong>: Tests run quickly and reliably but require additional infrastructure to generate realistic media characteristics and edge cases</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Test Media Type</th>\n<th>Generation Method</th>\n<th>Characteristics</th>\n<th>Test Scenarios</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Minimal Valid Image</td>\n<td>PIL/Pillow solid color generation</td>\n<td>1x1 to 100x100 pixels, RGB/RGBA</td>\n<td>Format detection, basic operations</td>\n</tr>\n<tr>\n<td>Edge Case Image</td>\n<td>Programmatic creation with specific properties</td>\n<td>Extreme dimensions, unusual aspect ratios</td>\n<td>Memory limit validation, resize edge cases</td>\n</tr>\n<tr>\n<td>Corrupted Image</td>\n<td>Binary manipulation of valid images</td>\n<td>Truncated headers, invalid metadata</td>\n<td>Error handling, graceful degradation</td>\n</tr>\n<tr>\n<td>Video Test Files</td>\n<td>FFmpeg synthetic generation</td>\n<td>Short duration, known characteristics</td>\n<td>Transcoding logic, progress calculation</td>\n</tr>\n<tr>\n<td>Metadata Rich Media</td>\n<td>Embedded EXIF/metadata injection</td>\n<td>GPS, orientation, camera data</td>\n<td>Metadata extraction, privacy stripping</td>\n</tr>\n</tbody></table>\n<p><strong>Image Processing Unit Tests</strong></p>\n<p>Image processing unit tests focus on validating individual operations like resizing algorithms, format conversion accuracy, and metadata handling without requiring large image files or lengthy processing times. Each test verifies specific algorithmic behavior using minimal synthetic images that exercise the same code paths as production media.</p>\n<p>The testing strategy emphasizes boundary condition validation, ensuring resize operations handle edge cases like 1-pixel images, extreme aspect ratios, and memory-constrained scenarios. Format conversion tests verify color space preservation, quality setting behavior, and lossy compression characteristics using programmatically generated test images with known pixel values.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Methods</th>\n<th>Validation Focus</th>\n<th>Expected Outcomes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Resize Operations</td>\n<td><code>test_resize_preserve_aspect_ratio()</code></td>\n<td>Aspect ratio mathematics</td>\n<td>Output dimensions match calculated ratios</td>\n</tr>\n<tr>\n<td>Interpolation</td>\n<td><code>test_lanczos_vs_bilinear_quality()</code></td>\n<td>Algorithm quality differences</td>\n<td>Measurable quality metrics comparison</td>\n</tr>\n<tr>\n<td>Format Conversion</td>\n<td><code>test_jpeg_quality_settings()</code></td>\n<td>Lossy compression behavior</td>\n<td>File size decreases with lower quality</td>\n</tr>\n<tr>\n<td>Metadata Handling</td>\n<td><code>test_exif_orientation_rotation()</code></td>\n<td>EXIF-based rotation accuracy</td>\n<td>Rotated image matches expected orientation</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td><code>test_large_image_memory_calculation()</code></td>\n<td>Resource requirement estimation</td>\n<td>Memory calculations match actual usage</td>\n</tr>\n<tr>\n<td>Error Conditions</td>\n<td><code>test_corrupted_image_graceful_failure()</code></td>\n<td>Exception handling patterns</td>\n<td>Specific exceptions with descriptive messages</td>\n</tr>\n</tbody></table>\n<p><strong>Video Processing Unit Tests</strong></p>\n<p>Video processing unit tests require a different approach because video operations typically involve external FFmpeg processes and longer execution times. The testing strategy focuses on command generation validation, progress parsing accuracy, and metadata extraction without executing full transcoding operations.</p>\n<p>FFmpeg integration tests use synthetic video files generated on-demand with specific characteristics like resolution, duration, and codec combinations. These minimal test videos allow validation of transcoding logic, adaptive bitrate variant generation, and thumbnail extraction without the overhead of processing production-sized media files.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example test structure for FFmpeg command generation validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_build_transcode_command_h264</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Validates FFmpeg command construction without executing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> VideoTranscodingConfig(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        video_codec</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'h264'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        target_bitrate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2000000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        crf_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">23</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    command </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> wrapper._build_transcode_command(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'input.mp4'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'output.mp4'</span><span style=\"color:#E1E4E8\">, config, test_metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#9ECBFF\"> '-c:v libx264'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> command</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#9ECBFF\"> '-b:v 2M'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> command</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#9ECBFF\"> '-crf 23'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> command</span></span></code></pre></div>\n\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Methods</th>\n<th>Validation Focus</th>\n<th>Mock Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Command Generation</td>\n<td><code>test_ffmpeg_command_building()</code></td>\n<td>Proper FFmpeg argument construction</td>\n<td>Mock metadata, validate command arrays</td>\n</tr>\n<tr>\n<td>Progress Parsing</td>\n<td><code>test_ffmpeg_progress_extraction()</code></td>\n<td>Progress percentage calculation</td>\n<td>Mock FFmpeg output streams</td>\n</tr>\n<tr>\n<td>Error Detection</td>\n<td><code>test_ffmpeg_error_classification()</code></td>\n<td>Exit code and stderr interpretation</td>\n<td>Simulated FFmpeg failure scenarios</td>\n</tr>\n<tr>\n<td>Codec Selection</td>\n<td><code>test_optimal_codec_selection()</code></td>\n<td>Input format to output codec mapping</td>\n<td>Mock format detection results</td>\n</tr>\n<tr>\n<td>ABR Variant Logic</td>\n<td><code>test_adaptive_bitrate_calculation()</code></td>\n<td>Multi-variant parameter generation</td>\n<td>Mock input video characteristics</td>\n</tr>\n</tbody></table>\n<p><strong>Job Queue Unit Tests</strong></p>\n<p>Job queue unit tests validate queue operations, priority handling, and worker coordination logic using in-memory implementations that preserve behavioral accuracy while eliminating Redis dependencies. These tests focus on race condition prevention, atomic operation behavior, and state transition validation.</p>\n<p>The testing approach uses dependency injection to replace Redis connections with thread-safe in-memory implementations that simulate Redis behavior including atomic operations, blocking pops, and pub/sub messaging. This allows comprehensive testing of queue logic without external infrastructure dependencies.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Methods</th>\n<th>Validation Focus</th>\n<th>Mock Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Submission</td>\n<td><code>test_atomic_job_submission()</code></td>\n<td>Deduplication and atomic operations</td>\n<td>In-memory job store with threading locks</td>\n</tr>\n<tr>\n<td>Priority Queuing</td>\n<td><code>test_priority_order_preservation()</code></td>\n<td>Higher priority jobs processed first</td>\n<td>Mock Redis sorted set behavior</td>\n</tr>\n<tr>\n<td>Worker Coordination</td>\n<td><code>test_multiple_worker_job_distribution()</code></td>\n<td>Jobs distributed fairly across workers</td>\n<td>Simulated worker pool with job assignment</td>\n</tr>\n<tr>\n<td>State Transitions</td>\n<td><code>test_job_lifecycle_state_machine()</code></td>\n<td>Valid state transitions only</td>\n<td>State machine validation with invalid transition attempts</td>\n</tr>\n<tr>\n<td>Dead Letter Queue</td>\n<td><code>test_failed_job_dead_letter_routing()</code></td>\n<td>Permanent failures routed correctly</td>\n<td>Mock failure classification and routing logic</td>\n</tr>\n</tbody></table>\n<p><strong>Progress Tracking Unit Tests</strong></p>\n<p>Progress tracking unit tests validate stage-based progress calculation, notification threshold logic, and webhook delivery mechanisms using mock HTTP endpoints and in-memory progress storage. These tests ensure accurate progress reporting and reliable notification delivery without external webhook dependencies.</p>\n<p>The testing strategy focuses on progress calculation accuracy, race condition prevention in concurrent updates, and webhook delivery retry logic. Mock HTTP servers simulate various webhook endpoint behaviors including success responses, timeout conditions, and authentication failures.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Methods</th>\n<th>Validation Focus</th>\n<th>Mock Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Progress Calculation</td>\n<td><code>test_stage_weighted_progress()</code></td>\n<td>Accurate percentage calculation</td>\n<td>Mock stage completion data</td>\n</tr>\n<tr>\n<td>Concurrent Updates</td>\n<td><code>test_progress_race_condition_prevention()</code></td>\n<td>Sequence number validation</td>\n<td>Multithreaded progress updates</td>\n</tr>\n<tr>\n<td>Webhook Delivery</td>\n<td><code>test_webhook_retry_exponential_backoff()</code></td>\n<td>Retry logic and backoff timing</td>\n<td>Mock HTTP server with failure simulation</td>\n</tr>\n<tr>\n<td>Signature Verification</td>\n<td><code>test_webhook_hmac_signature_validation()</code></td>\n<td>Cryptographic signature accuracy</td>\n<td>Known test vectors with expected signatures</td>\n</tr>\n</tbody></table>\n<h3 id=\"integration-testing-strategy\">Integration Testing Strategy</h3>\n<p><strong>End-to-End Media Processing Workflows</strong></p>\n<p>Integration testing validates complete media processing workflows using real media files and the full technology stack including Redis job queues, worker processes, and webhook notification delivery. These tests ensure all components work together correctly under realistic conditions and catch integration issues that unit tests cannot detect.</p>\n<p>The integration testing environment requires careful setup to provide realistic test conditions while maintaining fast execution and reliable cleanup. This includes containerized dependencies, realistic media file fixtures, and automated environment provisioning that matches production characteristics.</p>\n<blockquote>\n<p><strong>Decision: Containerized Integration Test Environment</strong></p>\n<ul>\n<li><strong>Context</strong>: Integration tests require Redis, potentially FFmpeg, webhook endpoint simulation, and file system storage that must be consistent across development machines and CI/CD pipelines</li>\n<li><strong>Options Considered</strong>: Local dependency installation, Docker Compose test environment, cloud-based test infrastructure</li>\n<li><strong>Decision</strong>: Docker Compose with service containers for Redis, mock webhook server, and shared volume for media files</li>\n<li><strong>Rationale</strong>: Containerized approach ensures consistent test environment, easy cleanup, parallel test execution without conflicts, and matches production deployment patterns</li>\n<li><strong>Consequences</strong>: Tests require Docker but provide reliable, reproducible results with easy local development and CI/CD integration</li>\n</ul>\n</blockquote>\n<p><strong>Real Media File Test Fixtures</strong></p>\n<p>Integration tests require carefully curated media file fixtures that represent realistic production scenarios without consuming excessive storage or processing time. The test fixture strategy balances comprehensive format coverage with practical test execution requirements.</p>\n<p>The fixture collection includes representative samples of each supported format with varying characteristics: different resolutions, aspect ratios, metadata complexity, and file sizes. Additionally, it includes problematic files that test edge cases like corrupted headers, unusual metadata, and format variants that require special handling.</p>\n<table>\n<thead>\n<tr>\n<th>File Category</th>\n<th>Format Examples</th>\n<th>Characteristics</th>\n<th>Test Scenarios</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Standard Images</td>\n<td>JPEG, PNG, WebP</td>\n<td>Common resolutions, normal metadata</td>\n<td>Basic processing workflows</td>\n</tr>\n<tr>\n<td>High Resolution</td>\n<td>Large JPEG, PNG</td>\n<td>4K+ resolution, substantial file size</td>\n<td>Memory management, processing time</td>\n</tr>\n<tr>\n<td>Unusual Formats</td>\n<td>GIF, TIFF, BMP</td>\n<td>Less common formats</td>\n<td>Format detection, conversion support</td>\n</tr>\n<tr>\n<td>Metadata Rich</td>\n<td>JPEG with GPS/EXIF</td>\n<td>Comprehensive metadata sets</td>\n<td>Privacy stripping, metadata preservation</td>\n</tr>\n<tr>\n<td>Problematic Files</td>\n<td>Corrupted headers, invalid metadata</td>\n<td>Various corruption types</td>\n<td>Error handling, graceful degradation</td>\n</tr>\n<tr>\n<td>Video Samples</td>\n<td>MP4 (H.264), WebM, MOV</td>\n<td>Short duration, various codecs</td>\n<td>Transcoding workflows, ABR generation</td>\n</tr>\n</tbody></table>\n<p><strong>Queue Processing Integration Tests</strong></p>\n<p>Queue processing integration tests validate the complete job lifecycle from submission through worker processing to completion notification. These tests use real Redis instances and actual worker processes to ensure proper job distribution, progress tracking, and error handling under concurrent conditions.</p>\n<p>The testing approach spawns multiple worker processes and submits various job types to validate queue behavior under realistic load conditions. Tests verify proper job prioritization, worker failure recovery, and resource cleanup after job completion or failure.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example integration test structure for complete job processing</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.mark.integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_complete_image_processing_workflow</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Submit image processing job with webhook</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> submit_job(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        input_file</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'fixtures/test_image.jpg'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        output_specs</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            OutputSpecification(</span><span style=\"color:#FFAB70\">format</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'webp'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">width</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">800</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">quality</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">85</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            OutputSpecification(</span><span style=\"color:#FFAB70\">format</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'png'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">width</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">400</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">quality</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        webhook_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'http://mock-server:8080/webhook'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Wait for processing completion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    final_status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> wait_for_job_completion(job.job_id, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Validate results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> final_status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> JobStatus.</span><span style=\"color:#79B8FF\">COMPLETED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(job.output_files) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(os.path.exists(f) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> f </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> job.output_files)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> webhook_received_completion_notification(job.job_id)</span></span></code></pre></div>\n\n<table>\n<thead>\n<tr>\n<th>Test Scenario</th>\n<th>Components Involved</th>\n<th>Validation Points</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single Image Job</td>\n<td>API, Queue, Worker, Storage</td>\n<td>Job completion, output file creation</td>\n<td>Files created with correct formats and dimensions</td>\n</tr>\n<tr>\n<td>Multiple Output Specs</td>\n<td>Worker, Format Converter</td>\n<td>All output specifications processed</td>\n<td>Each output meets specified requirements</td>\n</tr>\n<tr>\n<td>Video Transcoding</td>\n<td>Worker, FFmpeg, Progress Tracker</td>\n<td>Progress updates, final transcoding</td>\n<td>ABR variants created with proper manifests</td>\n</tr>\n<tr>\n<td>Webhook Notifications</td>\n<td>Progress Tracker, HTTP Client</td>\n<td>Notification delivery, retry logic</td>\n<td>Webhooks delivered with correct signatures</td>\n</tr>\n<tr>\n<td>Worker Failure Recovery</td>\n<td>Queue, Worker Coordinator</td>\n<td>Job reassignment, cleanup</td>\n<td>Jobs complete despite worker failures</td>\n</tr>\n<tr>\n<td>Priority Job Processing</td>\n<td>Queue, Multiple Workers</td>\n<td>Job ordering, execution priority</td>\n<td>High priority jobs processed first</td>\n</tr>\n</tbody></table>\n<p><strong>Webhook Integration Testing</strong></p>\n<p>Webhook integration tests validate notification delivery under various network conditions and endpoint behaviors. These tests use mock HTTP servers that simulate different webhook endpoint responses to ensure robust delivery logic and proper error handling.</p>\n<p>The testing infrastructure includes webhook endpoints that simulate success responses, timeout conditions, authentication failures, and temporary unavailability. Tests verify exponential backoff retry logic, signature verification, and eventual delivery guarantees.</p>\n<table>\n<thead>\n<tr>\n<th>Webhook Scenario</th>\n<th>Mock Server Behavior</th>\n<th>Test Validation</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Successful Delivery</td>\n<td>HTTP 200 response</td>\n<td>Single delivery attempt</td>\n<td>Webhook marked as delivered</td>\n</tr>\n<tr>\n<td>Temporary Failure</td>\n<td>HTTP 503, then 200</td>\n<td>Retry with backoff</td>\n<td>Eventually delivered after retries</td>\n</tr>\n<tr>\n<td>Permanent Failure</td>\n<td>Consistent HTTP 404</td>\n<td>Retry limit reached</td>\n<td>Marked as permanently failed</td>\n</tr>\n<tr>\n<td>Timeout Condition</td>\n<td>Connection timeout</td>\n<td>Timeout handling</td>\n<td>Retry with exponential backoff</td>\n</tr>\n<tr>\n<td>Invalid Signature</td>\n<td>Signature verification failure</td>\n<td>Security validation</td>\n<td>Webhook rejected by endpoint</td>\n</tr>\n</tbody></table>\n<p><strong>Performance and Load Testing</strong></p>\n<p>Integration tests include performance validation to ensure the system handles realistic workloads without degradation. These tests process multiple concurrent jobs with various media types and sizes to validate system behavior under load conditions.</p>\n<p>Performance tests measure processing throughput, queue latency, and resource utilization patterns. They establish baseline performance characteristics and detect performance regressions during development. The tests use realistic media files and job mixes that represent expected production workloads.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Test Type</th>\n<th>Load Characteristics</th>\n<th>Metrics Measured</th>\n<th>Acceptance Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent Image Processing</td>\n<td>50 simultaneous image jobs</td>\n<td>Jobs per second, memory usage</td>\n<td>&gt;10 images/sec, &lt;2GB memory per worker</td>\n</tr>\n<tr>\n<td>Mixed Workload</td>\n<td>Images and videos combined</td>\n<td>Queue latency, completion time</td>\n<td>&lt;5 second queue latency, predictable completion times</td>\n</tr>\n<tr>\n<td>Large File Handling</td>\n<td>High resolution images/videos</td>\n<td>Memory efficiency, processing time</td>\n<td>Linear scaling with file size</td>\n</tr>\n<tr>\n<td>Queue Saturation</td>\n<td>More jobs than worker capacity</td>\n<td>Queue depth, job assignment fairness</td>\n<td>Fair job distribution, no worker starvation</td>\n</tr>\n</tbody></table>\n<h3 id=\"milestone-checkpoints-and-validation\">Milestone Checkpoints and Validation</h3>\n<p><strong>Milestone 1: Image Processing Validation</strong></p>\n<p>The first milestone checkpoint validates core image processing functionality including format conversion, resizing operations, metadata handling, and thumbnail generation. This milestone establishes the foundation for media processing capabilities and must demonstrate reliable operation with various image formats and processing parameters.</p>\n<p>Validation focuses on algorithmic correctness, output quality, and proper handling of edge cases like extreme aspect ratios, unusual metadata, and format-specific requirements. The checkpoint ensures image processing operations produce consistent, high-quality results that meet specification requirements.</p>\n<blockquote>\n<p><strong>Milestone 1 Success Criteria</strong>: Image processing component handles all supported formats, produces outputs matching specifications, preserves or strips metadata as configured, and gracefully handles error conditions without crashing or corrupting data</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Validation Area</th>\n<th>Test Procedures</th>\n<th>Success Indicators</th>\n<th>Troubleshooting Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Format Detection</td>\n<td>Load images of each supported format</td>\n<td>Correct format identification for all files</td>\n<td>Check PIL/Pillow installation, verify file signatures</td>\n</tr>\n<tr>\n<td>Resize Operations</td>\n<td>Process images with various target dimensions</td>\n<td>Output dimensions match specifications with proper aspect ratio handling</td>\n<td>Validate resize algorithm selection, check interpolation settings</td>\n</tr>\n<tr>\n<td>Quality Settings</td>\n<td>Convert to lossy formats with different quality levels</td>\n<td>File size decreases appropriately, visual quality preserved</td>\n<td>Verify quality parameter mapping, check codec-specific settings</td>\n</tr>\n<tr>\n<td>Metadata Handling</td>\n<td>Process images with rich EXIF data</td>\n<td>Metadata preserved/stripped as configured, orientation handled correctly</td>\n<td>Test EXIF parsing library, validate rotation logic</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Process corrupted or invalid image files</td>\n<td>Graceful failure with descriptive error messages</td>\n<td>Check exception handling, validate error classification</td>\n</tr>\n</tbody></table>\n<p><strong>Checkpoint Commands and Expected Outputs</strong></p>\n<p>The milestone checkpoint includes specific commands to validate functionality and expected outputs that indicate successful implementation. These commands provide concrete validation steps that can be executed manually or integrated into automated testing pipelines.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Milestone 1 validation commands</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_image_processing.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/process_test_image.py</span><span style=\"color:#79B8FF\"> --input</span><span style=\"color:#9ECBFF\"> fixtures/test.jpg</span><span style=\"color:#79B8FF\"> --output</span><span style=\"color:#9ECBFF\"> /tmp/resized.webp</span><span style=\"color:#79B8FF\"> --width</span><span style=\"color:#79B8FF\"> 800</span><span style=\"color:#79B8FF\"> --quality</span><span style=\"color:#79B8FF\"> 85</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/extract_metadata.py</span><span style=\"color:#9ECBFF\"> fixtures/test_with_exif.jpg</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected outputs:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - All image processing tests pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Resized image created with correct dimensions and format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Metadata extracted and displayed properly formatted</span></span></code></pre></div>\n\n<table>\n<thead>\n<tr>\n<th>Command</th>\n<th>Expected Output</th>\n<th>Success Validation</th>\n<th>Common Issues</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>pytest tests/test_image_processing.py</code></td>\n<td>All tests pass, &lt;30 second execution</td>\n<td>Green test results, no failures</td>\n<td>Missing dependencies, incorrect test fixtures</td>\n</tr>\n<tr>\n<td>Image resize command</td>\n<td>Output file created with target dimensions</td>\n<td>File exists, correct size via <code>identify</code></td>\n<td>Memory errors, incorrect aspect ratio calculation</td>\n</tr>\n<tr>\n<td>Metadata extraction</td>\n<td>JSON output with EXIF fields</td>\n<td>GPS, orientation, camera data displayed</td>\n<td>EXIF library issues, binary data handling errors</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 2: Video Transcoding Validation</strong></p>\n<p>The second milestone validates video transcoding capabilities including format conversion, adaptive bitrate generation, thumbnail extraction, and FFmpeg integration. This milestone demonstrates the system&#39;s ability to handle complex video processing workflows with proper progress tracking and error recovery.</p>\n<p>Video processing validation requires longer execution times and more complex success criteria due to the computational complexity of video transcoding operations. The checkpoint ensures reliable FFmpeg integration, accurate progress reporting, and proper handling of various video formats and codecs.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Area</th>\n<th>Test Procedures</th>\n<th>Success Indicators</th>\n<th>Performance Expectations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Transcoding</td>\n<td>Convert MP4 to WebM, various resolutions</td>\n<td>Output plays correctly, maintains quality</td>\n<td>&lt;2x realtime for simple conversions</td>\n</tr>\n<tr>\n<td>ABR Generation</td>\n<td>Create HLS/DASH variants from source video</td>\n<td>Manifest files created, segments playable</td>\n<td>Multiple variants with proper bitrate ladder</td>\n</tr>\n<tr>\n<td>Progress Tracking</td>\n<td>Monitor transcoding progress updates</td>\n<td>Accurate percentage reporting throughout process</td>\n<td>Progress updates at least every 5 seconds</td>\n</tr>\n<tr>\n<td>Thumbnail Extraction</td>\n<td>Extract frames at various timestamps</td>\n<td>Thumbnail images created at correct times</td>\n<td>Frame accuracy within 1 second</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Process corrupted/unsupported video files</td>\n<td>Graceful failure, proper error classification</td>\n<td>Clear error messages, no worker crashes</td>\n</tr>\n</tbody></table>\n<p><strong>Checkpoint Validation Procedures</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Milestone 2 validation commands</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_video_transcoding.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/transcode_video.py</span><span style=\"color:#79B8FF\"> --input</span><span style=\"color:#9ECBFF\"> fixtures/test_video.mp4</span><span style=\"color:#79B8FF\"> --output</span><span style=\"color:#9ECBFF\"> /tmp/transcoded.webm</span><span style=\"color:#79B8FF\"> --resolution</span><span style=\"color:#9ECBFF\"> 720p</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/generate_abr.py</span><span style=\"color:#9ECBFF\"> fixtures/test_video.mp4</span><span style=\"color:#9ECBFF\"> /tmp/abr_output/</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/extract_thumbnails.py</span><span style=\"color:#9ECBFF\"> fixtures/test_video.mp4</span><span style=\"color:#9ECBFF\"> /tmp/thumbnails/</span><span style=\"color:#79B8FF\"> --timestamps</span><span style=\"color:#9ECBFF\"> 10,30,60</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Video transcoding completes successfully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - ABR variants created with manifest files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Thumbnail images extracted at correct timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Progress updates displayed during processing</span></span></code></pre></div>\n\n<table>\n<thead>\n<tr>\n<th>Validation Step</th>\n<th>Command</th>\n<th>Expected Result</th>\n<th>Troubleshooting</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Transcoding</td>\n<td><code>python scripts/transcode_video.py</code></td>\n<td>Output video file created, playable</td>\n<td>Check FFmpeg installation, validate input file</td>\n</tr>\n<tr>\n<td>ABR Generation</td>\n<td><code>python scripts/generate_abr.py</code></td>\n<td>Multiple resolution variants + manifest</td>\n<td>Verify HLS/DASH configuration, check segment creation</td>\n</tr>\n<tr>\n<td>Progress Monitoring</td>\n<td>Watch console during transcoding</td>\n<td>Regular progress updates, accurate percentages</td>\n<td>Check FFmpeg output parsing, validate progress calculation</td>\n</tr>\n<tr>\n<td>Thumbnail Extraction</td>\n<td><code>python scripts/extract_thumbnails.py</code></td>\n<td>Image files at specified timestamps</td>\n<td>Verify timestamp parsing, check frame extraction accuracy</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 3: Job Queue and Progress Validation</strong></p>\n<p>The final milestone validates complete asynchronous processing workflows including job queuing, worker coordination, progress tracking, and webhook notifications. This milestone demonstrates the system&#39;s ability to handle production workloads with reliable job processing and real-time progress updates.</p>\n<p>Validation encompasses the entire system integration including Redis queue operations, multiple worker processes, webhook delivery, and error recovery mechanisms. The checkpoint ensures the system can handle concurrent processing requests while maintaining job ordering, progress accuracy, and notification delivery reliability.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Area</th>\n<th>Test Procedures</th>\n<th>Success Indicators</th>\n<th>Scale Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Submission</td>\n<td>Submit multiple jobs with different priorities</td>\n<td>Jobs queued and processed in priority order</td>\n<td>Handle 100+ concurrent job submissions</td>\n</tr>\n<tr>\n<td>Worker Coordination</td>\n<td>Start multiple worker processes</td>\n<td>Jobs distributed fairly, no duplicate processing</td>\n<td>5+ workers processing simultaneously</td>\n</tr>\n<tr>\n<td>Progress Tracking</td>\n<td>Monitor real-time progress updates</td>\n<td>Accurate stage-based progress reporting</td>\n<td>Progress updates within 5-second intervals</td>\n</tr>\n<tr>\n<td>Webhook Delivery</td>\n<td>Configure webhook endpoints</td>\n<td>Notifications delivered reliably with retries</td>\n<td>99%+ delivery success rate</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Simulate worker failures, network issues</td>\n<td>Jobs complete despite infrastructure problems</td>\n<td>Automatic recovery within 30 seconds</td>\n</tr>\n</tbody></table>\n<p><strong>Complete System Validation</strong></p>\n<p>The final validation demonstrates end-to-end system functionality with realistic workloads and production-like conditions. This validation proves the system meets all functional and performance requirements established in the goals and non-goals section.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Milestone 3 complete system validation</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">docker-compose</span><span style=\"color:#9ECBFF\"> up</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> redis</span><span style=\"color:#9ECBFF\"> webhook-server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/integration/</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/submit_batch_jobs.py</span><span style=\"color:#79B8FF\"> --count</span><span style=\"color:#79B8FF\"> 20</span><span style=\"color:#79B8FF\"> --mixed-media</span><span style=\"color:#9ECBFF\"> fixtures/</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/monitor_queue_health.py</span><span style=\"color:#79B8FF\"> --duration</span><span style=\"color:#79B8FF\"> 300</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected system behavior:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - All integration tests pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Batch jobs processed successfully with progress tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Queue health monitoring shows stable operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># - Webhook notifications delivered reliably</span></span></code></pre></div>\n\n<table>\n<thead>\n<tr>\n<th>System Validation</th>\n<th>Command</th>\n<th>Success Criteria</th>\n<th>Performance Targets</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Integration Test Suite</td>\n<td><code>pytest tests/integration/</code></td>\n<td>All tests pass, &lt;5 minutes execution</td>\n<td>No test failures, consistent timing</td>\n</tr>\n<tr>\n<td>Batch Job Processing</td>\n<td><code>python scripts/submit_batch_jobs.py</code></td>\n<td>All jobs complete successfully</td>\n<td>&gt;10 concurrent jobs, fair processing</td>\n</tr>\n<tr>\n<td>Queue Health Monitoring</td>\n<td><code>python scripts/monitor_queue_health.py</code></td>\n<td>Stable metrics, no error accumulation</td>\n<td>&lt;100ms average queue latency</td>\n</tr>\n<tr>\n<td>Webhook Reliability</td>\n<td>Monitor webhook delivery logs</td>\n<td>&gt;99% delivery success, proper retries</td>\n<td>&lt;5 second delivery latency</td>\n</tr>\n</tbody></table>\n<p><strong>⚠️ Common Milestone Pitfalls</strong></p>\n<p><strong>Pitfall: Insufficient Test Media Coverage</strong>\nMany implementations fail milestone validation because test fixtures don&#39;t cover edge cases like unusual aspect ratios, extreme file sizes, or format-specific quirks. Always include problematic media files that exercise error handling paths and boundary conditions.</p>\n<p><strong>Pitfall: Race Conditions in Progress Updates</strong>\nProgress tracking often fails under concurrent load due to race conditions between progress updates and job completion. Implement proper sequence number validation and atomic update operations to prevent progress reversals and inconsistent state.</p>\n<p><strong>Pitfall: Webhook Delivery Assumptions</strong>\nWebhook notification testing frequently assumes reliable network conditions and responsive endpoints. Production environments have unreliable webhooks, so test with flaky network conditions, slow endpoints, and authentication failures to validate retry logic.</p>\n<p><strong>Pitfall: Resource Cleanup Failures</strong>\nFailed tests often leave temporary files, processes, or queue entries that interfere with subsequent test runs. Implement comprehensive cleanup procedures in test teardown methods and use isolated test environments to prevent cross-test contamination.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Test Framework</td>\n<td>pytest with basic fixtures</td>\n<td>pytest with custom plugins, parameterized tests</td>\n</tr>\n<tr>\n<td>Mock Library</td>\n<td>unittest.mock for basic mocking</td>\n<td>responses library for HTTP mocking, fakeredis for Redis</td>\n</tr>\n<tr>\n<td>Media Generation</td>\n<td>PIL solid color images</td>\n<td>FFmpeg synthetic video generation</td>\n</tr>\n<tr>\n<td>Test Orchestration</td>\n<td>Manual test execution</td>\n<td>Docker Compose with service containers</td>\n</tr>\n<tr>\n<td>Assertion Library</td>\n<td>Basic assert statements</td>\n<td>Custom media validation assertions</td>\n</tr>\n<tr>\n<td>Coverage Analysis</td>\n<td>pytest-cov for basic coverage</td>\n<td>Coverage with branch analysis and media-specific metrics</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media-processing-pipeline/\n├── tests/\n│   ├── unit/\n│   │   ├── test_image_processing.py      ← Image component unit tests\n│   │   ├── test_video_transcoding.py     ← Video component unit tests\n│   │   ├── test_job_queue.py             ← Queue operations unit tests\n│   │   └── test_progress_tracking.py     ← Progress component unit tests\n│   ├── integration/\n│   │   ├── test_end_to_end_workflows.py  ← Complete processing workflows\n│   │   ├── test_queue_integration.py     ← Queue with real Redis\n│   │   └── test_webhook_delivery.py      ← Webhook notification tests\n│   ├── fixtures/\n│   │   ├── images/                       ← Test image files\n│   │   ├── videos/                       ← Test video files\n│   │   └── generate_fixtures.py          ← Synthetic media generation\n│   ├── helpers/\n│   │   ├── test_media_utils.py           ← Media validation helpers\n│   │   ├── mock_servers.py               ← Mock webhook/API servers\n│   │   └── redis_test_utils.py           ← Redis testing utilities\n│   └── conftest.py                       ← pytest configuration and shared fixtures\n├── scripts/\n│   ├── validate_milestone_1.py           ← Milestone 1 validation script\n│   ├── validate_milestone_2.py           ← Milestone 2 validation script\n│   ├── validate_milestone_3.py           ← Milestone 3 validation script\n│   └── performance_benchmark.py          ← Performance testing script\n└── docker-compose.test.yml               ← Test environment services</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p><strong>Test Media Generation Utilities</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Test media generation utilities for creating synthetic test fixtures.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> tempfile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#79B8FF\"> PIL</span><span style=\"color:#F97583\"> import</span><span style=\"color:#E1E4E8\"> Image, ImageDraw</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Tuple, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestMediaGenerator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generates synthetic media files for testing purposes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_test_image</span><span style=\"color:#E1E4E8\">(width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, height: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, format: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'JPEG'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         color: Tuple[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">255</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)) -> </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a simple solid color test image in memory.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            width: Image width in pixels</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            height: Image height in pixels  </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            format: Output format (JPEG, PNG, WebP)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            color: RGB color tuple</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Image data as bytes</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        image </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Image.new(</span><span style=\"color:#9ECBFF\">'RGB'</span><span style=\"color:#E1E4E8\">, (width, height), color)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add some visual pattern for resize testing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        draw </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ImageDraw.Draw(image)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        draw.rectangle([width</span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, height</span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">width</span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">height</span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                      outline</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">255</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">255</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">255</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#FFAB70\">width</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Save to bytes buffer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> io</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buffer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> io.BytesIO()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        image.save(buffer, </span><span style=\"color:#FFAB70\">format</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">format</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">quality</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">85</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> format</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> 'JPEG'</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> buffer.getvalue()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_image_with_metadata</span><span style=\"color:#E1E4E8\">(width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, height: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, exif_data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create test image with embedded EXIF metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        from</span><span style=\"color:#79B8FF\"> PIL</span><span style=\"color:#E1E4E8\">.ExifTags </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> TAGS</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> piexif</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        image </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Image.new(</span><span style=\"color:#9ECBFF\">'RGB'</span><span style=\"color:#E1E4E8\">, (width, height), (</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">150</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">200</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert metadata dict to EXIF format</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        exif_dict </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"0th\"</span><span style=\"color:#E1E4E8\">: {}, </span><span style=\"color:#9ECBFF\">\"Exif\"</span><span style=\"color:#E1E4E8\">: {}, </span><span style=\"color:#9ECBFF\">\"GPS\"</span><span style=\"color:#E1E4E8\">: {}}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#9ECBFF\"> 'orientation'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> exif_data:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exif_dict[</span><span style=\"color:#9ECBFF\">\"0th\"</span><span style=\"color:#E1E4E8\">][piexif.ImageIFD.Orientation] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exif_data[</span><span style=\"color:#9ECBFF\">'orientation'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#9ECBFF\"> 'camera_make'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> exif_data:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exif_dict[</span><span style=\"color:#9ECBFF\">\"0th\"</span><span style=\"color:#E1E4E8\">][piexif.ImageIFD.Make] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exif_data[</span><span style=\"color:#9ECBFF\">'camera_make'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#9ECBFF\"> 'gps_latitude'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> exif_data </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> 'gps_longitude'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> exif_data:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exif_dict[</span><span style=\"color:#9ECBFF\">\"GPS\"</span><span style=\"color:#E1E4E8\">][piexif.</span><span style=\"color:#79B8FF\">GPSIFD</span><span style=\"color:#E1E4E8\">.GPSLatitude] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exif_data[</span><span style=\"color:#9ECBFF\">'gps_latitude'</span><span style=\"color:#E1E4E8\">] </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            exif_dict[</span><span style=\"color:#9ECBFF\">\"GPS\"</span><span style=\"color:#E1E4E8\">][piexif.</span><span style=\"color:#79B8FF\">GPSIFD</span><span style=\"color:#E1E4E8\">.GPSLongitude] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> exif_data[</span><span style=\"color:#9ECBFF\">'gps_longitude'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        exif_bytes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> piexif.dump(exif_dict)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        buffer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> io.BytesIO()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        image.save(buffer, </span><span style=\"color:#FFAB70\">format</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'JPEG'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exif</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">exif_bytes)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> buffer.getvalue()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MockWebhookServer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple HTTP server for testing webhook delivery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, port: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 8080</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.port </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> port</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.received_webhooks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.response_status </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 200</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.response_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_server</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start mock webhook server in background thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        from</span><span style=\"color:#E1E4E8\"> http.server </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> HTTPServer, BaseHTTPRequestHandler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        class</span><span style=\"color:#B392F0\"> WebhookHandler</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">BaseHTTPRequestHandler</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            def</span><span style=\"color:#B392F0\"> do_POST</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                content_length </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.headers.get(</span><span style=\"color:#9ECBFF\">'Content-Length'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                body </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.rfile.read(content_length)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Record received webhook</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.server.webhook_server.received_webhooks.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'path'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.path,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'headers'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.headers),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'body'</span><span style=\"color:#E1E4E8\">: body.decode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'timestamp'</span><span style=\"color:#E1E4E8\">: time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Simulate response delay</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.server.webhook_server.response_delay </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    time.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.server.webhook_server.response_delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Send configured response</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.send_response(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.server.webhook_server.response_status)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.send_header(</span><span style=\"color:#9ECBFF\">'Content-Type'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'application/json'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.end_headers()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.wfile.write(</span><span style=\"color:#F97583\">b</span><span style=\"color:#9ECBFF\">'{\"status\": \"received\"}'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        server </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HTTPServer((</span><span style=\"color:#9ECBFF\">'localhost'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.port), WebhookHandler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        server.webhook_server </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        def</span><span style=\"color:#B392F0\"> run_server</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            server.serve_forever()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.server_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">run_server, </span><span style=\"color:#FFAB70\">daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.server_thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_received_webhooks</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get list of received webhook requests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.received_webhooks.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> clear_webhooks</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clear received webhook history.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.received_webhooks.clear()</span></span></code></pre></div>\n\n<p><strong>Redis Testing Utilities</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Redis testing utilities for queue integration tests.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RedisTestManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages Redis connections and cleanup for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, host</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'localhost'</span><span style=\"color:#E1E4E8\">, port</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">6379</span><span style=\"color:#E1E4E8\">, db</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">15</span><span style=\"color:#E1E4E8\">):  </span><span style=\"color:#6A737D\"># Use high DB number for tests</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'host'</span><span style=\"color:#E1E4E8\">: host, </span><span style=\"color:#9ECBFF\">'port'</span><span style=\"color:#E1E4E8\">: port, </span><span style=\"color:#9ECBFF\">'db'</span><span style=\"color:#E1E4E8\">: db}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> redis_connection</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager providing clean Redis connection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis.Redis(</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.redis_config, </span><span style=\"color:#FFAB70\">decode_responses</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Clear test database</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.client.flushdb()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            yield</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Cleanup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.client.flushdb()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.client.close()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> wait_for_job_completion</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, timeout: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Wait for job to reach terminal state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> timeout:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.client.hgetall(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> job_data </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> job_data.get(</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'completed'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'failed'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> job_data.get(</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # Timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> simulate_worker_processing</span><span style=\"color:#E1E4E8\">(self, job_message: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Simulate worker processing a job for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job_message[</span><span style=\"color:#9ECBFF\">'job_id'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Mark job as processing</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"processing\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"started_at\"</span><span style=\"color:#E1E4E8\">, time.time())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Simulate progress updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> progress </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">25</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">75</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            time.sleep(</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Brief processing simulation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.client.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"progress_percentage\"</span><span style=\"color:#E1E4E8\">, progress)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Mark as completed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"completed\"</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.client.hset(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"job:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"completed_at\"</span><span style=\"color:#E1E4E8\">, time.time())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<p><strong>Image Processing Test Structure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Image processing unit test skeletons.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#79B8FF\"> PIL</span><span style=\"color:#F97583\"> import</span><span style=\"color:#E1E4E8\"> Image</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> media_processing.image_processor </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ImageProcessor, ImageProcessingConfig</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestImageProcessing</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Unit tests for image processing operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> image_processor</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create ImageProcessor instance for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ImageProcessor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.fixture</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_image_data</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate test image data for consistent testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use TestMediaGenerator to create 100x100 RGB test image</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return image data as bytes for loading tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_resize_preserve_aspect_ratio</span><span style=\"color:#E1E4E8\">(self, image_processor, test_image_data):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test image resizing with aspect ratio preservation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load test image from bytes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create ImageProcessingConfig with target dimensions 200x100</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call resize_image() with PRESERVE_ASPECT resize mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify output dimensions maintain original aspect ratio</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that image quality is preserved after resize</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_format_conversion_quality_settings</span><span style=\"color:#E1E4E8\">(self, image_processor, test_image_data):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test format conversion with different quality settings.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load test image as PIL Image object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Convert to JPEG with quality=95, measure file size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Convert to JPEG with quality=50, measure file size  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Assert that lower quality produces smaller file size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify both outputs are valid JPEG images</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_exif_orientation_handling</span><span style=\"color:#E1E4E8\">(self, image_processor):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test proper EXIF orientation rotation.\"\"\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create test image with EXIF orientation=6 (90° rotation)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Process image with metadata handling enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify image is rotated correctly based on EXIF data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that output image orientation is normalized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use TestMediaGenerator.create_image_with_metadata()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_metadata_privacy_stripping</span><span style=\"color:#E1E4E8\">(self, image_processor):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test removal of privacy-sensitive metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create image with GPS coordinates and camera data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Process with metadata_mode='strip_privacy'</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify GPS data is removed from output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that basic image data (dimensions, format) is preserved</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_memory_limit_validation</span><span style=\"color:#E1E4E8\">(self, image_processor):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test memory requirement calculation for large images.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate memory needed for 8000x8000 RGB image</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call check_image_memory_requirements()  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify calculation matches expected bytes (width * height * channels)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test with different color modes (RGB, RGBA, CMYK)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_corrupted_image_error_handling</span><span style=\"color:#E1E4E8\">(self, image_processor):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test graceful handling of corrupted image files.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create corrupted image data (truncated JPEG header)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Attempt to load corrupted image</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify specific exception type is raised</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check error message contains useful diagnostic information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Integration Test Structure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Integration test skeletons for complete workflows.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> media_processing.api </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> submit_job, ProcessingJob</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> media_processing.queue </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JobStatus</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestEndToEndWorkflows</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Integration tests for complete media processing workflows.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> redis_test_manager</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set up Redis test environment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RedisTestManager()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> manager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> mock_webhook_server</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start mock webhook server for notification testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize MockWebhookServer on port 8080</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Start server in background thread</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return server instance for test assertions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.mark.integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_image_processing_complete_workflow</span><span style=\"color:#E1E4E8\">(self, redis_test_manager, mock_webhook_server):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test complete image processing from submission to completion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> redis_test_manager.redis_connection() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> redis:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create test image file in temporary directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Submit job with multiple output specifications  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Start worker process to handle job</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Wait for job completion using redis_test_manager.wait_for_job_completion()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify all output files exist with correct formats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check webhook notification was delivered</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate job status is COMPLETED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.mark.integration</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_video_transcoding_with_progress</span><span style=\"color:#E1E4E8\">(self, redis_test_manager):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test video transcoding with progress tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> redis_test_manager.redis_connection() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> redis:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create short test video using FFmpeg</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Submit transcoding job with ABR variants</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Monitor progress updates during processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify progress moves through expected stages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check final output includes HLS manifest and segments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate video segments are playable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.mark.integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_concurrent_job_processing</span><span style=\"color:#E1E4E8\">(self, redis_test_manager):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test multiple jobs processed concurrently.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> redis_test_manager.redis_connection() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> redis:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Submit 10 image processing jobs simultaneously</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Start 3 worker processes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Monitor job completion across all workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify all jobs complete successfully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check jobs are distributed fairly across workers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate no duplicate processing occurs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @pytest.mark.integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_webhook_delivery_retry_logic</span><span style=\"color:#E1E4E8\">(self, mock_webhook_server):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test webhook notification retry on delivery failures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure webhook server to return HTTP 503 initially</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Submit job with webhook notification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Wait for first webhook delivery attempt (should fail)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Configure server to return HTTP 200</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify webhook is eventually delivered via retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check exponential backoff timing between retries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li><strong>PIL/Pillow</strong>: Use <code>Image.save()</code> with <code>optimize=True</code> for smaller file sizes</li>\n<li><strong>pytest fixtures</strong>: Use <code>@pytest.fixture(scope=&quot;session&quot;)</code> for expensive setup like Redis connections</li>\n<li><strong>Temporary files</strong>: Use <code>tempfile.TemporaryDirectory()</code> context manager for automatic cleanup</li>\n<li><strong>Redis testing</strong>: Use high database numbers (10-15) to avoid conflicts with development data</li>\n<li><strong>FFmpeg subprocess</strong>: Use <code>subprocess.Popen()</code> with <code>PIPE</code> for stdout/stderr capture</li>\n<li><strong>Threading</strong>: Use <code>threading.Event()</code> for coordinating between test threads and background workers</li>\n<li><strong>Mock HTTP</strong>: <code>responses</code> library provides easier HTTP mocking than <code>unittest.mock</code></li>\n<li><strong>Binary data</strong>: Use <code>bytes()</code> and <code>io.BytesIO()</code> for in-memory binary file simulation</li>\n</ul>\n<p><strong>F. Milestone Checkpoints</strong></p>\n<p><strong>Milestone 1 Checkpoint</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run these commands to validate Milestone 1 completion</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">cd</span><span style=\"color:#9ECBFF\"> media-processing-pipeline</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_image_processing.py::TestImageProcessing</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/validate_milestone_1.py</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: All image processing tests pass, validation script reports success</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Signs of problems: PIL import errors, memory issues with large images, EXIF orientation bugs</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint</strong> </p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run these commands to validate Milestone 2 completion</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_video_transcoding.py::TestVideoTranscoding</span><span style=\"color:#79B8FF\"> -v</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/validate_milestone_2.py</span><span style=\"color:#79B8FF\"> --input</span><span style=\"color:#9ECBFF\"> fixtures/test_video.mp4</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Video transcoding tests pass, ABR variants generated correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Signs of problems: FFmpeg not found, progress parsing failures, codec errors</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run these commands to validate Milestone 3 completion</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">docker-compose</span><span style=\"color:#79B8FF\"> -f</span><span style=\"color:#9ECBFF\"> docker-compose.test.yml</span><span style=\"color:#9ECBFF\"> up</span><span style=\"color:#79B8FF\"> -d</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/integration/</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/validate_milestone_3.py</span><span style=\"color:#79B8FF\"> --concurrent-jobs</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: All integration tests pass, concurrent processing works correctly  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Signs of problems: Redis connection failures, webhook delivery issues, race conditions in progress tracking</span></span></code></pre></div>\n\n\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as debugging skills are essential for troubleshooting image processing, video transcoding, and job queue issues across the entire system</p>\n</blockquote>\n<h3 id=\"mental-model-hospital-emergency-room\">Mental Model: Hospital Emergency Room</h3>\n<p>Think of debugging a media processing pipeline as running a hospital emergency room. When a patient (processing job) arrives with symptoms (failed processing, stuck progress, resource exhaustion), you need to quickly diagnose the root cause from observable symptoms, apply the right diagnostic tools, and implement targeted treatment while preventing the problem from affecting other patients.</p>\n<p>Just as emergency room doctors follow systematic triage protocols—checking vital signs, running targeted tests, consulting specialists—effective media processing debugging requires structured approaches to symptom identification, diagnostic tool usage, and performance analysis. The goal is rapid diagnosis followed by precise intervention that resolves the immediate issue while preventing future occurrences.</p>\n<p>The emergency room analogy extends to resource management: you must balance immediate patient needs with overall system capacity, handle multiple concurrent cases without cross-contamination, and maintain detailed records for pattern recognition and quality improvement.</p>\n<h3 id=\"common-symptoms-and-diagnosis\">Common Symptoms and Diagnosis</h3>\n<p>Effective debugging starts with systematic symptom-to-cause mapping that enables rapid problem identification and resolution. The media processing pipeline exhibits distinct failure patterns that correspond to specific underlying issues across image processing, video transcoding, and job queue management.</p>\n<p><strong>Job Queue and Worker Coordination Issues</strong></p>\n<p>The job queue represents the central nervous system of the media processing pipeline. When queue operations fail, the symptoms manifest in predictable patterns that reveal the underlying coordination problems.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Observable Behavior</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Jobs stuck in PENDING forever</td>\n<td><code>ProcessingJob.status</code> remains PENDING, no worker picks up jobs</td>\n<td>Worker processes crashed or not starting</td>\n<td>Check worker process status, verify Redis connectivity, examine worker logs for startup errors</td>\n</tr>\n<tr>\n<td>Progress updates stop mid-processing</td>\n<td>Job shows 45% complete but no further updates for &gt;5 minutes</td>\n<td>Worker crashed during processing without cleanup</td>\n<td>Search logs for worker PID, check for OOM kills, verify temp file cleanup</td>\n</tr>\n<tr>\n<td>Duplicate job processing</td>\n<td>Same input file processed multiple times concurrently</td>\n<td>Race condition in job deduplication logic</td>\n<td>Check Redis atomic operations, verify <code>job_id</code> generation uniqueness</td>\n</tr>\n<tr>\n<td>High-priority jobs processed after low-priority</td>\n<td>Urgent jobs wait behind normal priority jobs</td>\n<td>Priority queue implementation bug</td>\n<td>Examine Redis ZADD/ZPOP operations, verify priority score calculation</td>\n</tr>\n<tr>\n<td>Memory exhaustion crashes</td>\n<td>Worker processes killed with exit code 137 (SIGKILL)</td>\n<td>Large media files exceeding worker memory limits</td>\n<td>Monitor memory usage patterns, check file size validation</td>\n</tr>\n</tbody></table>\n<p><strong>Image Processing Failures</strong></p>\n<p>Image processing failures typically stem from format incompatibilities, memory constraints, or metadata handling issues. The failure patterns reveal specific problems in the image manipulation pipeline.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Observable Behavior</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Images rotated incorrectly</td>\n<td>Output images appear sideways or upside-down</td>\n<td>EXIF orientation tag not processed before resize</td>\n<td>Check input EXIF data, verify <code>ImageMetadata.orientation</code> handling</td>\n</tr>\n<tr>\n<td>Corrupted output files</td>\n<td>Generated images cannot be opened or display artifacts</td>\n<td>Memory corruption during processing or invalid format conversion</td>\n<td>Test with smaller images, check memory allocation patterns</td>\n</tr>\n<tr>\n<td>WebP conversion fails silently</td>\n<td>WebP format specified but JPEG output produced</td>\n<td>Pillow WebP support not installed or misconfigured</td>\n<td>Test WebP encoding capability, verify Pillow installation</td>\n</tr>\n<tr>\n<td>Thumbnail generation produces black images</td>\n<td>Thumbnails created but contain no visible content</td>\n<td>Crop coordinates exceed image boundaries</td>\n<td>Log crop calculations, verify smart cropping algorithm</td>\n</tr>\n<tr>\n<td>EXIF privacy stripping incomplete</td>\n<td>Output images still contain GPS coordinates</td>\n<td>Metadata handling configuration error</td>\n<td>Audit metadata extraction, test privacy stripping modes</td>\n</tr>\n</tbody></table>\n<p><strong>Video Transcoding and FFmpeg Issues</strong></p>\n<p>Video transcoding problems often involve FFmpeg integration issues, codec compatibility, or resource management failures during long-running processing operations.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Observable Behavior</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>FFmpeg process hangs indefinitely</td>\n<td>Video transcoding never completes, no progress updates</td>\n<td>FFmpeg waiting for input or misconfigured parameters</td>\n<td>Check FFmpeg command construction, verify input file accessibility</td>\n</tr>\n<tr>\n<td>Transcoding fails with &quot;Codec not found&quot;</td>\n<td>FFmpeg exits with codec-related error messages</td>\n<td>Target codec not supported in FFmpeg build</td>\n<td>Test codec availability, check FFmpeg compilation flags</td>\n</tr>\n<tr>\n<td>HLS segments not playable</td>\n<td>Individual .ts files generated but playlist broken</td>\n<td>Segment alignment or manifest generation issues</td>\n<td>Validate HLS manifest syntax, test segment playback individually</td>\n</tr>\n<tr>\n<td>Audio sync issues in output</td>\n<td>Video and audio streams misaligned in transcoded output</td>\n<td>Stream mapping or timing configuration problems</td>\n<td>Examine input metadata, verify audio/video synchronization settings</td>\n</tr>\n<tr>\n<td>Progress calculation wildly inaccurate</td>\n<td>Progress jumps from 10% to 90% instantly</td>\n<td>FFmpeg duration parsing failed or progress regex incorrect</td>\n<td>Test duration extraction, validate progress parsing logic</td>\n</tr>\n</tbody></table>\n<p><strong>Webhook and Notification Problems</strong></p>\n<p>Webhook delivery failures create visibility gaps that make it difficult to track job completion and handle errors appropriately in client applications.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Observable Behavior</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Webhooks never delivered</td>\n<td>Client never receives job completion notifications</td>\n<td>Network connectivity or webhook URL configuration issues</td>\n<td>Test webhook URL accessibility, verify HMAC signature generation</td>\n</tr>\n<tr>\n<td>Webhook delivery storms</td>\n<td>Client receives hundreds of duplicate notifications</td>\n<td>Retry logic not tracking successful deliveries</td>\n<td>Check webhook delivery tracking, audit retry backoff implementation</td>\n</tr>\n<tr>\n<td>Authentication failures</td>\n<td>Client rejects webhooks with signature errors</td>\n<td>HMAC signature mismatch or clock skew</td>\n<td>Verify shared secret configuration, test signature generation</td>\n</tr>\n<tr>\n<td>Progress webhooks out of order</td>\n<td>Client receives 75% progress after 100% completion</td>\n<td>Race conditions in progress update delivery</td>\n<td>Check sequence number implementation, verify atomic progress updates</td>\n</tr>\n<tr>\n<td>Webhook timeouts</td>\n<td>Webhook delivery fails with timeout errors</td>\n<td>Client webhook endpoint slow or unresponsive</td>\n<td>Monitor webhook response times, implement client-side timeout handling</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Structured Diagnostic Approach</strong></p>\n<ul>\n<li><strong>Context</strong>: Media processing involves complex interactions between multiple components, making ad-hoc debugging ineffective and time-consuming</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Reactive debugging responding to individual symptoms</li>\n<li>Comprehensive logging with manual correlation</li>\n<li>Structured symptom-to-cause mapping with standardized diagnostic procedures</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement structured diagnostic approach with symptom classification</li>\n<li><strong>Rationale</strong>: Systematic diagnosis reduces mean-time-to-resolution and enables junior developers to handle complex debugging scenarios effectively</li>\n<li><strong>Consequences</strong>: Requires upfront investment in diagnostic procedures but dramatically improves debugging efficiency and reduces escalation requirements</li>\n</ul>\n</blockquote>\n<p><img src=\"/api/project/media-processing/architecture-doc/asset?path=diagrams%2Ferror-recovery-paths.svg\" alt=\"Error Recovery Decision Tree\"></p>\n<p><strong>Redis and Storage Layer Issues</strong></p>\n<p>The storage layer provides persistence for job state, progress tracking, and metadata. Storage failures cascade through the entire system and require careful diagnosis to prevent data loss.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Observable Behavior</th>\n<th>Likely Root Cause</th>\n<th>Diagnostic Steps</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job state inconsistencies</td>\n<td>Job shows COMPLETED in Redis but PROCESSING in PostgreSQL</td>\n<td>Hybrid storage synchronization failure</td>\n<td>Compare Redis and PostgreSQL state, check transaction boundaries</td>\n</tr>\n<tr>\n<td>Progress updates rejected</td>\n<td>Progress tracking returns false from update operations</td>\n<td>Sequence number conflicts or atomic operation failures</td>\n<td>Check Redis transaction logs, verify sequence number generation</td>\n</tr>\n<tr>\n<td>File cleanup failures</td>\n<td>Temporary files accumulate without deletion</td>\n<td>Error handling not cleaning up resources properly</td>\n<td>Audit file lifecycle management, check error handler cleanup paths</td>\n</tr>\n<tr>\n<td>Storage quota exceeded</td>\n<td>Jobs fail with disk space errors</td>\n<td>Temporary file cleanup not working or storage monitoring absent</td>\n<td>Check disk usage patterns, verify cleanup job execution</td>\n</tr>\n<tr>\n<td>Redis connection pool exhausted</td>\n<td>New jobs cannot be queued with connection timeout errors</td>\n<td>Connection leaks or insufficient pool sizing</td>\n<td>Monitor Redis connection usage, check connection cleanup</td>\n</tr>\n</tbody></table>\n<h3 id=\"debugging-tools-and-techniques\">Debugging Tools and Techniques</h3>\n<p>Effective debugging requires a comprehensive toolkit that provides visibility into system behavior, enables rapid problem isolation, and supports root cause analysis across distributed components.</p>\n<h3 id=\"mental-model-detective-investigation-kit\">Mental Model: Detective Investigation Kit</h3>\n<p>Think of debugging tools as a detective&#39;s investigation kit. Just as detectives use fingerprints for identity verification, surveillance cameras for timeline reconstruction, and forensic analysis for evidence correlation, media processing debugging requires specialized tools for different types of evidence collection and analysis.</p>\n<p>Each tool serves a specific investigative purpose: logs provide timeline evidence, metrics reveal behavioral patterns, tracing shows interaction sequences, and profiling exposes performance bottlenecks. The key is knowing which tool to use for each type of investigation and how to correlate evidence across multiple sources.</p>\n<p><strong>Structured Logging Implementation</strong></p>\n<p>Structured logging forms the foundation of effective debugging by providing consistent, searchable, and correlatable log data across all system components. The logging strategy must balance information richness with performance impact.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Log Level</th>\n<th>Required Fields</th>\n<th>Sample Message Structure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>API Gateway</td>\n<td>INFO, WARN, ERROR</td>\n<td>request_id, job_id, user_id, endpoint, duration_ms</td>\n<td><code>{&quot;timestamp&quot;: &quot;2024-01-15T10:30:45Z&quot;, &quot;level&quot;: &quot;INFO&quot;, &quot;component&quot;: &quot;api&quot;, &quot;request_id&quot;: &quot;req_123&quot;, &quot;job_id&quot;: &quot;job_456&quot;, &quot;message&quot;: &quot;Job submitted successfully&quot;}</code></td>\n</tr>\n<tr>\n<td>Job Queue</td>\n<td>DEBUG, INFO, ERROR</td>\n<td>job_id, worker_id, queue_depth, processing_time</td>\n<td><code>{&quot;timestamp&quot;: &quot;2024-01-15T10:31:00Z&quot;, &quot;level&quot;: &quot;DEBUG&quot;, &quot;component&quot;: &quot;queue&quot;, &quot;job_id&quot;: &quot;job_456&quot;, &quot;worker_id&quot;: &quot;worker_01&quot;, &quot;queue_depth&quot;: 23, &quot;message&quot;: &quot;Job dequeued for processing&quot;}</code></td>\n</tr>\n<tr>\n<td>Image Processor</td>\n<td>INFO, WARN, ERROR</td>\n<td>job_id, input_file, output_specs, memory_usage_mb</td>\n<td><code>{&quot;timestamp&quot;: &quot;2024-01-15T10:31:15Z&quot;, &quot;level&quot;: &quot;INFO&quot;, &quot;component&quot;: &quot;image&quot;, &quot;job_id&quot;: &quot;job_456&quot;, &quot;input_file&quot;: &quot;photo.jpg&quot;, &quot;memory_usage_mb&quot;: 450, &quot;message&quot;: &quot;Image resize completed&quot;}</code></td>\n</tr>\n<tr>\n<td>Video Processor</td>\n<td>INFO, WARN, ERROR</td>\n<td>job_id, ffmpeg_command, progress_percentage, estimated_remaining</td>\n<td><code>{&quot;timestamp&quot;: &quot;2024-01-15T10:32:30Z&quot;, &quot;level&quot;: &quot;INFO&quot;, &quot;component&quot;: &quot;video&quot;, &quot;job_id&quot;: &quot;job_456&quot;, &quot;progress_percentage&quot;: 45.2, &quot;estimated_remaining&quot;: &quot;2m30s&quot;, &quot;message&quot;: &quot;Transcoding progress update&quot;}</code></td>\n</tr>\n<tr>\n<td>Progress Tracker</td>\n<td>DEBUG, INFO, ERROR</td>\n<td>job_id, stage, webhook_url, delivery_attempt</td>\n<td><code>{&quot;timestamp&quot;: &quot;2024-01-15T10:35:45Z&quot;, &quot;level&quot;: &quot;INFO&quot;, &quot;component&quot;: &quot;progress&quot;, &quot;job_id&quot;: &quot;job_456&quot;, &quot;stage&quot;: &quot;FORMAT_CONVERSION&quot;, &quot;webhook_url&quot;: &quot;https://client.example.com/webhooks&quot;, &quot;delivery_attempt&quot;: 1, &quot;message&quot;: &quot;Webhook delivered successfully&quot;}</code></td>\n</tr>\n</tbody></table>\n<p>The <code>job_logging_context</code> function provides correlation across all log messages for a specific job, enabling end-to-end request tracing through the entire processing pipeline. This correlation proves essential when debugging complex failure scenarios that span multiple components.</p>\n<p><strong>Monitoring and Metrics Collection</strong></p>\n<p>Real-time metrics provide early warning systems for performance degradation and resource constraints before they escalate to critical failures.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Key Metrics</th>\n<th>Collection Frequency</th>\n<th>Alert Thresholds</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queue Health</td>\n<td>Queue depth, processing rate, worker utilization</td>\n<td>Every 30 seconds</td>\n<td>Queue depth &gt; 1000, processing rate &lt; 10 jobs/minute</td>\n</tr>\n<tr>\n<td>Resource Usage</td>\n<td>Memory consumption, CPU utilization, disk usage</td>\n<td>Every 10 seconds</td>\n<td>Memory &gt; 80%, CPU &gt; 90%, disk &gt; 85%</td>\n</tr>\n<tr>\n<td>Processing Performance</td>\n<td>Job completion time, error rates, format-specific metrics</td>\n<td>Per job completion</td>\n<td>Error rate &gt; 5%, completion time &gt; 2x average</td>\n</tr>\n<tr>\n<td>Webhook Delivery</td>\n<td>Delivery success rate, retry frequency, response times</td>\n<td>Per webhook attempt</td>\n<td>Success rate &lt; 95%, response time &gt; 5 seconds</td>\n</tr>\n<tr>\n<td>Storage Operations</td>\n<td>Redis operations/second, PostgreSQL query time, file I/O rates</td>\n<td>Every 15 seconds</td>\n<td>Query time &gt; 100ms, Redis ops/sec &gt; 10000</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Metrics vs Logs Balance</strong></p>\n<ul>\n<li><strong>Context</strong>: Need visibility into system behavior without overwhelming storage or affecting performance</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Comprehensive logging of all operations</li>\n<li>Metrics-only approach with minimal logging</li>\n<li>Balanced approach with structured logs and targeted metrics</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Balanced approach with job-level tracing and system-level metrics</li>\n<li><strong>Rationale</strong>: Logs provide detailed debugging context while metrics enable proactive monitoring and alerting</li>\n<li><strong>Consequences</strong>: Requires careful log level management and metrics aggregation but provides comprehensive observability</li>\n</ul>\n</blockquote>\n<p><strong>Diagnostic Command-Line Tools</strong></p>\n<p>Command-line diagnostic tools enable rapid system inspection during incident response and provide detailed component-level debugging capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Tool Purpose</th>\n<th>Command Examples</th>\n<th>Expected Output</th>\n<th>Usage Scenarios</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queue Status</td>\n<td><code>redis-cli LLEN media:queue:high</code></td>\n<td><code>23</code> (queue depth)</td>\n<td>Check job backlog during performance issues</td>\n</tr>\n<tr>\n<td>Worker Health</td>\n<td><code>ps aux | grep media_worker</code></td>\n<td>Process list with memory usage</td>\n<td>Verify worker processes running correctly</td>\n</tr>\n<tr>\n<td>Job Inspection</td>\n<td><code>redis-cli HGETALL job:job_456</code></td>\n<td>Job status and metadata</td>\n<td>Debug specific job failure</td>\n</tr>\n<tr>\n<td>Progress Check</td>\n<td><code>redis-cli GET progress:job_456</code></td>\n<td>JSON progress object</td>\n<td>Verify progress tracking accuracy</td>\n</tr>\n<tr>\n<td>File Validation</td>\n<td><code>ffprobe -v quiet -print_format json input.mp4</code></td>\n<td>Video metadata JSON</td>\n<td>Validate input file before processing</td>\n</tr>\n<tr>\n<td>Storage Usage</td>\n<td><code>du -sh /tmp/media_processing/*</code></td>\n<td>Directory size breakdown</td>\n<td>Check temporary file cleanup</td>\n</tr>\n</tbody></table>\n<p><strong>Log Analysis and Correlation Techniques</strong></p>\n<p>Effective log analysis requires tools and techniques that can correlate events across multiple components and identify patterns in large log volumes.</p>\n<p>The correlation strategy relies on consistent <code>job_id</code> and <code>request_id</code> fields that enable end-to-end tracing through the processing pipeline. Advanced log analysis uses tools like <code>grep</code>, <code>jq</code>, and <code>awk</code> for command-line investigation combined with log aggregation platforms for pattern recognition.</p>\n<p>Sample correlation commands demonstrate how to trace a specific job through the entire system:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Extract all log entries for specific job</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">grep</span><span style=\"color:#9ECBFF\"> \"job_456\"</span><span style=\"color:#9ECBFF\"> /var/log/media_processing/</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#9ECBFF\">.log</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Analyze webhook delivery patterns</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">jq</span><span style=\"color:#9ECBFF\"> 'select(.component == \"progress\" and .delivery_attempt > 1)'</span><span style=\"color:#9ECBFF\"> /var/log/media_processing/webhooks.log</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Find memory-related errors</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">grep</span><span style=\"color:#79B8FF\"> -E</span><span style=\"color:#9ECBFF\"> \"(OOM|memory|killed)\"</span><span style=\"color:#9ECBFF\"> /var/log/media_processing/</span><span style=\"color:#79B8FF\">*</span><span style=\"color:#9ECBFF\">.log</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> head</span><span style=\"color:#79B8FF\"> -20</span></span></code></pre></div>\n\n<p><strong>Integration with External Monitoring</strong></p>\n<p>External monitoring tools provide centralized visibility across distributed deployments and enable automated alerting based on system behavior patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Integration Type</th>\n<th>Tool Examples</th>\n<th>Configuration Requirements</th>\n<th>Benefits</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Log Aggregation</td>\n<td>ELK Stack, Splunk, CloudWatch</td>\n<td>Structured JSON output, log shipping</td>\n<td>Centralized search, pattern analysis</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Prometheus + Grafana, DataDog</td>\n<td>StatsD or HTTP endpoints</td>\n<td>Real-time dashboards, historical analysis</td>\n</tr>\n<tr>\n<td>Alerting Systems</td>\n<td>PagerDuty, Slack notifications</td>\n<td>Webhook integration</td>\n<td>Automated incident response</td>\n</tr>\n<tr>\n<td>APM Tools</td>\n<td>Jaeger, Zipkin, New Relic</td>\n<td>Distributed tracing headers</td>\n<td>End-to-end request visibility</td>\n</tr>\n<tr>\n<td>Health Checks</td>\n<td>Consul, etcd, custom endpoints</td>\n<td>HTTP health check endpoints</td>\n<td>Service discovery and load balancing</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Log Volume Overwhelming Storage</strong>\nMany implementations generate excessive log volume that fills disk space and makes analysis difficult. The solution requires log level configuration based on environment (DEBUG in development, INFO in production) and log rotation policies that balance retention with storage constraints. Use sampling for high-volume DEBUG messages and ensure log rotation prevents disk exhaustion.</p>\n<h3 id=\"performance-and-resource-debugging\">Performance and Resource Debugging</h3>\n<p>Performance debugging in media processing systems requires specialized techniques that account for the unique characteristics of multimedia workloads, including variable processing times, large memory requirements, and I/O-intensive operations.</p>\n<h3 id=\"mental-model-formula-one-pit-crew\">Mental Model: Formula One Pit Crew</h3>\n<p>Think of performance debugging as running a Formula One pit crew during a race. The pit crew must quickly diagnose performance issues (slow lap times, tire wear, fuel consumption) while the race continues, identify bottlenecks that prevent optimal performance, and implement targeted optimizations without disrupting ongoing operations.</p>\n<p>Just as pit crews use telemetry data to understand car performance, real-time metrics reveal system bottlenecks. Like mechanics who know that tire temperature affects grip and aerodynamics impact fuel efficiency, media processing performance debugging requires understanding how memory usage affects processing speed, how file size impacts I/O patterns, and how codec selection influences CPU utilization.</p>\n<p><strong>Memory Usage Analysis and Optimization</strong></p>\n<p>Memory management presents the primary performance challenge in media processing due to the large working sets required for high-resolution images and video frames. Memory debugging must identify both peak usage patterns and memory leaks that accumulate over time.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Issue Type</th>\n<th>Symptoms</th>\n<th>Diagnostic Approach</th>\n<th>Optimization Strategies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Peak Memory Spikes</td>\n<td>OOM kills during large file processing</td>\n<td>Monitor memory usage per job, profile allocation patterns</td>\n<td>Implement streaming processing, reduce buffer sizes</td>\n</tr>\n<tr>\n<td>Memory Leaks</td>\n<td>Gradual memory growth over time</td>\n<td>Track memory usage trends, identify allocation without deallocation</td>\n<td>Audit resource cleanup, use memory profiling tools</td>\n</tr>\n<tr>\n<td>Fragmentation</td>\n<td>Available memory but allocation failures</td>\n<td>Monitor memory fragmentation metrics</td>\n<td>Implement object pooling, use consistent buffer sizes</td>\n</tr>\n<tr>\n<td>Buffer Overflow</td>\n<td>Crashes or corruption during processing</td>\n<td>Memory sanitizers, bounds checking</td>\n<td>Validate input sizes, implement memory limits</td>\n</tr>\n<tr>\n<td>Swap Thrashing</td>\n<td>Extremely slow processing with high disk I/O</td>\n<td>Monitor swap usage and page faults</td>\n<td>Reduce memory footprint, increase available RAM</td>\n</tr>\n</tbody></table>\n<p>Memory profiling for image processing requires understanding the relationship between image dimensions and memory requirements. A 4K image (3840x2160) in RGBA format requires approximately 33MB of uncompressed memory, while video processing may require multiple frame buffers simultaneously.</p>\n<p><strong>Processing Time Optimization Strategies</strong></p>\n<p>Processing time optimization requires identifying bottlenecks in the media processing pipeline and implementing targeted improvements that maintain quality while reducing latency.</p>\n<table>\n<thead>\n<tr>\n<th>Processing Stage</th>\n<th>Common Bottlenecks</th>\n<th>Profiling Techniques</th>\n<th>Optimization Options</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image Loading</td>\n<td>File I/O and format parsing</td>\n<td>Time file read operations, profile decoder performance</td>\n<td>Implement format-specific optimizations, use memory-mapped files</td>\n</tr>\n<tr>\n<td>Resize Operations</td>\n<td>Interpolation algorithm complexity</td>\n<td>Compare algorithm performance, measure CPU usage</td>\n<td>Choose optimal interpolation method per use case</td>\n</tr>\n<tr>\n<td>Format Conversion</td>\n<td>Encoding complexity and quality settings</td>\n<td>Profile encoder performance, measure compression ratios</td>\n<td>Optimize quality settings, use hardware acceleration</td>\n</tr>\n<tr>\n<td>Video Transcoding</td>\n<td>CPU-intensive encoding operations</td>\n<td>Monitor CPU utilization, measure encoding speed</td>\n<td>Implement hardware acceleration, optimize FFmpeg parameters</td>\n</tr>\n<tr>\n<td>I/O Operations</td>\n<td>Disk bandwidth and latency</td>\n<td>Monitor disk usage patterns, measure throughput</td>\n<td>Use SSD storage, implement async I/O</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Processing Time vs Quality Trade-offs</strong></p>\n<ul>\n<li><strong>Context</strong>: Media processing involves inherent trade-offs between processing speed, output quality, and resource consumption</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Optimize for maximum quality regardless of processing time</li>\n<li>Optimize for minimum processing time with acceptable quality loss</li>\n<li>Implement configurable quality/speed profiles based on use case</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Configurable quality profiles with performance-oriented defaults</li>\n<li><strong>Rationale</strong>: Different use cases have different requirements; thumbnails need speed while archival processing needs quality</li>\n<li><strong>Consequences</strong>: Requires careful profile configuration and testing but provides optimal performance for each use case</li>\n</ul>\n</blockquote>\n<p><strong>Worker Bottleneck Identification</strong></p>\n<p>Worker bottleneck analysis identifies coordination issues that prevent optimal resource utilization and job throughput across the distributed processing system.</p>\n<table>\n<thead>\n<tr>\n<th>Bottleneck Type</th>\n<th>Observable Symptoms</th>\n<th>Root Cause Analysis</th>\n<th>Resolution Approaches</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queue Starvation</td>\n<td>Workers idle while jobs remain queued</td>\n<td>Examine job distribution logic, check priority queue implementation</td>\n<td>Improve job distribution algorithm, rebalance worker assignments</td>\n</tr>\n<tr>\n<td>Resource Contention</td>\n<td>Multiple workers competing for shared resources</td>\n<td>Monitor file system locks, database connections, memory usage</td>\n<td>Implement resource pooling, use worker-specific resources</td>\n</tr>\n<tr>\n<td>Coordination Overhead</td>\n<td>High messaging overhead relative to processing time</td>\n<td>Measure message queue latency, profile worker communication</td>\n<td>Optimize message formats, reduce coordination frequency</td>\n</tr>\n<tr>\n<td>Uneven Load Distribution</td>\n<td>Some workers overloaded while others idle</td>\n<td>Analyze job assignment patterns, monitor worker utilization</td>\n<td>Implement load-aware scheduling, use worker capability matching</td>\n</tr>\n<tr>\n<td>Cascading Failures</td>\n<td>Worker failures causing additional workers to fail</td>\n<td>Trace failure propagation, examine error handling</td>\n<td>Implement circuit breakers, improve error isolation</td>\n</tr>\n</tbody></table>\n<p>Worker performance profiling requires measuring both individual worker efficiency and collective system throughput. Key metrics include jobs processed per hour per worker, average memory usage per job type, and worker restart frequency.</p>\n<p><strong>Storage and I/O Performance Analysis</strong></p>\n<p>Storage performance directly impacts media processing throughput due to the large file sizes involved in video transcoding and high-resolution image processing.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Metric</th>\n<th>Measurement Technique</th>\n<th>Performance Targets</th>\n<th>Optimization Strategies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Read Throughput</td>\n<td>Monitor bytes/second during file loading</td>\n<td>&gt;500 MB/s for video processing</td>\n<td>Use SSD storage, implement read-ahead caching</td>\n</tr>\n<tr>\n<td>Write Throughput</td>\n<td>Measure output file creation speed</td>\n<td>&gt;200 MB/s for encoded output</td>\n<td>Optimize write buffer sizes, use async writes</td>\n</tr>\n<tr>\n<td>IOPS (Input/Output Operations Per Second)</td>\n<td>Count file operations per second</td>\n<td>&gt;1000 IOPS for thumbnail generation</td>\n<td>Use NVMe storage, batch small file operations</td>\n</tr>\n<tr>\n<td>Latency</td>\n<td>Measure file open/close times</td>\n<td>&lt;10ms for file access operations</td>\n<td>Implement file handle pooling, reduce metadata operations</td>\n</tr>\n<tr>\n<td>Temporary File Management</td>\n<td>Track temp file lifecycle</td>\n<td>Zero leaked files after job completion</td>\n<td>Implement proper cleanup, use defer/finally patterns</td>\n</tr>\n</tbody></table>\n<p>Database performance for job queue operations requires optimization for high-frequency read/write patterns with strong consistency requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Database Operation</th>\n<th>Performance Target</th>\n<th>Optimization Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Queue Operations</td>\n<td>&lt;5ms per operation</td>\n<td>Use Redis for queue state, optimize data structures</td>\n</tr>\n<tr>\n<td>Progress Updates</td>\n<td>&lt;2ms per update</td>\n<td>Implement atomic operations, use appropriate data types</td>\n</tr>\n<tr>\n<td>Metadata Queries</td>\n<td>&lt;10ms per query</td>\n<td>Index frequently queried fields, use read replicas</td>\n</tr>\n<tr>\n<td>Job History</td>\n<td>&lt;50ms per complex query</td>\n<td>Implement data archiving, optimize query patterns</td>\n</tr>\n</tbody></table>\n<p><strong>Resource Monitoring and Alerting</strong></p>\n<p>Comprehensive resource monitoring enables proactive performance management and prevents resource exhaustion before it impacts job processing.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Monitoring Metrics</th>\n<th>Alert Thresholds</th>\n<th>Automated Responses</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU Utilization</td>\n<td>Per-core usage, load average, process CPU time</td>\n<td>&gt;80% sustained for 5+ minutes</td>\n<td>Scale worker pool, throttle job intake</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>RSS, virtual memory, swap usage, memory growth rate</td>\n<td>&gt;85% physical memory, any swap usage</td>\n<td>Reduce concurrent jobs, restart workers</td>\n</tr>\n<tr>\n<td>Disk Space</td>\n<td>Used space, free space, growth rate, inode usage</td>\n<td>&gt;90% used space, &lt;1GB free</td>\n<td>Clean temporary files, archive old jobs</td>\n</tr>\n<tr>\n<td>Network Bandwidth</td>\n<td>Bytes sent/received, connection count, latency</td>\n<td>&gt;80% bandwidth utilization</td>\n<td>Throttle uploads, implement traffic shaping</td>\n</tr>\n<tr>\n<td>File Handles</td>\n<td>Open file count, handle leaks</td>\n<td>&gt;80% of system limit</td>\n<td>Audit file handle usage, implement handle pooling</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Premature Optimization</strong>\nMany developers optimize components that aren&#39;t actual bottlenecks, wasting effort and potentially reducing code maintainability. The solution requires measurement-driven optimization: profile the system under realistic load, identify the true bottlenecks using data, and optimize only the components that measurably impact performance. Use profiling tools consistently and maintain performance benchmarks to validate optimization effectiveness.</p>\n<p>⚠️ <strong>Pitfall: Resource Monitoring Overhead</strong>\nExcessive monitoring can itself become a performance bottleneck, consuming CPU and memory resources needed for media processing. The solution requires careful monitoring configuration that balances visibility with overhead. Use sampling for high-frequency metrics, implement monitoring on/off switches for debugging, and ensure monitoring tools don&#39;t compete with processing workloads for resources.</p>\n<blockquote>\n<p>The critical insight for performance debugging is understanding that media processing workloads have fundamentally different characteristics than typical web applications. Video transcoding may legitimately consume 100% CPU for extended periods, image processing may require gigabytes of memory for single operations, and file I/O patterns involve large sequential reads rather than small random accesses. Performance optimization must account for these unique requirements rather than applying generic optimization techniques.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This subsection provides practical tools and code examples for implementing comprehensive debugging capabilities across the media processing pipeline.</p>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logging Framework</td>\n<td>Python <code>logging</code> with JSON formatter</td>\n<td>Structured logging with <code>structlog</code> + ELK Stack</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Custom StatsD client</td>\n<td>Prometheus client with Grafana dashboards</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td><code>cProfile</code> and <code>memory_profiler</code></td>\n<td><code>py-spy</code> for production profiling</td>\n</tr>\n<tr>\n<td>Log Analysis</td>\n<td><code>grep</code> + <code>jq</code> command line tools</td>\n<td>Centralized logging with Elasticsearch</td>\n</tr>\n<tr>\n<td>Resource Monitoring</td>\n<td><code>psutil</code> for system metrics</td>\n<td>Full observability with DataDog/New Relic</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media_processing/\n  debug/\n    __init__.py                    ← debug utilities exports\n    logging_config.py              ← structured logging setup\n    metrics.py                     ← metrics collection and reporting\n    profiling.py                   ← performance profiling utilities\n    diagnostics.py                 ← diagnostic command implementations\n    monitoring.py                  ← resource monitoring and alerting\n    health_checks.py               ← system health validation\n  tools/\n    job_inspector.py               ← command-line job debugging tool\n    queue_analyzer.py              ← queue performance analysis\n    resource_monitor.py            ← real-time resource monitoring\n    log_analyzer.py                ← log correlation and analysis\n  tests/\n    test_debugging.py              ← debugging utility tests\n    fixtures/                      ← test data for debugging scenarios</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># debug/logging_config.py - Complete structured logging setup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructuredFormatter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">logging</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Formatter</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"JSON formatter for structured logging with media processing context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.hostname </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._get_hostname()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.process_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> os.getpid()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> format</span><span style=\"color:#E1E4E8\">(self, record: logging.LogRecord) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert log record to structured JSON format.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log_entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: datetime.utcnow().isoformat() </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"Z\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"level\"</span><span style=\"color:#E1E4E8\">: record.levelname,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"logger\"</span><span style=\"color:#E1E4E8\">: record.name,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"message\"</span><span style=\"color:#E1E4E8\">: record.getMessage(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"hostname\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.hostname,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"process_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.process_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"thread_id\"</span><span style=\"color:#E1E4E8\">: threading.current_thread().ident</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add job context if available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_context </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(threading.current_thread(), </span><span style=\"color:#9ECBFF\">'job_context'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> job_context:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log_entry.update(job_context)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add exception info if present</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> record.exc_info:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log_entry[</span><span style=\"color:#9ECBFF\">\"exception\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.formatException(record.exc_info)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add extra fields from log record</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> record.</span><span style=\"color:#79B8FF\">__dict__</span><span style=\"color:#E1E4E8\">.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'name'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'msg'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'args'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'levelname'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'levelno'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                          'pathname'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'filename'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'module'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'lineno'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                          'funcName'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'created'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'msecs'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'relativeCreated'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                          'thread'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'threadName'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'processName'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'process'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                          'getMessage'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'exc_info'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'exc_text'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'stack_info'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                log_entry[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> json.dumps(log_entry, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_hostname</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        import</span><span style=\"color:#E1E4E8\"> socket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> socket.gethostname()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> setup_logging</span><span style=\"color:#E1E4E8\">(debug: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">, log_file: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configure structured logging for the media processing system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    level </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">DEBUG</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> debug </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> logging.</span><span style=\"color:#79B8FF\">INFO</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Configure root logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    root_logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    root_logger.setLevel(level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Remove existing handlers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> root_logger.handlers[:]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        root_logger.removeHandler(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Console handler with structured formatting</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    console_handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.StreamHandler(sys.stdout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    console_handler.setLevel(level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    console_handler.setFormatter(StructuredFormatter())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    root_logger.addHandler(console_handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # File handler if specified</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> log_file:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file_handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.FileHandler(log_file)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file_handler.setLevel(level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file_handler.setFormatter(StructuredFormatter())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        root_logger.addHandler(file_handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Configure third-party loggers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.getLogger(</span><span style=\"color:#9ECBFF\">'PIL'</span><span style=\"color:#E1E4E8\">).setLevel(logging.</span><span style=\"color:#79B8FF\">WARNING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logging.getLogger(</span><span style=\"color:#9ECBFF\">'urllib3'</span><span style=\"color:#E1E4E8\">).setLevel(logging.</span><span style=\"color:#79B8FF\">WARNING</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> job_logging_context</span><span style=\"color:#E1E4E8\">(job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, correlation_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for adding job information to all log messages.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.current_thread()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    old_context </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(thread, </span><span style=\"color:#9ECBFF\">'job_context'</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    new_context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_context.copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    new_context.update({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'job_id'</span><span style=\"color:#E1E4E8\">: job_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        'correlation_id'</span><span style=\"color:#E1E4E8\">: correlation_id </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> job_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    thread.job_context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> new_context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        thread.job_context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> old_context</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># debug/metrics.py - Metrics collection infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> threading </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Lock</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetricPoint</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual metric measurement.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetricsCollector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe metrics collection with configurable backends.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._counters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._gauges: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._histograms: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Lock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._enabled </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> counter</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">, tags: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Increment a counter metric.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._enabled:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._metric_key(name, tags)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._counters[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._counters.get(key, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> gauge</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, tags: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set a gauge metric to specific value.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._enabled:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._metric_key(name, tags)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._gauges[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> histogram</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, tags: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add value to histogram metric.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._enabled:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._metric_key(name, tags)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._histograms:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._histograms[key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._histograms[key].append(value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> timing</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, duration_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, tags: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record timing metric in seconds.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.histogram(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">.duration\"</span><span style=\"color:#E1E4E8\">, duration_seconds, tags)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _metric_key</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tags: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate unique key for metric with tags.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> tags:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tag_str </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \",\"</span><span style=\"color:#E1E4E8\">.join(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">k</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">v</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> sorted</span><span style=\"color:#E1E4E8\">(tags.items()))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">[</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">tag_str</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">]\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_snapshot</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current metrics snapshot.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'counters'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._counters.copy(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'gauges'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._gauges.copy(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'histograms'</span><span style=\"color:#E1E4E8\">: {k: v.copy() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._histograms.items()},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'timestamp'</span><span style=\"color:#E1E4E8\">: time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> reset</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reset all metrics (for testing).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._counters.clear()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._gauges.clear()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._histograms.clear()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global metrics instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> MetricsCollector()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@contextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> timed_operation</span><span style=\"color:#E1E4E8\">(operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tags: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for timing operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    success </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    finally</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result_tags </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (tags </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}).copy()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result_tags[</span><span style=\"color:#9ECBFF\">'success'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(success)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics.timing(operation_name, duration, result_tags)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># debug/diagnostics.py - Diagnostic utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> redis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> subprocess</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SystemDiagnostics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"System-level diagnostic utilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: redis.Redis):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_redis_health</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Comprehensive Redis health check.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            info </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis_client.info()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'connected'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'memory_used_mb'</span><span style=\"color:#E1E4E8\">: info.get(</span><span style=\"color:#9ECBFF\">'used_memory'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'connected_clients'</span><span style=\"color:#E1E4E8\">: info.get(</span><span style=\"color:#9ECBFF\">'connected_clients'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'operations_per_sec'</span><span style=\"color:#E1E4E8\">: info.get(</span><span style=\"color:#9ECBFF\">'instantaneous_ops_per_sec'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'keyspace_hits'</span><span style=\"color:#E1E4E8\">: info.get(</span><span style=\"color:#9ECBFF\">'keyspace_hits'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'keyspace_misses'</span><span style=\"color:#E1E4E8\">: info.get(</span><span style=\"color:#9ECBFF\">'keyspace_misses'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'connected'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'error'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_worker_processes</span><span style=\"color:#E1E4E8\">(self) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check status of worker processes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        workers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> proc </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> psutil.process_iter([</span><span style=\"color:#9ECBFF\">'pid'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'name'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'memory_info'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'cpu_percent'</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#9ECBFF\"> 'media_worker'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> proc.info[</span><span style=\"color:#9ECBFF\">'name'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    workers.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        'pid'</span><span style=\"color:#E1E4E8\">: proc.info[</span><span style=\"color:#9ECBFF\">'pid'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        'memory_mb'</span><span style=\"color:#E1E4E8\">: proc.info[</span><span style=\"color:#9ECBFF\">'memory_info'</span><span style=\"color:#E1E4E8\">].rss </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        'cpu_percent'</span><span style=\"color:#E1E4E8\">: proc.info[</span><span style=\"color:#9ECBFF\">'cpu_percent'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        'status'</span><span style=\"color:#E1E4E8\">: proc.status()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    })</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> (psutil.NoSuchProcess, psutil.AccessDenied):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> workers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_disk_space</span><span style=\"color:#E1E4E8\">(self, paths: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check disk space for critical paths.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        disk_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> path </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> paths:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.disk_usage(path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                disk_info[path] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'total_gb'</span><span style=\"color:#E1E4E8\">: usage.total </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'used_gb'</span><span style=\"color:#E1E4E8\">: usage.used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'free_gb'</span><span style=\"color:#E1E4E8\">: usage.free </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'percent_used'</span><span style=\"color:#E1E4E8\">: (usage.used </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> usage.total) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                disk_info[path] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'error'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> disk_info</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_queue_statistics</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get comprehensive queue statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> priority </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'urgent'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'high'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'normal'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'low'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                queue_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"media:queue:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">priority</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                depth </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis_client.llen(queue_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                stats[</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">priority</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_queue_depth\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> depth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Get processing job count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            processing_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis_client.hlen(</span><span style=\"color:#9ECBFF\">\"jobs:processing\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            stats[</span><span style=\"color:#9ECBFF\">'jobs_processing'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> processing_jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Get failed job count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            failed_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis_client.llen(</span><span style=\"color:#9ECBFF\">\"media:queue:failed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            stats[</span><span style=\"color:#9ECBFF\">'jobs_failed'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> failed_jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> stats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">'error'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># debug/profiling.py - Performance profiling utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> cProfile</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pstats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> io</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> memory_profiler</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Callable, Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceProfiler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive performance profiling for media processing operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, enable_memory_profiling: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.enable_memory_profiling </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> enable_memory_profiling</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._profiles: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> profile_function</span><span style=\"color:#E1E4E8\">(self, func: Callable, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Any:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Profile function execution with CPU and memory analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create cProfile.Profile instance for CPU profiling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start memory monitoring if enabled using memory_profiler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute function with profiling enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Stop profiling and collect statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store profile results with timestamp and function name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return original function result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use memory_profiler.LineProfiler for detailed memory analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_memory_usage</span><span style=\"color:#E1E4E8\">(self, func: Callable, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get detailed memory usage during function execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Record baseline memory usage before execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute function while monitoring memory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Record peak memory usage during execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate memory delta and peak usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return comprehensive memory statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use memory_profiler.memory_usage with interval parameter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_bottlenecks</span><span style=\"color:#E1E4E8\">(self, profile_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze CPU profiling results to identify bottlenecks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve stored profile data for analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract top functions by cumulative time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify functions with high call counts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate time per call for each function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate bottleneck report with recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ResourceMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Real-time resource monitoring for worker processes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, sample_interval: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.sample_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sample_interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._monitoring </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._samples: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_monitoring</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start resource monitoring for specific job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize monitoring state and sample storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Start background thread for periodic sampling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Record initial resource baseline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Set monitoring flag to enable sample collection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use threading.Thread with daemon=True for background monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop_monitoring</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop monitoring and return comprehensive resource report.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set monitoring flag to false to stop sampling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Wait for background thread to complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Analyze collected samples for patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate peak usage, averages, and trends</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate summary report with recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _collect_sample</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Collect single resource usage sample.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current process CPU and memory usage using psutil</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check disk I/O statistics for current process</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Record file handle count and network connections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Include timestamp for temporal analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return complete sample data structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># tools/job_inspector.py - Command-line job debugging tool</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobInspector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Command-line tool for inspecting job state and debugging issues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_client: redis.Redis):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> inspect_job</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get comprehensive job state and history.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve job data from Redis using job_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get current progress information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Collect job processing history and state transitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check for associated temporary files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Analyze job for common failure patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return complete job inspection report</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> diagnose_stuck_job</span><span style=\"color:#E1E4E8\">(self, job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Diagnose why job appears stuck in processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check job status and last progress update time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify worker process is still running</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check for temporary files and processing artifacts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Analyze memory usage of worker process</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Determine if job should be marked failed or retried</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_queue_health</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze overall queue health and performance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get queue depths across all priority levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate processing rates and throughput</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify jobs that have been pending too long</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check worker utilization and availability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate queue health report with recommendations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>logging.getLogger(__name__)</code> in each module to create component-specific loggers</li>\n<li><code>psutil.Process().memory_info().rss</code> provides resident set size for memory usage</li>\n<li><code>redis-py</code> client provides <code>info()</code> method for Redis diagnostics</li>\n<li>Use <code>contextlib.contextmanager</code> for resource monitoring decorators</li>\n<li><code>json.dumps(obj, default=str)</code> handles datetime serialization in log messages</li>\n<li><code>threading.local()</code> provides thread-safe storage for job context</li>\n<li><code>subprocess.run()</code> with <code>capture_output=True</code> captures command output for analysis</li>\n</ul>\n<p><strong>F. Milestone Checkpoint:</strong></p>\n<p>After implementing the debugging infrastructure:</p>\n<p><strong>What to run:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> tools.job_inspector</span><span style=\"color:#79B8FF\"> --job-id</span><span style=\"color:#9ECBFF\"> job_123</span><span style=\"color:#79B8FF\"> --diagnose</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> debug.monitoring</span><span style=\"color:#79B8FF\"> --check-health</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"from debug.logging_config import setup_logging; setup_logging(debug=True)\"</span></span></code></pre></div>\n\n<p><strong>Expected behavior:</strong></p>\n<ul>\n<li>Structured JSON logs appear in console with job correlation IDs</li>\n<li>Job inspector returns comprehensive job state information</li>\n<li>Health checks report Redis connectivity and worker process status</li>\n<li>Metrics collection tracks job processing rates and resource usage</li>\n</ul>\n<p><strong>Signs of success:</strong></p>\n<ul>\n<li>Log messages include consistent <code>job_id</code> fields for correlation</li>\n<li>Performance profiling identifies bottlenecks in media processing functions</li>\n<li>Resource monitoring detects memory spikes during large file processing</li>\n<li>Queue health checks provide actionable insights for performance optimization</li>\n</ul>\n<p><strong>What to verify manually:</strong></p>\n<ul>\n<li>Submit test job and trace log messages through entire pipeline</li>\n<li>Verify webhook delivery failures trigger appropriate retry logic</li>\n<li>Confirm memory monitoring detects resource exhaustion scenarios</li>\n<li>Test diagnostic tools provide useful information for debugging failures</li>\n</ul>\n<h2 id=\"future-extensions-and-scalability\">Future Extensions and Scalability</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as this section provides architectural patterns and extension points that build upon the foundational image processing, video transcoding, and job queue capabilities</p>\n</blockquote>\n<h3 id=\"mental-model-growing-entertainment-studio\">Mental Model: Growing Entertainment Studio</h3>\n<p>Think of the media processing pipeline as a small independent film studio that started with basic equipment and a handful of employees. Initially, they could handle simple photo shoots and short video projects in a single office. As their reputation grows and demand increases, they face several expansion challenges: they need multiple production facilities (horizontal scaling), sophisticated equipment like motion capture and AI-powered editing tools (advanced processing features), and partnerships with distributors, streaming platforms, and equipment rental companies (external integrations). Each growth phase requires careful planning to maintain quality while dramatically increasing capacity and capabilities.</p>\n<p>The key insight is that successful scaling isn&#39;t just about adding more resources—it&#39;s about evolving the entire operational model. A studio that simply adds more editing bays without upgrading their project management systems, client communication processes, and delivery pipelines will quickly become chaotic and inefficient. Similarly, our media processing pipeline must evolve its architecture, not just multiply its components.</p>\n<h2 id=\"horizontal-scaling-patterns\">Horizontal Scaling Patterns</h2>\n<h3 id=\"multi-node-deployment-architecture\">Multi-Node Deployment Architecture</h3>\n<p><strong>Resource-aware scheduling</strong> becomes critical when distributing processing jobs across multiple nodes. Unlike the single-node deployment where all worker processes share the same hardware resources, multi-node deployments must consider the heterogeneous capabilities of different machines. Some nodes might have GPU acceleration for video transcoding, while others excel at CPU-intensive image processing tasks.</p>\n<p>The <strong>job queue</strong> evolution requires sophisticated routing logic that goes beyond simple priority-based distribution. Each processing job must be matched with nodes that have the appropriate hardware capabilities, available memory, and current load levels. This transforms the simple Redis-based queue into a distributed scheduling system with node registration, capability advertising, and intelligent job placement.</p>\n<blockquote>\n<p><strong>Decision: Distributed Queue Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Single Redis instance becomes bottleneck and single point of failure as worker nodes scale beyond 10-15 machines</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Redis Cluster with consistent hashing</li>\n<li>RabbitMQ federation across data centers</li>\n<li>Apache Kafka with partitioned topics</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Redis Cluster for job queue with RabbitMQ for inter-node coordination</li>\n<li><strong>Rationale</strong>: Redis Cluster provides horizontal scaling for job distribution while maintaining sub-millisecond job pop latency. RabbitMQ handles complex routing patterns for progress updates and administrative messages that don&#39;t require Redis-level performance.</li>\n<li><strong>Consequences</strong>: Introduces complexity of managing two message broker systems but enables independent scaling of job throughput vs coordination message handling</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Queue Component</th>\n<th>Single Node</th>\n<th>Multi-Node Cluster</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Job Distribution</td>\n<td>Simple Redis LPOP</td>\n<td>Redis Cluster with hash slots</td>\n</tr>\n<tr>\n<td>Node Discovery</td>\n<td>Static configuration</td>\n<td>Dynamic registration with heartbeats</td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td>OS process scheduler</td>\n<td>Custom weighted round-robin</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>Process-level checks</td>\n<td>Network-aware health probes</td>\n</tr>\n<tr>\n<td>Failure Recovery</td>\n<td>Process restart</td>\n<td>Node failover with job reassignment</td>\n</tr>\n</tbody></table>\n<p><strong>Worker coordination</strong> across nodes requires a fundamentally different approach than local process management. The system must track which nodes are healthy, what their current capacity utilization looks like, and how to redistribute work when nodes fail or new ones join the cluster. This involves implementing a <strong>distributed consensus mechanism</strong> for cluster membership and a <strong>gossip protocol</strong> for sharing load information.</p>\n<p>The coordination layer maintains several critical data structures across the cluster:</p>\n<table>\n<thead>\n<tr>\n<th>Data Structure</th>\n<th>Purpose</th>\n<th>Replication Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Node Registry</td>\n<td>Track active nodes and capabilities</td>\n<td>Raft consensus with leader election</td>\n</tr>\n<tr>\n<td>Job Assignments</td>\n<td>Map active jobs to processing nodes</td>\n<td>Replicated state machine</td>\n</tr>\n<tr>\n<td>Load Metrics</td>\n<td>CPU, memory, queue depth per node</td>\n<td>Eventually consistent gossip</td>\n</tr>\n<tr>\n<td>Health Status</td>\n<td>Node availability and performance</td>\n<td>Heartbeat with failure detection</td>\n</tr>\n</tbody></table>\n<h3 id=\"load-balancing-strategies\">Load Balancing Strategies</h3>\n<p><strong>Geographic distribution</strong> adds another layer of complexity to load balancing. Media files are often large, and network transfer costs become significant when moving multi-gigabyte video files between data centers. The ideal architecture processes media files on nodes that are network-close to the storage location, but this creates uneven load distribution challenges.</p>\n<p>Smart load balancing must consider multiple factors simultaneously:</p>\n<ol>\n<li><strong>Network locality</strong>: Prefer nodes in the same data center as the source media file</li>\n<li><strong>Specialized hardware</strong>: Route video transcoding jobs to GPU-enabled nodes</li>\n<li><strong>Current load</strong>: Avoid overloading nodes that are already processing intensive jobs</li>\n<li><strong>Historical performance</strong>: Learn which nodes complete similar jobs fastest</li>\n<li><strong>Cost optimization</strong>: Balance processing speed against infrastructure costs</li>\n</ol>\n<p>The load balancer maintains a <strong>capability matrix</strong> that tracks each node&#39;s hardware specifications, current resource utilization, and recent performance metrics. When a new job arrives, the balancer scores potential nodes using a weighted algorithm that considers all these factors.</p>\n<blockquote>\n<p><strong>Decision: Adaptive Load Balancing Algorithm</strong></p>\n<ul>\n<li><strong>Context</strong>: Static load balancing wastes GPU resources and creates uneven processing times across different media types</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Round-robin with manual node tagging</li>\n<li>Least-connections with capability filtering  </li>\n<li>Machine learning-based predictive scheduling</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Weighted scoring algorithm with runtime capability learning</li>\n<li><strong>Rationale</strong>: Provides good performance without ML complexity. Scoring weights can be tuned based on observed job completion patterns and resource utilization metrics.</li>\n<li><strong>Consequences</strong>: Requires maintaining performance history and periodic rebalancing, but achieves 30-40% better resource utilization than simple strategies</li>\n</ul>\n</blockquote>\n<h3 id=\"distributed-storage-integration\">Distributed Storage Integration</h3>\n<p><strong>Object storage backends</strong> like Amazon S3, Google Cloud Storage, or Azure Blob Storage become essential for multi-node deployments. Local file storage doesn&#39;t work when jobs can be processed on any node in the cluster. However, integrating distributed storage introduces new challenges around data transfer costs, processing locality, and temporary file management.</p>\n<p>The storage abstraction layer must handle several scenarios seamlessly:</p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Challenge</th>\n<th>Solution Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Large input files</td>\n<td>Network transfer time dominates processing</td>\n<td>Stream processing with progressive download</td>\n</tr>\n<tr>\n<td>Multiple output formats</td>\n<td>Parallel generation on different nodes</td>\n<td>Distributed fan-out with result aggregation</td>\n</tr>\n<tr>\n<td>Temporary processing files</td>\n<td>Cleanup across node failures</td>\n<td>Distributed lease management with TTL</td>\n</tr>\n<tr>\n<td>Bandwidth optimization</td>\n<td>Minimize redundant transfers</td>\n<td>Content-addressed caching with deduplication</td>\n</tr>\n</tbody></table>\n<p><strong>Caching strategies</strong> become critical for performance when dealing with distributed storage. Frequently accessed source files should be cached locally on multiple nodes, while processed outputs might benefit from edge caching closer to end users. The cache invalidation logic must handle scenarios where source files are updated or processing parameters change.</p>\n<p>A sophisticated caching layer implements multiple tiers:</p>\n<ol>\n<li><strong>Local node cache</strong>: Fast SSD storage for active job files</li>\n<li><strong>Cluster shared cache</strong>: High-speed network storage shared across nodes</li>\n<li><strong>Edge cache</strong>: CDN integration for processed output delivery</li>\n<li><strong>Cold storage</strong>: Long-term archival with slower retrieval times</li>\n</ol>\n<h3 id=\"auto-scaling-and-resource-management\">Auto-scaling and Resource Management</h3>\n<p><strong>Dynamic scaling policies</strong> must react to both queue depth and resource utilization patterns. Unlike web services that scale primarily based on request volume, media processing workloads have highly variable resource requirements. A single 4K video transcoding job might consume more resources than 100 image resize operations.</p>\n<p>The auto-scaling system monitors multiple metrics simultaneously:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Type</th>\n<th>Scaling Signal</th>\n<th>Response Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queue Depth</td>\n<td>Jobs waiting &gt; 5 minutes</td>\n<td>Add general-purpose nodes</td>\n</tr>\n<tr>\n<td>GPU Utilization</td>\n<td>Video queue backlog</td>\n<td>Add GPU-enabled nodes</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>Failed jobs due to OOM</td>\n<td>Add high-memory nodes</td>\n</tr>\n<tr>\n<td>Network Bandwidth</td>\n<td>Transfer times &gt; SLA</td>\n<td>Add nodes in new regions</td>\n</tr>\n<tr>\n<td>Cost Optimization</td>\n<td>Low utilization periods</td>\n<td>Remove excess capacity</td>\n</tr>\n</tbody></table>\n<p><strong>Resource quotas and limits</strong> prevent individual jobs from consuming excessive resources and impacting other workloads. The quota system must be sophisticated enough to handle legitimate large media files while preventing abuse or runaway processing jobs.</p>\n<h2 id=\"advanced-processing-features\">Advanced Processing Features</h2>\n<h3 id=\"ai-powered-content-analysis\">AI-Powered Content Analysis</h3>\n<p><strong>Smart cropping algorithms</strong> represent a significant evolution beyond simple center-crop or face detection. Modern AI-powered cropping uses computer vision models trained on millions of images to understand visual composition, subject importance, and aesthetic principles. These systems can identify the most visually interesting regions of an image and crop accordingly, even for complex scenes with multiple subjects.</p>\n<p>The smart cropping pipeline involves several stages:</p>\n<ol>\n<li><strong>Content analysis</strong>: Object detection, face recognition, and scene understanding using pre-trained neural networks</li>\n<li><strong>Composition scoring</strong>: Evaluate different crop regions using rules of thirds, leading lines, and symmetry principles  </li>\n<li><strong>Multi-format optimization</strong>: Generate different crops optimized for square thumbnails, banner images, and mobile screens</li>\n<li><strong>A/B testing integration</strong>: Track engagement metrics to continuously improve cropping decisions</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>AI Model Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>Computational Cost</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Object Detection</td>\n<td>Full resolution image</td>\n<td>Bounding boxes with confidence</td>\n<td>50-200ms GPU time</td>\n</tr>\n<tr>\n<td>Saliency Mapping</td>\n<td>Image regions</td>\n<td>Visual importance heatmap</td>\n<td>20-100ms GPU time</td>\n</tr>\n<tr>\n<td>Composition Analysis</td>\n<td>Crop candidates</td>\n<td>Aesthetic quality scores</td>\n<td>10-50ms CPU time</td>\n</tr>\n<tr>\n<td>Face Detection</td>\n<td>Image regions</td>\n<td>Face locations and orientations</td>\n<td>5-30ms GPU time</td>\n</tr>\n</tbody></table>\n<p><strong>Content-aware optimization</strong> goes beyond smart cropping to include automated quality adjustments, color correction, and format selection based on image content. For example, images with large areas of solid color compress better as PNG, while photographic content benefits from JPEG compression. AI models can analyze image characteristics and automatically select optimal processing parameters.</p>\n<p>The content analysis results feed into processing decisions:</p>\n<ul>\n<li><strong>Format selection</strong>: Choose JPEG for photos, PNG for graphics, WebP for web delivery</li>\n<li><strong>Compression settings</strong>: Adjust quality levels based on content complexity and target use case  </li>\n<li><strong>Color space conversion</strong>: Optimize for target display characteristics (sRGB for web, P3 for modern displays)</li>\n<li><strong>Sharpening parameters</strong>: Apply appropriate sharpening based on content type and output size</li>\n</ul>\n<h3 id=\"automated-video-enhancement\">Automated Video Enhancement</h3>\n<p><strong>Scene detection and keyframe extraction</strong> enables sophisticated video processing that goes beyond simple transcoding. AI-powered scene detection can identify shot boundaries, recognize different types of content (talking heads vs action sequences), and extract the most representative frames for thumbnail generation.</p>\n<p>Advanced scene analysis provides rich metadata for downstream processing:</p>\n<table>\n<thead>\n<tr>\n<th>Scene Analysis Output</th>\n<th>Use Cases</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shot boundaries</td>\n<td>Chapter markers, thumbnail selection</td>\n<td>Medium - temporal analysis</td>\n</tr>\n<tr>\n<td>Content classification</td>\n<td>Encoding parameter optimization</td>\n<td>High - requires trained models</td>\n</tr>\n<tr>\n<td>Motion analysis</td>\n<td>Adaptive bitrate tuning</td>\n<td>Medium - optical flow calculation</td>\n</tr>\n<tr>\n<td>Audio classification</td>\n<td>Music vs speech detection</td>\n<td>High - audio ML models</td>\n</tr>\n</tbody></table>\n<p><strong>Adaptive transcoding parameters</strong> use content analysis to optimize encoding settings for each video segment. Fast-motion action sequences might benefit from higher bitrates and keyframe frequency, while talking-head segments can use more aggressive compression without quality loss.</p>\n<p>The adaptive transcoding system builds a <strong>content fingerprint</strong> for each video:</p>\n<ol>\n<li><strong>Motion analysis</strong>: Calculate optical flow vectors to measure scene complexity</li>\n<li><strong>Spatial complexity</strong>: Analyze texture and detail levels in each frame</li>\n<li><strong>Temporal consistency</strong>: Measure how much content changes between frames</li>\n<li><strong>Audio characteristics</strong>: Detect music, speech, silence, and ambient sound</li>\n</ol>\n<p>These fingerprints drive encoding decisions:</p>\n<ul>\n<li><strong>Variable bitrate curves</strong>: Allocate bits based on scene complexity</li>\n<li><strong>Keyframe placement</strong>: Insert keyframes at scene boundaries for better seeking</li>\n<li><strong>Encoding speed vs quality</strong>: Use slower, higher-quality encoding for important content</li>\n<li><strong>Format selection</strong>: Choose codecs optimized for content characteristics</li>\n</ul>\n<h3 id=\"batch-processing-optimization\">Batch Processing Optimization</h3>\n<p><strong>Pipeline parallelization</strong> enables processing multiple versions of the same media file simultaneously across different nodes. Instead of sequentially generating thumbnail, mobile, and desktop versions, the system can create all variants in parallel and combine the results.</p>\n<p>The parallel processing coordinator must handle several challenges:</p>\n<table>\n<thead>\n<tr>\n<th>Challenge</th>\n<th>Impact</th>\n<th>Solution Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Resource contention</td>\n<td>Multiple jobs competing for same input file</td>\n<td>Shared read-only cache with reference counting</td>\n</tr>\n<tr>\n<td>Result synchronization</td>\n<td>Coordinating completion of parallel tasks</td>\n<td>Distributed barrier with timeout handling</td>\n</tr>\n<tr>\n<td>Partial failure handling</td>\n<td>Some variants succeed while others fail</td>\n<td>Per-variant status tracking with retry logic</td>\n</tr>\n<tr>\n<td>Progress aggregation</td>\n<td>Combining progress from multiple parallel jobs</td>\n<td>Weighted progress calculation across variants</td>\n</tr>\n</tbody></table>\n<p><strong>Dependency-aware scheduling</strong> optimizes processing pipelines by understanding relationships between different processing steps. For example, video thumbnail extraction can begin as soon as transcoding produces the first few seconds of output, rather than waiting for complete transcoding to finish.</p>\n<p>The dependency graph represents processing relationships:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Input Video → Scene Analysis → Thumbnail Extraction\n            → Transcoding → [Mobile, Desktop, Streaming variants]\n            → Audio Extraction → Podcast version</code></pre></div>\n\n<p>Each node in the graph can start as soon as its dependencies are satisfied, dramatically reducing overall processing time for complex workflows.</p>\n<h2 id=\"external-integration-opportunities\">External Integration Opportunities</h2>\n<h3 id=\"content-delivery-network-integration\">Content Delivery Network Integration</h3>\n<p><strong>Multi-CDN strategies</strong> become essential for global media delivery at scale. Different CDNs have varying performance characteristics across geographic regions and content types. A sophisticated system can route video streaming through CDN A while serving image thumbnails through CDN B based on real-time performance monitoring.</p>\n<p>The CDN integration layer must handle several responsibilities:</p>\n<table>\n<thead>\n<tr>\n<th>Integration Aspect</th>\n<th>Challenge</th>\n<th>Solution Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Geographic routing</td>\n<td>Optimal CDN selection per region</td>\n<td>Real-time latency monitoring with failover</td>\n</tr>\n<tr>\n<td>Cache invalidation</td>\n<td>Coordinating updates across multiple CDNs</td>\n<td>Distributed invalidation with retry logic</td>\n</tr>\n<tr>\n<td>Cost optimization</td>\n<td>Minimizing bandwidth and storage costs</td>\n<td>Intelligent tier placement and caching policies</td>\n</tr>\n<tr>\n<td>Format negotiation</td>\n<td>Serving optimal formats per client</td>\n<td>Edge-side logic with capability detection</td>\n</tr>\n</tbody></table>\n<p><strong>Edge computing integration</strong> pushes simple processing tasks closer to end users. Image resizing for common thumbnail sizes can happen at CDN edge nodes rather than central processing clusters, reducing latency and bandwidth costs. However, this requires careful orchestration between central processing and edge capabilities.</p>\n<p>Edge processing scenarios include:</p>\n<ol>\n<li><strong>On-demand resizing</strong>: Generate thumbnail sizes not pre-computed centrally</li>\n<li><strong>Format conversion</strong>: Convert between WebP, AVIF, and JPEG based on browser support</li>\n<li><strong>Quality adaptation</strong>: Adjust compression based on detected connection speed</li>\n<li><strong>Watermark application</strong>: Add region-specific branding at delivery time</li>\n</ol>\n<h3 id=\"cloud-storage-backend-integration\">Cloud Storage Backend Integration</h3>\n<p><strong>Multi-cloud storage strategies</strong> provide resilience, cost optimization, and geographic distribution capabilities. Different cloud providers offer varying price points and performance characteristics for media storage and processing workloads.</p>\n<p>The storage abstraction layer manages complexity across providers:</p>\n<table>\n<thead>\n<tr>\n<th>Storage Tier</th>\n<th>Primary Provider</th>\n<th>Backup Provider</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hot storage</td>\n<td>AWS S3 Standard</td>\n<td>Google Cloud Storage</td>\n<td>Active media files</td>\n</tr>\n<tr>\n<td>Warm storage</td>\n<td>S3 Infrequent Access</td>\n<td>Azure Cool Blob Storage</td>\n<td>Recently processed files</td>\n</tr>\n<tr>\n<td>Cold storage</td>\n<td>S3 Glacier</td>\n<td>Google Archive Storage</td>\n<td>Long-term backup</td>\n</tr>\n<tr>\n<td>Processing cache</td>\n<td>Local NVMe SSD</td>\n<td>S3 Standard</td>\n<td>Active job temporary files</td>\n</tr>\n</tbody></table>\n<p><strong>Storage lifecycle management</strong> automatically transitions media files between storage tiers based on access patterns and retention policies. Newly uploaded files remain in hot storage for immediate processing, while older processed outputs move to cheaper storage tiers over time.</p>\n<p>Lifecycle policies consider multiple factors:</p>\n<ul>\n<li><strong>Access frequency</strong>: Files not accessed for 30 days move to warm storage</li>\n<li><strong>Processing requirements</strong>: Source files for ongoing jobs remain in hot storage</li>\n<li><strong>Compliance requirements</strong>: Legal holds prevent automatic archive transitions</li>\n<li><strong>Cost optimization</strong>: Balance access time against storage costs</li>\n</ul>\n<h3 id=\"third-party-processing-service-integration\">Third-Party Processing Service Integration</h3>\n<p><strong>Hybrid processing architectures</strong> combine on-premise processing with cloud-based specialized services. For example, basic image resizing might happen locally while AI-powered content tagging uses cloud APIs from Google Vision or Amazon Rekognition.</p>\n<p>The service integration layer provides a unified interface across providers:</p>\n<table>\n<thead>\n<tr>\n<th>Processing Type</th>\n<th>Local Implementation</th>\n<th>Cloud Service Option</th>\n<th>Fallback Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image resizing</td>\n<td>Pillow + custom algorithms</td>\n<td>None needed</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Local ML models</td>\n<td>Google Vision API</td>\n<td>Disable feature on API failure</td>\n</tr>\n<tr>\n<td>Video transcoding</td>\n<td>FFmpeg cluster</td>\n<td>AWS Elemental MediaConvert</td>\n<td>Queue jobs until service recovery</td>\n</tr>\n<tr>\n<td>Content moderation</td>\n<td>Rule-based filtering</td>\n<td>Amazon Rekognition</td>\n<td>Manual review queue</td>\n</tr>\n</tbody></table>\n<p><strong>API rate limiting and cost management</strong> becomes critical when integrating external services. The system must track usage across different API endpoints and implement circuit breakers to prevent runaway costs from processing spikes or API failures.</p>\n<p>Rate limiting strategies include:</p>\n<ol>\n<li><strong>Per-service quotas</strong>: Daily/monthly limits for each external service</li>\n<li><strong>Priority-based throttling</strong>: Reserve API capacity for high-priority jobs</li>\n<li><strong>Cost-based fallbacks</strong>: Switch to local processing when API costs exceed thresholds</li>\n<li><strong>Batching optimization</strong>: Combine multiple requests to reduce API call overhead</li>\n</ol>\n<h3 id=\"monitoring-and-analytics-integration\">Monitoring and Analytics Integration</h3>\n<p><strong>Processing analytics</strong> provide insights into system performance, job patterns, and optimization opportunities. Integration with tools like Prometheus, Grafana, and ELK stack enables comprehensive monitoring of the distributed media processing pipeline.</p>\n<p>Key metrics to track include:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Example Metrics</th>\n<th>Monitoring Tool</th>\n<th>Alert Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Processing Performance</td>\n<td>Jobs/hour, processing time percentiles</td>\n<td>Prometheus + Grafana</td>\n<td>p95 processing time &gt; SLA</td>\n</tr>\n<tr>\n<td>Resource Utilization</td>\n<td>CPU, memory, GPU usage per node</td>\n<td>Node Exporter</td>\n<td>Sustained &gt; 90% utilization</td>\n</tr>\n<tr>\n<td>Quality Metrics</td>\n<td>Transcoding quality scores, error rates</td>\n<td>Custom collectors</td>\n<td>Error rate &gt; 1%</td>\n</tr>\n<tr>\n<td>Business Metrics</td>\n<td>Processing costs, throughput trends</td>\n<td>Application metrics</td>\n<td>Cost growth &gt; revenue growth</td>\n</tr>\n</tbody></table>\n<p><strong>Machine learning feedback loops</strong> use processing outcomes to continuously improve system performance. For example, tracking which smart crop selections get the most user engagement can improve the AI model training data over time.</p>\n<p>Feedback collection mechanisms:</p>\n<ul>\n<li><strong>A/B testing frameworks</strong>: Compare different processing parameters and measure outcomes</li>\n<li><strong>User engagement tracking</strong>: Monitor click-through rates, view duration, and interaction patterns</li>\n<li><strong>Quality assessment</strong>: Automated quality metrics and occasional human evaluation</li>\n<li><strong>Performance correlation</strong>: Link processing parameters to user satisfaction metrics</li>\n</ul>\n<blockquote>\n<p>The key insight for successful scaling is that each integration point becomes a potential failure mode, but also an opportunity for optimization. The system must be designed with graceful degradation in mind—if the AI cropping service fails, fall back to center crop rather than failing the entire job.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance focuses on the foundational patterns and infrastructure needed to support horizontal scaling and advanced features. The code provides working examples that can be extended for specific scaling scenarios.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Container Orchestration</td>\n<td>Docker Compose</td>\n<td>Kubernetes with custom operators</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static configuration files</td>\n<td>Consul or etcd with DNS integration</td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td>HAProxy with static config</td>\n<td>Envoy with dynamic configuration</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Prometheus + Grafana</td>\n<td>Datadog or New Relic APM</td>\n</tr>\n<tr>\n<td>Log Aggregation</td>\n<td>File-based logging</td>\n<td>ELK Stack or Splunk</td>\n</tr>\n<tr>\n<td>Secret Management</td>\n<td>Environment variables</td>\n<td>HashiCorp Vault or AWS Secrets Manager</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>media-pipeline/\n├── cmd/\n│   ├── coordinator/           ← cluster coordinator service\n│   ├── worker/               ← distributed worker nodes\n│   └── scheduler/            ← intelligent job scheduler\n├── internal/\n│   ├── scaling/\n│   │   ├── cluster.go        ← node registration and discovery\n│   │   ├── balancer.go       ← load balancing algorithms  \n│   │   ├── autoscaler.go     ← dynamic scaling policies\n│   │   └── storage.go        ← distributed storage abstraction\n│   ├── ai/\n│   │   ├── cropping.go       ← smart cropping algorithms\n│   │   ├── analysis.go       ← content analysis pipelines\n│   │   └── models.go         ← AI model integration\n│   ├── integration/\n│   │   ├── cdn.go           ← CDN provider integrations\n│   │   ├── cloud.go         ← multi-cloud storage\n│   │   └── external.go      ← third-party API clients\n│   └── monitoring/\n│       ├── metrics.go       ← performance metrics collection\n│       ├── tracing.go       ← distributed tracing\n│       └── alerts.go        ← alerting and notification\n├── deployments/\n│   ├── kubernetes/          ← K8s manifests\n│   ├── terraform/           ← infrastructure as code\n│   └── docker/              ← container configurations\n└── docs/\n    ├── scaling-guide.md     ← operational scaling procedures\n    ├── integration-api.md   ← external API documentation\n    └── monitoring-runbook.md ← troubleshooting procedures</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Cluster Node Registry</strong> - Complete implementation for node discovery and health tracking:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> aioredis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> GPUtil</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NodeCapability</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IMAGE_PROCESSING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"image\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VIDEO_TRANSCODING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"video\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    GPU_ACCELERATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"gpu\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HIGH_MEMORY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"highmem\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> NodeInfo</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hostname: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    capabilities: List[NodeCapability]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_concurrent_jobs: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_load: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_gb: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    network_region: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_heartbeat: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            **</span><span style=\"color:#E1E4E8\">asdict(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'capabilities'</span><span style=\"color:#E1E4E8\">: [c.value </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> c </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.capabilities],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'last_heartbeat'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.last_heartbeat</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_dict</span><span style=\"color:#E1E4E8\">(cls, data: Dict) -> </span><span style=\"color:#9ECBFF\">'NodeInfo'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            node_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'node_id'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            hostname</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'hostname'</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            capabilities</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[NodeCapability(c) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> c </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data[</span><span style=\"color:#9ECBFF\">'capabilities'</span><span style=\"color:#E1E4E8\">]],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            max_concurrent_jobs</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'max_concurrent_jobs'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            current_load</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'current_load'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_gb</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'memory_gb'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            gpu_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'gpu_count'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            network_region</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'network_region'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            last_heartbeat</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data[</span><span style=\"color:#9ECBFF\">'last_heartbeat'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ClusterRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, redis_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, heartbeat_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> redis_url</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.heartbeat_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> heartbeat_interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.local_node_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._heartbeat_task </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize Redis connection and start heartbeat.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.redis </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> aioredis.from_url(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.redis_url)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.ping()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to connect to Redis: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> register_node</span><span style=\"color:#E1E4E8\">(self, node_info: NodeInfo) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register this node in the cluster registry.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.local_node_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node_info.node_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        node_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"cluster:nodes:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">node_info.node_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Store node information with TTL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.hset(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node_key,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"info\"</span><span style=\"color:#E1E4E8\">: json.dumps(node_info.to_dict()),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"registered_at\"</span><span style=\"color:#E1E4E8\">: time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.expire(node_key, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.heartbeat_interval </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Add to active nodes set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.sadd(</span><span style=\"color:#9ECBFF\">\"cluster:active_nodes\"</span><span style=\"color:#E1E4E8\">, node_info.node_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Start heartbeat task</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._heartbeat_task </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asyncio.create_task(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._heartbeat_loop(node_info)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Node </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">node_info.node_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> registered successfully\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to register node: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _heartbeat_loop</span><span style=\"color:#E1E4E8\">(self, node_info: NodeInfo):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Continuous heartbeat to maintain node registration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Update current system metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node_info.current_load </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> psutil.cpu_percent(</span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node_info.last_heartbeat </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Update GPU utilization if available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> node_info.gpu_count </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        gpus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> GPUtil.getGPUs()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        if</span><span style=\"color:#E1E4E8\"> gpus:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            node_info.current_load </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                node_info.current_load,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                gpus[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].load </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    except</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        pass</span><span style=\"color:#6A737D\">  # GPU monitoring is optional</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"cluster:nodes:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">node_info.node_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.hset(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    node_key,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    mapping</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"info\"</span><span style=\"color:#E1E4E8\">: json.dumps(node_info.to_dict())}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.expire(node_key, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.heartbeat_interval </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> asyncio.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.heartbeat_interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> asyncio.CancelledError:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                break</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Heartbeat error: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> asyncio.sleep(</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Retry on error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get_active_nodes</span><span style=\"color:#E1E4E8\">(self) -> List[NodeInfo]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get list of all currently active nodes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            node_ids </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.smembers(</span><span style=\"color:#9ECBFF\">\"cluster:active_nodes\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> node_id </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> node_ids:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"cluster:nodes:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">node_id.decode()</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.hget(node_key, </span><span style=\"color:#9ECBFF\">\"info\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> node_data:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    node_info </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> NodeInfo.from_dict(json.loads(node_data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Check if node is still alive (heartbeat within 3 intervals)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> node_info.last_heartbeat </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.heartbeat_interval </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        nodes.append(node_info)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                        # Remove stale node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cleanup_stale_node(node_id.decode())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to get active nodes: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _cleanup_stale_node</span><span style=\"color:#E1E4E8\">(self, node_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove stale node from registry.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.srem(</span><span style=\"color:#9ECBFF\">\"cluster:active_nodes\"</span><span style=\"color:#E1E4E8\">, node_id)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.delete(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"cluster:nodes:</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">node_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Removed stale node: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">node_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> find_best_nodes</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            job_requirements: Dict,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">) -> List[NodeInfo]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find best nodes for a job based on requirements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        active_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_active_nodes()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> active_nodes:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Filter nodes by capabilities</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        required_capabilities </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> job_requirements.get(</span><span style=\"color:#9ECBFF\">'capabilities'</span><span style=\"color:#E1E4E8\">, [])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> required_capabilities:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            filtered_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> active_nodes:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                node_caps </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {cap.value </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> cap </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> node.capabilities}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(required_capabilities).issubset(node_caps):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    filtered_nodes.append(node)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            active_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> filtered_nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Score nodes based on current load and capabilities</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scored_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> active_nodes:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            score </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._calculate_node_score(node, job_requirements)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            scored_nodes.append((score, node))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sort by score (higher is better) and return top nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scored_nodes.sort(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#FFAB70\">reverse</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> [node </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> _, node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> scored_nodes[:count]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_node_score</span><span style=\"color:#E1E4E8\">(self, node: NodeInfo, requirements: Dict) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate suitability score for a node.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        score </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 100.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Penalize high load</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        score </span><span style=\"color:#F97583\">-=</span><span style=\"color:#E1E4E8\"> node.current_load </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Bonus for GPU capability if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#9ECBFF\"> 'gpu'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> requirements.get(</span><span style=\"color:#9ECBFF\">'capabilities'</span><span style=\"color:#E1E4E8\">, []):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> NodeCapability.</span><span style=\"color:#79B8FF\">GPU_ACCELERATION</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> node.capabilities:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 20.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                score </span><span style=\"color:#F97583\">-=</span><span style=\"color:#79B8FF\"> 50.0</span><span style=\"color:#6A737D\">  # Heavy penalty if GPU required but not available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Bonus for sufficient memory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        required_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> requirements.get(</span><span style=\"color:#9ECBFF\">'min_memory_gb'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> node.memory_gb </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> required_memory:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 10.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            score </span><span style=\"color:#F97583\">-=</span><span style=\"color:#79B8FF\"> 30.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Network locality bonus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        preferred_region </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> requirements.get(</span><span style=\"color:#9ECBFF\">'network_region'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> preferred_region </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> node.network_region </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> preferred_region:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 15.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">, score)  </span><span style=\"color:#6A737D\"># Ensure non-negative score</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> shutdown</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Clean shutdown of registry client.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._heartbeat_task:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._heartbeat_task.cancel()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._heartbeat_task</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> asyncio.CancelledError:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.local_node_id </span><span style=\"color:#F97583\">and</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cleanup_stale_node(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.local_node_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.redis.close()</span></span></code></pre></div>\n\n<p><strong>Intelligent Load Balancer</strong> - Complete implementation with weighted scoring:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> heapq</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    IMAGE_RESIZE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"image_resize\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    VIDEO_TRANSCODE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"video_transcode\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    THUMBNAIL_GENERATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"thumbnail_gen\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BATCH_PROCESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"batch_process\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JobRequirements</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    job_type: JobType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_duration: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#6A737D\">  # seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_mb: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cores: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    requires_gpu: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_size_mb: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    priority: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    network_region: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoadBalancer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, cluster_registry: ClusterRegistry):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cluster_registry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.node_assignments </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># node_id -> list of assigned jobs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.performance_history </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}  </span><span style=\"color:#6A737D\"># (node_id, job_type) -> avg_duration</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_rebalance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rebalance_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#6A737D\">  # seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> assign_job</span><span style=\"color:#E1E4E8\">(self, job: </span><span style=\"color:#9ECBFF\">'ProcessingJob'</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Assign job to the most suitable node.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        requirements </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._extract_job_requirements(job)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Get candidate nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        candidates </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.registry.find_best_nodes(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'capabilities'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._job_type_to_capabilities(requirements.job_type),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'min_memory_gb'</span><span style=\"color:#E1E4E8\">: requirements.memory_mb </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'network_region'</span><span style=\"color:#E1E4E8\">: requirements.network_region</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#6A737D\">  # Get top 5 candidates for detailed scoring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> candidates:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"No suitable nodes found for job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job.job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Score candidates with detailed algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        best_node </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        best_score </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> -</span><span style=\"color:#79B8FF\">1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> candidates:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            score </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._calculate_detailed_score(node, requirements)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> score </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> best_score:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                best_score </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> score</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                best_node </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> best_node:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Track assignment for load balancing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> best_node.node_id </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.node_assignments:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.node_assignments[best_node.node_id] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.node_assignments[best_node.node_id].append(job.job_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Assigned job </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">job.job_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> to node </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">best_node.node_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (score: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">best_score</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> best_node.node_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _calculate_detailed_score</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                      node: NodeInfo, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                      requirements: JobRequirements) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate detailed suitability score for node.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        base_score </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 100.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Current load penalty (0-40 point penalty)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        load_penalty </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node.current_load </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        base_score </span><span style=\"color:#F97583\">-=</span><span style=\"color:#E1E4E8\"> load_penalty</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Memory adequacy (bonus for having enough, penalty for insufficient)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        memory_ratio </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node.memory_gb </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> /</span><span style=\"color:#E1E4E8\"> requirements.memory_mb</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> memory_ratio </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 15.0</span><span style=\"color:#6A737D\">  # Plenty of memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> memory_ratio </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1.5</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#6A737D\">  # Adequate memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> memory_ratio </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#6A737D\">   # Just enough memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_score </span><span style=\"color:#F97583\">-=</span><span style=\"color:#79B8FF\"> 30.0</span><span style=\"color:#6A737D\">  # Insufficient memory</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # GPU requirement matching</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> requirements.requires_gpu:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> NodeCapability.</span><span style=\"color:#79B8FF\">GPU_ACCELERATION</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> node.capabilities:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                base_score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 25.0</span><span style=\"color:#6A737D\">  # GPU available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#6A737D\">  # Cannot handle GPU jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Historical performance bonus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        perf_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (node.node_id, requirements.job_type.value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> perf_key </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.performance_history:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            avg_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.performance_history[perf_key]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            expected_duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> requirements.estimated_duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> avg_duration </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> expected_duration </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                base_score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#6A737D\">  # Consistently fast</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> avg_duration </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> expected_duration </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1.2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                base_score </span><span style=\"color:#F97583\">-=</span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#6A737D\">  # Consistently slow</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Current assignment load penalty</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        current_assignments </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.node_assignments.get(node.node_id, []))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> current_assignments </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> node.max_concurrent_jobs:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#6A737D\">  # Node at capacity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> current_assignments </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> node.max_concurrent_jobs </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_score </span><span style=\"color:#F97583\">-=</span><span style=\"color:#79B8FF\"> 20.0</span><span style=\"color:#6A737D\">  # Node nearly at capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Network locality bonus</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> (requirements.network_region </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            node.network_region </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> requirements.network_region):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 12.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Priority job bonus for less loaded nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> requirements.priority </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 8</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># High priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_score </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> node.current_load) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">, base_score)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _extract_job_requirements</span><span style=\"color:#E1E4E8\">(self, job: </span><span style=\"color:#9ECBFF\">'ProcessingJob'</span><span style=\"color:#E1E4E8\">) -> JobRequirements:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract resource requirements from processing job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This would analyze the job's output specifications to determine requirements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Default requirements based on job type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'video'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> spec.format </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> spec </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> job.output_specifications):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> JobRequirements(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                job_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">JobType.</span><span style=\"color:#79B8FF\">VIDEO_TRANSCODE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                estimated_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># 5 minutes default</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                memory_mb</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">2048</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cpu_cores</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                requires_gpu</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                input_size_mb</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Estimated</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job.priority.value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> JobRequirements(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                job_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">JobType.</span><span style=\"color:#79B8FF\">IMAGE_RESIZE</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                estimated_duration</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># 30 seconds default</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                memory_mb</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">512</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                cpu_cores</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                requires_gpu</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                input_size_mb</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">,   </span><span style=\"color:#6A737D\"># Estimated</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job.priority.value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _job_type_to_capabilities</span><span style=\"color:#E1E4E8\">(self, job_type: JobType) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Map job types to required node capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        capability_map </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            JobType.</span><span style=\"color:#79B8FF\">IMAGE_RESIZE</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'image'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            JobType.</span><span style=\"color:#79B8FF\">VIDEO_TRANSCODE</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'video'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'gpu'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            JobType.</span><span style=\"color:#79B8FF\">THUMBNAIL_GENERATE</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'image'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            JobType.</span><span style=\"color:#79B8FF\">BATCH_PROCESS</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">'image'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'video'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> capability_map.get(job_type, [])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> record_job_completion</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  node_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  job_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  job_type: JobType,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  duration: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record job completion for performance tracking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update performance history</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        perf_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (node_id, job_type.value)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> perf_key </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.performance_history:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.performance_history[perf_key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> duration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Exponential moving average</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.performance_history[perf_key] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                0.7</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.performance_history[perf_key] </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 0.3</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Remove from current assignments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> node_id </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.node_assignments:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.node_assignments[node_id].remove(job_id)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span><span style=\"color:#6A737D\">  # Job not in list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check if rebalancing is needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_rebalance </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.rebalance_interval:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._rebalance_if_needed()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.last_rebalance </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> _rebalance_if_needed</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Rebalance load across nodes if imbalance detected.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        active_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.registry.get_active_nodes()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(active_nodes) </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#6A737D\">  # No rebalancing needed with &#x3C; 2 nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Calculate load distribution</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        loads </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> active_nodes:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            current_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.node_assignments.get(node.node_id, []))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            load_ratio </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> current_jobs </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> node.max_concurrent_jobs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            loads.append((load_ratio, node.node_id))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        loads.sort()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        min_load </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loads[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        max_load </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> loads[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # If imbalance > 30%, log recommendation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> max_load </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> min_load </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0.3</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Load imbalance detected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">min_load</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> to </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">max_load</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Consider implementing job migration for better balance\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get_cluster_statistics</span><span style=\"color:#E1E4E8\">(self) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current cluster load statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        active_nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.registry.get_active_nodes()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_capacity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(node.max_concurrent_jobs </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> active_nodes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_assigned </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(jobs) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> jobs </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.node_assignments.values())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        node_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> active_nodes:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            assigned_jobs </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.node_assignments.get(node.node_id, []))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            utilization </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> assigned_jobs </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> node.max_concurrent_jobs </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> node.max_concurrent_jobs </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            node_stats.append({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'node_id'</span><span style=\"color:#E1E4E8\">: node.node_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'assigned_jobs'</span><span style=\"color:#E1E4E8\">: assigned_jobs,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'max_jobs'</span><span style=\"color:#E1E4E8\">: node.max_concurrent_jobs,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'utilization'</span><span style=\"color:#E1E4E8\">: utilization,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'cpu_load'</span><span style=\"color:#E1E4E8\">: node.current_load,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'capabilities'</span><span style=\"color:#E1E4E8\">: [cap.value </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> cap </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> node.capabilities]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'total_nodes'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(active_nodes),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'total_capacity'</span><span style=\"color:#E1E4E8\">: total_capacity,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'total_assigned'</span><span style=\"color:#E1E4E8\">: total_assigned,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'cluster_utilization'</span><span style=\"color:#E1E4E8\">: total_assigned </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> total_capacity </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> total_capacity </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'node_statistics'</span><span style=\"color:#E1E4E8\">: node_stats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Auto-scaling Controller</strong> - Framework for dynamic scaling decisions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AutoScaler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, cluster_registry: ClusterRegistry, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 load_balancer: LoadBalancer,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 config: Dict):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.registry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cluster_registry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.load_balancer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> load_balancer</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.scaling_decisions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> evaluate_scaling_needs</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Evaluate current cluster state and return scaling recommendations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Dict mapping node types to desired count changes</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Example: {'gpu_nodes': +2, 'general_nodes': -1}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get current cluster statistics from load balancer</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Analyze queue depth by job type from Redis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate average wait times for different job types  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check if any resource type is consistently over 80% utilized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check if any resource type is consistently under 30% utilized</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Apply scaling policies from config (min/max nodes, cooldown periods)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return scaling recommendations with justification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use exponential moving averages for load metrics to avoid thrashing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> execute_scaling_action</span><span style=\"color:#E1E4E8\">(self, node_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, count_change: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Execute scaling action through infrastructure provider.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            node_type: Type of nodes to scale ('gpu', 'general', 'highmem')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            count_change: Positive to scale up, negative to scale down</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if scaling action was initiated successfully</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate scaling action against configured limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For scale down: ensure nodes to terminate have no active jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For scale up: determine optimal instance types/sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Call infrastructure API (AWS Auto Scaling, K8s HPA, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Record scaling action with timestamp and reason</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Set cooldown period to prevent rapid scaling oscillations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Implement graceful scale-down by draining nodes before termination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Smart Content Analysis</strong> - Framework for AI-powered processing decisions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ContentAnalyzer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, model_config: Dict):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.models </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> model_config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> analyze_image_content</span><span style=\"color:#E1E4E8\">(self, image_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze image content to optimize processing parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Analysis results including optimal formats, quality settings, crop regions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load image and extract basic properties (dimensions, format, size)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run object detection to identify main subjects and regions of interest</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate saliency map to understand visual importance across image</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Analyze color distribution and complexity for compression optimization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Detect faces and important objects for smart cropping recommendations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Generate optimal crop coordinates for different aspect ratios</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Recommend optimal output formats based on content characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Calculate quality settings that balance file size and visual quality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Cache analysis results using content hash to avoid recomputation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> optimize_video_encoding</span><span style=\"color:#E1E4E8\">(self, video_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    target_formats: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze video content to optimize encoding parameters.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Optimized encoding parameters for each target format</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract video metadata (duration, resolution, bitrate, codec)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Perform scene detection to identify different content types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Analyze motion vectors and temporal complexity per scene</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Detect content types (talking head, action, graphics, text)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate optimal bitrate curves for variable rate encoding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Determine keyframe placement at scene boundaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Select encoder presets based on content complexity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Generate separate parameter sets for each target format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use ffprobe for metadata extraction and scene detection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Horizontal Scaling Validation</strong>:</p>\n<ol>\n<li>Deploy cluster registry with 3 nodes: <code>python -m pytest tests/test_cluster_registry.py -v</code></li>\n<li>Verify node discovery: Check that all nodes appear in Redis registry within 60 seconds</li>\n<li>Test load balancer: Submit 10 mixed jobs, verify distribution across nodes based on capabilities</li>\n<li>Simulate node failure: Stop one node, verify jobs get reassigned within 2 minutes</li>\n<li>Test auto-scaling: Generate sustained load, verify new nodes join cluster automatically</li>\n</ol>\n<p><strong>Advanced Features Integration</strong>:</p>\n<ol>\n<li>Deploy AI analysis services: <code>python -m pytest tests/test_content_analysis.py -v</code></li>\n<li>Upload test images with different characteristics (photos, graphics, text)</li>\n<li>Verify smart cropping generates different crops for square vs landscape outputs</li>\n<li>Test adaptive video encoding with sample talking-head vs action footage</li>\n<li>Validate that content analysis improves processing quality metrics by 15%+</li>\n</ol>\n<p><strong>External Integration Testing</strong>:</p>\n<ol>\n<li>Configure multi-CDN setup with test accounts</li>\n<li>Upload processed media and verify delivery through optimal CDN per region</li>\n<li>Test storage lifecycle: Verify files transition to cheaper storage after 30 days</li>\n<li>Load test third-party APIs with rate limiting and fallback logic</li>\n<li>Validate monitoring dashboards show all scaling and processing metrics</li>\n</ol>\n<p>Signs of successful implementation:</p>\n<ul>\n<li><strong>Cluster coordination</strong>: Nodes register/deregister cleanly, load balancing distributes work optimally</li>\n<li><strong>Intelligent scaling</strong>: System scales up under load and scales down during quiet periods</li>\n<li><strong>Quality improvements</strong>: AI-powered features measurably improve output quality</li>\n<li><strong>Cost optimization</strong>: Multi-tier storage and intelligent CDN routing reduce operational costs</li>\n<li><strong>Reliability</strong>: System gracefully handles node failures, API outages, and traffic spikes</li>\n</ul>\n<h2 id=\"glossary-and-reference\">Glossary and Reference</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (1-3) as this terminology reference supports understanding across image processing, video transcoding, and job queue components</p>\n</blockquote>\n<h3 id=\"mental-model-technical-dictionary-for-digital-media\">Mental Model: Technical Dictionary for Digital Media</h3>\n<p>Think of this glossary as a technical dictionary specifically curated for digital media processing systems. Just as a medical dictionary helps doctors communicate precisely about anatomical structures and procedures, this reference ensures everyone on the team uses consistent terminology when discussing codecs, queuing systems, and processing workflows. Each term has been carefully chosen to reflect industry standards while maintaining clarity for developers at all experience levels.</p>\n<p>The terminology is organized into three categories that mirror the major architectural domains of our media processing pipeline: media processing concepts that deal with the technical aspects of manipulating images and videos, architecture and queue concepts that handle the distributed systems challenges of coordinating work across multiple processes, and acronyms that provide quick reference for the alphabet soup of technical abbreviations common in media processing.</p>\n<h3 id=\"media-processing-terms\">Media Processing Terms</h3>\n<p>The media processing domain contains a rich vocabulary of technical terms that describe how digital content is analyzed, transformed, and optimized. These terms form the foundation for understanding how images and videos are manipulated programmatically.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>processing job</strong></td>\n<td>Unit of work containing input file and output specifications that defines a complete media transformation task</td>\n<td>Core concept used throughout job queue and worker coordination</td>\n</tr>\n<tr>\n<td><strong>media metadata</strong></td>\n<td>Technical information extracted from media files including dimensions, format details, creation timestamps, and embedded data</td>\n<td>Essential for processing decisions and output optimization</td>\n</tr>\n<tr>\n<td><strong>output specification</strong></td>\n<td>Configuration defining desired processing output including format, dimensions, quality settings, and optimization parameters</td>\n<td>Drives all media transformation operations</td>\n</tr>\n<tr>\n<td><strong>interpolation algorithm</strong></td>\n<td>Mathematical method for calculating new pixel values during resize operations, affecting quality and performance</td>\n<td>Critical for image resizing quality - Lanczos for downscaling, bicubic for upscaling</td>\n</tr>\n<tr>\n<td><strong>EXIF orientation</strong></td>\n<td>Metadata tag indicating camera rotation when photo was taken, requiring image rotation before processing</td>\n<td>Common source of bugs - must rotate before resize/crop operations</td>\n</tr>\n<tr>\n<td><strong>color space conversion</strong></td>\n<td>Transformation between different color representation systems like RGB, CMYK, and YUV</td>\n<td>Required for format conversion and display optimization</td>\n</tr>\n<tr>\n<td><strong>smart cropping</strong></td>\n<td>Automated cropping that preserves visually important image regions using content analysis algorithms</td>\n<td>Preserves subject matter better than center-crop for thumbnails</td>\n</tr>\n<tr>\n<td><strong>metadata stripping</strong></td>\n<td>Removal of embedded information like GPS coordinates and camera details for privacy protection</td>\n<td>Security requirement for public-facing image processing</td>\n</tr>\n<tr>\n<td><strong>lossy compression</strong></td>\n<td>Compression that reduces file size by discarding some image data, trading quality for smaller files</td>\n<td>JPEG and WebP compression - quality parameter controls data loss</td>\n</tr>\n<tr>\n<td><strong>aspect ratio preservation</strong></td>\n<td>Maintaining original width to height ratio during resize operations to prevent image distortion</td>\n<td>Default behavior - use letterboxing or cropping when aspect ratios don&#39;t match</td>\n</tr>\n<tr>\n<td><strong>adaptive bitrate streaming</strong></td>\n<td>Delivery technique with multiple quality variants allowing clients to switch based on network conditions</td>\n<td>Core video streaming technology - requires synchronized segments</td>\n</tr>\n<tr>\n<td><strong>quality ladder</strong></td>\n<td>Set of video renditions at different resolutions and bitrates optimized for various network conditions</td>\n<td>Standard includes 240p, 360p, 480p, 720p, 1080p variants</td>\n</tr>\n<tr>\n<td><strong>keyframe alignment</strong></td>\n<td>Synchronized keyframe placement across ABR variants enabling smooth quality switching</td>\n<td>Critical for seamless adaptive streaming experience</td>\n</tr>\n<tr>\n<td><strong>CRF encoding</strong></td>\n<td>Constant rate factor approach maintaining consistent perceptual quality across video content</td>\n<td>Preferred over target bitrate for quality-focused encoding</td>\n</tr>\n<tr>\n<td><strong>GOP structure</strong></td>\n<td>Group of pictures organization with keyframes and predicted frames affecting seek performance</td>\n<td>Keyframe interval of 2-4 seconds optimal for streaming</td>\n</tr>\n<tr>\n<td><strong>container format</strong></td>\n<td>File wrapper that holds video, audio and metadata streams in a standardized structure</td>\n<td>MP4 for broad compatibility, WebM for web-optimized delivery</td>\n</tr>\n<tr>\n<td><strong>codec compatibility matrix</strong></td>\n<td>Validation table ensuring codec/container combinations work on target platforms</td>\n<td>H.264 + MP4 for maximum compatibility, VP9 + WebM for efficiency</td>\n</tr>\n<tr>\n<td><strong>hardware acceleration</strong></td>\n<td>GPU-based video encoding for improved performance and reduced CPU usage</td>\n<td>Significant speedup but requires compatible hardware and drivers</td>\n</tr>\n<tr>\n<td><strong>progressive download</strong></td>\n<td>Streaming technique allowing playback before complete file download using optimized file structure</td>\n<td>Requires &#39;faststart&#39; flag to move metadata to file beginning</td>\n</tr>\n<tr>\n<td><strong>segment alignment</strong></td>\n<td>Synchronized segment boundaries across ABR quality variants for smooth switching</td>\n<td>Essential for DASH and HLS adaptive streaming</td>\n</tr>\n</tbody></table>\n<h3 id=\"architecture-and-queue-terms\">Architecture and Queue Terms</h3>\n<p>The distributed systems architecture of our media processing pipeline introduces concepts related to coordinating work across multiple processes, managing job lifecycles, and ensuring reliable processing under various failure conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>job queue</strong></td>\n<td>Message queue system for distributing processing tasks across available worker processes</td>\n<td>Central coordination mechanism using Redis or RabbitMQ</td>\n</tr>\n<tr>\n<td><strong>worker process</strong></td>\n<td>Background process that executes media processing operations by consuming jobs from the queue</td>\n<td>Isolated processes with resource limits and health monitoring</td>\n</tr>\n<tr>\n<td><strong>progress tracking</strong></td>\n<td>System for monitoring and reporting job completion status through multiple processing stages</td>\n<td>Real-time updates via webhooks and queryable status API</td>\n</tr>\n<tr>\n<td><strong>webhook notification</strong></td>\n<td>HTTP callback sent on job status changes including progress updates and completion events</td>\n<td>Reliable delivery with retry logic and signature verification</td>\n</tr>\n<tr>\n<td><strong>resource-aware scheduling</strong></td>\n<td>Job distribution based on node capabilities and current resource utilization</td>\n<td>Prevents overloading workers with memory-intensive video processing</td>\n</tr>\n<tr>\n<td><strong>stage-based progress</strong></td>\n<td>Progress reporting based on processing phases rather than simple percentage complete</td>\n<td>More accurate than time-based estimation for complex operations</td>\n</tr>\n<tr>\n<td><strong>exponential backoff</strong></td>\n<td>Retry strategy with increasing delays between attempts to prevent overwhelming failed services</td>\n<td>Standard pattern: base_delay * (backoff_factor ^ attempt_number)</td>\n</tr>\n<tr>\n<td><strong>job lifecycle</strong></td>\n<td>Progression of job through pending, processing, completed/failed states with defined transitions</td>\n<td>State machine prevents invalid transitions and ensures consistency</td>\n</tr>\n<tr>\n<td><strong>serialization</strong></td>\n<td>Converting data structures to/from JSON for storage and transmission between components</td>\n<td>Consistent format enables cross-language compatibility</td>\n</tr>\n<tr>\n<td><strong>state machine</strong></td>\n<td>Formal model governing valid job status transitions and preventing invalid state changes</td>\n<td>Ensures job integrity and proper cleanup of resources</td>\n</tr>\n<tr>\n<td><strong>resource constraints</strong></td>\n<td>Limits on memory, CPU, and processing time for jobs to prevent system overload</td>\n<td>Video processing limited by memory, image processing by CPU</td>\n</tr>\n<tr>\n<td><strong>dead letter queue</strong></td>\n<td>Queue for jobs that failed permanently after exhausting retry attempts</td>\n<td>Prevents infinite retry loops while preserving failed jobs for analysis</td>\n</tr>\n<tr>\n<td><strong>worker coordination</strong></td>\n<td>Distributed process management including job assignment, health monitoring, and scaling</td>\n<td>Maintains optimal worker pool size based on queue depth</td>\n</tr>\n<tr>\n<td><strong>heartbeat monitoring</strong></td>\n<td>Health check system for worker processes to detect failures and trigger recovery</td>\n<td>Regular status updates enable prompt detection of worker crashes</td>\n</tr>\n<tr>\n<td><strong>priority queue</strong></td>\n<td>Queue that processes higher priority jobs first while maintaining fairness for lower priority work</td>\n<td>Urgent jobs bypass normal queue while preventing starvation</td>\n</tr>\n<tr>\n<td><strong>atomic operation</strong></td>\n<td>Indivisible database operation that prevents race conditions in job status updates</td>\n<td>Critical for maintaining consistency in distributed processing</td>\n</tr>\n<tr>\n<td><strong>deduplication</strong></td>\n<td>Preventing duplicate job processing through idempotency keys and job fingerprinting</td>\n<td>Prevents wasted resources and conflicting outputs</td>\n</tr>\n<tr>\n<td><strong>graceful shutdown</strong></td>\n<td>Orderly termination allowing job completion before process exit</td>\n<td>Prevents job corruption and resource leaks during deployment</td>\n</tr>\n<tr>\n<td><strong>sequence number</strong></td>\n<td>Monotonically increasing identifier for preventing out-of-order progress updates</td>\n<td>Ensures progress only moves forward despite network delays</td>\n</tr>\n<tr>\n<td><strong>notification threshold</strong></td>\n<td>Progress percentage that triggers webhook delivery to reduce notification spam</td>\n<td>Typically 10%, 25%, 50%, 75%, 90%, 100% to balance updates with overhead</td>\n</tr>\n<tr>\n<td><strong>HMAC signature</strong></td>\n<td>Cryptographic hash for webhook authentication and integrity verification</td>\n<td>Prevents spoofed notifications using shared secret</td>\n</tr>\n<tr>\n<td><strong>monotonic progression</strong></td>\n<td>Progress values that only increase, never decrease, preventing confusing status reversals</td>\n<td>Enforced through sequence numbers and validation logic</td>\n</tr>\n<tr>\n<td><strong>stage transition</strong></td>\n<td>Moving from one processing phase to the next with appropriate progress weight calculation</td>\n<td>Enables accurate overall progress despite varying stage complexity</td>\n</tr>\n<tr>\n<td><strong>webhook delivery storm</strong></td>\n<td>Excessive notification traffic that overwhelms recipient systems</td>\n<td>Prevented through batching, rate limiting, and threshold-based delivery</td>\n</tr>\n<tr>\n<td><strong>progress reversal</strong></td>\n<td>Backward movement of progress percentage due to race conditions between workers</td>\n<td>Prevented through atomic updates and sequence number validation</td>\n</tr>\n<tr>\n<td><strong>hybrid storage</strong></td>\n<td>Combination of Redis for real-time access and PostgreSQL for durability</td>\n<td>Redis for active jobs, PostgreSQL for historical data and recovery</td>\n</tr>\n<tr>\n<td><strong>idempotency key</strong></td>\n<td>Unique identifier for detecting and preventing duplicate operations</td>\n<td>Prevents double-processing when clients retry requests</td>\n</tr>\n<tr>\n<td><strong>message serialization</strong></td>\n<td>Converting data structures to JSON for storage and transmission in message queues</td>\n<td>Standardized format enables language-agnostic worker processes</td>\n</tr>\n<tr>\n<td><strong>circuit breaker</strong></td>\n<td>Pattern to prevent cascade failures when external dependencies become unavailable</td>\n<td>Fails fast when FFmpeg or storage systems are unresponsive</td>\n</tr>\n<tr>\n<td><strong>retry storm</strong></td>\n<td>Overwhelming retry attempts that prevent system recovery during outages</td>\n<td>Prevented through exponential backoff and circuit breakers</td>\n</tr>\n<tr>\n<td><strong>resource cleanup</strong></td>\n<td>Systematic cleanup of temporary files and resources after job completion or failure</td>\n<td>Essential for preventing disk space exhaustion</td>\n</tr>\n<tr>\n<td><strong>worker recovery</strong></td>\n<td>Process of detecting and restarting failed worker processes</td>\n<td>Maintains processing capacity despite individual worker failures</td>\n</tr>\n<tr>\n<td><strong>error classification</strong></td>\n<td>Categorizing errors to determine appropriate retry strategy</td>\n<td>Transient, permanent, and resource errors require different handling</td>\n</tr>\n<tr>\n<td><strong>horizontal scaling</strong></td>\n<td>Distributing processing across multiple nodes for increased capacity</td>\n<td>Add more worker nodes rather than upgrading individual machines</td>\n</tr>\n<tr>\n<td><strong>cluster coordination</strong></td>\n<td>Managing distributed nodes through service discovery and health monitoring</td>\n<td>Enables dynamic scaling and load balancing across regions</td>\n</tr>\n<tr>\n<td><strong>adaptive load balancing</strong></td>\n<td>Dynamic job assignment using weighted scoring algorithms based on node performance</td>\n<td>Considers CPU, memory, and historical processing times</td>\n</tr>\n<tr>\n<td><strong>auto-scaling policies</strong></td>\n<td>Rules for automatically adding or removing nodes based on demand metrics</td>\n<td>Responds to queue depth and processing latency thresholds</td>\n</tr>\n<tr>\n<td><strong>content-aware optimization</strong></td>\n<td>Using AI analysis to optimize processing parameters for specific media content</td>\n<td>Adjusts encoding settings based on scene complexity and content type</td>\n</tr>\n<tr>\n<td><strong>smart cropping algorithms</strong></td>\n<td>AI-powered cropping that preserves visually important image regions</td>\n<td>Identifies faces, text, and salient objects for optimal framing</td>\n</tr>\n<tr>\n<td><strong>multi-CDN strategies</strong></td>\n<td>Using multiple content delivery networks for optimal global performance</td>\n<td>Reduces latency and provides redundancy for media delivery</td>\n</tr>\n<tr>\n<td><strong>storage lifecycle management</strong></td>\n<td>Automatically transitioning files between storage tiers based on access patterns</td>\n<td>Hot storage for active processing, cold storage for archives</td>\n</tr>\n<tr>\n<td><strong>hybrid processing architectures</strong></td>\n<td>Combining on-premise processing with cloud-based specialized services</td>\n<td>Balances cost, latency, and capability requirements</td>\n</tr>\n<tr>\n<td><strong>circuit breaker pattern</strong></td>\n<td>Preventing cascade failures when external dependencies become unavailable</td>\n<td>Monitors failure rates and temporarily disables failing services</td>\n</tr>\n<tr>\n<td><strong>graceful degradation</strong></td>\n<td>Maintaining core functionality when advanced features or dependencies fail</td>\n<td>Continues basic processing when AI services or optimization features fail</td>\n</tr>\n</tbody></table>\n<h3 id=\"acronym-quick-reference\">Acronym Quick Reference</h3>\n<p>The media processing domain makes extensive use of technical acronyms and abbreviations that can be overwhelming for developers new to the field. This reference provides quick definitions and context for the most commonly encountered terms.</p>\n<table>\n<thead>\n<tr>\n<th>Acronym</th>\n<th>Full Form</th>\n<th>Definition</th>\n<th>Usage Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HLS</strong></td>\n<td>HTTP Live Streaming</td>\n<td>Apple&#39;s adaptive bitrate streaming protocol using M3U8 playlists and TS segments</td>\n<td>Primary streaming format for iOS and Safari compatibility</td>\n</tr>\n<tr>\n<td><strong>DASH</strong></td>\n<td>Dynamic Adaptive Streaming over HTTP</td>\n<td>ISO standard for adaptive bitrate streaming using MPD manifests and fragmented MP4</td>\n<td>Industry standard streaming protocol with broad device support</td>\n</tr>\n<tr>\n<td><strong>EXIF</strong></td>\n<td>Exchangeable Image File Format</td>\n<td>Metadata standard for digital images including camera settings and GPS coordinates</td>\n<td>Contains orientation, timestamp, and privacy-sensitive location data</td>\n</tr>\n<tr>\n<td><strong>CRF</strong></td>\n<td>Constant Rate Factor</td>\n<td>Video encoding mode that maintains consistent perceptual quality</td>\n<td>Preferred over target bitrate for quality-focused encoding (18-23 range)</td>\n</tr>\n<tr>\n<td><strong>GOP</strong></td>\n<td>Group of Pictures</td>\n<td>Video compression structure organizing keyframes and predicted frames</td>\n<td>Affects seek performance and adaptive streaming segment boundaries</td>\n</tr>\n<tr>\n<td><strong>ABR</strong></td>\n<td>Adaptive Bitrate</td>\n<td>Streaming technique with multiple quality variants for network adaptation</td>\n<td>Core technology enabling smooth playback across varying network conditions</td>\n</tr>\n<tr>\n<td><strong>API</strong></td>\n<td>Application Programming Interface</td>\n<td>Contract defining how software components communicate with each other</td>\n<td>REST endpoints for job submission and status queries</td>\n</tr>\n<tr>\n<td><strong>JSON</strong></td>\n<td>JavaScript Object Notation</td>\n<td>Lightweight data interchange format used for configuration and messaging</td>\n<td>Standard serialization format for job definitions and progress updates</td>\n</tr>\n<tr>\n<td><strong>HTTP</strong></td>\n<td>Hypertext Transfer Protocol</td>\n<td>Application protocol for distributed systems communication</td>\n<td>Transport layer for API requests and webhook notifications</td>\n</tr>\n<tr>\n<td><strong>HMAC</strong></td>\n<td>Hash-based Message Authentication Code</td>\n<td>Cryptographic hash function for message authentication and integrity</td>\n<td>Secures webhook notifications against spoofing and tampering</td>\n</tr>\n<tr>\n<td><strong>GPU</strong></td>\n<td>Graphics Processing Unit</td>\n<td>Specialized processor optimized for parallel computations</td>\n<td>Hardware acceleration for video encoding and AI-powered image analysis</td>\n</tr>\n<tr>\n<td><strong>CPU</strong></td>\n<td>Central Processing Unit</td>\n<td>Primary processor handling general computation tasks</td>\n<td>Image processing and job coordination workloads</td>\n</tr>\n<tr>\n<td><strong>RAM</strong></td>\n<td>Random Access Memory</td>\n<td>Volatile memory for active data storage during processing</td>\n<td>Critical resource constraint for large video processing operations</td>\n</tr>\n<tr>\n<td><strong>SSD</strong></td>\n<td>Solid State Drive</td>\n<td>Flash-based storage providing fast random access for temporary files</td>\n<td>Preferred for video transcoding working directory</td>\n</tr>\n<tr>\n<td><strong>CDN</strong></td>\n<td>Content Delivery Network</td>\n<td>Distributed network of servers for efficient content delivery</td>\n<td>Optimizes delivery of processed media files to end users</td>\n</tr>\n<tr>\n<td><strong>RGB</strong></td>\n<td>Red Green Blue</td>\n<td>Additive color space used for digital displays and image processing</td>\n<td>Standard color representation for most image manipulation operations</td>\n</tr>\n<tr>\n<td><strong>CMYK</strong></td>\n<td>Cyan Magenta Yellow Black</td>\n<td>Subtractive color space used for print media</td>\n<td>Requires conversion to RGB for digital processing</td>\n</tr>\n<tr>\n<td><strong>YUV</strong></td>\n<td>Luma Chroma</td>\n<td>Color space separating brightness from color information</td>\n<td>Efficient for video compression and broadcast applications</td>\n</tr>\n<tr>\n<td><strong>DPI</strong></td>\n<td>Dots Per Inch</td>\n<td>Resolution measurement for print media and high-density displays</td>\n<td>Affects image quality for print output and retina displays</td>\n</tr>\n<tr>\n<td><strong>IPTC</strong></td>\n<td>International Press Telecommunications Council</td>\n<td>Metadata standard for news and media organizations</td>\n<td>Contains copyright, caption, and editorial information</td>\n</tr>\n<tr>\n<td><strong>XMP</strong></td>\n<td>Extensible Metadata Platform</td>\n<td>Adobe&#39;s metadata framework for creative files</td>\n<td>Comprehensive metadata system supporting custom fields</td>\n</tr>\n<tr>\n<td><strong>MIME</strong></td>\n<td>Multipurpose Internet Mail Extensions</td>\n<td>Standard for indicating document types on the internet</td>\n<td>Content-Type header for proper file handling and validation</td>\n</tr>\n<tr>\n<td><strong>UUID</strong></td>\n<td>Universally Unique Identifier</td>\n<td>128-bit identifier guaranteed to be unique across systems</td>\n<td>Job IDs and correlation identifiers in distributed processing</td>\n</tr>\n<tr>\n<td><strong>TCP</strong></td>\n<td>Transmission Control Protocol</td>\n<td>Reliable connection-oriented network protocol</td>\n<td>Ensures message delivery for critical job queue operations</td>\n</tr>\n<tr>\n<td><strong>UDP</strong></td>\n<td>User Datagram Protocol</td>\n<td>Connectionless network protocol for low-latency communication</td>\n<td>Used for real-time progress updates and heartbeat monitoring</td>\n</tr>\n<tr>\n<td><strong>TLS</strong></td>\n<td>Transport Layer Security</td>\n<td>Cryptographic protocol for secure communication over networks</td>\n<td>Encrypts webhook notifications and API communication</td>\n</tr>\n<tr>\n<td><strong>REST</strong></td>\n<td>Representational State Transfer</td>\n<td>Architectural style for web services using HTTP methods</td>\n<td>Design pattern for job management and status query APIs</td>\n</tr>\n<tr>\n<td><strong>CORS</strong></td>\n<td>Cross-Origin Resource Sharing</td>\n<td>Web security feature controlling cross-domain requests</td>\n<td>Browser security for web-based media processing interfaces</td>\n</tr>\n<tr>\n<td><strong>JWT</strong></td>\n<td>JSON Web Token</td>\n<td>Compact token format for securely transmitting information</td>\n<td>Authentication mechanism for API access and webhook validation</td>\n</tr>\n<tr>\n<td><strong>SQL</strong></td>\n<td>Structured Query Language</td>\n<td>Language for managing relational databases</td>\n<td>PostgreSQL queries for job history and metadata persistence</td>\n</tr>\n<tr>\n<td><strong>ACID</strong></td>\n<td>Atomicity Consistency Isolation Durability</td>\n<td>Properties ensuring reliable database transactions</td>\n<td>Critical for job status updates and progress tracking</td>\n</tr>\n<tr>\n<td><strong>FIFO</strong></td>\n<td>First In First Out</td>\n<td>Queue processing order where earliest jobs are processed first</td>\n<td>Default job queue behavior for fair processing order</td>\n</tr>\n<tr>\n<td><strong>LIFO</strong></td>\n<td>Last In First Out</td>\n<td>Stack processing order where newest items are processed first</td>\n<td>Not suitable for job queues as it causes starvation</td>\n</tr>\n<tr>\n<td><strong>LRU</strong></td>\n<td>Least Recently Used</td>\n<td>Cache eviction policy removing oldest unused items</td>\n<td>Memory management for image processing cache</td>\n</tr>\n<tr>\n<td><strong>TTL</strong></td>\n<td>Time To Live</td>\n<td>Expiration mechanism for cached data or temporary resources</td>\n<td>Cleanup policy for temporary files and cached thumbnails</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Critical Integration Note</strong>: These terms form an interconnected vocabulary where understanding one concept often requires familiarity with several others. For example, <strong>adaptive bitrate streaming</strong> relies on <strong>keyframe alignment</strong>, <strong>segment alignment</strong>, and <strong>quality ladders</strong> to function properly, while <strong>webhook notifications</strong> depend on <strong>HMAC signatures</strong>, <strong>exponential backoff</strong>, and <strong>idempotency keys</strong> for reliable delivery.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The terminology and concepts defined in this glossary should be consistently used throughout code, documentation, and team communication. Establishing a shared vocabulary prevents miscommunication and ensures that all team members understand system behavior in the same way.</p>\n<h4 id=\"code-documentation-standards\">Code Documentation Standards</h4>\n<p>When implementing the media processing pipeline, use these exact terms in code comments, variable names, and function documentation. For example, use <code>processing_job</code> rather than variations like <code>media_task</code>, <code>work_item</code>, or <code>job_request</code>. This consistency makes the codebase self-documenting and reduces the learning curve for new team members.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ProcessingJob</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Core unit of work containing input file and output specifications.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Represents a complete media transformation task that progresses through</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    the job lifecycle from pending to completed/failed states.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> submit_job</span><span style=\"color:#E1E4E8\">(input_file: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, output_specs: List[OutputSpecification]) -> ProcessingJob:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Create and queue new processing job for worker execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        input_file: Path to source media file for processing</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        output_specs: List of desired output configurations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        ProcessingJob with unique job_id and PENDING status</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span></code></pre></div>\n\n<h4 id=\"api-documentation-consistency\">API Documentation Consistency</h4>\n<p>REST API endpoints and webhook payloads should use the terminology from this glossary to ensure consistency between the system internals and external interfaces. This makes integration easier for client developers who can reference the same glossary.</p>\n<table>\n<thead>\n<tr>\n<th>Endpoint</th>\n<th>Request Fields</th>\n<th>Response Fields</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>POST /jobs</code></td>\n<td><code>input_file_path</code>, <code>output_specifications</code>, <code>webhook_url</code></td>\n<td><code>job_id</code>, <code>status</code>, <code>estimated_duration</code></td>\n</tr>\n<tr>\n<td><code>GET /jobs/{job_id}/progress</code></td>\n<td>None</td>\n<td><code>progress_percentage</code>, <code>current_stage</code>, <code>stage_progress</code></td>\n</tr>\n<tr>\n<td><code>GET /jobs/{job_id}/metadata</code></td>\n<td>None</td>\n<td><code>media_metadata</code>, <code>processing_config</code>, <code>resource_constraints</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"error-message-standardization\">Error Message Standardization</h4>\n<p>Error messages and logging should use these standardized terms to make debugging more efficient. Instead of generic messages like &quot;processing failed,&quot; use specific terminology like &quot;video transcoding failed during GOP structure analysis&quot; or &quot;image processing failed during EXIF orientation handling.&quot;</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Good: Specific terminology</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger.error(</span><span style=\"color:#9ECBFF\">\"CRF encoding failed: invalid GOP structure for adaptive bitrate streaming\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Bad: Generic terminology  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logger.error(</span><span style=\"color:#9ECBFF\">\"video processing error occurred\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"team-communication-guidelines\">Team Communication Guidelines</h4>\n<p>During code reviews, architectural discussions, and incident response, team members should reference these terms consistently. This shared vocabulary accelerates problem diagnosis and solution design by eliminating ambiguity about what each component does and how it behaves.</p>\n<p>When discussing system behavior, use the precise term rather than approximations. For example, say &quot;the job queue exhibits head-of-line blocking&quot; rather than &quot;jobs are getting stuck,&quot; or &quot;we need exponential backoff for webhook notification retries&quot; rather than &quot;we should try sending webhooks again.&quot;</p>\n<p>This glossary serves as the definitive reference for terminology throughout the media processing pipeline project, ensuring that everyone from junior developers to senior architects speaks the same technical language.</p>\n","toc":[{"level":1,"text":"Media Processing Pipeline: Design Document","id":"media-processing-pipeline-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"Mental Model: Digital Photo Lab","id":"mental-model-digital-photo-lab"},{"level":3,"text":"Existing Solutions Comparison","id":"existing-solutions-comparison"},{"level":3,"text":"Core Technical Challenges","id":"core-technical-challenges"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Mental Model: Restaurant Menu Planning","id":"mental-model-restaurant-menu-planning"},{"level":3,"text":"Functional Requirements","id":"functional-requirements"},{"level":3,"text":"Non-Functional Requirements","id":"non-functional-requirements"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Mental Model: Modern Manufacturing Assembly Line","id":"mental-model-modern-manufacturing-assembly-line"},{"level":3,"text":"Component Overview","id":"component-overview"},{"level":4,"text":"API Gateway Layer","id":"api-gateway-layer"},{"level":4,"text":"Job Queue Layer","id":"job-queue-layer"},{"level":4,"text":"Worker Process Layer","id":"worker-process-layer"},{"level":4,"text":"Storage Layer","id":"storage-layer"},{"level":3,"text":"Request Processing Flow","id":"request-processing-flow"},{"level":4,"text":"Job Submission Phase","id":"job-submission-phase"},{"level":4,"text":"Job Execution Phase","id":"job-execution-phase"},{"level":4,"text":"Progress Reporting and Notification Flow","id":"progress-reporting-and-notification-flow"},{"level":3,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Top-Level Directory Organization","id":"top-level-directory-organization"},{"level":4,"text":"API Gateway Module Structure","id":"api-gateway-module-structure"},{"level":4,"text":"Job Queue Module Structure","id":"job-queue-module-structure"},{"level":4,"text":"Worker Process Module Structure","id":"worker-process-module-structure"},{"level":4,"text":"Shared Models and Utilities Structure","id":"shared-models-and-utilities-structure"},{"level":4,"text":"Configuration and Deployment Structure","id":"configuration-and-deployment-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Data Model and Types","id":"data-model-and-types"},{"level":3,"text":"Mental Model: Blueprint Library","id":"mental-model-blueprint-library"},{"level":3,"text":"Job and Task Entities","id":"job-and-task-entities"},{"level":3,"text":"Media Metadata Structures","id":"media-metadata-structures"},{"level":3,"text":"Processing Configuration Types","id":"processing-configuration-types"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Image Processing Component","id":"image-processing-component"},{"level":3,"text":"Mental Model: Digital Darkroom","id":"mental-model-digital-darkroom"},{"level":3,"text":"Core Image Operations","id":"core-image-operations"},{"level":3,"text":"EXIF and Metadata Management","id":"exif-and-metadata-management"},{"level":3,"text":"Image Processing Architecture Decisions","id":"image-processing-architecture-decisions"},{"level":3,"text":"Common Image Processing Pitfalls","id":"common-image-processing-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations Table","id":"technology-recommendations-table"},{"level":4,"text":"Recommended File/Module Structure","id":"recommended-filemodule-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Video Transcoding Component","id":"video-transcoding-component"},{"level":3,"text":"Mental Model: Film Studio Pipeline","id":"mental-model-film-studio-pipeline"},{"level":3,"text":"FFmpeg Integration Layer","id":"ffmpeg-integration-layer"},{"level":3,"text":"Adaptive Bitrate Streaming","id":"adaptive-bitrate-streaming"},{"level":3,"text":"Video Transcoding Architecture Decisions","id":"video-transcoding-architecture-decisions"},{"level":3,"text":"Common Video Processing Pitfalls","id":"common-video-processing-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Job Queue and Scheduling Component","id":"job-queue-and-scheduling-component"},{"level":3,"text":"Mental Model: Restaurant Kitchen","id":"mental-model-restaurant-kitchen"},{"level":3,"text":"Queue Operations and Priority","id":"queue-operations-and-priority"},{"level":3,"text":"Worker Process Management","id":"worker-process-management"},{"level":3,"text":"Job Queue Architecture Decisions","id":"job-queue-architecture-decisions"},{"level":3,"text":"Common Queue Implementation Pitfalls","id":"common-queue-implementation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Progress Tracking and Notification Component","id":"progress-tracking-and-notification-component"},{"level":3,"text":"Mental Model: Package Delivery Tracking","id":"mental-model-package-delivery-tracking"},{"level":3,"text":"Progress Calculation Strategies","id":"progress-calculation-strategies"},{"level":4,"text":"Stage-Based Progress Architecture","id":"stage-based-progress-architecture"},{"level":4,"text":"Time Estimation Algorithms","id":"time-estimation-algorithms"},{"level":4,"text":"Progress Update Mechanisms","id":"progress-update-mechanisms"},{"level":3,"text":"Webhook Notification System","id":"webhook-notification-system"},{"level":4,"text":"Webhook Event Types and Payloads","id":"webhook-event-types-and-payloads"},{"level":4,"text":"Reliable Delivery Implementation","id":"reliable-delivery-implementation"},{"level":4,"text":"Security and Authentication","id":"security-and-authentication"},{"level":3,"text":"Progress Tracking Architecture Decisions","id":"progress-tracking-architecture-decisions"},{"level":4,"text":"Storage Architecture for Progress Data","id":"storage-architecture-for-progress-data"},{"level":4,"text":"Real-Time Update Mechanisms","id":"real-time-update-mechanisms"},{"level":4,"text":"Concurrency Control and Race Conditions","id":"concurrency-control-and-race-conditions"},{"level":3,"text":"Common Progress Tracking Pitfalls","id":"common-progress-tracking-pitfalls"},{"level":4,"text":"⚠️ Pitfall: Progress Reversals and Non-Monotonic Updates","id":"-pitfall-progress-reversals-and-non-monotonic-updates"},{"level":4,"text":"⚠️ Pitfall: Webhook Delivery Storms and Rate Limiting","id":"-pitfall-webhook-delivery-storms-and-rate-limiting"},{"level":4,"text":"⚠️ Pitfall: Memory Leaks in Progress Storage","id":"-pitfall-memory-leaks-in-progress-storage"},{"level":4,"text":"⚠️ Pitfall: Inaccurate Time Estimates and User Expectations","id":"-pitfall-inaccurate-time-estimates-and-user-expectations"},{"level":4,"text":"⚠️ Pitfall: Race Conditions in Progress Percentage Calculations","id":"-pitfall-race-conditions-in-progress-percentage-calculations"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Progress Storage Infrastructure (Complete Implementation)","id":"progress-storage-infrastructure-complete-implementation"},{"level":4,"text":"Webhook Security and Signature Verification (Complete Implementation)","id":"webhook-security-and-signature-verification-complete-implementation"},{"level":4,"text":"Core Progress Tracking Logic (Implementation Skeleton)","id":"core-progress-tracking-logic-implementation-skeleton"},{"level":4,"text":"Stage-Based Progress Calculation (Implementation Skeleton)","id":"stage-based-progress-calculation-implementation-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Component Interactions and Data Flow","id":"component-interactions-and-data-flow"},{"level":3,"text":"Mental Model: Orchestra Performance","id":"mental-model-orchestra-performance"},{"level":3,"text":"API Interfaces and Contracts","id":"api-interfaces-and-contracts"},{"level":4,"text":"Job Submission Endpoint","id":"job-submission-endpoint"},{"level":4,"text":"Response Data Structures","id":"response-data-structures"},{"level":4,"text":"Content Negotiation and Format Support","id":"content-negotiation-and-format-support"},{"level":4,"text":"Error Response Format","id":"error-response-format"},{"level":3,"text":"Internal Message Formats","id":"internal-message-formats"},{"level":4,"text":"Job Queue Message Structure","id":"job-queue-message-structure"},{"level":4,"text":"Progress Update Messages","id":"progress-update-messages"},{"level":4,"text":"Error and Notification Messages","id":"error-and-notification-messages"},{"level":3,"text":"Processing Sequence Scenarios","id":"processing-sequence-scenarios"},{"level":4,"text":"Image Processing Workflow Sequence","id":"image-processing-workflow-sequence"},{"level":4,"text":"Video Transcoding Workflow Sequence","id":"video-transcoding-workflow-sequence"},{"level":4,"text":"Batch Processing Coordination Scenario","id":"batch-processing-coordination-scenario"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"API Server Infrastructure Code","id":"api-server-infrastructure-code"},{"level":4,"text":"Queue Management Infrastructure Code","id":"queue-management-infrastructure-code"},{"level":4,"text":"Core Processing Logic Skeleton","id":"core-processing-logic-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Mental Model: Hospital Emergency Response System","id":"mental-model-hospital-emergency-response-system"},{"level":2,"text":"Failure Modes and Detection","id":"failure-modes-and-detection"},{"level":3,"text":"Mental Model: Weather Monitoring System","id":"mental-model-weather-monitoring-system"},{"level":3,"text":"System-Level Failure Categories","id":"system-level-failure-categories"},{"level":3,"text":"Failure Detection Mechanisms","id":"failure-detection-mechanisms"},{"level":3,"text":"Error Classification and Triage","id":"error-classification-and-triage"},{"level":2,"text":"Retry and Backoff Strategies","id":"retry-and-backoff-strategies"},{"level":3,"text":"Mental Model: Air Traffic Control During Storms","id":"mental-model-air-traffic-control-during-storms"},{"level":3,"text":"Exponential Backoff Implementation","id":"exponential-backoff-implementation"},{"level":3,"text":"Circuit Breaker Pattern for External Dependencies","id":"circuit-breaker-pattern-for-external-dependencies"},{"level":3,"text":"Retry Queue Management","id":"retry-queue-management"},{"level":3,"text":"Dead Letter Queue Management","id":"dead-letter-queue-management"},{"level":2,"text":"Resource Cleanup and Recovery","id":"resource-cleanup-and-recovery"},{"level":3,"text":"Mental Model: Restaurant Kitchen Cleanup","id":"mental-model-restaurant-kitchen-cleanup"},{"level":3,"text":"Temporary File Management","id":"temporary-file-management"},{"level":3,"text":"Memory Management and Resource Limits","id":"memory-management-and-resource-limits"},{"level":3,"text":"Worker Process Recovery","id":"worker-process-recovery"},{"level":3,"text":"Database Transaction Cleanup","id":"database-transaction-cleanup"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Testing Strategy and Milestones","id":"testing-strategy-and-milestones"},{"level":3,"text":"Mental Model: Quality Assurance Laboratory","id":"mental-model-quality-assurance-laboratory"},{"level":3,"text":"Unit Testing Approach","id":"unit-testing-approach"},{"level":3,"text":"Integration Testing Strategy","id":"integration-testing-strategy"},{"level":3,"text":"Milestone Checkpoints and Validation","id":"milestone-checkpoints-and-validation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: Hospital Emergency Room","id":"mental-model-hospital-emergency-room"},{"level":3,"text":"Common Symptoms and Diagnosis","id":"common-symptoms-and-diagnosis"},{"level":3,"text":"Debugging Tools and Techniques","id":"debugging-tools-and-techniques"},{"level":3,"text":"Mental Model: Detective Investigation Kit","id":"mental-model-detective-investigation-kit"},{"level":3,"text":"Performance and Resource Debugging","id":"performance-and-resource-debugging"},{"level":3,"text":"Mental Model: Formula One Pit Crew","id":"mental-model-formula-one-pit-crew"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Future Extensions and Scalability","id":"future-extensions-and-scalability"},{"level":3,"text":"Mental Model: Growing Entertainment Studio","id":"mental-model-growing-entertainment-studio"},{"level":2,"text":"Horizontal Scaling Patterns","id":"horizontal-scaling-patterns"},{"level":3,"text":"Multi-Node Deployment Architecture","id":"multi-node-deployment-architecture"},{"level":3,"text":"Load Balancing Strategies","id":"load-balancing-strategies"},{"level":3,"text":"Distributed Storage Integration","id":"distributed-storage-integration"},{"level":3,"text":"Auto-scaling and Resource Management","id":"auto-scaling-and-resource-management"},{"level":2,"text":"Advanced Processing Features","id":"advanced-processing-features"},{"level":3,"text":"AI-Powered Content Analysis","id":"ai-powered-content-analysis"},{"level":3,"text":"Automated Video Enhancement","id":"automated-video-enhancement"},{"level":3,"text":"Batch Processing Optimization","id":"batch-processing-optimization"},{"level":2,"text":"External Integration Opportunities","id":"external-integration-opportunities"},{"level":3,"text":"Content Delivery Network Integration","id":"content-delivery-network-integration"},{"level":3,"text":"Cloud Storage Backend Integration","id":"cloud-storage-backend-integration"},{"level":3,"text":"Third-Party Processing Service Integration","id":"third-party-processing-service-integration"},{"level":3,"text":"Monitoring and Analytics Integration","id":"monitoring-and-analytics-integration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Glossary and Reference","id":"glossary-and-reference"},{"level":3,"text":"Mental Model: Technical Dictionary for Digital Media","id":"mental-model-technical-dictionary-for-digital-media"},{"level":3,"text":"Media Processing Terms","id":"media-processing-terms"},{"level":3,"text":"Architecture and Queue Terms","id":"architecture-and-queue-terms"},{"level":3,"text":"Acronym Quick Reference","id":"acronym-quick-reference"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Code Documentation Standards","id":"code-documentation-standards"},{"level":4,"text":"API Documentation Consistency","id":"api-documentation-consistency"},{"level":4,"text":"Error Message Standardization","id":"error-message-standardization"},{"level":4,"text":"Team Communication Guidelines","id":"team-communication-guidelines"}],"title":"Media Processing Pipeline: Design Document","markdown":"# Media Processing Pipeline: Design Document\n\n\n## Overview\n\nThis system builds a scalable media processing service that handles image resizing, video transcoding, and thumbnail generation through an asynchronous job queue. The key architectural challenge is efficiently processing large media files while providing real-time progress tracking and reliable error recovery across distributed worker processes.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones (1-3) as this foundational understanding applies throughout the system\n\nThe explosive growth of digital media content creation and consumption has created an unprecedented demand for scalable media processing services. Modern applications routinely handle thousands of user-uploaded images and videos daily, each requiring transformation into multiple formats, resolutions, and quality levels to support diverse devices and network conditions. This section establishes the real-world drivers behind media processing systems and examines the core technical challenges that make building such systems complex.\n\n### Mental Model: Digital Photo Lab\n\n> **Mental Model**: Think of this system as a modern digital photo lab that never closes and can process thousands of orders simultaneously.\n\nIn the era of film photography, a photo lab operated as a specialized facility where customers dropped off rolls of film and returned later to collect developed prints. The lab employed skilled technicians who understood the chemistry of different film types, the characteristics of various paper stocks, and the precise timing required for each processing step. Customers could request different print sizes, finishes, and quantities, with each order requiring specific handling procedures.\n\nOur media processing pipeline operates on remarkably similar principles, but at digital scale. Users submit \"orders\" by uploading raw media files through an API gateway, much like dropping off film canisters at a lab counter. Each upload triggers the creation of a **processing job** that contains detailed instructions about desired output formats, resolutions, and quality settings—analogous to the order forms customers filled out for their prints.\n\nThe job queue functions as the lab's work order system, organizing incoming requests by priority and distributing them to available technicians. In our digital lab, these technicians are **worker processes** that specialize in different types of media transformation. Just as a photo lab might have separate stations for developing, printing, and finishing, our workers are equipped with specialized tools: Pillow for image manipulation, FFmpeg for video transcoding, and custom progress tracking systems.\n\nThe most crucial similarity lies in the concept of **batch processing with progress tracking**. Traditional photo labs provided customers with claim tickets and estimated completion times. Our system maintains detailed progress records for each job, sending webhook notifications that function like the phone calls labs made when orders were ready for pickup. Both systems must handle the reality that some processes take much longer than others—a simple wallet-sized print versus a large format enlargement in the traditional lab, or thumbnail generation versus 4K video transcoding in our digital system.\n\nThis analogy helps clarify why certain architectural decisions are essential: job queues prevent the system from being overwhelmed during peak times (like the Christmas photo rush), worker specialization ensures optimal resource utilization, and progress tracking provides transparency that builds user trust in a process they cannot directly observe.\n\n### Existing Solutions Comparison\n\nThe media processing landscape offers three primary architectural approaches, each with distinct trade-offs that influence system design decisions. Understanding these alternatives provides context for the architectural choices made in this implementation.\n\n**Cloud Service Integration Approach**\n\nCloud-based solutions like AWS MediaConvert, Google Cloud Video Intelligence, and Azure Media Services represent the \"software-as-a-service\" model for media processing. These services handle infrastructure scaling automatically and provide pre-optimized processing pipelines with minimal configuration requirements.\n\n| Aspect | Cloud Services | Assessment |\n|--------|---------------|------------|\n| **Setup Complexity** | Minimal - API integration only | Fastest time to market |\n| **Scaling** | Automatic horizontal scaling | Handles traffic spikes seamlessly |\n| **Cost Model** | Pay-per-operation pricing | Can become expensive at scale |\n| **Customization** | Limited to provided parameters | Insufficient for specialized workflows |\n| **Latency** | Network round-trip overhead | Adds 200-500ms per operation |\n| **Data Residency** | Files transmitted to cloud | Potential privacy/compliance issues |\n| **Vendor Lock-in** | Tight coupling to provider APIs | Migration complexity increases over time |\n\nCloud services excel for applications with straightforward processing requirements and variable usage patterns. However, they become prohibitively expensive for high-volume applications and offer insufficient control for specialized processing workflows that require custom algorithms or unusual format support.\n\n**On-Premise Processing Solutions**\n\nSelf-hosted media processing represents the traditional approach where organizations deploy and manage their own processing infrastructure. This includes both custom-built solutions using libraries like FFmpeg and Pillow, as well as commercial software packages designed for media workflow automation.\n\n| Aspect | On-Premise | Assessment |\n|--------|------------|------------|\n| **Infrastructure Control** | Complete hardware/software control | Optimal for specialized requirements |\n| **Processing Latency** | Local processing - minimal network overhead | Best performance for real-time needs |\n| **Customization** | Unlimited - full source code access | Supports any processing algorithm |\n| **Scaling Complexity** | Manual capacity planning required | Requires infrastructure expertise |\n| **Operational Overhead** | Full system administration burden | Significant ongoing maintenance costs |\n| **Initial Investment** | High upfront hardware/software costs | Substantial capital expenditure |\n| **Reliability** | Single points of failure without redundancy | Requires sophisticated disaster recovery |\n\nOn-premise solutions provide maximum control and can achieve the lowest per-operation costs at high volumes. They are essential for organizations with strict data residency requirements or highly specialized processing needs. However, they demand significant infrastructure expertise and cannot easily handle unpredictable traffic patterns.\n\n**Hybrid Processing Architecture**\n\nThe hybrid approach combines self-hosted processing capabilities with cloud service integration, allowing systems to optimize for both control and scalability. This architecture typically handles routine processing operations locally while leveraging cloud services for overflow capacity or specialized operations.\n\n| Aspect | Hybrid Approach | Assessment |\n|--------|----------------|------------|\n| **Cost Optimization** | Local processing for base load, cloud for peaks | Balances fixed and variable costs |\n| **Flexibility** | Custom algorithms locally, cloud for standard operations | Best of both worlds approach |\n| **Complexity** | Requires orchestration between multiple systems | Significant architectural complexity |\n| **Reliability** | Cloud failover for local system outages | Enhanced disaster recovery options |\n| **Data Management** | Intelligent routing based on content sensitivity | Supports compliance requirements |\n| **Scaling Strategy** | Predictable local capacity with elastic overflow | Handles both steady state and spikes |\n\n> **Decision: Self-Hosted Processing with Cloud-Ready Architecture**\n> - **Context**: This implementation targets organizations that need control over processing algorithms, data residency, and per-operation costs, while maintaining the flexibility to integrate cloud services in the future\n> - **Options Considered**: Pure cloud integration, pure on-premise deployment, hybrid architecture from the start\n> - **Decision**: Build a self-hosted system with modular components that can integrate with cloud services later\n> - **Rationale**: Self-hosted provides learning value for understanding media processing fundamentals, offers maximum customization for specialized requirements, and delivers the lowest long-term operational costs for high-volume scenarios. The modular architecture ensures cloud integration remains possible without major refactoring.\n> - **Consequences**: Higher initial development complexity and operational overhead, but provides deep understanding of media processing challenges and maximum future flexibility\n\n### Core Technical Challenges\n\nBuilding a scalable media processing pipeline presents several interconnected technical challenges that differentiate it from typical web application development. Each challenge requires specific architectural decisions and implementation strategies that influence the entire system design.\n\n**Memory Management and Resource Utilization**\n\nMedia processing operations are inherently memory-intensive, with resource requirements that scale dramatically based on content characteristics. A single 4K video file can require several gigabytes of RAM during transcoding operations, while high-resolution image processing may need temporary buffers that exceed available system memory.\n\nThe fundamental challenge lies in the **unpredictable nature of resource requirements**. Unlike traditional web services where request processing consumes predictable amounts of memory for predictable durations, media processing operations vary enormously based on input characteristics. A 30-second 1080p video might require 500MB of working memory, while a 2-hour 4K video could need 8GB or more.\n\n| Resource Type | Challenge | Impact | Mitigation Strategy |\n|---------------|-----------|--------|-------------------|\n| **Memory Usage** | Unpredictable working set sizes | Worker process crashes, system instability | Pre-flight content analysis, memory-based job routing |\n| **Disk I/O** | Large temporary file requirements | Storage exhaustion, I/O bottlenecks | Streaming processing, temporary file cleanup |\n| **CPU Utilization** | CPU-intensive encoding operations | Worker starvation, system responsiveness | Process isolation, CPU quota enforcement |\n| **Network Bandwidth** | Large file transfers during processing | Network congestion, timeout issues | Local processing, chunked transfers |\n\nThe memory management challenge extends beyond simple allocation tracking to include **temporal memory usage patterns**. Video transcoding typically follows a sawtooth pattern where memory usage builds during frame buffering, spikes during encoding operations, then drops during I/O operations. Image processing may require sudden allocation of large contiguous buffers for pixel manipulation operations.\n\nEffective memory management requires implementing **resource-aware job scheduling** that considers both current system utilization and estimated resource requirements for queued jobs. This involves analyzing input media characteristics before processing begins and routing jobs to workers with appropriate resource availability.\n\n**Format Diversity and Compatibility**\n\nThe media processing ecosystem encompasses dozens of container formats, codecs, and encoding parameters, each with unique characteristics and compatibility requirements. Modern applications must support legacy formats for backward compatibility while adopting emerging formats like WebP, AVIF, and AV1 for optimal efficiency.\n\nThe complexity extends beyond simple format conversion to include **parameter optimization for different use cases**. A single input image might need transformation into JPEG for broad compatibility, WebP for size optimization, and AVIF for cutting-edge browsers, each with different quality settings optimized for the intended display context.\n\n| Format Category | Complexity Factor | Technical Challenge | Implementation Requirement |\n|----------------|------------------|-------------------|--------------------------|\n| **Image Formats** | Color space variations (RGB, CMYK, YUV) | Color accuracy preservation | Comprehensive color management |\n| **Video Containers** | Codec/container compatibility matrices | Format selection optimization | Intelligent codec mapping |\n| **Encoding Parameters** | Quality vs. size trade-offs | Context-aware optimization | Adaptive parameter selection |\n| **Metadata Handling** | Format-specific metadata standards | Information preservation/privacy | Configurable metadata policies |\n\nThe challenge intensifies when considering **cross-format metadata preservation**. EXIF data in JPEG images contains orientation, camera settings, and potentially sensitive location information. Video files include complex metadata about encoding parameters, timestamps, and technical characteristics that may need preservation or stripping based on privacy policies.\n\nFormat diversity also introduces **version compatibility issues**. The WebP format supports both lossy and lossless compression with different browser support timelines. H.265 video encoding provides superior compression but requires hardware acceleration for practical performance and has complex licensing requirements.\n\n**Processing Time Estimation and Progress Tracking**\n\nUnlike typical web service operations that complete in milliseconds, media processing operations can require minutes or hours depending on content characteristics and output requirements. Users expect accurate progress estimates and reliable completion notifications, but media processing progress is notoriously difficult to predict accurately.\n\nThe fundamental challenge stems from the **non-linear nature of media processing operations**. Video transcoding progress varies dramatically based on scene complexity—static scenes encode quickly while high-motion sequences require significantly more processing time per frame. Image processing operations may complete in stages with vastly different durations for each phase.\n\nTraditional percentage-based progress indicators prove inadequate for media processing workflows. A more effective approach involves **stage-based progress tracking** that provides users with meaningful information about current processing phases rather than potentially inaccurate time estimates.\n\n| Processing Stage | Progress Characteristics | Estimation Challenge | Tracking Strategy |\n|-----------------|------------------------|-------------------|------------------|\n| **Content Analysis** | Fast, predictable duration | Minimal - file I/O bound | Simple percentage based on bytes read |\n| **Format Conversion** | Highly variable based on complexity | Significant - depends on content characteristics | Stage completion milestones |\n| **Quality Optimization** | Iterative with unknown convergence | Extreme - optimization algorithms vary | Iteration count with quality targets |\n| **Output Generation** | Predictable but format-dependent | Moderate - based on output size estimates | Bytes written vs. estimated output size |\n\nProgress tracking complexity increases with **multi-output processing scenarios**. A single input video might generate multiple resolution variants, each requiring separate transcoding operations with different complexity characteristics. Users need visibility into overall job progress rather than individual operation progress.\n\nThe challenge extends to **failure recovery and retry scenarios**. When processing operations fail partway through completion, the system must determine whether to restart from the beginning or resume from intermediate checkpoints. This decision impacts both progress accuracy and resource utilization efficiency.\n\n**Worker Coordination and Distributed Processing**\n\nMedia processing systems require sophisticated coordination between multiple worker processes to achieve optimal throughput while avoiding resource conflicts and ensuring reliable job completion. Unlike stateless web service workers, media processing workers maintain significant state during long-running operations and require careful coordination to prevent resource exhaustion.\n\nThe primary coordination challenge involves **load balancing with resource awareness**. Traditional round-robin or random load balancing proves inadequate when workers have different resource profiles and jobs have varying resource requirements. A worker processing a large video file may be unavailable for new jobs for hours, while a worker handling image resizing might complete dozens of operations in the same timeframe.\n\n| Coordination Aspect | Challenge | Impact | Solution Approach |\n|-------------------|-----------|--------|------------------|\n| **Resource-Aware Scheduling** | Matching job requirements with worker capabilities | Suboptimal resource utilization, job failures | Dynamic worker capability advertising |\n| **Failure Detection** | Long-running operations mask worker failures | Jobs stuck in processing state indefinitely | Heartbeat monitoring with timeout escalation |\n| **State Management** | Workers maintain processing state across operations | Inconsistent state after failures | Checkpoint-based recovery mechanisms |\n| **Priority Handling** | High-priority jobs blocked by long-running operations | Poor user experience for urgent requests | Preemptive scheduling with job priority queues |\n\nWorker coordination must also handle **graceful degradation scenarios** where individual workers become unavailable due to resource exhaustion, process failures, or system maintenance. The coordination system needs mechanisms to redistribute work from failed workers while preserving processing progress and avoiding duplicate work.\n\nThe challenge extends to **dynamic scaling scenarios** where worker capacity needs to increase or decrease based on current load patterns. Adding new workers requires capability discovery and job redistribution, while removing workers requires graceful job migration and state preservation.\n\n> **Architecture Insight**: The combination of these four core challenges—memory management, format diversity, progress estimation, and worker coordination—creates a system complexity profile that differs significantly from typical web applications. Success requires architectural patterns that embrace asynchronous processing, resource-aware scheduling, and comprehensive error recovery mechanisms.\n\nThese technical challenges inform every major architectural decision in the media processing pipeline, from the choice of message queue technology to the design of progress tracking APIs. Understanding their interconnected nature helps explain why media processing systems require specialized patterns and cannot simply apply traditional web application architectures.\n\nThe following sections will detail how each system component addresses these challenges through specific design decisions, implementation patterns, and architectural trade-offs that provide reliable, scalable media processing capabilities.\n\n### Implementation Guidance\n\nThis implementation guidance provides practical technology choices and starter code to address the core challenges identified in the problem analysis. The recommendations balance learning value with production readiness, focusing on Python-based solutions that provide clear visibility into media processing concepts.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|----------------|-----------|\n| **Message Queue** | Redis with `rq` library | RabbitMQ with Celery | Redis offers simpler setup for learning; RabbitMQ provides better production reliability |\n| **Image Processing** | Pillow (PIL fork) | OpenCV with Pillow fallback | Pillow handles most use cases; OpenCV adds computer vision capabilities |\n| **Video Processing** | subprocess calls to FFmpeg | python-ffmpeg wrapper | Direct subprocess gives full control; wrapper adds convenience |\n| **Progress Storage** | Redis key-value store | PostgreSQL with real-time updates | Redis provides fast updates; PostgreSQL offers ACID guarantees |\n| **File Storage** | Local filesystem | AWS S3 with local caching | Local files simplify development; S3 enables production scaling |\n| **Worker Management** | Simple process pool | Kubernetes job scheduling | Process pool suitable for single-machine; K8s enables distributed workers |\n\n**Recommended Project Structure**\n\nThe project structure separates core media processing logic from infrastructure concerns, enabling focused development on each component while maintaining clear boundaries for testing and maintenance.\n\n```\nmedia_pipeline/\n├── src/\n│   ├── media_pipeline/\n│   │   ├── __init__.py\n│   │   ├── api/                    # REST API and job submission\n│   │   │   ├── __init__.py\n│   │   │   ├── handlers.py         # Request handlers\n│   │   │   └── schemas.py          # Request/response models\n│   │   ├── core/                   # Core business logic\n│   │   │   ├── __init__.py\n│   │   │   ├── job_manager.py      # Job lifecycle management\n│   │   │   ├── progress_tracker.py # Progress monitoring\n│   │   │   └── webhook_sender.py   # Notification delivery\n│   │   ├── processors/             # Media processing components\n│   │   │   ├── __init__.py\n│   │   │   ├── image_processor.py  # Image operations\n│   │   │   ├── video_processor.py  # Video transcoding\n│   │   │   └── base_processor.py   # Common processing interface\n│   │   ├── queue/                  # Job queue implementation\n│   │   │   ├── __init__.py\n│   │   │   ├── redis_queue.py      # Redis-based queue\n│   │   │   ├── worker.py           # Worker process logic\n│   │   │   └── scheduler.py        # Job scheduling\n│   │   ├── storage/                # File and metadata storage\n│   │   │   ├── __init__.py\n│   │   │   ├── file_manager.py     # File I/O operations\n│   │   │   └── metadata_store.py   # Job and progress storage\n│   │   └── utils/                  # Common utilities\n│   │       ├── __init__.py\n│   │       ├── config.py           # Configuration management\n│   │       ├── logging.py          # Logging setup\n│   │       └── errors.py           # Custom exception types\n├── tests/                          # Test files mirror src structure\n│   ├── unit/\n│   ├── integration/\n│   └── fixtures/                   # Sample media files\n├── scripts/                        # Operational scripts\n│   ├── start_workers.py            # Worker process management\n│   └── health_check.py             # System monitoring\n├── config/                         # Configuration files\n│   ├── development.yaml\n│   └── production.yaml\n├── requirements/                   # Dependency specifications\n│   ├── base.txt                    # Core dependencies\n│   ├── dev.txt                     # Development tools\n│   └── prod.txt                    # Production additions\n└── docker/                        # Container definitions\n    ├── Dockerfile.api\n    └── Dockerfile.worker\n```\n\n**Infrastructure Starter Code**\n\nThe following components provide complete, production-ready infrastructure that supports the core learning objectives without requiring deep implementation of supporting systems.\n\n**Configuration Management System** (`src/media_pipeline/utils/config.py`):\n\n```python\n\"\"\"\nConfiguration management for media processing pipeline.\nHandles environment-specific settings and validation.\n\"\"\"\nimport os\nimport yaml\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, Optional\nfrom pathlib import Path\n\n@dataclass\nclass RedisConfig:\n    host: str = \"localhost\"\n    port: int = 6379\n    db: int = 0\n    password: Optional[str] = None\n    \n@dataclass\nclass StorageConfig:\n    base_path: str = \"./storage\"\n    temp_path: str = \"./temp\"\n    max_file_size: int = 100 * 1024 * 1024  # 100MB\n    \n@dataclass\nclass ProcessingConfig:\n    max_workers: int = 4\n    job_timeout: int = 3600  # 1 hour\n    retry_attempts: int = 3\n    webhook_timeout: int = 30\n    \n@dataclass\nclass AppConfig:\n    redis: RedisConfig\n    storage: StorageConfig\n    processing: ProcessingConfig\n    debug: bool = False\n    \n    @classmethod\n    def from_yaml(cls, config_path: str) -> 'AppConfig':\n        \"\"\"Load configuration from YAML file with environment variable substitution.\"\"\"\n        with open(config_path, 'r') as f:\n            config_data = yaml.safe_load(f)\n        \n        # Substitute environment variables\n        config_data = cls._substitute_env_vars(config_data)\n        \n        return cls(\n            redis=RedisConfig(**config_data.get('redis', {})),\n            storage=StorageConfig(**config_data.get('storage', {})),\n            processing=ProcessingConfig(**config_data.get('processing', {})),\n            debug=config_data.get('debug', False)\n        )\n    \n    @staticmethod\n    def _substitute_env_vars(data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Recursively substitute environment variables in configuration.\"\"\"\n        if isinstance(data, dict):\n            return {k: AppConfig._substitute_env_vars(v) for k, v in data.items()}\n        elif isinstance(data, str) and data.startswith('${') and data.endswith('}'):\n            env_var = data[2:-1]\n            return os.getenv(env_var, data)\n        else:\n            return data\n\n# Global configuration instance\nconfig: Optional[AppConfig] = None\n\ndef init_config(config_path: Optional[str] = None) -> AppConfig:\n    \"\"\"Initialize global configuration from file or environment.\"\"\"\n    global config\n    \n    if config_path is None:\n        env = os.getenv('ENVIRONMENT', 'development')\n        config_path = f\"config/{env}.yaml\"\n    \n    config = AppConfig.from_yaml(config_path)\n    return config\n\ndef get_config() -> AppConfig:\n    \"\"\"Get global configuration, initializing if necessary.\"\"\"\n    global config\n    if config is None:\n        config = init_config()\n    return config\n```\n\n**Logging Infrastructure** (`src/media_pipeline/utils/logging.py`):\n\n```python\n\"\"\"\nCentralized logging configuration for media processing pipeline.\nProvides structured logging with correlation IDs for job tracking.\n\"\"\"\nimport logging\nimport logging.config\nimport json\nimport uuid\nfrom contextlib import contextmanager\nfrom typing import Optional, Dict, Any\nfrom datetime import datetime\n\nclass JobContextFilter(logging.Filter):\n    \"\"\"Add job context information to log records.\"\"\"\n    \n    def filter(self, record):\n        # Add job_id from context if available\n        record.job_id = getattr(self, 'job_id', 'no-job')\n        record.correlation_id = getattr(self, 'correlation_id', str(uuid.uuid4()))\n        return True\n\nclass JSONFormatter(logging.Formatter):\n    \"\"\"Format log records as JSON for structured logging.\"\"\"\n    \n    def format(self, record):\n        log_entry = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'level': record.levelname,\n            'message': record.getMessage(),\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno,\n            'job_id': getattr(record, 'job_id', None),\n            'correlation_id': getattr(record, 'correlation_id', None),\n        }\n        \n        if record.exc_info:\n            log_entry['exception'] = self.formatException(record.exc_info)\n        \n        return json.dumps(log_entry)\n\n# Logging configuration\nLOGGING_CONFIG = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'standard': {\n            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s (job=%(job_id)s)'\n        },\n        'json': {\n            '()': JSONFormatter,\n        },\n    },\n    'filters': {\n        'job_context': {\n            '()': JobContextFilter,\n        },\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'level': 'INFO',\n            'formatter': 'standard',\n            'filters': ['job_context'],\n            'stream': 'ext://sys.stdout'\n        },\n        'file': {\n            'class': 'logging.handlers.RotatingFileHandler',\n            'level': 'DEBUG',\n            'formatter': 'json',\n            'filters': ['job_context'],\n            'filename': 'logs/media_pipeline.log',\n            'maxBytes': 10485760,  # 10MB\n            'backupCount': 5\n        },\n    },\n    'root': {\n        'level': 'INFO',\n        'handlers': ['console', 'file']\n    }\n}\n\ndef setup_logging(debug: bool = False):\n    \"\"\"Initialize logging configuration.\"\"\"\n    if debug:\n        LOGGING_CONFIG['root']['level'] = 'DEBUG'\n        LOGGING_CONFIG['handlers']['console']['level'] = 'DEBUG'\n    \n    logging.config.dictConfig(LOGGING_CONFIG)\n\n@contextmanager\ndef job_logging_context(job_id: str, correlation_id: Optional[str] = None):\n    \"\"\"Context manager that adds job information to all log records.\"\"\"\n    if correlation_id is None:\n        correlation_id = str(uuid.uuid4())\n    \n    # Get the job context filter and set context\n    logger = logging.getLogger()\n    job_filter = None\n    \n    for handler in logger.handlers:\n        for filter_obj in handler.filters:\n            if isinstance(filter_obj, JobContextFilter):\n                job_filter = filter_obj\n                break\n    \n    if job_filter:\n        old_job_id = getattr(job_filter, 'job_id', None)\n        old_correlation_id = getattr(job_filter, 'correlation_id', None)\n        \n        job_filter.job_id = job_id\n        job_filter.correlation_id = correlation_id\n        \n        try:\n            yield\n        finally:\n            job_filter.job_id = old_job_id\n            job_filter.correlation_id = old_correlation_id\n    else:\n        yield\n```\n\n**Core Logic Skeleton Code**\n\nThe following skeletons provide the structure for core components that learners should implement themselves, with detailed TODO comments mapping to the architectural concepts discussed above.\n\n**Job Management Core** (`src/media_pipeline/core/job_manager.py`):\n\n```python\n\"\"\"\nCore job management functionality for media processing pipeline.\nHandles job lifecycle, state transitions, and resource coordination.\n\"\"\"\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime\nimport uuid\n\nclass JobStatus(Enum):\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\nclass JobPriority(Enum):\n    LOW = 1\n    NORMAL = 5\n    HIGH = 10\n    URGENT = 20\n\n@dataclass\nclass ProcessingJob:\n    \"\"\"Represents a media processing job with all required metadata.\"\"\"\n    job_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    input_file_path: str = \"\"\n    output_specifications: List[Dict[str, Any]] = field(default_factory=list)\n    priority: JobPriority = JobPriority.NORMAL\n    status: JobStatus = JobStatus.PENDING\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error_message: Optional[str] = None\n    retry_count: int = 0\n    webhook_url: Optional[str] = None\n    progress_percentage: float = 0.0\n    estimated_duration: Optional[int] = None\n\nclass JobManager:\n    \"\"\"\n    Manages the complete lifecycle of media processing jobs.\n    Coordinates between job queue, progress tracking, and worker processes.\n    \"\"\"\n    \n    def __init__(self, queue_backend, progress_store, webhook_sender):\n        self.queue = queue_backend\n        self.progress_store = progress_store\n        self.webhook_sender = webhook_sender\n        self.active_jobs: Dict[str, ProcessingJob] = {}\n    \n    def submit_job(self, input_file: str, output_specs: List[Dict[str, Any]], \n                   priority: JobPriority = JobPriority.NORMAL, \n                   webhook_url: Optional[str] = None) -> ProcessingJob:\n        \"\"\"\n        Submit a new processing job to the queue.\n        \n        Args:\n            input_file: Path to input media file\n            output_specs: List of output format specifications\n            priority: Job processing priority\n            webhook_url: Optional webhook for status notifications\n            \n        Returns:\n            ProcessingJob instance with assigned job_id\n        \"\"\"\n        # TODO 1: Create ProcessingJob instance with unique job_id\n        # TODO 2: Validate input file exists and is readable\n        # TODO 3: Validate output specifications contain required fields\n        # TODO 4: Estimate processing duration based on input file characteristics\n        # TODO 5: Store job metadata in progress store\n        # TODO 6: Submit job to queue with priority handling\n        # TODO 7: Send initial webhook notification if webhook_url provided\n        # TODO 8: Return created job instance\n        pass\n    \n    def update_job_progress(self, job_id: str, progress_percentage: float, \n                           stage: str, details: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Update job progress and notify interested parties.\n        \n        Args:\n            job_id: Unique job identifier\n            progress_percentage: Completion percentage (0.0 - 100.0)\n            stage: Current processing stage description\n            details: Optional additional progress details\n        \"\"\"\n        # TODO 1: Validate job_id exists in active jobs\n        # TODO 2: Update job progress in memory and persistent storage\n        # TODO 3: Calculate estimated time remaining based on progress rate\n        # TODO 4: Check for progress milestone thresholds (25%, 50%, 75%)\n        # TODO 5: Send webhook notifications for milestone progress\n        # TODO 6: Update job status if progress indicates completion\n        # Hint: Progress updates can arrive out of order - handle gracefully\n        pass\n    \n    def mark_job_completed(self, job_id: str, output_files: List[str]):\n        \"\"\"\n        Mark job as completed and perform cleanup operations.\n        \n        Args:\n            job_id: Unique job identifier\n            output_files: List of generated output file paths\n        \"\"\"\n        # TODO 1: Validate job exists and is in PROCESSING state\n        # TODO 2: Update job status to COMPLETED with completion timestamp\n        # TODO 3: Store output file paths in job metadata\n        # TODO 4: Remove job from active jobs tracking\n        # TODO 5: Send completion webhook notification with output file details\n        # TODO 6: Schedule cleanup of temporary files after retention period\n        pass\n    \n    def mark_job_failed(self, job_id: str, error_message: str, \n                       retry_eligible: bool = True):\n        \"\"\"\n        Handle job failure with retry logic and error reporting.\n        \n        Args:\n            job_id: Unique job identifier  \n            error_message: Detailed error description\n            retry_eligible: Whether job can be retried\n        \"\"\"\n        # TODO 1: Retrieve job and increment retry_count\n        # TODO 2: Check if retry_count exceeds maximum retry attempts\n        # TODO 3: If retries available and retry_eligible, requeue job with exponential backoff\n        # TODO 4: If no retries remaining, mark job as permanently FAILED\n        # TODO 5: Send failure webhook notification with error details\n        # TODO 6: Clean up any partial output files\n        # TODO 7: Log failure details for debugging and monitoring\n        # Hint: Exponential backoff delays should be: 1min, 2min, 4min, 8min...\n        pass\n```\n\n**Language-Specific Implementation Hints**\n\n**Python Media Processing Essentials:**\n\n- Use `Pillow.Image.MAX_IMAGE_PIXELS = None` to handle large images, but implement custom size limits to prevent memory exhaustion\n- Install FFmpeg system package: `apt-get install ffmpeg` (Ubuntu) or `brew install ffmpeg` (macOS)  \n- Use `subprocess.run()` with `capture_output=True` and `text=True` for FFmpeg integration\n- Implement timeout handling with `subprocess.run(timeout=seconds)` to prevent hung processes\n- Use `pathlib.Path` for cross-platform file path handling instead of string concatenation\n- Redis connection pooling: `redis.ConnectionPool(max_connections=20)` prevents connection exhaustion\n\n**Memory Management Patterns:**\n\n- Process images in chunks for large files: read → process → write → release memory\n- Use `gc.collect()` after processing large files to force garbage collection\n- Monitor memory usage with `psutil.Process().memory_info().rss` and fail fast when limits exceeded  \n- Implement file streaming: `with open(file, 'rb') as f: for chunk in iter(lambda: f.read(8192), b''):`\n\n**Error Handling Strategies:**\n\n- Catch `PIL.Image.DecompressionBombError` for oversized images\n- Handle `subprocess.CalledProcessError` for FFmpeg failures and parse stderr for specific error types\n- Use `try/except/finally` blocks to ensure temporary file cleanup even when processing fails\n- Implement circuit breaker pattern for external services (webhook delivery, storage APIs)\n\n**Milestone Checkpoint**\n\nAfter implementing the core infrastructure, verify the following behaviors:\n\n**Configuration and Logging Verification:**\n```bash\n# Test configuration loading\npython -c \"from media_pipeline.utils.config import init_config; print(init_config('config/development.yaml'))\"\n\n# Verify logging output includes job context\npython -c \"\nfrom media_pipeline.utils.logging import setup_logging, job_logging_context\nimport logging\nsetup_logging(debug=True)\nwith job_logging_context('test-job-123'):\n    logging.info('Test message with job context')\n\"\n```\n\n**Expected Output Indicators:**\n- Configuration loads without errors and displays all nested settings\n- Log messages include job_id and correlation_id in structured format\n- No import errors when loading core modules\n\n**Common Setup Issues:**\n- **ModuleNotFoundError**: Ensure `PYTHONPATH` includes `src/` directory or install package in development mode\n- **FFmpeg not found**: Install system FFmpeg package, verify with `ffmpeg -version`\n- **Redis connection errors**: Start Redis server with `redis-server` or use Docker container\n- **Permission errors**: Ensure write access to configured storage and log directories\n\nThe infrastructure code provides a solid foundation that handles configuration management, logging, and error handling patterns. This allows learners to focus on implementing the core media processing algorithms and job coordination logic without getting distracted by supporting infrastructure concerns.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones (1-3) as these requirements drive the entire system design and implementation scope\n\n### Mental Model: Restaurant Menu Planning\n\nThink of defining system goals like planning a restaurant menu. A successful restaurant doesn't try to serve every possible dish—instead, it carefully selects a focused set of offerings that it can execute exceptionally well with its available kitchen staff, equipment, and ingredients. The chef explicitly decides what goes on the menu (functional requirements), sets quality standards for each dish (non-functional requirements), and importantly, decides what NOT to serve (non-goals) to maintain focus and quality.\n\nOur media processing pipeline follows this same principle. We must clearly define what we will build, how well it must perform, and what we intentionally exclude to ensure we can deliver a high-quality, maintainable system within our resource constraints.\n\n### Functional Requirements\n\nThe **functional requirements** define the core capabilities our media processing pipeline must deliver. These represent the essential features that users directly interact with and depend upon for their media processing workflows.\n\n> **Decision: Core Media Processing Operations**\n> - **Context**: Users need to process various media types with different output requirements, from simple image resizing to complex video transcoding workflows\n> - **Options Considered**: \n>   1. Image-only processing system with simple resize operations\n>   2. Video-only transcoding service focusing on streaming formats\n>   3. Comprehensive media pipeline supporting both images and videos with advanced features\n> - **Decision**: Comprehensive media pipeline with full image and video processing capabilities\n> - **Rationale**: Modern applications require both image and video processing, and building separate systems would create operational complexity and code duplication. A unified pipeline provides better resource utilization and simpler client integration.\n> - **Consequences**: Increased system complexity requiring careful resource management, but provides complete media processing solution that scales with user needs\n\n| Capability Category | Specific Requirements | Input Formats | Output Formats | Configuration Options |\n|---------------------|----------------------|---------------|----------------|---------------------|\n| **Image Processing** | Resize with aspect ratio preservation | JPEG, PNG, WebP, GIF | JPEG, PNG, WebP, AVIF | Target dimensions, interpolation algorithm, quality settings |\n| | Format conversion with quality control | | | Compression level, progressive encoding, color space |\n| | Thumbnail generation with smart cropping | | | Standard sizes, center-crop vs smart-crop, preview quality |\n| | EXIF metadata handling | | | Preserve, strip, or selectively copy metadata fields |\n| **Video Processing** | Multi-format transcoding | MP4, AVI, MOV, WebM, MKV | MP4, WebM, HLS, DASH | Codec selection, bitrate, resolution, frame rate |\n| | Adaptive bitrate variant generation | | | Multiple quality levels, resolution ladder, segment duration |\n| | Video thumbnail extraction | | JPEG, PNG, WebP | Time offset, frame selection, thumbnail size |\n| | Audio track handling | | | Audio codec, bitrate, channel configuration, synchronization |\n| **Job Management** | Asynchronous processing with priority queuing | All supported formats | All output formats | Priority levels, processing timeout, resource allocation |\n| | Progress tracking and status reporting | | | Real-time updates, stage-based progress, time estimation |\n| | Webhook notifications for status changes | | | Completion, failure, progress milestones, retry events |\n| | Error handling with automatic retry logic | | | Exponential backoff, retry limits, dead letter handling |\n\nThe system must support **batch processing operations** where multiple output variants are generated from a single input file. For example, a single uploaded image might produce thumbnails at three different sizes, convert to WebP format for web display, and generate a high-quality JPEG for download—all as part of one `ProcessingJob` with multiple output specifications.\n\n**Resource-aware scheduling** ensures that memory-intensive video transcoding jobs don't overwhelm the system while lightweight image resizing operations continue processing. The job queue must intelligently distribute work based on worker process capabilities and current system resource utilization.\n\n> **Critical Design Insight**: The functional requirements prioritize **flexibility over simplicity**. Rather than building separate specialized tools, we create a unified pipeline that handles diverse media processing scenarios through configurable job specifications. This approach serves both simple use cases (single image resize) and complex workflows (multi-variant video transcoding with thumbnails).\n\n### Non-Functional Requirements\n\nThe **non-functional requirements** establish the quality standards and operational characteristics that make our media processing pipeline suitable for production environments. These requirements directly impact architecture decisions and implementation strategies.\n\n> **Decision: Performance and Scalability Targets**\n> - **Context**: Media processing is computationally intensive with highly variable workloads, from small profile images to large video files requiring hours of processing time\n> - **Options Considered**:\n>   1. Single-node processing with simple scaling through instance size increases\n>   2. Horizontally scalable worker pool with shared job queue\n>   3. Serverless processing using cloud functions with automatic scaling\n> - **Decision**: Horizontally scalable worker pool architecture with intelligent job distribution\n> - **Rationale**: Provides predictable costs, allows optimization for different media types, and enables resource-aware scheduling that serverless cannot provide. Single-node scaling hits memory and I/O limits with large video files.\n> - **Consequences**: Requires distributed system complexity but provides linear scaling and cost control\n\n| Requirement Category | Specification | Measurement Method | Business Justification |\n|---------------------|--------------|-------------------|----------------------|\n| **Processing Performance** | Image resize operations complete within 5 seconds for files up to 50MB | End-to-end job completion time | User experience for real-time workflows |\n| | Video transcoding processes at 2x real-time speed for 1080p content | FFmpeg processing rate measurement | Acceptable wait times for uploaded content |\n| | System processes 1000 concurrent image jobs | Active job queue depth monitoring | Peak load handling for content publishing |\n| | 95th percentile job completion time under 30 seconds for standard operations | Job duration histogram analysis | Predictable processing times for SLA commitments |\n| **Reliability and Availability** | 99.5% job success rate excluding invalid input files | Success/failure ratio tracking | Minimize user frustration from processing failures |\n| | Failed jobs automatically retry with exponential backoff | Retry attempt logging and outcome tracking | Handles transient failures without manual intervention |\n| | System recovers from worker crashes within 30 seconds | Worker health monitoring and replacement time | Maintains processing capacity during failures |\n| | Zero data loss for accepted processing jobs | Job persistence and completion verification | Protects user content and maintains service trust |\n| **Scalability Characteristics** | Linear scaling to 50 worker processes per node | Throughput measurement across worker counts | Handles traffic growth through horizontal scaling |\n| | Supports 10,000 queued jobs without performance degradation | Queue depth vs processing latency correlation | Manages traffic spikes and batch processing loads |\n| | Worker processes scale up/down based on queue depth | Auto-scaling trigger timing and effectiveness | Cost optimization during variable load periods |\n| | Storage scales to 100TB of processing temp files | Disk usage monitoring and cleanup verification | Supports large-scale video processing operations |\n| **Resource Management** | Individual jobs limited to 8GB memory usage | Process memory monitoring and enforcement | Prevents single large jobs from destabilizing system |\n| | Temp file cleanup within 1 hour of job completion | File system monitoring and cleanup verification | Maintains disk space availability for ongoing operations |\n| | CPU utilization averaged across workers stays below 80% | System resource monitoring and alert thresholds | Maintains responsive performance during peak loads |\n| | Network bandwidth utilization for job coordination under 100Mbps | Inter-component traffic measurement | Ensures job queue doesn't become network bottleneck |\n\n**Progress tracking accuracy** represents a particularly challenging non-functional requirement. Video transcoding progress estimation requires stage-based reporting rather than simple percentage completion, since FFmpeg progress varies significantly based on content complexity and encoding parameters. The system must provide meaningful progress updates that help users understand processing status without making overly precise time commitments.\n\n> **Decision: Consistency vs Availability Trade-offs**\n> - **Context**: Media processing jobs represent significant computational work that shouldn't be lost, but the system must remain responsive during partial failures\n> - **Options Considered**:\n>   1. Strict consistency requiring all components to acknowledge job state changes\n>   2. Eventual consistency with job state synchronization through message queue\n>   3. Hybrid approach with strong consistency for job submission, eventual consistency for progress updates\n> - **Decision**: Hybrid consistency model with durable job submission and eventually consistent progress reporting\n> - **Rationale**: Job submission must be durable to prevent lost work, but progress updates can be eventually consistent since they're informational rather than critical for system correctness\n> - **Consequences**: Simplified client integration with reliable job submission, but progress updates may occasionally show stale information\n\n**Security and compliance requirements** focus on protecting user content and metadata privacy. The system must provide configurable EXIF metadata stripping to remove potentially sensitive location and device information from processed images. All temporary files must be securely deleted after processing completion, and webhook notifications must use signature verification to prevent spoofing attacks.\n\n### Explicit Non-Goals\n\nThe **explicit non-goals** define functionality that we intentionally exclude from this media processing pipeline implementation. These boundaries prevent scope creep and ensure we can deliver high-quality core functionality rather than attempting to solve every possible media processing challenge.\n\n> **Decision: Content Storage and Management Exclusion**\n> - **Context**: Many media processing systems also provide content storage, CDN integration, and digital asset management features\n> - **Options Considered**:\n>   1. Full-featured media management platform with storage, metadata, and delivery\n>   2. Processing-only service that integrates with external storage systems\n>   3. Hybrid approach with basic storage and advanced processing capabilities\n> - **Decision**: Processing-only service with clear storage integration points\n> - **Rationale**: Content storage, CDN management, and digital asset management represent separate domain expertise areas with different scaling characteristics and operational requirements\n> - **Consequences**: Simpler system design focused on processing excellence, but requires clients to manage their own storage and delivery infrastructure\n\n| Non-Goal Category | Specific Exclusions | Rationale | Alternative Solutions |\n|-------------------|-------------------|-----------|---------------------|\n| **Content Management** | Long-term storage of original or processed media files | Storage requirements vary dramatically by use case and compliance needs | Integrate with S3, Google Cloud Storage, or local file systems |\n| | Digital asset management with metadata search and organization | Requires different expertise in search indexing and content categorization | Use dedicated DAM systems like Adobe Experience Manager or custom solutions |\n| | Content delivery network integration and optimization | CDN management requires separate operational expertise and provider relationships | Integrate processed outputs with CloudFlare, AWS CloudFront, or similar |\n| | User authentication and access control for media files | Authentication adds complexity unrelated to media processing core competency | Handle auth at API gateway or application layer before job submission |\n| **Advanced Processing Features** | AI-powered content analysis and automatic tagging | Machine learning model management requires different infrastructure and expertise | Integrate with cloud AI services or specialized content analysis tools |\n| | Real-time live streaming and broadcast processing | Live streaming has different latency, resource, and reliability requirements than batch processing | Use dedicated streaming platforms like Wowza, OBS, or cloud streaming services |\n| | Advanced video editing operations beyond transcoding | Complex editing requires timeline management, effects processing, and interactive workflows | Integrate with video editing APIs or desktop applications for advanced editing |\n| | Content moderation and inappropriate content detection | Moderation requires specialized models, human review workflows, and policy management | Use dedicated moderation services like AWS Rekognition or manual review processes |\n| **Specialized Format Support** | Legacy or proprietary media formats with complex licensing requirements | Specialized formats require additional dependencies and potentially expensive licensing | Handle conversion to standard formats before submitting to processing pipeline |\n| | Professional broadcast formats and workflows | Broadcast requirements include specialized metadata, color spaces, and workflow integration | Use professional broadcast tools and integrate outputs with standard pipeline |\n| | 3D media processing, VR/AR content optimization | Immersive media has different processing requirements and specialized libraries | Develop separate processing pipeline optimized for 3D workflows |\n| | Medical or scientific imaging format support | Specialized domains require DICOM, microscopy format expertise | Use domain-specific processing tools and convert outputs to standard formats |\n| **Infrastructure and Operations** | Built-in monitoring, alerting, and observability dashboards | Monitoring requirements vary by deployment environment and operational preferences | Integrate with existing monitoring stack using structured logging and metrics |\n| | Automatic cloud provider provisioning and infrastructure management | Infrastructure automation requires deep cloud-specific knowledge and operational procedures | Use existing infrastructure-as-code tools like Terraform or cloud-native auto-scaling |\n| | Multi-tenant processing with resource isolation and billing | Multi-tenancy adds complexity in resource accounting, security isolation, and billing integration | Deploy separate instances per tenant or add tenancy features in future versions |\n| | Geographic distribution and edge processing capabilities | Edge deployment requires different architectural patterns and content synchronization strategies | Deploy multiple regional instances or integrate with edge computing platforms |\n\n**Real-time processing requirements** represent a significant non-goal that affects architecture decisions. While the system provides progress tracking and webhook notifications, it does not attempt to provide sub-second processing latency or real-time streaming capabilities. Batch processing optimization allows for better resource utilization and more reliable error recovery compared to real-time processing constraints.\n\n**Custom processing algorithm development** falls outside our scope. The system integrates with proven libraries like Pillow for image processing and FFmpeg for video transcoding rather than implementing custom algorithms. This decision ensures reliability and compatibility while focusing development effort on job orchestration, progress tracking, and error recovery—the areas where custom logic provides the most value.\n\n> **Critical Boundary Decision**: We explicitly exclude **content-aware processing features** like automatic cropping based on face detection, intelligent compression based on content analysis, or quality optimization based on viewing context. These features require machine learning integration and significantly more complex configuration management. Clients needing content-aware features should implement them as separate preprocessing steps or integrate with specialized AI services.\n\nThe non-goals serve as architectural constraints that simplify system design and clarify integration points. By excluding storage management, we avoid the complexity of data durability guarantees and focus on processing reliability. By excluding real-time requirements, we can optimize for throughput and implement simpler retry mechanisms. By excluding content analysis, we avoid the operational complexity of managing machine learning models and their dependencies.\n\nThese boundaries are not permanent limitations but represent conscious decisions about the current implementation scope. Future versions might incorporate some excluded features as optional components or extension points, but the initial system focuses on delivering excellent core media processing capabilities with clear integration patterns for external systems to provide the excluded functionality.\n\n### Implementation Guidance\n\nThis implementation guidance provides practical direction for translating the goals and requirements into working code structure and development milestones.\n\n**A. Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|--------------|----------------|\n| **Job Queue** | Redis with simple pub/sub | Celery with RabbitMQ broker |\n| **Progress Tracking** | Redis hash with periodic updates | WebSocket server with real-time updates |\n| **Webhook Delivery** | Python requests with basic retry | Task queue with exponential backoff |\n| **Configuration Management** | JSON config files | Environment-based config with validation |\n| **Logging** | Python logging to files | Structured logging with correlation IDs |\n| **Metrics** | Simple counters in Redis | Prometheus metrics with custom collectors |\n\n**B. Recommended File Structure:**\n\n```python\nmedia_pipeline/\n├── config/\n│   ├── __init__.py\n│   ├── app_config.py          # AppConfig, RedisConfig, StorageConfig\n│   └── settings.py            # Configuration loading and validation\n├── core/\n│   ├── __init__.py\n│   ├── job_manager.py         # ProcessingJob operations\n│   ├── progress_tracker.py    # Progress reporting and webhook delivery\n│   └── types.py              # JobStatus, JobPriority enums\n├── workers/\n│   ├── __init__.py\n│   ├── base_worker.py        # Abstract worker interface\n│   ├── image_worker.py       # Image processing implementation\n│   └── video_worker.py       # Video transcoding implementation\n├── api/\n│   ├── __init__.py\n│   ├── handlers.py           # HTTP request handlers\n│   └── validation.py         # Input validation and sanitization\n├── utils/\n│   ├── __init__.py\n│   ├── logging.py            # Structured logging setup\n│   └── storage.py            # File system utilities\n└── tests/\n    ├── integration/          # End-to-end job processing tests\n    ├── unit/                 # Component unit tests\n    └── fixtures/             # Sample media files for testing\n```\n\n**C. Infrastructure Starter Code:**\n\n```python\n# config/app_config.py - Complete configuration management\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport json\nimport os\n\n@dataclass\nclass RedisConfig:\n    host: str = \"localhost\"\n    port: int = 6379\n    db: int = 0\n    password: Optional[str] = None\n    \n    def connection_url(self) -> str:\n        if self.password:\n            return f\"redis://:{self.password}@{self.host}:{self.port}/{self.db}\"\n        return f\"redis://{self.host}:{self.port}/{self.db}\"\n\n@dataclass\nclass StorageConfig:\n    base_path: str = \"/tmp/media_processing\"\n    temp_path: str = \"/tmp/media_processing/temp\"\n    max_file_size: int = 1024 * 1024 * 1024  # 1GB\n    \n    def __post_init__(self):\n        os.makedirs(self.base_path, exist_ok=True)\n        os.makedirs(self.temp_path, exist_ok=True)\n\n@dataclass\nclass ProcessingConfig:\n    max_workers: int = 4\n    job_timeout: int = 3600  # 1 hour\n    retry_attempts: int = 3\n    webhook_timeout: int = 30\n    \n@dataclass\nclass AppConfig:\n    redis: RedisConfig\n    storage: StorageConfig\n    processing: ProcessingConfig\n    debug: bool = False\n\ndef init_config(config_path: Optional[str] = None) -> AppConfig:\n    \"\"\"Initialize application configuration from file or environment variables.\"\"\"\n    # TODO 1: Load config from JSON file if config_path provided\n    # TODO 2: Override with environment variables where present\n    # TODO 3: Validate configuration values and ranges\n    # TODO 4: Initialize storage directories\n    # TODO 5: Return complete AppConfig instance\n    pass\n\n# utils/logging.py - Complete logging infrastructure\nimport logging\nimport json\nfrom datetime import datetime\nfrom contextlib import contextmanager\nfrom typing import Optional\n\nclass JSONFormatter(logging.Formatter):\n    \"\"\"JSON formatter for structured logging.\"\"\"\n    \n    def format(self, record):\n        log_entry = {\n            'timestamp': datetime.utcnow().isoformat(),\n            'level': record.levelname,\n            'message': record.getMessage(),\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n        \n        # Add job context if present\n        if hasattr(record, 'job_id'):\n            log_entry['job_id'] = record.job_id\n        if hasattr(record, 'correlation_id'):\n            log_entry['correlation_id'] = record.correlation_id\n            \n        return json.dumps(log_entry)\n\ndef setup_logging(debug: bool = False):\n    \"\"\"Configure structured logging system.\"\"\"\n    level = logging.DEBUG if debug else logging.INFO\n    \n    handler = logging.StreamHandler()\n    handler.setFormatter(JSONFormatter())\n    \n    root_logger = logging.getLogger()\n    root_logger.setLevel(level)\n    root_logger.addHandler(handler)\n    \n    # Suppress noisy third-party logs\n    logging.getLogger('urllib3').setLevel(logging.WARNING)\n    logging.getLogger('requests').setLevel(logging.WARNING)\n\n@contextmanager\ndef job_logging_context(job_id: str, correlation_id: Optional[str] = None):\n    \"\"\"Context manager for job-aware logging.\"\"\"\n    # TODO 1: Get current logger\n    # TODO 2: Create logging adapter with job_id and correlation_id\n    # TODO 3: Yield adapter for use in job processing\n    # TODO 4: Clean up context on exit\n    pass\n```\n\n**D. Core Logic Skeleton Code:**\n\n```python\n# core/types.py - Core data types\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import List, Optional, Dict, Any\n\nclass JobStatus(Enum):\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\nclass JobPriority(Enum):\n    LOW = 1\n    NORMAL = 5\n    HIGH = 10\n    URGENT = 20\n\n@dataclass\nclass ProcessingJob:\n    job_id: str\n    input_file_path: str\n    output_specifications: List[Dict[str, Any]]\n    priority: JobPriority\n    status: JobStatus = JobStatus.PENDING\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error_message: Optional[str] = None\n    retry_count: int = 0\n    webhook_url: Optional[str] = None\n    progress_percentage: float = 0.0\n    estimated_duration: Optional[int] = None\n\n# core/job_manager.py - Job lifecycle management\nimport uuid\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\n\nfrom .types import ProcessingJob, JobStatus, JobPriority\n\nclass JobManager:\n    \"\"\"Manages processing job lifecycle and state transitions.\"\"\"\n    \n    def __init__(self, redis_client, storage_config):\n        self.redis = redis_client\n        self.storage = storage_config\n    \n    def submit_job(self, input_file: str, output_specs: List[Dict[str, Any]], \n                   priority: JobPriority, webhook_url: Optional[str] = None) -> ProcessingJob:\n        \"\"\"Create and queue new processing job.\"\"\"\n        # TODO 1: Generate unique job_id using uuid4\n        # TODO 2: Validate input_file exists and is readable\n        # TODO 3: Validate output_specs format and required fields\n        # TODO 4: Create ProcessingJob instance with provided parameters\n        # TODO 5: Serialize job to Redis with job:{job_id} key\n        # TODO 6: Add job to priority queue based on priority level\n        # TODO 7: Return created ProcessingJob instance\n        pass\n    \n    def update_job_progress(self, job_id: str, progress_percentage: float, \n                           stage: str, details: Dict[str, Any]):\n        \"\"\"Update job progress and send notifications.\"\"\"\n        # TODO 1: Retrieve job from Redis using job_id\n        # TODO 2: Update progress_percentage and add stage information\n        # TODO 3: Calculate estimated_duration based on progress rate\n        # TODO 4: Save updated job state to Redis\n        # TODO 5: Send webhook notification if configured\n        # TODO 6: Log progress update with job context\n        pass\n    \n    def mark_job_completed(self, job_id: str, output_files: List[str]):\n        \"\"\"Finalize completed job and cleanup.\"\"\"\n        # TODO 1: Retrieve job from Redis and validate current status\n        # TODO 2: Update job status to COMPLETED with completion timestamp\n        # TODO 3: Store output_files list in job record\n        # TODO 4: Send completion webhook notification\n        # TODO 5: Schedule cleanup of temporary files\n        # TODO 6: Update job completion metrics\n        pass\n    \n    def mark_job_failed(self, job_id: str, error_message: str, retry_eligible: bool):\n        \"\"\"Handle job failure with retry logic.\"\"\"\n        # TODO 1: Retrieve job from Redis and increment retry_count\n        # TODO 2: Check if retry_eligible and retry_count < max_retry_attempts\n        # TODO 3: If retryable, requeue job with exponential backoff delay\n        # TODO 4: If not retryable, mark job as FAILED permanently\n        # TODO 5: Store error_message and send failure webhook notification\n        # TODO 6: Clean up temporary files for failed job\n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- **Redis Integration**: Use `redis-py` library with connection pooling for job queue operations. Store jobs as JSON-serialized strings with TTL for automatic cleanup.\n- **File Processing**: Use `pathlib.Path` for cross-platform file path handling. Always use context managers (`with open()`) for file operations to ensure proper cleanup.\n- **Process Management**: Use `subprocess.run()` with timeout parameters for FFmpeg integration. Capture stdout/stderr for progress parsing and error reporting.\n- **Error Handling**: Create custom exception classes for different failure types (transient vs permanent). Use try/except blocks with specific exception types rather than bare `except:` clauses.\n- **Async Operations**: Consider `asyncio` for webhook delivery and I/O operations, but keep media processing in separate worker processes to avoid blocking the event loop.\n\n**F. Milestone Checkpoints:**\n\n**After Requirements Definition:**\n- Run `python -m pytest tests/test_requirements.py -v` to verify requirement validation logic\n- Expected: All functional requirement validation passes, non-functional requirement measurements return reasonable defaults\n- Manual verification: Create sample `ProcessingJob` instances with different priorities and validate serialization/deserialization\n\n**After Goal Validation Implementation:**\n- Test command: `python scripts/validate_goals.py --config config/test.json`\n- Expected output: \"✓ All functional requirements validated\", \"✓ Non-functional requirements initialized\", \"✓ Non-goals properly excluded\"\n- Manual verification: Attempt to submit job with excluded feature (should raise clear error), verify job priority ordering in queue\n\n**G. Common Implementation Pitfalls:**\n\n| Pitfall | Symptoms | Root Cause | Solution |\n|---------|----------|------------|----------|\n| **Requirement Validation Missing** | Jobs fail mysteriously during processing | No upfront validation of input parameters | Add comprehensive validation in `submit_job()` before queuing |\n| **Priority Queue Not Working** | High priority jobs wait behind low priority | Using simple FIFO queue instead of priority queue | Implement Redis sorted sets with priority scores for job ordering |\n| **Non-Functional Requirements Not Measured** | Performance degrades without notice | No monitoring of actual vs required performance | Add metrics collection for all non-functional requirements |\n| **Goal Scope Creep** | System becomes complex and unstable | Adding features that should be non-goals | Regularly review and reject features not in functional requirements |\n\nThe implementation should start with requirement validation and configuration management before building any processing capabilities. This foundation ensures all subsequent components operate within the defined goals and can be tested against concrete acceptance criteria.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** All milestones (1-3) as this architectural foundation supports image processing, video transcoding, and job queue components\n\n### Mental Model: Modern Manufacturing Assembly Line\n\nThink of our media processing pipeline as a modern manufacturing assembly line, but instead of building cars or electronics, we're transforming raw media files into optimized, web-ready content. Just like Toyota's production system revolutionized manufacturing with just-in-time delivery and quality control at every stage, our pipeline processes media files through specialized stations with real-time monitoring and automatic error recovery.\n\nThe **API gateway** acts as the customer service desk where orders (processing jobs) are received, validated, and entered into the system with detailed specifications. The **job queue** functions as the production control system that schedules work orders and routes them to the appropriate assembly stations based on priority and worker availability. Each **worker process** represents a specialized assembly station equipped with specific tools—some stations excel at image manipulation using precision instruments (Pillow), while others handle complex video assembly using industrial-grade equipment (FFmpeg). The **storage layer** serves as both the incoming materials warehouse and the finished goods depot, organizing raw inputs and polished outputs with proper inventory tracking.\n\nWhat makes this assembly line particularly sophisticated is its ability to handle rush orders (high-priority jobs), automatically retry failed operations when a station encounters problems, and provide real-time progress updates to customers waiting for their custom media products. Unlike traditional assembly lines that process identical widgets, our pipeline handles diverse media formats and transforms them according to unique specifications for each job.\n\n### Component Overview\n\nThe media processing pipeline consists of four primary architectural layers, each with distinct responsibilities and clear interfaces for maximum modularity and testability.\n\n#### API Gateway Layer\n\nThe **API Gateway** serves as the system's primary entry point, handling all external client interactions and providing a clean REST interface for job submission and status monitoring. This component owns the responsibility for request validation, authentication, rate limiting, and translating external API contracts into internal job specifications.\n\n| Component | Primary Responsibility | Key Operations | External Dependencies |\n|-----------|----------------------|----------------|---------------------|\n| REST API Server | HTTP request handling and response formatting | Job submission, status queries, file uploads | Flask/FastAPI framework |\n| Request Validator | Input sanitization and schema validation | Media file type checking, parameter validation | JSON schema libraries |\n| Authentication Handler | API key and webhook signature verification | Token validation, signature generation | JWT libraries |\n| Rate Limiter | Traffic control and abuse prevention | Request counting, backoff enforcement | Redis for distributed counting |\n\nThe gateway operates stateless by design, allowing horizontal scaling through simple load balancing. All persistent state lives in the job queue and storage layers, while the gateway focuses purely on protocol translation and input validation.\n\n#### Job Queue Layer\n\nThe **Job Queue** implements the core orchestration logic, managing the lifecycle of processing jobs from submission through completion. This layer abstracts away the complexities of distributed task scheduling and provides reliable message delivery semantics with exactly-once processing guarantees.\n\n| Component | Primary Responsibility | Key Operations | Storage Requirements |\n|-----------|----------------------|----------------|-------------------|\n| Job Manager | Job lifecycle coordination | Job creation, state transitions, cleanup | Job metadata persistence |\n| Priority Scheduler | Work distribution based on priority and resources | Queue ordering, worker assignment | Priority queue implementation |\n| Progress Tracker | Real-time status monitoring and reporting | Progress updates, time estimation | Progress state storage |\n| Webhook Dispatcher | External notification delivery | HTTP callbacks, retry logic | Webhook delivery logs |\n\nThe queue layer maintains strong consistency for job state while providing eventual consistency for progress updates. This design ensures that jobs never get lost while allowing for high-throughput progress reporting without blocking core operations.\n\n> **Decision: Redis vs RabbitMQ for Message Queuing**\n> - **Context**: Need reliable message delivery with priority support and progress tracking\n> - **Options Considered**: Redis with sorted sets, RabbitMQ with priority queues, Amazon SQS with message attributes\n> - **Decision**: Redis with sorted sets for primary queue, RabbitMQ for complex routing scenarios\n> - **Rationale**: Redis provides atomic operations for priority management and excellent performance for frequent progress updates. RabbitMQ adds complexity but offers superior durability guarantees for critical jobs.\n> - **Consequences**: Enables sub-second job scheduling with atomic priority updates but requires Redis persistence configuration and monitoring\n\n| Queue Option | Pros | Cons | Use Case Fit |\n|--------------|------|------|-------------|\n| Redis Sorted Sets | Atomic priority operations, excellent performance, built-in progress storage | Less durable than dedicated message brokers, memory-based | High-throughput processing with frequent priority changes |\n| RabbitMQ Priority Queues | Strong durability, mature ecosystem, complex routing | Higher latency for simple operations, more operational overhead | Mission-critical jobs requiring guaranteed delivery |\n| Amazon SQS | Fully managed, infinite scale, integrated with AWS | Vendor lock-in, limited priority levels, higher per-message cost | Cloud-native deployments with moderate priority requirements |\n\n#### Worker Process Layer\n\n**Worker Processes** execute the actual media transformation logic, providing isolated execution environments with resource monitoring and automatic recovery capabilities. Each worker specializes in specific media types while sharing common infrastructure for job acquisition, progress reporting, and error handling.\n\n| Worker Type | Specialization | Resource Requirements | Typical Processing Time |\n|-------------|----------------|-------------------|---------------------|\n| Image Worker | JPEG/PNG/WebP processing, thumbnail generation | 512MB RAM, moderate CPU | 1-30 seconds per job |\n| Video Worker | FFmpeg-based transcoding, segment generation | 2GB+ RAM, high CPU/disk I/O | 30 seconds to 2+ hours per job |\n| Batch Worker | Multi-file operations, archive processing | Variable based on batch size | Minutes to hours |\n\nWorkers implement a polling model with intelligent backoff, claiming jobs from the priority queue based on their capabilities and current resource utilization. Each worker process runs in isolation with configurable memory and CPU limits to prevent resource exhaustion from affecting other jobs.\n\n#### Storage Layer\n\nThe **Storage Layer** provides persistent storage for input files, intermediate processing artifacts, and final output files with proper lifecycle management and cleanup policies. This layer abstracts storage implementation details while providing consistent interfaces for file operations across different backend systems.\n\n| Storage Component | Purpose | Consistency Requirements | Cleanup Policy |\n|------------------|---------|------------------------|---------------|\n| Input File Storage | Raw uploaded media files | Strong consistency for uploads | Retain until processing complete |\n| Work Directory Manager | Temporary processing workspace | Local consistency per worker | Immediate cleanup after job completion |\n| Output File Storage | Processed media deliverables | Strong consistency for results | Configurable retention policy |\n| Metadata Database | Job state and processing logs | Strong consistency for job state | Archive after job expiration |\n\n### Request Processing Flow\n\nThe end-to-end processing flow orchestrates multiple components to transform uploaded media files into optimized outputs while providing comprehensive monitoring and error recovery.\n\n#### Job Submission Phase\n\nWhen a client submits a new processing job, the request flows through several validation and preparation stages before entering the execution queue:\n\n1. **Request Reception**: The API gateway receives an HTTP POST request containing the input media file, output specifications, priority level, and optional webhook URL for notifications.\n\n2. **Input Validation**: The request validator examines the uploaded file to determine media type, validates format support, checks file size limits, and ensures output specifications are achievable given the input characteristics.\n\n3. **Job Creation**: The job manager generates a unique job identifier, creates a `ProcessingJob` record with status `JobStatus.PENDING`, and calculates initial processing time estimates based on file size and requested operations.\n\n4. **Queue Insertion**: The priority scheduler inserts the job into the appropriate queue based on the requested `JobPriority` level and current system load, using atomic Redis operations to maintain queue consistency.\n\n5. **Client Response**: The API gateway returns the job identifier and initial status to the client, allowing them to track progress through subsequent status queries.\n\n| Phase | Duration | Failure Modes | Recovery Strategy |\n|-------|----------|--------------|------------------|\n| File Upload | 1-60 seconds | Network interruption, file corruption | Client retry with resumable uploads |\n| Validation | <1 second | Invalid format, unsupported operations | Immediate error response with specific details |\n| Job Creation | <100ms | Database connectivity, resource exhaustion | Automatic retry with exponential backoff |\n| Queue Insertion | <50ms | Redis connectivity, memory limits | Failover to secondary queue or delayed retry |\n\n#### Job Execution Phase\n\nOnce queued, jobs wait for available worker processes that match their resource requirements and processing capabilities:\n\n1. **Worker Job Acquisition**: Workers poll the priority queue using Redis ZPOPMIN operations to atomically claim the highest-priority job matching their capabilities, updating job status to `JobStatus.PROCESSING`.\n\n2. **Resource Preparation**: The assigned worker creates an isolated working directory, downloads the input file from storage, and initializes processing tools (Pillow for images, FFmpeg for videos) with job-specific configuration parameters.\n\n3. **Processing Execution**: The worker executes the media transformation pipeline, reporting progress updates at regular intervals using `update_job_progress()` calls that broadcast status changes to monitoring systems and trigger webhook notifications.\n\n4. **Output Generation**: Completed processing artifacts are uploaded to the output storage layer with proper naming conventions and metadata tags, ensuring atomic replacement of any existing files.\n\n5. **Job Completion**: The worker calls `mark_job_completed()` to update the job status, trigger final webhook notifications, and schedule cleanup of temporary resources.\n\nThe execution phase implements comprehensive error handling with automatic retries for transient failures and clear failure classification for permanent errors that require human intervention.\n\n#### Progress Reporting and Notification Flow\n\nThroughout processing, the system maintains real-time visibility into job status and estimated completion times:\n\n1. **Stage-Based Progress**: Workers report progress using discrete stages rather than time-based percentages, providing more accurate estimates for complex operations like video transcoding where processing speed varies significantly across different content sections.\n\n2. **Webhook Delivery**: The webhook dispatcher sends HTTP POST notifications to client-provided URLs for all significant job state changes, implementing retry logic with exponential backoff for failed delivery attempts.\n\n3. **Status Queries**: Clients can query job status at any time through the API gateway, receiving current progress percentage, estimated completion time, and detailed stage information without impacting processing performance.\n\n| Notification Type | Trigger Event | Payload Contents | Retry Policy |\n|-------------------|--------------|-----------------|-------------|\n| Job Started | Status change to PROCESSING | Job ID, worker assignment, estimated duration | 3 retries over 15 minutes |\n| Progress Update | Configurable percentage milestones | Current progress, stage description, time remaining | 2 retries over 5 minutes |\n| Job Completed | Successful processing finish | Job ID, output file URLs, final metrics | 5 retries over 1 hour |\n| Job Failed | Permanent processing failure | Job ID, error details, retry eligibility | 5 retries over 1 hour |\n\n### Recommended Project Structure\n\nThe codebase organization reflects the architectural layers while promoting clear separation of concerns and enabling independent testing of each component.\n\n#### Top-Level Directory Organization\n\nThe project structure follows Python packaging best practices with clear separation between application logic, configuration, and external interfaces:\n\n```\nmedia-processing-pipeline/\n├── src/\n│   └── media_processor/           # Main application package\n│       ├── __init__.py\n│       ├── api/                   # API Gateway Layer\n│       ├── queue/                 # Job Queue Layer\n│       ├── workers/               # Worker Process Layer\n│       ├── storage/               # Storage Layer\n│       ├── models/                # Shared data models\n│       └── common/                # Cross-cutting utilities\n├── config/                        # Configuration files\n│   ├── development.yaml\n│   ├── production.yaml\n│   └── testing.yaml\n├── migrations/                    # Database schema migrations\n├── tests/                         # Test organization mirrors src/\n│   ├── unit/\n│   ├── integration/\n│   └── fixtures/                  # Sample media files\n├── scripts/                       # Deployment and maintenance scripts\n├── docker/                        # Container configurations\n└── docs/                          # Architecture and API documentation\n```\n\n#### API Gateway Module Structure\n\nThe API layer implements a clean separation between HTTP handling, business logic, and external integrations:\n\n```\nsrc/media_processor/api/\n├── __init__.py\n├── app.py                         # FastAPI application factory\n├── routes/\n│   ├── __init__.py\n│   ├── jobs.py                    # Job submission and status endpoints\n│   ├── health.py                  # System health and monitoring\n│   └── webhooks.py                # Webhook management endpoints\n├── middleware/\n│   ├── __init__.py\n│   ├── authentication.py         # API key validation\n│   ├── rate_limiting.py          # Request throttling\n│   └── request_logging.py        # Structured request/response logging\n├── schemas/\n│   ├── __init__.py\n│   ├── request_models.py         # Input validation schemas\n│   └── response_models.py        # Output serialization schemas\n└── dependencies.py               # FastAPI dependency injection setup\n```\n\n#### Job Queue Module Structure\n\nThe queue layer encapsulates all aspects of job lifecycle management and worker coordination:\n\n```\nsrc/media_processor/queue/\n├── __init__.py\n├── job_manager.py                # Core job lifecycle operations\n├── priority_scheduler.py         # Queue ordering and worker assignment\n├── progress_tracker.py          # Real-time progress monitoring\n├── webhook_dispatcher.py        # External notification delivery\n├── backends/                     # Queue implementation adapters\n│   ├── __init__.py\n│   ├── redis_queue.py           # Redis-based queue operations\n│   └── rabbitmq_queue.py        # RabbitMQ integration (optional)\n└── serializers.py               # Job data serialization formats\n```\n\n#### Worker Process Module Structure\n\nThe worker layer organizes media processing capabilities into specialized modules with shared infrastructure:\n\n```\nsrc/media_processor/workers/\n├── __init__.py\n├── base_worker.py               # Common worker infrastructure\n├── worker_manager.py            # Worker process lifecycle management\n├── image_worker.py              # Image processing specialization\n├── video_worker.py              # Video transcoding specialization\n├── processors/                  # Media processing implementations\n│   ├── __init__.py\n│   ├── image_processor.py       # Pillow-based image operations\n│   ├── video_processor.py       # FFmpeg integration layer\n│   └── thumbnail_generator.py   # Cross-format thumbnail creation\n└── resource_monitor.py          # Memory and CPU usage tracking\n```\n\n#### Shared Models and Utilities Structure\n\nCommon code shared across all layers resides in dedicated modules to prevent circular dependencies:\n\n```\nsrc/media_processor/models/\n├── __init__.py\n├── job_models.py                # ProcessingJob and related entities\n├── config_models.py            # AppConfig and component configurations\n└── enums.py                     # JobStatus, JobPriority, and other constants\n\nsrc/media_processor/common/\n├── __init__.py\n├── logging_setup.py             # Structured logging configuration\n├── config_loader.py            # Configuration file parsing\n├── exceptions.py               # Custom exception hierarchy\n└── utils.py                    # Generic utility functions\n```\n\n> **Decision: Package Organization Strategy**\n> - **Context**: Need to balance modularity with import simplicity while preventing circular dependencies\n> - **Options Considered**: Flat structure with all modules at top level, strict layered packages, domain-driven module grouping\n> - **Decision**: Layered packages with shared models and utilities at the same level\n> - **Rationale**: Reflects architectural boundaries while making dependencies explicit. Shared models prevent import cycles between layers.\n> - **Consequences**: Enables independent testing of each layer and clear dependency direction from API → Queue → Workers → Storage\n\n#### Configuration and Deployment Structure\n\nEnvironment-specific configuration and deployment artifacts maintain clear separation to support multiple deployment targets:\n\n| Configuration File | Purpose | Contains | Environment |\n|-------------------|---------|-----------|-------------|\n| `config/development.yaml` | Local development settings | Debug logging, local Redis, relaxed timeouts | Developer workstations |\n| `config/testing.yaml` | Automated test configuration | In-memory queues, mock external services | CI/CD pipelines |\n| `config/production.yaml` | Production deployment settings | Cluster Redis, strict timeouts, monitoring | Production clusters |\n| `docker/worker.Dockerfile` | Worker container image | Media processing tools, Python runtime | All containerized environments |\n\n⚠️ **Pitfall: Circular Import Dependencies**\n\nA common mistake in Python projects is creating circular imports between layers, especially when the API layer needs to import job models that also reference API-specific types. This manifests as `ImportError: cannot import name` exceptions during module loading.\n\n**Why it's wrong**: Python cannot resolve module dependencies when they form cycles, leading to runtime import failures that only appear when specific code paths are executed.\n\n**How to fix**: Place all shared data models (`ProcessingJob`, `AppConfig`, etc.) in a dedicated `models` package that doesn't import from other application layers. Use dependency injection at the application boundary to provide layer-specific implementations to shared models.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|--------------|----------------|\n| Web Framework | Flask with Flask-RESTful | FastAPI with async support |\n| Message Queue | Redis with manual job management | Celery with Redis/RabbitMQ backend |\n| Configuration | Python configparser with INI files | Pydantic with YAML configuration |\n| Database | SQLite with simple schema | PostgreSQL with SQLAlchemy ORM |\n| File Storage | Local filesystem with organized directories | AWS S3 with boto3 integration |\n| Logging | Python logging with file rotation | Structured logging with JSON output |\n| Testing | unittest with manual mocks | pytest with pytest-asyncio and fixtures |\n| Containerization | Single Docker container | Multi-stage builds with Alpine Linux |\n\n#### Recommended File Structure\n\n```\nmedia-processing-pipeline/\n├── src/\n│   └── media_processor/\n│       ├── __init__.py\n│       ├── main.py                    # Application entry point\n│       ├── config.py                  # Configuration management\n│       ├── models/\n│       │   ├── __init__.py\n│       │   ├── job_models.py          # ProcessingJob, JobStatus, JobPriority\n│       │   └── config_models.py       # AppConfig, RedisConfig, etc.\n│       ├── api/\n│       │   ├── __init__.py\n│       │   ├── app.py                 # FastAPI application setup\n│       │   └── routes/\n│       │       ├── __init__.py\n│       │       └── jobs.py            # Job submission endpoints\n│       ├── queue/\n│       │   ├── __init__.py\n│       │   ├── job_manager.py         # Core job operations\n│       │   └── redis_queue.py         # Redis integration\n│       ├── workers/\n│       │   ├── __init__.py\n│       │   ├── base_worker.py         # Worker base class\n│       │   ├── image_worker.py        # Image processing worker\n│       │   └── video_worker.py        # Video processing worker\n│       └── storage/\n│           ├── __init__.py\n│           └── file_manager.py        # File operations\n├── config/\n│   ├── development.yaml\n│   └── production.yaml\n├── requirements.txt\n├── requirements-dev.txt\n└── tests/\n    ├── __init__.py\n    ├── test_job_manager.py\n    └── fixtures/\n        ├── sample.jpg\n        └── sample.mp4\n```\n\n#### Infrastructure Starter Code\n\n**Configuration Management (src/media_processor/config.py)**\n\n```python\nimport yaml\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass RedisConfig:\n    host: str = \"localhost\"\n    port: int = 6379\n    db: int = 0\n    password: Optional[str] = None\n\n@dataclass\nclass StorageConfig:\n    base_path: str = \"./storage\"\n    temp_path: str = \"./temp\"\n    max_file_size: int = 100 * 1024 * 1024  # 100MB\n\n@dataclass\nclass ProcessingConfig:\n    max_workers: int = 4\n    job_timeout: int = 3600  # 1 hour\n    retry_attempts: int = 3\n    webhook_timeout: int = 30\n\n@dataclass\nclass AppConfig:\n    redis: RedisConfig\n    storage: StorageConfig\n    processing: ProcessingConfig\n    debug: bool = False\n\ndef init_config(config_path: str) -> AppConfig:\n    \"\"\"Initialize application configuration from YAML file.\"\"\"\n    config_file = Path(config_path)\n    if not config_file.exists():\n        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n    \n    with open(config_file, 'r') as f:\n        config_data = yaml.safe_load(f)\n    \n    return AppConfig(\n        redis=RedisConfig(**config_data.get('redis', {})),\n        storage=StorageConfig(**config_data.get('storage', {})),\n        processing=ProcessingConfig(**config_data.get('processing', {})),\n        debug=config_data.get('debug', False)\n    )\n```\n\n**Logging Setup (src/media_processor/common/logging_setup.py)**\n\n```python\nimport logging\nimport sys\nfrom contextlib import contextmanager\nfrom typing import Optional\n\ndef setup_logging(debug: bool = False) -> None:\n    \"\"\"Configure structured logging for the application.\"\"\"\n    log_level = logging.DEBUG if debug else logging.INFO\n    log_format = (\n        \"%(asctime)s - %(name)s - %(levelname)s - \"\n        \"%(message)s [%(filename)s:%(lineno)d]\"\n    )\n    \n    logging.basicConfig(\n        level=log_level,\n        format=log_format,\n        handlers=[\n            logging.StreamHandler(sys.stdout),\n            logging.FileHandler('media_processor.log')\n        ]\n    )\n    \n    # Reduce noise from third-party libraries\n    logging.getLogger('urllib3').setLevel(logging.WARNING)\n    logging.getLogger('redis').setLevel(logging.WARNING)\n\n@contextmanager\ndef job_logging_context(job_id: str, correlation_id: Optional[str] = None):\n    \"\"\"Context manager for job-aware logging.\"\"\"\n    logger = logging.getLogger()\n    old_factory = logging.getLogRecordFactory()\n    \n    def record_factory(*args, **kwargs):\n        record = old_factory(*args, **kwargs)\n        record.job_id = job_id\n        record.correlation_id = correlation_id or job_id\n        return record\n    \n    logging.setLogRecordFactory(record_factory)\n    try:\n        yield logger\n    finally:\n        logging.setLogRecordFactory(old_factory)\n```\n\n#### Core Logic Skeleton Code\n\n**Job Manager Core (src/media_processor/queue/job_manager.py)**\n\n```python\nimport uuid\nimport logging\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom ..models.job_models import ProcessingJob, JobStatus, JobPriority\n\nlogger = logging.getLogger(__name__)\n\nclass JobManager:\n    \"\"\"Manages job lifecycle from creation to completion.\"\"\"\n    \n    def __init__(self, redis_client, storage_manager):\n        self.redis = redis_client\n        self.storage = storage_manager\n    \n    def submit_job(\n        self, \n        input_file: str, \n        output_specs: List[dict], \n        priority: JobPriority, \n        webhook_url: Optional[str] = None\n    ) -> ProcessingJob:\n        \"\"\"Create and queue new processing job.\"\"\"\n        # TODO 1: Generate unique job_id using uuid.uuid4()\n        # TODO 2: Validate input_file exists in storage\n        # TODO 3: Create ProcessingJob instance with PENDING status\n        # TODO 4: Store job metadata in Redis hash\n        # TODO 5: Add job to priority queue using Redis ZADD with priority score\n        # TODO 6: Log job creation with structured metadata\n        # Hint: Use job priority value as Redis sorted set score for automatic ordering\n        pass\n    \n    def update_job_progress(\n        self, \n        job_id: str, \n        progress_percentage: float, \n        stage: str, \n        details: Optional[str] = None\n    ) -> None:\n        \"\"\"Update job progress and send notifications.\"\"\"\n        # TODO 1: Retrieve current job from Redis\n        # TODO 2: Validate job exists and is in PROCESSING status\n        # TODO 3: Update progress_percentage and stage in job record\n        # TODO 4: Store updated job back to Redis\n        # TODO 5: Trigger webhook notification if configured\n        # TODO 6: Publish progress update to monitoring channels\n        # Hint: Use Redis HSET for atomic field updates\n        pass\n    \n    def mark_job_completed(\n        self, \n        job_id: str, \n        output_files: List[str]\n    ) -> None:\n        \"\"\"Finalize completed job and cleanup.\"\"\"\n        # TODO 1: Update job status to COMPLETED\n        # TODO 2: Set completed_at timestamp\n        # TODO 3: Store output file references\n        # TODO 4: Remove job from active processing queue\n        # TODO 5: Send completion webhook notification\n        # TODO 6: Schedule temporary file cleanup\n        # Hint: Use Redis transaction (MULTI/EXEC) for atomic state updates\n        pass\n    \n    def mark_job_failed(\n        self, \n        job_id: str, \n        error_message: str, \n        retry_eligible: bool\n    ) -> None:\n        \"\"\"Handle job failure with retry logic.\"\"\"\n        # TODO 1: Retrieve current job and increment retry_count\n        # TODO 2: Check if retry_count exceeds max_retry_attempts\n        # TODO 3: If retry eligible and under limit, requeue with exponential backoff\n        # TODO 4: Otherwise, mark as permanently FAILED\n        # TODO 5: Send failure webhook notification\n        # TODO 6: Log detailed error information for debugging\n        # Hint: Calculate backoff delay as 2^retry_count * base_delay seconds\n        pass\n```\n\n**API Application Setup (src/media_processor/api/app.py)**\n\n```python\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nimport redis\nfrom ..config import AppConfig\nfrom ..queue.job_manager import JobManager\nfrom ..storage.file_manager import FileManager\n\ndef create_app(config: AppConfig) -> FastAPI:\n    \"\"\"FastAPI application factory.\"\"\"\n    app = FastAPI(\n        title=\"Media Processing Pipeline\",\n        description=\"Scalable media processing with job queue\",\n        version=\"1.0.0\"\n    )\n    \n    # Configure CORS for web client access\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"] if config.debug else [\"https://yourdomain.com\"],\n        allow_credentials=True,\n        allow_methods=[\"GET\", \"POST\"],\n        allow_headers=[\"*\"],\n    )\n    \n    # Initialize core services\n    redis_client = redis.Redis(\n        host=config.redis.host,\n        port=config.redis.port,\n        db=config.redis.db,\n        password=config.redis.password,\n        decode_responses=True\n    )\n    \n    storage_manager = FileManager(config.storage)\n    job_manager = JobManager(redis_client, storage_manager)\n    \n    # Dependency injection for route handlers\n    def get_job_manager() -> JobManager:\n        return job_manager\n    \n    # TODO: Import and include route modules\n    # from .routes.jobs import router as jobs_router\n    # app.include_router(jobs_router, prefix=\"/api/v1\")\n    \n    return app\n```\n\n#### Language-Specific Hints\n\n**Python-Specific Considerations:**\n- Use `redis-py` client with connection pooling for high-throughput job operations\n- Configure `multiprocessing.Process` for worker isolation with proper signal handling\n- Use `pathlib.Path` instead of `os.path` for cross-platform file operations\n- Implement proper exception handling with custom exception hierarchy in `common/exceptions.py`\n- Use `dataclasses` or `pydantic` models for type safety and automatic validation\n- Configure proper virtual environment with `requirements.txt` and `requirements-dev.txt`\n\n**Redis Integration Tips:**\n- Use Redis hash structures (`HSET`/`HGET`) for job metadata storage\n- Implement atomic operations with Redis transactions (`MULTI`/`EXEC`) for job state changes\n- Use sorted sets (`ZADD`/`ZPOPMIN`) for priority queue implementation\n- Configure Redis persistence (AOF + RDB) for job durability in production\n- Set appropriate Redis memory policies for handling job queue growth\n\n**File Operation Best Practices:**\n- Create temporary directories using `tempfile.mkdtemp()` for isolated processing\n- Use context managers (`with` statements) for automatic file handle cleanup\n- Implement atomic file operations by writing to temporary files and renaming\n- Configure proper file permissions (0o600) for sensitive temporary files\n- Use `shutil.move()` for atomic cross-filesystem file operations\n\n#### Milestone Checkpoint\n\nAfter implementing the high-level architecture foundation:\n\n**What to verify manually:**\n1. Start the application with `python -m media_processor.main --config config/development.yaml`\n2. Submit a test job via curl: `curl -X POST http://localhost:8000/api/v1/jobs -F \"file=@test.jpg\"`\n3. Check Redis for job data: `redis-cli HGETALL job:[job_id]`\n4. Verify job appears in priority queue: `redis-cli ZRANGE jobs:pending 0 -1 WITHSCORES`\n\n**Expected behavior:**\n- Application starts without import errors\n- Configuration loads successfully from YAML file\n- Redis connection establishes and accepts job data\n- File uploads save to configured storage directory\n- Job IDs are properly formatted UUIDs\n- Priority queue maintains correct score ordering\n\n**Signs something is wrong:**\n- Import errors: Check Python path and package `__init__.py` files\n- Redis connection failures: Verify Redis server is running and configuration matches\n- File permission errors: Check storage directory exists and is writable\n- Configuration errors: Validate YAML syntax and required fields are present\n\n\n## Data Model and Types\n\n> **Milestone(s):** All milestones (1-3) as these core data structures underpin image processing, video transcoding, and job queue operations\n\n### Mental Model: Blueprint Library\n\nThink of our data model as a comprehensive blueprint library for a construction company. Just as architects need detailed blueprints that specify every dimension, material, and connection point before breaking ground, our media processing system needs precise data structures that define every job parameter, metadata field, and configuration option before processing begins. Each blueprint (data type) serves a specific purpose: job blueprints define what work needs to be done, media metadata blueprints capture the current state of materials, and configuration blueprints establish the standards and limits for all operations. Without these detailed specifications, workers would make inconsistent decisions, leading to structural failures (processing errors) and cost overruns (resource waste).\n\nThe power of a well-designed blueprint library lies in its ability to handle complexity through standardization. A residential blueprint looks different from a commercial one, just as image processing jobs have different data requirements than video transcoding jobs. However, both follow the same foundational patterns for dimensions, materials, and safety requirements. Similarly, our data structures establish common patterns for job identification, progress tracking, and error handling while allowing specialization for different media types and processing operations.\n\n![Data Model Relationships](./diagrams/data-model.svg)\n\n### Job and Task Entities\n\nThe job and task entities form the backbone of our asynchronous processing system, representing units of work from initial submission through final completion. These structures must capture not only the processing requirements but also the complete lifecycle state, error conditions, and progress information needed for reliable distributed processing.\n\nA **processing job** represents a single media processing request from a client, containing the input file, desired output specifications, and all metadata needed for execution and tracking. Each job moves through a well-defined lifecycle managed by the job queue system, with state transitions triggered by worker processes and external events. The job entity serves as the primary coordination point between the client-facing API, the job queue infrastructure, and the worker processes that perform actual media processing.\n\n**Processing jobs** contain both the specification of work to be performed and the runtime state of that work as it progresses through the system. This dual nature requires careful design to separate immutable job parameters (input file, output specifications) from mutable runtime state (current status, progress percentage, error conditions). The job entity must be serializable for storage in the job queue and database, while providing efficient access patterns for both job submission and progress tracking operations.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `job_id` | `str` | Unique identifier generated at job creation, used for all tracking and reference operations |\n| `input_file_path` | `str` | Absolute path to the source media file in the storage system, validated at submission time |\n| `output_specifications` | `List[OutputSpec]` | Collection of desired output formats, resolutions, and quality settings for this job |\n| `priority` | `JobPriority` | Processing priority level determining queue position and resource allocation preferences |\n| `status` | `JobStatus` | Current lifecycle state of the job, updated atomically by worker processes and queue system |\n| `created_at` | `datetime` | UTC timestamp when job was first submitted to the system |\n| `started_at` | `datetime` | UTC timestamp when worker process began processing, null if not yet started |\n| `completed_at` | `datetime` | UTC timestamp when processing finished (success or permanent failure) |\n| `error_message` | `str` | Human-readable error description for failed jobs, null for successful or pending jobs |\n| `retry_count` | `int` | Number of processing attempts made so far, incremented before each retry |\n| `webhook_url` | `str` | Optional HTTP endpoint for status notifications, validated as proper URL at submission |\n| `progress_percentage` | `float` | Current completion percentage (0.0-100.0) updated by worker during processing |\n| `estimated_duration` | `int` | Estimated total processing time in seconds, calculated based on file size and type |\n\n> **Design Insight**: The `ProcessingJob` structure intentionally separates immutable job specification (input file, output specs, webhook URL) from mutable runtime state (status, progress, timestamps). This separation enables safe concurrent access patterns where multiple processes can read the job specification while only the assigned worker updates the runtime state.\n\nThe job priority system uses numerical values to enable flexible scheduling while maintaining intuitive semantic meaning. Higher numerical values represent higher priority, allowing the job queue to use simple numerical comparison for ordering operations.\n\n| Priority Level | Numeric Value | Use Case | Typical Processing Time |\n|----------------|---------------|----------|------------------------|\n| `JobPriority.LOW` | 1 | Batch processing, background tasks | No specific SLA |\n| `JobPriority.NORMAL` | 5 | Standard user uploads, scheduled processing | Process within 1 hour |\n| `JobPriority.HIGH` | 10 | Interactive user requests, preview generation | Process within 15 minutes |\n| `JobPriority.URGENT` | 20 | Critical system operations, error recovery | Process immediately |\n\nThe job status enumeration defines a finite state machine that governs job lifecycle management and ensures consistent state transitions across the distributed system.\n\n| Status Value | Description | Valid Transitions | Worker Actions |\n|--------------|-------------|------------------|----------------|\n| `JobStatus.PENDING` | Job queued awaiting worker assignment | → PROCESSING | Worker claims job and begins processing |\n| `JobStatus.PROCESSING` | Job actively being processed by worker | → COMPLETED, FAILED | Worker updates progress and handles completion |\n| `JobStatus.COMPLETED` | Job finished successfully with outputs | None (terminal state) | Worker publishes results and cleans up |\n| `JobStatus.FAILED` | Job failed permanently after all retries | None (terminal state) | Worker logs error and triggers notifications |\n\n> **Decision: Single Job Entity vs Separate Task Breakdown**\n> - **Context**: Complex processing jobs (like video transcoding) involve multiple discrete steps that could be modeled as separate task entities\n> - **Options Considered**: \n>   1. Single `ProcessingJob` entity with stage-based progress\n>   2. `ProcessingJob` with separate `Task` entities for each processing step\n>   3. Hierarchical job structure with parent/child relationships\n> - **Decision**: Single `ProcessingJob` entity with stage-based progress tracking\n> - **Rationale**: Simplifies data model and reduces coordination complexity while still providing detailed progress information through stage reporting. Most media processing operations are sequential pipelines that don't benefit from independent task scheduling.\n> - **Consequences**: Enables simpler queue management and progress tracking, but limits ability to pause/resume individual processing steps or optimize resource allocation per step\n\n### Media Metadata Structures\n\nMedia metadata structures capture the essential characteristics of input files and processed outputs, enabling format-aware processing decisions and preserving important file attributes through the processing pipeline. These structures must handle the diversity of media formats while providing a consistent interface for processing components.\n\n**Media metadata** serves two critical functions in our processing pipeline: informing processing decisions based on source file characteristics, and preserving important metadata through format conversions. Different media types require different metadata structures, but all follow common patterns for basic file information, format specifications, and technical parameters that affect processing operations.\n\nThe metadata extraction process occurs early in job processing, with results cached to avoid repeated parsing of the same source files. This approach enables processing components to make informed decisions about optimal algorithms, quality settings, and output formats based on source file characteristics rather than relying on generic processing parameters.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `file_path` | `str` | Absolute path to the media file in storage |\n| `file_size` | `int` | File size in bytes for resource planning |\n| `mime_type` | `str` | MIME type determined by content inspection |\n| `format_name` | `str` | Human-readable format name (JPEG, PNG, MP4, etc.) |\n| `created_at` | `datetime` | File creation timestamp from filesystem |\n| `checksum` | `str` | SHA-256 hash for integrity verification |\n| `width` | `int` | Media width in pixels (images) or video resolution |\n| `height` | `int` | Media height in pixels (images) or video resolution |\n\n**Image metadata** extends the base media metadata with photography-specific information including color profiles, compression settings, and embedded metadata that affects processing quality and client compatibility.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `color_mode` | `str` | Color space representation (RGB, CMYK, Grayscale, Palette) |\n| `bit_depth` | `int` | Bits per channel for color precision (8, 16, 32) |\n| `compression` | `str` | Compression algorithm used in source file |\n| `quality` | `int` | Original quality setting for lossy formats (1-100) |\n| `has_transparency` | `bool` | Whether image contains alpha channel information |\n| `orientation` | `int` | EXIF orientation value for rotation correction |\n| `dpi` | `Tuple[int, int]` | Horizontal and vertical dots per inch |\n| `icc_profile` | `bytes` | Embedded ICC color profile data |\n| `exif_data` | `Dict[str, Any]` | Complete EXIF metadata dictionary |\n\n**Video metadata** captures motion picture characteristics including codecs, frame rates, and stream information necessary for transcoding decisions and quality preservation.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `duration` | `float` | Video length in seconds with millisecond precision |\n| `frame_rate` | `float` | Frames per second as decimal value |\n| `video_codec` | `str` | Video compression codec (H.264, H.265, VP9, etc.) |\n| `audio_codec` | `str` | Audio compression codec (AAC, MP3, Opus, etc.) |\n| `bitrate` | `int` | Average bitrate in bits per second |\n| `keyframe_interval` | `int` | GOP size in frames for seeking optimization |\n| `pixel_format` | `str` | Pixel format specification (yuv420p, rgb24, etc.) |\n| `aspect_ratio` | `str` | Display aspect ratio (16:9, 4:3, etc.) |\n| `audio_channels` | `int` | Number of audio channels (1=mono, 2=stereo, etc.) |\n| `audio_sample_rate` | `int` | Audio sample rate in Hz (44100, 48000, etc.) |\n| `subtitle_tracks` | `List[str]` | Available subtitle languages and formats |\n\n> **Decision: Embedded EXIF vs Separate EXIF Entity**\n> - **Context**: EXIF metadata can be extensive and includes nested structures like GPS coordinates, camera settings, and thumbnail images\n> - **Options Considered**:\n>   1. Store complete EXIF as JSON blob in image metadata\n>   2. Create separate EXIF entity with structured fields\n>   3. Extract only commonly-used EXIF fields into image metadata\n> - **Decision**: Store complete EXIF as JSON dictionary with extracted key fields in image metadata\n> - **Rationale**: Preserves complete EXIF data for applications that need it while providing fast access to commonly-used fields like orientation and GPS coordinates. JSON storage handles the nested and variable structure of EXIF data.\n> - **Consequences**: Enables flexible EXIF handling and preservation while maintaining query performance for common fields, but requires JSON parsing for detailed EXIF access\n\n### Processing Configuration Types\n\nProcessing configuration structures define the parameters and constraints that control media processing operations, enabling fine-tuned control over quality, performance, and output characteristics. These configurations must balance processing quality with resource consumption while providing sensible defaults for common use cases.\n\n**Processing configurations** serve as the bridge between high-level client requirements (resize to thumbnail, transcode for web playback) and low-level processing parameters (interpolation algorithms, codec settings, bitrate targets). The configuration system enables both simple preset-based processing and detailed parameter control for advanced use cases.\n\nConfiguration structures follow a hierarchical pattern where general processing parameters are inherited by specific operation types, allowing shared settings like quality levels and resource limits while enabling specialized parameters for image resizing, video transcoding, and thumbnail generation operations.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `output_path` | `str` | Target file path for processed output |\n| `format` | `str` | Output format specification (JPEG, PNG, WebP, MP4, WebM) |\n| `quality` | `int` | Quality level from 1-100 for lossy compression formats |\n| `overwrite_existing` | `bool` | Whether to replace existing files at output path |\n| `preserve_metadata` | `bool` | Whether to copy source metadata to output file |\n| `strip_private_data` | `bool` | Whether to remove GPS and personal information from metadata |\n\n**Image processing configurations** specify parameters for resize operations, format conversions, and optimization techniques that balance output quality with file size and processing performance.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `target_width` | `int` | Desired output width in pixels, null to preserve aspect ratio |\n| `target_height` | `int` | Desired output height in pixels, null to preserve aspect ratio |\n| `resize_mode` | `str` | Scaling behavior: 'fit', 'fill', 'crop', 'stretch' |\n| `interpolation` | `str` | Resampling algorithm: 'nearest', 'bilinear', 'bicubic', 'lanczos' |\n| `crop_position` | `str` | Crop anchor point: 'center', 'top', 'bottom', 'left', 'right' |\n| `background_color` | `str` | Fill color for letterboxing in hex format (#FFFFFF) |\n| `sharpen_amount` | `float` | Post-resize sharpening intensity (0.0-2.0) |\n| `optimize_for_web` | `bool` | Apply web-specific optimizations (progressive JPEG, etc.) |\n| `max_colors` | `int` | Color palette size for PNG optimization |\n\n**Video transcoding configurations** control codec selection, quality settings, and streaming optimizations for different playback scenarios and device capabilities.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `video_codec` | `str` | Output video codec: 'h264', 'h265', 'vp9', 'av1' |\n| `audio_codec` | `str` | Output audio codec: 'aac', 'mp3', 'opus', 'vorbis' |\n| `target_bitrate` | `int` | Average bitrate in kbps, null for CRF encoding |\n| `crf_value` | `int` | Constant Rate Factor for quality-based encoding (18-28) |\n| `frame_rate` | `float` | Output frame rate, null to preserve source frame rate |\n| `keyframe_interval` | `int` | GOP size in seconds for streaming optimization |\n| `preset` | `str` | Encoding speed preset: 'ultrafast', 'fast', 'medium', 'slow' |\n| `profile` | `str` | Codec profile: 'baseline', 'main', 'high' for H.264 compatibility |\n| `level` | `str` | Codec level constraint for device compatibility |\n| `two_pass_encoding` | `bool` | Use two-pass encoding for optimal bitrate distribution |\n\n**Adaptive bitrate configurations** define multiple quality variants for streaming applications, enabling clients to select appropriate quality based on network conditions and device capabilities.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `variant_name` | `str` | Human-readable quality identifier ('720p', '1080p', etc.) |\n| `width` | `int` | Video width for this quality variant |\n| `height` | `int` | Video height for this quality variant |\n| `bitrate` | `int` | Target bitrate in kbps for this variant |\n| `max_bitrate` | `int` | Maximum bitrate ceiling in kbps |\n| `buffer_size` | `int` | Rate control buffer size in kbits |\n| `audio_bitrate` | `int` | Audio bitrate in kbps for this variant |\n| `segment_duration` | `int` | HLS/DASH segment length in seconds |\n\nThe configuration system uses a preset-based approach for common scenarios while allowing parameter overrides for specialized requirements. This design enables simple integration for typical use cases while maintaining flexibility for advanced applications.\n\n| Preset Name | Use Case | Default Parameters | Typical Output Size |\n|-------------|----------|-------------------|-------------------|\n| `thumbnail` | Small preview images | 150x150 crop, JPEG quality 80 | 5-15 KB |\n| `web_optimized` | Website display images | Max 1920px, WebP quality 85 | 50-200 KB |\n| `mobile_video` | Mobile streaming | 720p H.264, 1.5 Mbps | 10-20% of source |\n| `web_video` | Desktop streaming | 1080p H.264, 3 Mbps | 20-30% of source |\n| `archive_quality` | Long-term storage | Lossless or very high quality | 80-95% of source |\n\n> **Decision: Preset-Based vs Parameter-Based Configuration**\n> - **Context**: Clients need both simple configuration for common cases and detailed control for specialized requirements\n> - **Options Considered**:\n>   1. Pure preset system with fixed parameters\n>   2. Pure parameter system requiring all settings\n>   3. Hybrid system with presets and parameter overrides\n> - **Decision**: Hybrid system with presets as starting points and parameter overrides\n> - **Rationale**: Provides ease of use for common scenarios while maintaining flexibility for advanced requirements. Presets encode best practices and tested parameter combinations.\n> - **Consequences**: Enables rapid integration and consistent results for typical use cases while supporting advanced customization, but requires careful preset design and parameter validation\n\n⚠️ **Pitfall: Quality Parameter Confusion**\n\nA common mistake is assuming that quality parameters work the same way across all formats and codecs. JPEG quality of 85 produces very different file sizes and visual quality compared to WebP quality 85 or H.264 CRF 23. Each format has its own quality scale, compression characteristics, and optimal parameter ranges.\n\n**Why this is wrong**: Using the same quality value across different formats can result in unnecessarily large files (over-encoding) or poor visual quality (under-encoding). For example, WebP typically achieves the same visual quality as JPEG at 15-20% lower quality settings.\n\n**How to fix it**: Use format-specific quality mapping in your configuration system. Define quality levels semantically ('low', 'medium', 'high', 'maximum') and map them to appropriate numerical values for each format. Provide format-specific presets that encode best practices for each codec and container combination.\n\n⚠️ **Pitfall: Missing Resource Constraints**\n\nProcessing configurations often omit resource limits, leading to worker processes consuming excessive memory or CPU time when processing large files or using computationally expensive settings like very slow encoding presets or high-quality interpolation algorithms.\n\n**Why this is wrong**: Without resource constraints, a single large video transcoding job can consume all available system memory, causing the worker process to crash or the entire system to become unresponsive. High-quality processing settings can extend job execution time from minutes to hours.\n\n**How to fix it**: Include resource constraint fields in your configuration types: `max_memory_mb`, `max_processing_time_seconds`, `cpu_threads`. Implement resource monitoring in your worker processes to enforce these limits and fail jobs gracefully when constraints are exceeded.\n\n### Implementation Guidance\n\nThe data model implementation focuses on creating clean, serializable data structures that support both SQL database storage and Redis job queue operations. Python's dataclasses provide an excellent foundation for these structures while maintaining compatibility with JSON serialization for API communication and queue message formats.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|----------------|\n| Data Validation | Python dataclasses with type hints | Pydantic models with validation |\n| Serialization | Built-in `json` module | `msgpack` for binary efficiency |\n| Configuration Loading | YAML with `pyyaml` | TOML with `tomli` for type safety |\n| Enum Management | Python `enum.Enum` | `enum.IntEnum` for database compatibility |\n\n**Recommended File Structure:**\n\n```\nmedia_processing/\n  models/\n    __init__.py                 ← export all public types\n    base.py                     ← base classes and common fields\n    jobs.py                     ← ProcessingJob and related types  \n    media.py                    ← media metadata structures\n    config.py                   ← processing configuration types\n    enums.py                    ← status, priority, format enums\n  serializers/\n    __init__.py\n    json_serializer.py          ← JSON serialization helpers\n    queue_serializer.py         ← queue message format handling\n```\n\n**Base Data Structure Infrastructure:**\n\n```python\n# models/base.py\nfrom dataclasses import dataclass, field, asdict\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional\nimport json\nimport uuid\n\n@dataclass\nclass BaseModel:\n    \"\"\"Base class for all data models with common serialization support.\"\"\"\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert model to dictionary with datetime serialization.\"\"\"\n        # TODO 1: Use dataclasses.asdict to get base dictionary\n        # TODO 2: Iterate through values and convert datetime objects to ISO strings\n        # TODO 3: Convert enums to their string/int values\n        # TODO 4: Handle None values appropriately\n        # TODO 5: Return clean dictionary ready for JSON serialization\n        pass\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]):\n        \"\"\"Create model instance from dictionary with type conversion.\"\"\"\n        # TODO 1: Get class field annotations for type information\n        # TODO 2: Parse datetime strings back to datetime objects\n        # TODO 3: Convert enum string/int values back to enum instances\n        # TODO 4: Handle optional fields with None defaults\n        # TODO 5: Create and return class instance with converted values\n        pass\n    \n    def to_json(self) -> str:\n        \"\"\"Serialize model to JSON string.\"\"\"\n        return json.dumps(self.to_dict(), separators=(',', ':'))\n    \n    @classmethod\n    def from_json(cls, json_str: str):\n        \"\"\"Deserialize model from JSON string.\"\"\"\n        return cls.from_dict(json.loads(json_str))\n\ndef generate_job_id() -> str:\n    \"\"\"Generate unique job identifier with timestamp prefix for sorting.\"\"\"\n    timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n    unique_id = str(uuid.uuid4())[:8]\n    return f\"{timestamp}_{unique_id}\"\n```\n\n**Enumeration Definitions:**\n\n```python\n# models/enums.py\nfrom enum import Enum, IntEnum\n\nclass JobStatus(Enum):\n    \"\"\"Job lifecycle status enumeration.\"\"\"\n    PENDING = \"pending\"\n    PROCESSING = \"processing\" \n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    \n    def is_terminal(self) -> bool:\n        \"\"\"Check if status represents a finished job.\"\"\"\n        return self in (JobStatus.COMPLETED, JobStatus.FAILED)\n    \n    def can_transition_to(self, new_status: 'JobStatus') -> bool:\n        \"\"\"Validate state transition rules.\"\"\"\n        # TODO 1: Define valid transitions from current status\n        # TODO 2: Return True only for legal state changes\n        # TODO 3: Prevent transitions from terminal states\n        pass\n\nclass JobPriority(IntEnum):\n    \"\"\"Job priority levels with numeric values for queue ordering.\"\"\"\n    LOW = 1\n    NORMAL = 5\n    HIGH = 10\n    URGENT = 20\n    \n    @classmethod\n    def from_string(cls, priority_str: str) -> 'JobPriority':\n        \"\"\"Parse priority from string representation.\"\"\"\n        # TODO 1: Handle case-insensitive string matching\n        # TODO 2: Return appropriate JobPriority enum value\n        # TODO 3: Raise ValueError for invalid priority strings\n        pass\n\nclass MediaFormat(Enum):\n    \"\"\"Supported media format enumeration.\"\"\"\n    JPEG = \"jpeg\"\n    PNG = \"png\" \n    WEBP = \"webp\"\n    AVIF = \"avif\"\n    MP4 = \"mp4\"\n    WEBM = \"webm\"\n    GIF = \"gif\"\n    \n    def is_image_format(self) -> bool:\n        \"\"\"Check if format is for still images.\"\"\"\n        return self in (MediaFormat.JPEG, MediaFormat.PNG, \n                       MediaFormat.WEBP, MediaFormat.AVIF, MediaFormat.GIF)\n    \n    def is_video_format(self) -> bool:\n        \"\"\"Check if format is for video content.\"\"\"\n        return self in (MediaFormat.MP4, MediaFormat.WEBM)\n```\n\n**Core Job Entity Implementation:**\n\n```python\n# models/jobs.py\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import List, Optional\nfrom .base import BaseModel, generate_job_id\nfrom .enums import JobStatus, JobPriority\n\n@dataclass\nclass OutputSpecification(BaseModel):\n    \"\"\"Specification for a single output file from processing job.\"\"\"\n    output_path: str\n    format: str\n    width: Optional[int] = None\n    height: Optional[int] = None\n    quality: Optional[int] = None\n    # TODO: Add additional format-specific parameters as needed\n\n@dataclass \nclass ProcessingJob(BaseModel):\n    \"\"\"Core job entity representing a media processing request.\"\"\"\n    job_id: str = field(default_factory=generate_job_id)\n    input_file_path: str = \"\"\n    output_specifications: List[OutputSpecification] = field(default_factory=list)\n    priority: JobPriority = JobPriority.NORMAL\n    status: JobStatus = JobStatus.PENDING\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error_message: Optional[str] = None\n    retry_count: int = 0\n    webhook_url: Optional[str] = None\n    progress_percentage: float = 0.0\n    estimated_duration: Optional[int] = None\n    \n    def mark_started(self) -> None:\n        \"\"\"Mark job as started and update status.\"\"\"\n        # TODO 1: Set started_at to current UTC time\n        # TODO 2: Change status to PROCESSING\n        # TODO 3: Validate that job was in PENDING status\n        pass\n    \n    def update_progress(self, percentage: float, stage: str = \"\") -> None:\n        \"\"\"Update job progress with validation.\"\"\"\n        # TODO 1: Validate percentage is between 0.0 and 100.0\n        # TODO 2: Ensure progress only increases (no going backwards)\n        # TODO 3: Update progress_percentage field\n        # TODO 4: Log progress update with stage information\n        pass\n    \n    def mark_completed(self, output_files: List[str]) -> None:\n        \"\"\"Mark job as successfully completed.\"\"\"\n        # TODO 1: Set completed_at to current UTC time\n        # TODO 2: Change status to COMPLETED  \n        # TODO 3: Set progress_percentage to 100.0\n        # TODO 4: Validate that output files exist and are readable\n        pass\n    \n    def mark_failed(self, error_msg: str, retryable: bool = True) -> None:\n        \"\"\"Mark job as failed with error details.\"\"\"\n        # TODO 1: Set error_message field\n        # TODO 2: Set completed_at to current UTC time\n        # TODO 3: Increment retry_count\n        # TODO 4: Set status to FAILED only if max retries exceeded\n        # TODO 5: Keep status as PENDING if retryable and under retry limit\n        pass\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the data model structures:\n\n1. **Run the data model tests:** `python -m pytest tests/models/ -v`\n2. **Expected output:** All serialization and validation tests should pass\n3. **Manual verification:** Create a `ProcessingJob` instance, serialize it to JSON, deserialize it back, and verify all fields match\n4. **Test job state transitions:** Verify that status transitions follow the defined state machine rules\n5. **Signs of problems:** \n   - Serialization errors indicate missing type conversion handling\n   - State transition failures suggest incomplete validation logic\n   - Datetime handling issues point to timezone or format problems\n\n**Language-Specific Python Tips:**\n\n- Use `dataclasses.field(default_factory=list)` for mutable default values to avoid shared state bugs\n- Implement `__post_init__` method for complex validation that requires multiple fields\n- Use `typing.Optional[T]` for nullable fields and handle None values in serialization\n- Consider `pydantic` models instead of dataclasses for production systems requiring extensive validation\n- Use `datetime.utcnow()` consistently for all timestamps to avoid timezone confusion\n- Implement custom JSON serialization for complex types like `datetime` and enums\n\n\n## Image Processing Component\n\n> **Milestone(s):** Milestone 1 (Image Processing) - this section covers the core image manipulation, format conversion, and metadata handling capabilities\n\n### Mental Model: Digital Darkroom\n\nThink of the image processing component as a sophisticated **digital darkroom** with automated equipment. In a traditional film darkroom, photographers would manipulate exposure, contrast, and cropping using enlargers, filters, and chemical baths. Our digital darkroom operates on the same principles but with pixel-perfect precision and unlimited repeatability.\n\nJust as a darkroom technician follows a recipe card specifying exposure time, paper grade, and chemical concentrations, our image processor follows an `ImageProcessingConfig` that specifies target dimensions, quality settings, and output formats. The \"enlarger\" is our resize algorithm that can scale images up or down with various interpolation methods. The \"cropping easel\" becomes our intelligent crop positioning that can center-crop, smart-crop, or preserve aspect ratios. The \"chemical baths\" for different paper types become our format converters that transform JPEG to WebP or PNG to AVIF while optimizing for web delivery.\n\nThe critical difference is that our digital darkroom can process hundreds of images simultaneously across multiple \"stations\" (worker processes) and automatically handle the complex chemistry of color spaces, bit depths, and compression algorithms that would require expert knowledge in the physical world. Each processing job represents a batch of prints from a single negative, where we might produce thumbnails (contact sheet), medium-resolution previews (work prints), and high-quality finals (exhibition prints) all from the same source image.\n\n### Core Image Operations\n\nThe image processing component implements four fundamental operations that mirror real-world image manipulation workflows but leverage computational precision for optimal results.\n\n**Image Loading and Format Detection**\n\nThe image loader serves as the component's entry point, responsible for reading diverse image formats into a standardized in-memory representation. This process involves more complexity than simply opening a file because images arrive with varying color spaces, bit depths, compression schemes, and metadata structures.\n\nThe loader first performs **format detection** by examining file headers rather than relying on file extensions, which can be misleading or absent. JPEG files begin with the hex signature `FFD8FF`, PNG files start with `89504E47`, and WebP files contain `52494646` followed by `57454250`. This signature-based detection prevents processing failures when files have incorrect extensions or no extensions at all.\n\nOnce the format is identified, the loader delegates to format-specific decoders that handle the intricacies of each format. JPEG decoding involves Huffman decompression and DCT (Discrete Cosine Transform) coefficient reconstruction. PNG decoding requires LZ77 decompression and filter row processing. WebP can contain either lossy VP8 or lossless VP8L data, requiring different decoder paths.\n\n| Operation | Input | Output | Critical Considerations |\n|-----------|-------|--------|------------------------|\n| Format Detection | Raw file bytes | Format enum + confidence | Must handle corrupted headers, ambiguous signatures |\n| JPEG Decoding | JPEG bitstream | RGB pixel array | EXIF orientation, CMYK→RGB conversion, progressive vs baseline |\n| PNG Decoding | PNG bitstream | RGBA pixel array | Transparency handling, gamma correction, color profile application |\n| WebP Decoding | WebP container | RGB/RGBA array | Lossy vs lossless detection, animation frame extraction |\n| GIF Decoding | GIF bitstream | RGB array + palette | First frame extraction, transparency index handling |\n\n**Resize Algorithms and Interpolation**\n\nImage resizing is mathematically equivalent to **resampling a discrete signal** in two dimensions. When scaling an image, we must calculate new pixel values at positions that don't correspond exactly to original pixel locations. The interpolation algorithm determines how we estimate these new values, directly impacting visual quality and processing time.\n\n**Nearest-neighbor interpolation** simply copies the value of the closest original pixel. This preserves sharp edges and is computationally fastest, making it suitable for pixel art or when preserving exact color values is critical. However, it produces jaggy, blocky results for photographic content.\n\n**Bilinear interpolation** computes each new pixel as a weighted average of the four nearest original pixels. The weights are based on distance, creating smooth gradients but potentially causing slight blurriness. This strikes a good balance for general-purpose resizing.\n\n**Bicubic interpolation** considers a 4×4 grid of surrounding pixels, using cubic polynomial curves to estimate new values. This produces sharper results than bilinear but requires significantly more computation. Bicubic is particularly effective for upscaling operations where detail preservation is paramount.\n\n**Lanczos resampling** applies a sophisticated windowed sinc function that minimizes aliasing artifacts while preserving edge sharpness. This is the gold standard for high-quality downscaling, especially when reducing images to small thumbnail sizes where detail loss would otherwise be severe.\n\n> **Key Insight**: The optimal interpolation algorithm depends on the scaling direction and ratio. Lanczos excels for downscaling by factors of 2× or more, while bicubic is preferred for modest upscaling. For real-time previews, bilinear provides acceptable quality with minimal latency.\n\n| Algorithm | Quality Score | Speed Score | Best Use Cases | Computational Complexity |\n|-----------|---------------|-------------|----------------|-------------------------|\n| Nearest-Neighbor | 2/10 | 10/10 | Pixel art, exact color preservation | O(1) per pixel |\n| Bilinear | 6/10 | 8/10 | General-purpose, real-time previews | O(4) per pixel |\n| Bicubic | 8/10 | 5/10 | High-quality upscaling, print preparation | O(16) per pixel |\n| Lanczos | 9/10 | 3/10 | Thumbnail generation, dramatic downscaling | O(64) per pixel |\n\n**Aspect Ratio Handling and Smart Cropping**\n\nWhen target dimensions don't match the source image's aspect ratio, the system must choose between **distortion, letterboxing, or cropping**. Each approach serves different use cases and requires careful implementation to avoid common pitfalls.\n\n**Preserve aspect ratio** scaling calculates the maximum scale factor that fits the image within target bounds without distortion. For a 1600×1200 source targeting 800×400, the limiting factor is height (1200→400 = 3×), so the result becomes 800×267 with potential letterboxing to reach exactly 800×400.\n\n**Center cropping** first scales the image so its smallest dimension matches the target, then crops from the center. This ensures the target dimensions are met exactly but may remove important content from image edges. The algorithm scales to fill the larger target dimension, then crops the excess from the perpendicular dimension.\n\n**Smart cropping** attempts to identify the most visually interesting region before cropping. Simple implementations use edge detection to find high-contrast areas. More sophisticated versions analyze face detection, rule-of-thirds positioning, or entropy maps to preserve the most important content.\n\n> **Decision: Smart Cropping Implementation**\n> - **Context**: Users upload images with varying compositions, and center cropping often removes important subjects\n> - **Options Considered**: Center crop only, edge-detection based smart crop, ML-based composition analysis\n> - **Decision**: Implement entropy-based smart cropping with center crop fallback\n> - **Rationale**: Entropy-based cropping identifies visually complex regions without requiring ML models, providing better results than center crop while remaining computationally efficient\n> - **Consequences**: Enables better automatic thumbnails but requires additional processing time; some images may still benefit from manual crop positioning\n\n**Format Conversion and Quality Optimization**\n\nFormat conversion involves more than changing file extensions—it requires understanding the capabilities and limitations of each target format, then optimizing encoding parameters for the intended use case.\n\n**JPEG optimization** centers on the quality parameter (1-100) which controls quantization table scaling. Quality 85-95 provides excellent visual results for most photographic content while achieving significant compression. The `optimize` flag enables Huffman table optimization, typically reducing file size by 5-10% with minimal processing overhead.\n\n**PNG optimization** focuses on compression level (0-9) and filter selection. PNG uses DEFLATE compression after applying prediction filters to reduce entropy. Filter selection can be automatic (Pillow chooses per-row) or fixed. For images with large solid areas, higher compression levels (7-9) provide substantial size reduction. For images with high-frequency detail, the computational cost of level 9 may not justify the modest size improvement.\n\n**WebP encoding** offers both lossy and lossless modes. Lossy WebP typically achieves 25-30% smaller file sizes than equivalent-quality JPEG while supporting alpha transparency. The quality parameter (0-100) behaves similarly to JPEG. Lossless WebP compresses better than PNG for photographic content but worse for simple graphics with few colors.\n\n| Format | Compression Type | Alpha Support | Animation Support | Browser Support | Typical Use Cases |\n|--------|-----------------|---------------|-------------------|-----------------|-------------------|\n| JPEG | Lossy only | No | No | Universal | Photographs, complex images |\n| PNG | Lossless only | Yes | No | Universal | Graphics, logos, screenshots |\n| WebP | Lossy + Lossless | Yes | Yes | Modern browsers | Web-optimized photos and graphics |\n| AVIF | Lossy + Lossless | Yes | Yes | Very modern | Next-gen web images, highest compression |\n\n### EXIF and Metadata Management\n\nImage metadata encompasses technical camera settings, geolocation data, copyright information, and processing history. Proper metadata handling is crucial for legal compliance, user privacy, and image display accuracy.\n\n**EXIF Data Structure and Extraction**\n\nEXIF (Exchangeable Image File Format) data is embedded within JPEG and TIFF files as a series of **IFD (Image File Directory) entries**. Each entry contains a tag ID, data type, count, and value or value offset. The structure is hierarchical, with primary IFD containing basic image information and sub-IFDs containing specialized data like GPS coordinates or camera-specific settings.\n\nThe most critical EXIF tag for image processing is **Orientation** (tag 0x0112), which indicates how the camera was rotated when the photo was taken. Digital cameras often save images in their sensor's native orientation, then record the intended viewing orientation in EXIF. Failing to handle this correctly results in sideways or upside-down images after processing.\n\n| EXIF Tag | Tag ID | Data Type | Critical for Processing | Privacy Implications |\n|----------|--------|-----------|------------------------|---------------------|\n| Orientation | 0x0112 | SHORT | Yes - affects display rotation | None |\n| DateTime | 0x0132 | ASCII | No - metadata only | Medium - reveals when photo taken |\n| GPS Latitude | 0x0002 | RATIONAL | No - metadata only | High - reveals location |\n| GPS Longitude | 0x0004 | RATIONAL | No - metadata only | High - reveals location |\n| Camera Make | 0x010F | ASCII | No - metadata only | Low - device information |\n| Camera Model | 0x0110 | ASCII | No - metadata only | Low - device information |\n| Software | 0x0131 | ASCII | No - metadata only | Low - processing software |\n\n**Orientation Handling and Rotation**\n\nEXIF orientation values range from 1-8, representing different combinations of rotation and mirroring. The standard defines these transformations relative to the top-left corner of the image as stored in the file.\n\n| Orientation Value | Transformation Required | Description |\n|-------------------|------------------------|-------------|\n| 1 | None | Normal (top-left) |\n| 2 | Horizontal flip | Mirrored |\n| 3 | 180° rotation | Upside down |\n| 4 | Vertical flip | Mirrored upside down |\n| 5 | 90° CCW + horizontal flip | Mirrored + rotated left |\n| 6 | 90° CW | Rotated right |\n| 7 | 90° CW + horizontal flip | Mirrored + rotated right |\n| 8 | 90° CCW | Rotated left |\n\nThe critical processing step is applying the orientation transformation **before** any resize or crop operations. This ensures that user-specified dimensions refer to the correctly oriented image. After applying the transformation, the orientation tag should be reset to 1 (normal) or removed entirely to prevent downstream applications from re-applying the correction.\n\n**Privacy and Metadata Stripping**\n\nFor web-facing applications, metadata stripping is essential for user privacy protection. GPS coordinates can reveal home addresses, timestamps can establish presence at specific locations, and camera serial numbers can link images to specific devices across platforms.\n\nThe system implements **configurable metadata preservation** based on output usage. Internal processing might preserve technical metadata for quality analysis, while public-facing outputs strip all potentially sensitive information.\n\n> **Decision: Selective Metadata Preservation**\n> - **Context**: Different use cases require different metadata handling—internal analysis needs technical data while public sharing requires privacy protection\n> - **Options Considered**: Strip all metadata, preserve all metadata, configurable per-output-type\n> - **Decision**: Implement three metadata modes: preserve_all, preserve_technical, strip_all\n> - **Rationale**: Provides flexibility for different use cases while defaulting to privacy-safe stripping for web outputs\n> - **Consequences**: Enables compliance with privacy regulations but requires careful configuration management to prevent accidental data exposure\n\n| Metadata Category | preserve_all | preserve_technical | strip_all |\n|-------------------|--------------|-------------------|-----------|\n| Camera settings (ISO, aperture, etc.) | ✓ | ✓ | ✗ |\n| GPS coordinates | ✓ | ✗ | ✗ |\n| Timestamps | ✓ | ✗ | ✗ |\n| Device serial numbers | ✓ | ✗ | ✗ |\n| Color profile (ICC) | ✓ | ✓ | ✗ |\n| Processing software | ✓ | ✓ | ✗ |\n\n### Image Processing Architecture Decisions\n\nThe image processing component's architecture must balance performance, quality, memory usage, and format compatibility while remaining maintainable and extensible.\n\n> **Decision: Pillow vs OpenCV vs ImageIO**\n> - **Context**: Python offers multiple image processing libraries with different performance characteristics and feature sets\n> - **Options Considered**: Pillow (PIL fork), OpenCV, ImageIO + scikit-image\n> - **Decision**: Use Pillow as primary library with ImageIO for AVIF support\n> - **Rationale**: Pillow provides excellent format support, straightforward API, and robust EXIF handling. OpenCV is overkill for basic operations and has complex deployment requirements. ImageIO fills format gaps.\n> - **Consequences**: Enables rapid development and broad format support but may require performance optimization for high-throughput scenarios\n\n**Memory Management Strategy**\n\nLarge images can quickly exhaust available memory, especially when processing multiple images concurrently. A 24-megapixel image (6000×4000 pixels) requires approximately 72MB of RAM when loaded as uncompressed RGB data (6000 × 4000 × 3 bytes). Processing operations often require additional temporary buffers, potentially doubling or tripling memory usage.\n\nThe system implements **streaming processing** for operations that support it, processing image tiles rather than loading entire images into memory. For operations requiring global image access (like smart cropping), the system enforces **memory limits per worker process** and queues large images for processing by workers with sufficient available memory.\n\n| Image Size | Uncompressed RGB | Peak Processing Memory | Recommended Worker Limit |\n|------------|------------------|----------------------|-------------------------|\n| 1MP (1024×1024) | 3MB | 9MB | 16 workers per 1GB RAM |\n| 6MP (3000×2000) | 18MB | 54MB | 4 workers per 1GB RAM |\n| 24MP (6000×4000) | 72MB | 216MB | 1 worker per 1GB RAM |\n| 50MP+ (8000×6000) | 144MB+ | 432MB+ | Queue for dedicated high-memory workers |\n\n> **Decision: Tile-Based vs Full-Image Processing**\n> - **Context**: Large images can exhaust worker process memory and impact overall system stability\n> - **Options Considered**: Always load full image, tile-based streaming, hybrid approach based on image size\n> - **Decision**: Implement hybrid approach—tile-based for resize/conversion, full-image for smart cropping\n> - **Rationale**: Resize and format conversion can be done on tiles, reducing memory usage. Smart cropping requires global image analysis.\n> - **Consequences**: Reduces memory footprint for large images but adds complexity for operations requiring global analysis\n\n**Color Space and Profile Management**\n\nDigital images can exist in various color spaces (RGB, CMYK, LAB, etc.) and may include ICC color profiles that define how colors should be interpreted for display or printing. Naive processing can result in significant color shifts or incorrect color reproduction.\n\nThe system standardizes on **sRGB as the working color space** for web delivery, as it's supported universally by browsers and provides predictable color reproduction on most displays. Images with embedded ICC profiles undergo color space conversion during loading to ensure consistent color handling.\n\nCMYK images (common in print workflows) require special handling because they cannot be directly displayed on RGB monitors. The conversion process involves ICC profile-based transformation or, when profiles are unavailable, approximation using standard CMYK-to-RGB conversion matrices.\n\n**Thread Safety and Concurrent Processing**\n\nPillow's underlying image objects are **not thread-safe**, meaning the same image instance cannot be safely accessed from multiple threads simultaneously. However, separate image instances can be processed concurrently without issues.\n\nThe system architecture ensures thread safety by creating **separate image instances per processing task** and never sharing image objects between worker threads. Each processing job operates on its own memory space and file handles, eliminating race conditions and allowing full parallel processing.\n\n### Common Image Processing Pitfalls\n\nUnderstanding and avoiding these common mistakes is crucial for building a robust image processing system that handles real-world image diversity.\n\n⚠️ **Pitfall: Ignoring EXIF Orientation**\n\nMany developers load images and immediately begin processing without checking EXIF orientation data. This results in images that appear correctly in photo viewers (which honor EXIF orientation) but display incorrectly after processing (when EXIF orientation is lost or ignored).\n\n**Why it's wrong**: Modern smartphones and digital cameras often store images in their sensor's native orientation (landscape) and record the intended viewing orientation in EXIF metadata. Processing the raw pixel data without applying the orientation transformation produces sideways or upside-down results.\n\n**How to fix**: Always read and apply EXIF orientation before any other processing operations. Use Pillow's `ImageOps.exif_transpose()` function which automatically handles all 8 orientation cases. After applying the transformation, either remove the orientation EXIF tag or set it to 1 (normal orientation).\n\n```python\n# Correct approach - handle orientation first\nimage = Image.open(input_path)\nimage = ImageOps.exif_transpose(image)  # Apply EXIF rotation\n# Now proceed with resize, crop, format conversion\n```\n\n⚠️ **Pitfall: Inappropriate Interpolation Algorithm Selection**\n\nUsing the same interpolation algorithm for all resize operations leads to suboptimal quality. Many developers default to bilinear interpolation for everything, missing opportunities for better quality or faster processing.\n\n**Why it's wrong**: Different scaling factors and image types benefit from different algorithms. Nearest-neighbor preserves sharp pixel boundaries for sprites and pixel art. Lanczos minimizes aliasing for dramatic downscaling. Bicubic provides better upscaling quality than bilinear.\n\n**How to fix**: Implement algorithm selection logic based on scaling factor and image characteristics. Use Lanczos for downscaling by 2× or more, bicubic for upscaling, and bilinear for real-time previews or modest scaling.\n\n⚠️ **Pitfall: Memory Exhaustion with Large Images**\n\nLoading very large images (50+ megapixels) into memory without size checks can crash worker processes or exhaust system memory, affecting other concurrent operations.\n\n**Why it's wrong**: A 100-megapixel image requires 300MB just to store the RGB pixel data, before any processing operations that might require additional temporary buffers. Multiple workers processing large images simultaneously can quickly exhaust available RAM.\n\n**How to fix**: Implement image dimension checks before loading. For images exceeding memory thresholds, either reject them with appropriate error messages, queue them for specialized high-memory workers, or implement tile-based processing for supported operations.\n\n⚠️ **Pitfall: Ignoring Color Space Conversion**\n\nProcessing CMYK images as if they were RGB, or failing to handle embedded ICC color profiles, results in dramatic color shifts and unprofessional output quality.\n\n**Why it's wrong**: CMYK uses subtractive color mixing (like printing inks) while RGB uses additive color mixing (like display pixels). Direct interpretation of CMYK values as RGB produces inverted, muddy colors. Similarly, images with wide-gamut color profiles may appear oversaturated or color-shifted when viewed in sRGB.\n\n**How to fix**: Always check the image's color mode and convert to RGB using proper color management. Use ICC profile-based conversion when profiles are available, or standard conversion matrices as fallback. Pillow's `Image.convert('RGB')` handles basic cases, but professional workflows may require more sophisticated color management.\n\n⚠️ **Pitfall: Lossy Recompression Cascades**\n\nRepeatedly loading and saving JPEG images during multi-step processing operations accumulates compression artifacts and degrades image quality with each iteration.\n\n**Why it's wrong**: JPEG uses lossy compression, meaning some image information is permanently discarded during each save operation. Processing workflows that load JPEG → process → save JPEG → load JPEG → process → save JPEG accumulate generational loss, similar to making photocopies of photocopies.\n\n**How to fix**: Maintain images in uncompressed format (PNG or in-memory bitmap) throughout multi-step processing pipelines. Only apply final compression when generating the ultimate output files. If intermediate files must be saved, use lossless PNG for temporary storage.\n\n⚠️ **Pitfall: Inconsistent Quality Parameter Interpretation**\n\nDifferent image formats interpret quality parameters differently, leading to inconsistent file sizes and visual quality when converting between formats.\n\n**Why it's wrong**: JPEG quality 85 and WebP quality 85 produce different visual quality levels and file sizes because they use different compression algorithms and quality scales. Users expecting consistent results across formats may be surprised by significant variations.\n\n**How to fix**: Implement format-specific quality mapping or target file size algorithms. Document quality parameter behavior clearly, or provide high-level quality settings (like \"web_optimized\", \"high_quality\", \"maximum_compression\") that map to appropriate format-specific parameters.\n\n### Implementation Guidance\n\n#### Technology Recommendations Table\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Image Library | Pillow (PIL) with basic operations | Pillow + ImageIO + pyheif for extended format support |\n| EXIF Handling | Pillow's built-in EXIF support | ExifRead library for detailed metadata parsing |\n| Color Management | Pillow's basic color conversion | colour-science library for professional color workflows |\n| Performance Optimization | Sequential processing with memory limits | Wand (ImageMagick binding) for high-performance operations |\n| Format Support | JPEG, PNG, WebP via Pillow | Add AVIF, HEIF via ImageIO plugins |\n\n#### Recommended File/Module Structure\n\n```\nmedia_processing/\n  image_processing/\n    __init__.py                    ← component exports\n    core/\n      processor.py                 ← ImageProcessor main class\n      operations.py                ← resize, crop, convert functions\n      metadata.py                  ← EXIF handling and metadata management\n      formats.py                   ← format detection and loader registry\n    config/\n      processing_config.py         ← ImageProcessingConfig and related types\n      quality_profiles.py          ← predefined quality settings\n    utils/\n      color_space.py              ← color conversion utilities\n      interpolation.py            ← algorithm selection logic\n      memory.py                   ← memory management helpers\n    tests/\n      test_processor.py           ← main processor tests\n      test_operations.py          ← operation-specific tests\n      fixtures/                   ← test images in various formats\n        sample.jpg\n        sample.png\n        sample_with_exif.jpg\n```\n\n#### Infrastructure Starter Code\n\n**Format Detection and Image Loader** (complete implementation):\n\n```python\n# image_processing/core/formats.py\nimport io\nfrom enum import Enum\nfrom typing import Optional, Tuple, Dict, Any\nfrom PIL import Image, ExifTags\nfrom PIL.ExifTags import ORIENTATION\n\nclass SupportedFormat(Enum):\n    JPEG = \"JPEG\"\n    PNG = \"PNG\" \n    WEBP = \"WEBP\"\n    GIF = \"GIF\"\n    AVIF = \"AVIF\"\n\nclass FormatDetector:\n    \"\"\"Detects image formats by file signature rather than extension.\"\"\"\n    \n    SIGNATURES = {\n        b'\\xff\\xd8\\xff': SupportedFormat.JPEG,\n        b'\\x89PNG\\r\\n\\x1a\\n': SupportedFormat.PNG,\n        b'RIFF': SupportedFormat.WEBP,  # Needs additional validation\n        b'GIF87a': SupportedFormat.GIF,\n        b'GIF89a': SupportedFormat.GIF,\n    }\n    \n    @classmethod\n    def detect_format(cls, file_path: str) -> Optional[SupportedFormat]:\n        \"\"\"Detect image format by reading file signature.\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                header = f.read(12)\n                \n            # Check standard signatures\n            for sig, fmt in cls.SIGNATURES.items():\n                if header.startswith(sig):\n                    if fmt == SupportedFormat.WEBP:\n                        # Verify WebP by checking for WEBP signature at offset 8\n                        return fmt if header[8:12] == b'WEBP' else None\n                    return fmt\n                    \n            # Check for AVIF (more complex signature)\n            if cls._is_avif(header):\n                return SupportedFormat.AVIF\n                \n            return None\n        except (IOError, IndexError):\n            return None\n    \n    @staticmethod\n    def _is_avif(header: bytes) -> bool:\n        \"\"\"Check for AVIF format signature.\"\"\"\n        return (len(header) >= 12 and \n                header[4:8] == b'ftyp' and \n                header[8:12] in [b'avif', b'avis'])\n\nclass ImageLoader:\n    \"\"\"Loads images from various formats into standardized PIL Image objects.\"\"\"\n    \n    def __init__(self):\n        self.detector = FormatDetector()\n    \n    def load_image(self, file_path: str) -> Tuple[Image.Image, Dict[str, Any]]:\n        \"\"\"Load image and extract metadata.\"\"\"\n        # Detect format\n        detected_format = self.detector.detect_format(file_path)\n        if not detected_format:\n            raise ValueError(f\"Unsupported image format: {file_path}\")\n        \n        # Load with PIL\n        image = Image.open(file_path)\n        \n        # Extract metadata\n        metadata = self._extract_metadata(image, detected_format)\n        \n        return image, metadata\n    \n    def _extract_metadata(self, image: Image.Image, fmt: SupportedFormat) -> Dict[str, Any]:\n        \"\"\"Extract comprehensive metadata from loaded image.\"\"\"\n        metadata = {\n            'format': fmt.value,\n            'size': image.size,\n            'mode': image.mode,\n            'has_transparency': self._has_transparency(image),\n        }\n        \n        # Extract EXIF if available\n        if hasattr(image, '_getexif') and image._getexif():\n            exif_dict = {}\n            exif = image._getexif()\n            for tag_id, value in exif.items():\n                tag = ExifTags.TAGS.get(tag_id, tag_id)\n                exif_dict[tag] = value\n            metadata['exif'] = exif_dict\n            \n            # Extract orientation specifically\n            if ORIENTATION in exif:\n                metadata['orientation'] = exif[ORIENTATION]\n        \n        return metadata\n    \n    @staticmethod\n    def _has_transparency(image: Image.Image) -> bool:\n        \"\"\"Check if image has transparency channel.\"\"\"\n        return (image.mode in ('RGBA', 'LA') or \n                'transparency' in image.info)\n```\n\n**Memory Management Helper** (complete implementation):\n\n```python\n# image_processing/utils/memory.py\nimport psutil\nimport os\nfrom typing import Tuple, Optional\nfrom PIL import Image\n\nclass MemoryManager:\n    \"\"\"Manages memory usage for image processing operations.\"\"\"\n    \n    def __init__(self, max_memory_mb: int = 512):\n        self.max_memory_mb = max_memory_mb\n        self.max_memory_bytes = max_memory_mb * 1024 * 1024\n    \n    def check_image_memory_requirements(self, width: int, height: int, \n                                      channels: int = 3) -> int:\n        \"\"\"Calculate memory required for image in bytes.\"\"\"\n        # Base memory for pixel data\n        pixel_memory = width * height * channels\n        \n        # Add overhead for processing (temporary buffers, etc.)\n        # Typical operations need 2-3x the base memory\n        total_memory = pixel_memory * 3\n        \n        return total_memory\n    \n    def can_process_image(self, image_path: str) -> Tuple[bool, Optional[str]]:\n        \"\"\"Check if image can be safely processed within memory limits.\"\"\"\n        try:\n            # Get image dimensions without fully loading\n            with Image.open(image_path) as img:\n                width, height = img.size\n                channels = len(img.getbands())\n            \n            required_memory = self.check_image_memory_requirements(\n                width, height, channels)\n            \n            if required_memory > self.max_memory_bytes:\n                return False, f\"Image requires {required_memory // 1024 // 1024}MB, limit is {self.max_memory_mb}MB\"\n            \n            # Check current system memory\n            available_memory = psutil.virtual_memory().available\n            if required_memory > available_memory * 0.8:  # Leave 20% buffer\n                return False, \"Insufficient system memory available\"\n            \n            return True, None\n            \n        except Exception as e:\n            return False, f\"Cannot analyze image: {str(e)}\"\n    \n    def get_optimal_tile_size(self, image_width: int, image_height: int, \n                             target_memory_mb: int = 64) -> Tuple[int, int]:\n        \"\"\"Calculate optimal tile size for streaming processing.\"\"\"\n        target_pixels = (target_memory_mb * 1024 * 1024) // (3 * 3)  # RGB + overhead\n        \n        # Try to maintain aspect ratio while staying under memory limit\n        aspect_ratio = image_width / image_height\n        \n        if target_pixels >= image_width * image_height:\n            return image_width, image_height  # Can process whole image\n        \n        # Calculate tile dimensions maintaining aspect ratio\n        tile_height = int((target_pixels / aspect_ratio) ** 0.5)\n        tile_width = int(tile_height * aspect_ratio)\n        \n        # Ensure minimum tile size for quality\n        tile_width = max(tile_width, 256)\n        tile_height = max(tile_height, 256)\n        \n        return tile_width, tile_height\n```\n\n#### Core Logic Skeleton Code\n\n**Main Image Processor** (signatures + TODOs):\n\n```python\n# image_processing/core/processor.py\nfrom typing import List, Dict, Any, Optional\nfrom PIL import Image, ImageOps\nfrom ..config.processing_config import ImageProcessingConfig\nfrom ..utils.memory import MemoryManager\n\nclass ImageProcessor:\n    \"\"\"Main image processing coordinator that orchestrates operations.\"\"\"\n    \n    def __init__(self, memory_limit_mb: int = 512):\n        self.memory_manager = MemoryManager(memory_limit_mb)\n        self.format_loader = ImageLoader()\n    \n    def process_image(self, input_path: str, output_specs: List[OutputSpecification]) -> List[str]:\n        \"\"\"\n        Process a single image according to multiple output specifications.\n        Returns list of generated output file paths.\n        \"\"\"\n        # TODO 1: Load and validate input image using ImageLoader\n        # TODO 2: Check memory requirements using MemoryManager\n        # TODO 3: Apply EXIF orientation correction using ImageOps.exif_transpose()\n        # TODO 4: For each output specification:\n        #   TODO 4a: Calculate resize parameters (target size, interpolation method)\n        #   TODO 4b: Apply resize operation with appropriate algorithm\n        #   TODO 4c: Apply cropping if aspect ratios don't match\n        #   TODO 4d: Convert to target format with quality settings\n        #   TODO 4e: Handle metadata according to privacy settings\n        #   TODO 4f: Save optimized output file\n        # TODO 5: Return list of successfully generated output paths\n        # Hint: Use try/except around each output spec to continue processing others on failure\n        pass\n    \n    def resize_image(self, image: Image.Image, target_width: int, target_height: int,\n                    interpolation: str = \"lanczos\", resize_mode: str = \"fit\") -> Image.Image:\n        \"\"\"\n        Resize image to target dimensions with specified algorithm and mode.\n        \n        Args:\n            image: Source PIL Image\n            target_width, target_height: Target dimensions\n            interpolation: Algorithm (\"nearest\", \"bilinear\", \"bicubic\", \"lanczos\")  \n            resize_mode: How to handle aspect ratio (\"fit\", \"fill\", \"crop\")\n        \"\"\"\n        # TODO 1: Select PIL resampling filter based on interpolation parameter\n        # TODO 2: Calculate scaling factors for width and height\n        # TODO 3: If resize_mode is \"fit\": scale to fit within bounds, preserve aspect ratio\n        # TODO 4: If resize_mode is \"fill\": scale to fill bounds exactly (may distort)\n        # TODO 5: If resize_mode is \"crop\": scale to fill bounds, crop excess\n        # TODO 6: Apply the resize operation using PIL Image.resize()\n        # TODO 7: Return resized image\n        # Hint: Use Image.Resampling.LANCZOS etc for PIL resampling constants\n        pass\n    \n    def convert_format(self, image: Image.Image, target_format: str, quality: int = 85,\n                      optimize: bool = True) -> bytes:\n        \"\"\"\n        Convert image to target format with specified quality settings.\n        Returns the converted image as bytes.\n        \"\"\"\n        # TODO 1: Validate target format against supported formats\n        # TODO 2: Handle color mode conversions (RGBA->RGB for JPEG, etc.)\n        # TODO 3: Set format-specific parameters (quality for JPEG/WebP, compression for PNG)\n        # TODO 4: Create BytesIO buffer for output\n        # TODO 5: Save image to buffer with format and parameters\n        # TODO 6: Return buffer contents as bytes\n        # Hint: JPEG doesn't support transparency, convert RGBA to RGB with background\n        pass\n    \n    def generate_thumbnail(self, image: Image.Image, size: int = 256, \n                          crop_mode: str = \"center\") -> Image.Image:\n        \"\"\"\n        Generate square thumbnail with smart cropping.\n        \n        Args:\n            image: Source PIL Image\n            size: Target thumbnail size (square)\n            crop_mode: Cropping strategy (\"center\", \"smart\", \"top\", \"bottom\")\n        \"\"\"\n        # TODO 1: Calculate current aspect ratio\n        # TODO 2: If crop_mode is \"center\": crop from center to square aspect ratio\n        # TODO 3: If crop_mode is \"smart\": use entropy-based cropping to find interesting region\n        # TODO 4: If crop_mode is \"top\"/\"bottom\": crop from specified edge\n        # TODO 5: Resize the cropped square image to target size using appropriate interpolation\n        # TODO 6: Apply sharpening filter if thumbnail is significantly smaller than source\n        # TODO 7: Return thumbnail image\n        # Hint: For smart cropping, analyze image entropy in sliding windows\n        pass\n\n    def extract_and_handle_metadata(self, image: Image.Image, \n                                   metadata_mode: str = \"strip_all\") -> Dict[str, Any]:\n        \"\"\"\n        Extract metadata and filter according to privacy settings.\n        \"\"\"\n        # TODO 1: Extract EXIF data using PIL image.getexif()\n        # TODO 2: Parse GPS coordinates if present\n        # TODO 3: Extract camera information (make, model, settings)\n        # TODO 4: Based on metadata_mode, filter what to preserve:\n        #   - \"preserve_all\": keep everything\n        #   - \"preserve_technical\": keep camera settings, strip GPS/timestamps\n        #   - \"strip_all\": remove all metadata\n        # TODO 5: Return filtered metadata dictionary\n        # Hint: Use ExifTags.TAGS to convert numeric tag IDs to readable names\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the image processing component, verify functionality with these tests:\n\n**Expected Behavior After Implementation:**\n1. **Format Detection**: Run `python -m image_processing.test_formats` - should correctly identify JPEG, PNG, WebP files by signature\n2. **EXIF Orientation**: Process a rotated smartphone photo - output should be correctly oriented even if input appears sideways\n3. **Memory Management**: Attempt to process a very large image (>100MB) - should either process successfully or fail gracefully with memory limit message\n4. **Quality Settings**: Convert the same image to JPEG at quality 50, 85, and 95 - file sizes should vary significantly with visible quality differences\n5. **Thumbnail Generation**: Generate thumbnails from landscape and portrait images - all should be properly cropped squares at specified size\n\n**Commands to Test:**\n```bash\n# Test basic functionality\npython -c \"\nfrom image_processing.core.processor import ImageProcessor\nprocessor = ImageProcessor()\noutputs = processor.process_image('test.jpg', [\n    OutputSpecification('thumb.jpg', 'JPEG', 256, 256, 85),\n    OutputSpecification('web.webp', 'WEBP', 1200, 800, 80)\n])\nprint(f'Generated: {outputs}')\n\"\n\n# Test memory limits\npython -c \"\nfrom image_processing.utils.memory import MemoryManager\nmem = MemoryManager(max_memory_mb=100)\ncan_process, reason = mem.can_process_image('large_image.jpg') \nprint(f'Can process: {can_process}, Reason: {reason}')\n\"\n```\n\n**Signs Something Is Wrong:**\n- Images appear rotated after processing → Check EXIF orientation handling\n- Process crashes with large images → Verify memory management implementation\n- Poor thumbnail quality → Check interpolation algorithm selection\n- Wrong colors in output → Verify color space conversion (CMYK→RGB)\n- Metadata leaking in outputs → Check metadata stripping implementation\n\n\n## Video Transcoding Component\n\n> **Milestone(s):** Milestone 2 (Video Transcoding) - this section covers video format conversion, adaptive bitrate encoding, thumbnail extraction, and FFmpeg integration\n\n### Mental Model: Film Studio Pipeline\n\nThink of the video transcoding component as a **modern film studio's post-production pipeline**. Just as a film studio receives raw footage from cameras and transforms it into multiple distribution formats for theaters, streaming services, and home video, our video transcoding component takes input video files and produces multiple optimized versions for different playback scenarios.\n\nIn a film studio, the workflow follows a structured pipeline. First, the **dailies** (raw footage) are ingested and cataloged with metadata about format, resolution, and quality. Then, the **color correction** and **audio sync** departments work their magic to optimize the content. Next, the **mastering** department creates different versions - a high-quality theatrical print, compressed versions for streaming, and lower-bitrate versions for mobile devices. Finally, the **quality control** department validates each output before distribution.\n\nOur video transcoding component mirrors this workflow exactly. The **FFmpeg integration layer** acts as our sophisticated film processing equipment - it can handle virtually any input format and codec, just like professional film equipment can work with different film stocks and digital formats. The **adaptive bitrate streaming** component functions as our distribution mastering department, creating multiple quality variants from a single source. The **progress tracking** system serves as our production coordinator, keeping stakeholders informed about which films are in what stage of processing.\n\nThis mental model is crucial because video transcoding involves the same core challenges as film production: **quality versus file size trade-offs**, **format compatibility** across different playback devices, **processing time management** for large files, and **workflow coordination** across multiple processing stages. Understanding this parallel helps developers appreciate why video transcoding requires sophisticated orchestration rather than simple format conversion.\n\n### FFmpeg Integration Layer\n\nThe **FFmpeg integration layer** serves as the foundational component that wraps the powerful FFmpeg command-line tools in a Python-friendly interface. FFmpeg is the industry-standard Swiss Army knife for multimedia processing, capable of handling hundreds of video codecs, container formats, and processing operations. However, integrating FFmpeg into a production system requires careful handling of process management, progress parsing, error interpretation, and resource monitoring.\n\nThe integration layer abstracts FFmpeg's complexity behind clean Python interfaces while preserving access to its full capabilities. This approach allows the media processing pipeline to leverage FFmpeg's battle-tested encoding algorithms and format support without requiring developers to become experts in FFmpeg's extensive command-line syntax.\n\n**Core FFmpeg Wrapper Architecture**\n\nThe wrapper architecture centers around a `FFmpegProcessor` class that manages FFmpeg subprocess execution with sophisticated error handling and progress monitoring. The processor maintains a **command builder pattern** that constructs FFmpeg commands from high-level configuration objects, ensuring type safety and preventing common command-line construction errors.\n\n| Component | Responsibility | Key Methods |\n|-----------|---------------|-------------|\n| `FFmpegProcessor` | Process lifecycle management | `execute_command`, `parse_progress`, `handle_errors` |\n| `CommandBuilder` | FFmpeg command construction | `build_transcode_command`, `build_thumbnail_command`, `add_codec_options` |\n| `ProgressParser` | Real-time progress extraction | `parse_ffmpeg_output`, `calculate_percentage`, `estimate_remaining_time` |\n| `ErrorHandler` | FFmpeg error interpretation | `classify_error_type`, `suggest_recovery_action`, `extract_error_details` |\n\nThe `FFmpegProcessor` implements a **non-blocking execution model** that spawns FFmpeg as a subprocess while continuously monitoring its stderr output for progress updates and error conditions. This design prevents long-running video transcoding operations from blocking the worker process and enables real-time progress reporting to end users.\n\n**Command Construction and Validation**\n\nThe command builder component translates high-level `VideoTranscodingConfig` objects into properly formatted FFmpeg command lines. This translation layer is critical because FFmpeg's command-line interface is extremely flexible but also prone to subtle errors that can result in failed transcoding jobs or suboptimal output quality.\n\n| FFmpeg Parameter Category | Configuration Fields | Validation Rules |\n|---------------------------|---------------------|------------------|\n| Input Handling | `input_file_path`, `input_format` | File existence, format detection, stream analysis |\n| Video Encoding | `video_codec`, `crf_value`, `preset`, `profile` | Codec compatibility, CRF range (0-51), preset validation |\n| Audio Processing | `audio_codec`, `audio_bitrate`, `audio_channels` | Channel count limits, bitrate ranges, codec support |\n| Output Options | `output_path`, `container_format`, `optimize_flags` | Path writability, format/codec compatibility |\n| Advanced Settings | `two_pass_encoding`, `keyframe_interval`, `pixel_format` | GOP structure validation, pixel format support |\n\nThe command builder implements **defensive validation** at multiple levels. First, it validates individual parameter values against known ranges and supported options. Then, it performs **cross-parameter validation** to ensure codec and container format compatibility. Finally, it adds **optimization flags** specific to the target use case, such as web streaming optimization or file size minimization.\n\n**Progress Parsing and Monitoring**\n\nFFmpeg outputs detailed progress information to stderr in a semi-structured format that requires careful parsing to extract meaningful progress metrics. The progress parser component implements a **state machine** that tracks FFmpeg's processing phases and translates raw output into standardized progress updates.\n\n| Processing Phase | FFmpeg Output Indicators | Progress Calculation Method |\n|-----------------|-------------------------|----------------------------|\n| Input Analysis | \"Input #0\", stream detection | 5% of total progress |\n| Video Processing | frame count, time position | (current_time / total_duration) * 0.85 |\n| Audio Processing | audio frame processing | Combined with video progress |\n| Output Finalization | muxing, index writing | Final 10% of progress |\n\nThe progress calculation uses **duration-based estimation** rather than frame-based counting because frame processing rates vary significantly based on video complexity, encoding settings, and system performance. The parser maintains a **rolling average** of processing speed to provide increasingly accurate time estimates as transcoding progresses.\n\n> **Key Insight**: FFmpeg progress parsing is inherently imprecise because encoding complexity varies throughout a video file. Scene changes, motion complexity, and audio dynamics all affect processing speed. The progress parser focuses on providing directionally accurate estimates rather than precise percentages.\n\n**Error Classification and Recovery**\n\nFFmpeg can fail for dozens of different reasons, from unsupported codecs to insufficient disk space to corrupted input files. The error handler component implements a **hierarchical error classification system** that categorizes failures by their root cause and suggests appropriate recovery strategies.\n\n| Error Category | Example Causes | Recovery Strategy | Retry Eligible |\n|---------------|---------------|------------------|----------------|\n| Input Errors | Corrupted file, unsupported format | Validate input, suggest format conversion | No |\n| Configuration Errors | Invalid codec combination, bad parameters | Adjust encoding settings, use fallback profile | Yes |\n| Resource Errors | Insufficient memory, disk full | Wait and retry, reduce quality settings | Yes |\n| System Errors | FFmpeg binary missing, permission denied | Check system configuration, adjust paths | No |\n\nThe error classification logic uses **pattern matching** against FFmpeg's stderr output combined with **exit code analysis** to determine the failure category. This dual approach is necessary because FFmpeg's error reporting is inconsistent - some errors produce specific exit codes while others require parsing text output for diagnostic information.\n\n> **Decision: Process Isolation Strategy**\n> - **Context**: FFmpeg processing can consume significant system resources and occasionally crash or hang, potentially affecting other jobs\n> - **Options Considered**: \n>   1. Run FFmpeg in same process with threading\n>   2. Spawn FFmpeg as subprocess with resource limits\n>   3. Use containerized FFmpeg execution\n> - **Decision**: Subprocess execution with resource monitoring and timeouts\n> - **Rationale**: Provides process isolation while maintaining simplicity. Resource limits prevent runaway processes, and subprocess cleanup ensures system stability\n> - **Consequences**: Enables robust error handling and resource management at the cost of slightly increased system overhead\n\n### Adaptive Bitrate Streaming\n\n**Adaptive Bitrate Streaming (ABR)** represents one of the most sophisticated aspects of modern video delivery. ABR enables video players to dynamically adjust quality based on network conditions, device capabilities, and user preferences by providing multiple quality variants of the same content. The video transcoding component must generate these variants efficiently while ensuring smooth transitions between quality levels during playback.\n\nThe ABR implementation centers around creating **quality ladders** - sets of video renditions at different resolutions and bitrates that cover the full spectrum from mobile networks to high-speed broadband. Each quality ladder must be carefully designed to provide meaningful quality improvements while avoiding wasteful bandwidth usage.\n\n**Quality Ladder Generation**\n\nThe quality ladder generation algorithm analyzes the source video's characteristics to determine optimal encoding targets. This analysis considers **source resolution**, **content complexity**, **motion characteristics**, and **target delivery scenarios** to create a customized quality ladder rather than applying a one-size-fits-all approach.\n\n| Resolution Tier | Typical Use Case | Bitrate Range | Audio Bitrate | Target Devices |\n|-----------------|-----------------|---------------|---------------|----------------|\n| 240p (426×240) | Very low bandwidth | 300-500 kbps | 64 kbps | Feature phones, poor connections |\n| 360p (640×360) | Mobile networks | 500-800 kbps | 96 kbps | Smartphones, tablet cellular |\n| 480p (854×480) | Standard definition | 800-1200 kbps | 128 kbps | Laptops, tablets, smart TVs |\n| 720p (1280×720) | High definition | 1500-3000 kbps | 128 kbps | Desktop, modern mobile devices |\n| 1080p (1920×1080) | Full HD | 3000-6000 kbps | 192 kbps | High-end devices, good connections |\n| 1440p (2560×1440) | 2K/QHD | 6000-12000 kbps | 256 kbps | Premium displays, excellent bandwidth |\n\nThe quality ladder algorithm implements **content-aware encoding** that adjusts bitrate targets based on video complexity analysis. Simple content like presentations or screencasts can achieve acceptable quality at lower bitrates, while complex content with rapid motion requires higher bitrates to maintain visual fidelity.\n\nThe encoding process follows a **parallel generation strategy** where multiple quality variants are produced simultaneously rather than sequentially. This approach reduces total processing time by leveraging multiple CPU cores and optimizing I/O patterns across the encoding pipeline.\n\n**HLS (HTTP Live Streaming) Implementation**\n\nHLS represents Apple's adaptive streaming standard that segments video content into small chunks (typically 6-10 seconds) and creates manifest files that describe available quality variants. The HLS implementation must generate properly formatted segments and manifests that ensure compatibility across a wide range of devices and players.\n\nThe HLS generation process involves several critical steps that must be executed with precise timing and format adherence:\n\n1. **Segment Duration Calculation**: Analyze source video keyframe intervals to determine optimal segment boundaries that align with GOP (Group of Pictures) structures\n2. **Multi-Variant Playlist Creation**: Generate the master manifest that lists all available quality variants with their technical specifications\n3. **Individual Stream Processing**: Create separate media playlists for each quality variant with segment references and timing information\n4. **Encryption Key Management**: Generate and rotate AES-128 encryption keys for content protection when required\n5. **Subtitle and Audio Track Handling**: Process additional streams for multi-language support and accessibility features\n\n| HLS Component | File Extension | Purpose | Update Frequency |\n|---------------|---------------|---------|------------------|\n| Master Playlist | `.m3u8` | Lists all variant streams | Static after creation |\n| Media Playlist | `.m3u8` | Segment list for specific quality | Dynamic for live streams |\n| Video Segments | `.ts` or `.m4s` | Actual video content chunks | Static after creation |\n| Key Files | `.key` | Encryption keys for protected content | Rotated periodically |\n\nThe HLS encoder implements **segment alignment** across quality variants to ensure smooth switching between streams. This alignment requires careful keyframe placement and GOP structure coordination across all encoding passes.\n\n**DASH (Dynamic Adaptive Streaming over HTTP) Support**\n\nDASH provides an alternative adaptive streaming standard that offers more flexibility than HLS through its XML-based manifest format and support for multiple container formats. The DASH implementation complements the HLS support to ensure broad client compatibility across different platforms and players.\n\nDASH manifest generation requires creating **Media Presentation Description (MPD)** files that describe the temporal structure of content, available representations, and addressing schemes for segment retrieval. The MPD format is more complex than HLS playlists but provides greater expressiveness for advanced streaming scenarios.\n\n| DASH Element | XML Structure | Content Description |\n|--------------|---------------|-------------------|\n| Period | `<Period>` | Temporal division of content |\n| AdaptationSet | `<AdaptationSet>` | Group of representations with same content |\n| Representation | `<Representation>` | Specific encoding variant |\n| SegmentTemplate | `<SegmentTemplate>` | URL pattern for segment addressing |\n\nThe DASH encoder supports both **SegmentTemplate** and **SegmentList** addressing modes, with template-based addressing preferred for its efficiency and scalability. Template addressing allows clients to construct segment URLs mathematically rather than downloading explicit segment lists.\n\n> **Decision: Dual ABR Format Support**\n> - **Context**: Different platforms and clients prefer different adaptive streaming formats (HLS vs DASH)\n> - **Options Considered**:\n>   1. HLS only (Apple/Safari focus)\n>   2. DASH only (open standard focus)\n>   3. Dual format support with shared segments\n> - **Decision**: Generate both HLS and DASH manifests from common segment files\n> - **Rationale**: Maximizes client compatibility while minimizing storage overhead by reusing segment files across formats\n> - **Consequences**: Increased processing complexity but broader device support and reduced storage costs\n\n### Video Transcoding Architecture Decisions\n\nThe video transcoding component requires numerous architectural decisions that significantly impact performance, quality, compatibility, and resource utilization. These decisions form the foundation for reliable, scalable video processing that meets diverse client requirements while maintaining operational efficiency.\n\n**Codec Selection Strategy**\n\nVideo codec selection represents one of the most impactful architectural decisions because it directly affects output quality, file sizes, encoding performance, and client compatibility. The transcoding component implements a **multi-codec strategy** that selects optimal codecs based on content characteristics, target platforms, and performance requirements.\n\n| Codec | Primary Use Case | Encoding Speed | Quality Efficiency | Browser Support | Hardware Acceleration |\n|-------|-----------------|----------------|-------------------|----------------|----------------------|\n| H.264 (AVC) | Universal compatibility | Fast | Good | Excellent (99%+) | Widely available |\n| H.265 (HEVC) | High efficiency, 4K content | Slow | Excellent | Limited (60%) | Growing support |\n| VP9 | Open source, web focus | Medium | Very Good | Good (80%) | Limited hardware support |\n| AV1 | Future standard, efficiency | Very Slow | Excellent | Growing (40%) | Emerging hardware support |\n\nThe codec selection algorithm implements **content analysis heuristics** that examine source video characteristics to determine the most appropriate encoding approach. High-resolution content with complex motion benefits from advanced codecs like H.265 or VP9, while simple content prioritizes H.264 for universal compatibility.\n\nThe system supports **fallback codec chains** where encoding attempts with advanced codecs automatically fall back to more compatible options if hardware acceleration is unavailable or encoding times exceed acceptable thresholds. This approach ensures job completion while optimizing for available resources.\n\n**Encoding Parameter Optimization**\n\nVideo encoding involves dozens of parameters that control the trade-offs between quality, file size, and processing time. The transcoding component implements **profile-based parameter management** that groups related settings into coherent configurations optimized for specific use cases.\n\n| Encoding Profile | Target Scenario | CRF Range | Preset | Special Flags |\n|------------------|----------------|-----------|---------|---------------|\n| Web Optimized | Streaming delivery | 20-24 | medium | `-movflags +faststart` |\n| High Quality | Archive/premium content | 16-20 | slow | `-tune film` |\n| Mobile Optimized | Cellular networks | 24-28 | fast | `-profile:v baseline` |\n| Low Latency | Live streaming | 22-26 | ultrafast | `-tune zerolatency` |\n\nThe **Constant Rate Factor (CRF)** approach provides superior quality control compared to target bitrate encoding because it maintains consistent perceptual quality across varying content complexity. The system automatically adjusts CRF values based on source resolution and target quality requirements.\n\n**Two-pass encoding** is selectively applied for scenarios where precise bitrate control is required, such as broadcast delivery or bandwidth-constrained environments. The first pass analyzes content complexity while the second pass optimizes bit allocation based on the analysis results.\n\n> **Decision: CRF-Based Quality Control**\n> - **Context**: Need consistent quality across diverse content while maintaining reasonable file sizes\n> - **Options Considered**:\n>   1. Target bitrate encoding (CBR/VBR)\n>   2. Constant Rate Factor (CRF) encoding\n>   3. Constrained Quality (CQ) encoding\n> - **Decision**: CRF as primary method with bitrate constraints for ABR\n> - **Rationale**: CRF provides consistent perceptual quality regardless of content complexity, resulting in better user experience and more predictable quality\n> - **Consequences**: File sizes vary based on content complexity, but quality remains consistent across all content types\n\n**Container Format and Streaming Optimization**\n\nContainer format selection affects compatibility, streaming performance, and metadata handling capabilities. The transcoding component supports multiple container formats with optimization flags tailored to specific delivery scenarios.\n\n| Container Format | Primary Use Case | Streaming Support | Metadata Capacity | Browser Compatibility |\n|------------------|-----------------|------------------|-------------------|---------------------|\n| MP4 | Web delivery, mobile | Excellent | Good | Universal |\n| WebM | Open web standards | Good | Limited | Modern browsers |\n| MKV | High-quality archival | Limited | Extensive | Desktop players |\n| HLS/TS | Adaptive streaming | Native | Limited | iOS, modern browsers |\n\nThe MP4 container receives **web streaming optimizations** including the `faststart` flag that relocates metadata to the file beginning, enabling progressive download and immediate playback initiation. This optimization is crucial for user experience in web-based video players.\n\n**Hardware Acceleration Integration**\n\nModern systems provide hardware-accelerated video encoding through GPU-based encoders like NVIDIA's NVENC, Intel's Quick Sync, and AMD's VCE. The transcoding component implements **intelligent hardware acceleration** that automatically detects available acceleration capabilities and falls back gracefully to software encoding when hardware acceleration is unavailable or insufficient.\n\n| Hardware Platform | Acceleration Technology | Supported Codecs | Performance Gain | Quality Trade-off |\n|-------------------|------------------------|-----------------|------------------|-------------------|\n| NVIDIA GPU | NVENC | H.264, H.265 | 5-10x faster | Slight quality reduction |\n| Intel CPU | Quick Sync | H.264, H.265, AV1 | 3-5x faster | Minimal quality impact |\n| AMD GPU | VCE/AMF | H.264, H.265 | 4-8x faster | Slight quality reduction |\n| Apple Silicon | VideoToolbox | H.264, H.265, ProRes | 6-12x faster | High quality maintained |\n\nThe hardware acceleration detection implements **capability probing** during system initialization to build a matrix of available acceleration options. This probe includes performance benchmarking to ensure hardware encoders provide meaningful speed improvements without unacceptable quality degradation.\n\nHardware encoding failures trigger **automatic fallback** to software encoding to ensure job completion. The system logs hardware failures for monitoring and analysis to identify patterns that might indicate driver issues or hardware problems.\n\n### Common Video Processing Pitfalls\n\nVideo processing introduces numerous opportunities for subtle bugs and performance issues that can severely impact system reliability and user experience. Understanding these common pitfalls helps developers build robust video transcoding systems that handle edge cases gracefully and maintain consistent performance under diverse conditions.\n\n⚠️ **Pitfall: Memory Exhaustion with High-Resolution Video**\n\nVideo processing requires substantial memory for frame buffering, especially when handling 4K or 8K content. A single uncompressed 4K frame requires approximately 33MB of memory (3840×2160×4 bytes per pixel), and FFmpeg typically buffers multiple frames simultaneously for encoding efficiency.\n\nThe primary mistake developers make is **not implementing memory monitoring** during video processing operations. Long-running transcoding jobs can gradually consume system memory until they trigger out-of-memory conditions that crash worker processes or destabilize the entire system.\n\n**Detection**: Monitor process memory usage during transcoding operations. Memory consumption that grows continuously rather than stabilizing indicates a potential memory leak or insufficient memory availability for the current job.\n\n**Prevention**: Implement **resource-aware job scheduling** that estimates memory requirements based on input resolution and rejects jobs that would exceed available system memory. Use FFmpeg's buffer size controls to limit memory usage during processing.\n\n```python\n# Memory estimation for job scheduling\ndef estimate_memory_requirements(video_metadata):\n    width = video_metadata.width\n    height = video_metadata.height\n    # Estimate memory for frame buffers (assuming 4 bytes per pixel, 8 frame buffer)\n    frame_memory = width * height * 4 * 8\n    # Add overhead for FFmpeg processing\n    total_memory = frame_memory * 1.5\n    return int(total_memory)\n```\n\n⚠️ **Pitfall: Inaccurate Progress Estimation**\n\nVideo transcoding progress is notoriously difficult to estimate accurately because encoding complexity varies dramatically throughout a video file. Scene changes, motion complexity, and content characteristics all affect processing speed in unpredictable ways.\n\nMany developers implement **linear progress estimation** based on processed time or frame count, leading to progress indicators that jump around unpredictably or stall for extended periods. This creates poor user experience and makes it difficult to detect genuinely stalled processing jobs.\n\n**Detection**: Progress updates that move backward, remain stalled for extended periods, or jump by large amounts indicate flawed progress calculation logic.\n\n**Solution**: Implement **stage-based progress reporting** that divides transcoding into distinct phases with predictable duration ranges. Use historical processing data to improve time estimates for similar content types.\n\n| Processing Stage | Typical Duration % | Progress Range | Key Indicators |\n|-----------------|-------------------|----------------|----------------|\n| Input Analysis | 2-5% | 0-5% | Stream detection, metadata extraction |\n| Video Transcoding | 80-90% | 5-90% | Frame processing, bitrate varies by content |\n| Audio Processing | 5-10% | 90-95% | Audio encoding, typically consistent speed |\n| Output Finalization | 2-5% | 95-100% | Container muxing, index generation |\n\n⚠️ **Pitfall: Codec Compatibility Issues**\n\nDifferent combinations of video codecs, audio codecs, and container formats create complex compatibility matrices that can result in files that encode successfully but fail to play on certain devices or browsers. This is particularly problematic for web delivery where broad compatibility is essential.\n\nThe most common mistake is **not validating codec/container combinations** before starting transcoding operations. Invalid combinations can waste significant processing time and produce unusable output files.\n\n**Detection**: Files that encode without errors but fail validation testing on target playback platforms indicate codec compatibility problems.\n\n**Prevention**: Implement **compatibility validation matrices** that verify codec and container combinations before job execution. Test output files on representative target platforms during development.\n\n| Target Platform | Recommended Video Codec | Audio Codec | Container | Profile Constraints |\n|------------------|------------------------|-------------|-----------|-------------------|\n| iOS Safari | H.264 | AAC | MP4 | Baseline/Main profile, Level 4.0 max |\n| Android Chrome | H.264/VP9 | AAC/Opus | MP4/WebM | High profile acceptable |\n| Desktop Browsers | H.264/VP9/AV1 | AAC/Opus | MP4/WebM | All profiles supported |\n| Smart TV | H.264 | AAC | MP4 | Main profile, conservative levels |\n\n⚠️ **Pitfall: Keyframe Alignment in ABR Streams**\n\nAdaptive bitrate streaming requires keyframes to be aligned across all quality variants to enable smooth switching between streams during playback. Misaligned keyframes cause visual artifacts, rebuffering, or playback failures when clients attempt to switch quality levels.\n\nMany implementations **generate ABR variants independently** without coordinating keyframe placement, resulting in keyframes that occur at different timestamps across quality levels. This breaks the fundamental requirement for seamless quality switching.\n\n**Detection**: ABR streams with playback stuttering or visual artifacts during quality changes indicate keyframe alignment problems. Analysis tools can detect timestamp mismatches in segment boundaries.\n\n**Solution**: Use **synchronized encoding** with fixed GOP (Group of Pictures) structure across all quality variants. Force keyframes at identical timestamps for all encoding passes.\n\n```python\n# Keyframe synchronization for ABR encoding\ndef generate_abr_variants(input_file, quality_configs):\n    # Calculate keyframe intervals based on segment duration\n    segment_duration = 6  # seconds\n    keyframe_interval = segment_duration * frame_rate\n    \n    common_flags = [\n        '-force_key_frames', f'expr:gte(t,n_forced*{segment_duration})',\n        '-g', str(keyframe_interval),\n        '-keyint_min', str(keyframe_interval),\n    ]\n    \n    # Apply common keyframe settings to all variants\n    for config in quality_configs:\n        config.encoding_flags.extend(common_flags)\n```\n\n⚠️ **Pitfall: Temporary File Management and Cleanup**\n\nVideo processing generates numerous temporary files including intermediate encode passes, segment files, and working directories. Poor temporary file management leads to disk space exhaustion, permission issues, and security vulnerabilities from abandoned sensitive content.\n\nThe most critical mistake is **not implementing comprehensive cleanup logic** that handles both successful completion and error scenarios. Temporary files can accumulate rapidly during high-volume processing, eventually exhausting available disk space.\n\n**Detection**: Disk usage that grows continuously over time, even after job completion, indicates temporary file cleanup problems. Monitor temporary directory sizes and file ages.\n\n**Prevention**: Implement **context managers** for temporary file handling that guarantee cleanup regardless of how processing completes. Use unique temporary directories for each job to prevent file conflicts.\n\n```python\nimport tempfile\nimport shutil\nfrom contextlib import contextmanager\n\n@contextmanager\ndef processing_workspace(job_id):\n    \"\"\"Create and cleanup temporary workspace for video processing\"\"\"\n    temp_dir = tempfile.mkdtemp(prefix=f\"video_job_{job_id}_\")\n    try:\n        yield temp_dir\n    finally:\n        # Cleanup regardless of success or failure\n        shutil.rmtree(temp_dir, ignore_errors=True)\n\n# Usage ensures cleanup\nwith processing_workspace(job.job_id) as workspace:\n    # All temporary files created in workspace\n    process_video(input_file, workspace)\n    # Automatic cleanup when context exits\n```\n\n![Video Transcoding Sequence](./diagrams/video-processing-sequence.svg)\n\n### Implementation Guidance\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|--------------|-----------------|\n| FFmpeg Integration | `subprocess` with `Popen` | `asyncio.subprocess` with process pools |\n| Progress Parsing | Regex pattern matching | State machine with tokenizer |\n| Error Handling | Exit code checking | ML-based error classification |\n| Temporary Files | `tempfile.mkdtemp()` | Memory-mapped temporary storage |\n| Resource Monitoring | `psutil` memory checking | cgroup-based resource limits |\n\n**Recommended Module Structure**\n\nThe video transcoding component integrates into the larger media processing pipeline through well-defined interfaces and module boundaries:\n\n```\nmedia_processing/\n  video/\n    __init__.py                 ← Public interface exports\n    transcoder.py              ← Main VideoTranscoder class\n    ffmpeg_wrapper.py          ← FFmpeg process management\n    progress_parser.py         ← Progress monitoring logic  \n    quality_ladder.py          ← ABR variant generation\n    codecs.py                  ← Codec compatibility matrices\n    containers.py              ← Container format handling\n    errors.py                  ← Video-specific error types\n  tests/\n    test_transcoder.py         ← Core transcoding tests\n    test_ffmpeg_wrapper.py     ← FFmpeg integration tests\n    fixtures/                  ← Sample video files for testing\n      sample_video.mp4\n      sample_audio.wav\n```\n\n**Complete FFmpeg Wrapper Implementation**\n\nThis wrapper provides production-ready FFmpeg integration that handles process management, progress parsing, and error recovery:\n\n```python\nimport subprocess\nimport re\nimport json\nimport logging\nfrom typing import Dict, List, Optional, Callable\nfrom pathlib import Path\nimport asyncio\nimport signal\nimport psutil\n\nclass FFmpegError(Exception):\n    \"\"\"FFmpeg processing error with detailed context\"\"\"\n    def __init__(self, message: str, exit_code: int = None, stderr: str = None):\n        super().__init__(message)\n        self.exit_code = exit_code\n        self.stderr = stderr\n\nclass FFmpegProgressParser:\n    \"\"\"Parses FFmpeg stderr output for progress information\"\"\"\n    \n    def __init__(self):\n        self.duration_pattern = re.compile(r'Duration: (\\d{2}):(\\d{2}):(\\d{2})\\.(\\d{2})')\n        self.progress_pattern = re.compile(r'time=(\\d{2}):(\\d{2}):(\\d{2})\\.(\\d{2})')\n        self.total_duration = 0.0\n    \n    def parse_duration(self, line: str) -> Optional[float]:\n        \"\"\"Extract total duration from FFmpeg output\"\"\"\n        match = self.duration_pattern.search(line)\n        if match:\n            hours, minutes, seconds, centiseconds = map(int, match.groups())\n            self.total_duration = hours * 3600 + minutes * 60 + seconds + centiseconds / 100\n            return self.total_duration\n        return None\n    \n    def parse_progress(self, line: str) -> Optional[float]:\n        \"\"\"Extract current progress from FFmpeg output\"\"\"\n        match = self.progress_pattern.search(line)\n        if match and self.total_duration > 0:\n            hours, minutes, seconds, centiseconds = map(int, match.groups())\n            current_time = hours * 3600 + minutes * 60 + seconds + centiseconds / 100\n            return min(current_time / self.total_duration, 1.0)\n        return None\n\nclass FFmpegWrapper:\n    \"\"\"Production-ready FFmpeg process wrapper\"\"\"\n    \n    def __init__(self, ffmpeg_path: str = \"ffmpeg\"):\n        self.ffmpeg_path = ffmpeg_path\n        self.logger = logging.getLogger(__name__)\n    \n    async def execute_command(\n        self,\n        command: List[str],\n        progress_callback: Optional[Callable[[float], None]] = None,\n        timeout: Optional[int] = None\n    ) -> subprocess.CompletedProcess:\n        \"\"\"Execute FFmpeg command with progress monitoring and timeout\"\"\"\n        \n        full_command = [self.ffmpeg_path] + command\n        self.logger.info(f\"Executing FFmpeg: {' '.join(full_command)}\")\n        \n        parser = FFmpegProgressParser()\n        process = None\n        \n        try:\n            # Start FFmpeg process with stderr capture\n            process = await asyncio.create_subprocess_exec(\n                *full_command,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                preexec_fn=None if os.name == 'nt' else os.setsid\n            )\n            \n            # Monitor progress in real-time\n            stderr_lines = []\n            while True:\n                try:\n                    line = await asyncio.wait_for(\n                        process.stderr.readline(), \n                        timeout=30.0  # Line timeout\n                    )\n                    if not line:\n                        break\n                    \n                    line_str = line.decode('utf-8', errors='ignore').strip()\n                    stderr_lines.append(line_str)\n                    \n                    # Parse duration on first encounter\n                    parser.parse_duration(line_str)\n                    \n                    # Report progress updates\n                    if progress_callback:\n                        progress = parser.parse_progress(line_str)\n                        if progress is not None:\n                            progress_callback(progress)\n                \n                except asyncio.TimeoutError:\n                    # Check if process is still alive\n                    if process.returncode is not None:\n                        break\n                    self.logger.warning(\"FFmpeg output timeout, continuing...\")\n            \n            # Wait for process completion\n            await asyncio.wait_for(process.wait(), timeout=timeout)\n            \n            stderr_output = '\\n'.join(stderr_lines)\n            \n            if process.returncode != 0:\n                raise FFmpegError(\n                    f\"FFmpeg failed with exit code {process.returncode}\",\n                    exit_code=process.returncode,\n                    stderr=stderr_output\n                )\n            \n            return subprocess.CompletedProcess(\n                args=full_command,\n                returncode=process.returncode,\n                stdout=None,  # We don't capture stdout\n                stderr=stderr_output\n            )\n        \n        except asyncio.TimeoutError:\n            if process:\n                await self._terminate_process(process)\n            raise FFmpegError(f\"FFmpeg timed out after {timeout} seconds\")\n        \n        except Exception as e:\n            if process:\n                await self._terminate_process(process)\n            raise FFmpegError(f\"FFmpeg execution failed: {str(e)}\")\n    \n    async def _terminate_process(self, process: asyncio.subprocess.Process):\n        \"\"\"Gracefully terminate FFmpeg process\"\"\"\n        try:\n            if os.name != 'nt':\n                # Send SIGTERM to process group\n                os.killpg(os.getpgid(process.pid), signal.SIGTERM)\n            else:\n                process.terminate()\n            \n            # Wait for graceful shutdown\n            try:\n                await asyncio.wait_for(process.wait(), timeout=5.0)\n            except asyncio.TimeoutError:\n                # Force kill if needed\n                if os.name != 'nt':\n                    os.killpg(os.getpgid(process.pid), signal.SIGKILL)\n                else:\n                    process.kill()\n                await process.wait()\n        \n        except (ProcessLookupError, OSError):\n            pass  # Process already terminated\n```\n\n**Core Video Transcoder Skeleton**\n\nThis skeleton provides the main transcoding interface that builds on the FFmpeg wrapper:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional, Callable\nimport asyncio\nimport json\nfrom pathlib import Path\n\n@dataclass\nclass VideoTranscodingConfig:\n    \"\"\"Configuration for video transcoding operation\"\"\"\n    video_codec: str = \"libx264\"\n    audio_codec: str = \"aac\"\n    crf_value: int = 23\n    preset: str = \"medium\"\n    profile: str = \"high\"\n    level: str = \"4.0\"\n    max_bitrate: Optional[int] = None\n    target_resolution: Optional[tuple] = None\n    segment_duration: Optional[int] = None  # For HLS/DASH\n    two_pass: bool = False\n\nclass VideoTranscoder:\n    \"\"\"Main video transcoding orchestrator\"\"\"\n    \n    def __init__(self, ffmpeg_wrapper: FFmpegWrapper):\n        self.ffmpeg = ffmpeg_wrapper\n        self.logger = logging.getLogger(__name__)\n    \n    async def transcode_video(\n        self,\n        input_path: str,\n        output_path: str,\n        config: VideoTranscodingConfig,\n        progress_callback: Optional[Callable[[float, str], None]] = None\n    ) -> Dict[str, any]:\n        \"\"\"\n        Transcode video file according to configuration.\n        \n        Args:\n            input_path: Source video file path\n            output_path: Destination file path\n            config: Transcoding parameters\n            progress_callback: Function called with (progress_percentage, stage_name)\n        \n        Returns:\n            Dictionary with transcoding results and metadata\n        \"\"\"\n        # TODO 1: Validate input file exists and is readable\n        # TODO 2: Analyze input video to extract metadata (resolution, duration, codecs)\n        # TODO 3: Build FFmpeg command based on config and input characteristics\n        # TODO 4: Execute transcoding with progress monitoring\n        # TODO 5: Validate output file was created successfully\n        # TODO 6: Extract output metadata and return results\n        pass\n    \n    def _build_transcode_command(\n        self, \n        input_path: str, \n        output_path: str, \n        config: VideoTranscodingConfig,\n        input_metadata: Dict\n    ) -> List[str]:\n        \"\"\"\n        Build FFmpeg command for transcoding operation.\n        \n        This method constructs the complete FFmpeg command line based on:\n        - Input file characteristics\n        - Target configuration parameters  \n        - Container format requirements\n        - Web optimization flags\n        \"\"\"\n        # TODO 1: Start with basic input/output file parameters\n        # TODO 2: Add video codec and encoding parameters (CRF, preset, profile)\n        # TODO 3: Add audio codec and quality settings\n        # TODO 4: Add resolution scaling if target_resolution specified\n        # TODO 5: Add container-specific optimization flags (faststart for MP4)\n        # TODO 6: Add any advanced encoding options (two-pass, keyframe intervals)\n        # Hint: Use list concatenation to build command parts: cmd += ['-c:v', config.video_codec]\n        pass\n    \n    async def generate_abr_variants(\n        self,\n        input_path: str,\n        output_dir: str,\n        variant_configs: List[AdaptiveBitrateConfig],\n        progress_callback: Optional[Callable[[float, str], None]] = None\n    ) -> Dict[str, List[str]]:\n        \"\"\"\n        Generate multiple quality variants for adaptive bitrate streaming.\n        \n        Returns:\n            Dictionary mapping variant names to lists of segment files\n        \"\"\"\n        # TODO 1: Create output directory structure for variants\n        # TODO 2: Process variants in parallel using asyncio.gather()\n        # TODO 3: Ensure keyframe alignment across all variants\n        # TODO 4: Generate HLS and/or DASH manifests\n        # TODO 5: Validate all segments were created successfully\n        # Hint: Use asyncio.gather(*tasks) to process multiple variants concurrently\n        pass\n    \n    async def extract_thumbnail(\n        self,\n        input_path: str,\n        output_path: str,\n        timestamp: float = 10.0,\n        resolution: tuple = (320, 180)\n    ) -> bool:\n        \"\"\"\n        Extract thumbnail frame from video at specified timestamp.\n        \n        Args:\n            timestamp: Time in seconds to extract frame\n            resolution: Target thumbnail resolution (width, height)\n        \n        Returns:\n            True if thumbnail was extracted successfully\n        \"\"\"\n        # TODO 1: Build FFmpeg command for single frame extraction\n        # TODO 2: Add timestamp seeking and resolution scaling\n        # TODO 3: Execute extraction command\n        # TODO 4: Verify output image file was created\n        # Hint: Use -ss for seeking, -vframes 1 for single frame, -s for resolution\n        pass\n```\n\n**Milestone Checkpoint**\n\nAfter implementing the video transcoding component, verify the following behaviors:\n\n1. **Basic Transcoding**: Convert a sample MP4 file to different resolutions and formats\n   ```bash\n   python -m media_processing.video.test_basic_transcoding\n   ```\n   Expected: Output files created with correct resolutions and playable in VLC\n\n2. **Progress Monitoring**: Watch progress updates during a long transcoding job  \n   Expected: Progress increases from 0% to 100% with reasonable time estimates\n\n3. **Error Handling**: Attempt to process a corrupted or missing video file\n   Expected: Clear error message with appropriate error classification\n\n4. **ABR Generation**: Create HLS segments from a sample video\n   Expected: Master playlist and variant playlists with properly segmented content\n\n5. **Resource Management**: Monitor memory usage during 4K video processing\n   Expected: Memory usage remains stable without continuous growth\n\n**Performance Debugging Tips**\n\n| Symptom | Likely Cause | Diagnosis Method | Solution |\n|---------|-------------|------------------|----------|\n| Transcoding hangs indefinitely | FFmpeg process deadlock | Check process tree, stderr output | Add timeout, restart worker |\n| Memory usage grows continuously | Frame buffer leaks | Monitor RSS memory over time | Implement buffer limits, restart workers |\n| Poor output quality | Wrong CRF/bitrate settings | Compare source vs output bitrates | Adjust quality parameters |\n| Slow encoding performance | Software encoding on server | Check for hardware acceleration | Enable NVENC/QSV if available |\n| Segment alignment issues | Keyframe misalignment | Analyze GOP structure in outputs | Force keyframes at segment boundaries |\n\n\n## Job Queue and Scheduling Component\n\n> **Milestone(s):** Milestone 3 (Processing Queue & Progress) - this section covers asynchronous job processing, priority queuing, worker management, and distributed task execution\n\n### Mental Model: Restaurant Kitchen\n\nThink of the media processing pipeline as a busy restaurant kitchen during dinner rush. When customers place orders (submit processing jobs), those orders don't go directly to the chefs—they go to an **order management system** that prioritizes them, assigns them to available cooks based on specialization and workload, and tracks progress from preparation through completion.\n\nThe **order tickets** represent processing jobs, each containing specific instructions: \"Table 12 wants a medium steak with truffle sauce, allergic to shellfish.\" Similarly, a processing job contains the input file path, desired output specifications, priority level, and callback information for notifications.\n\nThe **kitchen manager** acts like our job queue system, deciding which orders get processed first. Rush orders (high priority) jump ahead of regular dinner orders (normal priority), and simple appetizers (image resizing) might be handled by different stations than complex entrees requiring specialized equipment (video transcoding with FFmpeg).\n\nEach **cooking station** represents a worker process—some specialize in grilling (image processing), others handle sauce preparation (video encoding), and some can do general prep work. The kitchen manager knows each station's current workload and capabilities, distributing orders accordingly. When a dish is ready, the expediter (webhook notification system) alerts the wait staff and updates the order status.\n\nJust as restaurants handle kitchen fires, equipment breakdowns, and ingredient shortages without losing orders, our job queue system must gracefully handle worker crashes, resource exhaustion, and processing failures while ensuring no jobs disappear into the void.\n\n### Queue Operations and Priority\n\nThe job queue serves as the central coordination hub for all media processing operations, managing the entire lifecycle from job submission through completion notification. At its core, the queue implements a **priority-based work distribution system** that ensures time-sensitive jobs receive immediate attention while maintaining fair processing of routine work.\n\nWhen a client submits a processing job through the `submit_job` function, the system first validates the request parameters and generates a unique job identifier using `generate_job_id()`. This identifier includes a timestamp prefix to enable chronological sorting and debugging. The job gets wrapped in a `ProcessingJob` structure containing all necessary execution metadata: input file path, output specifications, priority level, webhook notification URL, and tracking timestamps.\n\n> **Decision: Priority Queue with Redis Sorted Sets**\n> - **Context**: Need to handle jobs with different urgency levels while maintaining FIFO order within each priority band\n> - **Options Considered**: Simple FIFO queue, weighted round-robin, Redis sorted sets with priority scores\n> - **Decision**: Redis sorted sets with numeric priority values\n> - **Rationale**: Sorted sets provide atomic priority insertion, efficient range queries for worker polling, and built-in deduplication. Priority scores allow fine-grained control over job ordering.\n> - **Consequences**: Enables responsive handling of urgent jobs while preventing starvation of lower-priority work. Requires careful priority score design to avoid edge cases.\n\nThe priority system uses enumerated levels that map to numeric scores for queue ordering. The priority-to-score mapping ensures that urgent jobs always process before high-priority jobs, which process before normal and low-priority jobs respectively.\n\n| Priority Level | Numeric Score | Use Case | Typical SLA |\n|---------------|---------------|----------|-------------|\n| `JobPriority.URGENT` | 20 | Real-time processing requests, critical system operations | < 30 seconds |\n| `JobPriority.HIGH` | 10 | User-facing operations, thumbnail generation | < 2 minutes |\n| `JobPriority.NORMAL` | 5 | Standard processing requests, background transcoding | < 10 minutes |\n| `JobPriority.LOW` | 1 | Batch operations, archive processing, non-critical tasks | Best effort |\n\nWorker processes continuously poll the queue using blocking pop operations that respect priority ordering. The polling mechanism implements a **hybrid pull model** where workers request jobs based on their current capacity and specialization. This approach prevents overwhelming individual workers while ensuring efficient resource utilization across the worker pool.\n\nThe job distribution algorithm considers several factors beyond simple priority ordering. Workers maintain capability flags indicating which media formats and processing types they support. For example, a worker running on a GPU-enabled machine might advertise video transcoding capabilities, while CPU-only workers focus on image processing tasks. The queue system matches job requirements to worker capabilities during the assignment process.\n\n> **Critical Insight**: Job priority affects scheduling order, but it doesn't preempt running jobs. Once a worker begins processing a job, it continues to completion regardless of higher-priority jobs arriving in the queue. This design prevents resource waste from partially completed work while still providing prioritization benefits for queued jobs.\n\nThe queue implements several advanced features to handle real-world operational challenges. **Job deduplication** prevents duplicate processing when clients accidentally submit the same request multiple times. The system generates a content-based hash from the input file checksum and output specifications, rejecting duplicate submissions within a configurable time window.\n\n**Batch job submission** allows clients to submit multiple related jobs atomically, ensuring they're processed as a cohesive unit. This feature proves particularly valuable for generating multiple video quality variants or processing image galleries where partial completion would be problematic.\n\nThe queue system maintains comprehensive metrics and monitoring data to support operational visibility. Key metrics include queue depth by priority level, average job processing time, worker utilization rates, and failure rates categorized by error type. These metrics feed into alerting systems that notify operators of queue backup, worker failures, or other operational issues requiring attention.\n\n### Worker Process Management\n\nWorker process management represents one of the most complex aspects of the job queue system, requiring careful coordination of resource allocation, process isolation, health monitoring, and failure recovery. Each worker operates as an independent process with clearly defined responsibilities and resource constraints.\n\nThe **worker lifecycle** follows a standardized pattern designed to ensure reliable operation under various failure scenarios. During startup, each worker registers with the job queue system, advertising its capabilities, resource limits, and processing specializations. This registration process includes a health check sequence that verifies the worker can access required dependencies like FFmpeg, storage systems, and temporary workspace directories.\n\n| Worker State | Description | Actions Permitted | Monitoring |\n|--------------|-------------|-------------------|------------|\n| `STARTING` | Worker initializing, loading configuration | Registration, dependency checks | Startup timeout |\n| `IDLE` | Worker available, polling for jobs | Accept jobs, health checks | Heartbeat monitoring |\n| `PROCESSING` | Worker executing job | Progress updates, resource monitoring | Job timeout, memory limits |\n| `DRAINING` | Worker finishing current job before shutdown | Complete current job only | Graceful shutdown timer |\n| `FAILED` | Worker encountered unrecoverable error | Cleanup, restart procedures | Error reporting |\n\nEach worker maintains a **resource budget** that prevents individual jobs from consuming excessive system resources. The budget includes memory limits, processing time constraints, temporary disk space allocation, and network bandwidth quotas. Before accepting a job, workers verify they have sufficient resources to complete the processing without exceeding their allocated limits.\n\nMemory management requires particular attention given the potentially large size of media files. Workers implement a **staged processing approach** where they estimate memory requirements before loading media files into memory. For video processing, this might involve analyzing file headers to determine resolution and codec information, then calculating expected memory usage based on frame buffer requirements and transcoding parameters.\n\n```\nMemory Estimation Process:\n1. Analyze input file headers to extract resolution and format information\n2. Calculate raw frame buffer size: width × height × channels × bit_depth\n3. Estimate transcoding memory overhead based on codec complexity\n4. Add safety margin for temporary buffers and processing overhead\n5. Compare total estimate to available worker memory budget\n6. Reject job if memory requirements exceed budget, otherwise proceed\n```\n\nThe worker pool implements **horizontal scaling** through a coordinator process that monitors overall system load and can spawn additional workers when queue depth exceeds configurable thresholds. Conversely, during low-demand periods, workers can be gracefully terminated to reduce resource consumption. This auto-scaling mechanism responds to both queue depth metrics and historical load patterns to anticipate demand fluctuations.\n\n**Process isolation** ensures that failures in one worker don't affect others or compromise system stability. Each worker runs in a separate process with restricted file system access, limited network permissions, and resource quotas enforced at the operating system level. Workers communicate with the job queue and storage systems through well-defined APIs that validate all inputs and sanitize file paths to prevent directory traversal attacks.\n\nHealth monitoring operates at multiple levels to detect various failure modes. **Heartbeat monitoring** requires workers to send periodic status updates to the queue system, including current job progress, resource utilization, and processing stage information. Workers that fail to send heartbeats within the configured timeout period are marked as failed and their current jobs are rescheduled for processing by healthy workers.\n\n> **Decision: Process-Based Worker Isolation**\n> - **Context**: Need to isolate media processing operations to prevent failures from affecting other jobs\n> - **Options Considered**: Thread-based workers, process-based workers, containerized workers\n> - **Decision**: Separate processes with resource limits\n> - **Rationale**: Processes provide strong isolation boundaries, prevent memory leaks from affecting other jobs, and enable independent restart of failed workers. Operating system process limits provide reliable resource enforcement.\n> - **Consequences**: Higher memory overhead compared to threads, but significantly improved fault tolerance and resource management. Simplified debugging since each worker has independent memory space.\n\nWorker coordination handles several challenging scenarios that arise in distributed processing environments. **Split-brain prevention** ensures that only one worker processes a given job, even during network partitions or queue system failures. This protection uses distributed locking with expiration timeouts to prevent jobs from being permanently orphaned.\n\nThe system implements **graceful degradation** when worker capacity becomes insufficient for demand. Rather than rejecting new jobs outright, the queue system can temporarily increase job timeout limits, reduce quality settings for non-critical processing, or defer low-priority jobs until capacity improves. These degradation strategies allow the system to continue operating during peak load periods without complete service disruption.\n\n### Job Queue Architecture Decisions\n\nThe architecture of the job queue system required careful evaluation of multiple design alternatives, each with significant implications for scalability, reliability, and operational complexity. These decisions form the foundation that enables the entire media processing pipeline to operate reliably under production workloads.\n\n**Message Broker Selection** represents the most fundamental architectural choice, as it determines the queue's performance characteristics, durability guarantees, and operational requirements. The decision between Redis, RabbitMQ, Apache Kafka, and cloud-native solutions like AWS SQS required analysis of both technical capabilities and operational constraints.\n\n> **Decision: Redis with Persistence for Message Broker**\n> - **Context**: Need reliable job queuing with priority support, reasonable throughput, and operational simplicity\n> - **Options Considered**: Redis (in-memory), RabbitMQ (AMQP), Apache Kafka (distributed log), AWS SQS (managed service)\n> - **Decision**: Redis with AOF persistence enabled\n> - **Rationale**: Redis sorted sets provide native priority queue functionality with atomic operations. AOF persistence ensures job durability across restarts. Simpler operational model than RabbitMQ or Kafka for media processing workloads. Single-node deployment sufficient for moderate scale.\n> - **Consequences**: Excellent performance for job queuing patterns, but requires careful memory management for large job volumes. Limited horizontal scaling compared to Kafka. Simplified deployment and monitoring.\n\n| Option | Pros | Cons | Verdict |\n|--------|------|------|---------|\n| **Redis** | Native priority queues, atomic operations, simple deployment | Memory-limited storage, single-node bottleneck | ✅ **Chosen** |\n| **RabbitMQ** | Mature AMQP implementation, flexible routing, persistent queues | Complex configuration, requires queue management | Considered |\n| **Apache Kafka** | High throughput, horizontal scaling, replay capability | Over-engineered for job queuing, complex operations | Rejected |\n| **AWS SQS** | Fully managed, automatic scaling, pay-per-use | Vendor lock-in, limited priority support, higher latency | Rejected |\n\n**Serialization Format** affects both performance and system evolution capability. The choice between JSON, MessagePack, Protocol Buffers, and other serialization approaches required balancing human readability, parsing performance, schema evolution support, and cross-language compatibility.\n\n> **Decision: JSON with Schema Validation**\n> - **Context**: Need human-readable job data for debugging with reasonable parsing performance\n> - **Options Considered**: JSON (human-readable), MessagePack (compact binary), Protocol Buffers (schema evolution)\n> - **Decision**: JSON with JSON Schema validation for job structures\n> - **Rationale**: Human readability crucial for debugging media processing issues. JSON parsing performance adequate for job queue throughput. Schema validation prevents malformed job data. Wide language support for future extensions.\n> - **Consequences**: Larger message sizes than binary formats, but acceptable for job queue usage patterns. Easier debugging and system monitoring. Simplified client integration.\n\nThe **concurrency model** determines how the system handles multiple simultaneous jobs and coordinates access to shared resources. The design choice between actor-based concurrency, shared-memory threading, and process-based isolation has far-reaching implications for system reliability and debugging complexity.\n\nProcess-based concurrency was selected over threading or actor models primarily due to the resource-intensive nature of media processing operations. FFmpeg and image processing libraries can consume significant memory and CPU resources, making process isolation essential for preventing resource leaks from affecting other jobs. Additionally, many media processing libraries have complex memory management patterns that are difficult to coordinate safely in multi-threaded environments.\n\n**Job Persistence Strategy** required balancing durability guarantees with system performance. The choice between storing complete job data in the message queue versus using lightweight job references with external storage affects both system complexity and failure recovery capabilities.\n\n| Persistence Approach | Description | Durability | Performance | Complexity |\n|---------------------|-------------|------------|-------------|------------|\n| **In-Queue Job Data** | Complete job specifications stored in Redis | High | Medium | Low |\n| **Reference + Database** | Job IDs in queue, full data in PostgreSQL | Very High | Lower | Medium |\n| **Hybrid Approach** | Essential data in queue, full specs in storage | High | High | Medium |\n\nThe system implements a **hybrid persistence approach** where the job queue contains essential execution metadata (job ID, priority, worker assignment) while complete job specifications and processing history are stored in external persistent storage. This design provides the performance benefits of in-memory job queuing while ensuring that job data survives system restarts and can be analyzed for historical reporting.\n\n**Worker Discovery and Load Balancing** mechanisms determine how jobs are distributed across available workers and how the system adapts to changing worker capacity. The choice between push-based job assignment and pull-based worker polling affects system responsiveness and load distribution fairness.\n\nThe implemented pull-based model allows workers to request jobs based on their current capacity and capabilities, preventing overloading of slower workers while ensuring that fast workers remain fully utilized. Workers include capability advertisements in their job requests, allowing the queue system to match job requirements with worker specializations automatically.\n\n**Error Handling and Recovery Architecture** establishes how the system responds to various failure scenarios, from temporary network issues to permanent worker failures. The decision between immediate retry, exponential backoff, dead letter queues, and manual intervention affects both system reliability and operational burden.\n\nThe multi-tiered error handling approach categorizes failures based on their likelihood of recovery and implements appropriate response strategies for each category. Transient failures (network timeouts, temporary resource exhaustion) trigger automatic retry with exponential backoff. Persistent failures (corrupted input files, unsupported formats) are moved to dead letter queues for manual investigation. System failures (worker crashes, storage unavailability) trigger immediate job rescheduling to healthy workers.\n\n### Common Queue Implementation Pitfalls\n\nImplementing a robust job queue system involves navigating numerous subtle pitfalls that can compromise reliability, performance, or data consistency. Understanding these common mistakes helps avoid production incidents and ensures the system operates correctly under all conditions.\n\n⚠️ **Pitfall: Job Deduplication Race Conditions**\n\nMany implementations fail to properly handle the race condition where multiple workers might pick up duplicate jobs before the deduplication check completes. This occurs when clients submit the same job multiple times in rapid succession, and the queue system hasn't completed processing the first submission before subsequent duplicates arrive.\n\nThe incorrect approach uses a simple \"check then insert\" pattern that's vulnerable to race conditions:\n```\n1. Check if job with same hash exists in queue\n2. If not found, insert new job\n3. Return job ID\n```\n\nBetween steps 1 and 2, another thread might insert the same job, resulting in duplicate processing. The correct implementation uses atomic operations provided by Redis to ensure deduplication works even under high concurrency. The system generates a content-based hash from the input file and output specifications, then uses Redis's SET with NX (not exists) flag to atomically insert the job only if no duplicate exists.\n\n⚠️ **Pitfall: Improper Dead Letter Queue Management**\n\nA common mistake is implementing dead letter queues without proper categorization of failure types, leading to either excessive manual intervention or silent data loss. Not all job failures should go to the dead letter queue—some represent permanent errors that should never be retried, while others might be temporary issues that resolve automatically.\n\nThe system should categorize failures into distinct types with appropriate handling strategies:\n- **Transient failures** (network timeouts, temporary resource exhaustion): Retry with exponential backoff up to maximum retry limit\n- **Permanent failures** (corrupted input files, unsupported formats): Move to dead letter queue with detailed error information for manual review\n- **System failures** (worker crashes, storage unavailability): Reschedule on different worker without counting against retry limit\n\nAdditionally, dead letter queues require active monitoring and periodic cleanup to prevent indefinite accumulation of failed jobs. The system should implement automated alerts when dead letter queue depth exceeds thresholds and provide tools for bulk job analysis and cleanup.\n\n⚠️ **Pitfall: Resource Cleanup on Worker Failure**\n\nWorker processes that crash or are forcibly terminated often leave behind temporary files, partially processed media, and locked resources. Without proper cleanup mechanisms, these artifacts accumulate over time and can exhaust disk space or cause resource conflicts for subsequent jobs.\n\nThe solution requires implementing cleanup procedures at multiple levels:\n1. **Graceful shutdown handlers** that cleanup temporary files when workers receive termination signals\n2. **Periodic cleanup processes** that identify and remove orphaned temporary files based on age and worker heartbeat status\n3. **Job reschedule logic** that ensures crashed jobs are properly reset and rescheduled rather than left in a permanent \"processing\" state\n4. **Resource lock recovery** that releases file locks and other resources held by failed workers\n\n⚠️ **Pitfall: Progress Estimation Accuracy**\n\nMany implementations provide progress estimates that are wildly inaccurate, either showing 90% completion for hours or jumping from 10% to 100% instantly. This occurs because developers base progress estimates on simple metrics like file size processed rather than actual processing complexity.\n\nMedia processing operations have highly variable computational requirements depending on content characteristics. A one-hour video with static scenes processes much faster than a one-hour video with rapid motion and complex visual effects, even though both files might be similar in size. The system should implement **stage-based progress reporting** rather than attempting to provide precise percentage estimates.\n\nThe correct approach divides processing into discrete stages (validation, preprocessing, encoding, postprocessing) and reports progress as stage completion rather than attempting granular percentage tracking. This provides users with meaningful progress information while avoiding the false precision of inaccurate time estimates.\n\n⚠️ **Pitfall: Worker Capacity Estimation**\n\nIncorrectly estimating worker processing capacity leads to either resource over-allocation (causing memory exhaustion and crashes) or under-utilization (leaving workers idle while jobs queue up). The challenge lies in predicting resource requirements for media processing jobs before loading the actual media files.\n\nThe system should implement **conservative capacity estimation** that analyzes input file metadata to predict memory and CPU requirements. For images, this involves calculating raw pixel buffer size based on resolution and bit depth. For videos, estimation includes frame buffer requirements for the target resolution plus transcoding overhead for the selected codec.\n\nWorkers should maintain resource budgets that account for peak memory usage during processing, not just average usage. A safety margin prevents edge cases from causing worker crashes, and jobs that exceed estimated resource requirements should be gracefully rejected rather than causing worker failures.\n\n⚠️ **Pitfall: Queue Monitoring Blind Spots**\n\nProduction issues often arise from monitoring gaps that miss critical failure modes. Common blind spots include gradual memory leaks in workers, increasing job processing times due to resource contention, and cascade failures where problems in one component affect others.\n\nComprehensive monitoring should track both technical metrics (queue depth, processing times, error rates) and business metrics (job completion rates, customer-facing latency, output quality measures). Alert thresholds should account for normal operational variance while detecting genuine issues early enough for proactive intervention.\n\nThe monitoring system should implement **trend analysis** that detects gradual degradation in addition to sudden failures. For example, slowly increasing job processing times might indicate resource exhaustion or performance regression that requires attention before it becomes a customer-impacting outage.\n\n### Implementation Guidance\n\nThe job queue and scheduling component serves as the coordination backbone for the entire media processing system, managing job distribution, worker coordination, and progress tracking across the processing pipeline. This implementation guidance provides the infrastructure foundation and core logic structure needed to build a production-ready job queue system.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Message Broker** | Redis with `redis-py` (in-memory sorted sets) | Redis Cluster with `redis-py-cluster` (distributed) |\n| **Worker Framework** | Custom worker processes with multiprocessing | Celery with Redis broker (full-featured task queue) |\n| **Job Persistence** | Redis with AOF persistence | PostgreSQL with Redis cache layer |\n| **Process Monitoring** | Python multiprocessing with custom health checks | Supervisor process management with monitoring |\n| **Configuration** | YAML files with `pyyaml` | Consul/etcd distributed configuration |\n| **Logging** | Python `logging` with structured JSON output | ELK stack (Elasticsearch, Logstash, Kibana) |\n\n#### Recommended File Structure\n\n```\nmedia_pipeline/\n  queue/\n    __init__.py\n    models.py                 ← Job, Priority, Status data structures\n    queue_manager.py          ← Job submission, priority handling\n    worker_pool.py           ← Worker process management and coordination\n    progress_tracker.py      ← Progress updates and webhook notifications\n    redis_backend.py         ← Redis operations and connection management\n    worker_process.py        ← Individual worker implementation\n  config/\n    queue_config.py          ← Queue configuration and validation\n    worker_config.py         ← Worker resource limits and capabilities\n  tests/\n    test_queue_operations.py ← Queue submission and priority tests\n    test_worker_management.py ← Worker lifecycle and coordination tests\n    integration/\n      test_job_processing.py ← End-to-end job processing tests\n  scripts/\n    start_workers.py         ← Worker startup script\n    queue_monitor.py         ← Queue monitoring and alerting\n    cleanup_jobs.py          ← Maintenance and cleanup utilities\n```\n\n#### Infrastructure Starter Code\n\n**Redis Backend Connection Management** (`redis_backend.py`):\n```python\nimport redis\nimport json\nimport uuid\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime, timedelta\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass RedisBackend:\n    \"\"\"Redis backend for job queue with connection pooling and error handling.\"\"\"\n    \n    def __init__(self, config: RedisConfig):\n        self.config = config\n        self.pool = redis.ConnectionPool(\n            host=config.host,\n            port=config.port,\n            db=config.db,\n            password=config.password,\n            decode_responses=True,\n            max_connections=20\n        )\n        self.redis = redis.Redis(connection_pool=self.pool)\n        \n        # Queue and status key patterns\n        self.job_queue_key = \"media:jobs:queue\"\n        self.job_data_key = \"media:jobs:data:{}\"\n        self.worker_status_key = \"media:workers:status\"\n        self.progress_key = \"media:jobs:progress:{}\"\n    \n    def health_check(self) -> bool:\n        \"\"\"Verify Redis connection and basic operations.\"\"\"\n        try:\n            return self.redis.ping()\n        except redis.RedisError as e:\n            logger.error(f\"Redis health check failed: {e}\")\n            return False\n    \n    def submit_job_atomic(self, job: ProcessingJob) -> bool:\n        \"\"\"Atomically submit job with deduplication check.\"\"\"\n        pipe = self.redis.pipeline()\n        try:\n            # Use job content hash for deduplication\n            dedup_key = f\"media:jobs:dedup:{job.content_hash()}\"\n            job_data_key = self.job_data_key.format(job.job_id)\n            \n            # Watch deduplication key for atomic operation\n            pipe.watch(dedup_key)\n            \n            if pipe.get(dedup_key):\n                logger.info(f\"Duplicate job detected for hash {job.content_hash()}\")\n                return False\n            \n            # Atomic job submission\n            pipe.multi()\n            pipe.setex(dedup_key, timedelta(hours=1), job.job_id)\n            pipe.hset(job_data_key, mapping=job.to_dict())\n            pipe.zadd(self.job_queue_key, {job.job_id: job.priority.value})\n            pipe.execute()\n            \n            logger.info(f\"Job {job.job_id} submitted successfully\")\n            return True\n            \n        except redis.WatchError:\n            logger.warning(\"Job submission failed due to concurrent modification\")\n            return False\n        except redis.RedisError as e:\n            logger.error(f\"Redis error during job submission: {e}\")\n            return False\n        finally:\n            pipe.reset()\n    \n    def pop_highest_priority_job(self, worker_id: str, timeout: int = 10) -> Optional[ProcessingJob]:\n        \"\"\"Blocking pop of highest priority job with worker assignment.\"\"\"\n        try:\n            # Use blocking pop on sorted set (highest score first)\n            result = self.redis.bzpopmax(self.job_queue_key, timeout=timeout)\n            if not result:\n                return None\n            \n            _, job_id, priority = result\n            job_data_key = self.job_data_key.format(job_id)\n            \n            # Atomically assign worker and update status\n            pipe = self.redis.pipeline()\n            job_data = pipe.hgetall(job_data_key)\n            pipe.hset(job_data_key, 'assigned_worker', worker_id)\n            pipe.hset(job_data_key, 'status', JobStatus.PROCESSING.value)\n            pipe.hset(job_data_key, 'started_at', datetime.utcnow().isoformat())\n            pipe.execute()\n            \n            if job_data:\n                job = ProcessingJob.from_dict(job_data)\n                logger.info(f\"Job {job_id} assigned to worker {worker_id}\")\n                return job\n            \n            logger.error(f\"Job data not found for job {job_id}\")\n            return None\n            \n        except redis.RedisError as e:\n            logger.error(f\"Redis error during job pop: {e}\")\n            return None\n    \n    def update_job_progress(self, job_id: str, progress_percentage: float, \n                          stage: str, details: Dict[str, Any] = None) -> bool:\n        \"\"\"Update job progress and timestamp.\"\"\"\n        try:\n            progress_data = {\n                'job_id': job_id,\n                'progress_percentage': progress_percentage,\n                'stage': stage,\n                'updated_at': datetime.utcnow().isoformat(),\n                'details': json.dumps(details or {})\n            }\n            \n            progress_key = self.progress_key.format(job_id)\n            job_data_key = self.job_data_key.format(job_id)\n            \n            pipe = self.redis.pipeline()\n            pipe.hset(progress_key, mapping=progress_data)\n            pipe.hset(job_data_key, 'progress_percentage', progress_percentage)\n            pipe.expire(progress_key, timedelta(days=1))  # Auto-expire progress data\n            pipe.execute()\n            \n            return True\n        except redis.RedisError as e:\n            logger.error(f\"Failed to update progress for job {job_id}: {e}\")\n            return False\n    \n    def mark_job_completed(self, job_id: str, output_files: List[str]) -> bool:\n        \"\"\"Mark job as completed and store output file paths.\"\"\"\n        try:\n            completion_data = {\n                'status': JobStatus.COMPLETED.value,\n                'completed_at': datetime.utcnow().isoformat(),\n                'output_files': json.dumps(output_files),\n                'progress_percentage': 100.0\n            }\n            \n            job_data_key = self.job_data_key.format(job_id)\n            self.redis.hset(job_data_key, mapping=completion_data)\n            \n            logger.info(f\"Job {job_id} marked as completed\")\n            return True\n        except redis.RedisError as e:\n            logger.error(f\"Failed to mark job {job_id} as completed: {e}\")\n            return False\n    \n    def mark_job_failed(self, job_id: str, error_message: str, retry_eligible: bool = True) -> bool:\n        \"\"\"Mark job as failed and handle retry logic.\"\"\"\n        try:\n            job_data_key = self.job_data_key.format(job_id)\n            current_data = self.redis.hgetall(job_data_key)\n            \n            if not current_data:\n                logger.error(f\"Job data not found for {job_id}\")\n                return False\n            \n            retry_count = int(current_data.get('retry_count', 0))\n            max_retries = 3  # Should come from config\n            \n            failure_data = {\n                'status': JobStatus.FAILED.value,\n                'error_message': error_message,\n                'failed_at': datetime.utcnow().isoformat(),\n                'retry_count': retry_count + 1\n            }\n            \n            pipe = self.redis.pipeline()\n            pipe.hset(job_data_key, mapping=failure_data)\n            \n            # Retry logic\n            if retry_eligible and retry_count < max_retries:\n                # Calculate exponential backoff delay\n                delay_seconds = min(300, 10 * (2 ** retry_count))  # Cap at 5 minutes\n                \n                # Reschedule job with delay\n                retry_time = datetime.utcnow() + timedelta(seconds=delay_seconds)\n                pipe.zadd(\"media:jobs:retry\", {job_id: retry_time.timestamp()})\n                \n                logger.info(f\"Job {job_id} scheduled for retry in {delay_seconds} seconds\")\n            else:\n                # Move to dead letter queue for manual review\n                pipe.lpush(\"media:jobs:dead_letter\", job_id)\n                logger.error(f\"Job {job_id} moved to dead letter queue after {retry_count} retries\")\n            \n            pipe.execute()\n            return True\n            \n        except redis.RedisError as e:\n            logger.error(f\"Failed to mark job {job_id} as failed: {e}\")\n            return False\n```\n\n**Worker Process Infrastructure** (`worker_process.py`):\n```python\nimport os\nimport signal\nimport psutil\nimport multiprocessing\nfrom typing import Dict, Any, Optional\nimport logging\nfrom datetime import datetime, timedelta\n\nlogger = logging.getLogger(__name__)\n\nclass WorkerProcess:\n    \"\"\"Individual worker process with resource management and health monitoring.\"\"\"\n    \n    def __init__(self, worker_id: str, config: ProcessingConfig, redis_backend: RedisBackend):\n        self.worker_id = worker_id\n        self.config = config\n        self.redis_backend = redis_backend\n        self.current_job: Optional[ProcessingJob] = None\n        self.start_time = datetime.utcnow()\n        self.is_shutting_down = False\n        self.resource_monitor = ResourceMonitor()\n        \n        # Set up signal handlers for graceful shutdown\n        signal.signal(signal.SIGTERM, self._handle_shutdown_signal)\n        signal.signal(signal.SIGINT, self._handle_shutdown_signal)\n    \n    def run(self):\n        \"\"\"Main worker loop with job processing and health monitoring.\"\"\"\n        logger.info(f\"Worker {self.worker_id} starting up\")\n        \n        try:\n            self._register_worker()\n            \n            while not self.is_shutting_down:\n                # TODO 1: Send heartbeat with current status and resource usage\n                # TODO 2: Check if worker should shutdown (maintenance mode, resource limits)\n                # TODO 3: Poll for available job with timeout\n                # TODO 4: If job received, validate worker can process it\n                # TODO 5: Execute job processing with progress callbacks\n                # TODO 6: Handle job completion or failure\n                # TODO 7: Cleanup temporary resources after job\n                # TODO 8: Brief sleep to prevent busy polling\n                pass\n                \n        except Exception as e:\n            logger.error(f\"Worker {self.worker_id} encountered fatal error: {e}\")\n            self._emergency_cleanup()\n        finally:\n            self._unregister_worker()\n            logger.info(f\"Worker {self.worker_id} shutdown complete\")\n    \n    def _register_worker(self):\n        \"\"\"Register worker with queue system and advertise capabilities.\"\"\"\n        # TODO 1: Create worker status record with capabilities\n        # TODO 2: Register in Redis worker registry\n        # TODO 3: Send initial heartbeat\n        # Hint: Include worker_id, start_time, capabilities, resource_limits\n        pass\n    \n    def _send_heartbeat(self) -> bool:\n        \"\"\"Send worker status update to queue system.\"\"\"\n        try:\n            status_data = {\n                'worker_id': self.worker_id,\n                'status': 'processing' if self.current_job else 'idle',\n                'current_job': self.current_job.job_id if self.current_job else None,\n                'heartbeat_time': datetime.utcnow().isoformat(),\n                'cpu_usage': self.resource_monitor.get_cpu_usage(),\n                'memory_usage': self.resource_monitor.get_memory_usage(),\n                'uptime_seconds': (datetime.utcnow() - self.start_time).total_seconds()\n            }\n            \n            # Store heartbeat with expiration\n            key = f\"media:workers:heartbeat:{self.worker_id}\"\n            self.redis_backend.redis.setex(key, timedelta(minutes=5), status_data)\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Failed to send heartbeat: {e}\")\n            return False\n    \n    def _handle_shutdown_signal(self, signum, frame):\n        \"\"\"Handle graceful shutdown signal.\"\"\"\n        logger.info(f\"Worker {self.worker_id} received shutdown signal {signum}\")\n        self.is_shutting_down = True\n        \n        if self.current_job:\n            logger.info(f\"Completing current job {self.current_job.job_id} before shutdown\")\n            # Allow current job to complete but don't accept new jobs\n\nclass ResourceMonitor:\n    \"\"\"Monitor worker resource usage and enforce limits.\"\"\"\n    \n    def __init__(self):\n        self.process = psutil.Process()\n        self.peak_memory = 0\n        self.start_time = datetime.utcnow()\n    \n    def get_cpu_usage(self) -> float:\n        \"\"\"Get current CPU usage percentage.\"\"\"\n        return self.process.cpu_percent()\n    \n    def get_memory_usage(self) -> Dict[str, float]:\n        \"\"\"Get detailed memory usage information.\"\"\"\n        memory_info = self.process.memory_info()\n        memory_percent = self.process.memory_percent()\n        \n        # Track peak memory usage\n        self.peak_memory = max(self.peak_memory, memory_info.rss)\n        \n        return {\n            'rss_mb': memory_info.rss / (1024 * 1024),\n            'vms_mb': memory_info.vms / (1024 * 1024),\n            'percent': memory_percent,\n            'peak_mb': self.peak_memory / (1024 * 1024)\n        }\n    \n    def check_resource_limits(self, limits: Dict[str, Any]) -> Dict[str, bool]:\n        \"\"\"Check if current resource usage exceeds configured limits.\"\"\"\n        # TODO 1: Compare current memory usage to limits.max_memory_mb\n        # TODO 2: Compare current CPU usage to limits.max_cpu_percent\n        # TODO 3: Check disk usage in temporary directories\n        # TODO 4: Return dict with limit_exceeded: bool for each resource\n        # Hint: This helps prevent workers from overwhelming the system\n        pass\n```\n\n#### Core Logic Skeleton Code\n\n**Job Queue Manager** (`queue_manager.py`):\n```python\nclass QueueManager:\n    \"\"\"Manages job submission, priority queuing, and worker coordination.\"\"\"\n    \n    def __init__(self, redis_backend: RedisBackend, config: AppConfig):\n        self.redis_backend = redis_backend\n        self.config = config\n        self.job_stats = JobStatistics()\n        \n    def submit_job(self, input_file: str, output_specs: List[OutputSpecification], \n                   priority: JobPriority, webhook_url: str = None) -> ProcessingJob:\n        \"\"\"Submit new processing job to queue with priority handling.\"\"\"\n        # TODO 1: Validate input_file exists and is readable\n        # TODO 2: Validate output_specs have valid formats and parameters\n        # TODO 3: Generate unique job_id using generate_job_id()\n        # TODO 4: Create ProcessingJob instance with all parameters\n        # TODO 5: Submit job atomically using redis_backend.submit_job_atomic()\n        # TODO 6: If submission successful, send webhook notification\n        # TODO 7: Update job statistics and metrics\n        # TODO 8: Return ProcessingJob instance\n        # Hint: Use try/except to handle validation and submission errors\n        pass\n    \n    def get_queue_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get current queue depth and processing statistics by priority.\"\"\"\n        # TODO 1: Query Redis for total jobs in queue\n        # TODO 2: Count jobs by priority level using ZRANGEBYSCORE\n        # TODO 3: Get count of active workers from heartbeat keys\n        # TODO 4: Calculate average job processing time from recent completions\n        # TODO 5: Get failed job count from dead letter queue\n        # TODO 6: Return comprehensive statistics dict\n        # Hint: Use Redis pipeline for efficient multi-query operations\n        pass\n    \n    def cleanup_expired_jobs(self, max_age_hours: int = 24):\n        \"\"\"Remove old job data and cleanup temporary resources.\"\"\"\n        # TODO 1: Find jobs older than max_age_hours in COMPLETED/FAILED status\n        # TODO 2: Archive job data to long-term storage if configured\n        # TODO 3: Remove job data keys from Redis\n        # TODO 4: Cleanup temporary files associated with old jobs\n        # TODO 5: Update cleanup metrics and log results\n        # Hint: Use SCAN to iterate through job keys efficiently\n        pass\n\nclass WorkerCoordinator:\n    \"\"\"Coordinates worker processes and manages worker pool scaling.\"\"\"\n    \n    def __init__(self, redis_backend: RedisBackend, config: ProcessingConfig):\n        self.redis_backend = redis_backend\n        self.config = config\n        self.worker_processes: Dict[str, multiprocessing.Process] = {}\n        self.target_worker_count = config.max_workers\n        \n    def start_worker_pool(self):\n        \"\"\"Start configured number of worker processes.\"\"\"\n        # TODO 1: Calculate optimal worker count based on CPU cores and memory\n        # TODO 2: Create worker processes using multiprocessing.Process\n        # TODO 3: Start each worker process and track PIDs\n        # TODO 4: Register signal handlers for pool shutdown\n        # TODO 5: Start monitoring thread for worker health checks\n        # Hint: Store worker PIDs for graceful shutdown management\n        pass\n    \n    def scale_worker_pool(self, target_count: int):\n        \"\"\"Dynamically scale worker pool up or down.\"\"\"\n        # TODO 1: Compare target_count to current active workers\n        # TODO 2: If scaling up, start additional worker processes\n        # TODO 3: If scaling down, signal workers to shutdown gracefully\n        # TODO 4: Wait for workers to complete current jobs before termination\n        # TODO 5: Update worker pool tracking and metrics\n        # Hint: Always allow current jobs to complete before shutdown\n        pass\n    \n    def monitor_worker_health(self):\n        \"\"\"Monitor worker heartbeats and restart failed workers.\"\"\"\n        # TODO 1: Check heartbeat timestamps for all registered workers\n        # TODO 2: Identify workers with stale heartbeats (likely crashed)\n        # TODO 3: Restart failed workers and reassign their jobs\n        # TODO 4: Send alerts for worker failures and restarts\n        # TODO 5: Update worker health metrics\n        # Hint: Use heartbeat timeout from config to detect failures\n        pass\n```\n\n#### Milestone Checkpoints\n\n**Checkpoint 1: Basic Queue Operations**\nAfter implementing the Redis backend and basic job submission:\n```bash\npython -m pytest tests/test_queue_operations.py -v\n```\n\nExpected behavior:\n- Jobs submitted with different priorities are retrieved in correct order\n- Duplicate job submission is properly rejected with deduplication\n- Job status transitions work correctly (PENDING → PROCESSING → COMPLETED)\n- Redis connection handling works with proper error recovery\n\nTest manually:\n```python\nfrom queue.queue_manager import QueueManager\nfrom queue.models import ProcessingJob, JobPriority\n\n# Submit test job\nmanager = QueueManager(redis_backend, config)\njob = manager.submit_job(\"test.jpg\", [output_spec], JobPriority.HIGH)\nprint(f\"Job submitted: {job.job_id}\")\n\n# Check queue statistics\nstats = manager.get_queue_statistics()\nprint(f\"Queue depth: {stats['total_jobs']}\")\n```\n\n**Checkpoint 2: Worker Process Management**\nAfter implementing worker processes and coordination:\n```bash\npython scripts/start_workers.py --worker-count 2\npython -m pytest tests/test_worker_management.py -v\n```\n\nExpected behavior:\n- Worker processes start and register with queue system\n- Heartbeat monitoring detects healthy and failed workers\n- Jobs are distributed to available workers based on capabilities\n- Graceful shutdown completes current jobs before terminating\n\n**Checkpoint 3: End-to-End Job Processing**\nAfter implementing complete job processing pipeline:\n```bash\npython -m pytest tests/integration/test_job_processing.py -v\n```\n\nExpected behavior:\n- Jobs flow from submission through processing to completion\n- Progress updates are tracked and reported correctly\n- Failed jobs are retried with exponential backoff\n- Webhook notifications are sent at appropriate stages\n- Resource cleanup occurs after job completion\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|-------------|-----------------|-----|\n| **Jobs stuck in PENDING** | No available workers or worker capability mismatch | Check worker heartbeats in Redis, verify worker capabilities | Start workers or check worker capability registration |\n| **Workers not picking up jobs** | Polling timeout too short or Redis connection issues | Check worker logs for Redis errors, verify queue key names | Increase polling timeout, verify Redis connectivity |\n| **Memory exhaustion crashes** | Worker memory limits not enforced or estimation incorrect | Monitor worker memory usage, check job size estimation | Implement proper memory limits, improve size estimation |\n| **Jobs processed multiple times** | Duplicate job submission or improper deduplication | Check job deduplication keys in Redis, verify atomic submission | Fix deduplication hash calculation, ensure atomic operations |\n| **Progress updates missing** | Worker not sending updates or Redis key expiration | Check progress update calls in worker, verify Redis keys | Add progress update calls, adjust Redis key expiration |\n| **Dead letter queue growing** | Permanent failures not categorized properly | Analyze dead letter queue jobs, check error categorization | Improve error classification, add manual job review process |\n\n\n## Progress Tracking and Notification Component\n\n> **Milestone(s):** Milestone 3 (Processing Queue & Progress) - this section covers real-time progress updates, webhook notifications, and job status management across the processing pipeline\n\n### Mental Model: Package Delivery Tracking\n\nThink of progress tracking like a modern package delivery service such as FedEx or UPS. When you ship a package, you receive a tracking number that lets you monitor its journey from pickup to delivery. The package moves through distinct stages: \"Package picked up\", \"Arrived at sorting facility\", \"Out for delivery\", and \"Delivered\". Each stage transition triggers an update to the tracking system, and you can receive notifications via SMS, email, or app push notifications when important milestones occur.\n\nSimilarly, our media processing pipeline treats each processing job like a package moving through a fulfillment system. The job progresses through well-defined stages: \"Job queued\", \"Processing started\", \"Image resizing complete\", \"Format conversion in progress\", \"Thumbnail generation\", and \"Processing complete\". Just as package tracking provides estimated delivery times based on historical data and current conditions, our system estimates completion times based on file size, processing complexity, and current worker load.\n\nThe notification system works like delivery alerts - when significant events occur (job completion, failure, or major progress milestones), the system sends webhook notifications to registered callback URLs, similar to how delivery services send status updates to your phone. If a notification fails to deliver (like when your phone is offline), the system retries with exponential backoff, ensuring important updates eventually reach their destination.\n\nThis mental model helps us understand three key principles: **stage-based progress** (discrete milestones rather than continuous percentages), **event-driven notifications** (updates triggered by state changes), and **reliable delivery** (guaranteed notification delivery with retry mechanisms).\n\n### Progress Calculation Strategies\n\nProgress tracking in media processing presents unique challenges compared to simple file operations. Unlike copying files where progress correlates linearly with bytes transferred, media processing involves multiple distinct phases with varying computational complexity and duration. Our system employs **stage-based progress** rather than time-based estimates, providing more accurate and meaningful progress updates to users.\n\n#### Stage-Based Progress Architecture\n\nThe foundation of our progress system divides each processing job into discrete, measurable stages. Each stage represents a significant computational milestone with clearly defined inputs, outputs, and completion criteria. This approach provides several advantages over percentage-based progress: stages are deterministic (a job always progresses through the same sequence), measurable (each stage has concrete completion criteria), and meaningful to users (stage names communicate actual processing activities).\n\n| Stage Name | Description | Percentage Weight | Duration Factors |\n|------------|-------------|------------------|------------------|\n| `VALIDATION` | File format detection and metadata extraction | 5% | File size, format complexity |\n| `PREPROCESSING` | EXIF handling, orientation correction | 10% | Metadata density, rotation needed |\n| `RESIZE_OPERATIONS` | Primary resizing and cropping operations | 40% | Output count, resolution changes |\n| `FORMAT_CONVERSION` | Transcoding between image/video formats | 35% | Codec complexity, quality settings |\n| `OPTIMIZATION` | Compression and web optimization | 8% | Quality targets, advanced features |\n| `FINALIZATION` | File writing and cleanup operations | 2% | Storage latency, temp file cleanup |\n\nEach stage maintains its own progress tracking mechanism. For example, during `RESIZE_OPERATIONS`, progress increments as each output specification completes processing. If a job requires generating five different image sizes, progress within this stage advances by 20% for each completed resize operation. This granular tracking provides meaningful feedback even for complex jobs with dozens of output variants.\n\n> **Design Insight**: Stage-based progress solves the estimation problem that plagues traditional progress bars. Instead of guessing \"45% complete in 3.7 minutes\", we can accurately report \"Format conversion in progress - 3 of 5 outputs complete\", which provides actionable information to users.\n\n#### Time Estimation Algorithms\n\nWhile stage-based progress forms our primary tracking mechanism, users still desire time estimates for planning purposes. Our system combines historical performance data with real-time job characteristics to generate dynamic estimates that improve in accuracy as processing progresses.\n\nThe estimation algorithm operates in three phases: **initial estimate** (based on file size and historical averages), **refinement phase** (adjusts estimates as early stages complete), and **final phase** (provides high-accuracy estimates for remaining work). Initial estimates use a regression model trained on historical job completion times, factoring in input file size, output specification count, and current worker load.\n\n```\nInitial Estimate Calculation:\n1. Extract job characteristics: file_size, output_count, format_complexity_score\n2. Query historical database for similar jobs (±20% file size, same format family)\n3. Calculate baseline duration using weighted average of historical completion times\n4. Apply load factor adjustment based on current queue depth and worker availability\n5. Add safety margin (typically 15-25%) to account for estimation uncertainty\n```\n\nAs processing progresses, the system refines estimates using actual stage completion times. If the `VALIDATION` stage completes faster than expected, the algorithm proportionally adjusts estimates for remaining stages. Conversely, if early stages take longer than predicted, the system increases estimates for subsequent work to maintain accuracy.\n\n> **Decision: Dynamic Estimation Refinement**\n> - **Context**: Static time estimates become inaccurate as processing reveals actual complexity\n> - **Options Considered**: Fixed estimates, linear interpolation, machine learning models\n> - **Decision**: Weighted refinement algorithm that adjusts estimates based on completed stage performance\n> - **Rationale**: Balances simplicity with accuracy, provides meaningful updates without complex ML infrastructure\n> - **Consequences**: Estimates improve in accuracy over time but may initially fluctuate as refinements apply\n\n#### Progress Update Mechanisms\n\nProgress updates must balance accuracy with performance overhead. Our system implements a **hierarchical update strategy** that provides frequent updates for user-facing operations while minimizing database write load and notification spam. The strategy employs different update frequencies based on the operation type and user visibility requirements.\n\nFor long-running operations like video transcoding, the system reports progress updates at regular time intervals (every 30 seconds) rather than after every processed frame. This prevents overwhelming the notification system while maintaining responsive user feedback. Short-duration operations like image resizing trigger updates only at stage boundaries to minimize overhead while still providing meaningful progress visibility.\n\n| Operation Type | Update Frequency | Trigger Events | Notification Threshold |\n|----------------|------------------|----------------|------------------------|\n| Image Processing | Stage boundaries | Stage complete, error encountered | 25% progress increments |\n| Video Transcoding | 30-second intervals | Time elapsed, segment complete | 20% progress increments |\n| Batch Operations | Per-item completion | Individual job complete | Per-job completion |\n| Format Conversion | Quality milestone | Encoding pass complete | Major milestone only |\n\nThe progress update pipeline employs **atomic state transitions** to prevent race conditions between multiple worker processes updating the same job. Each progress update includes a sequence number and timestamp to ensure updates apply in correct order and detect potential concurrency issues.\n\n```\nAtomic Progress Update Procedure:\n1. Acquire advisory lock on job record using job_id as lock key\n2. Read current job state and validate update is newer than stored progress\n3. Calculate new progress percentage based on stage completion and weights\n4. Update job record with new progress, stage, timestamp, and sequence number\n5. Determine if notification threshold reached since last webhook delivery\n6. Release advisory lock and trigger async notification if threshold met\n7. Log progress update with correlation_id for debugging and audit trails\n```\n\n### Webhook Notification System\n\nThe webhook notification system provides reliable, real-time communication between the media processing pipeline and external applications. Unlike simple HTTP callbacks, our implementation handles the complexities of distributed systems: network failures, service outages, duplicate detection, and security verification. The system guarantees **at-least-once delivery** for critical notifications while providing **exactly-once semantics** for idempotent operations.\n\n#### Webhook Event Types and Payloads\n\nThe notification system supports multiple event types, each with standardized payload structures optimized for different integration patterns. Events follow a consistent schema that includes correlation metadata, security signatures, and extensible payload sections for future enhancement without breaking existing integrations.\n\n| Event Type | Trigger Condition | Payload Contents | Retry Policy |\n|------------|-------------------|------------------|--------------|\n| `job.started` | Job processing begins | Job metadata, input file info | Standard retry |\n| `job.progress` | Major progress milestone | Current stage, percentage, ETA | No retry (informational) |\n| `job.completed` | Processing successfully finishes | Output file URLs, metadata | Aggressive retry |\n| `job.failed` | Permanent failure occurs | Error details, retry eligibility | Aggressive retry |\n| `job.retry` | Job scheduled for retry | Retry count, next attempt time | Standard retry |\n\nEach webhook payload includes a comprehensive event envelope that provides recipients with sufficient context for processing and debugging. The envelope structure promotes consistency across event types while allowing type-specific data in the payload section.\n\n```\nWebhook Payload Structure:\n{\n  \"event_id\": \"unique identifier for deduplication\",\n  \"event_type\": \"job.completed\",\n  \"event_timestamp\": \"ISO 8601 timestamp of event occurrence\", \n  \"correlation_id\": \"request tracking identifier\",\n  \"api_version\": \"v1\",\n  \"signature\": \"HMAC-SHA256 signature for verification\",\n  \"job\": {\n    \"job_id\": \"processing job identifier\",\n    \"status\": \"current job status enum value\",\n    \"progress_percentage\": \"current completion percentage\",\n    \"created_at\": \"job creation timestamp\",\n    \"started_at\": \"processing start timestamp\", \n    \"completed_at\": \"completion timestamp if applicable\"\n  },\n  \"payload\": {\n    // Event-specific data varies by event_type\n    \"output_files\": [\"array of generated file URLs\"],\n    \"processing_duration\": \"total processing time in seconds\",\n    \"metadata\": {\"extracted media metadata\"}\n  }\n}\n```\n\nThe signature field contains an HMAC-SHA256 hash computed over the entire payload body using a shared secret configured per webhook endpoint. This prevents tampering and allows recipients to verify message authenticity and integrity. The signature calculation includes the raw JSON payload to prevent parsing-related security vulnerabilities.\n\n#### Reliable Delivery Implementation\n\nWebhook delivery reliability requires sophisticated retry logic that balances prompt notification delivery with respectful behavior toward recipient services. Our implementation employs **exponential backoff with jitter** to prevent thundering herd effects while ensuring important notifications eventually reach their destinations even during extended outages.\n\nThe retry system categorizes delivery failures into **transient** (network timeouts, 5xx responses), **permanent** (4xx client errors, invalid URLs), and **throttling** (429 rate limit responses) categories, each with appropriate retry behavior. Transient failures trigger exponential backoff retries, permanent failures immediately move to dead letter storage, and throttling failures use the `Retry-After` header when provided by the recipient.\n\n| Failure Type | HTTP Status Codes | Retry Behavior | Max Attempts |\n|--------------|------------------|----------------|--------------|\n| Transient | 408, 500, 502, 503, 504 | Exponential backoff (1, 2, 4, 8, 16 min) | 5 attempts |\n| Permanent | 400, 401, 403, 404, 410 | No retry, dead letter | 1 attempt |\n| Rate Limiting | 429 | Honor Retry-After header | 3 attempts |\n| Network Error | Connection timeout, DNS failure | Exponential backoff | 5 attempts |\n\n```\nWebhook Delivery Algorithm:\n1. Construct webhook payload with current job state and event-specific data\n2. Generate HMAC-SHA256 signature using configured webhook secret\n3. Set HTTP headers: Content-Type, User-Agent, X-Webhook-Signature, X-Event-Type\n4. Attempt HTTP POST delivery with 30-second timeout\n5. Classify response: success (2xx), transient error, permanent error, rate limit\n6. For transient errors: schedule retry with exponential backoff plus random jitter\n7. For permanent errors: log failure and move to dead letter queue  \n8. For rate limits: respect Retry-After header or use default backoff\n9. Update delivery attempt metrics for monitoring and alerting\n```\n\nThe delivery system maintains **idempotency** through unique event identifiers that allow recipients to detect and discard duplicate deliveries. This enables aggressive retry policies without fear of causing duplicate processing in downstream systems.\n\n> **Decision: At-Least-Once Delivery Guarantee**\n> - **Context**: Network failures and service outages require robust notification delivery\n> - **Options Considered**: At-most-once, at-least-once, exactly-once delivery semantics\n> - **Decision**: At-least-once delivery with idempotency keys for duplicate detection\n> - **Rationale**: Balances reliability with complexity - easier to detect duplicates than recover lost messages\n> - **Consequences**: Recipients must implement idempotency checking but gain strong delivery guarantees\n\n#### Security and Authentication\n\nWebhook security protects against message tampering, replay attacks, and unauthorized webhook injection. Our implementation provides **cryptographic verification** through HMAC signatures, **timestamp validation** to prevent replay attacks, and **endpoint validation** to ensure webhook URLs point to expected domains.\n\nThe signature verification process uses HMAC-SHA256 with webhook-specific secrets that rotate periodically for enhanced security. Recipients verify signatures by computing the HMAC over the raw request body and comparing it to the provided signature header. Mismatched signatures result in rejection and security logging for investigation.\n\n```\nSignature Verification Process:\n1. Extract X-Webhook-Signature header from incoming request\n2. Retrieve webhook secret associated with the endpoint\n3. Compute HMAC-SHA256(secret, raw_request_body) \n4. Compare computed signature with provided signature using constant-time comparison\n5. Verify timestamp is within acceptable window (default: 5 minutes)\n6. Accept message if signature valid and timestamp fresh, otherwise reject\n```\n\nTimestamp validation prevents replay attacks where attackers capture valid webhook payloads and retransmit them later. The system includes the current timestamp in the signed payload and rejects messages older than a configurable threshold (typically 5-10 minutes).\n\n### Progress Tracking Architecture Decisions\n\nThe progress tracking system requires careful architectural decisions that balance real-time responsiveness, storage efficiency, and system reliability. These decisions shape how the system handles concurrent updates, stores progress data, and provides query interfaces for both internal components and external integrations.\n\n#### Storage Architecture for Progress Data\n\nProgress tracking data exhibits unique characteristics that influence storage design: **high write frequency** (frequent progress updates), **temporal access patterns** (recent data accessed most often), **moderate retention requirements** (historical progress needed for debugging), and **real-time query needs** (dashboard and API access). These characteristics favor a hybrid storage approach combining in-memory caching for active jobs with persistent storage for historical data.\n\n> **Decision: Redis + PostgreSQL Hybrid Storage**\n> - **Context**: Progress data requires high-frequency writes with real-time read access\n> - **Options Considered**: PostgreSQL only, Redis only, hybrid Redis+PostgreSQL approach\n> - **Decision**: Active job progress in Redis with periodic persistence to PostgreSQL\n> - **Rationale**: Redis provides sub-millisecond updates for active jobs, PostgreSQL ensures durability and complex queries\n> - **Consequences**: Adds storage complexity but enables real-time performance with data durability\n\nThe hybrid approach stores **active job progress** in Redis using optimized data structures for fast updates and queries. Each active job maintains a Redis hash containing current progress state, stage information, and update timestamps. Redis persistence occurs through periodic snapshots to PostgreSQL, ensuring progress data survives system restarts while maintaining real-time performance.\n\n| Data Category | Primary Storage | Persistence Layer | Access Pattern | Retention |\n|---------------|-----------------|-------------------|----------------|-----------|\n| Active Progress | Redis Hash | PostgreSQL sync every 30s | High frequency read/write | Until job complete |\n| Completed Jobs | PostgreSQL | Archive after 90 days | Occasional queries | 90 days active |\n| Progress History | PostgreSQL | Compressed storage | Analytics queries | 1 year archived |\n| Notification Log | PostgreSQL | Time-series partitioning | Debugging access | 30 days |\n\nRedis data structures optimize for common access patterns. Job progress uses Redis hashes for atomic field updates, while progress history uses sorted sets ordered by timestamp for efficient range queries. The notification system employs Redis lists for webhook delivery queues with automatic expiration for failed deliveries.\n\n```\nRedis Data Structure Design:\nprogress:job:{job_id} = Hash {\n  \"percentage\": \"current completion percentage\",\n  \"stage\": \"current processing stage name\", \n  \"stage_progress\": \"progress within current stage\",\n  \"updated_at\": \"timestamp of last update\",\n  \"estimated_completion\": \"ETA timestamp\",\n  \"notification_sent\": \"last notification percentage\"\n}\n\nprogress:history:{job_id} = Sorted Set {\n  \"timestamp1\": \"stage:percentage:details\",\n  \"timestamp2\": \"stage:percentage:details\" \n}\n\nwebhooks:pending = List [\n  \"webhook_delivery_request_1\",\n  \"webhook_delivery_request_2\"\n]\n```\n\n#### Real-Time Update Mechanisms\n\nReal-time progress updates serve multiple consumers with different latency requirements: **user dashboards** (sub-second updates), **API clients** (5-10 second intervals), **webhook notifications** (event-driven), and **monitoring systems** (1-minute aggregates). The system employs a **publish-subscribe pattern** with Redis Pub/Sub to efficiently distribute updates to all interested consumers without coupling update generation to consumption.\n\nProgress updates flow through a **multi-tier notification system** that provides different update frequencies and delivery guarantees based on consumer requirements. Real-time dashboard connections receive immediate updates via WebSocket connections, while API clients can poll for updates at configurable intervals. Webhook notifications trigger only when significant thresholds are crossed to prevent notification spam.\n\n| Consumer Type | Update Frequency | Delivery Method | Filtering |\n|---------------|------------------|------------------|-----------|\n| WebSocket Dashboard | Immediate | Redis Pub/Sub → WebSocket | All progress changes |\n| REST API Clients | On-demand poll | Direct Redis query | Current state only |\n| Webhook Notifications | Threshold-based | Async delivery queue | Major milestones |\n| Monitoring Systems | 1-minute aggregates | Background collection | Statistical summaries |\n\nThe pub/sub architecture decouples progress generation from consumption, allowing the system to scale notification consumers independently of progress update frequency. Workers publish progress updates to Redis channels without waiting for acknowledgment, ensuring update operations remain fast and don't block processing.\n\n```\nReal-Time Update Flow:\n1. Worker process calculates new progress percentage and stage information\n2. Worker atomically updates Redis progress hash with new values\n3. Worker publishes update message to Redis channel: progress:updates\n4. WebSocket service subscribed to channel receives update immediately\n5. WebSocket service filters updates and forwards to connected dashboard clients\n6. Background process periodically persists Redis state to PostgreSQL\n7. Webhook service checks notification thresholds and queues deliveries as needed\n```\n\n#### Concurrency Control and Race Conditions\n\nMultiple worker processes may attempt simultaneous progress updates for the same job, particularly during batch operations or parallel processing stages. The system prevents race conditions through **optimistic concurrency control** using Redis atomic operations and sequence numbers that detect out-of-order updates.\n\nEach progress update includes a **monotonically increasing sequence number** generated by the worker process performing the update. Redis WATCH/MULTI/EXEC transactions ensure progress updates apply atomically and reject out-of-order updates that could cause progress to move backward incorrectly.\n\n> **Decision: Optimistic Concurrency with Sequence Numbers**\n> - **Context**: Multiple workers may update progress for the same job simultaneously\n> - **Options Considered**: Pessimistic locking, optimistic concurrency, last-writer-wins\n> - **Decision**: Optimistic concurrency control with sequence numbers and Redis atomic operations\n> - **Rationale**: Avoids lock contention while preventing race conditions and progress reversals\n> - **Consequences**: Requires retry logic in workers but provides better scalability than locking\n\n```\nAtomic Progress Update Implementation:\n1. Worker generates sequence number: current_timestamp_microseconds\n2. Redis WATCH command monitors the progress hash for concurrent modifications  \n3. Worker reads current sequence number from progress hash\n4. If new sequence number <= current sequence, abort update (out of order)\n5. Begin Redis MULTI transaction for atomic updates\n6. Update progress hash fields: percentage, stage, sequence, timestamp\n7. Publish update message to progress channel\n8. EXEC transaction - succeeds only if no concurrent modifications detected\n9. If transaction fails, retry with exponential backoff up to 3 attempts\n```\n\nSequence number comparison prevents older updates from overwriting newer progress data, which could occur due to network delays or worker process variations in timing. Workers that detect rejected updates log the event for debugging but continue processing since another worker has already reported more recent progress.\n\n### Common Progress Tracking Pitfalls\n\nProgress tracking systems encounter predictable failure modes that can significantly impact user experience and system reliability. Understanding these pitfalls and their solutions helps avoid common implementation mistakes that lead to inaccurate progress reporting, notification failures, and poor system observability.\n\n#### ⚠️ **Pitfall: Progress Reversals and Non-Monotonic Updates**\n\nOne of the most confusing user experiences occurs when progress appears to move backward, showing 75% complete followed by 60% complete. This typically happens when multiple worker processes or processing stages report progress concurrently without proper coordination, or when different estimation algorithms provide conflicting progress calculations.\n\nProgress reversals commonly occur during **parallel processing scenarios** where multiple workers handle different output specifications for the same job. Worker A might complete 3 of 4 image resize operations (reporting 75% stage progress), while Worker B starts format conversion and reports early conversion progress (30% of final stage), causing the overall job progress to appear to decrease when calculated incorrectly.\n\n**Why this breaks user trust**: Users expect progress to monotonically increase toward completion. Backward movement suggests system errors or inaccurate estimates, damaging confidence in the processing pipeline. Dashboard interfaces may show confusing progress bar animations that move backward, creating a broken user experience.\n\n**Prevention and fixes**:\n- Implement sequence numbers for all progress updates to detect and reject out-of-order updates\n- Use atomic Redis operations to ensure progress updates apply consistently without race conditions  \n- Design stage-based progress weights that sum to 100% and validate updates maintain monotonic progression\n- Add progress validation logic that rejects updates showing backward movement unless explicitly resetting job state\n- Log progress reversals as warnings for debugging while maintaining the last valid monotonic progress value\n\n#### ⚠️ **Pitfall: Webhook Delivery Storms and Rate Limiting**\n\nAggressive progress reporting can overwhelm webhook endpoints with excessive notification traffic, particularly during batch processing operations or when multiple jobs complete simultaneously. This leads to rate limiting responses from recipient services, failed deliveries, and potential service degradation for webhook consumers.\n\nWebhook storms typically occur when the system sends notifications for every progress increment rather than meaningful milestones. A video transcoding job might generate hundreds of progress updates per minute, causing the notification system to attempt hundreds of webhook deliveries that quickly exceed recipient rate limits and cause cascading failures.\n\n**Why this causes problems**: Recipient services implement rate limiting to protect against abuse, and webhook storms can trigger these protections. Failed deliveries require retry logic that can compound the problem. Excessive webhook traffic also increases processing overhead and can impact system performance.\n\n**Prevention and fixes**:\n- Implement notification thresholds that trigger webhooks only for significant progress changes (typically 10-25% increments)\n- Use webhook consolidation that batches multiple progress updates into single notifications when appropriate\n- Add per-endpoint rate limiting in the webhook delivery system to respect recipient service limits\n- Implement backoff strategies that honor `Retry-After` headers and reduce delivery frequency for rate-limited endpoints\n- Provide webhook filtering configuration that lets recipients specify which event types and progress thresholds they want to receive\n\n#### ⚠️ **Pitfall: Memory Leaks in Progress Storage**\n\nLong-running systems accumulate progress tracking data for completed jobs, leading to memory exhaustion if not properly managed. Redis instances storing active job progress can grow unbounded when cleanup processes fail to remove completed job data, eventually causing out-of-memory errors and system instability.\n\nMemory leaks often occur because progress cleanup depends on external job completion notifications that may be lost due to system failures or race conditions. Completed jobs leave behind Redis hashes, sorted sets, and pub/sub subscriptions that continue consuming memory without providing value.\n\n**Why this causes system failures**: Redis operates primarily in memory, and unbounded growth leads to memory exhaustion. System administrators may not notice gradual memory increases until sudden spikes in job volume cause out-of-memory crashes. Recovery requires manual intervention and potential data loss.\n\n**Prevention and fixes**:\n- Implement automatic cleanup processes with Redis TTL (Time To Live) settings on all progress data structures\n- Use separate cleanup workers that periodically scan for completed jobs and remove associated progress data\n- Add memory monitoring and alerting for Redis instances to detect gradual memory growth trends\n- Implement circuit breaker patterns that stop accepting new jobs when Redis memory usage exceeds safe thresholds\n- Design progress data structures with explicit expiration policies rather than relying solely on cleanup processes\n\n#### ⚠️ **Pitfall: Inaccurate Time Estimates and User Expectations**\n\nTime estimation algorithms often provide wildly inaccurate completion estimates, particularly for complex media processing operations where actual processing time varies significantly based on content characteristics, system load, and processing complexity that's difficult to predict from input file metadata.\n\nEstimation errors frequently occur when algorithms rely solely on file size without considering content complexity factors like video codec efficiency, image compression characteristics, or the computational cost of specific processing operations. A large but simply-encoded video may process faster than a smaller file with complex motion that requires intensive encoding.\n\n**Why this damages user experience**: Inaccurate estimates lead to user frustration and poor planning. Estimates that consistently overshoot completion times cause users to abandon jobs they believe are stuck. Underestimated completion times cause users to wait expecting imminent completion.\n\n**Prevention and fixes**:\n- Use conservative estimation algorithms that tend to overestimate rather than underestimate completion times\n- Implement estimation confidence intervals that communicate uncertainty to users (\"15-25 minutes remaining\")\n- Refine estimates dynamically as processing progresses and actual performance data becomes available\n- Provide stage-based progress communication (\"Converting video format - 3 of 5 quality variants complete\") that gives meaningful information without specific time commitments\n- Track estimation accuracy over time and adjust algorithms based on historical performance data\n\n#### ⚠️ **Pitfall: Race Conditions in Progress Percentage Calculations**\n\nComplex jobs involving multiple parallel processing stages can produce race conditions when calculating overall progress percentages, leading to inconsistent or impossible progress values (such as percentages exceeding 100% or stage progress that doesn't align with overall job progress).\n\nRace conditions typically emerge when different processing threads update stage-specific progress simultaneously while another thread calculates overall job progress. The calculation may read partially updated stage progress values, producing incorrect overall percentages that don't reflect actual processing state.\n\n**Why this causes confusion**: Inconsistent progress reporting makes the system appear unreliable and can cause client applications to malfunction if they depend on progress values for logic decisions. Users see confusing progress displays that don't match the actual processing state.\n\n**Prevention and fixes**:\n- Use atomic read operations when calculating progress percentages to ensure consistent snapshots of all stage progress values\n- Implement progress calculation locks or atomic transactions that prevent concurrent modification during percentage calculation\n- Add validation logic that verifies calculated progress percentages fall within expected ranges (0-100%) and stage consistency\n- Design progress data structures that support atomic updates of multiple related values\n- Use eventual consistency patterns where progress displays may lag slightly but always show consistent, valid values\n\n![Webhook Notification Flow](./diagrams/webhook-notification-flow.svg)\n\n![Video Processing Sequence](./diagrams/video-processing-sequence.svg)\n\n![Worker Process Coordination](./diagrams/worker-coordination.svg)\n\n### Implementation Guidance\n\nThe progress tracking and notification system requires careful orchestration of real-time updates, persistent storage, and reliable webhook delivery. This implementation guidance provides complete starter code for the infrastructure components while identifying the core logic areas where learners should focus their implementation efforts.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Progress Storage | SQLite with in-memory caching | Redis + PostgreSQL hybrid |\n| Real-time Updates | HTTP polling with 5-second intervals | WebSocket connections with Redis Pub/Sub |\n| Webhook Delivery | Simple HTTP POST with basic retry | Async queue with exponential backoff |\n| Progress Calculation | Fixed percentage increments per stage | Dynamic stage weighting with time estimates |\n| Notification Filtering | All progress changes trigger webhooks | Threshold-based filtering with batching |\n\n#### Recommended File Structure\n\nThe progress tracking component integrates with job queue operations and external webhook delivery, requiring clean separation between progress calculation, storage management, and notification delivery concerns.\n\n```\nmedia_processing/\n  progress/\n    __init__.py                 ← component exports\n    tracker.py                  ← core ProgressTracker class\n    calculator.py               ← stage-based progress calculation\n    storage.py                  ← Redis/PostgreSQL hybrid storage\n    notifications.py            ← webhook notification system  \n    models.py                   ← progress data structures\n    webhooks/\n      __init__.py\n      delivery.py               ← reliable webhook delivery\n      retry.py                  ← exponential backoff logic\n      security.py               ← signature generation/verification\n    tests/\n      test_tracker.py           ← progress tracking tests\n      test_webhooks.py          ← webhook delivery tests\n      test_storage.py           ← storage integration tests\n  queue/\n    worker.py                   ← imports progress.tracker\n  main.py                       ← progress tracking configuration\n```\n\n#### Progress Storage Infrastructure (Complete Implementation)\n\n```python\n# progress/storage.py\nimport json\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Dict, List, Optional, Any\nimport redis\nimport psycopg2\nfrom psycopg2.extras import RealDictCursor\nimport logging\nfrom dataclasses import asdict\n\nfrom .models import JobProgress, ProgressUpdate\n\nlogger = logging.getLogger(__name__)\n\nclass ProgressStorage:\n    \"\"\"\n    Hybrid Redis + PostgreSQL storage for job progress tracking.\n    Redis provides real-time performance for active jobs.\n    PostgreSQL ensures durability and supports complex queries.\n    \"\"\"\n    \n    def __init__(self, redis_client: redis.Redis, pg_conn_string: str):\n        self.redis = redis_client\n        self.pg_conn_string = pg_conn_string\n        self._setup_schema()\n    \n    def _setup_schema(self):\n        \"\"\"Initialize PostgreSQL schema for progress persistence.\"\"\"\n        with psycopg2.connect(self.pg_conn_string) as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS job_progress (\n                        job_id VARCHAR(255) PRIMARY KEY,\n                        percentage FLOAT NOT NULL DEFAULT 0.0,\n                        stage VARCHAR(100) NOT NULL DEFAULT 'pending',\n                        stage_progress FLOAT NOT NULL DEFAULT 0.0,\n                        updated_at TIMESTAMP WITH TIME ZONE NOT NULL,\n                        estimated_completion TIMESTAMP WITH TIME ZONE,\n                        sequence_number BIGINT NOT NULL,\n                        details JSONB\n                    );\n                    \n                    CREATE TABLE IF NOT EXISTS progress_history (\n                        id SERIAL PRIMARY KEY,\n                        job_id VARCHAR(255) NOT NULL,\n                        percentage FLOAT NOT NULL,\n                        stage VARCHAR(100) NOT NULL,\n                        recorded_at TIMESTAMP WITH TIME ZONE NOT NULL,\n                        details JSONB\n                    );\n                    \n                    CREATE INDEX IF NOT EXISTS idx_progress_history_job_time \n                        ON progress_history(job_id, recorded_at);\n                \"\"\")\n                conn.commit()\n\n    def update_progress(self, job_id: str, progress: JobProgress) -> bool:\n        \"\"\"\n        Atomically update job progress in Redis with PostgreSQL backup.\n        Returns True if update succeeded, False if rejected (out of order).\n        \"\"\"\n        sequence_num = int(time.time() * 1000000)  # microsecond timestamp\n        \n        # Atomic update with sequence number validation\n        pipe = self.redis.pipeline()\n        pipe.watch(f\"progress:job:{job_id}\")\n        \n        try:\n            current_data = pipe.hgetall(f\"progress:job:{job_id}\")\n            current_seq = int(current_data.get('sequence_number', 0))\n            \n            if sequence_num <= current_seq:\n                logger.warning(f\"Out of order progress update for job {job_id}: \"\n                             f\"new={sequence_num}, current={current_seq}\")\n                return False\n            \n            pipe.multi()\n            progress_dict = asdict(progress)\n            progress_dict['sequence_number'] = sequence_num\n            progress_dict['updated_at'] = datetime.now(timezone.utc).isoformat()\n            \n            pipe.hset(f\"progress:job:{job_id}\", mapping=progress_dict)\n            pipe.zadd(f\"progress:history:{job_id}\", \n                     {json.dumps(progress_dict): sequence_num})\n            pipe.expire(f\"progress:job:{job_id}\", 86400)  # 24 hour TTL\n            pipe.expire(f\"progress:history:{job_id}\", 86400)\n            pipe.publish(f\"progress:updates\", \n                        json.dumps({\"job_id\": job_id, **progress_dict}))\n            \n            pipe.execute()\n            \n            # Async backup to PostgreSQL every 10% progress\n            if progress.percentage % 10.0 < 1.0:\n                self._backup_to_postgres(job_id, progress, sequence_num)\n            \n            return True\n            \n        except redis.WatchError:\n            logger.info(f\"Concurrent update detected for job {job_id}, retrying\")\n            return False\n        finally:\n            pipe.reset()\n\n    def get_progress(self, job_id: str) -> Optional[JobProgress]:\n        \"\"\"Retrieve current progress for job from Redis or PostgreSQL.\"\"\"\n        # Try Redis first for active jobs\n        data = self.redis.hgetall(f\"progress:job:{job_id}\")\n        if data:\n            return JobProgress(\n                job_id=job_id,\n                percentage=float(data['percentage']),\n                stage=data['stage'],\n                stage_progress=float(data['stage_progress']),\n                details=json.loads(data.get('details', '{}'))\n            )\n        \n        # Fallback to PostgreSQL for completed jobs\n        with psycopg2.connect(self.pg_conn_string) as conn:\n            with conn.cursor(cursor_factory=RealDictCursor) as cur:\n                cur.execute(\"\"\"\n                    SELECT percentage, stage, stage_progress, details, updated_at\n                    FROM job_progress WHERE job_id = %s\n                \"\"\", (job_id,))\n                row = cur.fetchone()\n                \n                if row:\n                    return JobProgress(\n                        job_id=job_id,\n                        percentage=row['percentage'],\n                        stage=row['stage'], \n                        stage_progress=row['stage_progress'],\n                        details=row['details'] or {}\n                    )\n        \n        return None\n\n    def _backup_to_postgres(self, job_id: str, progress: JobProgress, \n                           sequence_num: int):\n        \"\"\"Asynchronously backup progress to PostgreSQL.\"\"\"\n        try:\n            with psycopg2.connect(self.pg_conn_string) as conn:\n                with conn.cursor() as cur:\n                    cur.execute(\"\"\"\n                        INSERT INTO job_progress \n                        (job_id, percentage, stage, stage_progress, updated_at, \n                         sequence_number, details)\n                        VALUES (%s, %s, %s, %s, %s, %s, %s)\n                        ON CONFLICT (job_id) DO UPDATE SET\n                            percentage = EXCLUDED.percentage,\n                            stage = EXCLUDED.stage,\n                            stage_progress = EXCLUDED.stage_progress,\n                            updated_at = EXCLUDED.updated_at,\n                            sequence_number = EXCLUDED.sequence_number,\n                            details = EXCLUDED.details\n                    \"\"\", (job_id, progress.percentage, progress.stage,\n                          progress.stage_progress, datetime.now(timezone.utc),\n                          sequence_num, json.dumps(progress.details)))\n                    \n                    # Archive progress milestone to history\n                    cur.execute(\"\"\"\n                        INSERT INTO progress_history \n                        (job_id, percentage, stage, recorded_at, details)\n                        VALUES (%s, %s, %s, %s, %s)\n                    \"\"\", (job_id, progress.percentage, progress.stage,\n                          datetime.now(timezone.utc), json.dumps(progress.details)))\n                    \n                conn.commit()\n        except Exception as e:\n            logger.error(f\"Failed to backup progress for job {job_id}: {e}\")\n\n    def cleanup_completed_job(self, job_id: str):\n        \"\"\"Remove Redis data for completed job to prevent memory leaks.\"\"\"\n        pipe = self.redis.pipeline()\n        pipe.delete(f\"progress:job:{job_id}\")\n        pipe.delete(f\"progress:history:{job_id}\")\n        pipe.execute()\n```\n\n#### Webhook Security and Signature Verification (Complete Implementation)\n\n```python\n# progress/webhooks/security.py\nimport hmac\nimport hashlib\nimport time\nimport json\nfrom typing import Dict, Any, Optional\n\nclass WebhookSecurity:\n    \"\"\"\n    Provides HMAC signature generation and verification for webhook security.\n    Prevents tampering and replay attacks through cryptographic validation.\n    \"\"\"\n    \n    @staticmethod\n    def generate_signature(payload: str, secret: str) -> str:\n        \"\"\"\n        Generate HMAC-SHA256 signature for webhook payload.\n        \n        Args:\n            payload: Raw JSON payload string\n            secret: Webhook endpoint secret key\n            \n        Returns:\n            Hex-encoded signature for X-Webhook-Signature header\n        \"\"\"\n        signature = hmac.new(\n            secret.encode('utf-8'),\n            payload.encode('utf-8'), \n            hashlib.sha256\n        ).hexdigest()\n        \n        return f\"sha256={signature}\"\n    \n    @staticmethod\n    def verify_signature(payload: str, signature: str, secret: str, \n                        max_age: int = 300) -> bool:\n        \"\"\"\n        Verify webhook signature and timestamp to prevent replay attacks.\n        \n        Args:\n            payload: Raw request body string\n            signature: X-Webhook-Signature header value\n            secret: Webhook endpoint secret\n            max_age: Maximum message age in seconds (default 5 minutes)\n            \n        Returns:\n            True if signature valid and timestamp fresh\n        \"\"\"\n        if not signature.startswith('sha256='):\n            return False\n            \n        expected_signature = WebhookSecurity.generate_signature(payload, secret)\n        \n        # Constant-time comparison prevents timing attacks\n        if not hmac.compare_digest(signature, expected_signature):\n            return False\n        \n        # Verify timestamp to prevent replay attacks\n        try:\n            data = json.loads(payload)\n            event_timestamp = data.get('event_timestamp')\n            if event_timestamp:\n                event_time = time.mktime(time.strptime(\n                    event_timestamp, '%Y-%m-%dT%H:%M:%S.%fZ'\n                ))\n                if time.time() - event_time > max_age:\n                    return False\n        except (json.JSONDecodeError, ValueError, KeyError):\n            return False\n            \n        return True\n\n    @staticmethod\n    def create_webhook_payload(event_type: str, job_data: Dict[str, Any], \n                              payload_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Create standardized webhook payload with security metadata.\n        \n        Args:\n            event_type: Type of event (job.started, job.completed, etc.)\n            job_data: Job information dictionary  \n            payload_data: Event-specific payload data\n            \n        Returns:\n            Complete webhook payload ready for signing and delivery\n        \"\"\"\n        import uuid\n        \n        return {\n            \"event_id\": str(uuid.uuid4()),\n            \"event_type\": event_type,\n            \"event_timestamp\": time.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),\n            \"api_version\": \"v1\",\n            \"job\": job_data,\n            \"payload\": payload_data\n        }\n```\n\n#### Core Progress Tracking Logic (Implementation Skeleton)\n\n```python\n# progress/tracker.py\nfrom typing import Dict, List, Optional, Callable\nimport logging\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nfrom .storage import ProgressStorage  \nfrom .calculator import ProgressCalculator\nfrom .notifications import WebhookNotifier\nfrom .models import JobProgress, ProcessingStage\n\nlogger = logging.getLogger(__name__)\n\nclass ProgressTracker:\n    \"\"\"\n    Central progress tracking coordinator that manages stage-based progress\n    calculation, storage persistence, and webhook notification delivery.\n    \"\"\"\n    \n    def __init__(self, storage: ProgressStorage, calculator: ProgressCalculator,\n                 notifier: WebhookNotifier):\n        self.storage = storage\n        self.calculator = calculator\n        self.notifier = notifier\n        self.notification_thresholds = [25.0, 50.0, 75.0, 90.0, 100.0]\n\n    def initialize_job_progress(self, job_id: str, total_stages: List[str],\n                               webhook_url: Optional[str] = None) -> bool:\n        \"\"\"\n        Initialize progress tracking for new processing job.\n        \n        Args:\n            job_id: Unique job identifier\n            total_stages: List of processing stages in order\n            webhook_url: Optional webhook URL for notifications\n            \n        Returns:\n            True if initialization successful\n        \"\"\"\n        # TODO 1: Create initial JobProgress object with 0% completion\n        # TODO 2: Set stage to first stage in total_stages list  \n        # TODO 3: Initialize stage_progress to 0.0 and details to empty dict\n        # TODO 4: Store initial progress using self.storage.update_progress()\n        # TODO 5: Register webhook URL with notifier if provided\n        # TODO 6: Send job.started webhook notification if webhook_url configured\n        # Hint: Use self.calculator.initialize_stage_weights(total_stages)\n        pass\n\n    def update_stage_progress(self, job_id: str, stage: str, \n                             stage_percentage: float, \n                             details: Optional[Dict] = None) -> bool:\n        \"\"\"\n        Update progress within current processing stage.\n        \n        Args:\n            job_id: Job identifier\n            stage: Current processing stage name\n            stage_percentage: Completion percentage within stage (0-100)\n            details: Optional additional progress details\n            \n        Returns:\n            True if update successful and within monotonic constraints\n        \"\"\"\n        # TODO 1: Validate stage_percentage is between 0.0 and 100.0\n        # TODO 2: Get current progress from storage to check stage consistency\n        # TODO 3: Use calculator to compute overall job percentage from stage progress\n        # TODO 4: Validate new percentage >= current percentage (monotonic progression)\n        # TODO 5: Create updated JobProgress object with new values\n        # TODO 6: Store updated progress atomically using storage layer\n        # TODO 7: Check if notification threshold crossed since last webhook\n        # TODO 8: Trigger async webhook delivery if threshold met\n        # Hint: self.calculator.calculate_overall_progress(stage, stage_percentage)\n        pass\n\n    def advance_to_next_stage(self, job_id: str, next_stage: str,\n                             completion_details: Optional[Dict] = None) -> bool:\n        \"\"\"\n        Transition job to next processing stage with 100% completion of current stage.\n        \n        Args:\n            job_id: Job identifier  \n            next_stage: Name of next processing stage\n            completion_details: Optional details about completed stage\n            \n        Returns:\n            True if stage transition successful\n        \"\"\"\n        # TODO 1: Get current progress and validate job exists\n        # TODO 2: Mark current stage as 100% complete with stage completion details\n        # TODO 3: Validate next_stage is valid in the configured stage sequence\n        # TODO 4: Calculate new overall percentage with current stage complete\n        # TODO 5: Create JobProgress for new stage with 0% stage progress\n        # TODO 6: Update storage with new stage and overall progress\n        # TODO 7: Check for notification thresholds and send webhooks if needed\n        # Hint: Use self.calculator.get_stage_transition_percentage(next_stage)\n        pass\n\n    def mark_job_completed(self, job_id: str, output_files: List[str],\n                          processing_duration: float) -> bool:\n        \"\"\"\n        Mark job as 100% complete and send final webhook notification.\n        \n        Args:\n            job_id: Job identifier\n            output_files: List of generated output file paths\n            processing_duration: Total processing time in seconds\n            \n        Returns:\n            True if completion marking successful\n        \"\"\"\n        # TODO 1: Set job progress to 100% with 'completed' stage\n        # TODO 2: Include output_files and processing_duration in progress details  \n        # TODO 3: Store final progress state with completion timestamp\n        # TODO 4: Send job.completed webhook with output file information\n        # TODO 5: Clean up Redis progress data to prevent memory leaks\n        # TODO 6: Log completion event with duration and output file count\n        # Hint: self.storage.cleanup_completed_job(job_id) for memory management\n        pass\n\n    def mark_job_failed(self, job_id: str, error_message: str,\n                       retry_eligible: bool = False) -> bool:\n        \"\"\"\n        Mark job as permanently failed or eligible for retry.\n        \n        Args:\n            job_id: Job identifier\n            error_message: Detailed error description\n            retry_eligible: Whether job can be retried automatically\n            \n        Returns:\n            True if failure marking successful\n        \"\"\"\n        # TODO 1: Get current progress to preserve stage and percentage information\n        # TODO 2: Create failed JobProgress with error details and retry status\n        # TODO 3: Store failure state with error message and timestamp\n        # TODO 4: Send job.failed webhook with error details and retry information\n        # TODO 5: Clean up Redis data if failure is permanent (not retryable)\n        # TODO 6: Log failure event with error classification for monitoring\n        # Hint: Include current stage in failure details for debugging context\n        pass\n```\n\n#### Stage-Based Progress Calculation (Implementation Skeleton)\n\n```python\n# progress/calculator.py\nfrom typing import Dict, List, Optional\nfrom enum import Enum\n\nclass ProcessingStage(Enum):\n    \"\"\"Standard processing stages with predefined weight allocations.\"\"\"\n    VALIDATION = (\"validation\", 5.0)\n    PREPROCESSING = (\"preprocessing\", 10.0) \n    RESIZE_OPERATIONS = (\"resize_operations\", 40.0)\n    FORMAT_CONVERSION = (\"format_conversion\", 35.0)\n    OPTIMIZATION = (\"optimization\", 8.0)\n    FINALIZATION = (\"finalization\", 2.0)\n\nclass ProgressCalculator:\n    \"\"\"\n    Calculates overall job progress based on stage completion and weights.\n    Ensures monotonic progress and accurate percentage computation.\n    \"\"\"\n    \n    def __init__(self):\n        self.stage_weights: Dict[str, float] = {}\n        self.stage_order: List[str] = []\n\n    def initialize_stage_weights(self, stages: List[str], \n                                custom_weights: Optional[Dict[str, float]] = None) -> bool:\n        \"\"\"\n        Configure stage weights for progress calculation.\n        \n        Args:\n            stages: Ordered list of processing stages\n            custom_weights: Optional custom weight allocation per stage\n            \n        Returns:\n            True if weights total 100% and are valid\n        \"\"\"\n        # TODO 1: Use custom_weights if provided, otherwise use default ProcessingStage weights\n        # TODO 2: Validate that all stages in list have corresponding weight values\n        # TODO 3: Verify total weights sum to exactly 100.0 (within floating point tolerance)\n        # TODO 4: Store stage_order list to track stage progression sequence  \n        # TODO 5: Store stage_weights dict for percentage calculations\n        # TODO 6: Return False if weight validation fails, True on success\n        # Hint: sum(weights.values()) should equal 100.0 ± 0.01 for floating point comparison\n        pass\n\n    def calculate_overall_progress(self, current_stage: str, \n                                  stage_progress: float) -> float:\n        \"\"\"\n        Calculate overall job progress percentage based on current stage position.\n        \n        Args:\n            current_stage: Name of currently executing stage\n            stage_progress: Completion percentage within current stage (0-100)\n            \n        Returns:\n            Overall job completion percentage (0-100)\n        \"\"\"\n        # TODO 1: Validate current_stage exists in configured stage_order\n        # TODO 2: Calculate completed percentage from all previous stages\n        # TODO 3: Add weighted progress from current stage: (stage_weight * stage_progress / 100)\n        # TODO 4: Ensure result stays within 0-100 bounds\n        # TODO 5: Return total percentage with appropriate precision (1 decimal place)\n        # Hint: Use self.stage_order.index(current_stage) to find position\n        pass\n\n    def get_stage_transition_percentage(self, next_stage: str) -> float:\n        \"\"\"\n        Calculate progress percentage when transitioning to next stage.\n        \n        Args:\n            next_stage: Stage being transitioned to\n            \n        Returns:\n            Progress percentage with all previous stages 100% complete\n        \"\"\"\n        # TODO 1: Find index of next_stage in stage_order list\n        # TODO 2: Sum weights of all stages before next_stage (these are 100% complete)\n        # TODO 3: Return cumulative percentage for completed stages\n        # TODO 4: Validate result is reasonable (should be < 100% unless final stage)\n        # Hint: This shows progress at start of next_stage with previous stages done\n        pass\n\n    def estimate_remaining_time(self, current_progress: float,\n                               elapsed_time: float) -> Optional[float]:\n        \"\"\"\n        Estimate remaining processing time based on current progress rate.\n        \n        Args:\n            current_progress: Current job completion percentage  \n            elapsed_time: Time elapsed since job started (seconds)\n            \n        Returns:\n            Estimated remaining time in seconds, or None if insufficient data\n        \"\"\"\n        # TODO 1: Validate current_progress > 0 to avoid division by zero\n        # TODO 2: Calculate processing rate: elapsed_time / current_progress\n        # TODO 3: Estimate total time: processing_rate * 100.0\n        # TODO 4: Calculate remaining time: total_time - elapsed_time\n        # TODO 5: Return None if estimate seems unreasonable (negative or > 24 hours)\n        # Hint: Add safety margin (15-25%) to account for varying stage complexity\n        pass\n```\n\n#### Milestone Checkpoints\n\nAfter implementing the progress tracking system, verify functionality through these checkpoints:\n\n**Checkpoint 1: Basic Progress Updates**\n```bash\n# Test progress tracking with sample job\npython -m pytest progress/tests/test_tracker.py::test_stage_progression -v\n\n# Expected behavior:\n# - Job initializes with 0% progress at first stage\n# - Stage progress updates increment overall percentage correctly  \n# - Stage transitions advance to next stage with proper percentage\n# - Progress updates maintain monotonic progression (never go backward)\n```\n\n**Checkpoint 2: Webhook Notification Delivery**\n```bash\n# Start webhook test server to receive notifications\npython progress/tests/webhook_test_server.py --port 8080 &\n\n# Run webhook delivery tests\npython -m pytest progress/tests/test_webhooks.py::test_delivery_with_retry -v\n\n# Verify webhook server receives:\n# - job.started notification when job begins\n# - job.progress notifications at 25%, 50%, 75% thresholds  \n# - job.completed notification with output file URLs\n# - Valid HMAC signatures on all webhook payloads\n```\n\n**Checkpoint 3: Concurrent Progress Updates**\n```bash\n# Test concurrent worker progress updates\npython progress/tests/test_concurrency.py --workers 5 --job-count 10\n\n# Expected behavior:\n# - Multiple workers update same job without race conditions\n# - Progress values remain monotonic despite concurrent updates\n# - No progress reversals or inconsistent percentages\n# - Redis sequence numbers prevent out-of-order updates\n```\n\n**Signs of Implementation Issues:**\n- **Progress jumps backward**: Check sequence number implementation and Redis atomic operations\n- **Webhook delivery failures**: Verify signature generation and retry logic configuration\n- **Memory usage growth**: Confirm cleanup processes remove completed job data from Redis\n- **Inaccurate percentages**: Validate stage weight calculations sum to exactly 100%\n\n\n## Component Interactions and Data Flow\n\n> **Milestone(s):** All milestones (1-3) as this section details the communication patterns between image processing, video transcoding, job queue, and progress tracking components\n\n### Mental Model: Orchestra Performance\n\nThink of the media processing pipeline as a symphony orchestra performing a complex musical piece. The **API Gateway** acts as the conductor, receiving requests from the audience (clients) and coordinating the entire performance. The **job queue** serves as the sheet music distribution system, ensuring each musician (worker process) knows what to play and when. Individual **worker processes** are like specialized musicians - some excel at string instruments (image processing), others at brass (video transcoding), each following their part while contributing to the overall performance.\n\nThe **progress tracking system** functions like the concert program that audience members follow, providing real-time updates about which movement is currently playing and how much remains. **Webhook notifications** are like the applause cues in the program - they signal important moments to the audience at precisely the right time. Just as musicians must stay synchronized through visual cues and timing, our components communicate through well-defined message formats and sequenced interactions to produce a harmonious result.\n\nThis orchestration requires precise timing, clear communication channels, and graceful recovery when a musician (component) encounters difficulties. The conductor doesn't need to know how to play every instrument, but must understand how each section contributes to the whole performance and coordinate their interactions seamlessly.\n\n### API Interfaces and Contracts\n\nThe API layer serves as the primary interface between external clients and the internal media processing system, providing REST endpoints that abstract the complexity of job submission, status monitoring, and result retrieval. These interfaces follow RESTful principles while accommodating the asynchronous nature of media processing operations.\n\n#### Job Submission Endpoint\n\nThe job submission endpoint receives media processing requests and returns immediately with a job identifier, enabling clients to track processing asynchronously. This endpoint accepts multipart form data containing the input file and processing specifications, validating inputs before queue submission.\n\n| Method | Endpoint | Request Format | Response Format | Status Codes |\n|--------|----------|---------------|-----------------|--------------|\n| POST | `/jobs` | `multipart/form-data` with file and JSON config | JSON with `job_id`, `status`, `estimated_duration` | 201 Created, 400 Bad Request, 413 Payload Too Large, 422 Unprocessable Entity |\n| GET | `/jobs/{job_id}` | None | JSON with `ProcessingJob` details | 200 OK, 404 Not Found |\n| GET | `/jobs/{job_id}/progress` | None | JSON with `JobProgress` details | 200 OK, 404 Not Found |\n| DELETE | `/jobs/{job_id}` | None | JSON confirmation | 200 OK, 404 Not Found, 409 Conflict if processing |\n\nThe request payload structure accommodates multiple output specifications within a single job, allowing clients to request multiple formats, resolutions, or quality variants in one submission:\n\n| Field Name | Type | Required | Description | Validation Rules |\n|------------|------|----------|-------------|------------------|\n| `input_file` | File | Yes | Media file to process | Max 500MB, supported MIME types only |\n| `output_specifications` | JSON Array | Yes | List of desired outputs | 1-10 specifications per job |\n| `priority` | String | No | Job priority level | One of: low, normal, high, urgent |\n| `webhook_url` | URL | No | Callback URL for notifications | Must be HTTPS, reachable endpoint |\n| `metadata_handling` | String | No | EXIF/metadata preservation mode | strip, preserve, selective |\n| `processing_options` | JSON Object | No | Advanced processing parameters | Format-specific validation |\n\n#### Response Data Structures\n\nAPI responses provide comprehensive job information while maintaining consistency across different endpoint variations. The response format adapts to include relevant details based on the endpoint and job state.\n\n| Response Field | Type | Always Present | Description | Example Values |\n|----------------|------|---------------|-------------|----------------|\n| `job_id` | String | Yes | Unique job identifier | `img_20231215_143022_abc123` |\n| `status` | Enum | Yes | Current job status | `pending`, `processing`, `completed`, `failed` |\n| `created_at` | ISO8601 | Yes | Job submission timestamp | `2023-12-15T14:30:22Z` |\n| `started_at` | ISO8601 | If started | Processing start time | `2023-12-15T14:30:25Z` |\n| `completed_at` | ISO8601 | If terminal | Processing completion time | `2023-12-15T14:32:18Z` |\n| `progress_percentage` | Float | If processing | Overall completion percentage | `67.5` |\n| `current_stage` | String | If processing | Current processing phase | `resize_operations` |\n| `estimated_duration` | Integer | If processing | Remaining seconds estimate | `45` |\n| `output_files` | Array | If completed | Generated file download URLs | `[\"/downloads/thumb.jpg\"]` |\n| `error_message` | String | If failed | Human-readable error description | `Unsupported codec in input video` |\n| `retry_count` | Integer | Yes | Number of retry attempts made | `2` |\n\n> **Design Insight**: The API maintains idempotency by accepting optional `idempotency_key` headers. If a client submits the same key within 24 hours, the system returns the existing job rather than creating a duplicate. This prevents accidental duplicate processing when clients retry failed requests.\n\n#### Content Negotiation and Format Support\n\nThe API supports multiple response formats and content negotiation to accommodate different client requirements and integration patterns. Clients can request JSON, XML, or abbreviated formats based on their capabilities.\n\n| Accept Header | Response Format | Use Case | Content Type |\n|---------------|----------------|----------|--------------|\n| `application/json` | Full JSON with all fields | Web applications, modern APIs | `application/json; charset=utf-8` |\n| `application/json; compact=true` | Minimal JSON with essential fields | Mobile apps, bandwidth-limited | `application/json; charset=utf-8` |\n| `application/xml` | XML format for legacy systems | Enterprise integration | `application/xml; charset=utf-8` |\n| `text/plain` | Simple status text | Command-line tools, monitoring | `text/plain; charset=utf-8` |\n\n**Decision: REST over GraphQL for API Design**\n- **Context**: Need to choose API paradigm for client communication with varying complexity requirements\n- **Options Considered**: REST with JSON, GraphQL, gRPC with Protocol Buffers\n- **Decision**: REST with JSON as primary interface\n- **Rationale**: Media processing APIs have simple, resource-oriented operations (submit job, check status, retrieve results). REST provides excellent caching, is universally supported, and matches the async processing model naturally. GraphQL adds complexity without significant benefit for this domain.\n- **Consequences**: Enables standard HTTP caching, simple client integration, but requires multiple requests for complex queries. Webhook notifications complement REST's limitations for real-time updates.\n\n#### Error Response Format\n\nError responses follow RFC 7807 Problem Details format, providing structured error information that clients can programmatically handle while remaining human-readable for debugging purposes.\n\n| Error Field | Type | Description | Example |\n|-------------|------|-------------|---------|\n| `type` | URI | Problem type identifier | `/errors/unsupported-format` |\n| `title` | String | Short error summary | `Unsupported Media Format` |\n| `status` | Integer | HTTP status code | `422` |\n| `detail` | String | Detailed error explanation | `WebM container with AV1 codec not supported` |\n| `instance` | URI | Specific error instance | `/jobs/video_20231215_143022_xyz789` |\n| `timestamp` | ISO8601 | Error occurrence time | `2023-12-15T14:30:23Z` |\n| `request_id` | String | Request correlation ID | `req_abc123def456` |\n\n### Internal Message Formats\n\nInternal communication between components uses structured message formats that ensure reliable delivery, enable monitoring, and support system evolution. These messages flow through Redis-based queues with JSON serialization for cross-language compatibility.\n\n#### Job Queue Message Structure\n\nJob queue messages contain all information necessary for worker processes to execute media processing tasks independently. The message format balances completeness with serialization efficiency, supporting both immediate processing and delayed execution scenarios.\n\n| Message Field | Type | Description | Serialization Notes |\n|---------------|------|-------------|-------------------|\n| `job_id` | String | Unique job identifier | Used for correlation across all systems |\n| `message_type` | Enum | Message type discriminator | `PROCESS_MEDIA`, `CANCEL_JOB`, `HEARTBEAT` |\n| `priority` | Integer | Numeric priority for queue ordering | Higher numbers processed first |\n| `input_file_path` | String | Absolute path to input media file | Must be accessible to all workers |\n| `output_specifications` | Array | List of `OutputSpecification` objects | Serialized as nested JSON |\n| `processing_config` | Object | Processing parameters and constraints | Contains format-specific settings |\n| `webhook_url` | String | Callback URL for progress notifications | Optional, null if no notifications |\n| `metadata_options` | Object | EXIF and metadata handling preferences | Controls privacy and compatibility |\n| `retry_policy` | Object | Retry behavior configuration | Max attempts, backoff strategy |\n| `resource_constraints` | Object | Memory and CPU limits for processing | Helps with worker assignment |\n| `correlation_id` | String | Request tracking across components | Links API request to internal operations |\n| `created_at` | Integer | Unix timestamp of message creation | Used for TTL and aging policies |\n| `timeout_at` | Integer | Unix timestamp when job expires | Prevents infinite queue retention |\n\nThe `OutputSpecification` objects within job messages contain format-specific parameters that workers use to configure processing operations:\n\n| Output Spec Field | Type | Description | Format Applicability |\n|-------------------|------|-------------|---------------------|\n| `output_path` | String | Target file path for generated output | All formats |\n| `format` | String | Target format identifier | `jpeg`, `png`, `webp`, `mp4`, `webm` |\n| `width` | Integer | Target width in pixels | Images and videos |\n| `height` | Integer | Target height in pixels | Images and videos |\n| `quality` | Integer | Quality level (1-100) | Lossy formats only |\n| `optimization_level` | Integer | Processing effort vs speed tradeoff | Format-dependent scale |\n| `codec_settings` | Object | Format-specific encoding parameters | Videos: bitrate, CRF, preset |\n| `color_profile` | String | ICC color profile to apply | Images with color management |\n\n> **Architecture Decision**: Messages include complete processing specifications rather than references to external configuration. This design ensures workers can process jobs independently without additional database lookups, improving reliability and reducing latency. The tradeoff is larger message sizes, but media processing jobs are inherently heavy operations where message overhead is negligible.\n\n#### Progress Update Messages\n\nProgress update messages flow from worker processes to the progress tracking system, providing real-time visibility into job execution. These messages support both incremental updates within processing stages and major stage transitions.\n\n| Progress Field | Type | Description | Update Frequency |\n|----------------|------|-------------|-----------------|\n| `job_id` | String | Job identifier for correlation | Every message |\n| `message_type` | String | Update type discriminator | `STAGE_PROGRESS`, `STAGE_COMPLETE`, `JOB_COMPLETE` |\n| `overall_percentage` | Float | Total job completion percentage (0-100) | Every update |\n| `current_stage` | String | Processing stage name | Changes on stage transitions |\n| `stage_percentage` | Float | Completion within current stage (0-100) | Frequent updates during processing |\n| `stage_details` | Object | Stage-specific progress information | Contains operation-specific data |\n| `estimated_remaining` | Integer | Estimated seconds to completion | Recalculated on each update |\n| `worker_id` | String | Identifier of processing worker | For debugging and load analysis |\n| `timestamp` | Integer | Unix timestamp of progress measurement | Enables progress rate calculation |\n| `sequence_number` | Integer | Monotonically increasing sequence | Prevents out-of-order updates |\n| `resource_usage` | Object | Current memory and CPU utilization | Optional, for monitoring |\n\nThe `stage_details` object provides operation-specific progress information that varies by processing type:\n\n| Stage Detail Field | Type | Image Processing | Video Transcoding |\n|--------------------|------|------------------|-------------------|\n| `current_operation` | String | `resize`, `format_convert` | `encode_video`, `extract_audio` |\n| `processed_items` | Integer | Number of output variants completed | Number of segments processed |\n| `total_items` | Integer | Total output variants requested | Total segments in video |\n| `current_resolution` | String | `1920x1080` | `1280x720` |\n| `bitrate_kbps` | Integer | N/A (images) | Current encoding bitrate |\n| `fps_processed` | Float | N/A (images) | Frames processed per second |\n| `temp_file_size` | Integer | Intermediate file size in bytes | Current output file size |\n\n#### Error and Notification Messages\n\nError messages provide detailed failure information for debugging and recovery decisions. The message format distinguishes between transient errors (suitable for retry) and permanent failures (requiring human intervention or job cancellation).\n\n| Error Message Field | Type | Description | Recovery Usage |\n|---------------------|------|-------------|----------------|\n| `job_id` | String | Failed job identifier | Links error to specific job |\n| `error_type` | String | Error category | `INPUT_ERROR`, `PROCESSING_ERROR`, `RESOURCE_ERROR` |\n| `error_code` | String | Specific error identifier | `UNSUPPORTED_CODEC`, `MEMORY_LIMIT_EXCEEDED` |\n| `error_message` | String | Human-readable error description | Displayed to end users |\n| `technical_details` | String | Detailed technical information | For debugging and troubleshooting |\n| `retry_eligible` | Boolean | Whether error is potentially transient | Controls automatic retry behavior |\n| `failed_stage` | String | Processing stage where error occurred | Helps isolate problem area |\n| `worker_id` | String | Worker that encountered the error | For worker health monitoring |\n| `input_file_info` | Object | Information about input file | Helps identify problematic inputs |\n| `resource_state` | Object | System resource state at failure | Memory, disk space, CPU load |\n| `stack_trace` | String | Exception stack trace if available | Development and debugging |\n| `suggested_action` | String | Recommended resolution steps | Guides manual intervention |\n\n**Decision: JSON Message Serialization**\n- **Context**: Need serialization format for inter-component communication supporting multiple languages\n- **Options Considered**: JSON, Protocol Buffers, MessagePack, Apache Avro\n- **Decision**: JSON with schema validation\n- **Rationale**: JSON provides human readability for debugging, universal language support, and flexible schema evolution. Media processing messages are relatively infrequent compared to processing time, so serialization performance is not critical. Schema validation catches format errors early.\n- **Consequences**: Enables easy debugging and monitoring, but larger message sizes than binary formats. Schema evolution requires careful compatibility management.\n\n### Processing Sequence Scenarios\n\nThe media processing pipeline orchestrates complex sequences of operations across multiple components, with different workflows for image processing, video transcoding, and batch operations. These scenarios demonstrate how components interact and coordinate through the message passing and state management systems.\n\n#### Image Processing Workflow Sequence\n\nImage processing jobs follow a predictable sequence from submission through completion, with progress tracking and error handling at each stage. This workflow optimizes for quality while maintaining reasonable processing times for typical web and mobile image requirements.\n\n![Image Processing Workflow](./diagrams/image-processing-flow.svg)\n\n**Stage 1: Job Submission and Validation (5% of total progress)**\n\n1. **Client submits** image processing request to API Gateway with input file and output specifications\n2. **API Gateway validates** file format, size limits, and output specification parameters\n3. **Job identifier generation** creates unique job ID using timestamp and random suffix format\n4. **Input file storage** moves uploaded file to processing storage location with secure naming\n5. **Job record creation** initializes `ProcessingJob` with status `PENDING` in Redis and PostgreSQL\n6. **Queue submission** publishes job message to high-priority image processing queue\n7. **Response return** sends job ID and initial status to client immediately\n8. **Progress initialization** sets up progress tracking with stage weights and webhook configuration\n\n**Stage 2: Worker Assignment and Preprocessing (10% of total progress)**\n\n1. **Worker selection** occurs when available image processing worker pops job from queue\n2. **Resource reservation** worker claims memory and CPU resources based on job requirements\n3. **Input validation** worker verifies file accessibility, format support, and corruption checks\n4. **Metadata extraction** reads EXIF data, color profiles, dimensions, and technical specifications\n5. **Processing plan** creation determines optimal resize algorithms, format conversion steps, and quality settings\n6. **Progress notification** updates job status to `PROCESSING` and notifies webhook if configured\n7. **Temporary workspace** allocation creates isolated directory for intermediate processing files\n\n**Stage 3: Resize Operations (40% of total progress)**\n\n1. **Image loading** into memory with automatic EXIF orientation correction and color space handling\n2. **Memory optimization** calculates processing chunks if image exceeds available memory limits\n3. **Interpolation algorithm** selection based on resize ratio (upscaling vs downscaling requirements)\n4. **Resize execution** for each output specification with aspect ratio preservation or cropping\n5. **Quality assessment** validates output dimensions and visual quality against requirements\n6. **Progress updates** sent incrementally as each resize variant completes processing\n7. **Intermediate storage** saves resized images to temporary files before format conversion\n\n**Stage 4: Format Conversion and Optimization (35% of total progress)**\n\n1. **Format compatibility** checking ensures target formats support required features (transparency, color depth)\n2. **Compression optimization** applies format-specific encoding with quality vs file size balancing\n3. **Color profile** management preserves or converts ICC profiles based on target format capabilities\n4. **Metadata handling** strips or preserves EXIF data according to privacy and compatibility requirements\n5. **Quality validation** verifies output files meet specified quality levels and size constraints\n6. **Progressive encoding** setup for JPEG files to enable progressive download capabilities\n7. **Format-specific optimization** applies WebP/AVIF advanced features or PNG palette optimization\n\n**Stage 5: Finalization and Cleanup (10% of total progress)**\n\n1. **Output validation** confirms all requested variants were generated successfully with correct specifications\n2. **File verification** performs integrity checks and validates that files can be opened by standard readers\n3. **Secure storage** moves completed files to final output location with proper permissions and access controls\n4. **Database updates** marks job as `COMPLETED` with output file paths and processing statistics\n5. **Progress completion** sets overall percentage to 100% and sends final webhook notification\n6. **Resource cleanup** removes temporary files, releases memory, and frees worker for next job\n7. **Metrics recording** logs processing duration, resource usage, and quality metrics for monitoring\n\n> **Critical Timing Consideration**: Image processing typically completes within 5-30 seconds depending on input size and output variants. Progress updates occur every 2-3 seconds during active processing stages, balancing real-time visibility with system overhead. Workers batch multiple small resize operations to minimize progress update frequency.\n\n#### Video Transcoding Workflow Sequence\n\nVideo transcoding represents the most complex and resource-intensive processing scenario, requiring careful coordination between FFmpeg processes, adaptive bitrate variant generation, and long-running progress tracking. This workflow can span minutes to hours depending on video duration and output requirements.\n\n**Stage 1: Video Analysis and Planning (5% of total progress)**\n\n1. **Input validation** verifies video file accessibility, format support, and basic integrity through FFmpeg probe\n2. **Metadata extraction** analyzes video streams, audio tracks, subtitle tracks, and container properties\n3. **Codec compatibility** assessment determines optimal transcoding paths and identifies unsupported features\n4. **Resource estimation** calculates memory requirements, processing time, and temporary storage needs\n5. **Quality ladder** generation creates adaptive bitrate variant specifications based on input resolution\n6. **Processing strategy** selection chooses single-pass vs two-pass encoding based on quality requirements\n7. **Worker assignment** reserves video processing worker with sufficient resources for estimated job duration\n\n**Stage 2: Audio Stream Processing (15% of total progress)**\n\n1. **Audio stream** extraction from input video using FFmpeg with format preservation or conversion\n2. **Audio normalization** applies volume leveling and dynamic range compression for consistent playback\n3. **Multi-bitrate** audio encoding generates variants at different quality levels for adaptive streaming\n4. **Audio sync** verification ensures audio timing alignment is maintained throughout processing\n5. **Codec optimization** applies audio codec-specific settings for bandwidth efficiency and compatibility\n6. **Quality validation** verifies audio output meets bitrate and quality specifications through automated analysis\n\n**Stage 3: Video Stream Transcoding (65% of total progress)**\n\n1. **Keyframe analysis** identifies scene changes and optimal keyframe placement for streaming compatibility\n2. **Encoding initialization** starts FFmpeg processes for each required video quality variant simultaneously\n3. **Progress parsing** extracts completion percentage from FFmpeg output streams using regex pattern matching\n4. **Frame processing** monitoring tracks encoding speed and adjusts resource allocation dynamically\n5. **Quality control** sampling periodically validates output quality against CRF settings and bitrate targets\n6. **Segment generation** for HLS/DASH creates properly aligned segments across all quality variants\n7. **Parallel processing** coordination ensures keyframe alignment across multiple quality streams\n\n**Stage 4: Manifest Generation and Packaging (10% of total progress)**\n\n1. **Stream analysis** examines completed video and audio variants to extract technical specifications\n2. **HLS manifest** creation generates M3U8 playlists with proper segment references and bandwidth declarations\n3. **DASH manifest** generation creates MPD files with adaptation sets and representation metadata\n4. **Segment validation** verifies all media segments are properly formatted and accessible\n5. **Bandwidth testing** confirms actual bitrates match declared values in manifests\n6. **Compatibility verification** ensures manifests work with standard players and CDN requirements\n\n**Stage 5: Thumbnail Extraction and Cleanup (5% of total progress)**\n\n1. **Thumbnail timestamps** calculation identifies optimal frames representing video content variety\n2. **Frame extraction** uses FFmpeg to generate high-quality still images at specified time offsets\n3. **Thumbnail optimization** applies image processing techniques for consistent sizing and quality\n4. **Output verification** confirms all transcoded variants, manifests, and thumbnails are accessible\n5. **Cleanup operations** remove temporary files while preserving final outputs in secure storage\n6. **Completion notification** sends webhook with comprehensive job results and download URLs\n\n> **Resource Management**: Video transcoding can consume 2-8 GB RAM and utilize multiple CPU cores for hours. Workers implement memory monitoring and can pause/resume processing if system resources become constrained. Progress updates occur every 10-15 seconds due to the longer processing duration.\n\n#### Batch Processing Coordination Scenario\n\nBatch processing handles multiple related media files as a single logical operation, requiring coordination across multiple workers while maintaining consistent progress reporting and error handling. This scenario is common for photo album processing or video playlist transcoding.\n\n**Stage 1: Batch Job Decomposition**\n\n1. **Batch validation** confirms all input files are accessible and supported formats\n2. **Dependency analysis** identifies files that can be processed in parallel vs those requiring sequential processing  \n3. **Resource planning** estimates total processing time and memory requirements across all batch items\n4. **Work distribution** creates individual processing jobs for each file while maintaining batch coordination\n5. **Progress aggregation** setup configures batch-level progress calculation from individual job progress\n6. **Error handling** strategy defines batch behavior when individual items fail (fail-fast vs continue)\n\n**Stage 2: Parallel Processing Coordination**\n\n1. **Worker allocation** distributes batch items across available workers based on file types and resource requirements\n2. **Progress consolidation** aggregates progress from individual jobs into overall batch completion percentage\n3. **Error monitoring** tracks failed items and makes retry decisions based on batch error tolerance policy\n4. **Resource balancing** dynamically reassigns work if some workers complete tasks significantly faster than others\n5. **Checkpoint creation** periodically saves batch progress state to enable recovery from system failures\n6. **Completion detection** recognizes when all batch items reach terminal states (completed or permanently failed)\n\n**Stage 3: Batch Finalization**\n\n1. **Results aggregation** collects output files from all successful processing jobs into batch results\n2. **Error reporting** compiles detailed failure information for any items that could not be processed\n3. **Consistency verification** ensures all related outputs (thumbnails, variants) are properly linked\n4. **Batch manifest** generation creates index files linking all generated outputs for easy client consumption\n5. **Cleanup coordination** removes temporary files from all workers that participated in batch processing\n6. **Final notification** delivers comprehensive batch results including partial success scenarios\n\n⚠️ **Pitfall: Race Conditions in Progress Aggregation**\nBatch processing can create race conditions when multiple workers update progress simultaneously. Without proper synchronization, progress percentages may appear to move backwards or skip values. Implement sequence numbers in progress updates and use atomic Redis operations for progress consolidation. Each individual job maintains its own progress sequence, and the batch coordinator uses the latest sequence number from each job to calculate accurate batch progress.\n\n⚠️ **Pitfall: Memory Exhaustion During Video Processing**\nVideo transcoding jobs can easily consume all available system memory, especially when processing multiple large videos simultaneously. Workers must implement memory monitoring and refuse new jobs when memory usage exceeds safe thresholds (typically 80% of available RAM). Use FFmpeg memory limits and temporary file streaming to prevent out-of-memory crashes that corrupt partially processed outputs.\n\n⚠️ **Pitfall: Webhook Delivery Failures**\nWebhook notifications can fail due to network issues, endpoint unavailability, or client-side errors. Without proper retry logic, clients lose visibility into job progress and completion. Implement exponential backoff for webhook retries (1s, 2s, 4s, 8s intervals) with circuit breaker patterns. Store failed webhooks in a dead letter queue for manual review, and provide alternative mechanisms like polling for clients with unreliable webhook endpoints.\n\n⚠️ **Pitfall: Progress Estimation Inaccuracy**\nVideo processing progress is notoriously difficult to estimate accurately because encoding complexity varies dramatically based on video content (static scenes encode faster than high-motion scenes). Avoid time-based progress estimates and instead use stage-based progress with frame counts when possible. For FFmpeg integration, parse the total frame count from initial analysis and calculate progress based on frames processed rather than elapsed time.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| API Framework | Flask with Flask-RESTful | FastAPI with async/await |\n| Message Queue | Redis with basic pub/sub | Redis with Celery task queue |\n| HTTP Client | requests library | aiohttp for async requests |\n| JSON Validation | jsonschema library | Pydantic models with validation |\n| File Upload | werkzeug FileStorage | streaming upload with progress |\n| Authentication | API key headers | JWT tokens with scope validation |\n| Rate Limiting | Flask-Limiter | Redis-based sliding window |\n| Monitoring | Basic logging | Prometheus metrics + Grafana |\n\n#### Recommended Project Structure\n\n```\nmedia-processor/\n  api/\n    __init__.py\n    routes.py              ← REST endpoint definitions\n    models.py              ← Request/response models\n    validation.py          ← Input validation logic\n    middleware.py          ← Authentication, rate limiting\n  messaging/\n    __init__.py\n    queue_manager.py       ← Job queue operations\n    message_formats.py     ← Message serialization\n    progress_notifier.py   ← Webhook delivery system\n  workers/\n    __init__.py\n    base_worker.py         ← Common worker functionality\n    image_worker.py        ← Image processing worker\n    video_worker.py        ← Video transcoding worker\n  storage/\n    __init__.py\n    file_manager.py        ← File I/O and cleanup\n    metadata_store.py      ← Job status persistence\n  config/\n    __init__.py\n    settings.py            ← Configuration management\n    logging.py             ← Logging setup\n  tests/\n    api/                   ← API endpoint tests\n    workers/               ← Worker process tests\n    integration/           ← End-to-end tests\n  docker/\n    api.Dockerfile         ← API server container\n    worker.Dockerfile      ← Worker process container\n  requirements.txt\n  main.py                  ← Application entry point\n```\n\n#### API Server Infrastructure Code\n\n```python\n# api/routes.py - Complete REST API implementation\nfrom flask import Flask, request, jsonify, send_file\nfrom werkzeug.utils import secure_filename\nimport uuid\nimport os\nfrom datetime import datetime\nimport json\n\nfrom messaging.queue_manager import QueueManager\nfrom storage.metadata_store import MetadataStore\nfrom config.settings import AppConfig\n\napp = Flask(__name__)\nconfig = AppConfig()\nqueue_manager = QueueManager(config.redis)\nmetadata_store = MetadataStore(config.storage)\n\n@app.route('/jobs', methods=['POST'])\ndef submit_job():\n    \"\"\"Submit new media processing job with file upload and specifications.\"\"\"\n    # TODO 1: Validate multipart form data contains required fields\n    # TODO 2: Check file size against MAX_FILE_SIZE limit\n    # TODO 3: Validate file format using magic number detection\n    # TODO 4: Parse and validate output_specifications JSON\n    # TODO 5: Generate unique job_id using generate_job_id()\n    # TODO 6: Store uploaded file in secure storage location\n    # TODO 7: Create ProcessingJob object with validated parameters\n    # TODO 8: Submit job to appropriate queue based on media type\n    # TODO 9: Initialize progress tracking for the new job\n    # TODO 10: Return job details with 201 Created status\n    pass\n\n@app.route('/jobs/<job_id>', methods=['GET'])\ndef get_job_status(job_id):\n    \"\"\"Retrieve current job status and details.\"\"\"\n    # TODO 1: Validate job_id format and existence\n    # TODO 2: Load ProcessingJob from metadata store\n    # TODO 3: Get current progress from progress tracker\n    # TODO 4: Format response with job details and progress\n    # TODO 5: Return 404 if job not found\n    pass\n\n@app.route('/jobs/<job_id>/progress', methods=['GET'])\ndef get_job_progress(job_id):\n    \"\"\"Get detailed progress information for active job.\"\"\"\n    # TODO 1: Verify job exists and is in processing state\n    # TODO 2: Retrieve JobProgress from progress tracking system\n    # TODO 3: Calculate stage-specific progress details\n    # TODO 4: Include estimated remaining time if available\n    # TODO 5: Format response with progress breakdown\n    pass\n\n@app.route('/jobs/<job_id>', methods=['DELETE'])\ndef cancel_job(job_id):\n    \"\"\"Cancel pending or processing job.\"\"\"\n    # TODO 1: Load job and verify it can be cancelled\n    # TODO 2: Send cancellation message to worker if processing\n    # TODO 3: Update job status to cancelled in metadata store\n    # TODO 4: Clean up any temporary files\n    # TODO 5: Return confirmation or error if cannot cancel\n    pass\n\n@app.errorhandler(413)\ndef file_too_large(error):\n    \"\"\"Handle file upload size limit exceeded.\"\"\"\n    return jsonify({\n        'type': '/errors/file-too-large',\n        'title': 'File Too Large',\n        'status': 413,\n        'detail': f'File size exceeds maximum limit of {config.storage.max_file_size} bytes',\n        'timestamp': datetime.utcnow().isoformat()\n    }), 413\n\n@app.errorhandler(422)\ndef validation_error(error):\n    \"\"\"Handle request validation failures.\"\"\"\n    return jsonify({\n        'type': '/errors/validation-failed',\n        'title': 'Validation Error',\n        'status': 422,\n        'detail': str(error),\n        'timestamp': datetime.utcnow().isoformat()\n    }), 422\n```\n\n```python\n# messaging/message_formats.py - Message serialization utilities\nimport json\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\n\nclass MessageType(Enum):\n    PROCESS_MEDIA = \"process_media\"\n    PROGRESS_UPDATE = \"progress_update\"\n    JOB_COMPLETE = \"job_complete\"\n    JOB_FAILED = \"job_failed\"\n    CANCEL_JOB = \"cancel_job\"\n\n@dataclass\nclass JobMessage:\n    \"\"\"Complete job message format for queue communication.\"\"\"\n    job_id: str\n    message_type: MessageType\n    priority: int\n    input_file_path: str\n    output_specifications: List[Dict[str, Any]]\n    processing_config: Dict[str, Any]\n    webhook_url: Optional[str] = None\n    metadata_options: Optional[Dict[str, Any]] = None\n    retry_policy: Optional[Dict[str, Any]] = None\n    resource_constraints: Optional[Dict[str, Any]] = None\n    correlation_id: Optional[str] = None\n    created_at: Optional[int] = None\n    timeout_at: Optional[int] = None\n\n    def to_json(self) -> str:\n        \"\"\"Serialize message to JSON string for queue storage.\"\"\"\n        # TODO 1: Convert dataclass to dictionary with enum handling\n        # TODO 2: Format timestamps as Unix timestamps\n        # TODO 3: Handle None values appropriately\n        # TODO 4: Return JSON string with consistent formatting\n        pass\n\n    @classmethod\n    def from_json(cls, json_str: str) -> 'JobMessage':\n        \"\"\"Deserialize message from JSON string.\"\"\"\n        # TODO 1: Parse JSON string to dictionary\n        # TODO 2: Convert string enums back to enum objects\n        # TODO 3: Handle timestamp conversion from Unix format\n        # TODO 4: Create and return JobMessage instance\n        pass\n\n@dataclass\nclass ProgressMessage:\n    \"\"\"Progress update message format.\"\"\"\n    job_id: str\n    message_type: MessageType\n    overall_percentage: float\n    current_stage: str\n    stage_percentage: float\n    stage_details: Dict[str, Any]\n    estimated_remaining: Optional[int] = None\n    worker_id: Optional[str] = None\n    timestamp: Optional[int] = None\n    sequence_number: Optional[int] = None\n    resource_usage: Optional[Dict[str, Any]] = None\n\n    def to_json(self) -> str:\n        \"\"\"Serialize progress message to JSON.\"\"\"\n        # TODO: Implement similar to JobMessage.to_json()\n        pass\n\n    @classmethod\n    def from_json(cls, json_str: str) -> 'ProgressMessage':\n        \"\"\"Deserialize progress message from JSON.\"\"\"\n        # TODO: Implement similar to JobMessage.from_json()\n        pass\n```\n\n#### Queue Management Infrastructure Code\n\n```python\n# messaging/queue_manager.py - Redis-based job queue implementation\nimport redis\nimport json\nimport time\nfrom typing import Optional, List, Dict, Any\nfrom dataclasses import asdict\n\nfrom config.settings import RedisConfig\nfrom messaging.message_formats import JobMessage, MessageType\n\nclass QueueManager:\n    \"\"\"Manages job submission and worker coordination through Redis queues.\"\"\"\n    \n    def __init__(self, redis_config: RedisConfig):\n        self.redis_client = redis.Redis(\n            host=redis_config.host,\n            port=redis_config.port,\n            db=redis_config.db,\n            password=redis_config.password,\n            decode_responses=True\n        )\n        self.job_queue_key = \"media_jobs\"\n        self.progress_channel = \"job_progress\"\n        self.worker_heartbeat_key = \"worker_heartbeats\"\n    \n    def submit_job(self, job_message: JobMessage) -> bool:\n        \"\"\"Submit job to priority queue with deduplication check.\"\"\"\n        # TODO 1: Check for duplicate job_id in active jobs set\n        # TODO 2: Serialize job_message to JSON\n        # TODO 3: Add job to priority queue using ZADD with priority score\n        # TODO 4: Add job_id to active jobs set for deduplication\n        # TODO 5: Set job TTL based on timeout_at value\n        # TODO 6: Return True if successful, False if duplicate\n        pass\n    \n    def get_next_job(self, worker_id: str, timeout: int = 30) -> Optional[JobMessage]:\n        \"\"\"Blocking pop highest priority job from queue.\"\"\"\n        # TODO 1: Use BZPOPMAX to get highest priority job with timeout\n        # TODO 2: If job received, parse JSON to JobMessage\n        # TODO 3: Register worker assignment in Redis\n        # TODO 4: Update worker heartbeat timestamp\n        # TODO 5: Return JobMessage or None if timeout\n        pass\n    \n    def mark_job_completed(self, job_id: str, worker_id: str) -> bool:\n        \"\"\"Mark job as completed and clean up queue state.\"\"\"\n        # TODO 1: Remove job from active jobs set\n        # TODO 2: Remove worker assignment\n        # TODO 3: Update completion statistics\n        # TODO 4: Clean up any job-specific temporary data\n        # TODO 5: Return success status\n        pass\n    \n    def publish_progress(self, progress_message) -> bool:\n        \"\"\"Publish progress update to subscribers.\"\"\"\n        # TODO 1: Serialize progress_message to JSON\n        # TODO 2: Publish to progress channel using PUBLISH\n        # TODO 3: Store latest progress in Redis for polling clients\n        # TODO 4: Return publication success status\n        pass\n    \n    def get_queue_stats(self) -> Dict[str, Any]:\n        \"\"\"Get current queue statistics for monitoring.\"\"\"\n        # TODO 1: Count jobs in priority queue\n        # TODO 2: Count active worker assignments\n        # TODO 3: Get average job processing time\n        # TODO 4: Return comprehensive statistics dictionary\n        pass\n```\n\n#### Core Processing Logic Skeleton\n\n```python\n# workers/base_worker.py - Common worker functionality\nimport os\nimport time\nimport signal\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Dict, Any\n\nfrom messaging.queue_manager import QueueManager\nfrom messaging.message_formats import JobMessage, ProgressMessage\nfrom storage.metadata_store import MetadataStore\n\nclass BaseWorker(ABC):\n    \"\"\"Base class for all media processing workers.\"\"\"\n    \n    def __init__(self, worker_id: str, queue_manager: QueueManager, \n                 metadata_store: MetadataStore):\n        self.worker_id = worker_id\n        self.queue_manager = queue_manager\n        self.metadata_store = metadata_store\n        self.shutdown_requested = False\n        self.current_job = None\n        self.logger = logging.getLogger(f\"worker.{worker_id}\")\n    \n    def run(self):\n        \"\"\"Main worker loop - gets jobs and processes them.\"\"\"\n        # TODO 1: Set up signal handlers for graceful shutdown\n        # TODO 2: Start heartbeat thread for worker health monitoring\n        # TODO 3: Enter main processing loop until shutdown requested\n        # TODO 4: Get next job from queue with timeout\n        # TODO 5: Process job using subclass implementation\n        # TODO 6: Handle job completion or failure\n        # TODO 7: Clean up resources and update worker status\n        pass\n    \n    def update_progress(self, percentage: float, stage: str, \n                       stage_percentage: float, details: Dict[str, Any]):\n        \"\"\"Send progress update for current job.\"\"\"\n        # TODO 1: Validate progress values (0-100, stage exists)\n        # TODO 2: Create ProgressMessage with current job details\n        # TODO 3: Include worker resource usage information\n        # TODO 4: Publish progress message to queue manager\n        # TODO 5: Log progress update for debugging\n        pass\n    \n    @abstractmethod\n    def process_job(self, job: JobMessage) -> bool:\n        \"\"\"Process specific job type - implemented by subclasses.\"\"\"\n        # TODO: Subclasses implement specific processing logic\n        pass\n    \n    def handle_job_error(self, job: JobMessage, error: Exception) -> bool:\n        \"\"\"Handle job processing errors with retry logic.\"\"\"\n        # TODO 1: Determine if error is transient or permanent\n        # TODO 2: Check retry count against policy limits\n        # TODO 3: Apply exponential backoff for transient errors\n        # TODO 4: Mark job as failed if retry limit exceeded\n        # TODO 5: Clean up any partial processing results\n        # TODO 6: Return True if job should be retried\n        pass\n```\n\n#### Milestone Checkpoints\n\n**After API Implementation:**\n- Run `python -m pytest tests/api/` - all endpoint tests should pass\n- Start API server: `python main.py`\n- Test job submission: `curl -X POST -F \"file=@test.jpg\" -F \"output_specs=[{\\\"format\\\":\\\"webp\\\",\\\"width\\\":800}]\" http://localhost:5000/jobs`\n- Verify response contains job_id and status \"pending\"\n- Test status endpoint: `curl http://localhost:5000/jobs/{job_id}`\n\n**After Queue Implementation:**\n- Run Redis server: `redis-server`\n- Test queue operations: `python -c \"from messaging.queue_manager import QueueManager; q=QueueManager(); print('Queue connected')\"`\n- Submit test job and verify it appears in Redis: `redis-cli ZRANGE media_jobs 0 -1 WITHSCORES`\n- Check job message format: `redis-cli ZRANGE media_jobs 0 0` should show valid JSON\n\n**After Worker Implementation:**\n- Start worker process: `python -m workers.image_worker`\n- Worker should connect to queue and wait for jobs\n- Submit job through API - worker should pick it up and process\n- Check progress updates in Redis: `redis-cli SUBSCRIBE job_progress`\n- Verify completed job produces output files in storage directory\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Jobs stuck in pending | Worker not running or crashed | Check worker process status, Redis connection | Restart workers, verify Redis connectivity |\n| Progress updates missing | Worker not publishing or Redis pub/sub issue | Monitor Redis PUBSUB channels, check worker logs | Verify Redis PUBLISH permissions, restart progress tracking |\n| File upload fails | Storage permissions or disk space | Check file system permissions, disk usage | Fix directory permissions, free up storage space |\n| Memory errors during processing | Large files exceeding worker memory limits | Monitor worker memory usage during processing | Implement streaming processing, increase worker memory |\n| Webhook delivery fails | Network issues or client endpoint problems | Check webhook endpoint availability, review retry logs | Implement circuit breaker, verify endpoint certificates |\n| Race conditions in progress | Multiple workers updating same job | Check for duplicate job assignments in Redis | Implement atomic job assignment with Redis locks |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones (1-3) as robust error handling underlies image processing, video transcoding, and job queue reliability across the entire system\n\n### Mental Model: Hospital Emergency Response System\n\nThink of error handling in our media processing pipeline like a hospital's emergency response system. Just as a hospital has protocols for different types of medical emergencies — cardiac arrest, trauma, poisoning — each requiring different response procedures, our media processing system must handle different types of failures with appropriate recovery strategies.\n\nIn a hospital, triage nurses quickly categorize incoming patients by severity: critical (immediate attention), urgent (can wait briefly), less urgent (stable for now). Similarly, our system must rapidly classify errors as transient (retry immediately), resource-constrained (retry with backoff), or permanent failures (move to dead letter queue). The hospital maintains detailed logs of every emergency response to improve future care, just as our system logs every failure and recovery attempt to enable debugging and system improvement.\n\nThe hospital's emergency protocols ensure that even when multiple crises occur simultaneously, resources are allocated appropriately, staff coordinate effectively, and no patient is forgotten. Our error handling system provides the same guarantee: even when multiple jobs fail across different worker processes, failures are detected quickly, resources are cleaned up properly, and no job is lost or left in an inconsistent state.\n\n![Job Lifecycle State Machine](./diagrams/job-state-machine.svg)\n\n## Failure Modes and Detection\n\nComprehensive error detection requires understanding the different categories of failures that can occur in a distributed media processing system. Each failure mode has distinct characteristics, detection methods, and recovery strategies that must be implemented systematically.\n\n### Mental Model: Weather Monitoring System\n\nThink of failure detection like a weather monitoring system with multiple sensors detecting different types of atmospheric disturbances. Just as meteorologists use temperature sensors, pressure gauges, humidity detectors, and wind speed meters to identify incoming storms, our system deploys multiple detection mechanisms to identify different failure patterns before they cascade into larger system outages.\n\nWeather systems provide early warning alerts with different severity levels — watches, warnings, and emergencies — each triggering different response protocols. Our failure detection system similarly provides graduated alert levels that trigger increasingly aggressive recovery measures as failure severity escalates.\n\n### System-Level Failure Categories\n\nThe media processing pipeline encounters failures across multiple system layers, each requiring specialized detection and handling approaches.\n\n**Infrastructure Failures** represent the foundational layer of potential system disruption. Redis connection failures manifest when the job queue becomes unreachable due to network partitions, Redis server crashes, or configuration errors. These failures are detected through connection timeout exceptions and failed ping operations during health checks. Storage system failures occur when temporary file writes fail due to disk space exhaustion, permission errors, or filesystem corruption. Worker process crashes happen when FFmpeg consumes excessive memory, encounters segmentation faults, or is terminated by the operating system's out-of-memory killer.\n\n**Resource Constraint Failures** emerge when system resources become insufficient to process jobs effectively. Memory exhaustion occurs when large video files or high-resolution images exceed available RAM during processing operations. CPU throttling manifests as dramatically increased processing times when system load exceeds capacity. Disk space exhaustion prevents temporary file creation and output file writing. Network bandwidth limitations cause timeout failures when downloading input files or uploading processed results to external storage systems.\n\n**Application Logic Failures** stem from processing-specific errors and business rule violations. Invalid input files include corrupted media files, unsupported formats, or files that claim to be one format but contain different data. Configuration errors occur when output specifications request impossible operations like upscaling to extreme resolutions or using incompatible codec combinations. Processing timeout failures happen when jobs exceed configured time limits due to unexpectedly large input files or complex processing requirements.\n\n**External Dependency Failures** involve third-party systems and services that the processing pipeline relies upon. Webhook delivery failures occur when recipient endpoints are unreachable, return error status codes, or fail to respond within timeout windows. Cloud storage API failures prevent input file downloads or processed output uploads. FFmpeg execution failures happen when the underlying binary encounters unsupported codec combinations, corrupted input streams, or internal processing errors.\n\n### Failure Detection Mechanisms\n\nEarly and accurate failure detection requires implementing multiple monitoring layers that work together to provide comprehensive system visibility.\n\n| Detection Method | Monitors | Detection Time | Granularity | Implementation |\n|-----------------|----------|----------------|-------------|----------------|\n| Health Check Probes | Service availability | 30-60 seconds | Component level | Redis ping, filesystem write test |\n| Process Monitoring | Worker process status | 5-10 seconds | Process level | PID monitoring, heartbeat signals |\n| Resource Monitoring | CPU, memory, disk usage | 10-30 seconds | System level | Resource usage thresholds |\n| Job Timeout Detection | Processing duration | Real-time | Job level | Timer-based cancellation |\n| Progress Stall Detection | Progress update frequency | 60-120 seconds | Job level | Progress timestamp monitoring |\n| Exception Handling | Application errors | Immediate | Operation level | Try-catch blocks with logging |\n| Log Analysis | Error patterns | 1-5 minutes | System level | Log aggregation and pattern matching |\n| External Service Monitoring | API response codes | Real-time | Request level | HTTP status code analysis |\n\nThe `ResourceMonitor` component implements proactive resource constraint detection by continuously tracking system utilization and predicting when thresholds will be exceeded.\n\n| Resource Type | Warning Threshold | Critical Threshold | Detection Method | Response Action |\n|---------------|-------------------|-------------------|------------------|-----------------|\n| Memory Usage | 80% of available RAM | 90% of available RAM | Process memory monitoring | Pause new job acceptance |\n| Disk Space | 85% of temp directory | 95% of temp directory | Filesystem usage check | Clean up old temporary files |\n| CPU Load | 80% sustained over 5 minutes | 95% sustained over 2 minutes | Load average monitoring | Scale down concurrent jobs |\n| Network I/O | 80% of bandwidth limit | 90% of bandwidth limit | Network traffic analysis | Queue bandwidth-heavy jobs |\n\nProgress stall detection identifies jobs that appear to be running but have stopped making meaningful progress. This situation occurs when FFmpeg processes hang, infinite loops occur in image processing algorithms, or worker processes become unresponsive while maintaining their connection to the job queue.\n\n**Progress Stall Detection Algorithm:**\n\n1. The `ProgressTracker` maintains a timestamp for each job's most recent progress update\n2. A background monitor process scans all active jobs every 60 seconds\n3. For each job, calculate the time since the last progress update\n4. If the stall duration exceeds the job type's expected maximum interval, mark the job as potentially stalled\n5. Send a health check request to the worker process handling the stalled job\n6. If the worker fails to respond within 30 seconds, initiate job recovery procedures\n7. Log the stall event with job context for debugging analysis\n\n### Error Classification and Triage\n\nOnce failures are detected, they must be rapidly classified to determine the appropriate recovery strategy. This classification process mirrors medical triage protocols where patients are quickly categorized by severity and treatment urgency.\n\n**Transient Errors** are temporary failures that typically resolve themselves or can be resolved through immediate retry. Network connection timeouts to Redis often succeed on retry due to temporary network congestion. File system busy errors during temporary file operations usually succeed when retried after a brief delay. FFmpeg initialization failures sometimes resolve when retried due to transient resource contention.\n\n**Resource Errors** indicate system resource constraints that require managed backoff and resource cleanup before retry attempts. Memory allocation failures for large images require waiting for other jobs to complete and free memory. Disk space exhaustion requires cleanup of temporary files before retry. CPU throttling requires reducing concurrent job processing to allow system recovery.\n\n**Permanent Errors** represent fundamental problems that will not resolve through retry attempts and require human intervention or job rejection. Corrupted input files will never process successfully regardless of retry count. Invalid codec combinations in video transcoding specifications cannot be resolved automatically. Malformed webhook URLs will always fail delivery attempts.\n\n**Dependency Errors** involve external system failures that may be transient or permanent depending on the root cause. Database connection failures to PostgreSQL may be temporary network issues or permanent service outages. Cloud storage API errors might be temporary rate limiting or permanent authentication problems. Webhook delivery failures could be temporary recipient downtime or permanent endpoint changes.\n\nThe error classification algorithm evaluates multiple factors to determine the appropriate error category:\n\n| Error Type | Detection Criteria | Retry Eligible | Backoff Strategy | Max Retries |\n|------------|-------------------|----------------|------------------|-------------|\n| Network Timeout | Connection timeout exception | Yes | Exponential with jitter | 5 attempts |\n| File I/O Error | Permission denied, disk full | Depends on error code | Linear backoff | 3 attempts |\n| Memory Error | Out of memory exception | Yes | Resource-aware backoff | 2 attempts |\n| Format Error | Invalid file format | No | None | 0 attempts |\n| Configuration Error | Invalid parameters | No | None | 0 attempts |\n| FFmpeg Error | Exit code analysis | Depends on exit code | Job-specific backoff | 3 attempts |\n| Webhook Failure | HTTP status code | Depends on status code | Exponential | 7 attempts |\n\n> **Decision: Error Classification Strategy**\n> - **Context**: Failures need rapid classification to determine retry eligibility and recovery strategies without human intervention\n> - **Options Considered**: Rule-based classification, machine learning classification, hybrid approach\n> - **Decision**: Rule-based classification with exit code and exception type analysis\n> - **Rationale**: Rule-based systems provide deterministic behavior, easier debugging, and immediate classification without training data requirements\n> - **Consequences**: Enables automated retry decisions but requires manual rule updates as new error patterns emerge\n\n![Error Recovery Decision Tree](./diagrams/error-recovery-paths.svg)\n\n## Retry and Backoff Strategies\n\nEffective retry mechanisms must balance rapid error recovery with system stability, preventing retry storms that can overwhelm already-stressed system components. The retry strategy varies significantly based on error type, system load, and historical failure patterns.\n\n### Mental Model: Air Traffic Control During Storms\n\nThink of retry strategies like air traffic control managing flight delays during severe weather. When storms hit an airport, controllers don't immediately reroute all flights to the same alternate airport — that would overwhelm the backup facility. Instead, they implement graduated delays: some flights wait briefly for weather to clear, others are sent to different airports with spacing to prevent congestion, and some are cancelled entirely when conditions are too dangerous.\n\nSimilarly, our retry system implements intelligent backoff that prevents overwhelming system components during failure scenarios. When Redis becomes unavailable, we don't immediately retry all queued operations — instead, we implement exponential backoff with jitter so that retry attempts are distributed over time, giving the system opportunity to recover without being bombarded with retry traffic.\n\nAir traffic controllers also prioritize based on urgency: emergency landings get immediate priority, while routine flights can be delayed longer. Our retry system similarly prioritizes high-priority jobs for earlier retry attempts while allowing lower-priority jobs to wait longer between retries.\n\n### Exponential Backoff Implementation\n\nExponential backoff prevents retry storms by progressively increasing delays between retry attempts, giving system components time to recover from failure conditions.\n\nThe base retry calculation uses the formula: `delay = base_delay * (2^attempt_number) + random_jitter` where jitter prevents the thundering herd problem when multiple jobs fail simultaneously.\n\n**Exponential Backoff Configuration:**\n\n| Error Category | Base Delay | Max Delay | Jitter Range | Max Attempts | Backoff Factor |\n|---------------|------------|-----------|--------------|--------------|----------------|\n| Network Timeout | 1 second | 300 seconds | ±20% | 5 | 2.0 |\n| Resource Constraint | 5 seconds | 900 seconds | ±30% | 3 | 2.5 |\n| External API Error | 2 seconds | 600 seconds | ±25% | 7 | 2.0 |\n| File I/O Error | 0.5 seconds | 60 seconds | ±10% | 3 | 1.5 |\n| FFmpeg Error | 3 seconds | 180 seconds | ±15% | 3 | 2.0 |\n\nThe jitter component prevents synchronized retry attempts that can overwhelm recovering systems. Random jitter is calculated as: `jitter = delay * (1.0 + random(-jitter_range, +jitter_range))` where random generates values within the specified range.\n\n**Priority-Based Retry Scheduling** ensures that high-priority jobs receive preferential retry treatment while preventing lower-priority jobs from being starved indefinitely.\n\n| Job Priority | Retry Delay Multiplier | Queue Position | Max Concurrent Retries |\n|--------------|----------------------|----------------|----------------------|\n| `JobPriority.URGENT` | 0.5x | Front of retry queue | 10 |\n| `JobPriority.HIGH` | 0.75x | Priority section | 7 |\n| `JobPriority.NORMAL` | 1.0x | Standard section | 5 |\n| `JobPriority.LOW` | 1.5x | End of retry queue | 2 |\n\n### Circuit Breaker Pattern for External Dependencies\n\nCircuit breakers protect the system from cascade failures when external dependencies become unreliable. Like electrical circuit breakers that prevent electrical fires by cutting power during overload conditions, software circuit breakers prevent retry storms by temporarily stopping calls to failing services.\n\nThe circuit breaker maintains three states: **Closed** (normal operation), **Open** (blocking calls to failed service), and **Half-Open** (testing if service has recovered).\n\n**Circuit Breaker State Transitions:**\n\n| Current State | Condition | Next State | Action Taken |\n|---------------|-----------|------------|--------------|\n| Closed | Failure rate < threshold | Closed | Allow all requests |\n| Closed | Failure rate ≥ threshold | Open | Block all requests, start timeout |\n| Open | Timeout not expired | Open | Continue blocking requests |\n| Open | Timeout expired | Half-Open | Allow single test request |\n| Half-Open | Test request succeeds | Closed | Resume normal operation |\n| Half-Open | Test request fails | Open | Reset timeout, continue blocking |\n\nCircuit breaker configuration varies based on the criticality and typical failure patterns of different external dependencies:\n\n| Dependency Type | Failure Threshold | Timeout Duration | Test Interval | Impact |\n|----------------|-------------------|------------------|---------------|--------|\n| Redis Job Queue | 5 failures in 60s | 30 seconds | 10 seconds | Job processing stops |\n| Webhook Delivery | 10 failures in 300s | 120 seconds | 30 seconds | Notifications delayed |\n| Cloud Storage | 3 failures in 30s | 60 seconds | 15 seconds | Upload/download blocked |\n| PostgreSQL Progress | 3 failures in 45s | 45 seconds | 15 seconds | Progress tracking disabled |\n\n> **Decision: Circuit Breaker vs Simple Retry**\n> - **Context**: External dependencies like webhooks and cloud storage can become unreliable, causing job failures to cascade\n> - **Options Considered**: Simple retry with exponential backoff, circuit breaker pattern, hybrid approach with circuit breaker for critical paths\n> - **Decision**: Circuit breaker for external dependencies, simple retry for internal operations\n> - **Rationale**: Circuit breakers prevent cascade failures and retry storms during external service outages, while simple retry is sufficient for transient internal errors\n> - **Consequences**: Adds complexity but prevents system-wide outages when external services fail, improves overall system stability\n\n### Retry Queue Management\n\nFailed jobs requiring retry must be managed separately from the main processing queue to prevent blocking new job processing during system recovery periods. The retry queue implements priority-based scheduling and resource-aware retry timing.\n\nThe `QueueManager` maintains separate Redis lists for different retry categories, allowing independent processing and monitoring of retry workloads:\n\n| Queue Name | Purpose | Priority | Worker Pool |\n|------------|---------|----------|-------------|\n| `retry:immediate` | Network timeouts, transient errors | High | Dedicated retry workers |\n| `retry:resource` | Memory/CPU constraint errors | Medium | Resource-aware workers |\n| `retry:dependency` | External API, webhook failures | Low | Limited concurrency workers |\n| `dead_letter` | Permanently failed jobs | N/A | Manual review queue |\n\n**Retry Scheduling Algorithm:**\n\n1. Failed jobs are classified by error type and assigned to appropriate retry queue\n2. Retry delay is calculated based on error type, attempt number, job priority, and current system load\n3. Jobs are scheduled for retry using Redis sorted sets with retry timestamp as score\n4. Background scheduler process scans sorted sets every 10 seconds for jobs ready to retry\n5. Ready jobs are moved back to main processing queue with retry context preserved\n6. Retry metrics are updated for monitoring and alerting\n\nResource-aware retry scheduling monitors system health before processing retry jobs to prevent overwhelming an already-stressed system:\n\n| System Condition | Retry Processing | Queue Behavior | Worker Action |\n|------------------|------------------|----------------|---------------|\n| Normal load (<70%) | Process all retry types | Standard scheduling | Full retry processing |\n| High load (70-85%) | Pause resource-intensive retries | Delay resource retries | Process only network retries |\n| Critical load (>85%) | Pause all retries | Queue all retries | Focus on current jobs |\n| Recovery mode | Gradual retry resumption | Priority-based restart | Slowly increase retry rate |\n\n### Dead Letter Queue Management\n\nJobs that exceed maximum retry attempts or encounter permanent errors are moved to a dead letter queue for manual investigation and potential reprocessing. The dead letter queue serves as both a safety net preventing data loss and a debugging resource for system improvement.\n\n**Dead Letter Queue Structure:**\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `original_job` | `ProcessingJob` | Complete original job specification |\n| `failure_history` | `List[FailureRecord]` | Chronological list of all retry attempts |\n| `final_error` | `str` | Final error message that caused permanent failure |\n| `classification` | `str` | Error classification (permanent, retry_exhausted, configuration) |\n| `moved_at` | `datetime` | Timestamp when moved to dead letter queue |\n| `investigation_notes` | `str` | Manual notes from debugging investigation |\n| `resolution_action` | `str` | Action taken (reprocessed, cancelled, fixed) |\n\nDead letter queue monitoring provides insights into system reliability and helps identify recurring failure patterns:\n\n| Metric | Calculation | Alert Threshold | Action Required |\n|--------|-------------|-----------------|-----------------|\n| Dead letter rate | Dead letter jobs / total jobs | > 2% | Investigate error patterns |\n| Error pattern frequency | Count by error classification | > 10 same errors/hour | Fix underlying issue |\n| Resolution time | Time from dead letter to resolution | > 24 hours | Improve support process |\n| Reprocess success rate | Successful reprocessed jobs / total reprocessed | < 80% | Review error classification |\n\n⚠️ **Pitfall: Retry Storm Prevention**\n\n**The Problem:** When a shared dependency like Redis fails, all worker processes simultaneously retry their operations, creating a retry storm that prevents the dependency from recovering and may cause additional system failures.\n\n**Why It's Wrong:** Synchronized retries can overwhelm recovering systems, extend outage duration, and cause cascade failures to other system components.\n\n**How to Fix:** Implement jittered exponential backoff with circuit breakers, spread retry attempts over time, and monitor retry rates to detect storm conditions before they cause damage.\n\n## Resource Cleanup and Recovery\n\nComprehensive resource cleanup ensures that failed jobs do not consume system resources indefinitely and that worker processes can recover gracefully from various failure scenarios. Resource management must handle both normal cleanup during successful job completion and emergency cleanup during various failure modes.\n\n### Mental Model: Restaurant Kitchen Cleanup\n\nThink of resource cleanup like the closing procedures at a busy restaurant kitchen. Throughout the day, cooks use various workstations, utensils, and ingredients. When a dish is completed successfully, the cook cleans their station, puts away ingredients, and washes utensils. But when something goes wrong — a pot burns, a sauce spills, or a cook gets sick mid-service — there are emergency cleanup protocols.\n\nThe kitchen manager doesn't just abandon the messy workstation; they have systematic procedures: contain any spills to prevent spread, safely dispose of any ruined food, sanitize contaminated surfaces, and return the workstation to usable condition. Similarly, our media processing system must clean up temporary files, release memory allocations, terminate background processes, and reset worker state when jobs fail.\n\nJust as restaurants have health inspectors who ensure cleanup procedures are followed correctly, our system includes resource monitors that verify cleanup completion and alert operators when resources are not properly released.\n\n### Temporary File Management\n\nMedia processing operations create numerous temporary files during processing: extracted video frames, intermediate processing results, format conversion buffers, and FFmpeg output segments. These temporary files must be tracked and cleaned up reliably even when processing fails unexpectedly.\n\nThe `ResourceManager` maintains a registry of all temporary files associated with each processing job, enabling comprehensive cleanup regardless of how processing terminates.\n\n**Temporary File Lifecycle Management:**\n\n| Stage | File Types Created | Cleanup Trigger | Cleanup Method |\n|-------|-------------------|-----------------|----------------|\n| Job Initialization | Working directory, lock files | Job start failure | Immediate deletion |\n| Image Processing | Resized variants, format conversions | Processing completion/failure | Staged cleanup |\n| Video Transcoding | FFmpeg segments, audio extracts | Transcoding completion/failure | Background cleanup |\n| Output Generation | Final outputs, compressed archives | Job completion/timeout | Retention policy cleanup |\n| Error Recovery | Debug dumps, partial results | Recovery completion | Delayed cleanup |\n\nThe temporary file registry tracks file metadata and cleanup requirements for each job:\n\n| Registry Field | Type | Purpose |\n|----------------|------|---------|\n| `job_id` | `str` | Associate files with processing job |\n| `file_path` | `str` | Absolute path to temporary file |\n| `file_type` | `str` | Category for cleanup prioritization |\n| `size_bytes` | `int` | File size for cleanup planning |\n| `created_at` | `datetime` | Creation timestamp for age-based cleanup |\n| `cleanup_priority` | `int` | Cleanup order priority (1=immediate, 5=delayed) |\n| `retention_hours` | `int` | How long to keep file after job completion |\n\n**Cleanup Priority Algorithm:**\n\n1. **Immediate Priority (1)**: Lock files, PID files, and job control structures that prevent worker restart\n2. **High Priority (2)**: Large temporary files, intermediate processing results consuming significant disk space  \n3. **Medium Priority (3)**: Debug output files, processing logs, and diagnostic information\n4. **Low Priority (4)**: Backup files, cached intermediate results that might be useful for retry\n5. **Delayed Priority (5)**: Output files, final results that may need manual verification\n\nBackground cleanup processes run every 60 seconds to handle orphaned temporary files from crashed worker processes:\n\n```\nCleanup Algorithm:\n1. Scan temporary directory for files older than configured age threshold\n2. Cross-reference found files against active job registry\n3. Identify orphaned files not associated with running jobs\n4. Group files by cleanup priority for efficient processing\n5. Delete immediate priority files first, then work down priority levels\n6. Log cleanup statistics for monitoring and alerting\n7. Update disk space metrics after cleanup completion\n```\n\n### Memory Management and Resource Limits\n\nImage and video processing operations can consume substantial memory, particularly when handling high-resolution content or multiple concurrent jobs. Effective memory management prevents out-of-memory conditions and ensures fair resource allocation across worker processes.\n\n**Memory Allocation Tracking:**\n\nThe `ResourceMonitor` tracks memory usage at multiple levels to provide comprehensive memory management:\n\n| Memory Category | Tracking Method | Limit Enforcement | Cleanup Trigger |\n|----------------|-----------------|-------------------|-----------------|\n| Process Memory | RSS monitoring | Process limits | Memory threshold exceeded |\n| Image Buffers | Allocation tracking | Buffer size limits | Processing completion |\n| Video Frames | Frame buffer counting | Frame limit enforcement | Frame processing completion |\n| FFmpeg Processes | Child process monitoring | Process memory limits | FFmpeg completion/timeout |\n| Cache Memory | Cache size tracking | LRU eviction | Cache size limits |\n\nPre-processing memory estimation prevents jobs from starting when insufficient memory is available:\n\n**Memory Estimation Algorithm:**\n\n1. Analyze input file metadata to estimate processing requirements\n2. Calculate memory needed for image processing: `width * height * channels * bytes_per_channel * processing_factor`\n3. Estimate video processing memory: `frame_width * frame_height * fps * processing_duration * memory_factor`\n4. Add overhead estimates for format conversion, compression, and temporary buffers\n5. Compare total estimate against available system memory minus safety margin\n6. Reject job submission if estimated memory exceeds available resources\n7. Queue job with resource requirements for later processing when memory becomes available\n\n| Content Type | Base Memory Formula | Processing Factor | Safety Margin |\n|--------------|-------------------|-------------------|---------------|\n| JPEG Images | width × height × 3 bytes | 2.5x (for processing buffers) | 20% of available RAM |\n| PNG Images | width × height × 4 bytes | 3.0x (for transparency handling) | 20% of available RAM |\n| 4K Video | 3840 × 2160 × 3 × fps | 5.0x (for frame buffers) | 30% of available RAM |\n| HD Video | 1920 × 1080 × 3 × fps | 4.0x (for processing) | 25% of available RAM |\n\n### Worker Process Recovery\n\nWorker processes may fail due to memory exhaustion, segmentation faults, infinite loops, or external process termination. The worker recovery system detects failed processes and restarts them while preserving job queue integrity.\n\n**Worker Health Monitoring:**\n\n| Health Check Type | Frequency | Timeout | Failure Action |\n|------------------|-----------|---------|----------------|\n| Heartbeat Signal | 30 seconds | 45 seconds | Mark worker unhealthy |\n| Job Progress Update | Based on job type | 2x expected duration | Investigate job status |\n| Memory Usage Check | 60 seconds | N/A | Warn if approaching limits |\n| Process Existence | 15 seconds | N/A | Restart if process missing |\n| Queue Connection | 60 seconds | 10 seconds | Restart worker |\n\nThe `WorkerCoordinator` manages worker lifecycle and implements recovery procedures:\n\n**Worker Recovery Procedure:**\n\n1. **Failure Detection**: Monitor detects worker process has failed or become unresponsive\n2. **Job Status Assessment**: Determine status of job being processed by failed worker\n3. **Resource Cleanup**: Clean up temporary files, release memory, terminate child processes\n4. **Job State Recovery**: Mark in-progress job as failed and eligible for retry if appropriate\n5. **Worker Process Restart**: Launch new worker process with same configuration\n6. **Queue Reconnection**: Re-establish Redis connection and resume job processing\n7. **Health Verification**: Verify new worker process is healthy before accepting jobs\n8. **Monitoring Update**: Update worker registry and notify monitoring systems\n\n**Job Recovery State Machine:**\n\n| Job State at Worker Failure | Recovery Action | Job Disposition |\n|----------------------------|-----------------|-----------------|\n| `JobStatus.PENDING` | No action needed | Remains in queue for other worker |\n| `JobStatus.PROCESSING` (< 10% complete) | Mark as failed | Eligible for immediate retry |\n| `JobStatus.PROCESSING` (10-90% complete) | Mark as failed with progress | Eligible for retry with backoff |\n| `JobStatus.PROCESSING` (> 90% complete) | Attempt recovery | Check for partial outputs |\n| Partially completed | Validate outputs | Complete if outputs valid, retry if not |\n\n### Database Transaction Cleanup\n\nProgress tracking and job status updates use database transactions that must be properly cleaned up when worker processes fail unexpectedly. Abandoned transactions can lock database resources and prevent other workers from updating job status.\n\nThe `MetadataStore` implements transaction cleanup procedures:\n\n**Transaction Management:**\n\n| Transaction Type | Timeout | Cleanup Method | Recovery Action |\n|-----------------|---------|----------------|-----------------|\n| Job Status Update | 30 seconds | Auto-rollback | Retry with fresh transaction |\n| Progress Update | 15 seconds | Auto-rollback | Use cached progress value |\n| Batch Progress Update | 60 seconds | Partial commit | Commit successful updates |\n| Job Completion | 45 seconds | Manual cleanup | Verify final state |\n\nConnection pool management prevents resource exhaustion from failed transactions:\n\n```\nTransaction Cleanup Algorithm:\n1. Monitor active database connections for each worker process\n2. Track transaction duration and identify long-running transactions\n3. When worker process fails, identify orphaned database connections\n4. Rollback any active transactions associated with failed worker\n5. Close database connections cleanly to return them to pool\n6. Update connection pool statistics and alert on connection leaks\n7. Verify database consistency after cleanup completion\n```\n\n⚠️ **Pitfall: Incomplete Resource Cleanup**\n\n**The Problem:** When worker processes crash or are terminated forcefully, temporary files, database connections, and child processes may not be cleaned up properly, leading to resource exhaustion and system instability.\n\n**Why It's Wrong:** Accumulated resource leaks can cause disk space exhaustion, database connection pool exhaustion, zombie processes, and memory leaks that degrade system performance over time.\n\n**How to Fix:** Implement comprehensive resource tracking with cleanup verification, use signal handlers for graceful shutdown, implement resource monitors that detect and clean up orphaned resources, and test cleanup procedures under various failure scenarios.\n\n![Webhook Notification Flow](./diagrams/webhook-notification-flow.svg)\n\n### Implementation Guidance\n\nThis section provides practical implementation details for building robust error handling and recovery mechanisms in the media processing pipeline.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Error Classification | Exception type matching | Rule engine with configurable decision trees |\n| Retry Management | Redis sorted sets | Dedicated retry service with complex scheduling |\n| Circuit Breaker | Simple state tracking | Library like `pybreaker` with metrics |\n| Resource Monitoring | Basic system calls | APM tools like Prometheus with custom metrics |\n| Cleanup Scheduling | Cron-based cleanup | Event-driven cleanup with dependency tracking |\n\n**Recommended File Structure:**\n\n```\ninternal/\n  error_handling/\n    __init__.py              ← Error handling exports\n    error_classifier.py      ← Error classification logic  \n    retry_manager.py         ← Retry and backoff implementation\n    circuit_breaker.py       ← Circuit breaker for external deps\n    resource_cleanup.py      ← Cleanup and recovery procedures\n    recovery_coordinator.py  ← Overall recovery orchestration\n  monitoring/\n    resource_monitor.py      ← System resource monitoring\n    health_checker.py        ← Worker and service health checks\n  storage/\n    temp_file_manager.py     ← Temporary file lifecycle management\n    cleanup_scheduler.py     ← Background cleanup processes\n```\n\n**Infrastructure Starter Code:**\n\nHere's a complete error classification system that categorizes failures for appropriate retry handling:\n\n```python\nimport logging\nimport re\nfrom enum import Enum\nfrom typing import Dict, Optional, Tuple\nfrom dataclasses import dataclass\nimport subprocess\n\nclass ErrorCategory(Enum):\n    TRANSIENT = \"transient\"\n    RESOURCE = \"resource\" \n    PERMANENT = \"permanent\"\n    DEPENDENCY = \"dependency\"\n\n@dataclass\nclass ErrorClassification:\n    category: ErrorCategory\n    retry_eligible: bool\n    max_retries: int\n    backoff_factor: float\n    description: str\n\nclass ErrorClassifier:\n    \"\"\"\n    Classifies errors into categories for appropriate retry handling.\n    Uses exception types, error codes, and message patterns.\n    \"\"\"\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        self._init_classification_rules()\n    \n    def _init_classification_rules(self):\n        \"\"\"Initialize error classification rules.\"\"\"\n        # Network and connectivity errors\n        self.network_errors = {\n            'ConnectionError', 'TimeoutError', 'ConnectionResetError',\n            'redis.exceptions.ConnectionError', 'requests.exceptions.Timeout'\n        }\n        \n        # Resource constraint errors  \n        self.resource_errors = {\n            'MemoryError', 'OSError', 'IOError'\n        }\n        \n        # FFmpeg error code patterns\n        self.ffmpeg_exit_codes = {\n            # Transient errors\n            1: ErrorClassification(ErrorCategory.RESOURCE, True, 3, 2.0, \"Generic FFmpeg error\"),\n            # Permanent errors  \n            69: ErrorClassification(ErrorCategory.PERMANENT, False, 0, 0, \"Invalid input format\"),\n            # Add more as needed\n        }\n        \n        # Error message patterns\n        self.error_patterns = [\n            (re.compile(r'disk.*full|no space left', re.I), ErrorCategory.RESOURCE),\n            (re.compile(r'permission denied|access denied', re.I), ErrorCategory.PERMANENT),\n            (re.compile(r'invalid.*format|corrupt.*file', re.I), ErrorCategory.PERMANENT),\n            (re.compile(r'network.*unreachable|connection.*refused', re.I), ErrorCategory.DEPENDENCY),\n        ]\n    \n    def classify_error(self, exception: Exception, context: Dict = None) -> ErrorClassification:\n        \"\"\"\n        Classify an error for retry handling.\n        \n        Args:\n            exception: The exception that occurred\n            context: Additional context (exit codes, job info, etc.)\n            \n        Returns:\n            ErrorClassification with retry strategy\n        \"\"\"\n        exception_name = type(exception).__name__\n        error_message = str(exception)\n        \n        # Check for network/connectivity errors\n        if exception_name in self.network_errors:\n            return ErrorClassification(\n                ErrorCategory.TRANSIENT, True, 5, 2.0, \n                \"Network connectivity error\"\n            )\n        \n        # Check for resource errors\n        if exception_name in self.resource_errors:\n            return self._classify_resource_error(exception, error_message)\n        \n        # Check FFmpeg exit codes\n        if context and 'exit_code' in context:\n            if context['exit_code'] in self.ffmpeg_exit_codes:\n                return self.ffmpeg_exit_codes[context['exit_code']]\n        \n        # Check error message patterns\n        for pattern, category in self.error_patterns:\n            if pattern.search(error_message):\n                return self._get_default_classification(category)\n        \n        # Default to transient with limited retries\n        return ErrorClassification(\n            ErrorCategory.TRANSIENT, True, 2, 1.5,\n            \"Unclassified error - limited retry\"\n        )\n    \n    def _classify_resource_error(self, exception: Exception, message: str) -> ErrorClassification:\n        \"\"\"Classify resource-related errors more specifically.\"\"\"\n        if 'memory' in message.lower() or isinstance(exception, MemoryError):\n            return ErrorClassification(\n                ErrorCategory.RESOURCE, True, 2, 2.5,\n                \"Memory exhaustion error\"  \n            )\n        elif 'disk' in message.lower() or 'space' in message.lower():\n            return ErrorClassification(\n                ErrorCategory.RESOURCE, True, 3, 2.0,\n                \"Disk space error\"\n            )\n        else:\n            return ErrorClassification(\n                ErrorCategory.RESOURCE, True, 3, 2.0, \n                \"General resource error\"\n            )\n    \n    def _get_default_classification(self, category: ErrorCategory) -> ErrorClassification:\n        \"\"\"Get default classification for error category.\"\"\"\n        defaults = {\n            ErrorCategory.TRANSIENT: ErrorClassification(category, True, 5, 2.0, \"Transient error\"),\n            ErrorCategory.RESOURCE: ErrorClassification(category, True, 3, 2.5, \"Resource error\"), \n            ErrorCategory.PERMANENT: ErrorClassification(category, False, 0, 0, \"Permanent error\"),\n            ErrorCategory.DEPENDENCY: ErrorClassification(category, True, 7, 2.0, \"External dependency error\")\n        }\n        return defaults[category]\n```\n\n**Core Logic Skeleton Code:**\n\nHere's the main retry manager that needs to be implemented:\n\n```python\nimport asyncio\nimport time\nimport random\nfrom typing import Optional, Callable, Any\nfrom dataclasses import dataclass, asdict\nimport redis\nimport json\nimport logging\n\n@dataclass \nclass RetryContext:\n    job_id: str\n    attempt_number: int\n    last_error: str\n    classification: ErrorClassification\n    scheduled_at: float\n    priority: int\n\nclass RetryManager:\n    \"\"\"\n    Manages job retries with exponential backoff and priority scheduling.\n    Implements circuit breaker pattern for external dependencies.\n    \"\"\"\n    \n    def __init__(self, redis_client: redis.Redis, config: ProcessingConfig):\n        self.redis = redis_client\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n        self.error_classifier = ErrorClassifier()\n        \n        # Circuit breaker state tracking\n        self.circuit_breakers = {}\n    \n    async def schedule_retry(self, job: ProcessingJob, error: Exception, context: Dict = None) -> bool:\n        \"\"\"\n        Schedule a job for retry based on error classification.\n        \n        Args:\n            job: The failed processing job\n            error: Exception that caused failure  \n            context: Additional error context\n            \n        Returns:\n            True if retry scheduled, False if job moved to dead letter queue\n        \"\"\"\n        # TODO 1: Use error_classifier to classify the error and get retry strategy\n        # TODO 2: Check if job has exceeded maximum retry attempts for its error type\n        # TODO 3: If retry limit exceeded, move job to dead letter queue and return False\n        # TODO 4: Calculate retry delay using exponential backoff with jitter\n        # TODO 5: Create RetryContext with job info, attempt number, and scheduled time\n        # TODO 6: Store retry context in Redis sorted set with schedule time as score  \n        # TODO 7: Update job status to indicate retry scheduled\n        # TODO 8: Log retry scheduling with job context and delay information\n        # TODO 9: Update retry metrics for monitoring\n        # TODO 10: Return True to indicate retry was scheduled\n        \n        classification = self.error_classifier.classify_error(error, context)\n        \n        if not classification.retry_eligible:\n            return await self._move_to_dead_letter_queue(job, error, \"Non-retryable error\")\n            \n        if job.retry_count >= classification.max_retries:\n            return await self._move_to_dead_letter_queue(job, error, \"Retry limit exceeded\")\n            \n        # Implementation continues...\n        pass\n    \n    def calculate_retry_delay(self, attempt: int, base_delay: float, backoff_factor: float, \n                            max_delay: float, jitter_range: float = 0.2) -> float:\n        \"\"\"\n        Calculate retry delay with exponential backoff and jitter.\n        \n        Args:\n            attempt: Current retry attempt number (1-based)\n            base_delay: Base delay in seconds\n            backoff_factor: Exponential backoff multiplier  \n            max_delay: Maximum delay cap in seconds\n            jitter_range: Jitter as fraction of delay (0.0-1.0)\n            \n        Returns:\n            Calculated delay in seconds with jitter applied\n        \"\"\"\n        # TODO 1: Calculate exponential delay: base_delay * (backoff_factor ** (attempt - 1))\n        # TODO 2: Apply maximum delay cap to prevent excessive wait times\n        # TODO 3: Calculate jitter amount: delay * random value in [-jitter_range, +jitter_range]  \n        # TODO 4: Add jitter to delay to prevent thundering herd\n        # TODO 5: Ensure final delay is not negative\n        # TODO 6: Log calculated delay for debugging\n        # TODO 7: Return final delay value\n        pass\n    \n    async def process_retry_queue(self, worker_id: str) -> Optional[ProcessingJob]:\n        \"\"\"\n        Check retry queue for jobs ready to process.\n        \n        Args:\n            worker_id: ID of worker requesting retry job\n            \n        Returns:\n            Job ready for retry processing, or None if no jobs ready\n        \"\"\"\n        # TODO 1: Get current timestamp for comparing against scheduled retry times\n        # TODO 2: Query Redis sorted set for jobs with score <= current time (ready to retry)\n        # TODO 3: Use ZPOPMIN to atomically get and remove highest priority retry job\n        # TODO 4: If no retry jobs ready, return None\n        # TODO 5: Deserialize retry context from Redis data\n        # TODO 6: Load original ProcessingJob from retry context\n        # TODO 7: Increment job retry count and update retry context\n        # TODO 8: Mark job status as processing and assign to worker\n        # TODO 9: Log retry job assignment with worker and attempt information\n        # TODO 10: Return ProcessingJob ready for retry processing\n        pass\n    \n    async def _move_to_dead_letter_queue(self, job: ProcessingJob, error: Exception, \n                                       reason: str) -> bool:\n        \"\"\"Move job to dead letter queue for manual review.\"\"\"\n        # TODO 1: Create dead letter record with job, error history, and failure reason\n        # TODO 2: Store record in dead letter queue (Redis list or database)\n        # TODO 3: Update job status to FAILED with reason\n        # TODO 4: Send webhook notification about job failure if webhook_url configured\n        # TODO 5: Log dead letter queue movement with job ID and reason\n        # TODO 6: Update dead letter queue metrics\n        # TODO 7: Clean up any retry queue entries for this job\n        # TODO 8: Return True to indicate successful dead letter processing\n        pass\n    \n    def _get_circuit_breaker_key(self, service: str) -> str:\n        \"\"\"Generate Redis key for circuit breaker state.\"\"\"\n        return f\"circuit_breaker:{service}\"\n    \n    async def check_circuit_breaker(self, service: str) -> bool:\n        \"\"\"\n        Check if circuit breaker allows requests to service.\n        \n        Args:\n            service: Service name (e.g., 'webhooks', 'cloud_storage')\n            \n        Returns:\n            True if requests allowed, False if circuit is open\n        \"\"\"  \n        # TODO 1: Get circuit breaker state from Redis for the service\n        # TODO 2: If no state exists, initialize as CLOSED (allow requests)\n        # TODO 3: If state is CLOSED, allow request and return True\n        # TODO 4: If state is OPEN, check if timeout period has expired\n        # TODO 5: If timeout expired, transition to HALF_OPEN and allow single test request\n        # TODO 6: If still in timeout period, block request and return False  \n        # TODO 7: Log circuit breaker decision for monitoring\n        # TODO 8: Return decision (True = allow, False = block)\n        pass\n    \n    async def record_service_result(self, service: str, success: bool) -> None:\n        \"\"\"Record result of service call for circuit breaker logic.\"\"\"\n        # TODO 1: Get current circuit breaker state for service\n        # TODO 2: If state is HALF_OPEN and request succeeded, transition to CLOSED\n        # TODO 3: If state is HALF_OPEN and request failed, transition back to OPEN  \n        # TODO 4: If state is CLOSED, track failure rate over time window\n        # TODO 5: If failure rate exceeds threshold, transition to OPEN\n        # TODO 6: Update circuit breaker metrics and state in Redis\n        # TODO 7: Log state transitions for monitoring and alerting\n        pass\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the error handling system:\n\n1. **Test Error Classification**: Run `python -m pytest tests/test_error_classifier.py -v` to verify error classification logic\n2. **Test Retry Logic**: Simulate job failures and verify exponential backoff calculations\n3. **Test Circuit Breaker**: Simulate external service failures and verify circuit breaker state transitions  \n4. **Test Resource Cleanup**: Create temporary files, kill worker processes, verify cleanup completion\n5. **Manual Verification**: Submit a job that will fail (invalid input file), verify it moves through retry attempts and eventually reaches dead letter queue\n\n**Expected Behaviors:**\n- Transient network errors should retry immediately with exponential backoff\n- Resource errors should delay retry until system resources are available  \n- Permanent errors (invalid file format) should move directly to dead letter queue\n- Circuit breakers should prevent retry storms when external services fail\n- Temporary files should be cleaned up within 60 seconds of job failure\n- Worker processes should restart automatically and resume processing jobs\n\n\n## Testing Strategy and Milestones\n\n> **Milestone(s):** All milestones (1-3) as comprehensive testing strategy ensures reliable image processing, video transcoding, and job queue functionality across the entire media processing pipeline\n\n### Mental Model: Quality Assurance Laboratory\n\nThink of our testing strategy as a comprehensive quality assurance laboratory for a manufacturing facility. Just as a QA lab tests components individually (unit testing), validates the entire assembly line with real products (integration testing), and maintains strict checkpoints at each production stage (milestone validation), our media processing pipeline requires the same systematic approach to ensure reliability.\n\nIn a manufacturing QA lab, you have three distinct testing phases: component testing with synthetic materials to verify individual part specifications, full production line testing with actual products to validate end-to-end workflows, and stage-gate reviews at each manufacturing milestone to ensure quality before proceeding to the next phase. Our media processing testing strategy follows this exact pattern, ensuring each component works correctly in isolation, the entire pipeline processes real media files reliably, and each development milestone delivers measurable functionality.\n\nThe critical insight here is that media processing testing requires specialized approaches because we're dealing with binary data, time-consuming operations, and external dependencies. Unlike simple web applications where you can mock everything, media processing demands testing with actual image and video files, realistic processing times, and real queue behavior to catch the subtle bugs that only emerge under production conditions.\n\n### Unit Testing Approach\n\n**Component Isolation Strategy**\n\nUnit testing in media processing systems requires a sophisticated approach to isolating components while maintaining realistic test conditions. Each processing component—image manipulation, video transcoding, job queue management, and progress tracking—must be testable independently without requiring external dependencies or lengthy processing operations.\n\nThe fundamental challenge lies in creating test conditions that mirror production behavior without the overhead of processing large media files or waiting for complex transcoding operations. This demands carefully crafted test fixtures, mock implementations that preserve behavioral accuracy, and testing strategies that validate both happy path operations and edge case scenarios.\n\n> **Decision: Synthetic Test Media Generation**\n> - **Context**: Unit tests need realistic media files but cannot rely on large binary fixtures or external file dependencies that slow down test execution and complicate CI/CD pipelines\n> - **Options Considered**: Real media file fixtures, procedurally generated test media, mocked media objects with metadata simulation\n> - **Decision**: Implement procedurally generated test media with configurable characteristics\n> - **Rationale**: Generated media provides precise control over test conditions (specific resolutions, formats, corruption patterns) while maintaining fast test execution and avoiding binary repository bloat\n> - **Consequences**: Tests run quickly and reliably but require additional infrastructure to generate realistic media characteristics and edge cases\n\n| Test Media Type | Generation Method | Characteristics | Test Scenarios |\n|-----------------|-------------------|-----------------|-----------------|\n| Minimal Valid Image | PIL/Pillow solid color generation | 1x1 to 100x100 pixels, RGB/RGBA | Format detection, basic operations |\n| Edge Case Image | Programmatic creation with specific properties | Extreme dimensions, unusual aspect ratios | Memory limit validation, resize edge cases |\n| Corrupted Image | Binary manipulation of valid images | Truncated headers, invalid metadata | Error handling, graceful degradation |\n| Video Test Files | FFmpeg synthetic generation | Short duration, known characteristics | Transcoding logic, progress calculation |\n| Metadata Rich Media | Embedded EXIF/metadata injection | GPS, orientation, camera data | Metadata extraction, privacy stripping |\n\n**Image Processing Unit Tests**\n\nImage processing unit tests focus on validating individual operations like resizing algorithms, format conversion accuracy, and metadata handling without requiring large image files or lengthy processing times. Each test verifies specific algorithmic behavior using minimal synthetic images that exercise the same code paths as production media.\n\nThe testing strategy emphasizes boundary condition validation, ensuring resize operations handle edge cases like 1-pixel images, extreme aspect ratios, and memory-constrained scenarios. Format conversion tests verify color space preservation, quality setting behavior, and lossy compression characteristics using programmatically generated test images with known pixel values.\n\n| Test Category | Test Methods | Validation Focus | Expected Outcomes |\n|---------------|--------------|------------------|-------------------|\n| Resize Operations | `test_resize_preserve_aspect_ratio()` | Aspect ratio mathematics | Output dimensions match calculated ratios |\n| Interpolation | `test_lanczos_vs_bilinear_quality()` | Algorithm quality differences | Measurable quality metrics comparison |\n| Format Conversion | `test_jpeg_quality_settings()` | Lossy compression behavior | File size decreases with lower quality |\n| Metadata Handling | `test_exif_orientation_rotation()` | EXIF-based rotation accuracy | Rotated image matches expected orientation |\n| Memory Management | `test_large_image_memory_calculation()` | Resource requirement estimation | Memory calculations match actual usage |\n| Error Conditions | `test_corrupted_image_graceful_failure()` | Exception handling patterns | Specific exceptions with descriptive messages |\n\n**Video Processing Unit Tests**\n\nVideo processing unit tests require a different approach because video operations typically involve external FFmpeg processes and longer execution times. The testing strategy focuses on command generation validation, progress parsing accuracy, and metadata extraction without executing full transcoding operations.\n\nFFmpeg integration tests use synthetic video files generated on-demand with specific characteristics like resolution, duration, and codec combinations. These minimal test videos allow validation of transcoding logic, adaptive bitrate variant generation, and thumbnail extraction without the overhead of processing production-sized media files.\n\n```python\n# Example test structure for FFmpeg command generation validation\ndef test_build_transcode_command_h264():\n    # Validates FFmpeg command construction without executing\n    config = VideoTranscodingConfig(\n        video_codec='h264',\n        target_bitrate=2000000,\n        crf_value=23\n    )\n    command = wrapper._build_transcode_command(\n        'input.mp4', 'output.mp4', config, test_metadata\n    )\n    assert '-c:v libx264' in command\n    assert '-b:v 2M' in command\n    assert '-crf 23' in command\n```\n\n| Test Category | Test Methods | Validation Focus | Mock Strategy |\n|---------------|--------------|------------------|---------------|\n| Command Generation | `test_ffmpeg_command_building()` | Proper FFmpeg argument construction | Mock metadata, validate command arrays |\n| Progress Parsing | `test_ffmpeg_progress_extraction()` | Progress percentage calculation | Mock FFmpeg output streams |\n| Error Detection | `test_ffmpeg_error_classification()` | Exit code and stderr interpretation | Simulated FFmpeg failure scenarios |\n| Codec Selection | `test_optimal_codec_selection()` | Input format to output codec mapping | Mock format detection results |\n| ABR Variant Logic | `test_adaptive_bitrate_calculation()` | Multi-variant parameter generation | Mock input video characteristics |\n\n**Job Queue Unit Tests**\n\nJob queue unit tests validate queue operations, priority handling, and worker coordination logic using in-memory implementations that preserve behavioral accuracy while eliminating Redis dependencies. These tests focus on race condition prevention, atomic operation behavior, and state transition validation.\n\nThe testing approach uses dependency injection to replace Redis connections with thread-safe in-memory implementations that simulate Redis behavior including atomic operations, blocking pops, and pub/sub messaging. This allows comprehensive testing of queue logic without external infrastructure dependencies.\n\n| Test Category | Test Methods | Validation Focus | Mock Implementation |\n|---------------|--------------|------------------|-------------------|\n| Job Submission | `test_atomic_job_submission()` | Deduplication and atomic operations | In-memory job store with threading locks |\n| Priority Queuing | `test_priority_order_preservation()` | Higher priority jobs processed first | Mock Redis sorted set behavior |\n| Worker Coordination | `test_multiple_worker_job_distribution()` | Jobs distributed fairly across workers | Simulated worker pool with job assignment |\n| State Transitions | `test_job_lifecycle_state_machine()` | Valid state transitions only | State machine validation with invalid transition attempts |\n| Dead Letter Queue | `test_failed_job_dead_letter_routing()` | Permanent failures routed correctly | Mock failure classification and routing logic |\n\n**Progress Tracking Unit Tests**\n\nProgress tracking unit tests validate stage-based progress calculation, notification threshold logic, and webhook delivery mechanisms using mock HTTP endpoints and in-memory progress storage. These tests ensure accurate progress reporting and reliable notification delivery without external webhook dependencies.\n\nThe testing strategy focuses on progress calculation accuracy, race condition prevention in concurrent updates, and webhook delivery retry logic. Mock HTTP servers simulate various webhook endpoint behaviors including success responses, timeout conditions, and authentication failures.\n\n| Test Category | Test Methods | Validation Focus | Mock Strategy |\n|---------------|--------------|------------------|---------------|\n| Progress Calculation | `test_stage_weighted_progress()` | Accurate percentage calculation | Mock stage completion data |\n| Concurrent Updates | `test_progress_race_condition_prevention()` | Sequence number validation | Multithreaded progress updates |\n| Webhook Delivery | `test_webhook_retry_exponential_backoff()` | Retry logic and backoff timing | Mock HTTP server with failure simulation |\n| Signature Verification | `test_webhook_hmac_signature_validation()` | Cryptographic signature accuracy | Known test vectors with expected signatures |\n\n### Integration Testing Strategy\n\n**End-to-End Media Processing Workflows**\n\nIntegration testing validates complete media processing workflows using real media files and the full technology stack including Redis job queues, worker processes, and webhook notification delivery. These tests ensure all components work together correctly under realistic conditions and catch integration issues that unit tests cannot detect.\n\nThe integration testing environment requires careful setup to provide realistic test conditions while maintaining fast execution and reliable cleanup. This includes containerized dependencies, realistic media file fixtures, and automated environment provisioning that matches production characteristics.\n\n> **Decision: Containerized Integration Test Environment**\n> - **Context**: Integration tests require Redis, potentially FFmpeg, webhook endpoint simulation, and file system storage that must be consistent across development machines and CI/CD pipelines\n> - **Options Considered**: Local dependency installation, Docker Compose test environment, cloud-based test infrastructure\n> - **Decision**: Docker Compose with service containers for Redis, mock webhook server, and shared volume for media files\n> - **Rationale**: Containerized approach ensures consistent test environment, easy cleanup, parallel test execution without conflicts, and matches production deployment patterns\n> - **Consequences**: Tests require Docker but provide reliable, reproducible results with easy local development and CI/CD integration\n\n**Real Media File Test Fixtures**\n\nIntegration tests require carefully curated media file fixtures that represent realistic production scenarios without consuming excessive storage or processing time. The test fixture strategy balances comprehensive format coverage with practical test execution requirements.\n\nThe fixture collection includes representative samples of each supported format with varying characteristics: different resolutions, aspect ratios, metadata complexity, and file sizes. Additionally, it includes problematic files that test edge cases like corrupted headers, unusual metadata, and format variants that require special handling.\n\n| File Category | Format Examples | Characteristics | Test Scenarios |\n|---------------|-----------------|-----------------|------------------|\n| Standard Images | JPEG, PNG, WebP | Common resolutions, normal metadata | Basic processing workflows |\n| High Resolution | Large JPEG, PNG | 4K+ resolution, substantial file size | Memory management, processing time |\n| Unusual Formats | GIF, TIFF, BMP | Less common formats | Format detection, conversion support |\n| Metadata Rich | JPEG with GPS/EXIF | Comprehensive metadata sets | Privacy stripping, metadata preservation |\n| Problematic Files | Corrupted headers, invalid metadata | Various corruption types | Error handling, graceful degradation |\n| Video Samples | MP4 (H.264), WebM, MOV | Short duration, various codecs | Transcoding workflows, ABR generation |\n\n**Queue Processing Integration Tests**\n\nQueue processing integration tests validate the complete job lifecycle from submission through worker processing to completion notification. These tests use real Redis instances and actual worker processes to ensure proper job distribution, progress tracking, and error handling under concurrent conditions.\n\nThe testing approach spawns multiple worker processes and submits various job types to validate queue behavior under realistic load conditions. Tests verify proper job prioritization, worker failure recovery, and resource cleanup after job completion or failure.\n\n```python\n# Example integration test structure for complete job processing\n@pytest.mark.integration\ndef test_complete_image_processing_workflow():\n    # Submit image processing job with webhook\n    job = submit_job(\n        input_file='fixtures/test_image.jpg',\n        output_specs=[\n            OutputSpecification(format='webp', width=800, quality=85),\n            OutputSpecification(format='png', width=400, quality=None)\n        ],\n        webhook_url='http://mock-server:8080/webhook'\n    )\n    \n    # Wait for processing completion\n    final_status = wait_for_job_completion(job.job_id, timeout=30)\n    \n    # Validate results\n    assert final_status == JobStatus.COMPLETED\n    assert len(job.output_files) == 2\n    assert all(os.path.exists(f) for f in job.output_files)\n    assert webhook_received_completion_notification(job.job_id)\n```\n\n| Test Scenario | Components Involved | Validation Points | Expected Behavior |\n|---------------|-------------------|-------------------|-------------------|\n| Single Image Job | API, Queue, Worker, Storage | Job completion, output file creation | Files created with correct formats and dimensions |\n| Multiple Output Specs | Worker, Format Converter | All output specifications processed | Each output meets specified requirements |\n| Video Transcoding | Worker, FFmpeg, Progress Tracker | Progress updates, final transcoding | ABR variants created with proper manifests |\n| Webhook Notifications | Progress Tracker, HTTP Client | Notification delivery, retry logic | Webhooks delivered with correct signatures |\n| Worker Failure Recovery | Queue, Worker Coordinator | Job reassignment, cleanup | Jobs complete despite worker failures |\n| Priority Job Processing | Queue, Multiple Workers | Job ordering, execution priority | High priority jobs processed first |\n\n**Webhook Integration Testing**\n\nWebhook integration tests validate notification delivery under various network conditions and endpoint behaviors. These tests use mock HTTP servers that simulate different webhook endpoint responses to ensure robust delivery logic and proper error handling.\n\nThe testing infrastructure includes webhook endpoints that simulate success responses, timeout conditions, authentication failures, and temporary unavailability. Tests verify exponential backoff retry logic, signature verification, and eventual delivery guarantees.\n\n| Webhook Scenario | Mock Server Behavior | Test Validation | Expected Outcome |\n|------------------|---------------------|-----------------|------------------|\n| Successful Delivery | HTTP 200 response | Single delivery attempt | Webhook marked as delivered |\n| Temporary Failure | HTTP 503, then 200 | Retry with backoff | Eventually delivered after retries |\n| Permanent Failure | Consistent HTTP 404 | Retry limit reached | Marked as permanently failed |\n| Timeout Condition | Connection timeout | Timeout handling | Retry with exponential backoff |\n| Invalid Signature | Signature verification failure | Security validation | Webhook rejected by endpoint |\n\n**Performance and Load Testing**\n\nIntegration tests include performance validation to ensure the system handles realistic workloads without degradation. These tests process multiple concurrent jobs with various media types and sizes to validate system behavior under load conditions.\n\nPerformance tests measure processing throughput, queue latency, and resource utilization patterns. They establish baseline performance characteristics and detect performance regressions during development. The tests use realistic media files and job mixes that represent expected production workloads.\n\n| Performance Test Type | Load Characteristics | Metrics Measured | Acceptance Criteria |\n|----------------------|---------------------|------------------|-------------------|\n| Concurrent Image Processing | 50 simultaneous image jobs | Jobs per second, memory usage | >10 images/sec, <2GB memory per worker |\n| Mixed Workload | Images and videos combined | Queue latency, completion time | <5 second queue latency, predictable completion times |\n| Large File Handling | High resolution images/videos | Memory efficiency, processing time | Linear scaling with file size |\n| Queue Saturation | More jobs than worker capacity | Queue depth, job assignment fairness | Fair job distribution, no worker starvation |\n\n### Milestone Checkpoints and Validation\n\n**Milestone 1: Image Processing Validation**\n\nThe first milestone checkpoint validates core image processing functionality including format conversion, resizing operations, metadata handling, and thumbnail generation. This milestone establishes the foundation for media processing capabilities and must demonstrate reliable operation with various image formats and processing parameters.\n\nValidation focuses on algorithmic correctness, output quality, and proper handling of edge cases like extreme aspect ratios, unusual metadata, and format-specific requirements. The checkpoint ensures image processing operations produce consistent, high-quality results that meet specification requirements.\n\n> **Milestone 1 Success Criteria**: Image processing component handles all supported formats, produces outputs matching specifications, preserves or strips metadata as configured, and gracefully handles error conditions without crashing or corrupting data\n\n| Validation Area | Test Procedures | Success Indicators | Troubleshooting Steps |\n|-----------------|-----------------|-------------------|----------------------|\n| Format Detection | Load images of each supported format | Correct format identification for all files | Check PIL/Pillow installation, verify file signatures |\n| Resize Operations | Process images with various target dimensions | Output dimensions match specifications with proper aspect ratio handling | Validate resize algorithm selection, check interpolation settings |\n| Quality Settings | Convert to lossy formats with different quality levels | File size decreases appropriately, visual quality preserved | Verify quality parameter mapping, check codec-specific settings |\n| Metadata Handling | Process images with rich EXIF data | Metadata preserved/stripped as configured, orientation handled correctly | Test EXIF parsing library, validate rotation logic |\n| Error Handling | Process corrupted or invalid image files | Graceful failure with descriptive error messages | Check exception handling, validate error classification |\n\n**Checkpoint Commands and Expected Outputs**\n\nThe milestone checkpoint includes specific commands to validate functionality and expected outputs that indicate successful implementation. These commands provide concrete validation steps that can be executed manually or integrated into automated testing pipelines.\n\n```bash\n# Milestone 1 validation commands\npython -m pytest tests/test_image_processing.py -v\npython scripts/process_test_image.py --input fixtures/test.jpg --output /tmp/resized.webp --width 800 --quality 85\npython scripts/extract_metadata.py fixtures/test_with_exif.jpg\n\n# Expected outputs:\n# - All image processing tests pass\n# - Resized image created with correct dimensions and format\n# - Metadata extracted and displayed properly formatted\n```\n\n| Command | Expected Output | Success Validation | Common Issues |\n|---------|-----------------|-------------------|---------------|\n| `pytest tests/test_image_processing.py` | All tests pass, <30 second execution | Green test results, no failures | Missing dependencies, incorrect test fixtures |\n| Image resize command | Output file created with target dimensions | File exists, correct size via `identify` | Memory errors, incorrect aspect ratio calculation |\n| Metadata extraction | JSON output with EXIF fields | GPS, orientation, camera data displayed | EXIF library issues, binary data handling errors |\n\n**Milestone 2: Video Transcoding Validation**\n\nThe second milestone validates video transcoding capabilities including format conversion, adaptive bitrate generation, thumbnail extraction, and FFmpeg integration. This milestone demonstrates the system's ability to handle complex video processing workflows with proper progress tracking and error recovery.\n\nVideo processing validation requires longer execution times and more complex success criteria due to the computational complexity of video transcoding operations. The checkpoint ensures reliable FFmpeg integration, accurate progress reporting, and proper handling of various video formats and codecs.\n\n| Validation Area | Test Procedures | Success Indicators | Performance Expectations |\n|-----------------|-----------------|-------------------|-------------------------|\n| Basic Transcoding | Convert MP4 to WebM, various resolutions | Output plays correctly, maintains quality | <2x realtime for simple conversions |\n| ABR Generation | Create HLS/DASH variants from source video | Manifest files created, segments playable | Multiple variants with proper bitrate ladder |\n| Progress Tracking | Monitor transcoding progress updates | Accurate percentage reporting throughout process | Progress updates at least every 5 seconds |\n| Thumbnail Extraction | Extract frames at various timestamps | Thumbnail images created at correct times | Frame accuracy within 1 second |\n| Error Recovery | Process corrupted/unsupported video files | Graceful failure, proper error classification | Clear error messages, no worker crashes |\n\n**Checkpoint Validation Procedures**\n\n```bash\n# Milestone 2 validation commands\npython -m pytest tests/test_video_transcoding.py -v\npython scripts/transcode_video.py --input fixtures/test_video.mp4 --output /tmp/transcoded.webm --resolution 720p\npython scripts/generate_abr.py fixtures/test_video.mp4 /tmp/abr_output/\npython scripts/extract_thumbnails.py fixtures/test_video.mp4 /tmp/thumbnails/ --timestamps 10,30,60\n\n# Expected behavior:\n# - Video transcoding completes successfully\n# - ABR variants created with manifest files\n# - Thumbnail images extracted at correct timestamps\n# - Progress updates displayed during processing\n```\n\n| Validation Step | Command | Expected Result | Troubleshooting |\n|-----------------|---------|-----------------|-----------------|\n| Basic Transcoding | `python scripts/transcode_video.py` | Output video file created, playable | Check FFmpeg installation, validate input file |\n| ABR Generation | `python scripts/generate_abr.py` | Multiple resolution variants + manifest | Verify HLS/DASH configuration, check segment creation |\n| Progress Monitoring | Watch console during transcoding | Regular progress updates, accurate percentages | Check FFmpeg output parsing, validate progress calculation |\n| Thumbnail Extraction | `python scripts/extract_thumbnails.py` | Image files at specified timestamps | Verify timestamp parsing, check frame extraction accuracy |\n\n**Milestone 3: Job Queue and Progress Validation**\n\nThe final milestone validates complete asynchronous processing workflows including job queuing, worker coordination, progress tracking, and webhook notifications. This milestone demonstrates the system's ability to handle production workloads with reliable job processing and real-time progress updates.\n\nValidation encompasses the entire system integration including Redis queue operations, multiple worker processes, webhook delivery, and error recovery mechanisms. The checkpoint ensures the system can handle concurrent processing requests while maintaining job ordering, progress accuracy, and notification delivery reliability.\n\n| Validation Area | Test Procedures | Success Indicators | Scale Requirements |\n|-----------------|-----------------|-------------------|--------------------|\n| Job Submission | Submit multiple jobs with different priorities | Jobs queued and processed in priority order | Handle 100+ concurrent job submissions |\n| Worker Coordination | Start multiple worker processes | Jobs distributed fairly, no duplicate processing | 5+ workers processing simultaneously |\n| Progress Tracking | Monitor real-time progress updates | Accurate stage-based progress reporting | Progress updates within 5-second intervals |\n| Webhook Delivery | Configure webhook endpoints | Notifications delivered reliably with retries | 99%+ delivery success rate |\n| Error Recovery | Simulate worker failures, network issues | Jobs complete despite infrastructure problems | Automatic recovery within 30 seconds |\n\n**Complete System Validation**\n\nThe final validation demonstrates end-to-end system functionality with realistic workloads and production-like conditions. This validation proves the system meets all functional and performance requirements established in the goals and non-goals section.\n\n```bash\n# Milestone 3 complete system validation\ndocker-compose up -d redis webhook-server\npython -m pytest tests/integration/ -v\npython scripts/submit_batch_jobs.py --count 20 --mixed-media fixtures/\npython scripts/monitor_queue_health.py --duration 300\n\n# Expected system behavior:\n# - All integration tests pass\n# - Batch jobs processed successfully with progress tracking\n# - Queue health monitoring shows stable operations\n# - Webhook notifications delivered reliably\n```\n\n| System Validation | Command | Success Criteria | Performance Targets |\n|-------------------|---------|------------------|-------------------|\n| Integration Test Suite | `pytest tests/integration/` | All tests pass, <5 minutes execution | No test failures, consistent timing |\n| Batch Job Processing | `python scripts/submit_batch_jobs.py` | All jobs complete successfully | >10 concurrent jobs, fair processing |\n| Queue Health Monitoring | `python scripts/monitor_queue_health.py` | Stable metrics, no error accumulation | <100ms average queue latency |\n| Webhook Reliability | Monitor webhook delivery logs | >99% delivery success, proper retries | <5 second delivery latency |\n\n**⚠️ Common Milestone Pitfalls**\n\n**Pitfall: Insufficient Test Media Coverage**\nMany implementations fail milestone validation because test fixtures don't cover edge cases like unusual aspect ratios, extreme file sizes, or format-specific quirks. Always include problematic media files that exercise error handling paths and boundary conditions.\n\n**Pitfall: Race Conditions in Progress Updates**\nProgress tracking often fails under concurrent load due to race conditions between progress updates and job completion. Implement proper sequence number validation and atomic update operations to prevent progress reversals and inconsistent state.\n\n**Pitfall: Webhook Delivery Assumptions**\nWebhook notification testing frequently assumes reliable network conditions and responsive endpoints. Production environments have unreliable webhooks, so test with flaky network conditions, slow endpoints, and authentication failures to validate retry logic.\n\n**Pitfall: Resource Cleanup Failures**\nFailed tests often leave temporary files, processes, or queue entries that interfere with subsequent test runs. Implement comprehensive cleanup procedures in test teardown methods and use isolated test environments to prevent cross-test contamination.\n\n### Implementation Guidance\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Test Framework | pytest with basic fixtures | pytest with custom plugins, parameterized tests |\n| Mock Library | unittest.mock for basic mocking | responses library for HTTP mocking, fakeredis for Redis |\n| Media Generation | PIL solid color images | FFmpeg synthetic video generation |\n| Test Orchestration | Manual test execution | Docker Compose with service containers |\n| Assertion Library | Basic assert statements | Custom media validation assertions |\n| Coverage Analysis | pytest-cov for basic coverage | Coverage with branch analysis and media-specific metrics |\n\n**B. Recommended File Structure**\n\n```\nmedia-processing-pipeline/\n├── tests/\n│   ├── unit/\n│   │   ├── test_image_processing.py      ← Image component unit tests\n│   │   ├── test_video_transcoding.py     ← Video component unit tests\n│   │   ├── test_job_queue.py             ← Queue operations unit tests\n│   │   └── test_progress_tracking.py     ← Progress component unit tests\n│   ├── integration/\n│   │   ├── test_end_to_end_workflows.py  ← Complete processing workflows\n│   │   ├── test_queue_integration.py     ← Queue with real Redis\n│   │   └── test_webhook_delivery.py      ← Webhook notification tests\n│   ├── fixtures/\n│   │   ├── images/                       ← Test image files\n│   │   ├── videos/                       ← Test video files\n│   │   └── generate_fixtures.py          ← Synthetic media generation\n│   ├── helpers/\n│   │   ├── test_media_utils.py           ← Media validation helpers\n│   │   ├── mock_servers.py               ← Mock webhook/API servers\n│   │   └── redis_test_utils.py           ← Redis testing utilities\n│   └── conftest.py                       ← pytest configuration and shared fixtures\n├── scripts/\n│   ├── validate_milestone_1.py           ← Milestone 1 validation script\n│   ├── validate_milestone_2.py           ← Milestone 2 validation script\n│   ├── validate_milestone_3.py           ← Milestone 3 validation script\n│   └── performance_benchmark.py          ← Performance testing script\n└── docker-compose.test.yml               ← Test environment services\n```\n\n**C. Infrastructure Starter Code**\n\n**Test Media Generation Utilities**\n\n```python\n\"\"\"Test media generation utilities for creating synthetic test fixtures.\"\"\"\n\nimport os\nimport tempfile\nfrom PIL import Image, ImageDraw\nfrom typing import Tuple, Dict, Any\nimport json\n\nclass TestMediaGenerator:\n    \"\"\"Generates synthetic media files for testing purposes.\"\"\"\n    \n    @staticmethod\n    def create_test_image(width: int, height: int, format: str = 'JPEG', \n                         color: Tuple[int, int, int] = (255, 0, 0)) -> bytes:\n        \"\"\"Create a simple solid color test image in memory.\n        \n        Args:\n            width: Image width in pixels\n            height: Image height in pixels  \n            format: Output format (JPEG, PNG, WebP)\n            color: RGB color tuple\n            \n        Returns:\n            Image data as bytes\n        \"\"\"\n        image = Image.new('RGB', (width, height), color)\n        \n        # Add some visual pattern for resize testing\n        draw = ImageDraw.Draw(image)\n        draw.rectangle([width//4, height//4, 3*width//4, 3*height//4], \n                      outline=(255, 255, 255), width=2)\n        \n        # Save to bytes buffer\n        import io\n        buffer = io.BytesIO()\n        image.save(buffer, format=format, quality=85 if format == 'JPEG' else None)\n        return buffer.getvalue()\n    \n    @staticmethod\n    def create_image_with_metadata(width: int, height: int, exif_data: Dict[str, Any]) -> bytes:\n        \"\"\"Create test image with embedded EXIF metadata.\"\"\"\n        from PIL.ExifTags import TAGS\n        import piexif\n        \n        image = Image.new('RGB', (width, height), (100, 150, 200))\n        \n        # Convert metadata dict to EXIF format\n        exif_dict = {\"0th\": {}, \"Exif\": {}, \"GPS\": {}}\n        \n        if 'orientation' in exif_data:\n            exif_dict[\"0th\"][piexif.ImageIFD.Orientation] = exif_data['orientation']\n        if 'camera_make' in exif_data:\n            exif_dict[\"0th\"][piexif.ImageIFD.Make] = exif_data['camera_make']\n        if 'gps_latitude' in exif_data and 'gps_longitude' in exif_data:\n            exif_dict[\"GPS\"][piexif.GPSIFD.GPSLatitude] = exif_data['gps_latitude'] \n            exif_dict[\"GPS\"][piexif.GPSIFD.GPSLongitude] = exif_data['gps_longitude']\n            \n        exif_bytes = piexif.dump(exif_dict)\n        \n        buffer = io.BytesIO()\n        image.save(buffer, format='JPEG', exif=exif_bytes)\n        return buffer.getvalue()\n\nclass MockWebhookServer:\n    \"\"\"Simple HTTP server for testing webhook delivery.\"\"\"\n    \n    def __init__(self, port: int = 8080):\n        self.port = port\n        self.received_webhooks = []\n        self.response_status = 200\n        self.response_delay = 0\n        \n    def start_server(self):\n        \"\"\"Start mock webhook server in background thread.\"\"\"\n        import threading\n        from http.server import HTTPServer, BaseHTTPRequestHandler\n        \n        class WebhookHandler(BaseHTTPRequestHandler):\n            def do_POST(self):\n                content_length = int(self.headers.get('Content-Length', 0))\n                body = self.rfile.read(content_length)\n                \n                # Record received webhook\n                self.server.webhook_server.received_webhooks.append({\n                    'path': self.path,\n                    'headers': dict(self.headers),\n                    'body': body.decode('utf-8'),\n                    'timestamp': time.time()\n                })\n                \n                # Simulate response delay\n                if self.server.webhook_server.response_delay > 0:\n                    time.sleep(self.server.webhook_server.response_delay)\n                \n                # Send configured response\n                self.send_response(self.server.webhook_server.response_status)\n                self.send_header('Content-Type', 'application/json')\n                self.end_headers()\n                self.wfile.write(b'{\"status\": \"received\"}')\n        \n        server = HTTPServer(('localhost', self.port), WebhookHandler)\n        server.webhook_server = self\n        \n        def run_server():\n            server.serve_forever()\n            \n        self.server_thread = threading.Thread(target=run_server, daemon=True)\n        self.server_thread.start()\n        return server\n        \n    def get_received_webhooks(self):\n        \"\"\"Get list of received webhook requests.\"\"\"\n        return self.received_webhooks.copy()\n        \n    def clear_webhooks(self):\n        \"\"\"Clear received webhook history.\"\"\"\n        self.received_webhooks.clear()\n```\n\n**Redis Testing Utilities**\n\n```python\n\"\"\"Redis testing utilities for queue integration tests.\"\"\"\n\nimport redis\nimport time\nimport json\nfrom typing import Optional, Dict, Any\nfrom contextlib import contextmanager\n\nclass RedisTestManager:\n    \"\"\"Manages Redis connections and cleanup for testing.\"\"\"\n    \n    def __init__(self, host='localhost', port=6379, db=15):  # Use high DB number for tests\n        self.redis_config = {'host': host, 'port': port, 'db': db}\n        self.client = None\n        \n    @contextmanager\n    def redis_connection(self):\n        \"\"\"Context manager providing clean Redis connection.\"\"\"\n        self.client = redis.Redis(**self.redis_config, decode_responses=True)\n        try:\n            # Clear test database\n            self.client.flushdb()\n            yield self.client\n        finally:\n            # Cleanup\n            if self.client:\n                self.client.flushdb()\n                self.client.close()\n                \n    def wait_for_job_completion(self, job_id: str, timeout: int = 30) -> Optional[str]:\n        \"\"\"Wait for job to reach terminal state.\"\"\"\n        start_time = time.time()\n        \n        while time.time() - start_time < timeout:\n            job_data = self.client.hgetall(f\"job:{job_id}\")\n            if job_data and job_data.get('status') in ['completed', 'failed']:\n                return job_data.get('status')\n            time.sleep(0.5)\n            \n        return None  # Timeout\n        \n    def simulate_worker_processing(self, job_message: Dict[str, Any]) -> bool:\n        \"\"\"Simulate worker processing a job for testing.\"\"\"\n        job_id = job_message['job_id']\n        \n        # Mark job as processing\n        self.client.hset(f\"job:{job_id}\", \"status\", \"processing\")\n        self.client.hset(f\"job:{job_id}\", \"started_at\", time.time())\n        \n        # Simulate progress updates\n        for progress in [25, 50, 75, 100]:\n            time.sleep(0.1)  # Brief processing simulation\n            self.client.hset(f\"job:{job_id}\", \"progress_percentage\", progress)\n            \n        # Mark as completed\n        self.client.hset(f\"job:{job_id}\", \"status\", \"completed\") \n        self.client.hset(f\"job:{job_id}\", \"completed_at\", time.time())\n        \n        return True\n```\n\n**D. Core Logic Skeleton Code**\n\n**Image Processing Test Structure**\n\n```python\n\"\"\"Image processing unit test skeletons.\"\"\"\n\nimport pytest\nfrom PIL import Image\nfrom media_processing.image_processor import ImageProcessor, ImageProcessingConfig\n\nclass TestImageProcessing:\n    \"\"\"Unit tests for image processing operations.\"\"\"\n    \n    @pytest.fixture\n    def image_processor(self):\n        \"\"\"Create ImageProcessor instance for testing.\"\"\"\n        return ImageProcessor()\n        \n    @pytest.fixture  \n    def test_image_data(self):\n        \"\"\"Generate test image data for consistent testing.\"\"\"\n        # TODO: Use TestMediaGenerator to create 100x100 RGB test image\n        # TODO: Return image data as bytes for loading tests\n        pass\n        \n    def test_resize_preserve_aspect_ratio(self, image_processor, test_image_data):\n        \"\"\"Test image resizing with aspect ratio preservation.\"\"\"\n        # TODO: Load test image from bytes\n        # TODO: Create ImageProcessingConfig with target dimensions 200x100\n        # TODO: Call resize_image() with PRESERVE_ASPECT resize mode\n        # TODO: Verify output dimensions maintain original aspect ratio\n        # TODO: Check that image quality is preserved after resize\n        pass\n        \n    def test_format_conversion_quality_settings(self, image_processor, test_image_data):\n        \"\"\"Test format conversion with different quality settings.\"\"\"\n        # TODO: Load test image as PIL Image object\n        # TODO: Convert to JPEG with quality=95, measure file size\n        # TODO: Convert to JPEG with quality=50, measure file size  \n        # TODO: Assert that lower quality produces smaller file size\n        # TODO: Verify both outputs are valid JPEG images\n        pass\n        \n    def test_exif_orientation_handling(self, image_processor):\n        \"\"\"Test proper EXIF orientation rotation.\"\"\" \n        # TODO: Create test image with EXIF orientation=6 (90° rotation)\n        # TODO: Process image with metadata handling enabled\n        # TODO: Verify image is rotated correctly based on EXIF data\n        # TODO: Check that output image orientation is normalized\n        # Hint: Use TestMediaGenerator.create_image_with_metadata()\n        pass\n        \n    def test_metadata_privacy_stripping(self, image_processor):\n        \"\"\"Test removal of privacy-sensitive metadata.\"\"\"\n        # TODO: Create image with GPS coordinates and camera data\n        # TODO: Process with metadata_mode='strip_privacy'\n        # TODO: Verify GPS data is removed from output\n        # TODO: Check that basic image data (dimensions, format) is preserved\n        pass\n        \n    def test_memory_limit_validation(self, image_processor):\n        \"\"\"Test memory requirement calculation for large images.\"\"\"\n        # TODO: Calculate memory needed for 8000x8000 RGB image\n        # TODO: Call check_image_memory_requirements()  \n        # TODO: Verify calculation matches expected bytes (width * height * channels)\n        # TODO: Test with different color modes (RGB, RGBA, CMYK)\n        pass\n        \n    def test_corrupted_image_error_handling(self, image_processor):\n        \"\"\"Test graceful handling of corrupted image files.\"\"\"\n        # TODO: Create corrupted image data (truncated JPEG header)\n        # TODO: Attempt to load corrupted image\n        # TODO: Verify specific exception type is raised\n        # TODO: Check error message contains useful diagnostic information\n        pass\n```\n\n**Integration Test Structure**\n\n```python\n\"\"\"Integration test skeletons for complete workflows.\"\"\"\n\nimport pytest\nimport asyncio\nfrom media_processing.api import submit_job, ProcessingJob\nfrom media_processing.queue import JobStatus\n\nclass TestEndToEndWorkflows:\n    \"\"\"Integration tests for complete media processing workflows.\"\"\"\n    \n    @pytest.fixture\n    def redis_test_manager(self):\n        \"\"\"Set up Redis test environment.\"\"\"\n        manager = RedisTestManager()\n        return manager\n        \n    @pytest.fixture\n    def mock_webhook_server(self):\n        \"\"\"Start mock webhook server for notification testing.\"\"\"\n        # TODO: Initialize MockWebhookServer on port 8080\n        # TODO: Start server in background thread\n        # TODO: Return server instance for test assertions\n        pass\n        \n    @pytest.mark.integration\n    def test_image_processing_complete_workflow(self, redis_test_manager, mock_webhook_server):\n        \"\"\"Test complete image processing from submission to completion.\"\"\"\n        with redis_test_manager.redis_connection() as redis:\n            # TODO: Create test image file in temporary directory\n            # TODO: Submit job with multiple output specifications  \n            # TODO: Start worker process to handle job\n            # TODO: Wait for job completion using redis_test_manager.wait_for_job_completion()\n            # TODO: Verify all output files exist with correct formats\n            # TODO: Check webhook notification was delivered\n            # TODO: Validate job status is COMPLETED\n            pass\n            \n    @pytest.mark.integration  \n    def test_video_transcoding_with_progress(self, redis_test_manager):\n        \"\"\"Test video transcoding with progress tracking.\"\"\"\n        with redis_test_manager.redis_connection() as redis:\n            # TODO: Create short test video using FFmpeg\n            # TODO: Submit transcoding job with ABR variants\n            # TODO: Monitor progress updates during processing\n            # TODO: Verify progress moves through expected stages\n            # TODO: Check final output includes HLS manifest and segments\n            # TODO: Validate video segments are playable\n            pass\n            \n    @pytest.mark.integration\n    def test_concurrent_job_processing(self, redis_test_manager):\n        \"\"\"Test multiple jobs processed concurrently.\"\"\"\n        with redis_test_manager.redis_connection() as redis:\n            # TODO: Submit 10 image processing jobs simultaneously\n            # TODO: Start 3 worker processes\n            # TODO: Monitor job completion across all workers\n            # TODO: Verify all jobs complete successfully\n            # TODO: Check jobs are distributed fairly across workers\n            # TODO: Validate no duplicate processing occurs\n            pass\n            \n    @pytest.mark.integration\n    def test_webhook_delivery_retry_logic(self, mock_webhook_server):\n        \"\"\"Test webhook notification retry on delivery failures.\"\"\"\n        # TODO: Configure webhook server to return HTTP 503 initially\n        # TODO: Submit job with webhook notification\n        # TODO: Wait for first webhook delivery attempt (should fail)\n        # TODO: Configure server to return HTTP 200\n        # TODO: Verify webhook is eventually delivered via retry\n        # TODO: Check exponential backoff timing between retries\n        pass\n```\n\n**E. Language-Specific Hints**\n\n- **PIL/Pillow**: Use `Image.save()` with `optimize=True` for smaller file sizes\n- **pytest fixtures**: Use `@pytest.fixture(scope=\"session\")` for expensive setup like Redis connections\n- **Temporary files**: Use `tempfile.TemporaryDirectory()` context manager for automatic cleanup\n- **Redis testing**: Use high database numbers (10-15) to avoid conflicts with development data\n- **FFmpeg subprocess**: Use `subprocess.Popen()` with `PIPE` for stdout/stderr capture\n- **Threading**: Use `threading.Event()` for coordinating between test threads and background workers\n- **Mock HTTP**: `responses` library provides easier HTTP mocking than `unittest.mock`\n- **Binary data**: Use `bytes()` and `io.BytesIO()` for in-memory binary file simulation\n\n**F. Milestone Checkpoints**\n\n**Milestone 1 Checkpoint**\n```bash\n# Run these commands to validate Milestone 1 completion\ncd media-processing-pipeline\npython -m pytest tests/unit/test_image_processing.py::TestImageProcessing -v\npython scripts/validate_milestone_1.py\n\n# Expected: All image processing tests pass, validation script reports success\n# Signs of problems: PIL import errors, memory issues with large images, EXIF orientation bugs\n```\n\n**Milestone 2 Checkpoint** \n```bash\n# Run these commands to validate Milestone 2 completion\npython -m pytest tests/unit/test_video_transcoding.py::TestVideoTranscoding -v  \npython scripts/validate_milestone_2.py --input fixtures/test_video.mp4\n\n# Expected: Video transcoding tests pass, ABR variants generated correctly\n# Signs of problems: FFmpeg not found, progress parsing failures, codec errors\n```\n\n**Milestone 3 Checkpoint**\n```bash\n# Run these commands to validate Milestone 3 completion\ndocker-compose -f docker-compose.test.yml up -d\npython -m pytest tests/integration/ -v\npython scripts/validate_milestone_3.py --concurrent-jobs 10\n\n# Expected: All integration tests pass, concurrent processing works correctly  \n# Signs of problems: Redis connection failures, webhook delivery issues, race conditions in progress tracking\n\n```\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones (1-3) as debugging skills are essential for troubleshooting image processing, video transcoding, and job queue issues across the entire system\n\n### Mental Model: Hospital Emergency Room\n\nThink of debugging a media processing pipeline as running a hospital emergency room. When a patient (processing job) arrives with symptoms (failed processing, stuck progress, resource exhaustion), you need to quickly diagnose the root cause from observable symptoms, apply the right diagnostic tools, and implement targeted treatment while preventing the problem from affecting other patients.\n\nJust as emergency room doctors follow systematic triage protocols—checking vital signs, running targeted tests, consulting specialists—effective media processing debugging requires structured approaches to symptom identification, diagnostic tool usage, and performance analysis. The goal is rapid diagnosis followed by precise intervention that resolves the immediate issue while preventing future occurrences.\n\nThe emergency room analogy extends to resource management: you must balance immediate patient needs with overall system capacity, handle multiple concurrent cases without cross-contamination, and maintain detailed records for pattern recognition and quality improvement.\n\n### Common Symptoms and Diagnosis\n\nEffective debugging starts with systematic symptom-to-cause mapping that enables rapid problem identification and resolution. The media processing pipeline exhibits distinct failure patterns that correspond to specific underlying issues across image processing, video transcoding, and job queue management.\n\n**Job Queue and Worker Coordination Issues**\n\nThe job queue represents the central nervous system of the media processing pipeline. When queue operations fail, the symptoms manifest in predictable patterns that reveal the underlying coordination problems.\n\n| Symptom | Observable Behavior | Likely Root Cause | Diagnostic Steps |\n|---------|-------------------|-------------------|-----------------|\n| Jobs stuck in PENDING forever | `ProcessingJob.status` remains PENDING, no worker picks up jobs | Worker processes crashed or not starting | Check worker process status, verify Redis connectivity, examine worker logs for startup errors |\n| Progress updates stop mid-processing | Job shows 45% complete but no further updates for >5 minutes | Worker crashed during processing without cleanup | Search logs for worker PID, check for OOM kills, verify temp file cleanup |\n| Duplicate job processing | Same input file processed multiple times concurrently | Race condition in job deduplication logic | Check Redis atomic operations, verify `job_id` generation uniqueness |\n| High-priority jobs processed after low-priority | Urgent jobs wait behind normal priority jobs | Priority queue implementation bug | Examine Redis ZADD/ZPOP operations, verify priority score calculation |\n| Memory exhaustion crashes | Worker processes killed with exit code 137 (SIGKILL) | Large media files exceeding worker memory limits | Monitor memory usage patterns, check file size validation |\n\n**Image Processing Failures**\n\nImage processing failures typically stem from format incompatibilities, memory constraints, or metadata handling issues. The failure patterns reveal specific problems in the image manipulation pipeline.\n\n| Symptom | Observable Behavior | Likely Root Cause | Diagnostic Steps |\n|---------|-------------------|-------------------|-----------------|\n| Images rotated incorrectly | Output images appear sideways or upside-down | EXIF orientation tag not processed before resize | Check input EXIF data, verify `ImageMetadata.orientation` handling |\n| Corrupted output files | Generated images cannot be opened or display artifacts | Memory corruption during processing or invalid format conversion | Test with smaller images, check memory allocation patterns |\n| WebP conversion fails silently | WebP format specified but JPEG output produced | Pillow WebP support not installed or misconfigured | Test WebP encoding capability, verify Pillow installation |\n| Thumbnail generation produces black images | Thumbnails created but contain no visible content | Crop coordinates exceed image boundaries | Log crop calculations, verify smart cropping algorithm |\n| EXIF privacy stripping incomplete | Output images still contain GPS coordinates | Metadata handling configuration error | Audit metadata extraction, test privacy stripping modes |\n\n**Video Transcoding and FFmpeg Issues**\n\nVideo transcoding problems often involve FFmpeg integration issues, codec compatibility, or resource management failures during long-running processing operations.\n\n| Symptom | Observable Behavior | Likely Root Cause | Diagnostic Steps |\n|---------|-------------------|-------------------|-----------------|\n| FFmpeg process hangs indefinitely | Video transcoding never completes, no progress updates | FFmpeg waiting for input or misconfigured parameters | Check FFmpeg command construction, verify input file accessibility |\n| Transcoding fails with \"Codec not found\" | FFmpeg exits with codec-related error messages | Target codec not supported in FFmpeg build | Test codec availability, check FFmpeg compilation flags |\n| HLS segments not playable | Individual .ts files generated but playlist broken | Segment alignment or manifest generation issues | Validate HLS manifest syntax, test segment playback individually |\n| Audio sync issues in output | Video and audio streams misaligned in transcoded output | Stream mapping or timing configuration problems | Examine input metadata, verify audio/video synchronization settings |\n| Progress calculation wildly inaccurate | Progress jumps from 10% to 90% instantly | FFmpeg duration parsing failed or progress regex incorrect | Test duration extraction, validate progress parsing logic |\n\n**Webhook and Notification Problems**\n\nWebhook delivery failures create visibility gaps that make it difficult to track job completion and handle errors appropriately in client applications.\n\n| Symptom | Observable Behavior | Likely Root Cause | Diagnostic Steps |\n|---------|-------------------|-------------------|-----------------|\n| Webhooks never delivered | Client never receives job completion notifications | Network connectivity or webhook URL configuration issues | Test webhook URL accessibility, verify HMAC signature generation |\n| Webhook delivery storms | Client receives hundreds of duplicate notifications | Retry logic not tracking successful deliveries | Check webhook delivery tracking, audit retry backoff implementation |\n| Authentication failures | Client rejects webhooks with signature errors | HMAC signature mismatch or clock skew | Verify shared secret configuration, test signature generation |\n| Progress webhooks out of order | Client receives 75% progress after 100% completion | Race conditions in progress update delivery | Check sequence number implementation, verify atomic progress updates |\n| Webhook timeouts | Webhook delivery fails with timeout errors | Client webhook endpoint slow or unresponsive | Monitor webhook response times, implement client-side timeout handling |\n\n> **Decision: Structured Diagnostic Approach**\n> - **Context**: Media processing involves complex interactions between multiple components, making ad-hoc debugging ineffective and time-consuming\n> - **Options Considered**: \n>   1. Reactive debugging responding to individual symptoms\n>   2. Comprehensive logging with manual correlation\n>   3. Structured symptom-to-cause mapping with standardized diagnostic procedures\n> - **Decision**: Implement structured diagnostic approach with symptom classification\n> - **Rationale**: Systematic diagnosis reduces mean-time-to-resolution and enables junior developers to handle complex debugging scenarios effectively\n> - **Consequences**: Requires upfront investment in diagnostic procedures but dramatically improves debugging efficiency and reduces escalation requirements\n\n![Error Recovery Decision Tree](./diagrams/error-recovery-paths.svg)\n\n**Redis and Storage Layer Issues**\n\nThe storage layer provides persistence for job state, progress tracking, and metadata. Storage failures cascade through the entire system and require careful diagnosis to prevent data loss.\n\n| Symptom | Observable Behavior | Likely Root Cause | Diagnostic Steps |\n|---------|-------------------|-------------------|-----------------|\n| Job state inconsistencies | Job shows COMPLETED in Redis but PROCESSING in PostgreSQL | Hybrid storage synchronization failure | Compare Redis and PostgreSQL state, check transaction boundaries |\n| Progress updates rejected | Progress tracking returns false from update operations | Sequence number conflicts or atomic operation failures | Check Redis transaction logs, verify sequence number generation |\n| File cleanup failures | Temporary files accumulate without deletion | Error handling not cleaning up resources properly | Audit file lifecycle management, check error handler cleanup paths |\n| Storage quota exceeded | Jobs fail with disk space errors | Temporary file cleanup not working or storage monitoring absent | Check disk usage patterns, verify cleanup job execution |\n| Redis connection pool exhausted | New jobs cannot be queued with connection timeout errors | Connection leaks or insufficient pool sizing | Monitor Redis connection usage, check connection cleanup |\n\n### Debugging Tools and Techniques\n\nEffective debugging requires a comprehensive toolkit that provides visibility into system behavior, enables rapid problem isolation, and supports root cause analysis across distributed components.\n\n### Mental Model: Detective Investigation Kit\n\nThink of debugging tools as a detective's investigation kit. Just as detectives use fingerprints for identity verification, surveillance cameras for timeline reconstruction, and forensic analysis for evidence correlation, media processing debugging requires specialized tools for different types of evidence collection and analysis.\n\nEach tool serves a specific investigative purpose: logs provide timeline evidence, metrics reveal behavioral patterns, tracing shows interaction sequences, and profiling exposes performance bottlenecks. The key is knowing which tool to use for each type of investigation and how to correlate evidence across multiple sources.\n\n**Structured Logging Implementation**\n\nStructured logging forms the foundation of effective debugging by providing consistent, searchable, and correlatable log data across all system components. The logging strategy must balance information richness with performance impact.\n\n| Component | Log Level | Required Fields | Sample Message Structure |\n|-----------|-----------|----------------|------------------------|\n| API Gateway | INFO, WARN, ERROR | request_id, job_id, user_id, endpoint, duration_ms | `{\"timestamp\": \"2024-01-15T10:30:45Z\", \"level\": \"INFO\", \"component\": \"api\", \"request_id\": \"req_123\", \"job_id\": \"job_456\", \"message\": \"Job submitted successfully\"}` |\n| Job Queue | DEBUG, INFO, ERROR | job_id, worker_id, queue_depth, processing_time | `{\"timestamp\": \"2024-01-15T10:31:00Z\", \"level\": \"DEBUG\", \"component\": \"queue\", \"job_id\": \"job_456\", \"worker_id\": \"worker_01\", \"queue_depth\": 23, \"message\": \"Job dequeued for processing\"}` |\n| Image Processor | INFO, WARN, ERROR | job_id, input_file, output_specs, memory_usage_mb | `{\"timestamp\": \"2024-01-15T10:31:15Z\", \"level\": \"INFO\", \"component\": \"image\", \"job_id\": \"job_456\", \"input_file\": \"photo.jpg\", \"memory_usage_mb\": 450, \"message\": \"Image resize completed\"}` |\n| Video Processor | INFO, WARN, ERROR | job_id, ffmpeg_command, progress_percentage, estimated_remaining | `{\"timestamp\": \"2024-01-15T10:32:30Z\", \"level\": \"INFO\", \"component\": \"video\", \"job_id\": \"job_456\", \"progress_percentage\": 45.2, \"estimated_remaining\": \"2m30s\", \"message\": \"Transcoding progress update\"}` |\n| Progress Tracker | DEBUG, INFO, ERROR | job_id, stage, webhook_url, delivery_attempt | `{\"timestamp\": \"2024-01-15T10:35:45Z\", \"level\": \"INFO\", \"component\": \"progress\", \"job_id\": \"job_456\", \"stage\": \"FORMAT_CONVERSION\", \"webhook_url\": \"https://client.example.com/webhooks\", \"delivery_attempt\": 1, \"message\": \"Webhook delivered successfully\"}` |\n\nThe `job_logging_context` function provides correlation across all log messages for a specific job, enabling end-to-end request tracing through the entire processing pipeline. This correlation proves essential when debugging complex failure scenarios that span multiple components.\n\n**Monitoring and Metrics Collection**\n\nReal-time metrics provide early warning systems for performance degradation and resource constraints before they escalate to critical failures.\n\n| Metric Category | Key Metrics | Collection Frequency | Alert Thresholds |\n|----------------|-------------|---------------------|------------------|\n| Queue Health | Queue depth, processing rate, worker utilization | Every 30 seconds | Queue depth > 1000, processing rate < 10 jobs/minute |\n| Resource Usage | Memory consumption, CPU utilization, disk usage | Every 10 seconds | Memory > 80%, CPU > 90%, disk > 85% |\n| Processing Performance | Job completion time, error rates, format-specific metrics | Per job completion | Error rate > 5%, completion time > 2x average |\n| Webhook Delivery | Delivery success rate, retry frequency, response times | Per webhook attempt | Success rate < 95%, response time > 5 seconds |\n| Storage Operations | Redis operations/second, PostgreSQL query time, file I/O rates | Every 15 seconds | Query time > 100ms, Redis ops/sec > 10000 |\n\n> **Decision: Metrics vs Logs Balance**\n> - **Context**: Need visibility into system behavior without overwhelming storage or affecting performance\n> - **Options Considered**:\n>   1. Comprehensive logging of all operations\n>   2. Metrics-only approach with minimal logging\n>   3. Balanced approach with structured logs and targeted metrics\n> - **Decision**: Balanced approach with job-level tracing and system-level metrics\n> - **Rationale**: Logs provide detailed debugging context while metrics enable proactive monitoring and alerting\n> - **Consequences**: Requires careful log level management and metrics aggregation but provides comprehensive observability\n\n**Diagnostic Command-Line Tools**\n\nCommand-line diagnostic tools enable rapid system inspection during incident response and provide detailed component-level debugging capabilities.\n\n| Tool Purpose | Command Examples | Expected Output | Usage Scenarios |\n|--------------|------------------|------------------|-----------------|\n| Queue Status | `redis-cli LLEN media:queue:high` | `23` (queue depth) | Check job backlog during performance issues |\n| Worker Health | `ps aux \\| grep media_worker` | Process list with memory usage | Verify worker processes running correctly |\n| Job Inspection | `redis-cli HGETALL job:job_456` | Job status and metadata | Debug specific job failure |\n| Progress Check | `redis-cli GET progress:job_456` | JSON progress object | Verify progress tracking accuracy |\n| File Validation | `ffprobe -v quiet -print_format json input.mp4` | Video metadata JSON | Validate input file before processing |\n| Storage Usage | `du -sh /tmp/media_processing/*` | Directory size breakdown | Check temporary file cleanup |\n\n**Log Analysis and Correlation Techniques**\n\nEffective log analysis requires tools and techniques that can correlate events across multiple components and identify patterns in large log volumes.\n\nThe correlation strategy relies on consistent `job_id` and `request_id` fields that enable end-to-end tracing through the processing pipeline. Advanced log analysis uses tools like `grep`, `jq`, and `awk` for command-line investigation combined with log aggregation platforms for pattern recognition.\n\nSample correlation commands demonstrate how to trace a specific job through the entire system:\n\n```bash\n# Extract all log entries for specific job\ngrep \"job_456\" /var/log/media_processing/*.log\n\n# Analyze webhook delivery patterns\njq 'select(.component == \"progress\" and .delivery_attempt > 1)' /var/log/media_processing/webhooks.log\n\n# Find memory-related errors\ngrep -E \"(OOM|memory|killed)\" /var/log/media_processing/*.log | head -20\n```\n\n**Integration with External Monitoring**\n\nExternal monitoring tools provide centralized visibility across distributed deployments and enable automated alerting based on system behavior patterns.\n\n| Integration Type | Tool Examples | Configuration Requirements | Benefits |\n|------------------|---------------|---------------------------|----------|\n| Log Aggregation | ELK Stack, Splunk, CloudWatch | Structured JSON output, log shipping | Centralized search, pattern analysis |\n| Metrics Collection | Prometheus + Grafana, DataDog | StatsD or HTTP endpoints | Real-time dashboards, historical analysis |\n| Alerting Systems | PagerDuty, Slack notifications | Webhook integration | Automated incident response |\n| APM Tools | Jaeger, Zipkin, New Relic | Distributed tracing headers | End-to-end request visibility |\n| Health Checks | Consul, etcd, custom endpoints | HTTP health check endpoints | Service discovery and load balancing |\n\n⚠️ **Pitfall: Log Volume Overwhelming Storage**\nMany implementations generate excessive log volume that fills disk space and makes analysis difficult. The solution requires log level configuration based on environment (DEBUG in development, INFO in production) and log rotation policies that balance retention with storage constraints. Use sampling for high-volume DEBUG messages and ensure log rotation prevents disk exhaustion.\n\n### Performance and Resource Debugging\n\nPerformance debugging in media processing systems requires specialized techniques that account for the unique characteristics of multimedia workloads, including variable processing times, large memory requirements, and I/O-intensive operations.\n\n### Mental Model: Formula One Pit Crew\n\nThink of performance debugging as running a Formula One pit crew during a race. The pit crew must quickly diagnose performance issues (slow lap times, tire wear, fuel consumption) while the race continues, identify bottlenecks that prevent optimal performance, and implement targeted optimizations without disrupting ongoing operations.\n\nJust as pit crews use telemetry data to understand car performance, real-time metrics reveal system bottlenecks. Like mechanics who know that tire temperature affects grip and aerodynamics impact fuel efficiency, media processing performance debugging requires understanding how memory usage affects processing speed, how file size impacts I/O patterns, and how codec selection influences CPU utilization.\n\n**Memory Usage Analysis and Optimization**\n\nMemory management presents the primary performance challenge in media processing due to the large working sets required for high-resolution images and video frames. Memory debugging must identify both peak usage patterns and memory leaks that accumulate over time.\n\n| Memory Issue Type | Symptoms | Diagnostic Approach | Optimization Strategies |\n|-------------------|----------|-------------------|------------------------|\n| Peak Memory Spikes | OOM kills during large file processing | Monitor memory usage per job, profile allocation patterns | Implement streaming processing, reduce buffer sizes |\n| Memory Leaks | Gradual memory growth over time | Track memory usage trends, identify allocation without deallocation | Audit resource cleanup, use memory profiling tools |\n| Fragmentation | Available memory but allocation failures | Monitor memory fragmentation metrics | Implement object pooling, use consistent buffer sizes |\n| Buffer Overflow | Crashes or corruption during processing | Memory sanitizers, bounds checking | Validate input sizes, implement memory limits |\n| Swap Thrashing | Extremely slow processing with high disk I/O | Monitor swap usage and page faults | Reduce memory footprint, increase available RAM |\n\nMemory profiling for image processing requires understanding the relationship between image dimensions and memory requirements. A 4K image (3840x2160) in RGBA format requires approximately 33MB of uncompressed memory, while video processing may require multiple frame buffers simultaneously.\n\n**Processing Time Optimization Strategies**\n\nProcessing time optimization requires identifying bottlenecks in the media processing pipeline and implementing targeted improvements that maintain quality while reducing latency.\n\n| Processing Stage | Common Bottlenecks | Profiling Techniques | Optimization Options |\n|------------------|-------------------|---------------------|---------------------|\n| Image Loading | File I/O and format parsing | Time file read operations, profile decoder performance | Implement format-specific optimizations, use memory-mapped files |\n| Resize Operations | Interpolation algorithm complexity | Compare algorithm performance, measure CPU usage | Choose optimal interpolation method per use case |\n| Format Conversion | Encoding complexity and quality settings | Profile encoder performance, measure compression ratios | Optimize quality settings, use hardware acceleration |\n| Video Transcoding | CPU-intensive encoding operations | Monitor CPU utilization, measure encoding speed | Implement hardware acceleration, optimize FFmpeg parameters |\n| I/O Operations | Disk bandwidth and latency | Monitor disk usage patterns, measure throughput | Use SSD storage, implement async I/O |\n\n> **Decision: Processing Time vs Quality Trade-offs**\n> - **Context**: Media processing involves inherent trade-offs between processing speed, output quality, and resource consumption\n> - **Options Considered**:\n>   1. Optimize for maximum quality regardless of processing time\n>   2. Optimize for minimum processing time with acceptable quality loss\n>   3. Implement configurable quality/speed profiles based on use case\n> - **Decision**: Configurable quality profiles with performance-oriented defaults\n> - **Rationale**: Different use cases have different requirements; thumbnails need speed while archival processing needs quality\n> - **Consequences**: Requires careful profile configuration and testing but provides optimal performance for each use case\n\n**Worker Bottleneck Identification**\n\nWorker bottleneck analysis identifies coordination issues that prevent optimal resource utilization and job throughput across the distributed processing system.\n\n| Bottleneck Type | Observable Symptoms | Root Cause Analysis | Resolution Approaches |\n|-----------------|-------------------|---------------------|----------------------|\n| Queue Starvation | Workers idle while jobs remain queued | Examine job distribution logic, check priority queue implementation | Improve job distribution algorithm, rebalance worker assignments |\n| Resource Contention | Multiple workers competing for shared resources | Monitor file system locks, database connections, memory usage | Implement resource pooling, use worker-specific resources |\n| Coordination Overhead | High messaging overhead relative to processing time | Measure message queue latency, profile worker communication | Optimize message formats, reduce coordination frequency |\n| Uneven Load Distribution | Some workers overloaded while others idle | Analyze job assignment patterns, monitor worker utilization | Implement load-aware scheduling, use worker capability matching |\n| Cascading Failures | Worker failures causing additional workers to fail | Trace failure propagation, examine error handling | Implement circuit breakers, improve error isolation |\n\nWorker performance profiling requires measuring both individual worker efficiency and collective system throughput. Key metrics include jobs processed per hour per worker, average memory usage per job type, and worker restart frequency.\n\n**Storage and I/O Performance Analysis**\n\nStorage performance directly impacts media processing throughput due to the large file sizes involved in video transcoding and high-resolution image processing.\n\n| Storage Metric | Measurement Technique | Performance Targets | Optimization Strategies |\n|---------------|----------------------|-------------------|-------------------------|\n| Read Throughput | Monitor bytes/second during file loading | >500 MB/s for video processing | Use SSD storage, implement read-ahead caching |\n| Write Throughput | Measure output file creation speed | >200 MB/s for encoded output | Optimize write buffer sizes, use async writes |\n| IOPS (Input/Output Operations Per Second) | Count file operations per second | >1000 IOPS for thumbnail generation | Use NVMe storage, batch small file operations |\n| Latency | Measure file open/close times | <10ms for file access operations | Implement file handle pooling, reduce metadata operations |\n| Temporary File Management | Track temp file lifecycle | Zero leaked files after job completion | Implement proper cleanup, use defer/finally patterns |\n\nDatabase performance for job queue operations requires optimization for high-frequency read/write patterns with strong consistency requirements.\n\n| Database Operation | Performance Target | Optimization Approach |\n|-------------------|-------------------|----------------------|\n| Job Queue Operations | <5ms per operation | Use Redis for queue state, optimize data structures |\n| Progress Updates | <2ms per update | Implement atomic operations, use appropriate data types |\n| Metadata Queries | <10ms per query | Index frequently queried fields, use read replicas |\n| Job History | <50ms per complex query | Implement data archiving, optimize query patterns |\n\n**Resource Monitoring and Alerting**\n\nComprehensive resource monitoring enables proactive performance management and prevents resource exhaustion before it impacts job processing.\n\n| Resource Type | Monitoring Metrics | Alert Thresholds | Automated Responses |\n|---------------|-------------------|------------------|-------------------|\n| CPU Utilization | Per-core usage, load average, process CPU time | >80% sustained for 5+ minutes | Scale worker pool, throttle job intake |\n| Memory Usage | RSS, virtual memory, swap usage, memory growth rate | >85% physical memory, any swap usage | Reduce concurrent jobs, restart workers |\n| Disk Space | Used space, free space, growth rate, inode usage | >90% used space, <1GB free | Clean temporary files, archive old jobs |\n| Network Bandwidth | Bytes sent/received, connection count, latency | >80% bandwidth utilization | Throttle uploads, implement traffic shaping |\n| File Handles | Open file count, handle leaks | >80% of system limit | Audit file handle usage, implement handle pooling |\n\n⚠️ **Pitfall: Premature Optimization**\nMany developers optimize components that aren't actual bottlenecks, wasting effort and potentially reducing code maintainability. The solution requires measurement-driven optimization: profile the system under realistic load, identify the true bottlenecks using data, and optimize only the components that measurably impact performance. Use profiling tools consistently and maintain performance benchmarks to validate optimization effectiveness.\n\n⚠️ **Pitfall: Resource Monitoring Overhead**\nExcessive monitoring can itself become a performance bottleneck, consuming CPU and memory resources needed for media processing. The solution requires careful monitoring configuration that balances visibility with overhead. Use sampling for high-frequency metrics, implement monitoring on/off switches for debugging, and ensure monitoring tools don't compete with processing workloads for resources.\n\n> The critical insight for performance debugging is understanding that media processing workloads have fundamentally different characteristics than typical web applications. Video transcoding may legitimately consume 100% CPU for extended periods, image processing may require gigabytes of memory for single operations, and file I/O patterns involve large sequential reads rather than small random accesses. Performance optimization must account for these unique requirements rather than applying generic optimization techniques.\n\n### Implementation Guidance\n\nThis subsection provides practical tools and code examples for implementing comprehensive debugging capabilities across the media processing pipeline.\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Logging Framework | Python `logging` with JSON formatter | Structured logging with `structlog` + ELK Stack |\n| Metrics Collection | Custom StatsD client | Prometheus client with Grafana dashboards |\n| Performance Profiling | `cProfile` and `memory_profiler` | `py-spy` for production profiling |\n| Log Analysis | `grep` + `jq` command line tools | Centralized logging with Elasticsearch |\n| Resource Monitoring | `psutil` for system metrics | Full observability with DataDog/New Relic |\n\n**B. Recommended File/Module Structure:**\n\n```\nmedia_processing/\n  debug/\n    __init__.py                    ← debug utilities exports\n    logging_config.py              ← structured logging setup\n    metrics.py                     ← metrics collection and reporting\n    profiling.py                   ← performance profiling utilities\n    diagnostics.py                 ← diagnostic command implementations\n    monitoring.py                  ← resource monitoring and alerting\n    health_checks.py               ← system health validation\n  tools/\n    job_inspector.py               ← command-line job debugging tool\n    queue_analyzer.py              ← queue performance analysis\n    resource_monitor.py            ← real-time resource monitoring\n    log_analyzer.py                ← log correlation and analysis\n  tests/\n    test_debugging.py              ← debugging utility tests\n    fixtures/                      ← test data for debugging scenarios\n```\n\n**C. Infrastructure Starter Code:**\n\n```python\n# debug/logging_config.py - Complete structured logging setup\nimport logging\nimport json\nimport sys\nfrom datetime import datetime\nfrom contextlib import contextmanager\nfrom typing import Dict, Any, Optional\nimport threading\n\nclass StructuredFormatter(logging.Formatter):\n    \"\"\"JSON formatter for structured logging with media processing context.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.hostname = self._get_hostname()\n        self.process_id = os.getpid()\n    \n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"Convert log record to structured JSON format.\"\"\"\n        log_entry = {\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n            \"hostname\": self.hostname,\n            \"process_id\": self.process_id,\n            \"thread_id\": threading.current_thread().ident\n        }\n        \n        # Add job context if available\n        job_context = getattr(threading.current_thread(), 'job_context', None)\n        if job_context:\n            log_entry.update(job_context)\n        \n        # Add exception info if present\n        if record.exc_info:\n            log_entry[\"exception\"] = self.formatException(record.exc_info)\n        \n        # Add extra fields from log record\n        for key, value in record.__dict__.items():\n            if key not in ['name', 'msg', 'args', 'levelname', 'levelno', \n                          'pathname', 'filename', 'module', 'lineno', \n                          'funcName', 'created', 'msecs', 'relativeCreated',\n                          'thread', 'threadName', 'processName', 'process',\n                          'getMessage', 'exc_info', 'exc_text', 'stack_info']:\n                log_entry[key] = value\n        \n        return json.dumps(log_entry, default=str)\n    \n    def _get_hostname(self) -> str:\n        import socket\n        try:\n            return socket.gethostname()\n        except Exception:\n            return \"unknown\"\n\ndef setup_logging(debug: bool = False, log_file: Optional[str] = None):\n    \"\"\"Configure structured logging for the media processing system.\"\"\"\n    level = logging.DEBUG if debug else logging.INFO\n    \n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(level)\n    \n    # Remove existing handlers\n    for handler in root_logger.handlers[:]:\n        root_logger.removeHandler(handler)\n    \n    # Console handler with structured formatting\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(level)\n    console_handler.setFormatter(StructuredFormatter())\n    root_logger.addHandler(console_handler)\n    \n    # File handler if specified\n    if log_file:\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(level)\n        file_handler.setFormatter(StructuredFormatter())\n        root_logger.addHandler(file_handler)\n    \n    # Configure third-party loggers\n    logging.getLogger('PIL').setLevel(logging.WARNING)\n    logging.getLogger('urllib3').setLevel(logging.WARNING)\n\n@contextmanager\ndef job_logging_context(job_id: str, correlation_id: Optional[str] = None):\n    \"\"\"Context manager for adding job information to all log messages.\"\"\"\n    thread = threading.current_thread()\n    old_context = getattr(thread, 'job_context', {})\n    \n    new_context = old_context.copy()\n    new_context.update({\n        'job_id': job_id,\n        'correlation_id': correlation_id or job_id\n    })\n    \n    thread.job_context = new_context\n    try:\n        yield\n    finally:\n        thread.job_context = old_context\n\n# debug/metrics.py - Metrics collection infrastructure\nimport time\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass, field\nfrom threading import Lock\nimport json\n\n@dataclass\nclass MetricPoint:\n    \"\"\"Individual metric measurement.\"\"\"\n    name: str\n    value: float\n    timestamp: float\n    tags: Dict[str, str] = field(default_factory=dict)\n    \nclass MetricsCollector:\n    \"\"\"Thread-safe metrics collection with configurable backends.\"\"\"\n    \n    def __init__(self):\n        self._counters: Dict[str, float] = {}\n        self._gauges: Dict[str, float] = {}\n        self._histograms: Dict[str, list] = {}\n        self._lock = Lock()\n        self._enabled = True\n    \n    def counter(self, name: str, value: float = 1.0, tags: Optional[Dict[str, str]] = None):\n        \"\"\"Increment a counter metric.\"\"\"\n        if not self._enabled:\n            return\n        \n        with self._lock:\n            key = self._metric_key(name, tags)\n            self._counters[key] = self._counters.get(key, 0) + value\n    \n    def gauge(self, name: str, value: float, tags: Optional[Dict[str, str]] = None):\n        \"\"\"Set a gauge metric to specific value.\"\"\"\n        if not self._enabled:\n            return\n        \n        with self._lock:\n            key = self._metric_key(name, tags)\n            self._gauges[key] = value\n    \n    def histogram(self, name: str, value: float, tags: Optional[Dict[str, str]] = None):\n        \"\"\"Add value to histogram metric.\"\"\"\n        if not self._enabled:\n            return\n        \n        with self._lock:\n            key = self._metric_key(name, tags)\n            if key not in self._histograms:\n                self._histograms[key] = []\n            self._histograms[key].append(value)\n    \n    def timing(self, name: str, duration_seconds: float, tags: Optional[Dict[str, str]] = None):\n        \"\"\"Record timing metric in seconds.\"\"\"\n        self.histogram(f\"{name}.duration\", duration_seconds, tags)\n    \n    def _metric_key(self, name: str, tags: Optional[Dict[str, str]]) -> str:\n        \"\"\"Generate unique key for metric with tags.\"\"\"\n        if not tags:\n            return name\n        tag_str = \",\".join(f\"{k}={v}\" for k, v in sorted(tags.items()))\n        return f\"{name}[{tag_str}]\"\n    \n    def get_snapshot(self) -> Dict[str, Any]:\n        \"\"\"Get current metrics snapshot.\"\"\"\n        with self._lock:\n            return {\n                'counters': self._counters.copy(),\n                'gauges': self._gauges.copy(),\n                'histograms': {k: v.copy() for k, v in self._histograms.items()},\n                'timestamp': time.time()\n            }\n    \n    def reset(self):\n        \"\"\"Reset all metrics (for testing).\"\"\"\n        with self._lock:\n            self._counters.clear()\n            self._gauges.clear()\n            self._histograms.clear()\n\n# Global metrics instance\nmetrics = MetricsCollector()\n\n@contextmanager\ndef timed_operation(operation_name: str, tags: Optional[Dict[str, str]] = None):\n    \"\"\"Context manager for timing operations.\"\"\"\n    start_time = time.time()\n    success = False\n    try:\n        yield\n        success = True\n    finally:\n        duration = time.time() - start_time\n        result_tags = (tags or {}).copy()\n        result_tags['success'] = str(success)\n        metrics.timing(operation_name, duration, result_tags)\n\n# debug/diagnostics.py - Diagnostic utilities\nimport psutil\nimport redis\nfrom typing import Dict, Any, List, Optional\nimport subprocess\nimport json\n\nclass SystemDiagnostics:\n    \"\"\"System-level diagnostic utilities.\"\"\"\n    \n    def __init__(self, redis_client: redis.Redis):\n        self.redis_client = redis_client\n    \n    def check_redis_health(self) -> Dict[str, Any]:\n        \"\"\"Comprehensive Redis health check.\"\"\"\n        try:\n            info = self.redis_client.info()\n            return {\n                'connected': True,\n                'memory_used_mb': info.get('used_memory', 0) / (1024 * 1024),\n                'connected_clients': info.get('connected_clients', 0),\n                'operations_per_sec': info.get('instantaneous_ops_per_sec', 0),\n                'keyspace_hits': info.get('keyspace_hits', 0),\n                'keyspace_misses': info.get('keyspace_misses', 0)\n            }\n        except Exception as e:\n            return {'connected': False, 'error': str(e)}\n    \n    def check_worker_processes(self) -> List[Dict[str, Any]]:\n        \"\"\"Check status of worker processes.\"\"\"\n        workers = []\n        for proc in psutil.process_iter(['pid', 'name', 'memory_info', 'cpu_percent']):\n            try:\n                if 'media_worker' in proc.info['name']:\n                    workers.append({\n                        'pid': proc.info['pid'],\n                        'memory_mb': proc.info['memory_info'].rss / (1024 * 1024),\n                        'cpu_percent': proc.info['cpu_percent'],\n                        'status': proc.status()\n                    })\n            except (psutil.NoSuchProcess, psutil.AccessDenied):\n                continue\n        return workers\n    \n    def check_disk_space(self, paths: List[str]) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Check disk space for critical paths.\"\"\"\n        disk_info = {}\n        for path in paths:\n            try:\n                usage = psutil.disk_usage(path)\n                disk_info[path] = {\n                    'total_gb': usage.total / (1024**3),\n                    'used_gb': usage.used / (1024**3),\n                    'free_gb': usage.free / (1024**3),\n                    'percent_used': (usage.used / usage.total) * 100\n                }\n            except Exception as e:\n                disk_info[path] = {'error': str(e)}\n        return disk_info\n    \n    def get_queue_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive queue statistics.\"\"\"\n        try:\n            stats = {}\n            for priority in ['urgent', 'high', 'normal', 'low']:\n                queue_name = f\"media:queue:{priority}\"\n                depth = self.redis_client.llen(queue_name)\n                stats[f\"{priority}_queue_depth\"] = depth\n            \n            # Get processing job count\n            processing_jobs = self.redis_client.hlen(\"jobs:processing\")\n            stats['jobs_processing'] = processing_jobs\n            \n            # Get failed job count\n            failed_jobs = self.redis_client.llen(\"media:queue:failed\")\n            stats['jobs_failed'] = failed_jobs\n            \n            return stats\n        except Exception as e:\n            return {'error': str(e)}\n```\n\n**D. Core Logic Skeleton Code:**\n\n```python\n# debug/profiling.py - Performance profiling utilities\nimport cProfile\nimport pstats\nimport io\nimport memory_profiler\nfrom typing import Any, Callable, Dict, Optional\nimport time\n\nclass PerformanceProfiler:\n    \"\"\"Comprehensive performance profiling for media processing operations.\"\"\"\n    \n    def __init__(self, enable_memory_profiling: bool = True):\n        self.enable_memory_profiling = enable_memory_profiling\n        self._profiles: Dict[str, Any] = {}\n    \n    def profile_function(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"Profile function execution with CPU and memory analysis.\"\"\"\n        # TODO 1: Create cProfile.Profile instance for CPU profiling\n        # TODO 2: Start memory monitoring if enabled using memory_profiler\n        # TODO 3: Execute function with profiling enabled\n        # TODO 4: Stop profiling and collect statistics\n        # TODO 5: Store profile results with timestamp and function name\n        # TODO 6: Return original function result\n        # Hint: Use memory_profiler.LineProfiler for detailed memory analysis\n        pass\n    \n    def get_memory_usage(self, func: Callable, *args, **kwargs) -> Dict[str, Any]:\n        \"\"\"Get detailed memory usage during function execution.\"\"\"\n        # TODO 1: Record baseline memory usage before execution\n        # TODO 2: Execute function while monitoring memory\n        # TODO 3: Record peak memory usage during execution\n        # TODO 4: Calculate memory delta and peak usage\n        # TODO 5: Return comprehensive memory statistics\n        # Hint: Use memory_profiler.memory_usage with interval parameter\n        pass\n    \n    def analyze_bottlenecks(self, profile_name: str) -> Dict[str, Any]:\n        \"\"\"Analyze CPU profiling results to identify bottlenecks.\"\"\"\n        # TODO 1: Retrieve stored profile data for analysis\n        # TODO 2: Extract top functions by cumulative time\n        # TODO 3: Identify functions with high call counts\n        # TODO 4: Calculate time per call for each function\n        # TODO 5: Generate bottleneck report with recommendations\n        pass\n\nclass ResourceMonitor:\n    \"\"\"Real-time resource monitoring for worker processes.\"\"\"\n    \n    def __init__(self, sample_interval: float = 1.0):\n        self.sample_interval = sample_interval\n        self._monitoring = False\n        self._samples: List[Dict[str, Any]] = []\n    \n    def start_monitoring(self, job_id: str):\n        \"\"\"Start resource monitoring for specific job.\"\"\"\n        # TODO 1: Initialize monitoring state and sample storage\n        # TODO 2: Start background thread for periodic sampling\n        # TODO 3: Record initial resource baseline\n        # TODO 4: Set monitoring flag to enable sample collection\n        # Hint: Use threading.Thread with daemon=True for background monitoring\n        pass\n    \n    def stop_monitoring(self) -> Dict[str, Any]:\n        \"\"\"Stop monitoring and return comprehensive resource report.\"\"\"\n        # TODO 1: Set monitoring flag to false to stop sampling\n        # TODO 2: Wait for background thread to complete\n        # TODO 3: Analyze collected samples for patterns\n        # TODO 4: Calculate peak usage, averages, and trends\n        # TODO 5: Generate summary report with recommendations\n        pass\n    \n    def _collect_sample(self) -> Dict[str, Any]:\n        \"\"\"Collect single resource usage sample.\"\"\"\n        # TODO 1: Get current process CPU and memory usage using psutil\n        # TODO 2: Check disk I/O statistics for current process\n        # TODO 3: Record file handle count and network connections\n        # TODO 4: Include timestamp for temporal analysis\n        # TODO 5: Return complete sample data structure\n        pass\n\n# tools/job_inspector.py - Command-line job debugging tool\nclass JobInspector:\n    \"\"\"Command-line tool for inspecting job state and debugging issues.\"\"\"\n    \n    def __init__(self, redis_client: redis.Redis):\n        self.redis_client = redis_client\n    \n    def inspect_job(self, job_id: str) -> Dict[str, Any]:\n        \"\"\"Get comprehensive job state and history.\"\"\"\n        # TODO 1: Retrieve job data from Redis using job_id\n        # TODO 2: Get current progress information\n        # TODO 3: Collect job processing history and state transitions\n        # TODO 4: Check for associated temporary files\n        # TODO 5: Analyze job for common failure patterns\n        # TODO 6: Return complete job inspection report\n        pass\n    \n    def diagnose_stuck_job(self, job_id: str) -> Dict[str, Any]:\n        \"\"\"Diagnose why job appears stuck in processing.\"\"\"\n        # TODO 1: Check job status and last progress update time\n        # TODO 2: Verify worker process is still running\n        # TODO 3: Check for temporary files and processing artifacts\n        # TODO 4: Analyze memory usage of worker process\n        # TODO 5: Determine if job should be marked failed or retried\n        pass\n    \n    def analyze_queue_health(self) -> Dict[str, Any]:\n        \"\"\"Analyze overall queue health and performance.\"\"\"\n        # TODO 1: Get queue depths across all priority levels\n        # TODO 2: Calculate processing rates and throughput\n        # TODO 3: Identify jobs that have been pending too long\n        # TODO 4: Check worker utilization and availability\n        # TODO 5: Generate queue health report with recommendations\n        pass\n```\n\n**E. Language-Specific Hints:**\n\n- Use `logging.getLogger(__name__)` in each module to create component-specific loggers\n- `psutil.Process().memory_info().rss` provides resident set size for memory usage\n- `redis-py` client provides `info()` method for Redis diagnostics\n- Use `contextlib.contextmanager` for resource monitoring decorators\n- `json.dumps(obj, default=str)` handles datetime serialization in log messages\n- `threading.local()` provides thread-safe storage for job context\n- `subprocess.run()` with `capture_output=True` captures command output for analysis\n\n**F. Milestone Checkpoint:**\n\nAfter implementing the debugging infrastructure:\n\n**What to run:**\n```bash\npython -m tools.job_inspector --job-id job_123 --diagnose\npython -m debug.monitoring --check-health\npython -c \"from debug.logging_config import setup_logging; setup_logging(debug=True)\"\n```\n\n**Expected behavior:**\n- Structured JSON logs appear in console with job correlation IDs\n- Job inspector returns comprehensive job state information\n- Health checks report Redis connectivity and worker process status\n- Metrics collection tracks job processing rates and resource usage\n\n**Signs of success:**\n- Log messages include consistent `job_id` fields for correlation\n- Performance profiling identifies bottlenecks in media processing functions\n- Resource monitoring detects memory spikes during large file processing\n- Queue health checks provide actionable insights for performance optimization\n\n**What to verify manually:**\n- Submit test job and trace log messages through entire pipeline\n- Verify webhook delivery failures trigger appropriate retry logic\n- Confirm memory monitoring detects resource exhaustion scenarios\n- Test diagnostic tools provide useful information for debugging failures\n\n\n## Future Extensions and Scalability\n\n> **Milestone(s):** All milestones (1-3) as this section provides architectural patterns and extension points that build upon the foundational image processing, video transcoding, and job queue capabilities\n\n### Mental Model: Growing Entertainment Studio\n\nThink of the media processing pipeline as a small independent film studio that started with basic equipment and a handful of employees. Initially, they could handle simple photo shoots and short video projects in a single office. As their reputation grows and demand increases, they face several expansion challenges: they need multiple production facilities (horizontal scaling), sophisticated equipment like motion capture and AI-powered editing tools (advanced processing features), and partnerships with distributors, streaming platforms, and equipment rental companies (external integrations). Each growth phase requires careful planning to maintain quality while dramatically increasing capacity and capabilities.\n\nThe key insight is that successful scaling isn't just about adding more resources—it's about evolving the entire operational model. A studio that simply adds more editing bays without upgrading their project management systems, client communication processes, and delivery pipelines will quickly become chaotic and inefficient. Similarly, our media processing pipeline must evolve its architecture, not just multiply its components.\n\n## Horizontal Scaling Patterns\n\n### Multi-Node Deployment Architecture\n\n**Resource-aware scheduling** becomes critical when distributing processing jobs across multiple nodes. Unlike the single-node deployment where all worker processes share the same hardware resources, multi-node deployments must consider the heterogeneous capabilities of different machines. Some nodes might have GPU acceleration for video transcoding, while others excel at CPU-intensive image processing tasks.\n\nThe **job queue** evolution requires sophisticated routing logic that goes beyond simple priority-based distribution. Each processing job must be matched with nodes that have the appropriate hardware capabilities, available memory, and current load levels. This transforms the simple Redis-based queue into a distributed scheduling system with node registration, capability advertising, and intelligent job placement.\n\n> **Decision: Distributed Queue Architecture**\n> - **Context**: Single Redis instance becomes bottleneck and single point of failure as worker nodes scale beyond 10-15 machines\n> - **Options Considered**: \n>   - Redis Cluster with consistent hashing\n>   - RabbitMQ federation across data centers\n>   - Apache Kafka with partitioned topics\n> - **Decision**: Redis Cluster for job queue with RabbitMQ for inter-node coordination\n> - **Rationale**: Redis Cluster provides horizontal scaling for job distribution while maintaining sub-millisecond job pop latency. RabbitMQ handles complex routing patterns for progress updates and administrative messages that don't require Redis-level performance.\n> - **Consequences**: Introduces complexity of managing two message broker systems but enables independent scaling of job throughput vs coordination message handling\n\n| Queue Component | Single Node | Multi-Node Cluster |\n|-----------------|-------------|-------------------|\n| Job Distribution | Simple Redis LPOP | Redis Cluster with hash slots |\n| Node Discovery | Static configuration | Dynamic registration with heartbeats |\n| Load Balancing | OS process scheduler | Custom weighted round-robin |\n| Health Monitoring | Process-level checks | Network-aware health probes |\n| Failure Recovery | Process restart | Node failover with job reassignment |\n\n**Worker coordination** across nodes requires a fundamentally different approach than local process management. The system must track which nodes are healthy, what their current capacity utilization looks like, and how to redistribute work when nodes fail or new ones join the cluster. This involves implementing a **distributed consensus mechanism** for cluster membership and a **gossip protocol** for sharing load information.\n\nThe coordination layer maintains several critical data structures across the cluster:\n\n| Data Structure | Purpose | Replication Strategy |\n|----------------|---------|---------------------|\n| Node Registry | Track active nodes and capabilities | Raft consensus with leader election |\n| Job Assignments | Map active jobs to processing nodes | Replicated state machine |\n| Load Metrics | CPU, memory, queue depth per node | Eventually consistent gossip |\n| Health Status | Node availability and performance | Heartbeat with failure detection |\n\n### Load Balancing Strategies\n\n**Geographic distribution** adds another layer of complexity to load balancing. Media files are often large, and network transfer costs become significant when moving multi-gigabyte video files between data centers. The ideal architecture processes media files on nodes that are network-close to the storage location, but this creates uneven load distribution challenges.\n\nSmart load balancing must consider multiple factors simultaneously:\n\n1. **Network locality**: Prefer nodes in the same data center as the source media file\n2. **Specialized hardware**: Route video transcoding jobs to GPU-enabled nodes\n3. **Current load**: Avoid overloading nodes that are already processing intensive jobs\n4. **Historical performance**: Learn which nodes complete similar jobs fastest\n5. **Cost optimization**: Balance processing speed against infrastructure costs\n\nThe load balancer maintains a **capability matrix** that tracks each node's hardware specifications, current resource utilization, and recent performance metrics. When a new job arrives, the balancer scores potential nodes using a weighted algorithm that considers all these factors.\n\n> **Decision: Adaptive Load Balancing Algorithm**\n> - **Context**: Static load balancing wastes GPU resources and creates uneven processing times across different media types\n> - **Options Considered**:\n>   - Round-robin with manual node tagging\n>   - Least-connections with capability filtering  \n>   - Machine learning-based predictive scheduling\n> - **Decision**: Weighted scoring algorithm with runtime capability learning\n> - **Rationale**: Provides good performance without ML complexity. Scoring weights can be tuned based on observed job completion patterns and resource utilization metrics.\n> - **Consequences**: Requires maintaining performance history and periodic rebalancing, but achieves 30-40% better resource utilization than simple strategies\n\n### Distributed Storage Integration\n\n**Object storage backends** like Amazon S3, Google Cloud Storage, or Azure Blob Storage become essential for multi-node deployments. Local file storage doesn't work when jobs can be processed on any node in the cluster. However, integrating distributed storage introduces new challenges around data transfer costs, processing locality, and temporary file management.\n\nThe storage abstraction layer must handle several scenarios seamlessly:\n\n| Scenario | Challenge | Solution Pattern |\n|----------|-----------|------------------|\n| Large input files | Network transfer time dominates processing | Stream processing with progressive download |\n| Multiple output formats | Parallel generation on different nodes | Distributed fan-out with result aggregation |\n| Temporary processing files | Cleanup across node failures | Distributed lease management with TTL |\n| Bandwidth optimization | Minimize redundant transfers | Content-addressed caching with deduplication |\n\n**Caching strategies** become critical for performance when dealing with distributed storage. Frequently accessed source files should be cached locally on multiple nodes, while processed outputs might benefit from edge caching closer to end users. The cache invalidation logic must handle scenarios where source files are updated or processing parameters change.\n\nA sophisticated caching layer implements multiple tiers:\n\n1. **Local node cache**: Fast SSD storage for active job files\n2. **Cluster shared cache**: High-speed network storage shared across nodes\n3. **Edge cache**: CDN integration for processed output delivery\n4. **Cold storage**: Long-term archival with slower retrieval times\n\n### Auto-scaling and Resource Management\n\n**Dynamic scaling policies** must react to both queue depth and resource utilization patterns. Unlike web services that scale primarily based on request volume, media processing workloads have highly variable resource requirements. A single 4K video transcoding job might consume more resources than 100 image resize operations.\n\nThe auto-scaling system monitors multiple metrics simultaneously:\n\n| Metric Type | Scaling Signal | Response Action |\n|-------------|----------------|-----------------|\n| Queue Depth | Jobs waiting > 5 minutes | Add general-purpose nodes |\n| GPU Utilization | Video queue backlog | Add GPU-enabled nodes |\n| Memory Pressure | Failed jobs due to OOM | Add high-memory nodes |\n| Network Bandwidth | Transfer times > SLA | Add nodes in new regions |\n| Cost Optimization | Low utilization periods | Remove excess capacity |\n\n**Resource quotas and limits** prevent individual jobs from consuming excessive resources and impacting other workloads. The quota system must be sophisticated enough to handle legitimate large media files while preventing abuse or runaway processing jobs.\n\n## Advanced Processing Features\n\n### AI-Powered Content Analysis\n\n**Smart cropping algorithms** represent a significant evolution beyond simple center-crop or face detection. Modern AI-powered cropping uses computer vision models trained on millions of images to understand visual composition, subject importance, and aesthetic principles. These systems can identify the most visually interesting regions of an image and crop accordingly, even for complex scenes with multiple subjects.\n\nThe smart cropping pipeline involves several stages:\n\n1. **Content analysis**: Object detection, face recognition, and scene understanding using pre-trained neural networks\n2. **Composition scoring**: Evaluate different crop regions using rules of thirds, leading lines, and symmetry principles  \n3. **Multi-format optimization**: Generate different crops optimized for square thumbnails, banner images, and mobile screens\n4. **A/B testing integration**: Track engagement metrics to continuously improve cropping decisions\n\n| AI Model Component | Input | Output | Computational Cost |\n|-------------------|-------|---------|-------------------|\n| Object Detection | Full resolution image | Bounding boxes with confidence | 50-200ms GPU time |\n| Saliency Mapping | Image regions | Visual importance heatmap | 20-100ms GPU time |\n| Composition Analysis | Crop candidates | Aesthetic quality scores | 10-50ms CPU time |\n| Face Detection | Image regions | Face locations and orientations | 5-30ms GPU time |\n\n**Content-aware optimization** goes beyond smart cropping to include automated quality adjustments, color correction, and format selection based on image content. For example, images with large areas of solid color compress better as PNG, while photographic content benefits from JPEG compression. AI models can analyze image characteristics and automatically select optimal processing parameters.\n\nThe content analysis results feed into processing decisions:\n\n- **Format selection**: Choose JPEG for photos, PNG for graphics, WebP for web delivery\n- **Compression settings**: Adjust quality levels based on content complexity and target use case  \n- **Color space conversion**: Optimize for target display characteristics (sRGB for web, P3 for modern displays)\n- **Sharpening parameters**: Apply appropriate sharpening based on content type and output size\n\n### Automated Video Enhancement\n\n**Scene detection and keyframe extraction** enables sophisticated video processing that goes beyond simple transcoding. AI-powered scene detection can identify shot boundaries, recognize different types of content (talking heads vs action sequences), and extract the most representative frames for thumbnail generation.\n\nAdvanced scene analysis provides rich metadata for downstream processing:\n\n| Scene Analysis Output | Use Cases | Implementation Complexity |\n|----------------------|-----------|---------------------------|\n| Shot boundaries | Chapter markers, thumbnail selection | Medium - temporal analysis |\n| Content classification | Encoding parameter optimization | High - requires trained models |\n| Motion analysis | Adaptive bitrate tuning | Medium - optical flow calculation |\n| Audio classification | Music vs speech detection | High - audio ML models |\n\n**Adaptive transcoding parameters** use content analysis to optimize encoding settings for each video segment. Fast-motion action sequences might benefit from higher bitrates and keyframe frequency, while talking-head segments can use more aggressive compression without quality loss.\n\nThe adaptive transcoding system builds a **content fingerprint** for each video:\n\n1. **Motion analysis**: Calculate optical flow vectors to measure scene complexity\n2. **Spatial complexity**: Analyze texture and detail levels in each frame\n3. **Temporal consistency**: Measure how much content changes between frames\n4. **Audio characteristics**: Detect music, speech, silence, and ambient sound\n\nThese fingerprints drive encoding decisions:\n\n- **Variable bitrate curves**: Allocate bits based on scene complexity\n- **Keyframe placement**: Insert keyframes at scene boundaries for better seeking\n- **Encoding speed vs quality**: Use slower, higher-quality encoding for important content\n- **Format selection**: Choose codecs optimized for content characteristics\n\n### Batch Processing Optimization\n\n**Pipeline parallelization** enables processing multiple versions of the same media file simultaneously across different nodes. Instead of sequentially generating thumbnail, mobile, and desktop versions, the system can create all variants in parallel and combine the results.\n\nThe parallel processing coordinator must handle several challenges:\n\n| Challenge | Impact | Solution Pattern |\n|-----------|--------|------------------|\n| Resource contention | Multiple jobs competing for same input file | Shared read-only cache with reference counting |\n| Result synchronization | Coordinating completion of parallel tasks | Distributed barrier with timeout handling |\n| Partial failure handling | Some variants succeed while others fail | Per-variant status tracking with retry logic |\n| Progress aggregation | Combining progress from multiple parallel jobs | Weighted progress calculation across variants |\n\n**Dependency-aware scheduling** optimizes processing pipelines by understanding relationships between different processing steps. For example, video thumbnail extraction can begin as soon as transcoding produces the first few seconds of output, rather than waiting for complete transcoding to finish.\n\nThe dependency graph represents processing relationships:\n\n```\nInput Video → Scene Analysis → Thumbnail Extraction\n            → Transcoding → [Mobile, Desktop, Streaming variants]\n            → Audio Extraction → Podcast version\n```\n\nEach node in the graph can start as soon as its dependencies are satisfied, dramatically reducing overall processing time for complex workflows.\n\n## External Integration Opportunities\n\n### Content Delivery Network Integration\n\n**Multi-CDN strategies** become essential for global media delivery at scale. Different CDNs have varying performance characteristics across geographic regions and content types. A sophisticated system can route video streaming through CDN A while serving image thumbnails through CDN B based on real-time performance monitoring.\n\nThe CDN integration layer must handle several responsibilities:\n\n| Integration Aspect | Challenge | Solution Approach |\n|-------------------|-----------|-------------------|\n| Geographic routing | Optimal CDN selection per region | Real-time latency monitoring with failover |\n| Cache invalidation | Coordinating updates across multiple CDNs | Distributed invalidation with retry logic |\n| Cost optimization | Minimizing bandwidth and storage costs | Intelligent tier placement and caching policies |\n| Format negotiation | Serving optimal formats per client | Edge-side logic with capability detection |\n\n**Edge computing integration** pushes simple processing tasks closer to end users. Image resizing for common thumbnail sizes can happen at CDN edge nodes rather than central processing clusters, reducing latency and bandwidth costs. However, this requires careful orchestration between central processing and edge capabilities.\n\nEdge processing scenarios include:\n\n1. **On-demand resizing**: Generate thumbnail sizes not pre-computed centrally\n2. **Format conversion**: Convert between WebP, AVIF, and JPEG based on browser support\n3. **Quality adaptation**: Adjust compression based on detected connection speed\n4. **Watermark application**: Add region-specific branding at delivery time\n\n### Cloud Storage Backend Integration\n\n**Multi-cloud storage strategies** provide resilience, cost optimization, and geographic distribution capabilities. Different cloud providers offer varying price points and performance characteristics for media storage and processing workloads.\n\nThe storage abstraction layer manages complexity across providers:\n\n| Storage Tier | Primary Provider | Backup Provider | Use Case |\n|-------------|------------------|-----------------|----------|\n| Hot storage | AWS S3 Standard | Google Cloud Storage | Active media files |\n| Warm storage | S3 Infrequent Access | Azure Cool Blob Storage | Recently processed files |\n| Cold storage | S3 Glacier | Google Archive Storage | Long-term backup |\n| Processing cache | Local NVMe SSD | S3 Standard | Active job temporary files |\n\n**Storage lifecycle management** automatically transitions media files between storage tiers based on access patterns and retention policies. Newly uploaded files remain in hot storage for immediate processing, while older processed outputs move to cheaper storage tiers over time.\n\nLifecycle policies consider multiple factors:\n\n- **Access frequency**: Files not accessed for 30 days move to warm storage\n- **Processing requirements**: Source files for ongoing jobs remain in hot storage\n- **Compliance requirements**: Legal holds prevent automatic archive transitions\n- **Cost optimization**: Balance access time against storage costs\n\n### Third-Party Processing Service Integration\n\n**Hybrid processing architectures** combine on-premise processing with cloud-based specialized services. For example, basic image resizing might happen locally while AI-powered content tagging uses cloud APIs from Google Vision or Amazon Rekognition.\n\nThe service integration layer provides a unified interface across providers:\n\n| Processing Type | Local Implementation | Cloud Service Option | Fallback Strategy |\n|----------------|---------------------|---------------------|------------------|\n| Image resizing | Pillow + custom algorithms | None needed | N/A |\n| Object detection | Local ML models | Google Vision API | Disable feature on API failure |\n| Video transcoding | FFmpeg cluster | AWS Elemental MediaConvert | Queue jobs until service recovery |\n| Content moderation | Rule-based filtering | Amazon Rekognition | Manual review queue |\n\n**API rate limiting and cost management** becomes critical when integrating external services. The system must track usage across different API endpoints and implement circuit breakers to prevent runaway costs from processing spikes or API failures.\n\nRate limiting strategies include:\n\n1. **Per-service quotas**: Daily/monthly limits for each external service\n2. **Priority-based throttling**: Reserve API capacity for high-priority jobs\n3. **Cost-based fallbacks**: Switch to local processing when API costs exceed thresholds\n4. **Batching optimization**: Combine multiple requests to reduce API call overhead\n\n### Monitoring and Analytics Integration\n\n**Processing analytics** provide insights into system performance, job patterns, and optimization opportunities. Integration with tools like Prometheus, Grafana, and ELK stack enables comprehensive monitoring of the distributed media processing pipeline.\n\nKey metrics to track include:\n\n| Metric Category | Example Metrics | Monitoring Tool | Alert Conditions |\n|----------------|-----------------|-----------------|------------------|\n| Processing Performance | Jobs/hour, processing time percentiles | Prometheus + Grafana | p95 processing time > SLA |\n| Resource Utilization | CPU, memory, GPU usage per node | Node Exporter | Sustained > 90% utilization |\n| Quality Metrics | Transcoding quality scores, error rates | Custom collectors | Error rate > 1% |\n| Business Metrics | Processing costs, throughput trends | Application metrics | Cost growth > revenue growth |\n\n**Machine learning feedback loops** use processing outcomes to continuously improve system performance. For example, tracking which smart crop selections get the most user engagement can improve the AI model training data over time.\n\nFeedback collection mechanisms:\n\n- **A/B testing frameworks**: Compare different processing parameters and measure outcomes\n- **User engagement tracking**: Monitor click-through rates, view duration, and interaction patterns\n- **Quality assessment**: Automated quality metrics and occasional human evaluation\n- **Performance correlation**: Link processing parameters to user satisfaction metrics\n\n> The key insight for successful scaling is that each integration point becomes a potential failure mode, but also an opportunity for optimization. The system must be designed with graceful degradation in mind—if the AI cropping service fails, fall back to center crop rather than failing the entire job.\n\n### Implementation Guidance\n\nThis implementation guidance focuses on the foundational patterns and infrastructure needed to support horizontal scaling and advanced features. The code provides working examples that can be extended for specific scaling scenarios.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Container Orchestration | Docker Compose | Kubernetes with custom operators |\n| Service Discovery | Static configuration files | Consul or etcd with DNS integration |\n| Load Balancing | HAProxy with static config | Envoy with dynamic configuration |\n| Monitoring | Prometheus + Grafana | Datadog or New Relic APM |\n| Log Aggregation | File-based logging | ELK Stack or Splunk |\n| Secret Management | Environment variables | HashiCorp Vault or AWS Secrets Manager |\n\n#### Recommended File Structure\n\n```\nmedia-pipeline/\n├── cmd/\n│   ├── coordinator/           ← cluster coordinator service\n│   ├── worker/               ← distributed worker nodes\n│   └── scheduler/            ← intelligent job scheduler\n├── internal/\n│   ├── scaling/\n│   │   ├── cluster.go        ← node registration and discovery\n│   │   ├── balancer.go       ← load balancing algorithms  \n│   │   ├── autoscaler.go     ← dynamic scaling policies\n│   │   └── storage.go        ← distributed storage abstraction\n│   ├── ai/\n│   │   ├── cropping.go       ← smart cropping algorithms\n│   │   ├── analysis.go       ← content analysis pipelines\n│   │   └── models.go         ← AI model integration\n│   ├── integration/\n│   │   ├── cdn.go           ← CDN provider integrations\n│   │   ├── cloud.go         ← multi-cloud storage\n│   │   └── external.go      ← third-party API clients\n│   └── monitoring/\n│       ├── metrics.go       ← performance metrics collection\n│       ├── tracing.go       ← distributed tracing\n│       └── alerts.go        ← alerting and notification\n├── deployments/\n│   ├── kubernetes/          ← K8s manifests\n│   ├── terraform/           ← infrastructure as code\n│   └── docker/              ← container configurations\n└── docs/\n    ├── scaling-guide.md     ← operational scaling procedures\n    ├── integration-api.md   ← external API documentation\n    └── monitoring-runbook.md ← troubleshooting procedures\n```\n\n#### Infrastructure Starter Code\n\n**Cluster Node Registry** - Complete implementation for node discovery and health tracking:\n\n```python\nimport asyncio\nimport json\nimport time\nfrom typing import Dict, List, Optional, Set\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nimport aioredis\nimport psutil\nimport GPUtil\n\nclass NodeCapability(Enum):\n    IMAGE_PROCESSING = \"image\"\n    VIDEO_TRANSCODING = \"video\" \n    GPU_ACCELERATION = \"gpu\"\n    HIGH_MEMORY = \"highmem\"\n\n@dataclass\nclass NodeInfo:\n    node_id: str\n    hostname: str\n    capabilities: List[NodeCapability]\n    max_concurrent_jobs: int\n    current_load: float\n    memory_gb: int\n    gpu_count: int\n    network_region: str\n    last_heartbeat: float\n    \n    def to_dict(self) -> Dict:\n        return {\n            **asdict(self),\n            'capabilities': [c.value for c in self.capabilities],\n            'last_heartbeat': self.last_heartbeat\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict) -> 'NodeInfo':\n        return cls(\n            node_id=data['node_id'],\n            hostname=data['hostname'], \n            capabilities=[NodeCapability(c) for c in data['capabilities']],\n            max_concurrent_jobs=data['max_concurrent_jobs'],\n            current_load=data['current_load'],\n            memory_gb=data['memory_gb'],\n            gpu_count=data['gpu_count'],\n            network_region=data['network_region'],\n            last_heartbeat=data['last_heartbeat']\n        )\n\nclass ClusterRegistry:\n    def __init__(self, redis_url: str, heartbeat_interval: int = 30):\n        self.redis_url = redis_url\n        self.heartbeat_interval = heartbeat_interval\n        self.redis = None\n        self.local_node_id = None\n        self._heartbeat_task = None\n        \n    async def initialize(self) -> bool:\n        \"\"\"Initialize Redis connection and start heartbeat.\"\"\"\n        try:\n            self.redis = aioredis.from_url(self.redis_url)\n            await self.redis.ping()\n            return True\n        except Exception as e:\n            print(f\"Failed to connect to Redis: {e}\")\n            return False\n    \n    async def register_node(self, node_info: NodeInfo) -> bool:\n        \"\"\"Register this node in the cluster registry.\"\"\"\n        self.local_node_id = node_info.node_id\n        node_key = f\"cluster:nodes:{node_info.node_id}\"\n        \n        try:\n            # Store node information with TTL\n            await self.redis.hset(\n                node_key,\n                mapping={\n                    \"info\": json.dumps(node_info.to_dict()),\n                    \"registered_at\": time.time()\n                }\n            )\n            await self.redis.expire(node_key, self.heartbeat_interval * 3)\n            \n            # Add to active nodes set\n            await self.redis.sadd(\"cluster:active_nodes\", node_info.node_id)\n            \n            # Start heartbeat task\n            self._heartbeat_task = asyncio.create_task(\n                self._heartbeat_loop(node_info)\n            )\n            \n            print(f\"Node {node_info.node_id} registered successfully\")\n            return True\n            \n        except Exception as e:\n            print(f\"Failed to register node: {e}\")\n            return False\n    \n    async def _heartbeat_loop(self, node_info: NodeInfo):\n        \"\"\"Continuous heartbeat to maintain node registration.\"\"\"\n        while True:\n            try:\n                # Update current system metrics\n                node_info.current_load = psutil.cpu_percent(interval=1)\n                node_info.last_heartbeat = time.time()\n                \n                # Update GPU utilization if available\n                if node_info.gpu_count > 0:\n                    try:\n                        gpus = GPUtil.getGPUs()\n                        if gpus:\n                            node_info.current_load = max(\n                                node_info.current_load,\n                                gpus[0].load * 100\n                            )\n                    except:\n                        pass  # GPU monitoring is optional\n                \n                node_key = f\"cluster:nodes:{node_info.node_id}\"\n                await self.redis.hset(\n                    node_key,\n                    mapping={\"info\": json.dumps(node_info.to_dict())}\n                )\n                await self.redis.expire(node_key, self.heartbeat_interval * 3)\n                \n                await asyncio.sleep(self.heartbeat_interval)\n                \n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                print(f\"Heartbeat error: {e}\")\n                await asyncio.sleep(5)  # Retry on error\n    \n    async def get_active_nodes(self) -> List[NodeInfo]:\n        \"\"\"Get list of all currently active nodes.\"\"\"\n        try:\n            node_ids = await self.redis.smembers(\"cluster:active_nodes\")\n            nodes = []\n            \n            for node_id in node_ids:\n                node_key = f\"cluster:nodes:{node_id.decode()}\"\n                node_data = await self.redis.hget(node_key, \"info\")\n                \n                if node_data:\n                    node_info = NodeInfo.from_dict(json.loads(node_data))\n                    \n                    # Check if node is still alive (heartbeat within 3 intervals)\n                    if time.time() - node_info.last_heartbeat < self.heartbeat_interval * 3:\n                        nodes.append(node_info)\n                    else:\n                        # Remove stale node\n                        await self._cleanup_stale_node(node_id.decode())\n            \n            return nodes\n            \n        except Exception as e:\n            print(f\"Failed to get active nodes: {e}\")\n            return []\n    \n    async def _cleanup_stale_node(self, node_id: str):\n        \"\"\"Remove stale node from registry.\"\"\"\n        await self.redis.srem(\"cluster:active_nodes\", node_id)\n        await self.redis.delete(f\"cluster:nodes:{node_id}\")\n        print(f\"Removed stale node: {node_id}\")\n    \n    async def find_best_nodes(self, \n                            job_requirements: Dict,\n                            count: int = 1) -> List[NodeInfo]:\n        \"\"\"Find best nodes for a job based on requirements.\"\"\"\n        active_nodes = await self.get_active_nodes()\n        \n        if not active_nodes:\n            return []\n        \n        # Filter nodes by capabilities\n        required_capabilities = job_requirements.get('capabilities', [])\n        if required_capabilities:\n            filtered_nodes = []\n            for node in active_nodes:\n                node_caps = {cap.value for cap in node.capabilities}\n                if set(required_capabilities).issubset(node_caps):\n                    filtered_nodes.append(node)\n            active_nodes = filtered_nodes\n        \n        # Score nodes based on current load and capabilities\n        scored_nodes = []\n        for node in active_nodes:\n            score = self._calculate_node_score(node, job_requirements)\n            scored_nodes.append((score, node))\n        \n        # Sort by score (higher is better) and return top nodes\n        scored_nodes.sort(key=lambda x: x[0], reverse=True)\n        return [node for _, node in scored_nodes[:count]]\n    \n    def _calculate_node_score(self, node: NodeInfo, requirements: Dict) -> float:\n        \"\"\"Calculate suitability score for a node.\"\"\"\n        score = 100.0\n        \n        # Penalize high load\n        score -= node.current_load * 0.5\n        \n        # Bonus for GPU capability if needed\n        if 'gpu' in requirements.get('capabilities', []):\n            if NodeCapability.GPU_ACCELERATION in node.capabilities:\n                score += 20.0\n            else:\n                score -= 50.0  # Heavy penalty if GPU required but not available\n        \n        # Bonus for sufficient memory\n        required_memory = requirements.get('min_memory_gb', 4)\n        if node.memory_gb >= required_memory:\n            score += 10.0\n        else:\n            score -= 30.0\n        \n        # Network locality bonus\n        preferred_region = requirements.get('network_region')\n        if preferred_region and node.network_region == preferred_region:\n            score += 15.0\n        \n        return max(0.0, score)  # Ensure non-negative score\n    \n    async def shutdown(self):\n        \"\"\"Clean shutdown of registry client.\"\"\"\n        if self._heartbeat_task:\n            self._heartbeat_task.cancel()\n            try:\n                await self._heartbeat_task\n            except asyncio.CancelledError:\n                pass\n        \n        if self.local_node_id and self.redis:\n            await self._cleanup_stale_node(self.local_node_id)\n        \n        if self.redis:\n            await self.redis.close()\n```\n\n**Intelligent Load Balancer** - Complete implementation with weighted scoring:\n\n```python\nimport asyncio\nimport heapq\nimport time\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass JobType(Enum):\n    IMAGE_RESIZE = \"image_resize\"\n    VIDEO_TRANSCODE = \"video_transcode\"  \n    THUMBNAIL_GENERATE = \"thumbnail_gen\"\n    BATCH_PROCESS = \"batch_process\"\n\n@dataclass\nclass JobRequirements:\n    job_type: JobType\n    estimated_duration: int  # seconds\n    memory_mb: int\n    cpu_cores: int\n    requires_gpu: bool\n    input_size_mb: int\n    priority: int\n    network_region: Optional[str] = None\n    \nclass LoadBalancer:\n    def __init__(self, cluster_registry: ClusterRegistry):\n        self.registry = cluster_registry\n        self.node_assignments = {}  # node_id -> list of assigned jobs\n        self.performance_history = {}  # (node_id, job_type) -> avg_duration\n        self.last_rebalance = time.time()\n        self.rebalance_interval = 60  # seconds\n        \n    async def assign_job(self, job: 'ProcessingJob') -> Optional[str]:\n        \"\"\"Assign job to the most suitable node.\"\"\"\n        requirements = self._extract_job_requirements(job)\n        \n        # Get candidate nodes\n        candidates = await self.registry.find_best_nodes(\n            {\n                'capabilities': self._job_type_to_capabilities(requirements.job_type),\n                'min_memory_gb': requirements.memory_mb // 1024,\n                'network_region': requirements.network_region\n            },\n            count=5  # Get top 5 candidates for detailed scoring\n        )\n        \n        if not candidates:\n            print(f\"No suitable nodes found for job {job.job_id}\")\n            return None\n        \n        # Score candidates with detailed algorithm\n        best_node = None\n        best_score = -1.0\n        \n        for node in candidates:\n            score = await self._calculate_detailed_score(node, requirements)\n            if score > best_score:\n                best_score = score\n                best_node = node\n        \n        if best_node:\n            # Track assignment for load balancing\n            if best_node.node_id not in self.node_assignments:\n                self.node_assignments[best_node.node_id] = []\n            self.node_assignments[best_node.node_id].append(job.job_id)\n            \n            print(f\"Assigned job {job.job_id} to node {best_node.node_id} (score: {best_score:.2f})\")\n            return best_node.node_id\n        \n        return None\n    \n    async def _calculate_detailed_score(self, \n                                      node: NodeInfo, \n                                      requirements: JobRequirements) -> float:\n        \"\"\"Calculate detailed suitability score for node.\"\"\"\n        base_score = 100.0\n        \n        # Current load penalty (0-40 point penalty)\n        load_penalty = node.current_load * 0.4\n        base_score -= load_penalty\n        \n        # Memory adequacy (bonus for having enough, penalty for insufficient)\n        memory_ratio = node.memory_gb * 1024 / requirements.memory_mb\n        if memory_ratio >= 2.0:\n            base_score += 15.0  # Plenty of memory\n        elif memory_ratio >= 1.5:\n            base_score += 10.0  # Adequate memory\n        elif memory_ratio >= 1.0:\n            base_score += 0.0   # Just enough memory\n        else:\n            base_score -= 30.0  # Insufficient memory\n        \n        # GPU requirement matching\n        if requirements.requires_gpu:\n            if NodeCapability.GPU_ACCELERATION in node.capabilities:\n                base_score += 25.0  # GPU available\n            else:\n                return 0.0  # Cannot handle GPU jobs\n        \n        # Historical performance bonus\n        perf_key = (node.node_id, requirements.job_type.value)\n        if perf_key in self.performance_history:\n            avg_duration = self.performance_history[perf_key]\n            expected_duration = requirements.estimated_duration\n            \n            if avg_duration < expected_duration * 0.8:\n                base_score += 10.0  # Consistently fast\n            elif avg_duration > expected_duration * 1.2:\n                base_score -= 10.0  # Consistently slow\n        \n        # Current assignment load penalty\n        current_assignments = len(self.node_assignments.get(node.node_id, []))\n        if current_assignments >= node.max_concurrent_jobs:\n            return 0.0  # Node at capacity\n        elif current_assignments >= node.max_concurrent_jobs * 0.8:\n            base_score -= 20.0  # Node nearly at capacity\n        \n        # Network locality bonus\n        if (requirements.network_region and \n            node.network_region == requirements.network_region):\n            base_score += 12.0\n        \n        # Priority job bonus for less loaded nodes\n        if requirements.priority >= 8:  # High priority\n            base_score += (100 - node.current_load) * 0.1\n        \n        return max(0.0, base_score)\n    \n    def _extract_job_requirements(self, job: 'ProcessingJob') -> JobRequirements:\n        \"\"\"Extract resource requirements from processing job.\"\"\"\n        # This would analyze the job's output specifications to determine requirements\n        \n        # Default requirements based on job type\n        if any('video' in spec.format for spec in job.output_specifications):\n            return JobRequirements(\n                job_type=JobType.VIDEO_TRANSCODE,\n                estimated_duration=300,  # 5 minutes default\n                memory_mb=2048,\n                cpu_cores=4,\n                requires_gpu=True,\n                input_size_mb=100,  # Estimated\n                priority=job.priority.value\n            )\n        else:\n            return JobRequirements(\n                job_type=JobType.IMAGE_RESIZE,\n                estimated_duration=30,   # 30 seconds default\n                memory_mb=512,\n                cpu_cores=1,\n                requires_gpu=False,\n                input_size_mb=10,   # Estimated\n                priority=job.priority.value\n            )\n    \n    def _job_type_to_capabilities(self, job_type: JobType) -> List[str]:\n        \"\"\"Map job types to required node capabilities.\"\"\"\n        capability_map = {\n            JobType.IMAGE_RESIZE: ['image'],\n            JobType.VIDEO_TRANSCODE: ['video', 'gpu'],\n            JobType.THUMBNAIL_GENERATE: ['image'],\n            JobType.BATCH_PROCESS: ['image', 'video']\n        }\n        return capability_map.get(job_type, [])\n    \n    async def record_job_completion(self, \n                                  node_id: str,\n                                  job_id: str, \n                                  job_type: JobType,\n                                  duration: int):\n        \"\"\"Record job completion for performance tracking.\"\"\"\n        # Update performance history\n        perf_key = (node_id, job_type.value)\n        if perf_key not in self.performance_history:\n            self.performance_history[perf_key] = duration\n        else:\n            # Exponential moving average\n            self.performance_history[perf_key] = (\n                0.7 * self.performance_history[perf_key] + 0.3 * duration\n            )\n        \n        # Remove from current assignments\n        if node_id in self.node_assignments:\n            try:\n                self.node_assignments[node_id].remove(job_id)\n            except ValueError:\n                pass  # Job not in list\n        \n        # Check if rebalancing is needed\n        if time.time() - self.last_rebalance > self.rebalance_interval:\n            await self._rebalance_if_needed()\n            self.last_rebalance = time.time()\n    \n    async def _rebalance_if_needed(self):\n        \"\"\"Rebalance load across nodes if imbalance detected.\"\"\"\n        active_nodes = await self.registry.get_active_nodes()\n        if len(active_nodes) < 2:\n            return  # No rebalancing needed with < 2 nodes\n        \n        # Calculate load distribution\n        loads = []\n        for node in active_nodes:\n            current_jobs = len(self.node_assignments.get(node.node_id, []))\n            load_ratio = current_jobs / node.max_concurrent_jobs\n            loads.append((load_ratio, node.node_id))\n        \n        loads.sort()\n        min_load = loads[0][0]\n        max_load = loads[-1][0]\n        \n        # If imbalance > 30%, log recommendation\n        if max_load - min_load > 0.3:\n            print(f\"Load imbalance detected: {min_load:.2f} to {max_load:.2f}\")\n            print(\"Consider implementing job migration for better balance\")\n    \n    async def get_cluster_statistics(self) -> Dict:\n        \"\"\"Get current cluster load statistics.\"\"\"\n        active_nodes = await self.registry.get_active_nodes()\n        \n        total_capacity = sum(node.max_concurrent_jobs for node in active_nodes)\n        total_assigned = sum(len(jobs) for jobs in self.node_assignments.values())\n        \n        node_stats = []\n        for node in active_nodes:\n            assigned_jobs = len(self.node_assignments.get(node.node_id, []))\n            utilization = assigned_jobs / node.max_concurrent_jobs if node.max_concurrent_jobs > 0 else 0\n            \n            node_stats.append({\n                'node_id': node.node_id,\n                'assigned_jobs': assigned_jobs,\n                'max_jobs': node.max_concurrent_jobs,\n                'utilization': utilization,\n                'cpu_load': node.current_load,\n                'capabilities': [cap.value for cap in node.capabilities]\n            })\n        \n        return {\n            'total_nodes': len(active_nodes),\n            'total_capacity': total_capacity,\n            'total_assigned': total_assigned,\n            'cluster_utilization': total_assigned / total_capacity if total_capacity > 0 else 0,\n            'node_statistics': node_stats\n        }\n```\n\n#### Core Logic Skeleton Code\n\n**Auto-scaling Controller** - Framework for dynamic scaling decisions:\n\n```python\nclass AutoScaler:\n    def __init__(self, cluster_registry: ClusterRegistry, \n                 load_balancer: LoadBalancer,\n                 config: Dict):\n        self.registry = cluster_registry\n        self.load_balancer = load_balancer\n        self.config = config\n        self.scaling_decisions = []\n        \n    async def evaluate_scaling_needs(self) -> Dict[str, int]:\n        \"\"\"\n        Evaluate current cluster state and return scaling recommendations.\n        \n        Returns:\n            Dict mapping node types to desired count changes\n            Example: {'gpu_nodes': +2, 'general_nodes': -1}\n        \"\"\"\n        # TODO 1: Get current cluster statistics from load balancer\n        # TODO 2: Analyze queue depth by job type from Redis\n        # TODO 3: Calculate average wait times for different job types  \n        # TODO 4: Check if any resource type is consistently over 80% utilized\n        # TODO 5: Check if any resource type is consistently under 30% utilized\n        # TODO 6: Apply scaling policies from config (min/max nodes, cooldown periods)\n        # TODO 7: Return scaling recommendations with justification\n        # Hint: Use exponential moving averages for load metrics to avoid thrashing\n        pass\n    \n    async def execute_scaling_action(self, node_type: str, count_change: int) -> bool:\n        \"\"\"\n        Execute scaling action through infrastructure provider.\n        \n        Args:\n            node_type: Type of nodes to scale ('gpu', 'general', 'highmem')\n            count_change: Positive to scale up, negative to scale down\n        \n        Returns:\n            True if scaling action was initiated successfully\n        \"\"\"\n        # TODO 1: Validate scaling action against configured limits\n        # TODO 2: For scale down: ensure nodes to terminate have no active jobs\n        # TODO 3: For scale up: determine optimal instance types/sizes\n        # TODO 4: Call infrastructure API (AWS Auto Scaling, K8s HPA, etc.)\n        # TODO 5: Record scaling action with timestamp and reason\n        # TODO 6: Set cooldown period to prevent rapid scaling oscillations\n        # Hint: Implement graceful scale-down by draining nodes before termination\n        pass\n```\n\n**Smart Content Analysis** - Framework for AI-powered processing decisions:\n\n```python\nclass ContentAnalyzer:\n    def __init__(self, model_config: Dict):\n        self.models = {}\n        self.model_config = model_config\n        \n    async def analyze_image_content(self, image_path: str) -> Dict:\n        \"\"\"\n        Analyze image content to optimize processing parameters.\n        \n        Returns:\n            Analysis results including optimal formats, quality settings, crop regions\n        \"\"\"\n        # TODO 1: Load image and extract basic properties (dimensions, format, size)\n        # TODO 2: Run object detection to identify main subjects and regions of interest\n        # TODO 3: Calculate saliency map to understand visual importance across image\n        # TODO 4: Analyze color distribution and complexity for compression optimization\n        # TODO 5: Detect faces and important objects for smart cropping recommendations\n        # TODO 6: Generate optimal crop coordinates for different aspect ratios\n        # TODO 7: Recommend optimal output formats based on content characteristics\n        # TODO 8: Calculate quality settings that balance file size and visual quality\n        # Hint: Cache analysis results using content hash to avoid recomputation\n        pass\n    \n    async def optimize_video_encoding(self, video_path: str, \n                                    target_formats: List[str]) -> Dict:\n        \"\"\"\n        Analyze video content to optimize encoding parameters.\n        \n        Returns:\n            Optimized encoding parameters for each target format\n        \"\"\"\n        # TODO 1: Extract video metadata (duration, resolution, bitrate, codec)\n        # TODO 2: Perform scene detection to identify different content types\n        # TODO 3: Analyze motion vectors and temporal complexity per scene\n        # TODO 4: Detect content types (talking head, action, graphics, text)\n        # TODO 5: Calculate optimal bitrate curves for variable rate encoding\n        # TODO 6: Determine keyframe placement at scene boundaries\n        # TODO 7: Select encoder presets based on content complexity\n        # TODO 8: Generate separate parameter sets for each target format\n        # Hint: Use ffprobe for metadata extraction and scene detection\n        pass\n```\n\n#### Milestone Checkpoints\n\n**Horizontal Scaling Validation**:\n1. Deploy cluster registry with 3 nodes: `python -m pytest tests/test_cluster_registry.py -v`\n2. Verify node discovery: Check that all nodes appear in Redis registry within 60 seconds\n3. Test load balancer: Submit 10 mixed jobs, verify distribution across nodes based on capabilities\n4. Simulate node failure: Stop one node, verify jobs get reassigned within 2 minutes\n5. Test auto-scaling: Generate sustained load, verify new nodes join cluster automatically\n\n**Advanced Features Integration**:\n1. Deploy AI analysis services: `python -m pytest tests/test_content_analysis.py -v`\n2. Upload test images with different characteristics (photos, graphics, text)\n3. Verify smart cropping generates different crops for square vs landscape outputs\n4. Test adaptive video encoding with sample talking-head vs action footage\n5. Validate that content analysis improves processing quality metrics by 15%+\n\n**External Integration Testing**:\n1. Configure multi-CDN setup with test accounts\n2. Upload processed media and verify delivery through optimal CDN per region\n3. Test storage lifecycle: Verify files transition to cheaper storage after 30 days\n4. Load test third-party APIs with rate limiting and fallback logic\n5. Validate monitoring dashboards show all scaling and processing metrics\n\nSigns of successful implementation:\n- **Cluster coordination**: Nodes register/deregister cleanly, load balancing distributes work optimally\n- **Intelligent scaling**: System scales up under load and scales down during quiet periods\n- **Quality improvements**: AI-powered features measurably improve output quality\n- **Cost optimization**: Multi-tier storage and intelligent CDN routing reduce operational costs\n- **Reliability**: System gracefully handles node failures, API outages, and traffic spikes\n\n\n## Glossary and Reference\n\n> **Milestone(s):** All milestones (1-3) as this terminology reference supports understanding across image processing, video transcoding, and job queue components\n\n### Mental Model: Technical Dictionary for Digital Media\n\nThink of this glossary as a technical dictionary specifically curated for digital media processing systems. Just as a medical dictionary helps doctors communicate precisely about anatomical structures and procedures, this reference ensures everyone on the team uses consistent terminology when discussing codecs, queuing systems, and processing workflows. Each term has been carefully chosen to reflect industry standards while maintaining clarity for developers at all experience levels.\n\nThe terminology is organized into three categories that mirror the major architectural domains of our media processing pipeline: media processing concepts that deal with the technical aspects of manipulating images and videos, architecture and queue concepts that handle the distributed systems challenges of coordinating work across multiple processes, and acronyms that provide quick reference for the alphabet soup of technical abbreviations common in media processing.\n\n### Media Processing Terms\n\nThe media processing domain contains a rich vocabulary of technical terms that describe how digital content is analyzed, transformed, and optimized. These terms form the foundation for understanding how images and videos are manipulated programmatically.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **processing job** | Unit of work containing input file and output specifications that defines a complete media transformation task | Core concept used throughout job queue and worker coordination |\n| **media metadata** | Technical information extracted from media files including dimensions, format details, creation timestamps, and embedded data | Essential for processing decisions and output optimization |\n| **output specification** | Configuration defining desired processing output including format, dimensions, quality settings, and optimization parameters | Drives all media transformation operations |\n| **interpolation algorithm** | Mathematical method for calculating new pixel values during resize operations, affecting quality and performance | Critical for image resizing quality - Lanczos for downscaling, bicubic for upscaling |\n| **EXIF orientation** | Metadata tag indicating camera rotation when photo was taken, requiring image rotation before processing | Common source of bugs - must rotate before resize/crop operations |\n| **color space conversion** | Transformation between different color representation systems like RGB, CMYK, and YUV | Required for format conversion and display optimization |\n| **smart cropping** | Automated cropping that preserves visually important image regions using content analysis algorithms | Preserves subject matter better than center-crop for thumbnails |\n| **metadata stripping** | Removal of embedded information like GPS coordinates and camera details for privacy protection | Security requirement for public-facing image processing |\n| **lossy compression** | Compression that reduces file size by discarding some image data, trading quality for smaller files | JPEG and WebP compression - quality parameter controls data loss |\n| **aspect ratio preservation** | Maintaining original width to height ratio during resize operations to prevent image distortion | Default behavior - use letterboxing or cropping when aspect ratios don't match |\n| **adaptive bitrate streaming** | Delivery technique with multiple quality variants allowing clients to switch based on network conditions | Core video streaming technology - requires synchronized segments |\n| **quality ladder** | Set of video renditions at different resolutions and bitrates optimized for various network conditions | Standard includes 240p, 360p, 480p, 720p, 1080p variants |\n| **keyframe alignment** | Synchronized keyframe placement across ABR variants enabling smooth quality switching | Critical for seamless adaptive streaming experience |\n| **CRF encoding** | Constant rate factor approach maintaining consistent perceptual quality across video content | Preferred over target bitrate for quality-focused encoding |\n| **GOP structure** | Group of pictures organization with keyframes and predicted frames affecting seek performance | Keyframe interval of 2-4 seconds optimal for streaming |\n| **container format** | File wrapper that holds video, audio and metadata streams in a standardized structure | MP4 for broad compatibility, WebM for web-optimized delivery |\n| **codec compatibility matrix** | Validation table ensuring codec/container combinations work on target platforms | H.264 + MP4 for maximum compatibility, VP9 + WebM for efficiency |\n| **hardware acceleration** | GPU-based video encoding for improved performance and reduced CPU usage | Significant speedup but requires compatible hardware and drivers |\n| **progressive download** | Streaming technique allowing playback before complete file download using optimized file structure | Requires 'faststart' flag to move metadata to file beginning |\n| **segment alignment** | Synchronized segment boundaries across ABR quality variants for smooth switching | Essential for DASH and HLS adaptive streaming |\n\n### Architecture and Queue Terms\n\nThe distributed systems architecture of our media processing pipeline introduces concepts related to coordinating work across multiple processes, managing job lifecycles, and ensuring reliable processing under various failure conditions.\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **job queue** | Message queue system for distributing processing tasks across available worker processes | Central coordination mechanism using Redis or RabbitMQ |\n| **worker process** | Background process that executes media processing operations by consuming jobs from the queue | Isolated processes with resource limits and health monitoring |\n| **progress tracking** | System for monitoring and reporting job completion status through multiple processing stages | Real-time updates via webhooks and queryable status API |\n| **webhook notification** | HTTP callback sent on job status changes including progress updates and completion events | Reliable delivery with retry logic and signature verification |\n| **resource-aware scheduling** | Job distribution based on node capabilities and current resource utilization | Prevents overloading workers with memory-intensive video processing |\n| **stage-based progress** | Progress reporting based on processing phases rather than simple percentage complete | More accurate than time-based estimation for complex operations |\n| **exponential backoff** | Retry strategy with increasing delays between attempts to prevent overwhelming failed services | Standard pattern: base_delay * (backoff_factor ^ attempt_number) |\n| **job lifecycle** | Progression of job through pending, processing, completed/failed states with defined transitions | State machine prevents invalid transitions and ensures consistency |\n| **serialization** | Converting data structures to/from JSON for storage and transmission between components | Consistent format enables cross-language compatibility |\n| **state machine** | Formal model governing valid job status transitions and preventing invalid state changes | Ensures job integrity and proper cleanup of resources |\n| **resource constraints** | Limits on memory, CPU, and processing time for jobs to prevent system overload | Video processing limited by memory, image processing by CPU |\n| **dead letter queue** | Queue for jobs that failed permanently after exhausting retry attempts | Prevents infinite retry loops while preserving failed jobs for analysis |\n| **worker coordination** | Distributed process management including job assignment, health monitoring, and scaling | Maintains optimal worker pool size based on queue depth |\n| **heartbeat monitoring** | Health check system for worker processes to detect failures and trigger recovery | Regular status updates enable prompt detection of worker crashes |\n| **priority queue** | Queue that processes higher priority jobs first while maintaining fairness for lower priority work | Urgent jobs bypass normal queue while preventing starvation |\n| **atomic operation** | Indivisible database operation that prevents race conditions in job status updates | Critical for maintaining consistency in distributed processing |\n| **deduplication** | Preventing duplicate job processing through idempotency keys and job fingerprinting | Prevents wasted resources and conflicting outputs |\n| **graceful shutdown** | Orderly termination allowing job completion before process exit | Prevents job corruption and resource leaks during deployment |\n| **sequence number** | Monotonically increasing identifier for preventing out-of-order progress updates | Ensures progress only moves forward despite network delays |\n| **notification threshold** | Progress percentage that triggers webhook delivery to reduce notification spam | Typically 10%, 25%, 50%, 75%, 90%, 100% to balance updates with overhead |\n| **HMAC signature** | Cryptographic hash for webhook authentication and integrity verification | Prevents spoofed notifications using shared secret |\n| **monotonic progression** | Progress values that only increase, never decrease, preventing confusing status reversals | Enforced through sequence numbers and validation logic |\n| **stage transition** | Moving from one processing phase to the next with appropriate progress weight calculation | Enables accurate overall progress despite varying stage complexity |\n| **webhook delivery storm** | Excessive notification traffic that overwhelms recipient systems | Prevented through batching, rate limiting, and threshold-based delivery |\n| **progress reversal** | Backward movement of progress percentage due to race conditions between workers | Prevented through atomic updates and sequence number validation |\n| **hybrid storage** | Combination of Redis for real-time access and PostgreSQL for durability | Redis for active jobs, PostgreSQL for historical data and recovery |\n| **idempotency key** | Unique identifier for detecting and preventing duplicate operations | Prevents double-processing when clients retry requests |\n| **message serialization** | Converting data structures to JSON for storage and transmission in message queues | Standardized format enables language-agnostic worker processes |\n| **circuit breaker** | Pattern to prevent cascade failures when external dependencies become unavailable | Fails fast when FFmpeg or storage systems are unresponsive |\n| **retry storm** | Overwhelming retry attempts that prevent system recovery during outages | Prevented through exponential backoff and circuit breakers |\n| **resource cleanup** | Systematic cleanup of temporary files and resources after job completion or failure | Essential for preventing disk space exhaustion |\n| **worker recovery** | Process of detecting and restarting failed worker processes | Maintains processing capacity despite individual worker failures |\n| **error classification** | Categorizing errors to determine appropriate retry strategy | Transient, permanent, and resource errors require different handling |\n| **horizontal scaling** | Distributing processing across multiple nodes for increased capacity | Add more worker nodes rather than upgrading individual machines |\n| **cluster coordination** | Managing distributed nodes through service discovery and health monitoring | Enables dynamic scaling and load balancing across regions |\n| **adaptive load balancing** | Dynamic job assignment using weighted scoring algorithms based on node performance | Considers CPU, memory, and historical processing times |\n| **auto-scaling policies** | Rules for automatically adding or removing nodes based on demand metrics | Responds to queue depth and processing latency thresholds |\n| **content-aware optimization** | Using AI analysis to optimize processing parameters for specific media content | Adjusts encoding settings based on scene complexity and content type |\n| **smart cropping algorithms** | AI-powered cropping that preserves visually important image regions | Identifies faces, text, and salient objects for optimal framing |\n| **multi-CDN strategies** | Using multiple content delivery networks for optimal global performance | Reduces latency and provides redundancy for media delivery |\n| **storage lifecycle management** | Automatically transitioning files between storage tiers based on access patterns | Hot storage for active processing, cold storage for archives |\n| **hybrid processing architectures** | Combining on-premise processing with cloud-based specialized services | Balances cost, latency, and capability requirements |\n| **circuit breaker pattern** | Preventing cascade failures when external dependencies become unavailable | Monitors failure rates and temporarily disables failing services |\n| **graceful degradation** | Maintaining core functionality when advanced features or dependencies fail | Continues basic processing when AI services or optimization features fail |\n\n### Acronym Quick Reference\n\nThe media processing domain makes extensive use of technical acronyms and abbreviations that can be overwhelming for developers new to the field. This reference provides quick definitions and context for the most commonly encountered terms.\n\n| Acronym | Full Form | Definition | Usage Context |\n|---------|-----------|------------|---------------|\n| **HLS** | HTTP Live Streaming | Apple's adaptive bitrate streaming protocol using M3U8 playlists and TS segments | Primary streaming format for iOS and Safari compatibility |\n| **DASH** | Dynamic Adaptive Streaming over HTTP | ISO standard for adaptive bitrate streaming using MPD manifests and fragmented MP4 | Industry standard streaming protocol with broad device support |\n| **EXIF** | Exchangeable Image File Format | Metadata standard for digital images including camera settings and GPS coordinates | Contains orientation, timestamp, and privacy-sensitive location data |\n| **CRF** | Constant Rate Factor | Video encoding mode that maintains consistent perceptual quality | Preferred over target bitrate for quality-focused encoding (18-23 range) |\n| **GOP** | Group of Pictures | Video compression structure organizing keyframes and predicted frames | Affects seek performance and adaptive streaming segment boundaries |\n| **ABR** | Adaptive Bitrate | Streaming technique with multiple quality variants for network adaptation | Core technology enabling smooth playback across varying network conditions |\n| **API** | Application Programming Interface | Contract defining how software components communicate with each other | REST endpoints for job submission and status queries |\n| **JSON** | JavaScript Object Notation | Lightweight data interchange format used for configuration and messaging | Standard serialization format for job definitions and progress updates |\n| **HTTP** | Hypertext Transfer Protocol | Application protocol for distributed systems communication | Transport layer for API requests and webhook notifications |\n| **HMAC** | Hash-based Message Authentication Code | Cryptographic hash function for message authentication and integrity | Secures webhook notifications against spoofing and tampering |\n| **GPU** | Graphics Processing Unit | Specialized processor optimized for parallel computations | Hardware acceleration for video encoding and AI-powered image analysis |\n| **CPU** | Central Processing Unit | Primary processor handling general computation tasks | Image processing and job coordination workloads |\n| **RAM** | Random Access Memory | Volatile memory for active data storage during processing | Critical resource constraint for large video processing operations |\n| **SSD** | Solid State Drive | Flash-based storage providing fast random access for temporary files | Preferred for video transcoding working directory |\n| **CDN** | Content Delivery Network | Distributed network of servers for efficient content delivery | Optimizes delivery of processed media files to end users |\n| **RGB** | Red Green Blue | Additive color space used for digital displays and image processing | Standard color representation for most image manipulation operations |\n| **CMYK** | Cyan Magenta Yellow Black | Subtractive color space used for print media | Requires conversion to RGB for digital processing |\n| **YUV** | Luma Chroma | Color space separating brightness from color information | Efficient for video compression and broadcast applications |\n| **DPI** | Dots Per Inch | Resolution measurement for print media and high-density displays | Affects image quality for print output and retina displays |\n| **IPTC** | International Press Telecommunications Council | Metadata standard for news and media organizations | Contains copyright, caption, and editorial information |\n| **XMP** | Extensible Metadata Platform | Adobe's metadata framework for creative files | Comprehensive metadata system supporting custom fields |\n| **MIME** | Multipurpose Internet Mail Extensions | Standard for indicating document types on the internet | Content-Type header for proper file handling and validation |\n| **UUID** | Universally Unique Identifier | 128-bit identifier guaranteed to be unique across systems | Job IDs and correlation identifiers in distributed processing |\n| **TCP** | Transmission Control Protocol | Reliable connection-oriented network protocol | Ensures message delivery for critical job queue operations |\n| **UDP** | User Datagram Protocol | Connectionless network protocol for low-latency communication | Used for real-time progress updates and heartbeat monitoring |\n| **TLS** | Transport Layer Security | Cryptographic protocol for secure communication over networks | Encrypts webhook notifications and API communication |\n| **REST** | Representational State Transfer | Architectural style for web services using HTTP methods | Design pattern for job management and status query APIs |\n| **CORS** | Cross-Origin Resource Sharing | Web security feature controlling cross-domain requests | Browser security for web-based media processing interfaces |\n| **JWT** | JSON Web Token | Compact token format for securely transmitting information | Authentication mechanism for API access and webhook validation |\n| **SQL** | Structured Query Language | Language for managing relational databases | PostgreSQL queries for job history and metadata persistence |\n| **ACID** | Atomicity Consistency Isolation Durability | Properties ensuring reliable database transactions | Critical for job status updates and progress tracking |\n| **FIFO** | First In First Out | Queue processing order where earliest jobs are processed first | Default job queue behavior for fair processing order |\n| **LIFO** | Last In First Out | Stack processing order where newest items are processed first | Not suitable for job queues as it causes starvation |\n| **LRU** | Least Recently Used | Cache eviction policy removing oldest unused items | Memory management for image processing cache |\n| **TTL** | Time To Live | Expiration mechanism for cached data or temporary resources | Cleanup policy for temporary files and cached thumbnails |\n\n> **Critical Integration Note**: These terms form an interconnected vocabulary where understanding one concept often requires familiarity with several others. For example, **adaptive bitrate streaming** relies on **keyframe alignment**, **segment alignment**, and **quality ladders** to function properly, while **webhook notifications** depend on **HMAC signatures**, **exponential backoff**, and **idempotency keys** for reliable delivery.\n\n### Implementation Guidance\n\nThe terminology and concepts defined in this glossary should be consistently used throughout code, documentation, and team communication. Establishing a shared vocabulary prevents miscommunication and ensures that all team members understand system behavior in the same way.\n\n#### Code Documentation Standards\n\nWhen implementing the media processing pipeline, use these exact terms in code comments, variable names, and function documentation. For example, use `processing_job` rather than variations like `media_task`, `work_item`, or `job_request`. This consistency makes the codebase self-documenting and reduces the learning curve for new team members.\n\n```python\nclass ProcessingJob:\n    \"\"\"\n    Core unit of work containing input file and output specifications.\n    \n    Represents a complete media transformation task that progresses through\n    the job lifecycle from pending to completed/failed states.\n    \"\"\"\n    \ndef submit_job(input_file: str, output_specs: List[OutputSpecification]) -> ProcessingJob:\n    \"\"\"\n    Create and queue new processing job for worker execution.\n    \n    Args:\n        input_file: Path to source media file for processing\n        output_specs: List of desired output configurations\n        \n    Returns:\n        ProcessingJob with unique job_id and PENDING status\n    \"\"\"\n```\n\n#### API Documentation Consistency\n\nREST API endpoints and webhook payloads should use the terminology from this glossary to ensure consistency between the system internals and external interfaces. This makes integration easier for client developers who can reference the same glossary.\n\n| Endpoint | Request Fields | Response Fields |\n|----------|---------------|----------------|\n| `POST /jobs` | `input_file_path`, `output_specifications`, `webhook_url` | `job_id`, `status`, `estimated_duration` |\n| `GET /jobs/{job_id}/progress` | None | `progress_percentage`, `current_stage`, `stage_progress` |\n| `GET /jobs/{job_id}/metadata` | None | `media_metadata`, `processing_config`, `resource_constraints` |\n\n#### Error Message Standardization\n\nError messages and logging should use these standardized terms to make debugging more efficient. Instead of generic messages like \"processing failed,\" use specific terminology like \"video transcoding failed during GOP structure analysis\" or \"image processing failed during EXIF orientation handling.\"\n\n```python\n# Good: Specific terminology\nlogger.error(\"CRF encoding failed: invalid GOP structure for adaptive bitrate streaming\")\n\n# Bad: Generic terminology  \nlogger.error(\"video processing error occurred\")\n```\n\n#### Team Communication Guidelines\n\nDuring code reviews, architectural discussions, and incident response, team members should reference these terms consistently. This shared vocabulary accelerates problem diagnosis and solution design by eliminating ambiguity about what each component does and how it behaves.\n\nWhen discussing system behavior, use the precise term rather than approximations. For example, say \"the job queue exhibits head-of-line blocking\" rather than \"jobs are getting stuck,\" or \"we need exponential backoff for webhook notification retries\" rather than \"we should try sending webhooks again.\"\n\nThis glossary serves as the definitive reference for terminology throughout the media processing pipeline project, ensuring that everyone from junior developers to senior architects speaks the same technical language.\n"}