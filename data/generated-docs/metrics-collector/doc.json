{"html":"<h1 id=\"prometheus-like-metrics-collection-system-design-document\">Prometheus-Like Metrics Collection System: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A distributed metrics collection system that scrapes time-series data from service endpoints, stores it with efficient compression, and provides a query engine for monitoring and alerting. The key architectural challenge is handling high-cardinality labeled metrics at scale while maintaining low latency for queries and minimal storage overhead.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the foundation for Milestones 1-4 by explaining why we need a metrics collection system and what technical challenges it must solve.</p>\n</blockquote>\n<p>Modern distributed systems have transformed from simple monolithic applications into complex networks of microservices, containers, and cloud-native components. This architectural evolution has created an unprecedented need for comprehensive observability — the ability to understand system behavior, performance, and health from the outside by examining the data it emits. At the heart of observability lies metrics collection: the systematic gathering, storage, and analysis of quantitative measurements that tell the story of how our systems are performing over time.</p>\n<p>The challenge we face is not merely collecting numbers from our applications. Any system can log metrics to files or send them to a database. The real complexity emerges when we consider the scale, velocity, and dimensional richness of modern metrics data. A typical microservices deployment might generate millions of time series, each representing a unique combination of metric name and label values, with new data points arriving every few seconds. These metrics must be stored efficiently, queried quickly, and remain available even as the underlying infrastructure scales and evolves.</p>\n<p>Existing monitoring solutions often fall short in one or more critical areas: they either cannot handle the scale of modern systems, lack the flexibility to model complex multi-dimensional data, or require architectural compromises that limit their effectiveness. The goal of this project is to build a Prometheus-like metrics collection system that addresses these shortcomings while providing the foundation for robust observability in distributed environments.</p>\n<h3 id=\"the-observatory-mental-model\">The Observatory Mental Model</h3>\n<p>To understand the architecture and challenges of a metrics collection system, consider the analogy of a <strong>weather monitoring network</strong>. Imagine you are tasked with building a system to monitor weather conditions across an entire continent. You need to track temperature, humidity, wind speed, and atmospheric pressure at thousands of locations, with measurements taken every few minutes, 24 hours a day.</p>\n<p>Your weather monitoring network would require several key components, each with specific responsibilities and challenges:</p>\n<p><strong>Weather Stations (Metrics Sources):</strong> Thousands of automated weather stations scattered across the continent continuously measure environmental conditions. Each station has a unique identifier (location) and can measure multiple phenomena simultaneously. The stations must be robust, autonomous, and capable of operating in diverse conditions. In our metrics system, these weather stations correspond to <strong>application instances, services, and infrastructure components</strong> that expose metrics through HTTP endpoints.</p>\n<p><strong>Data Collectors (Scrape Engine):</strong> Rather than having each weather station independently transmit its data, you deploy a network of data collectors that periodically visit stations to retrieve measurements. This pull-based approach offers several advantages: collectors can detect when stations are offline, they can apply consistent formatting and validation, and they can handle network issues gracefully. In our system, the <strong>scrape engine</strong> acts as these data collectors, periodically fetching metrics from configured targets.</p>\n<p><strong>Central Archives (Time Series Storage):</strong> All collected measurements must be stored in a central archive system that can handle the massive volume of time-stamped data points. The archive must compress data efficiently (weather measurements follow predictable patterns), provide fast access for analysis, and automatically manage data retention as storage fills up. Our <strong>time series storage engine</strong> serves this archival function, using specialized compression techniques designed for temporal data.</p>\n<p><strong>Research Interface (Query Engine):</strong> Scientists and analysts need to query the archived weather data to identify patterns, trends, and anomalies. They might ask questions like &quot;What was the average temperature in the Pacific Northwest during July?&quot; or &quot;Show me all locations where humidity exceeded 90% in the past week.&quot; The query interface must support complex filtering, aggregation, and time-based operations. Our <strong>PromQL query engine</strong> provides this analytical capability for metrics data.</p>\n<p>This mental model illuminates several key insights about metrics collection systems:</p>\n<p><strong>Pull vs Push Architecture:</strong> Just as weather stations don&#39;t individually transmit data to every interested party, metrics systems benefit from a pull-based architecture where a central collector retrieves data from sources. This approach provides better failure detection, reduces network overhead, and enables consistent data formatting.</p>\n<p><strong>Dimensional Data Challenges:</strong> Weather measurements aren&#39;t just numbers — they have dimensions (location, altitude, measurement type, instrument ID). Similarly, modern metrics are multi-dimensional, with labels that provide rich context. Managing the combinatorial explosion of unique dimension combinations becomes a critical scalability challenge.</p>\n<p><strong>Time Series Characteristics:</strong> Weather data exhibits temporal patterns that enable efficient compression — temperatures change gradually, measurements are taken at regular intervals, and similar conditions tend to cluster in time. Metrics data shares these characteristics, allowing specialized storage techniques that wouldn&#39;t work for general-purpose databases.</p>\n<p><strong>Query Pattern Optimization:</strong> Weather researchers typically ask time-based questions about recent data, with occasional historical analysis. Metrics queries follow similar patterns — most focus on recent time windows, with aggregation across multiple dimensions. Storage and indexing strategies can optimize for these access patterns.</p>\n<p>The weather monitoring analogy also reveals why building an effective metrics collection system is challenging. The scale is enormous (potentially millions of time series), the data arrives continuously, storage efficiency is crucial, and query performance must remain fast even as the dataset grows. These constraints shape every architectural decision in our system.</p>\n<h3 id=\"existing-solutions-analysis\">Existing Solutions Analysis</h3>\n<p>The metrics collection landscape includes several established approaches, each with distinct architectural philosophies and trade-offs. Understanding these existing solutions helps clarify the design space and motivates our architectural choices.</p>\n<p><strong>Push-Based Systems (StatsD, Graphite, InfluxDB):</strong></p>\n<p>Push-based systems follow a model where application instances actively send their metrics to a central collector. Applications use client libraries to emit metrics via UDP packets (StatsD) or HTTP requests, and a central aggregation service receives, processes, and stores the data.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network Overhead</td>\n<td>Applications control transmission timing</td>\n<td>High network traffic from many sources</td>\n</tr>\n<tr>\n<td>Failure Detection</td>\n<td>Immediate feedback on transmission errors</td>\n<td>Cannot distinguish between application down vs network issues</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>No need to configure scrape targets centrally</td>\n<td>Each application must know collector endpoints</td>\n</tr>\n<tr>\n<td>Scaling</td>\n<td>Horizontal scaling of collectors is straightforward</td>\n<td>Network storms during traffic spikes</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Applications handle their own endpoint resolution</td>\n<td>Difficult to ensure all services are monitored</td>\n</tr>\n</tbody></table>\n<p>The fundamental challenge with push-based systems is <strong>observability of the monitoring system itself</strong>. When metrics stop arriving, it&#39;s difficult to determine whether the application has failed, the network path is broken, or the application simply has no activity to report. This ambiguity complicates alerting and incident response.</p>\n<p><strong>Pull-Based Systems (Prometheus, DataDog Agent):</strong></p>\n<p>Pull-based systems invert the responsibility: a central collector actively retrieves metrics from application endpoints. Applications expose metrics via HTTP endpoints in a standardized format, and the collector periodically scrapes these endpoints according to a configured schedule.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Failure Detection</td>\n<td>Clear distinction between target down vs no data</td>\n<td>Applications must maintain HTTP endpoints</td>\n</tr>\n<tr>\n<td>Network Control</td>\n<td>Collector controls scrape timing and concurrency</td>\n<td>Requires network reachability from collector to targets</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Centralized target configuration with service discovery</td>\n<td>Initial setup complexity for service discovery</td>\n</tr>\n<tr>\n<td>Debugging</td>\n<td>Easy to manually inspect target endpoints</td>\n<td>Firewall and network policy complexity</td>\n</tr>\n<tr>\n<td>Consistency</td>\n<td>Uniform scrape intervals and timeout handling</td>\n<td>Potential polling overhead for idle applications</td>\n</tr>\n</tbody></table>\n<p><strong>Hybrid Approaches (Prometheus Push Gateway, Vector):</strong></p>\n<p>Some systems attempt to combine both approaches, typically by providing a push-to-pull bridge. Applications push metrics to an intermediate gateway, which exposes them via pull endpoints for the main collector.</p>\n<blockquote>\n<p><strong>Key Insight:</strong> The choice between push and pull fundamentally affects system observability, failure modes, and operational complexity. Pull-based systems provide better visibility into the health of the monitoring system itself, at the cost of additional networking complexity.</p>\n</blockquote>\n<p><strong>Architectural Decision Record:</strong></p>\n<blockquote>\n<p><strong>Decision: Pull-Based Scraping Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose between push-based (applications send metrics) vs pull-based (collector retrieves metrics) architecture for our metrics collection system</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Pure push-based with UDP/HTTP transmission from applications</li>\n<li>Pull-based scraping with HTTP endpoints on applications  </li>\n<li>Hybrid push-to-pull gateway model</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Pull-based scraping with HTTP endpoints</li>\n<li><strong>Rationale</strong>: Pull-based systems provide superior observability of the monitoring infrastructure itself. When scraping fails, we know definitively that a target is unreachable or unresponsive, enabling precise alerting. Centralized configuration simplifies service discovery integration and reduces per-application configuration burden. The HTTP endpoint model also enables manual debugging and testing of individual services.</li>\n<li><strong>Consequences</strong>: Applications must implement HTTP metrics endpoints and handle scraping load. Network policies must allow collector-to-target connectivity. However, we gain clear failure attribution, centralized configuration management, and better debugging capabilities.</li>\n</ul>\n</blockquote>\n<p><strong>Storage Engine Comparison:</strong></p>\n<p>Different metrics systems employ varying storage strategies, each optimized for different trade-offs:</p>\n<table>\n<thead>\n<tr>\n<th>System</th>\n<th>Storage Approach</th>\n<th>Compression</th>\n<th>Query Performance</th>\n<th>Operational Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Graphite</td>\n<td>Whisper files (RRD-style)</td>\n<td>Fixed retention buckets</td>\n<td>Fast for pre-aggregated data</td>\n<td>Medium - file system management</td>\n</tr>\n<tr>\n<td>InfluxDB</td>\n<td>Custom time series engine</td>\n<td>Snappy + series compression</td>\n<td>Variable based on cardinality</td>\n<td>High - clustering and consistency</td>\n</tr>\n<tr>\n<td>Prometheus</td>\n<td>Block-based with Gorilla compression</td>\n<td>Delta-of-delta + XOR</td>\n<td>Excellent for recent data</td>\n<td>Low - single node simplicity</td>\n</tr>\n<tr>\n<td>VictoriaMetrics</td>\n<td>Compressed column storage</td>\n<td>Multiple algorithms</td>\n<td>Optimized for high cardinality</td>\n<td>Medium - configuration complexity</td>\n</tr>\n</tbody></table>\n<p>Our system will adopt Prometheus&#39;s approach of <strong>block-based storage with Gorilla compression</strong> because it provides excellent compression ratios for typical metrics workloads while maintaining query performance for the most common use case: analyzing recent data.</p>\n<h3 id=\"core-technical-challenges\">Core Technical Challenges</h3>\n<p>Building an effective metrics collection system requires solving several interconnected technical challenges. Each challenge involves fundamental trade-offs that shape the system&#39;s architecture and performance characteristics.</p>\n<p><strong>Scale and Throughput Management:</strong></p>\n<p>Modern distributed systems can generate enormous volumes of metrics data. Consider a microservices deployment with 100 services, each running 10 instances, exposing 50 metrics each, scraped every 15 seconds. This generates 100 × 10 × 50 × (3600/15) = 1.2 million data points per hour, or approximately 330 samples per second just for the base metrics. Real deployments often exceed this by orders of magnitude when including infrastructure metrics, custom application metrics, and higher scrape frequencies.</p>\n<p>The scale challenge manifests in several dimensions:</p>\n<p><strong>Ingestion Rate:</strong> The system must sustainably ingest hundreds of thousands to millions of samples per second without dropping data or introducing excessive latency. This requires efficient parsing, concurrent processing, and careful memory management to avoid garbage collection pressure.</p>\n<p><strong>Storage Volume:</strong> With retention periods measured in weeks or months, the total storage requirements can reach terabytes. Each sample consists of a timestamp (8 bytes), a float64 value (8 bytes), plus the overhead of series identification and indexing. Without compression, a billion samples would require at least 16 GB of raw storage.</p>\n<p><strong>Query Latency:</strong> Despite the massive data volumes, queries must complete within seconds to support interactive dashboards and real-time alerting. This requires intelligent indexing, data locality optimization, and query execution strategies that minimize disk I/O.</p>\n<p><strong>Cardinality Explosion Problem:</strong></p>\n<p>The <strong>cardinality</strong> of a metrics system refers to the number of unique time series, where each series is defined by a unique combination of metric name and label values. This represents perhaps the most insidious scaling challenge in metrics systems.</p>\n<p>Consider a simple HTTP request counter with labels for method, status code, and endpoint:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>http_requests_total{method=&quot;GET&quot;, status=&quot;200&quot;, endpoint=&quot;/api/users&quot;}\nhttp_requests_total{method=&quot;POST&quot;, status=&quot;201&quot;, endpoint=&quot;/api/users&quot;}\nhttp_requests_total{method=&quot;GET&quot;, status=&quot;404&quot;, endpoint=&quot;/api/orders&quot;}</code></pre></div>\n\n<p>If your system has 10 HTTP methods, 20 possible status codes, and 100 endpoints, the potential cardinality is 10 × 20 × 100 = 20,000 unique time series for this single metric. Add labels for instance ID, deployment version, and geographic region, and the cardinality explodes exponentially.</p>\n<p>High cardinality creates multiple problems:</p>\n<table>\n<thead>\n<tr>\n<th>Problem Area</th>\n<th>Impact</th>\n<th>Root Cause</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Usage</td>\n<td>Each series requires indexing structures in RAM</td>\n<td>Series metadata and recent samples kept in memory</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td>Label matching becomes expensive</td>\n<td>Must scan large label indexes for selector evaluation</td>\n</tr>\n<tr>\n<td>Storage Overhead</td>\n<td>Small series create inefficient storage chunks</td>\n<td>Compression algorithms work poorly on short time series</td>\n</tr>\n<tr>\n<td>Ingestion Latency</td>\n<td>New series creation becomes a bottleneck</td>\n<td>Index updates and memory allocation under write load</td>\n</tr>\n</tbody></table>\n<p><strong>Storage Efficiency and Compression:</strong></p>\n<p>Time series data exhibits characteristics that enable sophisticated compression, but achieving optimal compression ratios requires careful algorithm selection and implementation. The challenge is balancing compression effectiveness with query performance and computational overhead.</p>\n<p><strong>Timestamp Compression:</strong> Metrics are typically collected at regular intervals, creating predictable timestamp patterns. The Gorilla compression algorithm exploits this by storing timestamps as delta-of-deltas: instead of storing absolute timestamps, it stores the difference between consecutive timestamp differences. For regular scrape intervals, this often compresses timestamps to just a few bits per sample.</p>\n<p><strong>Value Compression:</strong> Metric values often change gradually or remain constant for extended periods. Gorilla compression uses XOR-based encoding where each value is XORed with its predecessor, and only the differing bits are stored. When values are stable, this can compress 64-bit floats to just a few bits.</p>\n<p>However, compression introduces complexity:</p>\n<p><strong>Write Amplification:</strong> Compressed blocks must be periodically finalized and written to disk, creating bursty I/O patterns that can interfere with query performance.</p>\n<p><strong>Query Overhead:</strong> Reading compressed data requires decompression, adding CPU overhead to query execution. The system must balance compression ratios against query latency requirements.</p>\n<p><strong>Memory Pressure:</strong> Compression algorithms maintain state for active series, and this metadata can consume significant memory in high-cardinality environments.</p>\n<p><strong>Query Performance Under Load:</strong></p>\n<p>The query engine must support complex analytical operations across massive datasets while maintaining interactive response times. This challenge is complicated by the multi-dimensional nature of metrics data and the variety of query patterns users employ.</p>\n<p><strong>Range Query Scalability:</strong> Range queries that retrieve data across long time windows or high-cardinality label combinations can potentially scan terabytes of data. The query engine must use indexing, pruning, and parallel execution strategies to make such queries feasible.</p>\n<p><strong>Aggregation Efficiency:</strong> PromQL queries often perform aggregation operations (sum, average, percentile) across thousands of time series. These operations require careful memory management and algorithmic optimization to avoid excessive memory usage or computation time.</p>\n<p><strong>Concurrent Query Load:</strong> Production metrics systems serve multiple concurrent users running dashboards, alerts, and ad-hoc queries. The system must manage resource allocation to prevent expensive queries from interfering with critical alerting queries.</p>\n<p><strong>Index Maintenance:</strong> As new series are created and old series expire, the label indexes that enable fast series selection must be updated consistently without blocking ongoing queries.</p>\n<blockquote>\n<p><strong>Critical Design Principle:</strong> Every architectural decision in our metrics system must consider its impact on cardinality scaling. Features that seem innocent in small deployments can become system-breaking bottlenecks when cardinality grows from thousands to millions of series.</p>\n</blockquote>\n<p><strong>Real-Time vs Historical Query Patterns:</strong></p>\n<p>Metrics queries exhibit distinct patterns that the system can optimize for:</p>\n<p><strong>Hot Data Access:</strong> Most queries focus on recent data (last few hours to days), which should be kept in memory or fast storage for immediate access.</p>\n<p><strong>Cold Data Access:</strong> Historical queries are less frequent but often span longer time ranges, requiring different optimization strategies focused on I/O efficiency rather than memory access.</p>\n<p><strong>Alert Query Priority:</strong> Alerting queries must complete quickly and reliably, potentially requiring resource reservation or priority queuing mechanisms.</p>\n<p><strong>Dashboard Query Batching:</strong> Dashboard refreshes often trigger multiple related queries that could benefit from shared computation or caching strategies.</p>\n<p>Understanding these challenges provides the foundation for our architectural decisions. Each component of our metrics collection system — the data model, scrape engine, storage layer, and query engine — must be designed to address these fundamental scaling and performance constraints.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Cardinality Early</strong>\nMany developers building metrics systems focus initially on throughput and storage optimization while treating cardinality as a later concern. This approach leads to systems that work well in testing but fail catastrophically in production when label combinations proliferate. From the initial design phase, every data structure and algorithm must account for high-cardinality scenarios. For example, using hash maps keyed by series labels seems efficient until you have millions of series, at which point the hash map itself becomes a memory bottleneck and garbage collection issue.</p>\n<p>⚠️ <strong>Pitfall: Optimizing for Average Case</strong>\nMetrics systems exhibit highly variable load patterns — scraping creates periodic ingestion bursts, queries arrive in waves during incident response, and certain time periods (deployments, traffic spikes) generate disproportionate data volumes. Designing for average-case performance results in systems that become unresponsive precisely when reliability is most critical. Instead, all components must be designed to handle 99th percentile loads gracefully, with explicit backpressure and resource protection mechanisms.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building a metrics collection system requires careful technology selection and project organization. This guidance helps you make practical implementation decisions and avoid common pitfalls.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td><code>net/http</code> with standard mux</td>\n<td><code>gorilla/mux</code> or <code>gin</code></td>\n<td>Standard library sufficient for metrics endpoints</td>\n</tr>\n<tr>\n<td>Time Series Storage</td>\n<td>File-based chunks with <code>os.File</code></td>\n<td>Embedded key-value store like <code>bbolt</code></td>\n<td>File system provides simple persistence model</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>Basic delta encoding</td>\n<td>Full Gorilla algorithm</td>\n<td>Start simple, optimize when cardinality grows</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static YAML configuration</td>\n<td>Kubernetes API integration</td>\n<td>Static config easier to debug and understand</td>\n</tr>\n<tr>\n<td>Parsing</td>\n<td>Custom text parser</td>\n<td><code>prometheus/common/expfmt</code></td>\n<td>Custom parser teaches format understanding</td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td><code>goroutines</code> with channels</td>\n<td>Worker pool patterns</td>\n<td>Go&#39;s concurrency primitives handle most cases</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Project Structure:</strong></p>\n<p>Understanding how to organize your codebase prevents the common mistake of creating a monolithic main.go file that becomes unmaintainable:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>metrics-system/\n├── cmd/\n│   ├── collector/           ← Main collector binary\n│   │   └── main.go\n│   └── query/              ← Query API server\n│       └── main.go\n├── internal/\n│   ├── model/              ← Metrics data model (Milestone 1)\n│   │   ├── metric.go       ← Counter, Gauge, Histogram types\n│   │   ├── labels.go       ← Label handling and validation\n│   │   ├── sample.go       ← Sample and timestamp types\n│   │   └── metadata.go     ← Metric metadata storage\n│   ├── scrape/             ← Scrape engine (Milestone 2)\n│   │   ├── discovery.go    ← Target discovery logic\n│   │   ├── scraper.go      ← HTTP scraping implementation\n│   │   ├── scheduler.go    ← Scrape interval management\n│   │   └── parser.go       ← Metrics format parsing\n│   ├── storage/            ← Time series storage (Milestone 3)\n│   │   ├── engine.go       ← Main storage engine\n│   │   ├── compression.go  ← Gorilla compression implementation\n│   │   ├── index.go        ← Series indexing\n│   │   ├── retention.go    ← Data lifecycle management\n│   │   └── wal.go          ← Write-ahead logging\n│   ├── query/              ← Query engine (Milestone 4)\n│   │   ├── parser.go       ← PromQL parsing\n│   │   ├── executor.go     ← Query execution\n│   │   ├── aggregation.go  ← Aggregation functions\n│   │   └── matcher.go      ← Label matching logic\n│   └── api/                ← HTTP API handlers\n│       ├── scrape.go       ← Metrics exposition endpoints\n│       └── query.go        ← Query API endpoints\n├── pkg/                    ← Public interfaces (if building libraries)\n├── configs/                ← Example configuration files\n├── scripts/               ← Build and deployment scripts\n└── test/                  ← Integration tests and test data\n    ├── integration/\n    └── testdata/</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<p>Here&#39;s a complete HTTP server foundation that handles the basic networking and routing for both scrape targets and the query API:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/api/server.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> api</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Server wraps HTTP server functionality for metrics collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Server</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpServer </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ServeMux</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewServer creates a new API server instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">port</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewServeMux</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:         fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\":</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, port),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Handler:      mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        IdleTimeout:  </span><span style=\"color:#79B8FF\">120</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        httpServer: server,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mux:        mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterScrapeEndpoint adds a metrics exposition endpoint</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterScrapeEndpoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">handler</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(path, handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterQueryEndpoint adds a query API endpoint  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterQueryEndpoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">handler</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(path, handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins serving HTTP requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Starting metrics server on </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, s.httpServer.Addr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Shutdown gracefully stops the server</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Shutting down metrics server...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Configuration Management Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/config/config.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">gopkg.in/yaml.v2</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config holds all system configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Scrape   </span><span style=\"color:#B392F0\">ScrapeConfig</span><span style=\"color:#9ECBFF\">   `yaml:\"scrape\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage  </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\">  `yaml:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query    </span><span style=\"color:#B392F0\">QueryConfig</span><span style=\"color:#9ECBFF\">    `yaml:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScrapeConfig configures the scraping engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScrapeConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Interval        </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timeout         </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxConcurrency  </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">          `yaml:\"max_concurrency\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StaticTargets   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">     `yaml:\"static_targets\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageConfig configures time series storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataDir         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"data_dir\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionPeriod </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"retention_period\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkSize       </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"chunk_size\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EnableCompression </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">        `yaml:\"enable_compression\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryConfig configures the query engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxConcurrency </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"max_concurrency\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"query_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxSamples     </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"max_samples\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadFromFile reads configuration from YAML file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadFromFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> yaml.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetDefaults populates default configuration values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetDefaults</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Scrape.Interval </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Scrape.Interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 15</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Scrape.Timeout </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Scrape.Timeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Scrape.MaxConcurrency </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Scrape.MaxConcurrency </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 50</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Storage.DataDir </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Storage.DataDir </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"./data\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Storage.RetentionPeriod </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Storage.RetentionPeriod </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour </span><span style=\"color:#6A737D\">// 30 days</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Query.MaxConcurrency </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Query.MaxConcurrency </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Query.QueryTimeout </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Query.QueryTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Logging Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/logging/logger.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Logger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    infoLogger  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errorLogger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    debugLogger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">log</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLogger</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        infoLogger:  log.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(os.Stdout, </span><span style=\"color:#9ECBFF\">\"INFO: \"</span><span style=\"color:#E1E4E8\">, log.Ldate</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">log.Ltime</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">log.Lshortfile),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errorLogger: log.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(os.Stderr, </span><span style=\"color:#9ECBFF\">\"ERROR: \"</span><span style=\"color:#E1E4E8\">, log.Ldate</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">log.Ltime</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">log.Lshortfile),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        debugLogger: log.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(os.Stdout, </span><span style=\"color:#9ECBFF\">\"DEBUG: \"</span><span style=\"color:#E1E4E8\">, log.Ldate</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">log.Ltime</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">log.Lshortfile),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">v</span><span style=\"color:#F97583\"> ...interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.infoLogger.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(v</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Infof</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">format</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">v</span><span style=\"color:#F97583\"> ...interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.infoLogger.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(format, v</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">v</span><span style=\"color:#F97583\"> ...interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.errorLogger.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(v</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">format</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">v</span><span style=\"color:#F97583\"> ...interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.errorLogger.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(format, v</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Debug</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">v</span><span style=\"color:#F97583\"> ...interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.debugLogger.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(v</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Debugf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">format</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">v</span><span style=\"color:#F97583\"> ...interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.debugLogger.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(format, v</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Development Workflow Tips:</strong></p>\n<ol>\n<li><p><strong>Start with Static Configuration:</strong> Begin with YAML configuration files before implementing dynamic service discovery. This allows you to test the core functionality without network complexity.</p>\n</li>\n<li><p><strong>Use Table-Driven Tests:</strong> Go&#39;s table-driven test pattern works excellently for metrics systems where you need to test many label combinations and edge cases:</p>\n</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestLabelMatching</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tests </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        name     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        labels   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        matcher  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        expected </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#9ECBFF\">\"exact match\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"job\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"api\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">`job=\"api\"`</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#9ECBFF\">\"regex match\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"instance\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"web-01\"</span><span style=\"color:#E1E4E8\">}, </span><span style=\"color:#9ECBFF\">`instance=~\"web-.*\"`</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Add more test cases...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Test implementation...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Implement Graceful Shutdown:</strong> Metrics systems often run as long-lived services. Implement proper shutdown handling to avoid data loss:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// In your main.go</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">c </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Signal</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">signal.</span><span style=\"color:#B392F0\">Notify</span><span style=\"color:#E1E4E8\">(c, os.Interrupt, syscall.SIGTERM)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\">c</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> server.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Fatal</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Server forced to shutdown:\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints for Go:</strong></p>\n<ul>\n<li><strong>Memory Management:</strong> Use <code>sync.Pool</code> for frequently allocated objects like sample slices to reduce GC pressure</li>\n<li><strong>Goroutine Management:</strong> Always use <code>context.Context</code> for cancellation in long-running operations like scraping and queries</li>\n<li><strong>Time Handling:</strong> Use <code>time.Unix()</code> for timestamp conversions and always store timestamps in UTC</li>\n<li><strong>Error Handling:</strong> Wrap errors with <code>fmt.Errorf(&quot;operation failed: %w&quot;, err)</code> to maintain error context through the call stack</li>\n<li><strong>Concurrent Maps:</strong> Use <code>sync.RWMutex</code> to protect shared data structures like series indexes</li>\n<li><strong>File I/O:</strong> Always call <code>file.Sync()</code> after writing critical data like WAL entries to ensure durability</li>\n</ul>\n<p><strong>Common Development Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Blocking Operations in Hot Paths</strong>\nNever perform I/O operations directly in scraping or query processing goroutines without proper timeouts. Always use <code>context.WithTimeout()</code> and handle cancellation appropriately.</p>\n<p>⚠️ <strong>Pitfall: Unbounded Memory Growth</strong>\nMetrics systems can accumulate memory leaks through retained references to old samples or series. Use profiling tools like <code>go tool pprof</code> regularly during development to identify memory growth patterns.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Graceful Degradation</strong>\nBuild backpressure and circuit breaker patterns from the beginning. When storage is full or queries are slow, the system should reject new work rather than becoming unresponsive.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the scope and requirements that guide all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4).</p>\n</blockquote>\n<h3 id=\"the-observatory-charter-mental-model\">The Observatory Charter Mental Model</h3>\n<p>Think of this goals section as the charter document for a national weather observatory network. Just as meteorologists must decide whether their observatory will track temperature and rainfall (essential) versus tracking every atmospheric particle (impossible), we must define exactly what our metrics collection system will and won&#39;t accomplish. The charter prevents scope creep—when stakeholders later ask &quot;can it also do real-time alerting?&quot; we can point to this document and explain why alerting is explicitly out of scope for this implementation.</p>\n<p>This boundary-setting is crucial for complex systems. Without clear goals, developers often build everything they can imagine rather than building the core functionality excellently. A well-scoped metrics system that handles scraping, storage, and querying perfectly is far more valuable than a system that attempts alerting, dashboards, and federation but does none of them well.</p>\n<p>The goals also serve as acceptance criteria for the project. Each requirement must be measurable and testable—we can&#39;t claim success with vague objectives like &quot;good performance.&quot; Instead, we specify concrete targets: sub-second query response times, specific storage compression ratios, and exact retention capabilities.</p>\n<h3 id=\"functional-requirements\">Functional Requirements</h3>\n<p>The functional requirements define the core capabilities our metrics system must provide to be considered complete. These map directly to our four major milestones and represent the essential features that distinguish a metrics collection system from a generic time-series database.</p>\n<p><strong>Metrics Data Model Requirements</strong></p>\n<p>Our system must implement a comprehensive metrics data model that supports the standard observability metric types used in modern monitoring systems. The <code>Counter</code> type must enforce monotonic increase semantics, meaning values can only go up or reset to zero (typically on process restart). The <code>Gauge</code> type must allow arbitrary value changes to represent measurements like memory usage or queue depth that can increase or decrease freely. The <code>Histogram</code> type must track value distributions using configurable buckets, enabling percentile calculations and distribution analysis.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Type</th>\n<th>Semantic Behavior</th>\n<th>Example Use Case</th>\n<th>Key Constraint</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Counter</td>\n<td>Monotonic increase, resets on restart</td>\n<td>HTTP requests served</td>\n<td>Value ≥ previous value or reset to 0</td>\n</tr>\n<tr>\n<td>Gauge</td>\n<td>Arbitrary value changes</td>\n<td>Memory usage bytes</td>\n<td>No constraints on value changes</td>\n</tr>\n<tr>\n<td>Histogram</td>\n<td>Distribution tracking in buckets</td>\n<td>Request latency distribution</td>\n<td>Bucket boundaries fixed after creation</td>\n</tr>\n<tr>\n<td>Summary</td>\n<td>Client-side quantile calculation</td>\n<td>Response time percentiles</td>\n<td>Quantiles calculated at source</td>\n</tr>\n</tbody></table>\n<p>The labeling system must support multi-dimensional metrics where each time series is uniquely identified by its metric name plus label set. Labels enable powerful filtering and aggregation—a single <code>http_requests_total</code> metric with <code>method</code>, <code>status_code</code>, and <code>handler</code> labels can answer questions like &quot;What&#39;s the error rate for POST requests to the login endpoint?&quot; The system must validate label names and values, rejecting reserved prefixes like <code>__</code> and ensuring label values don&#39;t contain characters that break the exposition format.</p>\n<blockquote>\n<p><strong>Decision: Multi-Dimensional Labeling</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support filtering and aggregation across different metric dimensions</li>\n<li><strong>Options Considered</strong>: Single-dimensional metrics with encoded names, hierarchical metric names, multi-dimensional labels</li>\n<li><strong>Decision</strong>: Multi-dimensional labels attached to each metric</li>\n<li><strong>Rationale</strong>: Labels provide flexibility for ad-hoc queries without predefined metric names, enable efficient storage of related time series, and match Prometheus compatibility</li>\n<li><strong>Consequences</strong>: Enables powerful querying but introduces cardinality explosion risks that must be managed</li>\n</ul>\n</blockquote>\n<p><strong>Scrape Engine Requirements</strong></p>\n<p>The scrape engine must implement pull-based metrics collection that actively retrieves metrics from configured targets. Static configuration must support defining scrape targets through YAML configuration files, specifying endpoints, intervals, and timeouts. The system must parse the Prometheus exposition format, handling both the standard text format and optional metric metadata including help text and type information.</p>\n<p>Service discovery integration must automatically update the target list when services are added or removed. The system must support at minimum static file-based service discovery, where external systems can update JSON or YAML files containing current service endpoints. The scrape scheduler must respect per-target intervals, ensuring targets are scraped consistently without drift or overlap.</p>\n<table>\n<thead>\n<tr>\n<th>Scrape Component</th>\n<th>Responsibility</th>\n<th>Configuration</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target Discovery</td>\n<td>Find scrape endpoints</td>\n<td>Static config, service discovery files</td>\n<td>Log discovery errors, continue with known targets</td>\n</tr>\n<tr>\n<td>HTTP Client</td>\n<td>Fetch metrics from endpoints</td>\n<td>Timeout, retry count, headers</td>\n<td>Mark target down, continue scrape cycle</td>\n</tr>\n<tr>\n<td>Parser</td>\n<td>Parse exposition format text</td>\n<td>Format validation, metadata extraction</td>\n<td>Skip malformed metrics, log parse errors</td>\n</tr>\n<tr>\n<td>Scheduler</td>\n<td>Trigger scrapes at intervals</td>\n<td>Per-target intervals, jitter</td>\n<td>Compensate for missed scrapes, prevent overlap</td>\n</tr>\n</tbody></table>\n<p>The scrape engine must handle target failures gracefully. Network timeouts must not block other targets&#39; scrape cycles. HTTP errors (4xx, 5xx) must be logged and reported but not crash the scraper. Malformed exposition format must be handled by skipping unparseable lines while processing valid metrics from the same target.</p>\n<p><strong>Storage Engine Requirements</strong></p>\n<p>The storage engine must provide efficient time-series storage with compression achieving less than 2 bytes per sample on average. The system must implement Gorilla-style compression using delta-of-delta encoding for timestamps and XOR encoding for floating-point values. This compression is essential for handling high-cardinality metrics at scale without exhausting storage capacity.</p>\n<p>The storage must support configurable retention periods, automatically deleting data older than the specified duration. The default retention period must be 30 days, but the system must support retention periods from hours to years. Data deletion must be efficient, removing entire blocks rather than individual samples to avoid fragmentation.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Feature</th>\n<th>Requirement</th>\n<th>Performance Target</th>\n<th>Implementation Note</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compression Ratio</td>\n<td>&lt; 2 bytes per sample</td>\n<td>1.3 bytes average</td>\n<td>Gorilla delta-of-delta + XOR</td>\n</tr>\n<tr>\n<td>Retention</td>\n<td>Configurable age-based deletion</td>\n<td>30 days default</td>\n<td>Block-based deletion</td>\n</tr>\n<tr>\n<td>Write Throughput</td>\n<td>Handle scrape ingestion</td>\n<td>100k samples/sec</td>\n<td>Batch writes, async commits</td>\n</tr>\n<tr>\n<td>Read Latency</td>\n<td>Query response time</td>\n<td>&lt; 1 second typical</td>\n<td>Efficient indexing</td>\n</tr>\n</tbody></table>\n<p>The indexing system must enable fast lookup of time series by metric name and label combinations. The index must support exact label matching, regex matching, and negative matching (labels that don&#39;t equal a value). Query performance must remain reasonable even with high-cardinality label combinations, though the system may reject queries that would examine excessive numbers of series.</p>\n<p><strong>Query Engine Requirements</strong></p>\n<p>The query engine must implement a PromQL-compatible query language supporting instant queries (single timestamp), range queries (time window), and basic aggregation functions. The system must support label selectors using exact match (<code>=</code>), not-equal (<code>!=</code>), regex match (<code>=~</code>), and negative regex (<code>!~</code>) operators.</p>\n<p>Aggregation functions must include <code>sum</code>, <code>avg</code>, <code>max</code>, <code>min</code>, and <code>count</code> operations that can group results by specified label dimensions. Range queries must return data points at regular step intervals within the specified time window, interpolating missing values when necessary.</p>\n<table>\n<thead>\n<tr>\n<th>Query Type</th>\n<th>Input</th>\n<th>Output</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Instant</td>\n<td>Metric selector + timestamp</td>\n<td>Vector of current values</td>\n<td><code>up</code> at now</td>\n</tr>\n<tr>\n<td>Range</td>\n<td>Metric selector + time window + step</td>\n<td>Matrix of time series</td>\n<td><code>cpu_usage[5m]</code></td>\n</tr>\n<tr>\n<td>Aggregation</td>\n<td>Vector + grouping labels</td>\n<td>Grouped vector</td>\n<td><code>sum by (instance) (cpu_usage)</code></td>\n</tr>\n<tr>\n<td>Rate Calculation</td>\n<td>Counter range</td>\n<td>Rate vector</td>\n<td><code>rate(http_requests[5m])</code></td>\n</tr>\n</tbody></table>\n<p>The query engine must handle counter resets correctly when calculating rates and derivatives. When a counter value decreases (indicating a reset), the rate calculation must treat the reset as the beginning of a new monotonic sequence rather than a negative rate.</p>\n<h3 id=\"quality-attributes\">Quality Attributes</h3>\n<p>The quality attributes define the performance, scalability, and reliability characteristics our metrics system must achieve. These non-functional requirements are often more critical than features—a metrics system that loses data or responds slowly becomes unusable regardless of its feature completeness.</p>\n<p><strong>Performance Requirements</strong></p>\n<p>Query response time must remain under one second for typical queries examining up to 10,000 time series. Range queries spanning one week with five-minute resolution must complete within five seconds. These performance targets ensure the system remains usable for operational monitoring where slow queries impede incident response.</p>\n<p>Storage ingestion must handle sustained write rates of 100,000 samples per second without falling behind or dropping data. This throughput supports monitoring environments with thousands of services each exposing hundreds of metrics. Write latency must remain low enough that scraped metrics appear in queries within the scrape interval.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Metric</th>\n<th>Target</th>\n<th>Measurement Method</th>\n<th>Degradation Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Response</td>\n<td>&lt; 1 second typical</td>\n<td>95th percentile latency</td>\n<td>&gt; 5 seconds</td>\n</tr>\n<tr>\n<td>Range Query</td>\n<td>&lt; 5 seconds</td>\n<td>One week window</td>\n<td>&gt; 30 seconds</td>\n</tr>\n<tr>\n<td>Write Throughput</td>\n<td>100k samples/sec</td>\n<td>Sustained ingestion</td>\n<td>Falls behind scrape rate</td>\n</tr>\n<tr>\n<td>Storage Efficiency</td>\n<td>&lt; 2 bytes/sample</td>\n<td>Compression ratio</td>\n<td>&gt; 4 bytes/sample</td>\n</tr>\n</tbody></table>\n<p>Memory usage must remain bounded even with high-cardinality metrics. The system must reject queries or scrape configurations that would consume excessive memory, providing clear error messages about cardinality limits. Disk usage must grow predictably based on ingestion rate and retention period.</p>\n<p><strong>Scalability Requirements</strong></p>\n<p>The system must support monitoring environments with up to 1,000 scrape targets and 1 million active time series. While this implementation focuses on single-instance deployment, the architecture must not preclude future horizontal scaling through techniques like sharding or federation.</p>\n<p>Cardinality must be controlled to prevent label explosion that could exhaust memory or storage. The system must provide mechanisms to limit the number of unique label value combinations, either through validation rules or runtime limits. High-cardinality label combinations (&gt; 10,000 series per metric name) should trigger warnings or rejections.</p>\n<table>\n<thead>\n<tr>\n<th>Scalability Dimension</th>\n<th>Limit</th>\n<th>Rationale</th>\n<th>Failure Mode</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scrape Targets</td>\n<td>1,000 targets</td>\n<td>Single-instance HTTP client limits</td>\n<td>Connection exhaustion</td>\n</tr>\n<tr>\n<td>Active Series</td>\n<td>1M series</td>\n<td>Memory for index and active chunks</td>\n<td>Out of memory errors</td>\n</tr>\n<tr>\n<td>Label Cardinality</td>\n<td>10k series per metric</td>\n<td>Prevent exponential explosion</td>\n<td>Query performance degradation</td>\n</tr>\n<tr>\n<td>Retention Data</td>\n<td>30 days × ingestion rate</td>\n<td>Disk capacity planning</td>\n<td>Disk full, query slowness</td>\n</tr>\n</tbody></table>\n<p><strong>Reliability Requirements</strong></p>\n<p>The system must provide durability guarantees ensuring that successfully scraped metrics are not lost due to process crashes or system failures. Write-ahead logging must persist ingested samples before acknowledging success. Recovery after crashes must restore the system to a consistent state without data loss.</p>\n<p>Target scraping must be resilient to individual target failures. Network partitions, service restarts, or malformed responses from one target must not affect scraping of other targets. The scrape engine must continue operating with degraded target coverage rather than failing completely.</p>\n<blockquote>\n<p><strong>Decision: Pull-Based Scraping Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose between push-based (targets send metrics) and pull-based (collector fetches metrics) architectures</li>\n<li><strong>Options Considered</strong>: Push-based with HTTP POST, pull-based with HTTP GET, hybrid push/pull</li>\n<li><strong>Decision</strong>: Pull-based scraping with HTTP GET requests</li>\n<li><strong>Rationale</strong>: Pull model provides better failure isolation (target failures don&#39;t crash collector), enables service discovery integration, matches Prometheus ecosystem, and simplifies target authentication</li>\n<li><strong>Consequences</strong>: Requires targets to expose HTTP endpoints, may have higher network overhead, but provides better operational control and debugging</li>\n</ul>\n</blockquote>\n<p>Data consistency must be maintained under concurrent access. Multiple queries must be able to execute simultaneously without corrupting results or crashing the system. Write operations must not interfere with concurrent reads beyond brief locking periods.</p>\n<p>The system must handle resource exhaustion gracefully rather than crashing or corrupting data. When memory usage approaches limits, the system should reject new queries or reduce cache sizes while continuing to serve existing requests. Disk full conditions should pause ingestion while preserving existing data.</p>\n<h3 id=\"scope-limitations\">Scope Limitations</h3>\n<p>The scope limitations define features explicitly excluded from this implementation. These boundaries prevent scope creep and ensure we build the core metrics collection capabilities excellently rather than attempting everything mediocrely.</p>\n<p><strong>Alerting System Exclusion</strong></p>\n<p>This implementation does not include alerting capabilities such as rule evaluation, threshold monitoring, or notification delivery. While alerting is a natural extension of metrics collection, implementing it properly requires additional components for rule management, evaluation scheduling, notification routing, and alert state tracking.</p>\n<p>Building alerting well requires solving problems orthogonal to metrics collection: template rendering for notifications, integration with external systems (email, Slack, PagerDuty), alert deduplication and grouping, and escalation policies. Including alerting would double the system complexity without improving the core collection, storage, and querying capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Alerting Feature</th>\n<th>Excluded Reason</th>\n<th>Alternative Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Evaluation</td>\n<td>Complex scheduling and state management</td>\n<td>External alerting system queries our API</td>\n</tr>\n<tr>\n<td>Notification Delivery</td>\n<td>Requires integration with many external services</td>\n<td>Use dedicated alerting tools</td>\n</tr>\n<tr>\n<td>Alert State Tracking</td>\n<td>Complex state machine for firing/resolved states</td>\n<td>Stateless query-based detection</td>\n</tr>\n<tr>\n<td>Escalation Policies</td>\n<td>Complex workflow management</td>\n<td>External incident management tools</td>\n</tr>\n</tbody></table>\n<p>Future implementations can add alerting by building a separate alert manager that queries this metrics system&#39;s API, maintaining clean separation of concerns.</p>\n<p><strong>Dashboard and Visualization Exclusion</strong></p>\n<p>The system does not include built-in dashboards, graphing capabilities, or web-based visualization tools. While metrics are most valuable when visualized, building excellent charting and dashboard functionality requires deep frontend expertise and extensive JavaScript development.</p>\n<p>Visualization tools have different requirements than metrics collection: real-time updates, interactive charts, dashboard templating, user authentication, and responsive design. These concerns are better addressed by specialized tools like Grafana that can query our system&#39;s API.</p>\n<p><strong>Advanced Query Functions Exclusion</strong></p>\n<p>The PromQL implementation includes only basic aggregation functions (<code>sum</code>, <code>avg</code>, <code>max</code>, <code>min</code>, <code>count</code>) and excludes advanced functions like <code>predict_linear</code>, <code>histogram_quantile</code>, or complex mathematical operations. These advanced functions require sophisticated algorithms and extensive testing to implement correctly.</p>\n<p>Similarly, the system excludes recording rules (pre-computed queries stored as new metrics) and complex rate calculations beyond basic <code>rate()</code> function. These features add significant complexity to the query engine without being essential for basic monitoring.</p>\n<table>\n<thead>\n<tr>\n<th>Advanced Feature</th>\n<th>Excluded Reason</th>\n<th>Basic Alternative</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>predict_linear</code></td>\n<td>Requires linear regression algorithms</td>\n<td>Use external analysis tools</td>\n</tr>\n<tr>\n<td><code>histogram_quantile</code></td>\n<td>Complex quantile calculation from buckets</td>\n<td>Use summary metrics instead</td>\n</tr>\n<tr>\n<td>Recording Rules</td>\n<td>Requires rule management and scheduling</td>\n<td>Run queries manually or externally</td>\n</tr>\n<tr>\n<td>Complex Math Functions</td>\n<td>Requires extensive function library</td>\n<td>Perform calculations in analysis tools</td>\n</tr>\n</tbody></table>\n<p><strong>Multi-Instance Federation Exclusion</strong></p>\n<p>This implementation targets single-instance deployment and excludes federation capabilities that would allow multiple metrics instances to share data or provide hierarchical aggregation. Federation requires solving distributed systems problems like data replication, conflict resolution, and cross-instance querying.</p>\n<p>Building federation properly requires consensus protocols, network partition handling, and complex query routing logic. These distributed systems challenges are significant projects themselves and would overshadow the core metrics collection functionality.</p>\n<p><strong>Enterprise Features Exclusion</strong></p>\n<p>The system excludes enterprise-oriented features like user authentication, multi-tenancy, access control, and audit logging. These features require extensive security implementation and user management capabilities that are orthogonal to metrics collection.</p>\n<p>Similarly, the system excludes advanced operational features like automatic backup/restore, cluster management, or sophisticated monitoring of the metrics system itself. These features are valuable for production deployment but not essential for understanding how metrics collection works.</p>\n<blockquote>\n<p><strong>Design Insight: Scope Boundaries Enable Excellence</strong>\nBy explicitly excluding advanced features, we can focus engineering effort on making the core metrics collection, storage, and querying capabilities excellent. A metrics system that handles these fundamentals perfectly provides a solid foundation for adding excluded features later, while a system that attempts everything often does nothing well.</p>\n</blockquote>\n<p>The excluded features can be addressed through external integration rather than built-in functionality. Alerting systems can query our API, visualization tools can display our data, and operational tools can manage our deployment. This approach follows the Unix philosophy of building tools that do one thing excellently and compose well with other tools.</p>\n<p><strong>⚠️ Common Pitfalls in Scope Definition</strong></p>\n<p>Developers often make these mistakes when defining project scope:</p>\n<p><strong>Pitfall: Vague Non-Goals</strong> - Writing &quot;won&#39;t include advanced features&quot; without specifying exactly which features. This leads to scope creep when stakeholders assume their favorite feature isn&#39;t &quot;advanced.&quot; Instead, explicitly list each excluded capability with reasoning.</p>\n<p><strong>Pitfall: Feature Creep During Implementation</strong> - Adding &quot;quick features&quot; during development because they seem easy. A simple dashboard or basic alerting might seem straightforward, but each addition introduces new failure modes, test requirements, and maintenance burden. Stick to the defined scope religiously.</p>\n<p><strong>Pitfall: Unrealistic Performance Targets</strong> - Setting performance requirements without understanding the underlying constraints. Claiming sub-millisecond query times while implementing complex aggregations over millions of series. Base performance targets on realistic measurements from similar systems.</p>\n<p><strong>Pitfall: Missing Quality Attributes</strong> - Focusing only on functional requirements while ignoring performance, reliability, and scalability needs. A metrics system that handles all the required features but responds slowly or loses data is unusable for monitoring production systems.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The requirements established in this section drive architectural decisions throughout the remaining design. Each functional requirement maps to specific implementation choices, while quality attributes establish measurable targets for validation.</p>\n<p><strong>Requirements Traceability Matrix</strong></p>\n<table>\n<thead>\n<tr>\n<th>Requirement Category</th>\n<th>Implementation Component</th>\n<th>Validation Method</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metrics Data Model</td>\n<td><code>Counter</code>, <code>Gauge</code>, <code>Histogram</code> types</td>\n<td>Unit tests with semantic validation</td>\n<td>Monotonic counter behavior, flexible gauge updates</td>\n</tr>\n<tr>\n<td>Multi-dimensional Labels</td>\n<td>Label map with validation</td>\n<td>Integration tests with high-cardinality scenarios</td>\n<td>Supports 10k series per metric name</td>\n</tr>\n<tr>\n<td>Pull-based Scraping</td>\n<td>HTTP client with service discovery</td>\n<td>Load testing with 1000 targets</td>\n<td>Maintains scrape intervals under load</td>\n</tr>\n<tr>\n<td>Gorilla Compression</td>\n<td>Delta-of-delta and XOR encoding</td>\n<td>Compression ratio measurement</td>\n<td>&lt; 2 bytes per sample average</td>\n</tr>\n<tr>\n<td>PromQL Queries</td>\n<td>Parser and execution engine</td>\n<td>Query correctness and performance tests</td>\n<td>&lt; 1 second response for typical queries</td>\n</tr>\n</tbody></table>\n<p><strong>Technology Stack Recommendations</strong></p>\n<p>For implementing the requirements defined in this section, the technology choices directly impact our ability to meet performance and scalability targets:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td><code>net/http</code> standard library</td>\n<td><code>fasthttp</code> or <code>gin</code> framework</td>\n<td>Standard library sufficient for scraping loads</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td><code>yaml.v3</code> for static config</td>\n<td><code>viper</code> for dynamic config</td>\n<td>YAML parsing meets static config requirements</td>\n</tr>\n<tr>\n<td>Time Series Storage</td>\n<td>Append-only files with custom format</td>\n<td>Embedded database like BadgerDB</td>\n<td>Custom format provides compression control</td>\n</tr>\n<tr>\n<td>Indexing</td>\n<td>In-memory maps with periodic snapshots</td>\n<td>LSM-tree based indexes</td>\n<td>Memory indexes meet single-instance scale</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>Custom Gorilla implementation</td>\n<td>Existing libraries like <code>tsz</code></td>\n<td>Custom implementation for educational value</td>\n</tr>\n</tbody></table>\n<p><strong>Configuration Structure Foundation</strong></p>\n<p>The requirements drive a specific configuration structure that supports all functional needs while maintaining simplicity:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Config represents the complete system configuration matching our functional requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Scrape configuration supports pull-based collection requirement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Scrape </span><span style=\"color:#B392F0\">ScrapeConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"scrape\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Storage configuration meets retention and compression requirements  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Query configuration enforces performance and resource limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query </span><span style=\"color:#B392F0\">QueryConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Server configuration for HTTP endpoints</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScrapeConfig implements target discovery and interval management requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScrapeConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Global defaults applied to all targets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScrapeInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"scrape_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScrapeTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"scrape_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Static target configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StaticConfigs []</span><span style=\"color:#B392F0\">StaticConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"static_configs\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Service discovery configuration  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FileServiceDiscovery []</span><span style=\"color:#B392F0\">FileSDConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"file_sd_configs\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageConfig meets retention and compression requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Data directory for time series storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataDirectory </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"data_directory\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Retention period for automatic cleanup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionPeriod </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"retention_period\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Compression settings for Gorilla algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompressionEnabled </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\"> `yaml:\"compression_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write-ahead log configuration for durability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WALEnabled </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\"> `yaml:\"wal_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryConfig enforces performance and resource limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Maximum query execution time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"query_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Maximum number of series a query can examine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxSeries </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `yaml:\"max_series\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Maximum range query duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxRangeDuration </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"max_range_duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Requirements Validation Framework</strong></p>\n<p>Each milestone should validate that requirements are being met through specific tests and measurements:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// RequirementsValidator provides methods to verify system meets defined requirements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RequirementsValidator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidatePerformanceRequirements checks that system meets performance targets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RequirementsValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidatePerformanceRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement query latency measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate write throughput under load</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check compression ratio achievement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify memory usage remains bounded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateScalabilityRequirements verifies system handles target scale</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RequirementsValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateScalabilityRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test with 1000 scrape targets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify 1M active time series support</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check cardinality limit enforcement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate retention period handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateReliabilityRequirements ensures durability and fault tolerance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RequirementsValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateReliabilityRequirements</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Test crash recovery with WAL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Verify target failure isolation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Check graceful resource exhaustion handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Validate concurrent access safety</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint: Requirements Verification</strong></p>\n<p>After implementing each milestone, verify it meets the requirements established in this section:</p>\n<p><strong>Milestone 1 Checkpoint - Metrics Data Model:</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/metrics/... -v</code></li>\n<li>Expected: All metric type tests pass, counter monotonicity enforced, labels validate correctly</li>\n<li>Manual verification: Create metrics with various label combinations, confirm cardinality limits work</li>\n<li>Performance check: Create 10k series, verify memory usage remains reasonable</li>\n</ul>\n<p><strong>Milestone 2 Checkpoint - Scrape Engine:</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/scraper/... -v</code> and start test HTTP targets</li>\n<li>Expected: Targets discovered from config, scraped at intervals, parse errors handled gracefully</li>\n<li>Manual verification: Configure 10 targets, observe scrape success rates and timing</li>\n<li>Performance check: Scrape 100 targets simultaneously, verify no blocking or missed intervals</li>\n</ul>\n<p><strong>Milestone 3 Checkpoint - Storage Engine:</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/storage/... -v</code> and measure compression ratios</li>\n<li>Expected: Data persisted durably, Gorilla compression achieves &lt; 2 bytes/sample, retention works</li>\n<li>Manual verification: Insert sample data, restart system, verify data recovers correctly</li>\n<li>Performance check: Sustain 10k samples/sec writes, measure query response times</li>\n</ul>\n<p><strong>Milestone 4 Checkpoint - Query Engine:</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/query/... -v</code> and execute sample PromQL queries</li>\n<li>Expected: Basic PromQL parsing works, aggregations produce correct results, range queries function</li>\n<li>Manual verification: Run queries against stored data, verify results match expectations</li>\n<li>Performance check: Query 1000 series over one week, verify sub-second response</li>\n</ul>\n<p>The requirements defined in this section provide the acceptance criteria for each milestone and the overall system success. Every implementation decision should trace back to these requirements, and every feature should be validated against these targets.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the architectural foundation for all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4).</p>\n</blockquote>\n<p>Think of our metrics collection system like a modern weather monitoring network. Weather stations (scrape targets) continuously measure temperature, humidity, and wind speed. Data collection trucks (scrape engine) visit each station on a schedule to gather measurements. The measurements are transported to a central archive (storage engine) where they&#39;re organized, compressed, and indexed for long-term preservation. Scientists and meteorologists (users) can then query the archive to analyze weather patterns, generate reports, and make predictions about future conditions.</p>\n<p>This mental model captures the essential flow of our system: autonomous data generation at distributed endpoints, scheduled collection via pull-based mechanisms, efficient storage with temporal organization, and flexible querying for analysis and monitoring. Just as weather data becomes valuable when aggregated and analyzed over time, metrics data provides observability insights when collected systematically and made queryable.</p>\n<p>Our architecture consists of four primary components that work together to implement this observability pipeline. Each component has distinct responsibilities and interfaces, but they&#39;re designed to work together seamlessly to provide a complete metrics collection and analysis solution.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<h3 id=\"component-responsibilities\">Component Responsibilities</h3>\n<p>The system is decomposed into four major components, each with clearly defined responsibilities and boundaries. This separation of concerns enables independent development, testing, and scaling of each component while maintaining clean interfaces between them.</p>\n<h4 id=\"scrape-engine-component\">Scrape Engine Component</h4>\n<p>The <strong>Scrape Engine</strong> serves as the data acquisition layer of our system, responsible for discovering targets and collecting metrics from distributed endpoints. Think of it as a fleet of postal workers who know every address in the city, visit each location on a precise schedule, and collect all the mail waiting for pickup.</p>\n<p>The scrape engine operates on a pull-based model, meaning it actively reaches out to configured targets rather than waiting for them to push data. This design choice provides several advantages: targets remain stateless and don&#39;t need to know about the collector, network failures are handled centrally, and the collector maintains complete control over data collection timing and frequency.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Key Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target Discovery</td>\n<td>Locate scrape endpoints through static config or service discovery</td>\n<td>Parse configuration files, query DNS records, interact with Kubernetes API</td>\n</tr>\n<tr>\n<td>Scrape Scheduling</td>\n<td>Execute HTTP requests to targets at configured intervals</td>\n<td>Manage per-target timers, handle concurrent scraping, track scrape success/failure</td>\n</tr>\n<tr>\n<td>Metrics Parsing</td>\n<td>Parse Prometheus exposition format from HTTP response bodies</td>\n<td>Tokenize text format, validate metric names and labels, extract timestamps</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>Track target availability and scrape success rates</td>\n<td>Record response times, detect timeouts, maintain target status</td>\n</tr>\n<tr>\n<td>Backpressure Control</td>\n<td>Prevent overwhelming targets with too-frequent requests</td>\n<td>Implement exponential backoff, respect rate limits, queue scrape requests</td>\n</tr>\n</tbody></table>\n<p>The scrape engine maintains its own configuration system with support for dynamic updates. When service discovery detects new targets, the engine automatically begins scraping them without requiring a restart. Similarly, when targets disappear, scraping stops and resources are cleaned up.</p>\n<blockquote>\n<p><strong>Decision: Pull-Based Collection Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Metrics can be collected via push (targets send data) or pull (collector retrieves data) models</li>\n<li><strong>Options Considered</strong>: Push-based with target-initiated connections, pull-based with collector-initiated connections, hybrid approach</li>\n<li><strong>Decision</strong>: Pull-based collection with HTTP scraping</li>\n<li><strong>Rationale</strong>: Pull model provides better failure isolation (targets can&#39;t overwhelm collector), simpler target implementation (no need to know collector address), and centralized control over collection frequency and timeout handling</li>\n<li><strong>Consequences</strong>: Requires targets to expose HTTP endpoints, collector must maintain target inventory, network failures affect data collection</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Collection Model</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Push-based</td>\n<td>Targets control timing, works behind firewalls</td>\n<td>Targets need collector address, backpressure harder to manage</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Pull-based</td>\n<td>Centralized control, simpler targets, better failure isolation</td>\n<td>Requires HTTP endpoints, collector needs target discovery</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Flexibility for different target types</td>\n<td>Increased complexity, two codepaths to maintain</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<h4 id=\"time-series-storage-component\">Time Series Storage Component</h4>\n<p>The <strong>Storage Engine</strong> functions as the system&#39;s memory and archive, responsible for persisting collected metrics efficiently and making them available for queries. Imagine a specialized library that stores millions of temperature readings, where each book represents a time series and pages contain chronologically ordered measurements compressed to save space.</p>\n<p>The storage layer implements several sophisticated optimizations to handle the unique characteristics of time series data. Unlike traditional databases that optimize for random access patterns, time series data is almost always written in chronological order and queried by time ranges. This access pattern enables aggressive compression and specialized indexing strategies.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Key Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Ingestion</td>\n<td>Accept scraped metrics and store them durably</td>\n<td>Validate metric format, assign timestamps, write to WAL, index updates</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>Reduce storage footprint using time series algorithms</td>\n<td>Apply Gorilla compression, manage chunk boundaries, optimize encoding</td>\n</tr>\n<tr>\n<td>Indexing</td>\n<td>Maintain lookups for metric names and label combinations</td>\n<td>Build inverted indexes, handle label cardinality, optimize query performance</td>\n</tr>\n<tr>\n<td>Retention Management</td>\n<td>Delete old data based on configured policies</td>\n<td>Scan for expired data, compact storage files, update indexes</td>\n</tr>\n<tr>\n<td>Query Processing</td>\n<td>Retrieve time series data for specified time ranges and labels</td>\n<td>Seek to time ranges, decompress data, filter by labels, return results</td>\n</tr>\n</tbody></table>\n<p>The storage engine uses a <strong>Write-Ahead Log (WAL)</strong> to ensure durability. Every metric write is first recorded in the WAL before being applied to the main storage structures. This guarantees that even if the system crashes during a write operation, the data can be recovered by replaying the WAL on startup.</p>\n<p>Storage is organized into <strong>time-based chunks</strong> that contain compressed time series data for a specific time window (typically 2 hours). This chunking strategy optimizes both compression ratio and query performance by ensuring that data accessed together is stored together physically.</p>\n<blockquote>\n<p><strong>Decision: Gorilla-Style Compression</strong></p>\n<ul>\n<li><strong>Context</strong>: Time series data has high redundancy that can be exploited for compression</li>\n<li><strong>Options Considered</strong>: Generic compression (gzip), column-oriented compression, Gorilla delta-of-delta compression</li>\n<li><strong>Decision</strong>: Implement Gorilla compression with delta-of-delta timestamps and XOR value encoding</li>\n<li><strong>Rationale</strong>: Gorilla compression achieves 12x space reduction on typical time series workloads, optimized specifically for time series patterns, and provides good query performance without full decompression</li>\n<li><strong>Consequences</strong>: More complex implementation than generic compression, requires careful chunk boundary management, but provides significant storage savings</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Compression Approach</th>\n<th>Compression Ratio</th>\n<th>Query Performance</th>\n<th>Implementation Complexity</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No compression</td>\n<td>1x (baseline)</td>\n<td>Fastest</td>\n<td>Simplest</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Generic (gzip)</td>\n<td>3-5x</td>\n<td>Slow (full decompression)</td>\n<td>Simple</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Gorilla algorithm</td>\n<td>10-15x</td>\n<td>Fast (partial decompression)</td>\n<td>Complex</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<h4 id=\"query-engine-component\">Query Engine Component</h4>\n<p>The <strong>Query Engine</strong> serves as the system&#39;s analytical brain, translating user queries into efficient data retrieval and processing operations. Think of it as a research librarian who understands both what researchers are looking for and the most efficient way to locate and combine information from the archive.</p>\n<p>The query engine implements a PromQL-compatible query language that supports both instant queries (single point in time) and range queries (time series over a window). The engine&#39;s job is to parse these queries, plan efficient execution, retrieve the necessary data from storage, and perform any required aggregations or mathematical operations.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Key Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Parsing</td>\n<td>Convert PromQL text into executable query plans</td>\n<td>Tokenize expressions, build AST, validate syntax, optimize plans</td>\n</tr>\n<tr>\n<td>Series Selection</td>\n<td>Find time series matching label selectors</td>\n<td>Query storage indexes, apply label filters, handle regex matching</td>\n</tr>\n<tr>\n<td>Data Retrieval</td>\n<td>Fetch time series data from storage for specified time ranges</td>\n<td>Issue storage queries, handle pagination, manage memory usage</td>\n</tr>\n<tr>\n<td>Aggregation</td>\n<td>Perform mathematical operations across time series</td>\n<td>Implement sum/avg/max/min/count, handle grouping, compute rates</td>\n</tr>\n<tr>\n<td>Result Formatting</td>\n<td>Convert internal data structures to API response format</td>\n<td>Serialize to JSON, apply time formatting, handle large result sets</td>\n</tr>\n</tbody></table>\n<p>The query engine is designed to be <strong>stateless</strong> - each query execution is independent and doesn&#39;t rely on cached state from previous queries. This design simplifies scaling and debugging but requires careful optimization to avoid repeatedly processing the same data.</p>\n<p>Query execution follows a <strong>pipeline model</strong> where data flows through a series of processing stages: series selection, data retrieval, time alignment, aggregation, and result formatting. Each stage can be optimized independently and may process data in streaming fashion to reduce memory usage.</p>\n<blockquote>\n<p><strong>Decision: AST-Based Query Execution</strong></p>\n<ul>\n<li><strong>Context</strong>: PromQL queries need to be parsed and executed efficiently with proper operator precedence</li>\n<li><strong>Options Considered</strong>: Direct interpreter, AST with visitor pattern, compiled query plans</li>\n<li><strong>Decision</strong>: Abstract Syntax Tree with recursive evaluation</li>\n<li><strong>Rationale</strong>: AST provides clear separation between parsing and execution, supports complex nested expressions, and enables query optimization passes</li>\n<li><strong>Consequences</strong>: More complex than direct interpretation but provides better extensibility and optimization opportunities</li>\n</ul>\n</blockquote>\n<h4 id=\"http-api-server-component\">HTTP API Server Component</h4>\n<p>The <strong>HTTP Server</strong> acts as the system&#39;s front door, providing REST endpoints for queries, configuration, and system status. It handles authentication, request routing, response formatting, and protocol translation between external HTTP clients and internal components.</p>\n<p>The server component is relatively lightweight since most of the heavy lifting is done by the specialized engines. Its primary responsibilities center around protocol handling, request validation, and response formatting rather than core metrics processing.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Key Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Routing</td>\n<td>Direct incoming HTTP requests to appropriate handlers</td>\n<td>Parse URLs, validate methods, route to query/config/status handlers</td>\n</tr>\n<tr>\n<td>Query API</td>\n<td>Expose PromQL query interface via HTTP endpoints</td>\n<td>Parse query parameters, invoke query engine, format responses</td>\n</tr>\n<tr>\n<td>Configuration API</td>\n<td>Allow runtime configuration updates</td>\n<td>Accept new scrape configs, validate settings, notify components</td>\n</tr>\n<tr>\n<td>Metrics Exposition</td>\n<td>Expose system&#39;s own metrics for monitoring</td>\n<td>Generate internal metrics, format in Prometheus format</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Provide consistent error responses and logging</td>\n<td>Map internal errors to HTTP status codes, log requests</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-flow-overview\">Data Flow Overview</h3>\n<p>Understanding how data moves through our system is crucial for both implementation and debugging. The data flow follows a clear pipeline from external targets through our components and back to users, with each stage transforming and enriching the data.</p>\n<h4 id=\"metrics-collection-flow\">Metrics Collection Flow</h4>\n<p>The metrics collection flow begins when the scrape engine identifies targets through service discovery and executes HTTP requests to gather metrics data. This process runs continuously in the background according to configured intervals.</p>\n<ol>\n<li><p><strong>Target Discovery Phase</strong>: The scrape engine queries configured service discovery backends (static files, DNS, Kubernetes API) to build a current list of scrape targets. Each target includes an endpoint URL, scrape interval, timeout, and optional labels.</p>\n</li>\n<li><p><strong>Scrape Scheduling Phase</strong>: A scheduler component maintains timing wheels for each target, triggering scrape operations according to their individual intervals. Multiple scrapes execute concurrently using worker pools to maximize throughput.</p>\n</li>\n<li><p><strong>HTTP Collection Phase</strong>: For each scrape, the engine sends an HTTP GET request to the target&#39;s metrics endpoint (typically <code>/metrics</code>). The request includes appropriate headers and handles authentication if configured.</p>\n</li>\n<li><p><strong>Metrics Parsing Phase</strong>: The response body contains metrics in Prometheus exposition format (text-based). The parser tokenizes this content, extracting metric names, labels, values, and timestamps. Invalid metrics are logged and discarded.</p>\n</li>\n<li><p><strong>Storage Ingestion Phase</strong>: Parsed metrics are sent to the storage engine, which validates them, assigns canonical timestamps, and writes them to the WAL. The data is then indexed and added to the appropriate time series chunks.</p>\n</li>\n<li><p><strong>Health Tracking Phase</strong>: The scrape engine records success/failure status, response times, and error details for each scrape operation. This information is used for target health monitoring and alerting.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n<th>Failure Mode</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target Discovery</td>\n<td>Service discovery configs</td>\n<td>Query APIs, parse responses</td>\n<td>Target list with endpoints</td>\n<td>Service discovery unavailable</td>\n</tr>\n<tr>\n<td>Scrape Scheduling</td>\n<td>Target list, intervals</td>\n<td>Manage timers, worker pools</td>\n<td>Scrape tasks</td>\n<td>Resource exhaustion</td>\n</tr>\n<tr>\n<td>HTTP Collection</td>\n<td>Target endpoints</td>\n<td>HTTP GET requests</td>\n<td>Raw metrics text</td>\n<td>Network timeout, HTTP errors</td>\n</tr>\n<tr>\n<td>Metrics Parsing</td>\n<td>Metrics text</td>\n<td>Tokenize, validate format</td>\n<td>Structured metrics</td>\n<td>Parse errors, invalid format</td>\n</tr>\n<tr>\n<td>Storage Ingestion</td>\n<td>Structured metrics</td>\n<td>Write WAL, update indexes</td>\n<td>Persisted time series</td>\n<td>Disk full, corruption</td>\n</tr>\n<tr>\n<td>Health Tracking</td>\n<td>Scrape results</td>\n<td>Record status, compute metrics</td>\n<td>Target health status</td>\n<td>Monitoring system failure</td>\n</tr>\n</tbody></table>\n<h4 id=\"query-processing-flow\">Query Processing Flow</h4>\n<p>The query processing flow is triggered when users submit PromQL queries via the HTTP API. This flow involves parsing the query, planning execution, retrieving data, and formatting results.</p>\n<ol>\n<li><p><strong>Query Reception Phase</strong>: The HTTP server receives a query request containing a PromQL expression, time range (for range queries), and optional parameters like timeout limits.</p>\n</li>\n<li><p><strong>Query Parsing Phase</strong>: The query engine tokenizes the PromQL expression and builds an Abstract Syntax Tree (AST) representing the query structure. Syntax errors are detected and reported at this stage.</p>\n</li>\n<li><p><strong>Query Planning Phase</strong>: The engine analyzes the AST to determine which time series need to be retrieved from storage, what time ranges are required, and what aggregations need to be performed.</p>\n</li>\n<li><p><strong>Series Selection Phase</strong>: Using the storage engine&#39;s indexes, the system identifies all time series that match the query&#39;s label selectors. This may involve regex matching and label combination lookups.</p>\n</li>\n<li><p><strong>Data Retrieval Phase</strong>: For each selected series, the storage engine retrieves data points within the query&#39;s time range. Gorilla compression is applied in reverse to decompress the stored data.</p>\n</li>\n<li><p><strong>Query Execution Phase</strong>: The engine applies aggregation functions, mathematical operations, and grouping logic to compute the final results according to the PromQL expression.</p>\n</li>\n<li><p><strong>Result Formatting Phase</strong>: The computed results are serialized into the appropriate response format (JSON for API queries) and returned to the client.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n<th>Performance Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Reception</td>\n<td>HTTP request</td>\n<td>Validate parameters, parse time ranges</td>\n<td>Query parameters</td>\n<td>Request validation overhead</td>\n</tr>\n<tr>\n<td>Query Parsing</td>\n<td>PromQL text</td>\n<td>Tokenize, build AST</td>\n<td>Query execution plan</td>\n<td>Parser complexity scales with query complexity</td>\n</tr>\n<tr>\n<td>Query Planning</td>\n<td>AST</td>\n<td>Analyze requirements, optimize</td>\n<td>Execution strategy</td>\n<td>Planning time vs execution efficiency tradeoff</td>\n</tr>\n<tr>\n<td>Series Selection</td>\n<td>Label selectors</td>\n<td>Index lookups, regex matching</td>\n<td>Matching series list</td>\n<td>Index efficiency critical for high cardinality</td>\n</tr>\n<tr>\n<td>Data Retrieval</td>\n<td>Series list, time range</td>\n<td>Storage queries, decompression</td>\n<td>Raw data points</td>\n<td>I/O bound, compression CPU cost</td>\n</tr>\n<tr>\n<td>Query Execution</td>\n<td>Raw data, operations</td>\n<td>Aggregation, math functions</td>\n<td>Computed results</td>\n<td>Memory usage scales with result size</td>\n</tr>\n<tr>\n<td>Result Formatting</td>\n<td>Computed results</td>\n<td>Serialize to JSON</td>\n<td>HTTP response</td>\n<td>Serialization cost for large result sets</td>\n</tr>\n</tbody></table>\n<h3 id=\"deployment-architecture\">Deployment Architecture</h3>\n<p>The deployment architecture defines how our components are distributed across machines and how they scale to handle production workloads. We support multiple deployment patterns depending on scale requirements and operational constraints.</p>\n<h4 id=\"single-instance-deployment\">Single-Instance Deployment</h4>\n<p>For development, testing, and smaller production workloads, all components can be deployed within a single process on one machine. This deployment pattern minimizes operational complexity while providing full functionality.</p>\n<p>In single-instance deployment, all components share the same process space and communicate via direct function calls rather than network protocols. The HTTP server, scrape engine, storage engine, and query engine all run as goroutines within the same binary.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Resource Usage</th>\n<th>Scaling Limit</th>\n<th>Failure Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scrape Engine</td>\n<td>CPU-bound (HTTP clients)</td>\n<td>~1000 targets per instance</td>\n<td>All scraping stops</td>\n</tr>\n<tr>\n<td>Storage Engine</td>\n<td>Disk I/O and memory</td>\n<td>~1M active series</td>\n<td>All data inaccessible</td>\n</tr>\n<tr>\n<td>Query Engine</td>\n<td>CPU and memory</td>\n<td>~100 concurrent queries</td>\n<td>All queries fail</td>\n</tr>\n<tr>\n<td>HTTP Server</td>\n<td>Network and CPU</td>\n<td>~1000 QPS</td>\n<td>All API access lost</td>\n</tr>\n</tbody></table>\n<p>The single instance stores all data locally using disk-based storage with configurable data directories. Configuration is provided via YAML files that specify scrape targets, retention policies, and server settings.</p>\n<blockquote>\n<p><strong>Decision: Single-Process Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance simplicity with scalability for initial implementation</li>\n<li><strong>Options Considered</strong>: Microservices from the start, single process with internal interfaces, monolithic design</li>\n<li><strong>Decision</strong>: Single process with clean component interfaces that can be split later</li>\n<li><strong>Rationale</strong>: Reduces operational complexity, simplifies debugging, enables easier testing while maintaining clean boundaries for future scaling</li>\n<li><strong>Consequences</strong>: Limited horizontal scalability but much simpler deployment and development</li>\n</ul>\n</blockquote>\n<h4 id=\"configuration-management\">Configuration Management</h4>\n<p>The system uses a hierarchical configuration approach with YAML files, environment variables, and command-line flags. Configuration can be reloaded at runtime without requiring a restart, enabling dynamic updates to scrape targets and other settings.</p>\n<p>The <code>Config</code> struct serves as the root configuration object, containing nested configuration for each component:</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Section</th>\n<th>Key Settings</th>\n<th>Reload Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ScrapeConfig</code></td>\n<td>scrape_interval, scrape_timeout, static_configs</td>\n<td>Dynamic reload, affects next scrape cycle</td>\n</tr>\n<tr>\n<td><code>StorageConfig</code></td>\n<td>data_directory, retention_period, compression_enabled</td>\n<td>Partial reload, retention changes apply immediately</td>\n</tr>\n<tr>\n<td><code>QueryConfig</code></td>\n<td>query_timeout, max_series, max_range_duration</td>\n<td>Dynamic reload, affects new queries</td>\n</tr>\n<tr>\n<td><code>ServerConfig</code></td>\n<td>port, read_timeout, write_timeout</td>\n<td>Requires restart</td>\n</tr>\n</tbody></table>\n<p>Configuration validation occurs at startup and during reload operations. The <code>RequirementsValidator</code> checks that the system can meet specified performance, scalability, and reliability requirements given the current configuration and available resources.</p>\n<h4 id=\"process-management-and-health-monitoring\">Process Management and Health Monitoring</h4>\n<p>The system exposes its own metrics via HTTP endpoints, enabling monitoring of internal performance and health. Key metrics include scrape success rates, query latency, storage utilization, and error rates.</p>\n<p>Health checks are available at multiple levels: individual component health (can the storage engine accept writes?), system health (are all components functioning?), and operational health (are performance targets being met?).</p>\n<table>\n<thead>\n<tr>\n<th>Health Check</th>\n<th>Endpoint</th>\n<th>Success Criteria</th>\n<th>Failure Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Liveness</td>\n<td><code>/health/live</code></td>\n<td>Process responding</td>\n<td>HTTP 503, restart container</td>\n</tr>\n<tr>\n<td>Readiness</td>\n<td><code>/health/ready</code></td>\n<td>All components initialized</td>\n<td>HTTP 503, remove from load balancer</td>\n</tr>\n<tr>\n<td>Component Health</td>\n<td><code>/health/components</code></td>\n<td>Each component passes self-check</td>\n<td>HTTP 200 with component status</td>\n</tr>\n</tbody></table>\n<p>The system implements graceful shutdown procedures that allow in-flight operations to complete before termination. Query processing completes current requests, scraping finishes active scrapes, and storage flushes pending writes to disk.</p>\n<h4 id=\"resource-requirements-and-capacity-planning\">Resource Requirements and Capacity Planning</h4>\n<p>Resource requirements scale primarily with the number of active time series (cardinality) and query load rather than the volume of individual metrics. A time series that receives one sample per minute versus one sample per second requires similar memory and storage resources due to compression efficiency.</p>\n<p>Memory usage is dominated by the time series index and uncompressed data in active chunks. Each active time series requires approximately 1KB of memory for index structures plus a proportional share of chunk overhead.</p>\n<table>\n<thead>\n<tr>\n<th>Workload Characteristic</th>\n<th>Memory Impact</th>\n<th>Storage Impact</th>\n<th>CPU Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High cardinality (many series)</td>\n<td>Linear growth in index size</td>\n<td>Linear growth in compressed data</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>High sample rate</td>\n<td>Minimal (better compression)</td>\n<td>Sublinear growth</td>\n<td>Linear in ingestion CPU</td>\n</tr>\n<tr>\n<td>Complex queries</td>\n<td>Temporary growth during execution</td>\n<td>None</td>\n<td>High CPU usage</td>\n</tr>\n<tr>\n<td>Large time ranges</td>\n<td>Memory for decompressed data</td>\n<td>None</td>\n<td>Decompression CPU cost</td>\n</tr>\n</tbody></table>\n<p>Storage requirements depend on retention period, compression efficiency, and cardinality. Gorilla compression typically achieves 12:1 reduction, so 1 million series with 1-minute resolution retained for 30 days requires approximately 180GB of disk space.</p>\n<h4 id=\"common-pitfalls\">Common Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Underestimating Label Cardinality Impact</strong>\nLabel combinations multiply to create the total number of time series. Adding labels like <code>instance_id</code> or <code>user_id</code> can explode cardinality exponentially. Monitor the number of active series and set cardinality limits to prevent memory exhaustion.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Resource Allocation for Query Engine</strong>\nComplex PromQL queries can consume significant memory and CPU, especially when aggregating across high cardinality labels. Set query timeouts and memory limits, and monitor query performance to identify expensive patterns.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Scrape Target Health</strong>\nFailed scrapes result in data gaps that affect query results and alerting. Implement proper monitoring of scrape success rates and set up alerting when targets become unavailable.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Storage Planning</strong>\nTime series data grows continuously, and running out of disk space causes immediate system failure. Plan for at least 20% overhead beyond calculated requirements and implement monitoring of disk usage with automated alerting.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation approach balances simplicity for learning purposes with realistic production patterns. We&#39;ll use Go as the primary language due to its excellent concurrency support and HTTP capabilities.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td>net/http with custom routing</td>\n<td>Gorilla Mux or Gin framework</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>encoding/json with structs</td>\n<td>YAML parsing with gopkg.in/yaml.v2</td>\n</tr>\n<tr>\n<td>Storage Backend</td>\n<td>Local filesystem with os package</td>\n<td>Embedded database like BadgerDB</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>Custom Gorilla implementation</td>\n<td>Existing library like prometheus/tsdb</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static file-based configuration</td>\n<td>Kubernetes client-go integration</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard log package</td>\n<td>Structured logging with logrus/zap</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>Organize the codebase to reflect the component boundaries and support independent development:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>metrics-collector/\n├── cmd/\n│   └── collector/\n│       └── main.go                 ← Entry point and CLI parsing\n├── internal/\n│   ├── config/\n│   │   ├── config.go               ← Config structs and loading\n│   │   └── validator.go            ← RequirementsValidator implementation\n│   ├── scrape/\n│   │   ├── engine.go               ← Scrape scheduling and execution\n│   │   ├── target.go               ← Target management and service discovery\n│   │   └── parser.go               ← Prometheus format parsing\n│   ├── storage/\n│   │   ├── engine.go               ← Main storage interface\n│   │   ├── series.go               ← Time series data structures\n│   │   ├── compression.go          ← Gorilla compression implementation\n│   │   └── index.go                ← Series indexing and lookup\n│   ├── query/\n│   │   ├── engine.go               ← Query planning and execution\n│   │   ├── parser.go               ← PromQL parsing and AST\n│   │   └── functions.go            ← Aggregation and math functions\n│   └── server/\n│       ├── server.go               ← HTTP server and routing\n│       └── handlers.go             ← API endpoint handlers\n├── pkg/\n│   ├── metrics/                    ← Shared metric type definitions\n│   └── model/                      ← Common data structures\n└── configs/\n    └── example.yaml                ← Example configuration file</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Here&#39;s the basic HTTP server infrastructure that handles routing and graceful shutdown:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/server/server.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> server</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Server wraps HTTP server for metrics endpoints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Server</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpServer </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ServeMux</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewServer creates HTTP server on specified port</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">port</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewServeMux</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        httpServer: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Addr:         fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\":</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, port),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Handler:      mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mux: mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterScrapeEndpoint adds metrics exposition endpoint</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterScrapeEndpoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">path</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">handler</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(path, handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins serving HTTP requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Shutdown gracefully stops server</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Configuration loading infrastructure with validation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/config/config.go  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io/ioutil</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">gopkg.in/yaml.v2</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Default constants for configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_SCRAPE_INTERVAL</span><span style=\"color:#F97583\">   =</span><span style=\"color:#79B8FF\"> 15</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_SCRAPE_TIMEOUT</span><span style=\"color:#F97583\">    =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_RETENTION_PERIOD</span><span style=\"color:#F97583\">  =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour  </span><span style=\"color:#6A737D\">// 30 days</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_QUERY_TIMEOUT</span><span style=\"color:#F97583\">     =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config is the root configuration structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Scrape  </span><span style=\"color:#B392F0\">ScrapeConfig</span><span style=\"color:#9ECBFF\">  `yaml:\"scrape\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query   </span><span style=\"color:#B392F0\">QueryConfig</span><span style=\"color:#9ECBFF\">   `yaml:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server  </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\">  `yaml:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScrapeConfig defines scraping parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScrapeConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScrapeInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">    `yaml:\"scrape_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScrapeTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">    `yaml:\"scrape_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StaticConfigs  []</span><span style=\"color:#B392F0\">StaticConfig</span><span style=\"color:#9ECBFF\">   `yaml:\"static_configs\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageConfig defines storage parameters  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataDirectory     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"data_directory\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionPeriod   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"retention_period\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompressionEnabled </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">         `yaml:\"compression_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryConfig defines query engine parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryTimeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"query_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxSeries         </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"max_series\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxRangeDuration  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"max_range_duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ServerConfig defines HTTP server parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ServerConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Port </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `yaml:\"port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StaticConfig defines a static scrape target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StaticConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Targets []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `yaml:\"targets\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadFromFile reads YAML configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadFromFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Read file content using ioutil.ReadFile</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Parse YAML using yaml.Unmarshal  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Call SetDefaults to populate missing values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Return parsed config or error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetDefaults populates default config values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetDefaults</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Set default values for all duration and numeric fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Ensure required string fields have sensible defaults</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Apply defaults recursively to nested config structs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>Here are the method signatures for the core components that learners should implement:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/scrape/engine.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> scrape</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScrapeEngine manages target discovery and metric collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScrapeEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    targets  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Target</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add fields for worker pools, scheduling, health tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewScrapeEngine creates a new scrape engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewScrapeEngine</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client:  </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{Timeout: DEFAULT_SCRAPE_TIMEOUT},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        targets: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Target</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateTargets refreshes the target list from configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateTargets</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">configs</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">StaticConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse static configs into Target structs with URLs and labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compare new targets with existing targets map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add new targets and remove obsolete targets  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Schedule scraping for new targets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update target labels for existing targets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// scrapeTarget performs HTTP request to collect metrics from one target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">scrapeTarget</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Target</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create HTTP GET request with proper headers and timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute request and handle network errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Read response body and validate HTTP status code</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse metrics from response body using exposition format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send parsed metrics to storage engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update target health status based on success/failure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/engine.go  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageEngine manages time series persistence and retrieval</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dataDir   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retention </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add fields for WAL, indexes, compression, chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewStorageEngine creates a storage engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStorageEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Initialize storage directories, WAL, indexes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Append adds new samples to time series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate samples have valid timestamps and metric names</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Write samples to WAL for durability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Group samples by time series (metric name + labels)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add samples to appropriate time series chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update inverted indexes for new label combinations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Trigger compression for completed chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Select retrieves time series data matching label selectors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Select</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">matchers</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">LabelMatcher</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">SeriesSet</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Use inverted indexes to find series matching label selectors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine which chunks overlap with [start, end] time range  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Read and decompress relevant chunks from disk</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Filter samples to exact time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return SeriesSet iterator over matching time series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Go-Specific Implementation Tips:</strong></p>\n<ul>\n<li>Use <code>sync.RWMutex</code> for concurrent access to the target map in the scrape engine</li>\n<li>Implement graceful shutdown using <code>context.Context</code> and <code>sync.WaitGroup</code> for worker goroutines  </li>\n<li>Use <code>time.Ticker</code> for scrape scheduling rather than <code>time.Sleep</code> in loops</li>\n<li>Apply <code>os.File.Sync()</code> after WAL writes to ensure durability</li>\n<li>Use <code>encoding/binary</code> for efficient serialization of timestamps and float values</li>\n<li>Implement the <code>sort.Interface</code> for time series to enable efficient range queries</li>\n<li>Use <code>regexp.Compile</code> once at startup for label regex matchers, not on every query</li>\n</ul>\n<p><strong>Error Handling Patterns:</strong></p>\n<ul>\n<li>Wrap errors with context using <code>fmt.Errorf(&quot;scraping target %s: %w&quot;, url, err)</code></li>\n<li>Use sentinel errors like <code>var ErrTargetTimeout = errors.New(&quot;target timeout&quot;)</code> for specific failures</li>\n<li>Implement retry logic with exponential backoff for transient network failures</li>\n<li>Log errors at appropriate levels: network timeouts as warnings, configuration errors as errors</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Milestone 1 (Metrics Data Model):</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/metrics/...</code> </li>\n<li>Expected: All metric type tests pass, Counter increases monotonically, Gauge accepts any value</li>\n<li>Manual verification: Create Counter and Gauge, observe values, confirm label attachment works</li>\n<li>Success indicator: Can create metrics with labels and retrieve current values</li>\n</ul>\n<p><strong>After Milestone 2 (Scrape Engine):</strong></p>\n<ul>\n<li>Run: <code>go run cmd/collector/main.go --config=configs/example.yaml</code></li>\n<li>Expected: See periodic scrape logs, target health status updates</li>\n<li>Manual verification: Configure a target, observe HTTP requests in target logs</li>\n<li>Success indicator: Metrics are successfully parsed from HTTP endpoints</li>\n</ul>\n<p><strong>After Milestone 3 (Time Series Storage):</strong>  </p>\n<ul>\n<li>Run: <code>go test ./internal/storage/... -v</code></li>\n<li>Expected: Compression tests show &gt;10x space reduction, retention policies delete old data</li>\n<li>Manual verification: Write samples, restart process, verify data persists</li>\n<li>Success indicator: Data survives restarts and compression reduces storage size</li>\n</ul>\n<p><strong>After Milestone 4 (Query Engine):</strong></p>\n<ul>\n<li>Run: Query API endpoint: <code>curl &quot;http://localhost:9090/api/v1/query?query=up&quot;</code></li>\n<li>Expected: JSON response with metric values and timestamps  </li>\n<li>Manual verification: Try range queries, aggregation functions, label filtering</li>\n<li>Success indicator: PromQL queries return correct results matching stored data</li>\n</ul>\n<h2 id=\"metrics-data-model\">Metrics Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section directly corresponds to Milestone 1 (Metrics Data Model) and establishes the foundational data structures that will be used throughout Milestones 2-4.</p>\n</blockquote>\n<p>The metrics data model forms the conceptual and technical foundation upon which the entire monitoring system is built. Think of it as the vocabulary and grammar of our observability language - just as human language needs nouns, verbs, and sentence structure to convey meaning, our metrics system needs well-defined data types, labeling semantics, and identification rules to effectively communicate system behavior over time.</p>\n<p>Understanding the metrics data model is crucial because every decision made here ripples through the entire system architecture. The choice of metric types determines what kinds of mathematical operations the query engine can perform. The labeling system directly impacts storage cardinality and memory usage. The time series identity model affects indexing strategies and query performance. Get these fundamentals wrong, and the entire system becomes either unusable due to poor performance or unreliable due to semantic inconsistencies.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fmetrics-data-model.svg\" alt=\"Metrics Data Model\"></p>\n<h3 id=\"metric-type-semantics\">Metric Type Semantics</h3>\n<p>Think of metric types as different kinds of measuring instruments in a scientific laboratory. A thermometer measures absolute temperature at a point in time (like a gauge), while a Geiger counter accumulates radiation exposure over time (like a counter). Each instrument has specific mathematical properties that determine what calculations make sense - you can subtract two temperature readings to find a delta, but subtracting Geiger counter readings would be meaningless without considering the time periods involved.</p>\n<p>Our metrics system supports four fundamental metric types, each with distinct semantic behaviors that enable different kinds of analysis and alerting patterns.</p>\n<blockquote>\n<p><strong>Decision: Four-Type Metric Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support diverse monitoring use cases from simple resource metrics to complex latency distributions while maintaining clear semantic boundaries</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Simple gauge-only model (Graphite-style)</li>\n<li>Four-type model (Counter, Gauge, Histogram, Summary)</li>\n<li>Extended model with additional types (Sets, Traces)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Four-type model matching Prometheus specification</li>\n<li><strong>Rationale</strong>: Balances expressiveness with implementation complexity, provides clear semantic guarantees for rate calculations and aggregations, widely understood by practitioners</li>\n<li><strong>Consequences</strong>: Query engine must understand type-specific operations, storage must preserve type information, scraping must validate type consistency</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Metric Type</th>\n<th>Mathematical Properties</th>\n<th>Use Cases</th>\n<th>Invalid Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Counter</code></td>\n<td>Monotonically increasing, resets to zero</td>\n<td>Request counts, error counts, bytes transferred</td>\n<td>Decrease operations, negative values, arbitrary sets</td>\n</tr>\n<tr>\n<td><code>Gauge</code></td>\n<td>Arbitrary value, can increase/decrease</td>\n<td>CPU usage, memory usage, queue depth, temperature</td>\n<td>Rate calculations without smoothing</td>\n</tr>\n<tr>\n<td><code>Histogram</code></td>\n<td>Distribution with predefined buckets</td>\n<td>Request latency, response sizes, batch sizes</td>\n<td>Bucket boundary changes after creation</td>\n</tr>\n<tr>\n<td><code>Summary</code></td>\n<td>Distribution with calculated quantiles</td>\n<td>Similar to histogram but client-calculated percentiles</td>\n<td>Server-side aggregation across instances</td>\n</tr>\n</tbody></table>\n<h4 id=\"counter-semantics-and-behavior\">Counter Semantics and Behavior</h4>\n<p><strong>Counters</strong> represent cumulative totals that only increase over time, with the critical exception of resets to zero when the monitored process restarts. The mathematical foundation of counters enables powerful rate calculations - the derivative of a counter represents the instantaneous rate of change, which is often more meaningful than the absolute value for operational monitoring.</p>\n<p>The semantic contract of a <code>Counter</code> includes several critical guarantees that both the client instrumenting code and the query engine must respect. First, values must never decrease except during resets to zero. Second, counter resets must be detectable by the monitoring system to avoid incorrect rate calculations. Third, the rate of change is more operationally significant than the absolute value in most use cases.</p>\n<table>\n<thead>\n<tr>\n<th>Counter Operation</th>\n<th>Input</th>\n<th>Behavior</th>\n<th>Semantic Guarantee</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Inc()</code></td>\n<td>None</td>\n<td>Increment by 1</td>\n<td>Value increases monotonically</td>\n</tr>\n<tr>\n<td><code>Add(value)</code></td>\n<td>Positive number</td>\n<td>Increment by value</td>\n<td>Rejects negative values</td>\n</tr>\n<tr>\n<td><code>Reset()</code></td>\n<td>None</td>\n<td>Set to zero</td>\n<td>Marks reset boundary for rate calculations</td>\n</tr>\n<tr>\n<td><code>Value()</code></td>\n<td>None</td>\n<td>Return current total</td>\n<td>Read-only access to current state</td>\n</tr>\n</tbody></table>\n<p>Counter resets present a particular challenge for rate calculations. When a process restarts, the counter begins again from zero, creating an artificial negative spike in the rate calculation if handled naively. The storage and query engine must detect these reset conditions and handle them appropriately by treating the first sample after a reset as a new baseline rather than calculating a rate from the previous higher value.</p>\n<p>Consider a web server request counter that increments with each HTTP request. Over one hour, it increases from 1000 to 1500 requests, indicating a rate of 500 requests per hour. If the server restarts and the counter resets to zero, then increases to 100 over the next hour, the rate should be calculated as 100 requests per hour, not as -1400 requests per hour based on the naive difference from the pre-restart value.</p>\n<blockquote>\n<p>The fundamental insight with counters is that the absolute value is rarely interesting - you care about the rate of change over time. A counter showing 1,847,392 total requests tells you nothing actionable, but knowing that requests are arriving at 50 per second tells you everything about current system load.</p>\n</blockquote>\n<h4 id=\"gauge-semantics-and-flexibility\">Gauge Semantics and Flexibility</h4>\n<p><strong>Gauges</strong> represent point-in-time measurements that can fluctuate arbitrarily in either direction. Unlike counters, gauges have no mathematical constraints on their values - they can increase, decrease, or remain constant between observations. This flexibility makes them suitable for representing resource levels, percentages, temperatures, and any other measurement where the current absolute value is meaningful.</p>\n<p>The semantic contract of a <code>Gauge</code> emphasizes current state over historical accumulation. Operations focus on setting, adjusting, and observing the current value rather than accumulating changes over time. This distinction is crucial for query operations - while rate calculations on gauges are mathematically valid, they often require smoothing or windowing to be operationally useful due to the potentially noisy nature of gauge values.</p>\n<table>\n<thead>\n<tr>\n<th>Gauge Operation</th>\n<th>Input</th>\n<th>Behavior</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Set(value)</code></td>\n<td>Any number</td>\n<td>Replace current value</td>\n<td>Setting absolute measurements</td>\n</tr>\n<tr>\n<td><code>Inc()</code></td>\n<td>None</td>\n<td>Increment by 1</td>\n<td>Simple upward adjustments</td>\n</tr>\n<tr>\n<td><code>Dec()</code></td>\n<td>None</td>\n<td>Decrement by 1</td>\n<td>Simple downward adjustments</td>\n</tr>\n<tr>\n<td><code>Add(value)</code></td>\n<td>Any number (including negative)</td>\n<td>Adjust by delta</td>\n<td>Relative adjustments</td>\n</tr>\n<tr>\n<td><code>Value()</code></td>\n<td>None</td>\n<td>Return current value</td>\n<td>Current state queries</td>\n</tr>\n</tbody></table>\n<p>Gauges excel at representing system state that fluctuates around operational ranges. CPU utilization naturally varies between 0% and 100%, memory usage grows and shrinks with allocation patterns, and queue depths rise and fall with load patterns. These measurements have meaningful absolute values at any point in time, unlike counters where only the rate of change provides operational insight.</p>\n<p>Consider monitoring database connection pool usage. The gauge might show 15 active connections out of a maximum 50, providing immediate insight into resource utilization. As queries complete and new requests arrive, the gauge fluctuates, with both the current absolute value (15 connections) and the trend over time (increasing, decreasing, or stable) providing valuable operational information.</p>\n<h4 id=\"histogram-design-and-bucket-strategy\">Histogram Design and Bucket Strategy</h4>\n<p><strong>Histograms</strong> capture the distribution of observed values by maintaining counts in predefined buckets, enabling calculation of percentiles, averages, and distribution shapes without storing individual samples. This aggregation approach provides powerful statistical insights while maintaining bounded memory usage regardless of observation volume.</p>\n<p>The core design challenge with histograms lies in choosing appropriate bucket boundaries. These boundaries must be defined when the histogram is created and cannot be changed later, as the bucketing strategy affects all subsequent statistical calculations. The bucket boundaries determine the precision and range of percentile calculations - finer buckets provide higher precision but consume more storage, while coarser buckets reduce storage at the cost of statistical accuracy.</p>\n<table>\n<thead>\n<tr>\n<th>Histogram Component</th>\n<th>Purpose</th>\n<th>Storage Requirement</th>\n<th>Query Capability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bucket counters</td>\n<td>Count observations in each range</td>\n<td>One counter per bucket</td>\n<td>Percentile estimation, distribution shape</td>\n</tr>\n<tr>\n<td>Total count</td>\n<td>Count all observations</td>\n<td>One counter</td>\n<td>Sample count for averaging</td>\n</tr>\n<tr>\n<td>Total sum</td>\n<td>Sum all observed values</td>\n<td>One counter</td>\n<td>Mean calculation</td>\n</tr>\n<tr>\n<td>Bucket boundaries</td>\n<td>Define ranges for categorization</td>\n<td>Metadata only</td>\n<td>Query planning and validation</td>\n</tr>\n</tbody></table>\n<p>The histogram bucket strategy directly impacts the accuracy of percentile calculations. Consider measuring HTTP response latency with buckets at [0.1s, 0.5s, 1.0s, 2.0s, 5.0s, +Inf]. This configuration provides good resolution for typical web response times but would poorly serve a system where most responses complete in microseconds. The query engine estimates percentiles using bucket interpolation - if the 95th percentile falls within the 1.0s-2.0s bucket, the exact value is estimated based on the distribution assumption within that range.</p>\n<blockquote>\n<p><strong>Decision: Cumulative Histogram Buckets</strong></p>\n<ul>\n<li><strong>Context</strong>: Need efficient percentile calculations while supporting aggregation across multiple instances</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Individual bucket counts (each bucket independent)</li>\n<li>Cumulative bucket counts (each bucket includes all smaller values)</li>\n<li>Sparse histogram with dynamic bucket boundaries</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Cumulative bucket counts following Prometheus model</li>\n<li><strong>Rationale</strong>: Enables efficient percentile calculation via bucket search, supports mathematical aggregation across instances, simplifies query engine implementation</li>\n<li><strong>Consequences</strong>: Slightly more complex bucket increment logic, but dramatically simplified percentile queries and cross-instance aggregation</li>\n</ul>\n</blockquote>\n<p>Histogram aggregation across multiple instances provides one of the most powerful features of this metric type. Unlike gauges or counters where cross-instance aggregation requires careful consideration of meaning (average of averages vs. total counts), histogram buckets aggregate naturally - the sum of bucket counts across instances represents the combined distribution. This property enables fleet-wide latency analysis and capacity planning based on aggregate behavior patterns.</p>\n<h4 id=\"summary-metrics-and-client-side-quantiles\">Summary Metrics and Client-Side Quantiles</h4>\n<p><strong>Summaries</strong> provide an alternative approach to distribution tracking by calculating quantiles (percentiles) on the client side and transmitting pre-calculated statistical values rather than bucket counts. This approach trades some query flexibility for reduced bandwidth and storage requirements, particularly valuable in high-volume environments where network efficiency is paramount.</p>\n<p>The fundamental difference between summaries and histograms lies in where the statistical calculation occurs. Histograms preserve the raw distribution information (via bucket counts) and calculate percentiles during query execution, while summaries calculate percentiles during observation and store only the results. This trade-off has profound implications for aggregation capabilities and query flexibility.</p>\n<table>\n<thead>\n<tr>\n<th>Summary Component</th>\n<th>Purpose</th>\n<th>Client Calculation</th>\n<th>Server Storage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Quantile values</td>\n<td>Pre-calculated percentiles (e.g., 0.5, 0.9, 0.99)</td>\n<td>Sliding window quantile estimation</td>\n<td>Direct storage of calculated values</td>\n</tr>\n<tr>\n<td>Total count</td>\n<td>Count all observations</td>\n<td>Simple counter increment</td>\n<td>Counter storage and rate calculations</td>\n</tr>\n<tr>\n<td>Total sum</td>\n<td>Sum all observed values</td>\n<td>Running sum accumulation</td>\n<td>Mean calculation support</td>\n</tr>\n<tr>\n<td>Observation window</td>\n<td>Time window for quantile calculation</td>\n<td>Sliding window management</td>\n<td>Window metadata only</td>\n</tr>\n</tbody></table>\n<p>The critical limitation of summaries becomes apparent during aggregation operations. Since quantiles are non-additive mathematical functions, combining the 95th percentile from multiple instances does not yield the fleet-wide 95th percentile. If instance A reports a 95th percentile latency of 500ms and instance B reports 750ms, the combined fleet 95th percentile could be anywhere from 500ms to 750ms (or even outside this range) depending on the distribution of requests across instances.</p>\n<blockquote>\n<p>Summaries excel in bandwidth-constrained environments where you need specific quantiles from individual instances but don&#39;t require fleet-wide distribution analysis. Histograms provide superior query flexibility and aggregation capabilities at the cost of higher bandwidth and storage requirements.</p>\n</blockquote>\n<h3 id=\"multi-dimensional-labeling\">Multi-Dimensional Labeling</h3>\n<p>Think of labels as the coordinate system that transforms flat metrics into a multi-dimensional space where you can slice, dice, and aggregate data along any combination of dimensions. Just as a GPS coordinate becomes meaningful only when you know it refers to latitude and longitude, a metric value becomes operationally useful only when you know the service, environment, region, and other contextual dimensions that produced it.</p>\n<p>Labels enable the transformation from simple time series (metric_name -&gt; value over time) to multi-dimensional time series (metric_name{label1=value1, label2=value2} -&gt; value over time). This dimensionality is what makes modern monitoring systems powerful - instead of creating separate metrics for each combination of conditions, you create one metric with appropriate labels and query across dimensions.</p>\n<p>The labeling system must balance expressiveness with performance. Each unique combination of label values creates a distinct time series, directly impacting memory usage, storage requirements, and query performance. Understanding this relationship is crucial for designing sustainable monitoring instrumentation that scales with system complexity.</p>\n<h4 id=\"label-structure-and-naming-conventions\">Label Structure and Naming Conventions</h4>\n<p>Labels consist of key-value pairs where both keys and values are strings, attached to metric observations to provide dimensional context. The label key represents the dimension name (such as &quot;method&quot;, &quot;status_code&quot;, or &quot;region&quot;) while the label value represents the specific instance of that dimension (such as &quot;GET&quot;, &quot;200&quot;, or &quot;us-west-2&quot;).</p>\n<p>Label naming conventions significantly impact long-term maintainability and query ergonomics. Well-chosen label names create intuitive query patterns and support natural aggregation operations, while poor label naming leads to confusion and complex query logic. The naming strategy should reflect the operational questions you need to answer rather than the technical implementation details of how metrics are collected.</p>\n<table>\n<thead>\n<tr>\n<th>Label Category</th>\n<th>Examples</th>\n<th>Purpose</th>\n<th>Cardinality Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Service Identity</td>\n<td><code>service</code>, <code>instance</code>, <code>job</code></td>\n<td>Identify metric source</td>\n<td>Linear with service count</td>\n</tr>\n<tr>\n<td>Request Context</td>\n<td><code>method</code>, <code>status_code</code>, <code>endpoint</code></td>\n<td>Categorize individual operations</td>\n<td>Multiplicative across dimensions</td>\n</tr>\n<tr>\n<td>Infrastructure</td>\n<td><code>region</code>, <code>availability_zone</code>, <code>datacenter</code></td>\n<td>Physical deployment context</td>\n<td>Linear with infrastructure diversity</td>\n</tr>\n<tr>\n<td>Application State</td>\n<td><code>version</code>, <code>environment</code>, <code>feature_flag</code></td>\n<td>Application configuration context</td>\n<td>Linear with deployment variations</td>\n</tr>\n</tbody></table>\n<p>The hierarchical nature of many label dimensions enables powerful aggregation patterns. Consider HTTP request metrics labeled with <code>{service=&quot;api&quot;, method=&quot;GET&quot;, endpoint=&quot;/users&quot;, status_code=&quot;200&quot;}</code>. You can aggregate across all endpoints to see service-level request rates, across all status codes to see endpoint-level traffic patterns, or across all methods to analyze API endpoint popularity. Each aggregation operation reduces dimensionality while preserving the ability to drill down into specific label combinations.</p>\n<p>Label value consistency across the system requires careful coordination between instrumentation and operational practices. The same conceptual entity must use identical label values across all metrics - a service identified as &quot;user-service&quot; in one metric and &quot;userservice&quot; in another creates artificial separation in queries and dashboards. This consistency extends beyond naming to include value normalization (lowercase vs. uppercase, hyphen vs. underscore) and handling of dynamic values (user IDs, request IDs) that create unbounded cardinality.</p>\n<h4 id=\"cardinality-mathematics-and-memory-impact\">Cardinality Mathematics and Memory Impact</h4>\n<p>Label cardinality represents the number of unique time series created by all possible combinations of label values for a given metric. Understanding cardinality mathematics is essential for predicting memory usage, storage requirements, and query performance as the system scales.</p>\n<p>The cardinality of a metric equals the cartesian product of all label value sets. A metric with labels <code>{service, method, status_code}</code> where service has 10 possible values, method has 4 values, and status_code has 15 values creates a maximum cardinality of 10 × 4 × 15 = 600 unique time series. In practice, not all combinations may exist (some services might not support all methods), but the maximum provides the upper bound for capacity planning.</p>\n<table>\n<thead>\n<tr>\n<th>Cardinality Factor</th>\n<th>Low Impact (1-10 values)</th>\n<th>Medium Impact (10-100 values)</th>\n<th>High Impact (100+ values)</th>\n<th>Unbounded (avoid)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Examples</td>\n<td>HTTP methods, status code classes</td>\n<td>Status codes, service names</td>\n<td>Instance IDs, container names</td>\n<td>User IDs, request IDs, timestamps</td>\n</tr>\n<tr>\n<td>Memory per series</td>\n<td>~1KB baseline + samples</td>\n<td>~1KB baseline + samples</td>\n<td>~1KB baseline + samples</td>\n<td>Unbounded growth</td>\n</tr>\n<tr>\n<td>Query performance</td>\n<td>Minimal impact</td>\n<td>Linear degradation</td>\n<td>Significant index overhead</td>\n<td>System instability</td>\n</tr>\n<tr>\n<td>Storage growth</td>\n<td>Predictable</td>\n<td>Manageable with planning</td>\n<td>Requires careful monitoring</td>\n<td>Leads to system failure</td>\n</tr>\n</tbody></table>\n<p>The memory impact of cardinality extends beyond simple multiplication due to indexing overhead. Each unique time series requires index entries for fast lookup during queries, and these indexes must support efficient filtering across multiple label dimensions simultaneously. The storage engine maintains inverted indexes mapping label values to time series identifiers, creating memory overhead that scales with both cardinality and label diversity.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Flabel-cardinality.svg\" alt=\"Label Cardinality Impact\"></p>\n<p>Consider a real-world example: instrumenting HTTP request duration with labels for service, method, endpoint, and status_code. With 5 services, 4 HTTP methods, 20 endpoints per service, and 10 status codes, the theoretical maximum cardinality is 5 × 4 × 20 × 10 = 4,000 time series. If each time series consumes approximately 1KB of memory for metadata plus sample storage, this single metric could consume 4MB of memory just for the index structures, before considering the actual time series data.</p>\n<blockquote>\n<p><strong>Decision: Label Cardinality Limits</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to prevent unbounded memory growth while supporting necessary operational dimensions</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>No limits (trust users to instrument responsibly)</li>\n<li>Hard limits per metric (e.g., max 1000 series per metric name)</li>\n<li>Soft limits with warnings and graduated enforcement</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Soft limits with configurable enforcement thresholds</li>\n<li><strong>Rationale</strong>: Provides safety against cardinality explosion while allowing legitimate high-cardinality use cases with explicit acknowledgment</li>\n<li><strong>Consequences</strong>: Requires monitoring of cardinality growth, adds complexity to ingestion pipeline, enables sustainable scaling</li>\n</ul>\n</blockquote>\n<h4 id=\"label-best-practices-and-anti-patterns\">Label Best Practices and Anti-Patterns</h4>\n<p>Effective label design requires understanding the difference between dimensions that add operational value and those that add only noise. The goal is to create labelsets that enable meaningful aggregation and filtering operations while maintaining reasonable cardinality bounds.</p>\n<p>High-value labels represent dimensions along which you regularly need to aggregate, filter, or alert. These typically correspond to operational boundaries (services, environments, regions) or request characteristics that affect system behavior (HTTP methods, cache hit/miss status, error types). Low-value labels often represent implementation details that don&#39;t align with operational questions or create unnecessarily high cardinality without proportional insight.</p>\n<table>\n<thead>\n<tr>\n<th>Pattern Type</th>\n<th>Good Practice</th>\n<th>Anti-Pattern</th>\n<th>Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Service Identity</td>\n<td><code>{service=&quot;user-api&quot;, environment=&quot;prod&quot;}</code></td>\n<td><code>{hostname=&quot;server-17-prod-usw2.internal&quot;}</code></td>\n<td>Service focus vs. infrastructure focus</td>\n</tr>\n<tr>\n<td>Request Classification</td>\n<td><code>{method=&quot;GET&quot;, status_class=&quot;2xx&quot;}</code></td>\n<td><code>{full_url=&quot;/api/users/12345/profile&quot;}</code></td>\n<td>Bounded vs. unbounded cardinality</td>\n</tr>\n<tr>\n<td>Error Categorization</td>\n<td><code>{error_type=&quot;timeout&quot;, subsystem=&quot;database&quot;}</code></td>\n<td><code>{error_message=&quot;connection refused to 10.0.0.1:5432&quot;}</code></td>\n<td>Actionable categories vs. specific instances</td>\n</tr>\n<tr>\n<td>Version Tracking</td>\n<td><code>{version=&quot;1.2.3&quot;, deployment_id=&quot;abc123&quot;}</code></td>\n<td><code>{build_timestamp=&quot;2023-10-15T14:30:22Z&quot;}</code></td>\n<td>Discrete versions vs. continuous values</td>\n</tr>\n</tbody></table>\n<p>The temporal aspect of label values requires special consideration. Labels that change frequently create natural time series boundaries - when a label value changes, a new time series begins and the old one effectively ends. This behavior is correct and desired for legitimate operational dimensions (like application version during deployments) but problematic for high-frequency changes (like current timestamp or active user count).</p>\n<p>Common anti-patterns include using user IDs, request IDs, or timestamps as label values. These create unbounded cardinality that grows continuously with system usage rather than stabilizing at a level proportional to system complexity. Instead, these high-cardinality identifiers should be either excluded from metrics entirely or aggregated into bounded categories (e.g., user_type instead of user_id, request_size_bucket instead of request_id).</p>\n<blockquote>\n<p>The litmus test for label appropriateness is simple: &quot;Will I ever want to aggregate or filter metrics along this dimension?&quot; If you can&#39;t imagine writing a query that groups by or filters on a label, it probably shouldn&#39;t be a label.</p>\n</blockquote>\n<h3 id=\"time-series-identity\">Time Series Identity</h3>\n<p>Think of time series identity as the unique &quot;address&quot; that allows the storage and query engines to locate specific metric streams within the vast multidimensional space of all possible measurements. Just as a postal address must uniquely identify a specific building, a time series identity must uniquely identify a specific sequence of timestamped values among potentially millions of similar sequences.</p>\n<p>The time series identity model determines how the system partitions the continuous stream of metric observations into discrete, queryable sequences. This partitioning directly affects storage layout, query performance, and cardinality management. Understanding identity semantics is crucial because it defines the granularity at which the system can filter, aggregate, and analyze metric data.</p>\n<h4 id=\"identity-composition-and-uniqueness\">Identity Composition and Uniqueness</h4>\n<p>A time series identity consists of the metric name combined with the complete set of label key-value pairs. Two time series are considered identical if and only if they have the same metric name and exactly the same set of labels with exactly the same values. Any difference in metric name, label keys, or label values creates a distinct time series identity.</p>\n<p>This strict equality requirement has important implications for instrumentation consistency. A metric observation with labels <code>{service=&quot;api&quot;, method=&quot;GET&quot;}</code> creates a different time series than an observation with labels <code>{method=&quot;GET&quot;, service=&quot;api&quot;}</code> even though the label content is semantically identical - the system treats these as separate identities. Most implementations normalize label ordering to avoid this pitfall, but the fundamental principle remains: identity requires exact matching.</p>\n<table>\n<thead>\n<tr>\n<th>Identity Component</th>\n<th>Contribution</th>\n<th>Example</th>\n<th>Uniqueness Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric name</td>\n<td>Primary classification</td>\n<td><code>http_requests_total</code></td>\n<td>Separates different measurement types</td>\n</tr>\n<tr>\n<td>Label keys</td>\n<td>Dimensional structure</td>\n<td><code>method</code>, <code>status_code</code></td>\n<td>Defines available aggregation dimensions</td>\n</tr>\n<tr>\n<td>Label values</td>\n<td>Specific instances</td>\n<td><code>GET</code>, <code>200</code></td>\n<td>Creates actual time series instances</td>\n</tr>\n<tr>\n<td>Label ordering</td>\n<td>Normalized during ingestion</td>\n<td>Consistent regardless of input order</td>\n<td>Implementation detail, not semantic</td>\n</tr>\n</tbody></table>\n<p>The mathematical relationship between labels and time series count becomes clear through the identity model. Each unique combination of label values creates one time series identity. If you have a metric <code>http_requests_total{method, status_code}</code> and observe requests with methods [GET, POST] and status codes [200, 404, 500], you create 2 × 3 = 6 distinct time series identities: <code>{method=&quot;GET&quot;, status_code=&quot;200&quot;}</code>, <code>{method=&quot;GET&quot;, status_code=&quot;404&quot;}</code>, <code>{method=&quot;GET&quot;, status_code=&quot;500&quot;}</code>, <code>{method=&quot;POST&quot;, status_code=&quot;200&quot;}</code>, <code>{method=&quot;POST&quot;, status_code=&quot;404&quot;}</code>, <code>{method=&quot;POST&quot;, status_code=&quot;500&quot;}</code>.</p>\n<h4 id=\"identity-lifecycle-and-creation\">Identity Lifecycle and Creation</h4>\n<p>Time series identities come into existence dynamically as the system observes new combinations of metric names and label values. Unlike traditional databases where schema defines the available table and column structure upfront, metrics systems create new time series identities on-demand as applications emit previously unseen labelset combinations.</p>\n<p>This dynamic creation model provides tremendous flexibility for evolving systems - new services, endpoints, or operational dimensions automatically create appropriate time series without schema migration or configuration changes. However, it also creates the risk of cardinality explosion if instrumentation code generates unbounded label values or fails to normalize label naming.</p>\n<table>\n<thead>\n<tr>\n<th>Lifecycle Stage</th>\n<th>Trigger</th>\n<th>System Action</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Creation</td>\n<td>First sample with new identity</td>\n<td>Allocate storage, create index entries</td>\n<td>Memory allocation, index update overhead</td>\n</tr>\n<tr>\n<td>Active</td>\n<td>Ongoing sample ingestion</td>\n<td>Append samples to existing series</td>\n<td>Minimal per-sample overhead</td>\n</tr>\n<tr>\n<td>Inactive</td>\n<td>No samples for retention period</td>\n<td>Mark for deletion, preserve for queries</td>\n<td>Index overhead without storage growth</td>\n</tr>\n<tr>\n<td>Deletion</td>\n<td>Retention policy expiration</td>\n<td>Remove from storage and indexes</td>\n<td>Memory reclamation, index cleanup</td>\n</tr>\n</tbody></table>\n<p>The moment of time series creation represents the highest overhead in the identity lifecycle. The storage engine must allocate memory structures, create index entries mapping labels to series identifiers, and update various metadata structures to track the new series. This creation overhead motivates batching strategies where multiple samples for the same identity are grouped together during ingestion.</p>\n<p>Consider the lifecycle of a time series tracking HTTP requests for a new API endpoint. The first request to <code>/api/users</code> with method GET creates the time series identity <code>http_requests_total{method=&quot;GET&quot;, endpoint=&quot;/api/users&quot;}</code>. Subsequent requests to the same endpoint with the same method append samples to this existing time series. If the endpoint is later deprecated and receives no traffic, the time series becomes inactive but remains queryable for historical analysis until the retention policy removes it entirely.</p>\n<h4 id=\"identity-normalization-and-canonical-form\">Identity Normalization and Canonical Form</h4>\n<p>To ensure consistent identity matching across system components, the metrics system must establish a canonical form for time series identities. This normalization process converts various equivalent representations into a single, standard format that enables reliable identity comparison and lookup operations.</p>\n<p>Label ordering normalization represents the most common identity canonicalization requirement. Since labels are conceptually an unordered set of key-value pairs, the system must establish a consistent ordering (typically lexicographic by key name) to ensure that <code>{method=&quot;GET&quot;, service=&quot;api&quot;}</code> and <code>{service=&quot;api&quot;, method=&quot;GET&quot;}</code> resolve to the same canonical identity.</p>\n<table>\n<thead>\n<tr>\n<th>Normalization Type</th>\n<th>Input Variation</th>\n<th>Canonical Form</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label ordering</td>\n<td><code>{b=&quot;2&quot;, a=&quot;1&quot;}</code></td>\n<td><code>{a=&quot;1&quot;, b=&quot;2&quot;}</code></td>\n<td>Consistent identity hashing and comparison</td>\n</tr>\n<tr>\n<td>Label key casing</td>\n<td><code>{Method=&quot;GET&quot;}</code></td>\n<td><code>{method=&quot;GET&quot;}</code></td>\n<td>Case-insensitive label key matching</td>\n</tr>\n<tr>\n<td>Value whitespace</td>\n<td><code>{status=&quot; 200 &quot;}</code></td>\n<td><code>{status=&quot;200&quot;}</code></td>\n<td>Trim accidental whitespace</td>\n</tr>\n<tr>\n<td>Empty labels</td>\n<td><code>{method=&quot;GET&quot;, unused=&quot;&quot;}</code></td>\n<td><code>{method=&quot;GET&quot;}</code></td>\n<td>Remove labels with empty values</td>\n</tr>\n</tbody></table>\n<p>The canonical identity form enables efficient storage and lookup operations through consistent hashing. The storage engine can hash the canonical identity string to determine storage location, index bucket, and cache keys without worrying about equivalent representations creating different hash values. This consistency is crucial for performance as the system scales to millions of time series.</p>\n<p>Identity normalization must balance consistency with preservation of meaningful distinctions. While trimming whitespace from label values usually represents error correction, case-sensitive label values might be semantically important (distinguishing between SQL table names &quot;Users&quot; and &quot;users&quot; in case-sensitive databases). The normalization rules should reflect the operational reality of the monitored systems rather than imposing arbitrary formatting requirements.</p>\n<blockquote>\n<p><strong>Decision: Strict Identity Immutability</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to ensure consistent time series identification across storage, indexing, and query operations</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Mutable identities (allow label value changes for existing series)</li>\n<li>Immutable identities (label changes create new time series)</li>\n<li>Partial mutability (allow changes to designated &quot;mutable&quot; labels)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Strict immutability - any label change creates a new time series</li>\n<li><strong>Rationale</strong>: Simplifies storage engine design, ensures query consistency, prevents data corruption from identity conflicts</li>\n<li><strong>Consequences</strong>: Application label changes require explicit time series migration, but system behavior remains predictable and reliable</li>\n</ul>\n</blockquote>\n<h3 id=\"cardinality-control\">Cardinality Control</h3>\n<p>Think of cardinality control as the immune system of the metrics infrastructure - it protects the overall system health by identifying and containing threats to stability before they can cause widespread damage. Just as a biological immune system must distinguish between beneficial bacteria and harmful pathogens, cardinality control must differentiate between legitimate high-dimensional metrics and pathological cardinality explosions.</p>\n<p>Uncontrolled cardinality growth represents one of the most common causes of metrics system failure in production environments. A single misbehaving application or poorly designed instrumentation can generate millions of unique time series in minutes, consuming all available memory and rendering the entire monitoring system unusable. Effective cardinality control requires both preventive measures (limits and validation) and reactive measures (detection and mitigation).</p>\n<h4 id=\"cardinality-explosion-detection\">Cardinality Explosion Detection</h4>\n<p>Cardinality explosion typically manifests as rapid, unexpected growth in the number of unique time series, often accompanied by degraded query performance and memory pressure. Early detection requires monitoring the rate of new time series creation and identifying patterns that indicate problematic instrumentation rather than legitimate system growth.</p>\n<p>The challenge in explosion detection lies in distinguishing between normal system evolution (new services, features, or infrastructure) and pathological growth (unbounded label values, instrumentation bugs). Normal growth typically correlates with planned deployments or infrastructure changes and exhibits predictable patterns. Pathological growth often appears sudden, accelerating, and disproportionate to actual system complexity changes.</p>\n<table>\n<thead>\n<tr>\n<th>Detection Signal</th>\n<th>Normal Growth Pattern</th>\n<th>Explosion Pattern</th>\n<th>Response Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>New series rate</td>\n<td>Gradual, correlated with deployments</td>\n<td>Sudden spike, continuously accelerating</td>\n<td>Immediate investigation and mitigation</td>\n</tr>\n<tr>\n<td>Label value diversity</td>\n<td>Bounded growth in known dimensions</td>\n<td>New dimensions or unbounded values</td>\n<td>Label audit and validation</td>\n</tr>\n<tr>\n<td>Memory usage growth</td>\n<td>Linear with feature complexity</td>\n<td>Exponential growth unrelated to features</td>\n<td>Emergency cardinality limiting</td>\n</tr>\n<tr>\n<td>Query performance</td>\n<td>Stable or gradually degrading</td>\n<td>Rapid degradation, timeout increases</td>\n<td>Query load balancing and limiting</td>\n</tr>\n</tbody></table>\n<p>Automated detection systems should monitor both absolute cardinality levels and growth rates across multiple time horizons. A metric that creates 1000 new time series in one minute might represent normal behavior for a high-traffic service during deployment, but the same rate sustained over an hour indicates a serious problem requiring immediate intervention.</p>\n<p>The distribution of cardinality across metrics provides additional detection signals. In healthy systems, most metrics have relatively low cardinality (10-100 time series), with a few high-cardinality metrics (1000+ series) that represent well-understood, business-critical dimensions. An explosion often manifests as a single metric suddenly dominating the cardinality budget, indicating a specific instrumentation problem rather than general system growth.</p>\n<h4 id=\"enforcement-strategies-and-policies\">Enforcement Strategies and Policies</h4>\n<p>Effective cardinality enforcement requires a graduated response system that can provide early warnings, impose soft limits during normal operation, and implement hard limits during emergency conditions. This approach balances the need for operational safety with the flexibility to support legitimate high-cardinality use cases when properly justified and monitored.</p>\n<p>Soft enforcement mechanisms focus on visibility and warnings rather than blocking operations. These approaches work well during normal operations when development teams can respond to notifications and adjust instrumentation practices. Hard enforcement mechanisms prioritize system stability over metric completeness, appropriate during crisis situations where the monitoring system&#39;s survival takes precedence over comprehensive data collection.</p>\n<table>\n<thead>\n<tr>\n<th>Enforcement Level</th>\n<th>Trigger Conditions</th>\n<th>Actions Taken</th>\n<th>Operational Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Warning</td>\n<td>50% of cardinality budget used</td>\n<td>Log warnings, send alerts</td>\n<td>No impact on data collection</td>\n</tr>\n<tr>\n<td>Soft limiting</td>\n<td>80% of cardinality budget used</td>\n<td>Rate limit new series creation</td>\n<td>Delayed ingestion, potential data loss</td>\n</tr>\n<tr>\n<td>Hard limiting</td>\n<td>95% of cardinality budget used</td>\n<td>Reject new series, shed existing series</td>\n<td>Guaranteed data loss, preserved system stability</td>\n</tr>\n<tr>\n<td>Emergency mode</td>\n<td>Memory pressure or system instability</td>\n<td>Aggressive series eviction, ingestion throttling</td>\n<td>Significant data loss, system preservation</td>\n</tr>\n</tbody></table>\n<p>The enforcement policy must define clear cardinality budgets and allocation strategies across different metric types and operational contexts. Production services might receive larger cardinality budgets than development environments, and business-critical metrics might be protected from enforcement actions that could affect their availability during incidents.</p>\n<p>Sample-based enforcement provides a middle ground between complete rejection and unlimited acceptance. When soft limits are exceeded, the system can randomly sample new time series creation, preserving statistical representativeness while controlling absolute cardinality growth. This approach works particularly well for metrics where complete enumeration isn&#39;t required for operational insight.</p>\n<blockquote>\n<p><strong>Decision: Hierarchical Cardinality Budgets</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to prevent system-wide cardinality explosion while allowing different services and metrics to have different cardinality requirements</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Global cardinality limit (single system-wide limit)</li>\n<li>Per-metric cardinality limits (each metric has independent limit)</li>\n<li>Hierarchical budgets (service -&gt; metric -&gt; label dimension limits)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hierarchical budgets with inheritance and override capabilities</li>\n<li><strong>Rationale</strong>: Provides granular control while enabling reasonable defaults, supports organizational responsibility boundaries, enables emergency override for critical metrics</li>\n<li><strong>Consequences</strong>: More complex configuration and monitoring, but much better operational control and blast radius limitation</li>\n</ul>\n</blockquote>\n<h4 id=\"label-value-validation-and-sanitization\">Label Value Validation and Sanitization</h4>\n<p>Proactive label value validation provides the first line of defense against cardinality explosion by identifying and rejecting problematic label values before they create persistent time series. Effective validation requires understanding common patterns that lead to unbounded cardinality and implementing sanitization rules that preserve operational meaning while enforcing cardinality bounds.</p>\n<p>High-risk label patterns include sequential identifiers (user IDs, request IDs, timestamps), unbounded categorical values (error messages, URLs with parameters), and accidentally dynamic values (configuration changes, memory addresses). Validation rules should detect these patterns and either reject the labels entirely or transform them into bounded categories.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Rule</th>\n<th>Pattern Detected</th>\n<th>Action Taken</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential numbers</td>\n<td>Label values matching <code>/^\\d+$/</code></td>\n<td>Reject or convert to ranges</td>\n<td>user_id=&quot;12345&quot; → user_type=&quot;premium&quot;</td>\n</tr>\n<tr>\n<td>Timestamp values</td>\n<td>ISO timestamp patterns</td>\n<td>Extract time component</td>\n<td>timestamp=&quot;2023-10-15T14:30:22Z&quot; → hour=&quot;14&quot;</td>\n</tr>\n<tr>\n<td>URL paths</td>\n<td>HTTP URL patterns with parameters</td>\n<td>Extract endpoint pattern</td>\n<td>path=&quot;/users/123/profile&quot; → endpoint=&quot;/users/{id}/profile&quot;</td>\n</tr>\n<tr>\n<td>Error messages</td>\n<td>Long strings with variable content</td>\n<td>Extract error category</td>\n<td>message=&quot;Connection failed to 10.0.0.1&quot; → error_type=&quot;connection_failed&quot;</td>\n</tr>\n<tr>\n<td>Excessive length</td>\n<td>Label values over N characters</td>\n<td>Truncate or reject</td>\n<td>Very long values usually indicate misuse</td>\n</tr>\n</tbody></table>\n<p>Label value allowlists and denylists provide explicit control over acceptable values for high-risk dimensions. Critical operational labels like service names, environments, or regions benefit from explicit allowlists that prevent typos and unauthorized values from creating unexpected cardinality. Dynamic labels that are known to be problematic can be explicitly blocked through denylists.</p>\n<p>The sanitization process must balance data preservation with cardinality control. Overly aggressive sanitization can remove legitimate operational dimensions, reducing the system&#39;s monitoring effectiveness. Conversely, insufficient sanitization allows cardinality explosions that threaten system stability. The validation rules should be tunable based on operational experience and regularly reviewed as instrumentation practices evolve.</p>\n<h4 id=\"memory-management-and-series-eviction\">Memory Management and Series Eviction</h4>\n<p>When cardinality control measures fail to prevent memory pressure, the system must implement series eviction strategies that preserve the most operationally valuable time series while reclaiming memory from less critical ones. Effective eviction requires understanding the relative importance of different time series and implementing policies that maintain monitoring effectiveness during resource constraints.</p>\n<p>Eviction strategies must consider both recency and operational importance when selecting series for removal. Recently active time series are more likely to be relevant for current operational questions, but historical data for critical business metrics may be more valuable than recent data for debugging metrics. The eviction policy should reflect organizational priorities and monitoring use cases.</p>\n<table>\n<thead>\n<tr>\n<th>Eviction Strategy</th>\n<th>Selection Criteria</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Least Recently Used (LRU)</td>\n<td>Time since last sample</td>\n<td>Simple to implement, preserves active series</td>\n<td>May evict important historical data</td>\n</tr>\n<tr>\n<td>Least Frequently Used (LFU)</td>\n<td>Sample count over time window</td>\n<td>Preserves high-volume series</td>\n<td>Complex tracking, biased toward noisy metrics</td>\n</tr>\n<tr>\n<td>Priority-based</td>\n<td>Explicit priority labels or metric patterns</td>\n<td>Aligns with business priorities</td>\n<td>Requires manual priority assignment</td>\n</tr>\n<tr>\n<td>Random eviction</td>\n<td>Random selection among candidates</td>\n<td>Unbiased, statistically representative</td>\n<td>May evict critical series by chance</td>\n</tr>\n</tbody></table>\n<p>The eviction process should be gradual and observable to prevent sudden monitoring capability loss during incidents. Aggressive eviction during a production outage could remove exactly the time series needed for root cause analysis, creating a double failure where the monitoring system fails simultaneously with the monitored system.</p>\n<p>Memory management extends beyond series eviction to include sample retention policies within individual time series. High-cardinality metrics might benefit from shorter sample retention periods, preserving the ability to create new time series while limiting the historical depth available for queries. This approach maintains monitoring coverage while managing memory growth over time.</p>\n<blockquote>\n<p>⚠️ <strong>Pitfall: Emergency Eviction During Incidents</strong>\nDuring production incidents, memory pressure often triggers aggressive eviction exactly when monitoring data is most critical. Design eviction policies to preserve incident-relevant metrics (error rates, latency, resource usage) even under extreme memory pressure. Consider pre-defining &quot;incident mode&quot; eviction policies that protect critical operational metrics at the expense of development or experimental metrics.</p>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: User ID as Labels</strong>\nAdding user IDs, request IDs, or other unbounded identifiers as metric labels creates unlimited cardinality that grows continuously with system usage. A metric labeled with <code>{user_id=&quot;user_12345&quot;}</code> creates one time series per user, potentially millions of series that consume memory and degrade query performance. Instead, use bounded categorical labels like <code>{user_type=&quot;premium&quot;, region=&quot;us-west&quot;}</code> that provide operational insight without explosive cardinality growth.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Label Naming</strong>\nUsing different label names or values for the same conceptual entity across metrics creates artificial separation during queries and aggregation. If one metric uses <code>{service=&quot;api&quot;}</code> while another uses <code>{svc=&quot;api&quot;}</code> or <code>{service=&quot;API&quot;}</code>, queries cannot correlate the metrics without complex label transformation. Establish and enforce consistent label naming conventions across all instrumentation to enable natural metric correlation and aggregation.</p>\n<p>⚠️ <strong>Pitfall: Counter Reset Handling</strong>\nFailing to properly handle counter resets leads to incorrect rate calculations that show impossible negative rates or dramatic spikes. When a monitored process restarts, counters reset to zero, but naive rate calculations compare the new zero value against the previous higher value. Implement counter reset detection in the query engine to treat post-reset samples as new baselines rather than continuing from pre-reset values.</p>\n<p>⚠️ <strong>Pitfall: Histogram Bucket Changes</strong>\nChanging histogram bucket boundaries after deployment creates inconsistent percentile calculations and breaks historical trend analysis. If you initially configure latency buckets as [0.1s, 0.5s, 1.0s] but later realize you need finer resolution and change to [0.05s, 0.1s, 0.2s, 0.5s, 1.0s], the old and new data become incomparable. Plan histogram bucket boundaries carefully based on expected data distributions and avoid changes that break historical continuity.</p>\n<p>⚠️ <strong>Pitfall: High-Cardinality Error Messages</strong>\nUsing complete error messages as label values creates unbounded cardinality since error messages often contain variable information like timestamps, IDs, or network addresses. Instead of <code>{error=&quot;Connection timeout to server-17 at 14:32:15&quot;}</code>, use categorized error types like <code>{error_type=&quot;connection_timeout&quot;, subsystem=&quot;database&quot;}</code> that provide actionable operational insight without cardinality explosion.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides concrete Go code structures and examples to transform the design concepts into working software. The focus is on creating type-safe, efficient implementations that enforce the semantic guarantees described in the design while providing clear interfaces for the scraping and query engines.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric Storage</td>\n<td>In-memory maps with mutex protection</td>\n<td>Lock-free concurrent data structures</td>\n</tr>\n<tr>\n<td>Label Validation</td>\n<td>Regex-based pattern matching</td>\n<td>Compiled finite state automata</td>\n</tr>\n<tr>\n<td>Identity Hashing</td>\n<td>Standard library SHA-256</td>\n<td>xxhash or similar fast hash functions</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Go garbage collector with manual monitoring</td>\n<td>Custom memory pools with explicit lifecycle</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON for simplicity</td>\n<td>Protocol Buffers for efficiency</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/metrics/\n  types.go              ← Core metric type definitions\n  labels.go             ← Label handling and validation\n  identity.go           ← Time series identity management\n  cardinality.go        ← Cardinality control and enforcement\n  registry.go           ← Metric registration and lookup\n  types_test.go         ← Comprehensive type behavior tests\n  examples/\n    instrumentation.go  ← Example usage patterns</code></pre></div>\n\n<h4 id=\"core-metric-type-infrastructure\">Core Metric Type Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package metrics provides the foundational data model for time series metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// with support for counters, gauges, histograms and multi-dimensional labeling.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> metrics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Sample represents a timestamped value in a time series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Sample</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels represents the multi-dimensional label set that identifies a time series.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels are stored as a sorted slice of key-value pairs for efficient comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// and consistent iteration order.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">LabelPair</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LabelPair</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String returns the canonical string representation of the labelset</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ls </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Sort labels by name to ensure consistent representation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Format as {name1=\"value1\", name2=\"value2\"} </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Escape quotes and backslashes in values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return empty string for empty labelset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Hash returns a consistent hash of the labelset for use in maps and indexing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ls </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Hash</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create canonical string representation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Use a fast hash function (xxhash recommended)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Ensure consistent hash values for equivalent labelsets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Counter represents a monotonically increasing metric that only goes up</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Counter</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add creation timestamp for reset detection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Inc increments the counter by 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Counter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Inc</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Increment value by 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Add increments the counter by the given value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Counter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate value is non-negative</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Acquire write lock  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add value to current total</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Release lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return error for negative values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Value returns the current counter value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Counter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Read current value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release lock and return value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Gauge represents a metric that can go up or down arbitrarily</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Gauge</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Set sets the gauge to the given value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Gauge</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set value to provided input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Inc increments the gauge by 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Gauge</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Inc</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Increment value by 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Dec decrements the gauge by 1  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Gauge</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Dec</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Decrement value by 1</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Add adds the given value to the gauge (can be negative)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Gauge</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add value to current gauge value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Value returns the current gauge value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Gauge</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Value</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Read current value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release lock and return value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Histogram tracks the distribution of observations in predefined buckets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Histogram</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buckets []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\"> // Bucket upper bounds (sorted)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    counts  []</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#6A737D\">  // Observation counts per bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sum     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#6A737D\">   // Sum of all observed values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    count   </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#6A737D\">    // Total number of observations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHistogram creates a histogram with the specified bucket boundaries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHistogram</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">buckets</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Histogram</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate buckets are sorted and finite</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add +Inf bucket if not present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize count slice to match bucket count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return configured histogram</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Observe records an observation in the appropriate bucket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Histogram</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Observe</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Find appropriate bucket using binary search</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Increment bucket count and total count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add value to running sum</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Release lock</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetBuckets returns the current bucket counts and boundaries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Histogram</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetBuckets</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">, []</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Copy bucket boundaries and counts to avoid race conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Release lock and return copies</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"label-management-and-validation\">Label Management and Validation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// LabelValidator enforces cardinality control and naming conventions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LabelValidator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxLabelLength    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxValueLength    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allowedPatterns   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Regexp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    forbiddenPatterns []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Regexp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLabelValidator creates a validator with default rules</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLabelValidator</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LabelValidator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize with reasonable defaults (label length &#x3C; 64, value length &#x3C; 256)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compile common forbidden patterns (sequential IDs, timestamps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set up standard allowed patterns for common label names</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return configured validator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateLabels checks a labelset for cardinality and naming violations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lv </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LabelValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateLabels</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check total number of labels against limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate each label name and value length</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check for forbidden patterns (user IDs, timestamps, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify label names match allowed character sets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return descriptive errors for violations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SanitizeLabels attempts to fix common labeling mistakes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">lv </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LabelValidator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SanitizeLabels</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Trim whitespace from names and values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Convert label names to lowercase</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Remove empty label values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Truncate overly long values with warning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Sort labels for canonical form</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CardinalityTracker monitors time series creation and enforces limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CardinalityTracker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seriesCounts    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">  // metric name -> series count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    totalSeries     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxTotalSeries  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxPerMetric    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    warningCallback </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#6A737D\">// metric, current, limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordSeries notifies the tracker of a new time series creation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ct </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CardinalityTracker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metricName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check against per-metric and total limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Increment appropriate counters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Trigger warnings if thresholds exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return error if hard limits violated</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Checkpoint 1: Basic Metric Types</strong>\nRun <code>go test ./internal/metrics/types_test.go</code> to verify:</p>\n<ul>\n<li>Counter increments correctly and rejects negative values</li>\n<li>Gauge accepts arbitrary values and supports increment/decrement operations</li>\n<li>Histogram correctly categorizes observations into buckets</li>\n<li>All operations are thread-safe under concurrent access</li>\n</ul>\n<p>Expected test output:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>=== RUN TestCounterIncrement\n--- PASS: TestCounterIncrement (0.00s)\n=== RUN TestGaugeOperations  \n--- PASS: TestGaugeOperations (0.00s)\n=== RUN TestHistogramBuckets\n--- PASS: TestHistogramBuckets (0.01s)</code></pre></div>\n\n<p><strong>Checkpoint 2: Label Handling</strong>\nCreate a simple program that demonstrates label cardinality:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create metrics with different label combinations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Verify that {service=\"api\", method=\"GET\"} and {method=\"GET\", service=\"api\"} </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // create the same time series identity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Show cardinality explosion with a loop creating user_id labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>You should see consistent identity hashing regardless of label order, and cardinality tracking should detect rapid series creation.</p>\n<p><strong>Checkpoint 3: Validation and Limits</strong>\nTest cardinality enforcement by instrumenting a metric with rapidly changing label values:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/metrics</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> 'metric_name=test_metric&#x26;user_id=12345&#x26;timestamp=2023-10-15T14:30:22Z'</span></span></code></pre></div>\n\n<p>The system should reject this request with a cardinality violation error, demonstrating effective protection against unbounded label values.</p>\n<h2 id=\"scrape-engine-design\">Scrape Engine Design</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section directly corresponds to Milestone 2 (Scrape Engine) and provides the HTTP-based metrics collection system that feeds data into the storage layer from Milestone 3.</p>\n</blockquote>\n<h3 id=\"the-observatory-network-mental-model\">The Observatory Network Mental Model</h3>\n<p>Before diving into the technical details of scraping, consider how the National Weather Service operates thousands of weather stations across a country. Each station is a <strong>scrape target</strong> that measures temperature, humidity, and wind speed at regular intervals. The central weather service doesn&#39;t wait for stations to call in—instead, it actively contacts each station every hour to collect readings. Some stations are permanently configured (static discovery), while others are mobile units that register and deregister dynamically (service discovery). When a station goes offline due to equipment failure or network issues, the central service marks it as unavailable but continues attempting to reconnect. This is exactly how our scrape engine works: it maintains a registry of metric-producing targets, actively pulls data from each one on a schedule, and gracefully handles failures while preserving the overall collection process.</p>\n<p>The scrape engine serves as the <strong>data ingestion coordinator</strong> that bridges the gap between distributed services exposing metrics and our centralized time series storage. Unlike push-based systems where applications actively send metrics to a collector, our pull-based approach gives the metrics system complete control over collection timing, failure handling, and resource management. This architectural choice provides several key advantages: the scrape engine can implement sophisticated retry logic without overwhelming targets, it can discover new targets automatically through service discovery, and it can apply consistent labeling and metadata enrichment across all collected metrics.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fscrape-sequence.svg\" alt=\"Scrape Operation Sequence\"></p>\n<p>The scrape engine operates through four tightly coordinated subsystems that work together to provide reliable metrics collection. The <strong>target discovery system</strong> maintains an up-to-date registry of all metric endpoints, automatically adding newly deployed services and removing decommissioned ones. The <strong>scrape scheduler</strong> manages the timing and concurrency of collection operations, ensuring each target is scraped at its configured interval without overwhelming either the scrape engine or the target services. The <strong>metrics parser</strong> handles the complex task of converting HTTP response bodies in Prometheus exposition format into structured time series samples that can be stored efficiently. Finally, the <strong>health management system</strong> tracks the availability of each target, implements retry logic for transient failures, and provides observability into the scraping process itself.</p>\n<blockquote>\n<p>The critical insight for pull-based scraping is that the metrics system becomes the <strong>authoritative source of timing</strong>. Unlike push-based systems where applications control when metrics are sent, our scrape engine determines exactly when each measurement is taken. This provides much stronger guarantees about data consistency and collection reliability.</p>\n</blockquote>\n<h3 id=\"target-discovery\">Target Discovery</h3>\n<p>Target discovery solves the fundamental question of &quot;which endpoints should I scrape for metrics?&quot; In modern distributed systems, services are constantly being deployed, scaled, and decommissioned across multiple hosts and containers. Static configuration files become outdated quickly and create operational overhead. Our target discovery system supports both static configuration for stable infrastructure and dynamic service discovery for ephemeral workloads.</p>\n<blockquote>\n<p><strong>Decision: Hybrid Discovery Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Modern deployments mix stable infrastructure (databases, load balancers) with dynamic workloads (microservices, containers). Pure static configuration requires manual updates, while pure dynamic discovery lacks control over stable services.</li>\n<li><strong>Options Considered</strong>: Static-only configuration, dynamic-only service discovery, hybrid approach supporting both</li>\n<li><strong>Decision</strong>: Implement hybrid discovery supporting both static targets and pluggable service discovery backends</li>\n<li><strong>Rationale</strong>: Static configuration provides explicit control and reliability for infrastructure components, while dynamic discovery automatically handles ephemeral services without operational overhead</li>\n<li><strong>Consequences</strong>: Increased complexity in target management but operational flexibility for mixed environments</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Discovery Method</th>\n<th>Use Cases</th>\n<th>Update Frequency</th>\n<th>Configuration Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Static Configuration</td>\n<td>Databases, load balancers, core infrastructure</td>\n<td>Manual updates only</td>\n<td>Low - direct endpoint lists</td>\n</tr>\n<tr>\n<td>DNS Service Discovery</td>\n<td>Services with stable DNS names</td>\n<td>DNS TTL intervals</td>\n<td>Medium - DNS queries and caching</td>\n</tr>\n<tr>\n<td>Kubernetes Service Discovery</td>\n<td>Containerized microservices</td>\n<td>Real-time via API</td>\n<td>High - API authentication and filtering</td>\n</tr>\n</tbody></table>\n<p>The <strong>static configuration system</strong> reads target lists from YAML files that specify exact endpoints, scrape intervals, and additional labels to attach to all metrics from each target. This approach works well for infrastructure components that have predictable network addresses and don&#39;t frequently change. The configuration supports grouping targets with similar characteristics and applying common labels that help identify the service, environment, or datacenter in query results.</p>\n<p>Static configuration follows this structure for maximum flexibility:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>targets</code></td>\n<td>[]string</td>\n<td>List of host:port endpoints to scrape</td>\n</tr>\n<tr>\n<td><code>labels</code></td>\n<td>map[string]string</td>\n<td>Additional labels attached to all metrics from these targets</td>\n</tr>\n<tr>\n<td><code>scrape_interval</code></td>\n<td>duration</td>\n<td>How frequently to collect metrics (defaults to system setting)</td>\n</tr>\n<tr>\n<td><code>scrape_timeout</code></td>\n<td>duration</td>\n<td>Maximum time to wait for HTTP response</td>\n</tr>\n<tr>\n<td><code>metrics_path</code></td>\n<td>string</td>\n<td>HTTP path for metrics endpoint (default: /metrics)</td>\n</tr>\n<tr>\n<td><code>scheme</code></td>\n<td>string</td>\n<td>HTTP or HTTPS protocol (default: http)</td>\n</tr>\n</tbody></table>\n<p>The <strong>dynamic service discovery system</strong> integrates with external service registries to automatically detect new targets and remove decommissioned ones. Each service discovery backend runs as an independent goroutine that maintains its own view of available targets and publishes changes through a unified target update interface. This design allows multiple discovery mechanisms to operate simultaneously—for example, DNS discovery for legacy services and Kubernetes API discovery for containerized workloads.</p>\n<p>Service discovery implementations must satisfy the <code>TargetDiscoverer</code> interface:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Discover</code></td>\n<td>ctx context.Context</td>\n<td>&lt;-chan []*Target</td>\n<td>Returns channel streaming target updates</td>\n</tr>\n<tr>\n<td><code>Stop</code></td>\n<td>none</td>\n<td>none</td>\n<td>Gracefully shuts down discovery process</td>\n</tr>\n</tbody></table>\n<p><strong>DNS-based service discovery</strong> queries SRV records to find service endpoints automatically. Many service mesh and load balancer systems publish SRV records that contain both the service port and priority information. The DNS discoverer performs periodic queries based on configurable intervals and TTL values, automatically adding new instances when they appear in DNS and removing them when they&#39;re no longer returned. This approach works particularly well for services that register themselves in DNS or are managed by orchestration systems that update DNS records.</p>\n<p><strong>Kubernetes service discovery</strong> uses the Kubernetes API to watch for pod and service changes in real-time. The discoverer connects to the Kubernetes API server and establishes watch streams for pods with specific annotations or labels that indicate they expose metrics. When new pods are scheduled or existing pods terminate, the API server immediately pushes updates through the watch stream. This provides much faster target updates compared to polling-based approaches and reduces the delay between service deployment and metrics collection.</p>\n<p>The Kubernetes discoverer supports sophisticated <strong>target filtering</strong> through label selectors and namespace restrictions:</p>\n<table>\n<thead>\n<tr>\n<th>Filter Type</th>\n<th>Configuration</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label Selector</td>\n<td><code>kubernetes_sd_configs.selector</code></td>\n<td>Only discover pods/services matching specific labels</td>\n</tr>\n<tr>\n<td>Namespace Filter</td>\n<td><code>kubernetes_sd_configs.namespaces</code></td>\n<td>Restrict discovery to specific Kubernetes namespaces</td>\n</tr>\n<tr>\n<td>Port Filter</td>\n<td><code>kubernetes_sd_configs.port_name</code></td>\n<td>Select specific named ports from multi-port pods</td>\n</tr>\n<tr>\n<td>Annotation Requirements</td>\n<td>Custom annotations</td>\n<td>Require specific annotations to enable scraping</td>\n</tr>\n</tbody></table>\n<p>All discovered targets flow through a <strong>target consolidation process</strong> that merges static and dynamic sources into a unified target registry. The consolidator handles conflicts when the same endpoint appears in multiple sources, applies target-specific configuration overrides, and maintains a consistent view of active targets across the entire scrape engine. This process runs continuously, updating the active target list whenever any discovery source reports changes.</p>\n<p>⚠️ <strong>Pitfall: Target Flapping</strong>\nWhen service discovery systems report rapid add/remove cycles for the same endpoint (often due to health check failures or network issues), the scrape engine can waste resources constantly starting and stopping scrape goroutines. The consolidator should implement <strong>target stability windows</strong> that require a target to remain stable for a minimum duration before activating scraping. This prevents resource thrashing while still responding quickly to legitimate topology changes.</p>\n<h3 id=\"scrape-scheduling\">Scrape Scheduling</h3>\n<p>The scrape scheduler orchestrates the complex task of collecting metrics from potentially thousands of targets simultaneously while respecting individual scrape intervals, timeouts, and resource constraints. Unlike simple cron-style scheduling, metrics scraping requires <strong>adaptive scheduling</strong> that can handle varying response times, target failures, and system load while maintaining consistent collection intervals for accurate time series analysis.</p>\n<blockquote>\n<p>Think of the scrape scheduler as an <strong>air traffic control system</strong> managing hundreds of flights (scrape operations) that must take off (start) at precise times, follow specific routes (HTTP collection), and land (complete) within strict deadlines (timeouts). Just as air traffic control prevents collisions and manages delays, the scrape scheduler prevents resource conflicts and manages scrape timing to optimize both accuracy and system performance.</p>\n</blockquote>\n<p>The core scheduling challenge is <strong>interval drift prevention</strong>. If a target is configured for 15-second scrapes but the HTTP request takes 2 seconds to complete, the next scrape should start 15 seconds after the previous scrape began, not 15 seconds after it completed. This ensures consistent sampling intervals that preserve the mathematical properties required for rate calculations and trend analysis. Naive implementations that wait for completion before scheduling the next scrape gradually drift away from their intended intervals.</p>\n<blockquote>\n<p><strong>Decision: Per-Target Goroutine Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to scrape hundreds of targets concurrently while maintaining precise intervals and independent timeout handling for each target</li>\n<li><strong>Options Considered</strong>: Global worker pool with shared queue, per-target goroutines, hybrid approach with target groups</li>\n<li><strong>Decision</strong>: Dedicate one goroutine per active target for independent scheduling and lifecycle management</li>\n<li><strong>Rationale</strong>: Goroutines are lightweight (8KB stack), provide natural isolation for timeouts and cancellation, and eliminate complex queue management logic</li>\n<li><strong>Consequences</strong>: Higher memory usage with many targets but much simpler concurrent programming model and better isolation</li>\n</ul>\n</blockquote>\n<p>Each target gets its own <strong>scrape goroutine</strong> that manages the complete lifecycle of that target&#39;s metric collection. The goroutine maintains a timer for the next scrape interval, handles HTTP requests with proper timeout context, parses the response, and forwards samples to storage. When a target is removed from discovery, its goroutine receives a cancellation signal and terminates cleanly. This model provides excellent isolation—a hanging HTTP request to one target cannot block scraping of other targets.</p>\n<p>The per-target scheduling algorithm follows this precise sequence:</p>\n<ol>\n<li><strong>Initialize interval timer</strong> using the target&#39;s configured scrape interval (e.g., 15 seconds)</li>\n<li><strong>Wait for timer expiration</strong> or cancellation signal from target discovery updates  </li>\n<li><strong>Record scrape start time</strong> to maintain consistent interval timing regardless of request duration</li>\n<li><strong>Create HTTP request context</strong> with scrape timeout deadline (e.g., 10 seconds maximum)</li>\n<li><strong>Execute HTTP GET request</strong> to the target&#39;s metrics endpoint with timeout context</li>\n<li><strong>Parse response body</strong> into time series samples if HTTP request succeeds</li>\n<li><strong>Forward samples to storage</strong> with additional target labels and scrape timestamp</li>\n<li><strong>Update target health metrics</strong> based on success/failure outcome and response time</li>\n<li><strong>Calculate next scrape time</strong> by adding interval to start time (not completion time)</li>\n<li><strong>Reset interval timer</strong> to maintain consistent scheduling and repeat the cycle</li>\n</ol>\n<p><strong>Timeout handling</strong> deserves special attention because it directly impacts both data quality and resource utilization. Each scrape operation runs within a context that automatically cancels after the configured timeout period. When cancellation occurs, the HTTP client immediately closes the connection and returns an error. The scrape goroutine records this as a timeout failure, updates the target&#39;s health status, and continues with its normal scheduling cycle. Importantly, timeouts do not delay subsequent scrapes—the next scrape timer is based on the start time, not the timeout completion time.</p>\n<table>\n<thead>\n<tr>\n<th>Timeout Scenario</th>\n<th>Behavior</th>\n<th>Next Scrape Timing</th>\n<th>Health Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request completes in 2s</td>\n<td>Normal processing, forward samples to storage</td>\n<td>15s after start time</td>\n<td>Mark target healthy</td>\n</tr>\n<tr>\n<td>Request times out after 10s</td>\n<td>Cancel HTTP context, record timeout error</td>\n<td>15s after start time</td>\n<td>Mark target unhealthy</td>\n</tr>\n<tr>\n<td>Target unreachable</td>\n<td>Immediate connection error</td>\n<td>15s after start time</td>\n<td>Mark target unreachable</td>\n</tr>\n<tr>\n<td>Invalid metrics format</td>\n<td>HTTP succeeds but parsing fails</td>\n<td>15s after start time</td>\n<td>Mark target returning bad data</td>\n</tr>\n</tbody></table>\n<p><strong>Concurrency control</strong> prevents the scrape engine from overwhelming either itself or target services with too many simultaneous requests. The scheduler implements several layers of protection: a global semaphore limits total concurrent scrapes across all targets, per-target state tracking prevents multiple simultaneous scrapes of the same endpoint, and adaptive backoff reduces scrape frequency for consistently failing targets.</p>\n<p>The global concurrency limiter uses a <strong>weighted semaphore</strong> that accounts for the expected resource cost of different types of scrapes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Max Concurrent Scrapes = min(\n    configured_max_concurrent,\n    available_memory / avg_scrape_memory,\n    network_connections / 2  // leave headroom for other operations\n)</code></pre></div>\n\n<p><strong>Adaptive backoff</strong> helps manage failing targets without completely abandoning them. When a target fails multiple consecutive scrapes, the scheduler gradually increases the time between scrape attempts while still respecting the configured minimum interval. This reduces resource waste on broken targets while ensuring they&#39;re automatically rediscovered when they recover.</p>\n<p>The backoff algorithm follows this progression:</p>\n<ol>\n<li><strong>First failure</strong>: Continue normal interval (temporary network glitch)</li>\n<li><strong>Second consecutive failure</strong>: Add 10% jitter to interval (reduce thundering herd)</li>\n<li><strong>Third consecutive failure</strong>: Double the interval up to maximum backoff limit</li>\n<li><strong>Continued failures</strong>: Maintain maximum interval with exponential decay attempts</li>\n<li><strong>First success</strong>: Immediately return to normal configured interval</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Scrape Time Drift</strong>\nA common mistake is calculating the next scrape time as <code>time.Now() + interval</code> instead of <code>scrape_start_time + interval</code>. This causes gradual drift where scrapes happen later and later over time as request processing duration accumulates. Always base the next scrape time on when the current scrape started, not when it completed. This maintains consistent sampling intervals essential for accurate rate calculations.</p>\n<h3 id=\"metrics-parsing\">Metrics Parsing</h3>\n<p>The metrics parsing subsystem transforms HTTP response bodies in Prometheus exposition format into structured <code>Sample</code> objects that can be efficiently stored and queried. This involves lexical analysis of text-based metric data, validation of metric names and label formats, type inference, and timestamp assignment. The parser must handle malformed data gracefully while preserving as much valid information as possible from each scrape response.</p>\n<blockquote>\n<p>Think of metrics parsing like <strong>translating documents</strong> from one language (text exposition format) to another (structured time series data). A good translator preserves the original meaning while adapting to the target language&#39;s grammar rules. Similarly, our parser preserves metric semantics while converting to our internal data structures. When encountering unclear passages (malformed metrics), a translator makes the best interpretation possible and continues rather than abandoning the entire document.</p>\n</blockquote>\n<p>Prometheus exposition format uses a simple text-based protocol that balances human readability with parsing efficiency. Each metric family begins with optional <code># HELP</code> and <code># TYPE</code> comments that provide metadata, followed by one or more sample lines containing the metric name, optional labels, value, and optional timestamp. The parser must handle this streaming format incrementally since response bodies can contain thousands of metrics from complex applications.</p>\n<p>A typical exposition format response looks like this structure:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code># HELP http_requests_total Total number of HTTP requests\n# TYPE http_requests_total counter\nhttp_requests_total{method=&quot;GET&quot;,status=&quot;200&quot;} 1234 1609459200000\nhttp_requests_total{method=&quot;POST&quot;,status=&quot;201&quot;} 56 1609459200000\n\n# HELP process_cpu_seconds_total Total user and system CPU seconds\n# TYPE process_cpu_seconds_total counter\nprocess_cpu_seconds_total 123.45</code></pre></div>\n\n<p>The <strong>lexical analyzer</strong> processes the input stream character by character, identifying tokens like metric names, label keys, label values, numeric values, and timestamps. This component must handle several parsing challenges: quoted label values may contain escaped characters, numeric values can be integers, floats, or special values like <code>+Inf</code> and <code>NaN</code>, and timestamp values are optional Unix milliseconds that default to scrape time when absent.</p>\n<table>\n<thead>\n<tr>\n<th>Token Type</th>\n<th>Pattern</th>\n<th>Examples</th>\n<th>Special Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric Name</td>\n<td><code>[a-zA-Z_:][a-zA-Z0-9_:]*</code></td>\n<td><code>http_requests_total</code>, <code>cpu:usage_rate</code></td>\n<td>Must start with letter, underscore, or colon</td>\n</tr>\n<tr>\n<td>Label Key</td>\n<td><code>[a-zA-Z_][a-zA-Z0-9_]*</code></td>\n<td><code>method</code>, <code>status_code</code></td>\n<td>Cannot start with <code>__</code> (reserved prefix)</td>\n</tr>\n<tr>\n<td>Label Value</td>\n<td><code>&quot;...&quot;</code> with escape sequences</td>\n<td><code>&quot;GET&quot;</code>, <code>&quot;HTTP/1.1&quot;</code></td>\n<td>Handle <code>\\&quot;</code>, <code>\\\\</code>, <code>\\n</code> escape sequences</td>\n</tr>\n<tr>\n<td>Numeric Value</td>\n<td>Float or special constants</td>\n<td><code>123.45</code>, <code>+Inf</code>, <code>NaN</code></td>\n<td>IEEE 754 compliance for special values</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>Integer milliseconds</td>\n<td><code>1609459200000</code></td>\n<td>Optional, defaults to scrape time</td>\n</tr>\n</tbody></table>\n<p><strong>Metric family grouping</strong> collects related metrics that share the same base name but have different label combinations or suffixes. For histogram metrics, the parser must recognize and group together the <code>_bucket</code>, <code>_sum</code>, and <code>_count</code> series that represent different aspects of the same histogram. This grouping enables proper validation—for example, ensuring histogram bucket boundaries are monotonically increasing and that all required series are present.</p>\n<p>The parser implements <strong>streaming validation</strong> that checks metric names, label formats, and values as they&#39;re encountered rather than buffering the entire response first. This approach provides better memory efficiency for large responses and enables early error detection. When validation failures occur, the parser logs the error with sufficient context for debugging but continues processing the remaining metrics in the response.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Rule</th>\n<th>Check</th>\n<th>Error Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric name format</td>\n<td>Matches <code>[a-zA-Z_:][a-zA-Z0-9_:]*</code></td>\n<td>Skip metric, log error with line number</td>\n</tr>\n<tr>\n<td>Label key format</td>\n<td>No <code>__</code> prefix, valid identifier</td>\n<td>Skip sample, preserve other labels</td>\n</tr>\n<tr>\n<td>Label value encoding</td>\n<td>Valid UTF-8, proper escaping</td>\n<td>Replace invalid chars, log warning</td>\n</tr>\n<tr>\n<td>Numeric value parsing</td>\n<td>Valid float or special constant</td>\n<td>Skip sample, increment parse error counter</td>\n</tr>\n<tr>\n<td>Histogram consistency</td>\n<td>Monotonic buckets, sum/count present</td>\n<td>Accept partial data, log inconsistency</td>\n</tr>\n</tbody></table>\n<p><strong>Type inference</strong> determines whether each metric represents a counter, gauge, histogram, or summary based on <code># TYPE</code> comments and naming conventions. When type comments are missing, the parser uses heuristics like metric name suffixes (<code>_total</code> suggests counter, <code>_bucket</code> suggests histogram) and value patterns (monotonically increasing suggests counter). Accurate type inference is crucial because it affects how the storage engine handles the data and how query functions like <code>rate()</code> operate.</p>\n<p>The <strong>sample construction process</strong> converts parsed tokens into <code>Sample</code> objects with proper timestamps and labels:</p>\n<ol>\n<li><strong>Parse metric name and labels</strong> from each sample line using the lexical analyzer</li>\n<li><strong>Apply target labels</strong> from service discovery configuration (instance, job, environment)  </li>\n<li><strong>Validate label cardinality</strong> against configured limits to prevent memory explosion</li>\n<li><strong>Assign timestamp</strong> using provided value or current scrape time with millisecond precision</li>\n<li><strong>Convert numeric value</strong> to internal float64 representation, handling special constants</li>\n<li><strong>Create Sample object</strong> with metric identifier (name + labels), timestamp, and value</li>\n<li><strong>Forward to storage engine</strong> through the ingestion pipeline</li>\n</ol>\n<p><strong>Error recovery</strong> strategies ensure that parsing errors don&#39;t cause complete scrape failure. The parser maintains error counters for different failure modes and implements <strong>best-effort processing</strong> that extracts valid metrics even from responses with some malformed data. This resilience is essential in production environments where applications may generate imperfect exposition format due to bugs or configuration issues.</p>\n<p>⚠️ <strong>Pitfall: Label Cardinality Explosion</strong>\nApplications sometimes generate labels with unbounded values like user IDs, request IDs, or timestamps. A single misbehaving service can create millions of unique time series, consuming all available memory. The parser must implement <strong>cardinality protection</strong> that limits the number of unique label combinations per metric name and rejects samples that would exceed these limits. Always validate cardinality before creating new time series, not after.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Ftarget-state-machine.svg\" alt=\"Scrape Target State Machine\"></p>\n<h3 id=\"health-and-error-handling\">Health and Error Handling</h3>\n<p>The health and error handling subsystem monitors scrape operations, classifies failures, implements recovery strategies, and provides observability into the scraping process. This component ensures that transient network issues don&#39;t cause permanent data loss, provides operators with visibility into collection problems, and maintains system stability under adverse conditions.</p>\n<blockquote>\n<p>Think of target health management like <strong>managing a fleet of field reporters</strong> who gather information from remote locations. Some reporters occasionally miss check-ins due to bad weather (network issues), others might send garbled reports (parsing errors), and some might go completely silent (service failures). A good news editor tracks which reporters are reliable, follows up on missed check-ins, and adjusts expectations based on each reporter&#39;s track record. Similarly, our health system tracks target reliability and adapts behavior accordingly.</p>\n</blockquote>\n<p>Target health exists in multiple dimensions that require independent tracking and different response strategies. <strong>Network health</strong> indicates whether the scrape engine can successfully connect to a target&#39;s endpoint. <strong>Application health</strong> shows whether the target service is running and responding to requests. <strong>Data health</strong> reflects whether the metrics data being returned is valid and parseable. A target might have good network and application health but poor data health due to bugs in its metrics exposition code.</p>\n<p>The health tracking system maintains state for each target using this comprehensive model:</p>\n<table>\n<thead>\n<tr>\n<th>Health Dimension</th>\n<th>States</th>\n<th>Transition Triggers</th>\n<th>Impact on Scheduling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Network</td>\n<td><code>Reachable</code>, <code>Unreachable</code>, <code>Timeout</code></td>\n<td>TCP connection success/failure</td>\n<td>Unreachable targets get exponential backoff</td>\n</tr>\n<tr>\n<td>Application</td>\n<td><code>Healthy</code>, <code>Error</code>, <code>Degraded</code></td>\n<td>HTTP status codes (200 vs 4xx/5xx)</td>\n<td>Error responses increase scrape interval</td>\n</tr>\n<tr>\n<td>Data</td>\n<td><code>Valid</code>, <code>Partial</code>, <code>Invalid</code></td>\n<td>Parsing success/failure rates</td>\n<td>Invalid data triggers diagnostic logging</td>\n</tr>\n<tr>\n<td>Overall</td>\n<td><code>Up</code>, <code>Down</code>, <code>Warning</code></td>\n<td>Combination of above dimensions</td>\n<td>Used for alerting and dashboard display</td>\n</tr>\n</tbody></table>\n<p><strong>Failure classification</strong> determines the appropriate response to different types of errors encountered during scraping. Not all failures are equal—a temporary DNS resolution failure should be handled differently than an HTTP 404 response, which should be handled differently than a timeout. The classification system groups errors into categories that each have specific retry strategies and escalation procedures.</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Examples</th>\n<th>Retry Strategy</th>\n<th>Escalation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transient Network</td>\n<td>DNS timeout, connection refused</td>\n<td>Immediate retry with jitter</td>\n<td>Exponential backoff after 3 failures</td>\n</tr>\n<tr>\n<td>Service Error</td>\n<td>HTTP 500, service unavailable</td>\n<td>Brief delay then retry</td>\n<td>Reduce scrape frequency after 5 failures</td>\n</tr>\n<tr>\n<td>Configuration Error</td>\n<td>HTTP 404, invalid endpoint</td>\n<td>Log error, continue normal interval</td>\n<td>Alert operator after 10 consecutive failures</td>\n</tr>\n<tr>\n<td>Data Format Error</td>\n<td>Malformed exposition format</td>\n<td>Process partial data, log details</td>\n<td>Report parsing statistics</td>\n</tr>\n</tbody></table>\n<p><strong>Adaptive retry logic</strong> balances the need for reliable data collection against the risk of overwhelming failing services. When targets experience failures, the retry system implements <strong>exponential backoff with jitter</strong> to reduce load while maintaining the possibility of recovery detection. The jitter component prevents <strong>thundering herd effects</strong> where many scrape engines simultaneously retry the same failed targets.</p>\n<p>The retry algorithm follows this progression for consecutive failures:</p>\n<ol>\n<li><strong>First failure</strong>: Record error, maintain normal scrape interval</li>\n<li><strong>Second failure</strong>: Add 10-30% random jitter to next scrape time  </li>\n<li><strong>Third failure</strong>: Double the scrape interval (15s becomes 30s)</li>\n<li><strong>Fourth failure</strong>: Double again with maximum cap (30s becomes 60s, capped at 5min)</li>\n<li><strong>Continued failures</strong>: Maintain maximum interval with occasional probe attempts</li>\n<li><strong>First success</strong>: Immediately return to configured normal interval</li>\n<li><strong>Partial success</strong>: Reduce interval by half until back to normal</li>\n</ol>\n<p><strong>Circuit breaker patterns</strong> protect both the scrape engine and target services from cascading failures. When a target fails consistently, the circuit breaker opens and stops sending requests temporarily. This prevents resource waste on the scrape engine side and reduces load on the failing service, potentially allowing it to recover. The circuit breaker periodically sends probe requests to detect recovery.</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Behavior</th>\n<th>Transition Condition</th>\n<th>Probe Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>Normal scraping at configured interval</td>\n<td>Failure rate &lt; 50% over 10 scrapes</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>Block all scrape attempts</td>\n<td>After 60 seconds or manual reset</td>\n<td>Every 60 seconds</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Allow single probe scrape</td>\n<td>Successful probe or probe failure</td>\n<td>Single attempt</td>\n</tr>\n</tbody></table>\n<p><strong>Resource protection</strong> mechanisms prevent failing targets from consuming excessive scrape engine resources. The protection system implements <strong>per-target resource limits</strong> on memory usage, connection time, and response body size. When targets exceed these limits, the scrape engine terminates the request and marks the target as misbehaving.</p>\n<table>\n<thead>\n<tr>\n<th>Resource</th>\n<th>Limit</th>\n<th>Protection Mechanism</th>\n<th>Action on Violation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Response Body Size</td>\n<td>10MB default</td>\n<td>Stream reader with size limit</td>\n<td>Truncate response, process partial data</td>\n</tr>\n<tr>\n<td>Connection Time</td>\n<td>Scrape timeout (10s default)</td>\n<td>Context cancellation</td>\n<td>Close connection, mark as timeout</td>\n</tr>\n<tr>\n<td>Memory per Target</td>\n<td>50MB default</td>\n<td>Sample buffer size limits</td>\n<td>Drop oldest samples, log warning</td>\n</tr>\n<tr>\n<td>Concurrent Connections</td>\n<td>Global semaphore</td>\n<td>Weighted semaphore acquisition</td>\n<td>Queue scrape, apply backpressure</td>\n</tr>\n</tbody></table>\n<p><strong>Health metrics collection</strong> provides observability into the scraping process itself through internal metrics that track success rates, error distributions, response times, and resource usage. These metrics are essential for operators to understand collection health and optimize scrape configurations. The health system exposes these metrics through the same HTTP endpoint used by external scrapers.</p>\n<p>Key internal metrics include:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Name</th>\n<th>Type</th>\n<th>Labels</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>scrape_duration_seconds</code></td>\n<td>Histogram</td>\n<td><code>job</code>, <code>instance</code></td>\n<td>Track response time distribution</td>\n</tr>\n<tr>\n<td><code>scrape_samples_scraped</code></td>\n<td>Gauge</td>\n<td><code>job</code>, <code>instance</code></td>\n<td>Number of samples per scrape</td>\n</tr>\n<tr>\n<td><code>scrape_series_added</code></td>\n<td>Counter</td>\n<td><code>job</code>, <code>instance</code></td>\n<td>New time series creation rate</td>\n</tr>\n<tr>\n<td><code>scrape_health</code></td>\n<td>Gauge</td>\n<td><code>job</code>, <code>instance</code></td>\n<td>Binary health indicator (1=up, 0=down)</td>\n</tr>\n<tr>\n<td><code>scrape_timeout_seconds</code></td>\n<td>Gauge</td>\n<td><code>job</code>, <code>instance</code></td>\n<td>Configured timeout for target</td>\n</tr>\n</tbody></table>\n<p><strong>Error aggregation and reporting</strong> collects detailed error information across all targets and provides structured access for debugging and alerting. Rather than logging each individual failure, the system aggregates errors by type, target, and time window to provide meaningful insights without overwhelming operators with noise.</p>\n<p>⚠️ <strong>Pitfall: Health Check Feedback Loops</strong>\nWhen scrape targets expose their own health status as metrics, failures can create confusing feedback loops. If a service reports itself as unhealthy in its metrics but still responds to HTTP requests, should the scrape engine consider it healthy or unhealthy? Design clear separation between <strong>collection health</strong> (can we scrape it?) and <strong>application health</strong> (what does it report about itself?). Never use application-reported health metrics to control scraping behavior.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Client</td>\n<td><code>net/http</code> with default client</td>\n<td>Custom client with connection pooling</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static file-based configuration</td>\n<td>Kubernetes API client (<code>k8s.io/client-go</code>)</td>\n</tr>\n<tr>\n<td>Concurrency Control</td>\n<td>Basic goroutines with sync.WaitGroup</td>\n<td>Worker pools with semaphores</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>YAML files with <code>gopkg.in/yaml.v2</code></td>\n<td>Configuration hot-reload with file watching</td>\n</tr>\n<tr>\n<td>Metrics Parsing</td>\n<td>Text scanner with regular expressions</td>\n<td>Custom lexer with finite state machine</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n├── cmd/scraper/\n│   └── main.go                    ← Entry point for scrape engine\n├── internal/scrape/\n│   ├── engine.go                  ← Main scrape coordinator\n│   ├── target.go                  ← Target management and state\n│   ├── scheduler.go               ← Per-target scrape scheduling  \n│   ├── parser.go                  ← Prometheus format parser\n│   ├── discovery/\n│   │   ├── static.go              ← Static file-based discovery\n│   │   ├── dns.go                 ← DNS SRV record discovery\n│   │   └── kubernetes.go          ← Kubernetes API discovery\n│   └── health/\n│       ├── tracker.go             ← Target health monitoring\n│       └── circuit_breaker.go     ← Circuit breaker implementation\n├── internal/config/\n│   └── scrape_config.go           ← Scrape configuration structures\n└── internal/storage/\n    └── ingestion.go               ← Interface to storage layer</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>HTTP Client with Timeouts (internal/scrape/client.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> scrape</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">io</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HTTPClient wraps net/http.Client with scraping-specific configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HTTPClient</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    userAgent </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxResponseSize </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHTTPClient creates a configured HTTP client for scraping</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHTTPClient</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPClient</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HTTPClient</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        client: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timeout: timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Transport: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Transport</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                MaxIdleConns:        </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                MaxIdleConnsPerHost: </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                IdleConnTimeout:     </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        userAgent:       </span><span style=\"color:#9ECBFF\">\"prometheus-scraper/1.0\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxResponseSize: </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// 10MB limit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScrapeTarget performs HTTP GET request with size limits and timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPClient</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ScrapeTarget</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">url</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewRequestWithContext</span><span style=\"color:#E1E4E8\">(ctx, </span><span style=\"color:#9ECBFF\">\"GET\"</span><span style=\"color:#E1E4E8\">, url, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"creating request: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"User-Agent\"</span><span style=\"color:#E1E4E8\">, c.userAgent)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    req.Header.</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Accept\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"text/plain;version=0.0.4\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resp, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> c.client.</span><span style=\"color:#B392F0\">Do</span><span style=\"color:#E1E4E8\">(req)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HTTP request failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> resp.Body.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> resp.StatusCode </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> http.StatusOK {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"HTTP </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, resp.StatusCode, resp.Status)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Limit response body size to prevent memory exhaustion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    limitedReader </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LimitedReader</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        R: resp.Body,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        N: c.maxResponseSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Read entire response into memory for parsing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    body, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> io.</span><span style=\"color:#B392F0\">ReadAll</span><span style=\"color:#E1E4E8\">(limitedReader)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"reading response body: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> bytes.</span><span style=\"color:#B392F0\">NewReader</span><span style=\"color:#E1E4E8\">(body), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Target Health Tracker (internal/scrape/health/tracker.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> health</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthStatus represents the current health state of a scrape target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthUnknown</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthUp</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthDown</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthDegraded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TargetHealth tracks health metrics for a single scrape target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TargetHealth</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status           </span><span style=\"color:#B392F0\">HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastScrapeTime   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastScrapeError  </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    consecutiveFailures </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    totalScrapes     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successfulScrapes </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastSuccessTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTargetHealth creates a new health tracker for a target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTargetHealth</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TargetHealth</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TargetHealth</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        status: HealthUnknown,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordSuccess updates health status for successful scrape</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TargetHealth</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordSuccess</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">scrapeDuration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> h.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthUp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.lastScrapeTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.lastScrapeError </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.consecutiveFailures </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.totalScrapes</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.successfulScrapes</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.lastSuccessTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RecordFailure updates health status for failed scrape</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TargetHealth</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordFailure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.mutex.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> h.mutex.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.lastScrapeTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.lastScrapeError </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.consecutiveFailures</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.totalScrapes</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> h.consecutiveFailures </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        h.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthDown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> h.consecutiveFailures </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        h.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthDegraded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetStatus returns current health status thread-safely</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TargetHealth</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetStatus</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> h.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> h.status, h.lastScrapeError, h.lastScrapeTime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SuccessRate calculates the percentage of successful scrapes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TargetHealth</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SuccessRate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    h.mutex.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> h.mutex.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> h.totalScrapes </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(h.successfulScrapes) </span><span style=\"color:#F97583\">/</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(h.totalScrapes)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeletons\">Core Logic Skeletons</h4>\n<p><strong>Main Scrape Engine (internal/scrape/engine.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ScrapeEngine coordinates target discovery, scheduling, and metrics collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScrapeEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    targets         </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Target</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    targetsMutex    </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    discoverers     []</span><span style=\"color:#B392F0\">TargetDiscoverer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage         </span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpClient      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HTTPClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger          </span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopChan        </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wg              </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewScrapeEngine creates and configures a new scrape engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewScrapeEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#B392F0\"> Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize ScrapeEngine struct with provided parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create HTTP client with configured timeout from config.ScrapeTimeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize empty targets map and stop channel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create discoverers list based on config (static, DNS, K8s)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sync.RWMutex for targets map since it's read frequently but written rarely</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins target discovery and scraping operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start all configured target discoverers in separate goroutines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Launch target discovery consolidation loop to merge discovered targets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start metrics collection goroutines for each active target</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Set up signal handling for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use errgroup.Group to manage multiple goroutines with error propagation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateTargets processes target changes from service discovery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateTargets</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">newTargets</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Target</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock on targets map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compare new target list with existing targets to find additions/removals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start scrape goroutines for newly discovered targets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Stop scrape goroutines for removed targets by cancelling their contexts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update internal targets map with new target set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use target.URL as unique identifier for comparison</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Target Scraper (internal/scrape/target.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// scrapeTarget performs a single scrape operation against a target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">scrapeTarget</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Target</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create HTTP request context with scrape timeout deadline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Record scrape start time for consistent interval calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Perform HTTP GET request to target.URL + target.MetricsPath</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check HTTP response status and handle non-200 responses</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse response body using Prometheus exposition format parser</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Add target labels (job, instance) to all parsed samples</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Forward samples to storage engine via Append() method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update target health status based on success/failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Record scrape metrics (duration, sample count, error status)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Always update health status even if storage append fails</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// runTargetScrapeLoop manages the continuous scraping lifecycle for one target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">runTargetScrapeLoop</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Target</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create ticker with target.ScrapeInterval duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize target health tracker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Enter infinite loop listening for ticker and context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: On each tick, call scrapeTarget() and handle any errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate next scrape time based on start time, not completion time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Implement exponential backoff for consecutive failures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Clean up ticker and update target registry on context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.NewTicker and defer ticker.Stop() for proper cleanup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Prometheus Format Parser (internal/scrape/parser.go):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ParsePrometheusFormat converts exposition format text to structured samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ParsePrometheusFormat</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">reader</span><span style=\"color:#B392F0\"> io</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Reader</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">defaultTimestamp</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create scanner to read input line by line</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize empty samples slice and current metric metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Process each line: comments (HELP/TYPE) vs sample lines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For comment lines, extract metric name and type information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: For sample lines, parse metric name, labels, value, optional timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Validate metric names match [a-zA-Z_:][a-zA-Z0-9_:]* pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Parse label sets with proper quote handling and escape sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Convert string values to float64, handle +Inf/-Inf/NaN special cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Use provided timestamp if sample doesn't include one</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 10: Return accumulated samples slice or first parsing error encountered</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use regexp for metric name validation, manual parsing for labels is more efficient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// parseSampleLine extracts components from a single metric sample line</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> parseSampleLine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">line</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#FFAB70\">metricName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timestamp</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Find metric name at start of line (ends at '{' or whitespace)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If '{' present, parse label set until matching '}' </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse numeric value after labels, handle special float constants</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check for optional timestamp at end of line (Unix milliseconds)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return parsed components or detailed error with line context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Be careful with quoted label values containing '{', '}', or whitespace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<ul>\n<li><p><strong>Goroutine Management</strong>: Use <code>sync.WaitGroup</code> to track scrape goroutines and <code>context.Context</code> for graceful cancellation. Each target gets its own goroutine for isolation.</p>\n</li>\n<li><p><strong>HTTP Timeouts</strong>: Set timeouts at multiple levels: <code>http.Client.Timeout</code> for overall request timeout, <code>context.WithTimeout</code> for individual scrape operations, and <code>io.LimitedReader</code> for response size limits.</p>\n</li>\n<li><p><strong>Memory Management</strong>: Pre-allocate slices for samples when possible (<code>make([]*Sample, 0, estimatedCount)</code>). Use object pooling for frequently allocated parser structures.</p>\n</li>\n<li><p><strong>Error Handling</strong>: Distinguish between permanent errors (HTTP 404) and transient errors (network timeout). Use <code>errors.Is()</code> and <code>errors.As()</code> for proper error classification.</p>\n</li>\n<li><p><strong>Configuration Reloading</strong>: Watch configuration files with <code>fsnotify</code> package and reload target lists without stopping active scrapes.</p>\n</li>\n</ul>\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing target discovery:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/scrape/discovery/...</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/scraper/main.go</span><span style=\"color:#79B8FF\"> -config=test-config.yaml</span></span></code></pre></div>\n\n<p>Expected behavior: Scraper should log discovered targets from static configuration and start attempting to scrape them (even if they fail). Check logs for &quot;discovered target&quot; messages.</p>\n<p><strong>After implementing scrape scheduling:</strong><br>Configure a target pointing to <code>http://localhost:8080/metrics</code> and run a simple HTTP server that returns basic Prometheus format. Verify scrapes happen at the configured interval by checking timestamps in logs.</p>\n<p><strong>After implementing metrics parsing:</strong>\nTest with malformed exposition format to verify parser handles errors gracefully and extracts valid metrics while skipping invalid ones.</p>\n<p><strong>Signs something is wrong:</strong></p>\n<ul>\n<li>Scrape times drift further apart → Check interval calculation logic</li>\n<li>Memory usage grows constantly → Check for target goroutine leaks  </li>\n<li>Targets always show as &quot;down&quot; → Verify HTTP client timeout configuration</li>\n<li>Parser crashes on certain inputs → Add more input validation and error recovery</li>\n</ul>\n<h2 id=\"time-series-storage-engine\">Time Series Storage Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section directly corresponds to Milestone 3 (Time Series Storage) and provides the efficient storage foundation that will be queried by Milestone 4 (Query Engine). The storage engine receives scraped metrics from Milestone 2 (Scrape Engine) and persists the time series data defined in Milestone 1 (Metrics Data Model).</p>\n</blockquote>\n<p>The time series storage engine serves as the memory and archive of our metrics collection system. It must handle the unique challenges of time series data: high write throughput from continuous scraping, efficient storage of timestamped numeric values, fast retrieval for queries, and automatic lifecycle management as data ages. Unlike traditional databases optimized for random access patterns, time series workloads exhibit distinct characteristics that demand specialized storage techniques.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fstorage-layout.svg\" alt=\"Storage Architecture\"></p>\n<p>The storage engine operates under several constraints that shape its design. Time series data arrives continuously with timestamps that are always increasing within each series, creating an append-only write pattern. Query patterns typically request recent data or ranges of historical data, rarely accessing individual points randomly. The volume of incoming data grows linearly with the number of monitored targets and their label cardinality, making storage efficiency critical for operational costs. Finally, old data has diminishing value and must be automatically removed to prevent unbounded storage growth.</p>\n<h3 id=\"library-archive-mental-model\">Library Archive Mental Model</h3>\n<p>Understanding time series storage becomes intuitive when we think of it like a physical library archive system. Imagine a vast library that specializes in storing weather measurement records from thousands of monitoring stations worldwide. Each monitoring station represents a unique time series identified by its location and the type of measurement (temperature, humidity, pressure). The library receives new measurements every minute and must organize them efficiently for both storage and retrieval.</p>\n<p>In our library analogy, each <strong>monitoring station</strong> corresponds to a time series identified by its metric name and label set. The station &quot;temperature.celsius{location=paris,sensor=outdoor}&quot; produces a continuous stream of timestamped temperature readings, just as our time series produces timestamped numeric samples. The library must create a unique storage location for each station&#39;s records, much like our storage engine creates separate storage chunks for each time series.</p>\n<p>The library organizes records using a two-level system: a <strong>card catalog</strong> (our inverted indexes) and <strong>storage boxes</strong> (our compressed chunks). The card catalog contains index cards that list which storage boxes contain records for each station and time period. When a researcher asks for &quot;all temperature readings from Paris outdoor sensors between March 1-15,&quot; the librarian first consults the card catalog to identify relevant boxes, then retrieves only those specific boxes rather than searching the entire archive.</p>\n<p>The storage boxes themselves use a clever compression technique. Instead of writing the full timestamp and value for each record, the librarian writes only the difference from the previous record. If yesterday&#39;s temperature was 20.5°C and today&#39;s is 21.2°C, the record stores &quot;+0.7&quot; instead of the full value. This <strong>delta compression</strong> dramatically reduces the space needed for each box, allowing the library to store decades of history in a reasonable amount of space.</p>\n<p>As records age, the library applies a <strong>retention policy</strong> similar to how old newspapers are eventually discarded or moved to deep storage. Records newer than one month are kept in full detail. Records older than one month but newer than one year are <strong>downsampled</strong> - instead of minute-by-minute readings, only hourly averages are preserved. Records older than one year are deleted entirely. This automatic lifecycle management prevents the archive from growing without bound while preserving the most valuable historical data.</p>\n<p>When the library receives new records, they&#39;re first written to a <strong>processing journal</strong> (our write-ahead log) before being filed in the appropriate storage boxes. If a power outage occurs while the librarian is updating multiple boxes, the journal can be replayed upon restart to ensure no records are lost. This provides <strong>durability</strong> even in the face of unexpected failures.</p>\n<p>The genius of this system is that it optimizes for the actual usage patterns of time series data: most writes are recent data appended to existing series, most reads request ranges of data from specific series, and old data becomes less valuable over time. Our storage engine implements this same organizational philosophy using modern computer science techniques.</p>\n<h3 id=\"gorilla-compression\">Gorilla Compression</h3>\n<p>The Gorilla compression algorithm, developed by Facebook for their time series database, provides remarkable space efficiency by exploiting the predictable patterns inherent in time series data. Most time series data exhibits two key properties: timestamps arrive at regular intervals, and values change gradually between adjacent samples. Gorilla compression leverages both patterns to achieve compression ratios of 10:1 or better.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fcompression-process.svg\" alt=\"Gorilla Compression Process\"></p>\n<blockquote>\n<p><strong>Decision: Gorilla Compression Algorithm</strong></p>\n<ul>\n<li><strong>Context</strong>: Time series data consumes enormous storage space when naively stored as timestamp-value pairs. A monitoring system collecting metrics every 15 seconds from 1000 targets generates over 5 million samples per day. Without compression, each sample requires 16 bytes (8-byte timestamp + 8-byte float64), consuming 80MB daily per target.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Store raw timestamp-value pairs without compression</li>\n<li>Generic compression (gzip/lz4) applied to time series chunks  </li>\n<li>Gorilla-style delta-of-delta timestamp and XOR value compression</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement Gorilla compression with delta-of-delta timestamps and XOR value encoding</li>\n<li><strong>Rationale</strong>: Gorilla compression is specifically designed for time series characteristics. Delta-of-delta encoding exploits regular timestamp intervals common in metrics collection. XOR value encoding exploits the fact that consecutive floating-point measurements often share common bit patterns. Generic compression doesn&#39;t understand time series structure and achieves lower compression ratios.</li>\n<li><strong>Consequences</strong>: Achieves 10:1 compression ratios typical of Gorilla. Requires more complex encoding/decoding logic than raw storage. CPU overhead for compression/decompression during writes and reads.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Compression Option</th>\n<th>Space Efficiency</th>\n<th>CPU Overhead</th>\n<th>Implementation Complexity</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Raw Storage</td>\n<td>16 bytes/sample</td>\n<td>None</td>\n<td>Minimal</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Generic Compression</td>\n<td>4-8 bytes/sample</td>\n<td>Medium</td>\n<td>Low</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Gorilla Compression</td>\n<td>1.4 bytes/sample</td>\n<td>High</td>\n<td>High</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<h4 id=\"delta-of-delta-timestamp-encoding\">Delta-of-Delta Timestamp Encoding</h4>\n<p>Timestamp compression exploits the regularity of metrics collection intervals. Consider a time series scraped every 15 seconds starting at timestamp 1640995200:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Timestamp: 1640995200, 1640995215, 1640995230, 1640995245, ...\nDeltas:    [baseline],       +15,       +15,       +15, ...\nDoD:       [baseline],   [baseline],        0,        0, ...</code></pre></div>\n\n<p>The algorithm stores the first timestamp as a 64-bit baseline. For the second timestamp, it calculates the delta (difference) from the first and stores this delta as a baseline delta. For subsequent timestamps, it calculates the &quot;delta-of-delta&quot; - the difference between the current delta and the expected delta based on the pattern.</p>\n<p>When the delta-of-delta is zero (indicating a perfectly regular interval), Gorilla stores just a single bit flag. When the delta-of-delta is small (within ±63 of the expected delta), it stores the value in 7 bits. Only when timestamps deviate significantly from the pattern does it fall back to larger encodings. This approach reduces regular 64-bit timestamps to often just 1 bit per sample.</p>\n<table>\n<thead>\n<tr>\n<th>Delta-of-Delta Range</th>\n<th>Encoding</th>\n<th>Bits Used</th>\n<th>Common Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0</td>\n<td>Single &#39;0&#39; bit</td>\n<td>1</td>\n<td>Regular intervals</td>\n</tr>\n<tr>\n<td>-63 to +64</td>\n<td>&#39;10&#39; + 7-bit signed value</td>\n<td>9</td>\n<td>Small time drift</td>\n</tr>\n<tr>\n<td>-255 to +256</td>\n<td>&#39;110&#39; + 9-bit signed value</td>\n<td>12</td>\n<td>Clock adjustments</td>\n</tr>\n<tr>\n<td>-2047 to +2048</td>\n<td>&#39;1110&#39; + 12-bit signed value</td>\n<td>16</td>\n<td>Irregular intervals</td>\n</tr>\n<tr>\n<td>Other</td>\n<td>&#39;1111&#39; + 32-bit signed value</td>\n<td>36</td>\n<td>Major time jumps</td>\n</tr>\n</tbody></table>\n<h4 id=\"xor-value-encoding\">XOR Value Encoding</h4>\n<p>Value compression leverages the observation that consecutive floating-point measurements often share common bit patterns. Temperature readings might vary from 20.1°C to 20.3°C, and the IEEE 754 binary representations of these values differ only in the least significant bits.</p>\n<p>The algorithm XORs each value with the previous value in the series. If the XOR result is zero (values are identical), it stores a single &#39;0&#39; bit. If the XOR result is non-zero, it analyzes the bit pattern to determine the most efficient encoding.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Value 1: 20.1 (binary: 0x4034199999999998)\nValue 2: 20.3 (binary: 0x403451999999999A)\nXOR:                   0x0000408000000002</code></pre></div>\n\n<p>The XOR result often has many leading and trailing zero bits. Gorilla stores only the significant bits between the leading and trailing zeros, along with metadata indicating their position. When consecutive values are very similar, this reduces 64-bit floating-point values to as few as 5-10 bits.</p>\n<table>\n<thead>\n<tr>\n<th>XOR Pattern</th>\n<th>Encoding Strategy</th>\n<th>Typical Bits</th>\n<th>Example Scenario</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Zero XOR</td>\n<td>Single &#39;0&#39; bit</td>\n<td>1</td>\n<td>Identical consecutive values</td>\n</tr>\n<tr>\n<td>Same pattern as previous</td>\n<td>&#39;10&#39; + compressed bits</td>\n<td>2-15</td>\n<td>Gradually changing values</td>\n</tr>\n<tr>\n<td>New pattern</td>\n<td>&#39;11&#39; + leading zeros + length + bits</td>\n<td>10-65</td>\n<td>Value jumps or first difference</td>\n</tr>\n</tbody></table>\n<h4 id=\"compression-implementation-strategy\">Compression Implementation Strategy</h4>\n<p>The compression algorithm operates on fixed-size chunks of time series data, typically containing 2-4 hours worth of samples. Each chunk begins with baseline values for the first timestamp and value, followed by compressed deltas and XOR patterns for subsequent samples. The chunk header contains metadata necessary for decompression: the baseline timestamp, baseline value, and the number of samples in the chunk.</p>\n<p>During compression, the algorithm maintains state about the previous timestamp delta and value to calculate deltas-of-deltas and XOR patterns. This state allows the compressor to make optimal encoding decisions based on the emerging patterns in the data. The bit-level nature of the encoding means that sample boundaries don&#39;t align with byte boundaries, requiring careful bit manipulation during both compression and decompression.</p>\n<p>Decompression reverses the process by starting with the baseline values and iteratively applying the stored deltas and XOR patterns to reconstruct the original timestamp-value pairs. The algorithm must handle variable-length encodings correctly, using the bit flags to determine how many subsequent bits to read for each sample.</p>\n<h3 id=\"series-indexing\">Series Indexing</h3>\n<p>Efficient querying requires fast lookup of time series by metric name and label combinations. With millions of active time series, a naive linear search through all series would make queries prohibitively slow. The storage engine employs inverted indexes that map from metric names and label values to the specific time series containing that metadata, enabling subsecond query response times even with high cardinality.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Flabel-cardinality.svg\" alt=\"Label Cardinality Impact\"></p>\n<p>The indexing challenge stems from the multi-dimensional nature of labeled time series. A query like <code>http_requests_total{method=&quot;GET&quot;, status=&quot;200&quot;}</code> must find all time series where the metric name matches exactly and both specified labels have the required values. The query engine may then need to aggregate across other label dimensions like <code>instance</code> or <code>job</code> that weren&#39;t specified in the query.</p>\n<blockquote>\n<p><strong>Decision: Inverted Index Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Queries must efficiently find time series matching metric names and label selectors from millions of active series. Label-based queries like <code>{job=&quot;web&quot;, method=~&quot;GET|POST&quot;}</code> require fast intersection of multiple label conditions.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single composite index mapping full series signatures to storage locations</li>\n<li>Separate indexes per metric name with secondary label indexes</li>\n<li>Inverted indexes mapping each label value to series containing that value</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement inverted indexes with efficient intersection algorithms</li>\n<li><strong>Rationale</strong>: Inverted indexes support arbitrary label selector combinations efficiently. They enable fast intersection operations when multiple labels are specified. They scale better with high cardinality than composite indexes.</li>\n<li><strong>Consequences</strong>: Requires more storage overhead for multiple indexes. Complex intersection logic for multi-label queries. Fast query performance for typical PromQL patterns.</li>\n</ul>\n</blockquote>\n<h4 id=\"primary-metric-index\">Primary Metric Index</h4>\n<p>The primary index maps metric names to lists of series identifiers that contain metrics with that name. This provides the first level of filtering for most queries, since PromQL queries typically start with a metric name like <code>http_requests_total</code> or <code>cpu_usage_seconds</code>. The index structure resembles a traditional database index optimized for prefix matching and range scans.</p>\n<table>\n<thead>\n<tr>\n<th>Index Component</th>\n<th>Structure</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric Name Map</td>\n<td><code>map[string][]uint64</code></td>\n<td>Maps metric names to series ID lists</td>\n</tr>\n<tr>\n<td>Series Registry</td>\n<td><code>map[uint64]*SeriesMetadata</code></td>\n<td>Maps series IDs to full label sets and storage locations</td>\n</tr>\n<tr>\n<td>Label Indexes</td>\n<td><code>map[string]map[string][]uint64</code></td>\n<td>Maps label name → value → series IDs for fast label filtering</td>\n</tr>\n</tbody></table>\n<p>Each series receives a unique 64-bit identifier when first created. The series registry maintains the authoritative mapping from this identifier to the complete label set and storage chunk locations for that series. This indirection allows the inverted indexes to store compact series IDs rather than full label sets, reducing memory usage as cardinality grows.</p>\n<h4 id=\"label-value-indexes\">Label Value Indexes</h4>\n<p>For each label name that appears in any time series, the storage engine maintains an inverted index mapping from label values to the series that contain those values. The label index for <code>method</code> might map <code>&quot;GET&quot;</code> to series IDs [1001, 1003, 1007, ...] and <code>&quot;POST&quot;</code> to series IDs [1002, 1005, 1008, ...]`.</p>\n<p>When processing a query like <code>http_requests_total{method=&quot;GET&quot;, status=&quot;200&quot;}</code>, the query engine:</p>\n<ol>\n<li>Looks up the metric name <code>http_requests_total</code> in the primary index to get candidate series IDs</li>\n<li>Looks up label value <code>&quot;GET&quot;</code> in the <code>method</code> label index to get matching series IDs  </li>\n<li>Looks up label value <code>&quot;200&quot;</code> in the <code>status</code> label index to get matching series IDs</li>\n<li>Computes the intersection of all three series ID lists to find series matching all conditions</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Query Processing Step</th>\n<th>Input</th>\n<th>Index Used</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric Name Filter</td>\n<td><code>http_requests_total{...}</code></td>\n<td>Primary metric index</td>\n<td>Series IDs [1001, 1002, 1003, ...]</td>\n</tr>\n<tr>\n<td>Label Filter <code>method=&quot;GET&quot;</code></td>\n<td>Series ID list</td>\n<td><code>method</code> label index</td>\n<td>Filtered series IDs [1001, 1003, ...]</td>\n</tr>\n<tr>\n<td>Label Filter <code>status=&quot;200&quot;</code></td>\n<td>Series ID list</td>\n<td><code>status</code> label index</td>\n<td>Final series IDs [1001, ...]</td>\n</tr>\n<tr>\n<td>Series Metadata Lookup</td>\n<td>Series ID list</td>\n<td>Series registry</td>\n<td>Label sets and chunk locations</td>\n</tr>\n</tbody></table>\n<h4 id=\"index-intersection-algorithms\">Index Intersection Algorithms</h4>\n<p>Efficiently computing intersections of series ID lists is critical for query performance, especially when multiple label conditions are specified. The storage engine implements several intersection algorithms optimized for different scenarios commonly encountered in time series queries.</p>\n<p>For small result sets (fewer than 1000 series), the engine uses a simple hash set intersection. It loads the smallest series ID list into a hash set, then iterates through other lists checking membership. This approach provides O(n) performance and minimal memory overhead for the common case of specific label value combinations.</p>\n<p>For larger result sets, the engine switches to a sorted list intersection algorithm. Since series IDs are assigned monotonically, the inverted index lists are naturally sorted. The algorithm uses multiple pointers to advance through the sorted lists simultaneously, similar to merging sorted arrays. This approach avoids the memory allocation overhead of hash sets when dealing with large intermediate results.</p>\n<p>The query optimizer chooses the intersection order based on the estimated cardinality of each label condition. Label values with lower cardinality (fewer matching series) are processed first to minimize the size of intermediate results. A label like <code>datacenter=&quot;us-west&quot;</code> might match thousands of series, while <code>instance=&quot;web-01&quot;</code> matches only one, making instance-first processing much more efficient.</p>\n<h4 id=\"cardinality-management\">Cardinality Management</h4>\n<p>High cardinality labels pose the greatest threat to index performance and memory usage. A label like <code>request_id</code> with unique values for every request would create millions of entries in the label value index, consuming enormous memory and slowing intersection operations. The storage engine implements several strategies to detect and mitigate cardinality explosion before it degrades system performance.</p>\n<table>\n<thead>\n<tr>\n<th>Cardinality Level</th>\n<th>Series Count</th>\n<th>Index Memory</th>\n<th>Query Performance</th>\n<th>Management Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Low</td>\n<td>&lt; 1,000</td>\n<td>&lt; 10MB</td>\n<td>Subsecond</td>\n<td>Normal operation</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>1,000 - 100,000</td>\n<td>10MB - 1GB</td>\n<td>1-5 seconds</td>\n<td>Monitor growth rate</td>\n</tr>\n<tr>\n<td>High</td>\n<td>100,000 - 1,000,000</td>\n<td>1GB - 10GB</td>\n<td>5-30 seconds</td>\n<td>Throttle ingestion</td>\n</tr>\n<tr>\n<td>Critical</td>\n<td>&gt; 1,000,000</td>\n<td>&gt; 10GB</td>\n<td>30+ seconds</td>\n<td>Reject new series</td>\n</tr>\n</tbody></table>\n<p>The <code>CardinalityTracker</code> component monitors the number of unique values for each label name and raises alerts when cardinality grows unexpectedly. It maintains moving averages of cardinality growth rates and can predict when a label will exceed safe thresholds based on current trends.</p>\n<p>When cardinality limits are approached, the storage engine can reject new time series that would exceed the configured limits. This prevents a runaway cardinality explosion from bringing down the entire monitoring system. The rejected series are logged for later analysis, allowing operators to identify the source of high-cardinality labels and fix the instrumentation.</p>\n<h3 id=\"data-lifecycle-management\">Data Lifecycle Management</h3>\n<p>Time series data has a natural lifecycle where recent data is most valuable for alerting and debugging, while historical data provides context for capacity planning and trend analysis. However, storing all historical data indefinitely is neither practical nor cost-effective. The storage engine implements automated lifecycle management that balances data retention needs with storage costs through configurable retention policies and downsampling strategies.</p>\n<p>The lifecycle management system operates on the principle that data value decreases over time, but at different rates for different use cases. Alerting requires minute-level granularity for recent data but can tolerate hour-level granularity for data older than a week. Capacity planning queries typically aggregate data over longer time periods and don&#39;t require full-resolution historical data. By automatically reducing resolution as data ages, the system maintains query capability while controlling storage growth.</p>\n<blockquote>\n<p><strong>Decision: Hierarchical Data Lifecycle Management</strong></p>\n<ul>\n<li><strong>Context</strong>: Time series data volume grows linearly with time, making indefinite full-resolution retention impossible. Different use cases have different precision requirements based on data age.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Fixed retention period with complete deletion after expiration</li>\n<li>Uniform downsampling (e.g., keep only hourly averages for all old data)</li>\n<li>Hierarchical retention with multiple resolution levels based on age</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement hierarchical retention with automatic downsampling at configurable age thresholds</li>\n<li><strong>Rationale</strong>: Provides optimal balance between storage efficiency and query utility. Recent data kept at full resolution for debugging. Historical data downsampled to enable long-term trend analysis. Configurable policies allow customization based on organizational needs.</li>\n<li><strong>Consequences</strong>: Requires complex logic to manage multiple resolution levels. Queries spanning multiple retention tiers need special handling. Provides excellent storage efficiency and query flexibility.</li>\n</ul>\n</blockquote>\n<h4 id=\"retention-policy-configuration\">Retention Policy Configuration</h4>\n<p>The storage engine supports flexible retention policies that define both the total retention period and the downsampling schedule as data ages. These policies are configured per metric or per metric pattern, allowing different retention strategies for different types of monitoring data.</p>\n<table>\n<thead>\n<tr>\n<th>Retention Tier</th>\n<th>Age Range</th>\n<th>Resolution</th>\n<th>Retention Period</th>\n<th>Storage Ratio</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High Resolution</td>\n<td>0-7 days</td>\n<td>Original (15s)</td>\n<td>7 days</td>\n<td>1.0x</td>\n</tr>\n<tr>\n<td>Medium Resolution</td>\n<td>7-30 days</td>\n<td>5 minutes</td>\n<td>23 days</td>\n<td>0.05x</td>\n</tr>\n<tr>\n<td>Low Resolution</td>\n<td>30-365 days</td>\n<td>1 hour</td>\n<td>335 days</td>\n<td>0.004x</td>\n</tr>\n<tr>\n<td>Archive</td>\n<td>1+ years</td>\n<td>Daily</td>\n<td>2 years</td>\n<td>0.0002x</td>\n</tr>\n</tbody></table>\n<p>A typical retention policy for application metrics might preserve 15-second resolution data for the past week, downsample to 5-minute resolution for the past month, and keep hourly averages for historical trend analysis. Critical business metrics might have longer high-resolution periods, while debugging metrics might have shorter retention.</p>\n<p>The retention configuration specifies not just the time boundaries but also the aggregation functions to use during downsampling. Counter metrics typically use rate calculations, gauge metrics use averages, and histogram metrics require careful handling to preserve distribution characteristics. The policy also specifies which labels to preserve during aggregation - high-cardinality labels like instance might be dropped while keeping service and datacenter labels.</p>\n<h4 id=\"automated-downsampling-process\">Automated Downsampling Process</h4>\n<p>The downsampling process operates as a background task that periodically identifies data chunks eligible for resolution reduction. Rather than processing individual samples, the system works with compressed chunks to maintain efficiency. When a chunk ages past a resolution threshold, the downsampling process decompresses the chunk, aggregates samples into lower-resolution buckets, and creates new compressed chunks at the target resolution.</p>\n<p>The aggregation process must handle different metric types appropriately to preserve semantic meaning. Counter metrics represent cumulative totals that should be converted to rates during downsampling. A counter chunk containing values [100, 115, 130, 145] over four 15-second intervals would become a single 1-minute rate of 0.75 increments per second. Gauge metrics represent point-in-time values and are typically averaged, though max, min, or last-value aggregations might be more appropriate depending on the metric semantics.</p>\n<p>Histogram metrics require special handling to preserve distribution information during downsampling. The system can&#39;t simply average the bucket counts, as this would destroy the ability to calculate accurate percentiles. Instead, histogram downsampling re-bins the constituent observations into the histogram buckets at the target resolution. This preserves the distributional characteristics while reducing storage overhead.</p>\n<p>The downsampling process maintains metadata linking the original high-resolution chunks to their downsampled derivatives. This enables the query engine to automatically select the appropriate resolution level based on the query time range and step size. A query requesting data over a 30-day period with 1-hour steps can use the low-resolution data directly rather than aggregating high-resolution samples.</p>\n<h4 id=\"garbage-collection-and-compaction\">Garbage Collection and Compaction</h4>\n<p>As data moves through the retention tiers and eventually expires, the storage engine must reclaim the associated storage space and update indexes accordingly. The garbage collection process operates independently of downsampling to avoid coupling data lifecycle decisions with storage management concerns.</p>\n<table>\n<thead>\n<tr>\n<th>Garbage Collection Phase</th>\n<th>Scope</th>\n<th>Actions</th>\n<th>Frequency</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Expired Data Deletion</td>\n<td>Chunks older than max retention</td>\n<td>Delete chunk files, remove index entries</td>\n<td>Daily</td>\n</tr>\n<tr>\n<td>Index Compaction</td>\n<td>Label indexes with many deleted entries</td>\n<td>Rebuild compact indexes, reclaim memory</td>\n<td>Weekly</td>\n</tr>\n<tr>\n<td>Chunk Compaction</td>\n<td>Small chunks from same series</td>\n<td>Merge chunks, improve compression ratio</td>\n<td>Continuous</td>\n</tr>\n<tr>\n<td>Orphan Cleanup</td>\n<td>Index entries without corresponding chunks</td>\n<td>Remove stale index entries</td>\n<td>Daily</td>\n</tr>\n</tbody></table>\n<p>The garbage collection process begins by identifying chunks that have exceeded their configured retention period. Rather than immediately deleting these chunks, the system marks them for deletion and continues serving queries from the remaining data. A separate cleanup process deletes the marked chunks during off-peak hours to minimize I/O impact on active queries.</p>\n<p>Index maintenance ensures that deleted time series don&#39;t leave stale entries in the inverted indexes. When all chunks for a particular series are deleted, the series registry entry is removed and all references to that series ID are purged from the label value indexes. This prevents memory leaks and maintains query performance as the active series set changes over time.</p>\n<p>Chunk compaction addresses the storage inefficiency that occurs when many small chunks exist for the same time series. These small chunks typically result from irregular scraping or brief monitoring periods. The compaction process merges consecutive chunks from the same series into larger chunks, improving compression ratios and reducing the overhead of chunk metadata.</p>\n<h3 id=\"persistence-and-recovery\">Persistence and Recovery</h3>\n<p>The storage engine must provide durability guarantees ensuring that accepted metric samples survive system crashes and hardware failures. Time series monitoring is often critical infrastructure used for alerting and incident response, making data loss unacceptable even in the face of unexpected failures. The persistence layer implements write-ahead logging and checkpoint-based recovery to provide these guarantees while maintaining write throughput under normal operation.</p>\n<p>The durability challenge stems from the batch-oriented nature of time series ingestion. Samples arrive continuously from scraped targets and are buffered in memory before being compressed and written to persistent chunks. A naive approach might lose several minutes of samples if the system crashed before the in-memory buffers were flushed. The write-ahead log ensures that every accepted sample is persisted before acknowledging receipt, providing recovery capability even if compression and indexing haven&#39;t completed.</p>\n<blockquote>\n<p><strong>Decision: Write-Ahead Log with Periodic Checkpoints</strong></p>\n<ul>\n<li><strong>Context</strong>: Time series samples must be durably stored before acknowledgment to prevent data loss. In-memory compression buffers improve throughput but create a window of vulnerability during crashes.</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Synchronous writes with immediate persistence of each sample</li>\n<li>Asynchronous writes with potential sample loss during crashes  </li>\n<li>Write-ahead log with background compression and checkpoint recovery</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement write-ahead logging with periodic background checkpointing</li>\n<li><strong>Rationale</strong>: WAL provides durability without sacrificing write throughput. Background compression maintains storage efficiency. Checkpoint recovery enables fast restart without replaying entire WAL history.</li>\n<li><strong>Consequences</strong>: Requires additional disk I/O for WAL writes. Adds complexity for crash recovery logic. Provides strong durability guarantees with good performance.</li>\n</ul>\n</blockquote>\n<h4 id=\"write-ahead-log-structure\">Write-Ahead Log Structure</h4>\n<p>The write-ahead log (WAL) provides an append-only record of all samples accepted by the storage engine. Unlike the compressed chunks optimized for query performance, the WAL prioritizes write speed and simplicity. Each WAL entry contains the complete information needed to reconstruct the sample: series identifier, timestamp, value, and metadata indicating the operation type.</p>\n<table>\n<thead>\n<tr>\n<th>WAL Entry Component</th>\n<th>Size</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Entry Type</td>\n<td>1 byte</td>\n<td>Distinguishes sample writes from metadata operations</td>\n</tr>\n<tr>\n<td>Series ID</td>\n<td>8 bytes</td>\n<td>Identifies the target time series</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>8 bytes</td>\n<td>Sample observation time</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>8 bytes</td>\n<td>Metric value as IEEE 754 float64</td>\n</tr>\n<tr>\n<td>Checksum</td>\n<td>4 bytes</td>\n<td>CRC32 for corruption detection</td>\n</tr>\n</tbody></table>\n<p>The WAL operates as a sequence of fixed-size segment files, each containing thousands of entries. When a segment reaches its maximum size (typically 64MB), the storage engine creates a new segment and continues appending to it. This segmented approach enables parallel processing during recovery and allows old segments to be deleted once their data has been safely incorporated into compressed chunks.</p>\n<p>Each WAL entry includes a CRC32 checksum to detect corruption from incomplete writes or storage hardware failures. During recovery, entries with invalid checksums are treated as the end of valid data, preventing corrupted entries from affecting the recovered state. The WAL also includes periodic checkpoint markers that record the current state of in-memory compression buffers, enabling incremental recovery rather than full replay.</p>\n<p>WAL writes use <code>fsync()</code> system calls to ensure data reaches persistent storage before returning control to the scrape engine. This provides strong durability guarantees at the cost of additional I/O latency. The storage engine batches multiple samples into single WAL writes when possible to amortize the <code>fsync()</code> overhead across multiple samples.</p>\n<h4 id=\"checkpoint-and-recovery-process\">Checkpoint and Recovery Process</h4>\n<p>The checkpoint process periodically captures the complete state of in-memory compression buffers and writes this state to persistent storage. Checkpoints enable fast recovery by providing a known good state that can be augmented with subsequent WAL entries rather than replaying the entire WAL history from system startup.</p>\n<p>During normal operation, the storage engine maintains several data structures in memory: partially compressed chunks for each active time series, label indexes mapping from values to series IDs, and series registries tracking metadata for all known series. The checkpoint process serializes all of this state into a compact binary format that can be quickly loaded during recovery.</p>\n<table>\n<thead>\n<tr>\n<th>Recovery Phase</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Checkpoint Load</td>\n<td>Last complete checkpoint file</td>\n<td>Deserialize in-memory state</td>\n<td>Active series and partial chunks</td>\n</tr>\n<tr>\n<td>WAL Replay</td>\n<td>WAL segments after checkpoint</td>\n<td>Apply entries to in-memory state</td>\n<td>Current state as of last WAL entry</td>\n</tr>\n<tr>\n<td>Index Rebuild</td>\n<td>Recovered series metadata</td>\n<td>Reconstruct label indexes</td>\n<td>Complete inverted indexes</td>\n</tr>\n<tr>\n<td>Background Flush</td>\n<td>In-memory chunks</td>\n<td>Compress and write to storage</td>\n<td>Cleaned up WAL segments</td>\n</tr>\n</tbody></table>\n<p>The recovery process begins by identifying the most recent valid checkpoint and loading it into memory. This provides a baseline state that may be several minutes or hours old, depending on checkpoint frequency. The system then replays all WAL entries written after the checkpoint timestamp, applying each sample to the appropriate in-memory compression buffer.</p>\n<p>WAL replay handles several edge cases that can occur during normal operation. If a series referenced in a WAL entry doesn&#39;t exist in the checkpoint, recovery creates a new series with the appropriate metadata. If a WAL entry has an invalid checksum or refers to an impossible timestamp, recovery logs the error but continues processing subsequent entries. This provides robustness against partial corruption while preserving as much data as possible.</p>\n<p>After WAL replay completes, the storage engine rebuilds any indexes or auxiliary data structures that aren&#39;t included in the checkpoint format. The label value indexes are reconstructed by iterating through all recovered series and building the inverted mappings. This process ensures that queries work correctly immediately after recovery without waiting for new data to populate the indexes.</p>\n<h4 id=\"failure-scenarios-and-recovery\">Failure Scenarios and Recovery</h4>\n<p>The storage engine must handle various failure scenarios gracefully, from clean shutdowns to unexpected power loss during active writes. Each scenario requires different recovery strategies to ensure data consistency and minimize data loss.</p>\n<p><strong>Clean Shutdown</strong>: When the storage engine receives a shutdown signal, it completes all in-flight compression operations and writes a final checkpoint before terminating. This provides a clean recovery state where the WAL contains no uncommitted data. Recovery from clean shutdown requires only loading the final checkpoint.</p>\n<p><strong>Crash During Compression</strong>: If the system crashes while compressing chunks, the compressed data may be incomplete or corrupted. Recovery detects this by validating chunk headers and checksums. Corrupted chunks are discarded, and the data is recovered from WAL entries instead. This may require replaying more WAL history but ensures data consistency.</p>\n<p><strong>Crash During WAL Write</strong>: A crash during WAL writing may leave a partially written entry at the end of the current segment. Recovery detects this using the entry checksums and treats the partial entry as the end of valid data. Subsequent entries (if any) are ignored, potentially losing the data from the incomplete write.</p>\n<p><strong>Disk Full During Operation</strong>: When disk space is exhausted, the storage engine stops accepting new samples and enters read-only mode. WAL writes are suspended to prevent further disk consumption. The system attempts to complete background garbage collection to free space before resuming normal operation. If garbage collection can&#39;t free sufficient space, the system remains in read-only mode until manual intervention.</p>\n<p>The recovery process includes extensive validation to detect and handle corruption in both WAL segments and checkpoint files. Checksums verify data integrity at multiple levels: individual WAL entries, segment boundaries, and complete checkpoint files. When corruption is detected, the system attempts to recover as much valid data as possible while clearly logging what data may have been lost.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Compression State Corruption During Concurrent Access</strong></p>\n<p>A common mistake is allowing multiple goroutines to modify the same compression chunk simultaneously. Gorilla compression maintains internal state about the previous timestamp and value to calculate deltas, and concurrent modifications corrupt this state. The symptoms are decompression failures or wildly incorrect values after decompression.</p>\n<p><strong>Why it&#39;s wrong</strong>: Compression algorithms like Gorilla rely on strict ordering of samples and consistent state across the compression sequence. Concurrent writes can interleave samples from different goroutines, violating the monotonic timestamp assumption and corrupting the delta calculations.</p>\n<p><strong>How to fix</strong>: Use mutex locks around each compression chunk during writes. The <code>ChunkBuilder</code> should include a <code>sync.Mutex</code> that protects the entire compress-and-append operation. Alternatively, use single-threaded compression with channels to serialize write access.</p>\n<p>⚠️ <strong>Pitfall: Index Cardinality Explosion from Naive Label Storage</strong></p>\n<p>Developers often create inverted indexes that store every possible label combination, leading to exponential memory growth. With labels like <code>{service, instance, method, status}</code>, the combinations explode quickly - 10 services × 100 instances × 10 methods × 5 statuses = 50,000 index entries.</p>\n<p><strong>Why it&#39;s wrong</strong>: Storing composite label combinations creates indexes that grow exponentially with label cardinality. Memory usage becomes unbounded, and index lookup performance degrades as hash tables become oversized.</p>\n<p><strong>How to fix</strong>: Create separate inverted indexes for each label name, not for label combinations. Store <code>service_name → [series IDs]</code> and <code>instance_name → [series IDs]</code> separately, then compute intersections during query time. This provides linear memory growth with label cardinality.</p>\n<p>⚠️ <strong>Pitfall: WAL Recovery Assumes Perfect Ordering</strong></p>\n<p>A frequent mistake is assuming that WAL entries are always perfectly ordered by timestamp during recovery. Network delays, clock drift, or buffering can cause samples to arrive and be written to the WAL in slightly different order than their actual timestamps.</p>\n<p><strong>Why it&#39;s wrong</strong>: Recovery code that assumes strict timestamp ordering will fail when replaying out-of-order samples. This can manifest as assertion failures, incorrect compression, or samples being assigned to wrong time windows during replay.</p>\n<p><strong>How to fix</strong>: During WAL replay, buffer samples in memory and sort them by timestamp before applying to compression chunks. Only apply samples to chunks when you&#39;re confident no earlier timestamp will arrive (e.g., after a time window has passed or during final recovery).</p>\n<p>⚠️ <strong>Pitfall: Retention Policies Delete Active Chunks</strong></p>\n<p>Developers sometimes implement retention policies that delete chunks based solely on the chunk creation time, not the timestamp of the data within the chunk. This can delete chunks containing recent samples that were written to older chunks due to delayed ingestion.</p>\n<p><strong>Why it&#39;s wrong</strong>: A chunk created yesterday might contain samples with timestamps from today if data arrived late. Deleting based on chunk creation time rather than sample timestamps causes data loss for delayed metrics.</p>\n<p><strong>How to fix</strong>: Examine the actual timestamp range within each chunk during retention policy evaluation. Only delete chunks where the maximum sample timestamp is older than the retention threshold. Include a safety margin to account for possible clock drift.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The storage engine requires careful coordination between compression algorithms, indexing structures, and persistence mechanisms. This implementation guidance provides concrete starting points for the core storage components while highlighting the areas where you&#39;ll implement the compression and lifecycle logic yourself.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Compression</td>\n<td>Custom bitstream with sync.Pool</td>\n<td>SIMD-optimized compression with cgo</td>\n</tr>\n<tr>\n<td>Indexing</td>\n<td>Go maps with RWMutex</td>\n<td>Radix trees or B+ trees</td>\n</tr>\n<tr>\n<td>Persistence</td>\n<td>Standard os.File with fsync</td>\n<td>mmap with explicit sync control</td>\n</tr>\n<tr>\n<td>WAL Format</td>\n<td>Binary encoding/gob</td>\n<td>Custom binary protocol</td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td>Mutex per chunk</td>\n<td>Lock-free data structures</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">internal</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">storage</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  storage.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">              ← Main StorageEngine implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  storage_test.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">         ← Integration tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  chunk</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunk.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">              ← Compression chunk implementation  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gorilla.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">            ← Gorilla compression algorithm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunk_test.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">         ← Compression algorithm tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  index</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inverted.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">           ← Inverted index implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cardinality.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">        ← Cardinality tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index_test.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">         ← Index operation tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  wal</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">                ← Write</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">ahead log</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    recovery.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">           ← Crash recovery logic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal_test.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">           ← WAL and recovery tests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  retention</span><span style=\"color:#F97583\">/</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    policy.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">             ← Retention policy evaluation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    compaction.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">         ← Background compaction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retention_test.</span><span style=\"color:#F97583\">go</span><span style=\"color:#E1E4E8\">     ← Lifecycle management tests</span></span></code></pre></div>\n\n<h4 id=\"storage-infrastructure-code\">Storage Infrastructure Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// storage.go - Complete infrastructure for storage operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageEngine manages time series persistence with compression and indexing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger      </span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    chunks      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CompressedChunk</span><span style=\"color:#6A737D\">  // series_id -> current chunk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    indexes     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">InvertedIndexes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WriteAheadLog</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shutdown    </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewStorageEngine creates a configured storage engine instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStorageEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#B392F0\"> Logger</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    se </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:   config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:   logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        chunks:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CompressedChunk</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        indexes:  </span><span style=\"color:#B392F0\">NewInvertedIndexes</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        shutdown: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    se.wal, err </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> NewWriteAheadLog</span><span style=\"color:#E1E4E8\">(config.data_directory, logger)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to initialize WAL: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> se.</span><span style=\"color:#B392F0\">recoverFromWAL</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"WAL recovery failed: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> se.</span><span style=\"color:#B392F0\">backgroundCompaction</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> se, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CompressedChunk represents a time series chunk with Gorilla compression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CompressedChunk</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seriesID     </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mint, maxt   </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">        // min/max timestamp in chunk</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samples      []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#6A737D\">       // compressed sample data  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sampleCount  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteAheadLog provides durable storage for incoming samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WriteAheadLog</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dir        </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    segments   []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger     </span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InvertedIndexes maintains label->series mappings for fast queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> InvertedIndexes</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metricNames  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#6A737D\">                    // metric -> series IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    labelValues  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#6A737D\">        // label_name -> value -> series IDs  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    series       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesMetadata</span><span style=\"color:#6A737D\">            // series_id -> metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesMetadata contains the complete label set and storage info for a series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SeriesMetadata</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID          </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels      </span><span style=\"color:#B392F0\">Labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MetricName  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ChunkRefs   []</span><span style=\"color:#B392F0\">ChunkRef</span><span style=\"color:#6A737D\">  // references to storage chunks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ChunkRef points to a specific chunk in persistent storage  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ChunkRef</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MinTime, MaxTime </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Offset          </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">   // file offset</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Length          </span><span style=\"color:#F97583\">int32</span><span style=\"color:#6A737D\">   // compressed size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-storage-logic-skeletons\">Core Storage Logic Skeletons</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Append stores samples for the specified time series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This is the main ingestion entry point called by the scrape engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">se </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Write samples to WAL before in-memory processing for durability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Group samples by series ID to batch processing per series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each series, find or create the current compression chunk</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Append samples to compression chunk using Gorilla algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If chunk reaches size limit, flush to persistent storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update inverted indexes with any new label values discovered</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return error if any step fails (WAL write, compression, indexing)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use se.mu.RLock() for reading, se.mu.Lock() for writing chunk map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Batch WAL writes for multiple samples to improve throughput</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Select retrieves time series matching the label matchers in the time range</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">se </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Select</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">matchers</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LabelMatcher</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">SeriesSet</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    se.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> se.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Use metric name matcher to get candidate series IDs from primary index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each remaining label matcher, get matching series IDs from label indexes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Compute intersection of all matcher results to get final series ID set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each matching series ID, load chunk references from series metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Filter chunk references to only those overlapping [start, end] time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return SeriesSet that can iterate through matching series and decompress chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Order matchers by selectivity (lowest cardinality first) for efficiency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sorted slice intersection for large result sets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"gorilla-compression-implementation-skeleton\">Gorilla Compression Implementation Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// gorilla.go - Gorilla compression algorithm implementation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> chunk</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/binary</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GorillaCompressor implements delta-of-delta timestamp and XOR value compression</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GorillaCompressor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buf              []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bitPos           </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Timestamp compression state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseTimestamp    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prevTimestamp    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prevDelta        </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Value compression state  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseValue        </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prevValue        </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prevLeadingZeros </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prevTrailingZeros </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewGorillaCompressor creates a new compressor with baseline values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewGorillaCompressor</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">baseTimestamp</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">baseValue</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GorillaCompressor</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize compressor with baseline timestamp and value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Allocate initial buffer for compressed bits (start with 1KB)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write baseline timestamp and value as uncompressed 64-bit values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Set initial state for delta-of-delta and XOR compression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Store baseline values in first 16 bytes for decompression</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddSample compresses and appends a timestamp-value pair</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">gc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GorillaCompressor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddSample</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">timestamp</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate timestamp delta from previous timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate delta-of-delta from previous delta</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Encode delta-of-delta using variable-length bit patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: XOR current value with previous value  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Encode XOR result using leading/trailing zero compression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update internal state for next sample (prev timestamp, value, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Expand buffer if needed to accommodate new bits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use bit manipulation helpers for variable-length encoding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Delta-of-delta of 0 encodes as single bit, others use longer patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CompressedData returns the final compressed chunk bytes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">gc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GorillaCompressor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CompressedData</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Finalize bit stream by padding to byte boundary if needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return copy of buffer truncated to actual used length  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Include header with baseline timestamp/value and sample count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Byte-align the final bit position for easier storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"wal-and-recovery-implementation-skeleton\">WAL and Recovery Implementation Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// wal.go - Write-ahead log for durability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> wal</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AppendSamples writes samples to WAL before in-memory processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">wal </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WriteAheadLog</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AppendSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> wal.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create WAL entry for each sample with type, series ID, timestamp, value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate CRC32 checksum for each entry to detect corruption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write entries to current WAL segment file  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call fsync() to ensure data reaches persistent storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If current segment exceeds size limit, rotate to new segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return error if any write or fsync operation fails</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Batch multiple samples into single write() call for efficiency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use binary.Write() for consistent cross-platform encoding</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// recoverFromWAL replays WAL entries to rebuild in-memory state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">se </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recoverFromWAL</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Find all WAL segment files in data directory sorted by sequence number</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Load most recent checkpoint file to get baseline state </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Replay WAL entries from checkpoint timestamp to end of latest segment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each entry, validate checksum and skip corrupted entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply valid entries to in-memory chunks and update indexes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Rebuild any derived state (label indexes, series registry)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Delete WAL segments that are fully incorporated into chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Buffer and sort samples by timestamp before applying to handle out-of-order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Log recovery progress for operational visibility</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the storage engine core:</p>\n<p><strong>Basic Functionality Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should pass tests for compression, indexing, and WAL recovery</span></span></code></pre></div>\n\n<p><strong>Manual Verification Steps:</strong></p>\n<ol>\n<li>Start storage engine and append 1000 samples across 10 different series</li>\n<li>Verify WAL file is created and contains expected number of entries  </li>\n<li>Query samples back using label matchers - should return correct data</li>\n<li>Stop process and restart - verify recovery loads all data correctly</li>\n<li>Wait for background compaction - verify compressed chunks are created</li>\n</ol>\n<p><strong>Performance Validation:</strong></p>\n<ul>\n<li>Should handle 10,000 samples/second ingestion rate</li>\n<li>Gorilla compression should achieve 5:1 compression ratio or better  </li>\n<li>Label queries should return results in &lt; 100ms for up to 10,000 series</li>\n<li>WAL recovery should complete in &lt; 30 seconds for 1GB of WAL data</li>\n</ul>\n<p><strong>Warning Signs:</strong></p>\n<ul>\n<li>Memory usage growing without bound (likely cardinality explosion)</li>\n<li>Compression ratio worse than 2:1 (implementation bug in Gorilla algorithm)</li>\n<li>Recovery taking &gt; 5 minutes (WAL segments not being cleaned up properly)</li>\n<li>Query timeouts on small datasets (inefficient index intersection)</li>\n</ul>\n<h2 id=\"promql-query-engine\">PromQL Query Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section directly corresponds to Milestone 4 (Query Engine) and integrates with all previous milestones by providing the query capabilities that retrieve and process the metrics data collected by the Scrape Engine (Milestone 2) and stored by the Time Series Storage Engine (Milestone 3) using the Metrics Data Model (Milestone 1).</p>\n</blockquote>\n<p>The PromQL query engine transforms user queries into results by parsing expressions, selecting time series, applying aggregations, and formatting output. This component sits at the top of our metrics system architecture, providing the interface through which users extract insights from collected observability data. The query engine must balance expressiveness with performance, supporting complex analytical queries while maintaining sub-second response times over millions of time series.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fquery-execution.svg\" alt=\"Query Execution Flow\"></p>\n<p>The query engine operates through five coordinated stages: expression parsing transforms text queries into executable plans, label selection filters the universe of available time series, aggregation operations combine data across dimensions, range processing handles time-windowed queries, and result formatting prepares output for consumption. Each stage must handle high cardinality gracefully while providing meaningful error messages when queries cannot be satisfied.</p>\n<h3 id=\"sql-for-time-series-mental-model-understanding-promql-through-database-query-analogies\">SQL for Time Series Mental Model: Understanding PromQL through database query analogies</h3>\n<p>Think of PromQL as <strong>SQL for time series data</strong>, where instead of selecting rows from tables, you&#39;re selecting and aggregating sequences of timestamped values. Just as SQL has <code>SELECT</code>, <code>FROM</code>, <code>WHERE</code>, <code>GROUP BY</code>, and aggregate functions, PromQL has metric selection, label filtering, time range specification, grouping operations, and mathematical functions. The key difference is that PromQL operates on the time dimension as a first-class concept, making it natural to ask questions like &quot;what was the average CPU usage over the last hour, grouped by service?&quot;</p>\n<p>In traditional SQL, you might write <code>SELECT AVG(cpu_percent) FROM metrics WHERE service=&#39;api&#39; AND timestamp &gt; NOW() - INTERVAL &#39;1 hour&#39;</code>. In PromQL, this becomes <code>avg_over_time(cpu_percent{service=&quot;api&quot;}[1h])</code>. The bracket notation <code>[1h]</code> selects a time range, the curly braces <code>{service=&quot;api&quot;}</code> provide label filtering (equivalent to WHERE clauses), and <code>avg_over_time()</code> aggregates across the time dimension rather than across rows.</p>\n<p>The <strong>metric name acts like a table name</strong> in SQL - it identifies the primary data source. Labels function like <strong>indexed columns</strong>, allowing efficient filtering and grouping operations. When you write <code>http_requests_total{method=&quot;GET&quot;, status=&quot;200&quot;}</code>, you&#39;re essentially saying &quot;FROM http_requests_total WHERE method=&#39;GET&#39; AND status=&#39;200&#39;&quot;. The time series database uses inverted indexes on labels just like SQL databases use B-tree indexes on columns.</p>\n<p><strong>Aggregation in PromQL differs from SQL aggregation</strong> because it operates across multiple dimensions simultaneously. SQL typically aggregates rows within groups, but PromQL aggregates both across time (within each series) and across series (within label groups). When you write <code>sum by (service) (rate(http_requests_total[5m]))</code>, you&#39;re first calculating the per-second rate for each individual time series over 5-minute windows, then summing those rates across all series that share the same <code>service</code> label value.</p>\n<p>The <strong>time dimension</strong> is what makes PromQL unique. Every query implicitly or explicitly operates over time ranges. An &quot;instant query&quot; like <code>cpu_usage{instance=&quot;server1&quot;}</code> asks for the most recent value, while a &quot;range query&quot; like <code>cpu_usage{instance=&quot;server1&quot;}[30m:1m]</code> asks for all values in the last 30 minutes at 1-minute resolution. This is similar to SQL window functions but built into the core query language.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: PromQL treats time as a fundamental dimension, not just another column. This makes temporal operations natural but requires different thinking than traditional SQL query planning.</p>\n</blockquote>\n<h3 id=\"query-parsing-lexical-analysis-and-ast-construction-for-promql-expressions\">Query Parsing: Lexical analysis and AST construction for PromQL expressions</h3>\n<p>The query parser transforms text-based PromQL expressions into an <strong>Abstract Syntax Tree (AST)</strong> that represents the logical structure of operations and their dependencies. This parsing process follows the standard compiler design pattern of lexical analysis (tokenization) followed by syntactic analysis (AST construction), but with domain-specific considerations for time series expressions, mathematical operators, and label selectors.</p>\n<p><strong>Lexical analysis</strong> (tokenization) breaks the input string into a sequence of meaningful tokens: metric names, label selectors, operators, function names, duration literals, and numeric constants. The lexer must handle PromQL-specific syntax like label selector curly braces <code>{job=&quot;api&quot;}</code>, range selectors with square brackets <code>[5m]</code>, and mathematical operators with appropriate precedence. Duration parsing requires special attention since PromQL supports units like <code>5m</code>, <code>2h</code>, <code>30s</code> that must be normalized to consistent internal representations.</p>\n<table>\n<thead>\n<tr>\n<th>Token Type</th>\n<th>Examples</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>IDENTIFIER</code></td>\n<td><code>http_requests_total</code>, <code>instance</code>, <code>job</code></td>\n<td>Metric names and label names</td>\n</tr>\n<tr>\n<td><code>STRING</code></td>\n<td><code>&quot;api&quot;</code>, <code>&quot;GET&quot;</code>, <code>&quot;/health&quot;</code></td>\n<td>Label values in quotes</td>\n</tr>\n<tr>\n<td><code>NUMBER</code></td>\n<td><code>123</code>, <code>3.14</code>, <code>1e6</code></td>\n<td>Numeric literals</td>\n</tr>\n<tr>\n<td><code>DURATION</code></td>\n<td><code>5m</code>, <code>2h</code>, <code>30s</code>, <code>1d</code></td>\n<td>Time duration specifiers</td>\n</tr>\n<tr>\n<td><code>OPERATOR</code></td>\n<td><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code>, <code>^</code></td>\n<td>Mathematical operators</td>\n</tr>\n<tr>\n<td><code>COMPARATOR</code></td>\n<td><code>==</code>, <code>!=</code>, <code>=~</code>, <code>!~</code></td>\n<td>Label matching operators</td>\n</tr>\n<tr>\n<td><code>AGGREGATOR</code></td>\n<td><code>sum</code>, <code>avg</code>, <code>max</code>, <code>min</code>, <code>count</code></td>\n<td>Aggregation function names</td>\n</tr>\n<tr>\n<td><code>FUNCTION</code></td>\n<td><code>rate</code>, <code>increase</code>, <code>histogram_quantile</code></td>\n<td>Built-in function names</td>\n</tr>\n<tr>\n<td><code>LBRACE</code></td>\n<td><code>{</code></td>\n<td>Start of label selector</td>\n</tr>\n<tr>\n<td><code>RBRACE</code></td>\n<td><code>}</code></td>\n<td>End of label selector</td>\n</tr>\n<tr>\n<td><code>LBRACKET</code></td>\n<td><code>[</code></td>\n<td>Start of range selector</td>\n</tr>\n<tr>\n<td><code>RBRACKET</code></td>\n<td><code>]</code></td>\n<td>End of range selector</td>\n</tr>\n<tr>\n<td><code>LPAREN</code></td>\n<td><code>(</code></td>\n<td>Function call or grouping start</td>\n</tr>\n<tr>\n<td><code>RPAREN</code></td>\n<td><code>)</code></td>\n<td>Function call or grouping end</td>\n</tr>\n</tbody></table>\n<p><strong>Syntactic analysis</strong> builds the AST by recognizing grammar patterns and enforcing operator precedence. PromQL expressions follow a hierarchy where function calls bind most tightly, followed by mathematical operators (with standard precedence: <code>^</code> &gt; <code>*</code>, <code>/</code>, <code>%</code> &gt; <code>+</code>, <code>-</code>), then aggregation operations, and finally binary operators between entire expressions. The parser must handle both instant queries (returning single values per series) and range queries (returning time-windowed data).</p>\n<p>The <strong>AST node structure</strong> represents each operation type with specific node classes that capture the operation&#39;s semantics and operands:</p>\n<table>\n<thead>\n<tr>\n<th>Node Type</th>\n<th>Purpose</th>\n<th>Children</th>\n<th>Attributes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MetricSelectorNode</code></td>\n<td>Selects time series by name and labels</td>\n<td>None</td>\n<td><code>MetricName string</code>, <code>LabelMatchers []LabelMatcher</code></td>\n</tr>\n<tr>\n<td><code>RangeSelectorNode</code></td>\n<td>Applies time range to metric selector</td>\n<td><code>MetricSelectorNode</code></td>\n<td><code>Duration time.Duration</code>, <code>Offset time.Duration</code></td>\n</tr>\n<tr>\n<td><code>FunctionCallNode</code></td>\n<td>Applies function to arguments</td>\n<td>Variable argument nodes</td>\n<td><code>FunctionName string</code>, <code>Args []ASTNode</code></td>\n</tr>\n<tr>\n<td><code>AggregationNode</code></td>\n<td>Groups and aggregates series</td>\n<td>Expression node</td>\n<td><code>Operation string</code>, <code>GroupBy []string</code>, <code>Without []string</code></td>\n</tr>\n<tr>\n<td><code>BinaryOperationNode</code></td>\n<td>Mathematical operation between expressions</td>\n<td>Left and right expression nodes</td>\n<td><code>Operator string</code>, <code>Matching *VectorMatching</code></td>\n</tr>\n<tr>\n<td><code>NumberLiteralNode</code></td>\n<td>Numeric constant</td>\n<td>None</td>\n<td><code>Value float64</code></td>\n</tr>\n<tr>\n<td><code>StringLiteralNode</code></td>\n<td>String constant</td>\n<td>None</td>\n<td><code>Value string</code></td>\n</tr>\n</tbody></table>\n<p><strong>Error handling during parsing</strong> must provide meaningful diagnostics that help users understand syntax problems. Common parsing errors include mismatched brackets in label selectors, invalid duration formats, undefined function names, and operator precedence confusion. The parser should report the exact character position and suggest corrections when possible.</p>\n<blockquote>\n<p><strong>Decision: Recursive Descent vs. Parser Generator</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose parsing strategy for PromQL expressions</li>\n<li><strong>Options Considered</strong>: Hand-written recursive descent, YACC/Bison parser generator, PEG parser</li>\n<li><strong>Decision</strong>: Hand-written recursive descent parser</li>\n<li><strong>Rationale</strong>: PromQL grammar is relatively simple, recursive descent provides better error messages and easier debugging, eliminates external tool dependencies</li>\n<li><strong>Consequences</strong>: More code to maintain but complete control over error reporting and parsing behavior</li>\n</ul>\n</blockquote>\n<p>The <strong>parsing algorithm</strong> follows these steps for each input query:</p>\n<ol>\n<li><strong>Initialize lexer</strong> with input string and begin tokenization</li>\n<li><strong>Parse primary expression</strong> starting with metric selector, number literal, or parenthesized expression</li>\n<li><strong>Handle range selector</strong> if current token is <code>[</code> - parse duration and optional offset</li>\n<li><strong>Process function calls</strong> if primary expression is followed by <code>(</code> - parse argument list recursively</li>\n<li><strong>Parse mathematical operators</strong> according to precedence rules - left-associative except for <code>^</code></li>\n<li><strong>Handle aggregation operations</strong> if expression starts with aggregation keyword - parse grouping clauses</li>\n<li><strong>Validate semantic constraints</strong> - ensure range selectors only applied to metric selectors, function argument types match</li>\n<li><strong>Return AST root node</strong> or parsing error with location information</li>\n</ol>\n<p><strong>Common parsing pitfalls</strong> include incorrect operator precedence handling, failure to validate semantic constraints during parsing (allowing syntactically valid but meaningless expressions), and poor error recovery that cascades single mistakes into many error messages.</p>\n<p>⚠️ <strong>Pitfall: Precedence Confusion</strong>\nMany developers incorrectly implement operator precedence, leading to expressions like <code>2 + 3 * 4</code> being parsed as <code>(2 + 3) * 4</code> instead of <code>2 + (3 * 4)</code>. Always implement precedence through recursive parsing methods where higher-precedence operators are parsed by deeper recursion levels, ensuring natural left-to-right evaluation with correct operator binding.</p>\n<h3 id=\"label-selector-engine-exact-regex-and-inequality-matching-for-filtering-time-series\">Label Selector Engine: Exact, regex, and inequality matching for filtering time series</h3>\n<p>The label selector engine filters the universe of available time series down to those matching specific label criteria, acting as the <strong>WHERE clause</strong> of PromQL queries. This component must efficiently handle four types of label matching: exact equality, inequality, regular expression matching, and negative regular expression matching. Performance is critical since label selection often processes millions of time series and determines the working set size for subsequent aggregation operations.</p>\n<p><strong>Label matching semantics</strong> define how series are included or excluded based on their label values. The four matcher types each serve different filtering use cases and have distinct performance characteristics:</p>\n<table>\n<thead>\n<tr>\n<th>Matcher Type</th>\n<th>Syntax</th>\n<th>Example</th>\n<th>Use Case</th>\n<th>Performance Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Exact Match</td>\n<td><code>label=&quot;value&quot;</code></td>\n<td><code>job=&quot;api&quot;</code></td>\n<td>Filter by known label values</td>\n<td>Fastest - direct index lookup</td>\n</tr>\n<tr>\n<td>Not Equal</td>\n<td><code>label!=&quot;value&quot;</code></td>\n<td><code>status!=&quot;200&quot;</code></td>\n<td>Exclude specific values</td>\n<td>Moderate - inverse index scan</td>\n</tr>\n<tr>\n<td>Regex Match</td>\n<td><code>label=~&quot;pattern&quot;</code></td>\n<td><code>instance=~&quot;web-.*&quot;</code></td>\n<td>Pattern-based inclusion</td>\n<td>Slowest - requires regex evaluation</td>\n</tr>\n<tr>\n<td>Negative Regex</td>\n<td><code>label!~&quot;pattern&quot;</code></td>\n<td><code>path!~&quot;/debug/.*&quot;</code></td>\n<td>Pattern-based exclusion</td>\n<td>Slowest - requires regex evaluation</td>\n</tr>\n</tbody></table>\n<p>The <strong>matching algorithm</strong> processes label selectors against the inverted indexes built by the storage engine. For exact matches, the engine performs direct hash table lookups in the <code>labelValues</code> index to retrieve the list of series IDs containing that label-value pair. For inequality matches, the engine retrieves the exact match set and computes its complement against all series containing that label name.</p>\n<p><strong>Regular expression matching</strong> requires compiling regex patterns and evaluating them against all possible values for the specified label name. This is inherently expensive since it cannot use pre-built indexes effectively. The selector engine maintains a <strong>regex compilation cache</strong> to avoid recompiling the same patterns repeatedly, and applies optimizations like prefix extraction when patterns start with literal strings.</p>\n<table>\n<thead>\n<tr>\n<th>Label Selection Step</th>\n<th>Operation</th>\n<th>Data Structure Used</th>\n<th>Time Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Exact Match Lookup</td>\n<td><code>labelValues[labelName][labelValue]</code></td>\n<td>Hash map of hash maps</td>\n<td>O(1) average case</td>\n</tr>\n<tr>\n<td>Inequality Filtering</td>\n<td>Set complement operation</td>\n<td>Bitmap or hash set</td>\n<td>O(n) where n = series count</td>\n</tr>\n<tr>\n<td>Regex Compilation</td>\n<td><code>regexp.Compile(pattern)</code></td>\n<td>Cached compiled regex</td>\n<td>O(m) where m = pattern complexity</td>\n</tr>\n<tr>\n<td>Regex Evaluation</td>\n<td>Pattern matching against all values</td>\n<td>Linear scan with regex</td>\n<td>O(k*m) where k = unique values</td>\n</tr>\n</tbody></table>\n<p>The <strong>series intersection algorithm</strong> combines multiple label matchers using set operations. When a query contains multiple label selectors like <code>{job=&quot;api&quot;, status!=&quot;500&quot;, instance=~&quot;web-.*&quot;}</code>, the engine must find series that satisfy ALL conditions simultaneously. The algorithm processes matchers in optimal order, starting with the most selective (smallest result set) to minimize subsequent work.</p>\n<p><strong>Intersection processing</strong> follows these steps:</p>\n<ol>\n<li><strong>Evaluate each matcher independently</strong> to get per-matcher series ID sets</li>\n<li><strong>Sort matchers by selectivity</strong> - exact matches first, then inequalities, finally regex patterns</li>\n<li><strong>Initialize result set</strong> with most selective matcher&#39;s series IDs</li>\n<li><strong>Intersect remaining matchers</strong> - remove series IDs that don&#39;t appear in subsequent matcher results</li>\n<li><strong>Handle special cases</strong> - empty label selectors match all series, contradictory selectors return empty sets</li>\n<li><strong>Return final series ID list</strong> sorted for consistent downstream processing</li>\n</ol>\n<p><strong>Label name filtering</strong> handles cases where queries filter on labels that may not exist on all series. A matcher like <code>{version=&quot;1.2&quot;}</code> should only consider series that actually have a <code>version</code> label, ignoring series where this label is absent entirely. The absence of a label is semantically different from a label with an empty string value.</p>\n<blockquote>\n<p><strong>Decision: Bitmap vs. Hash Set for Series Intersection</strong></p>\n<ul>\n<li><strong>Context</strong>: Need efficient set operations for combining label matcher results</li>\n<li><strong>Options Considered</strong>: Hash sets, sorted arrays with binary search, bitmap arrays</li>\n<li><strong>Decision</strong>: Hash sets for low cardinality, bitmap arrays for high cardinality with cutoff at 10,000 series</li>\n<li><strong>Rationale</strong>: Hash sets provide O(1) membership testing but have memory overhead; bitmaps are more space-efficient for dense ID ranges but require more memory for sparse ranges</li>\n<li><strong>Consequences</strong>: Adaptive algorithm provides good performance across different cardinality ranges but adds implementation complexity</li>\n</ul>\n</blockquote>\n<p><strong>Performance optimizations</strong> for label selection include:</p>\n<ul>\n<li><strong>Early termination</strong>: Stop processing additional matchers if intermediate result set becomes empty</li>\n<li><strong>Matcher reordering</strong>: Process most selective matchers first to minimize subsequent intersection work</li>\n<li><strong>Regex optimization</strong>: Extract literal prefixes from regex patterns to use index lookups before pattern matching</li>\n<li><strong>Cache compiled regexes</strong>: Avoid recompiling the same patterns across multiple query executions</li>\n<li><strong>Parallel evaluation</strong>: Process independent matchers concurrently when result sets are large</li>\n</ul>\n<p><strong>Common label selection pitfalls</strong> include incorrect handling of missing labels (treating absence as empty string), inefficient regex patterns that can&#39;t use index optimizations, and failure to validate regex syntax at parse time leading to runtime compilation errors.</p>\n<p>⚠️ <strong>Pitfall: Missing Label Semantics</strong>\nA common mistake is treating missing labels as empty strings. The query <code>{version=&quot;&quot;}</code> should match series with an empty <code>version</code> label, while <code>{version!=&quot;stable&quot;}</code> should match series with any <code>version</code> value except &quot;stable&quot; but NOT series missing the <code>version</code> label entirely. Always check for label existence before value comparison.</p>\n<h3 id=\"aggregation-operations-sum-average-percentile-and-grouping-operations-across-label-dimensions\">Aggregation Operations: Sum, average, percentile, and grouping operations across label dimensions</h3>\n<p>Aggregation operations combine multiple time series into fewer series by applying mathematical functions across specified label dimensions. Think of aggregation as <strong>GROUP BY operations in SQL</strong>, where you collapse many rows into summary values, except PromQL aggregates across both the series dimension (combining multiple time series) and optionally the time dimension (combining multiple timestamps within each series).</p>\n<p>The <strong>aggregation mental model</strong> treats each time series as a vector of timestamped values, and aggregation as matrix operations that combine these vectors according to grouping rules. When you write <code>sum by (service) (cpu_usage)</code>, you&#39;re partitioning all <code>cpu_usage</code> time series into groups based on their <code>service</code> label value, then computing the element-wise sum within each group to produce one output series per unique service.</p>\n<p><strong>Grouping semantics</strong> determine which series get combined together. PromQL supports two grouping modes: <code>by</code> (include specified labels in output) and <code>without</code> (exclude specified labels from output). The grouping operation creates <strong>aggregation groups</strong> where each group contains series that share identical values for the grouping labels.</p>\n<table>\n<thead>\n<tr>\n<th>Grouping Mode</th>\n<th>Syntax</th>\n<th>Example</th>\n<th>Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Group By</td>\n<td><code>sum by (label1, label2) (metric)</code></td>\n<td><code>sum by (service) (cpu_usage)</code></td>\n<td>Output series contain only specified labels</td>\n</tr>\n<tr>\n<td>Group Without</td>\n<td><code>sum without (label1, label2) (metric)</code></td>\n<td><code>sum without (instance) (memory_usage)</code></td>\n<td>Output series exclude specified labels</td>\n</tr>\n<tr>\n<td>No Grouping</td>\n<td><code>sum(metric)</code></td>\n<td><code>sum(active_connections)</code></td>\n<td>All series combined into single output series</td>\n</tr>\n</tbody></table>\n<p>The <strong>aggregation algorithm</strong> processes series in groups, applying the aggregation function to corresponding timestamps across all series within each group. For a timestamp T, the aggregator collects all values at timestamp T from series in the group, applies the mathematical function (sum, average, max, etc.), and produces a single output value for that timestamp.</p>\n<table>\n<thead>\n<tr>\n<th>Aggregation Function</th>\n<th>Mathematical Operation</th>\n<th>Null Value Handling</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>sum</code></td>\n<td>Add all values</td>\n<td>Skip missing values</td>\n<td>Total resource usage, request counts</td>\n</tr>\n<tr>\n<td><code>avg</code></td>\n<td>Sum divided by count</td>\n<td>Skip missing values</td>\n<td>Average response times, utilization rates</td>\n</tr>\n<tr>\n<td><code>max</code></td>\n<td>Largest value</td>\n<td>Skip missing values</td>\n<td>Peak resource usage, worst-case latencies</td>\n</tr>\n<tr>\n<td><code>min</code></td>\n<td>Smallest value</td>\n<td>Skip missing values</td>\n<td>Best-case performance, minimum availability</td>\n</tr>\n<tr>\n<td><code>count</code></td>\n<td>Number of series</td>\n<td>Count series with any value</td>\n<td>Number of active instances, error rate calculation</td>\n</tr>\n<tr>\n<td><code>stddev</code></td>\n<td>Standard deviation</td>\n<td>Skip missing values</td>\n<td>Performance consistency, outlier detection</td>\n</tr>\n<tr>\n<td><code>quantile</code></td>\n<td>Percentile calculation</td>\n<td>Skip missing values</td>\n<td>SLA monitoring, latency distribution analysis</td>\n</tr>\n</tbody></table>\n<p><strong>Quantile aggregation</strong> deserves special attention because it requires maintaining the full distribution of values rather than computing incremental statistics. The <code>quantile(0.95, metric)</code> function needs all individual values at each timestamp to determine the 95th percentile, making it more memory-intensive than functions like sum or average that can be computed incrementally.</p>\n<p><strong>Implementation steps</strong> for aggregation processing:</p>\n<ol>\n<li><strong>Parse grouping specification</strong> - extract <code>by</code> or <code>without</code> label lists from AST</li>\n<li><strong>Group series by label values</strong> - create aggregation groups based on grouping rules</li>\n<li><strong>Align timestamps across series</strong> - ensure all series in each group have consistent timestamp alignment</li>\n<li><strong>Apply aggregation function</strong> - for each timestamp, collect values from all series in group and compute result</li>\n<li><strong>Handle missing values</strong> - skip series with no value at specific timestamps or use last-observed-value</li>\n<li><strong>Construct output series</strong> - create result time series with group labels and aggregated values</li>\n<li><strong>Sort output consistently</strong> - ensure deterministic ordering for reproducible results</li>\n</ol>\n<p><strong>Memory management</strong> during aggregation requires careful attention since large aggregation groups can consume significant memory. The aggregator processes timestamps sequentially rather than loading entire series into memory, using a sliding window approach for functions that need historical context.</p>\n<p><strong>Label handling</strong> in aggregation output requires combining the labels from input series according to grouping rules. For <code>sum by (service) (http_requests{service=&quot;api&quot;, instance=&quot;web-1&quot;, method=&quot;GET&quot;})</code>, the output series contains only <code>{service=&quot;api&quot;}</code>, dropping the <code>instance</code> and <code>method</code> labels. The aggregator must compute the Cartesian product of all possible label value combinations for the grouping labels.</p>\n<table>\n<thead>\n<tr>\n<th>Input Series</th>\n<th>Labels</th>\n<th>Grouping: by (service, status)</th>\n<th>Output Group Key</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Series 1</td>\n<td><code>{service=&quot;api&quot;, status=&quot;200&quot;, instance=&quot;web-1&quot;}</code></td>\n<td>service=api, status=200</td>\n<td><code>{service=&quot;api&quot;, status=&quot;200&quot;}</code></td>\n</tr>\n<tr>\n<td>Series 2</td>\n<td><code>{service=&quot;api&quot;, status=&quot;200&quot;, instance=&quot;web-2&quot;}</code></td>\n<td>service=api, status=200</td>\n<td><code>{service=&quot;api&quot;, status=&quot;200&quot;}</code></td>\n</tr>\n<tr>\n<td>Series 3</td>\n<td><code>{service=&quot;api&quot;, status=&quot;500&quot;, instance=&quot;web-1&quot;}</code></td>\n<td>service=api, status=500</td>\n<td><code>{service=&quot;api&quot;, status=&quot;500&quot;}</code></td>\n</tr>\n<tr>\n<td>Series 4</td>\n<td><code>{service=&quot;db&quot;, status=&quot;200&quot;, instance=&quot;db-1&quot;}</code></td>\n<td>service=db, status=200</td>\n<td><code>{service=&quot;db&quot;, status=&quot;200&quot;}</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Streaming vs. Batch Aggregation</strong></p>\n<ul>\n<li><strong>Context</strong>: Choose processing model for aggregating large numbers of time series</li>\n<li><strong>Options Considered</strong>: Load all data then aggregate, stream processing with fixed memory, hybrid approach</li>\n<li><strong>Decision</strong>: Streaming aggregation with timestamp-aligned windows</li>\n<li><strong>Rationale</strong>: Provides bounded memory usage regardless of input size, enables processing datasets larger than available RAM, maintains low latency for small aggregations</li>\n<li><strong>Consequences</strong>: More complex implementation with windowing logic but scales to arbitrary dataset sizes without memory exhaustion</li>\n</ul>\n</blockquote>\n<p><strong>Performance considerations</strong> for aggregation include:</p>\n<ul>\n<li><strong>Series cardinality</strong>: Aggregations over high-cardinality labels create many output groups, increasing memory usage</li>\n<li><strong>Timestamp alignment</strong>: Series with misaligned timestamps require interpolation or bucketing to aggregate consistently</li>\n<li><strong>Lazy evaluation</strong>: Defer aggregation computation until results are actually needed by subsequent operations</li>\n<li><strong>Parallel processing</strong>: Process independent aggregation groups concurrently when group count is high</li>\n<li><strong>Memory pooling</strong>: Reuse buffers for intermediate calculations to reduce garbage collection pressure</li>\n</ul>\n<p><strong>Common aggregation mistakes</strong> include incorrect null value handling (treating missing data as zero instead of skipping it), memory exhaustion on high-cardinality aggregations, and timestamp alignment issues when series have different scrape intervals.</p>\n<p>⚠️ <strong>Pitfall: High-Cardinality Aggregation Groups</strong>\nAggregating <code>by (instance_id)</code> where <code>instance_id</code> is a UUID will create one output series per instance, potentially millions of series. This defeats the purpose of aggregation and can cause memory exhaustion. Always aggregate by low-cardinality labels like service names, not high-cardinality identifiers like instance IDs or request IDs.</p>\n<h3 id=\"range-query-execution-retrieving-and-interpolating-data-points-across-time-windows\">Range Query Execution: Retrieving and interpolating data points across time windows</h3>\n<p>Range queries retrieve time series data across specified time windows, returning multiple data points per series rather than single instant values. Think of range queries as <strong>time-windowed SELECT statements</strong> that scan horizontally across the time dimension, collecting all data points between start and end timestamps at regular step intervals. The range query executor must handle timestamp alignment, missing data interpolation, and efficient data retrieval from the underlying storage engine.</p>\n<p><strong>Range query semantics</strong> define three key parameters: start time (beginning of time window), end time (end of time window), and step duration (resolution of output data points). A range query like <code>/api/v1/query_range?query=cpu_usage&amp;start=1609459200&amp;end=1609462800&amp;step=60</code> requests CPU usage data from timestamp 1609459200 to 1609462800 with 60-second resolution, producing one output value per series every minute.</p>\n<p>The <strong>query execution model</strong> operates differently from instant queries because it must process multiple timestamps sequentially. Instead of asking &quot;what is the current value?&quot;, range queries ask &quot;what were all the values between time A and time B at intervals of N seconds?&quot;. This requires coordinating between timestamp generation (creating the output timeline) and value retrieval (fetching data from storage).</p>\n<table>\n<thead>\n<tr>\n<th>Range Query Component</th>\n<th>Purpose</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time Range Parser</td>\n<td>Extract start/end/step from parameters</td>\n<td>Query string parameters</td>\n<td>Parsed time boundaries</td>\n</tr>\n<tr>\n<td>Step Generator</td>\n<td>Create output timestamp sequence</td>\n<td>Start, end, step duration</td>\n<td>Array of query timestamps</td>\n</tr>\n<tr>\n<td>Series Selector</td>\n<td>Find matching time series</td>\n<td>Label matchers</td>\n<td>List of series IDs</td>\n</tr>\n<tr>\n<td>Data Retriever</td>\n<td>Fetch samples from storage</td>\n<td>Series IDs, time range</td>\n<td>Raw sample data</td>\n</tr>\n<tr>\n<td>Value Interpolator</td>\n<td>Fill gaps in data</td>\n<td>Sparse samples, query timestamps</td>\n<td>Dense timestamp-value pairs</td>\n</tr>\n<tr>\n<td>Result Formatter</td>\n<td>Structure output data</td>\n<td>Dense data, metadata</td>\n<td>JSON/HTTP response</td>\n</tr>\n</tbody></table>\n<p><strong>Timestamp alignment</strong> ensures that output data points appear at regular intervals regardless of when the underlying samples were actually collected. If the query requests data every 60 seconds starting at 12:00:00, the output timestamps should be 12:00:00, 12:01:00, 12:02:00, etc., even if the scraped samples occurred at 12:00:15, 12:01:03, 12:02:21, etc.</p>\n<p>The <strong>alignment algorithm</strong> follows these steps:</p>\n<ol>\n<li><strong>Generate query timeline</strong> - create sequence of timestamps from start to end at step intervals</li>\n<li><strong>Retrieve raw sample data</strong> - fetch all samples for selected series within the time range</li>\n<li><strong>Sort samples by timestamp</strong> - ensure chronological ordering for interpolation</li>\n<li><strong>Align to query timeline</strong> - for each query timestamp, find the closest sample within tolerance</li>\n<li><strong>Interpolate missing values</strong> - use last-observed-value or linear interpolation for gaps</li>\n<li><strong>Handle staleness</strong> - mark values as stale if no recent sample exists within staleness threshold</li>\n<li><strong>Format output matrix</strong> - structure results as matrix with consistent timestamp alignment</li>\n</ol>\n<p><strong>Missing data handling</strong> is crucial because real monitoring systems have gaps due to network failures, service restarts, or scraping errors. The range query executor must distinguish between three scenarios: temporarily missing data (use interpolation), permanently missing data (return null), and series that don&#39;t exist during part of the time range (return partial results).</p>\n<table>\n<thead>\n<tr>\n<th>Missing Data Scenario</th>\n<th>Detection Criteria</th>\n<th>Handling Strategy</th>\n<th>Output Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Recent Gap</td>\n<td>No sample within staleness threshold</td>\n<td>Return null value</td>\n<td>Gaps in output timeline</td>\n</tr>\n<tr>\n<td>Historical Gap</td>\n<td>Sample exists before/after gap</td>\n<td>Last-observed-value interpolation</td>\n<td>Filled with previous value</td>\n</tr>\n<tr>\n<td>Series Creation</td>\n<td>First sample after query start time</td>\n<td>Partial series data</td>\n<td>Null values before first sample</td>\n</tr>\n<tr>\n<td>Series Deletion</td>\n<td>Last sample before query end time</td>\n<td>Partial series data</td>\n<td>Null values after last sample</td>\n</tr>\n<tr>\n<td>Complete Absence</td>\n<td>No samples in entire time range</td>\n<td>Empty series</td>\n<td>All null values</td>\n</tr>\n</tbody></table>\n<p><strong>Storage integration</strong> requires efficient range scans that minimize disk I/O and decompression overhead. The range query executor works with the storage engine&#39;s <code>Select(start, end, matchers)</code> method to retrieve only relevant data, avoiding the cost of scanning unrelated time series or unnecessary time ranges.</p>\n<p><strong>Interpolation strategies</strong> determine how to fill gaps between actual sample timestamps and desired query timestamps. The most common approach is <strong>last-observed-value</strong> interpolation, where each query timestamp receives the value of the most recent actual sample. This reflects the reality that metrics typically represent ongoing states (like CPU usage) that remain valid until explicitly updated.</p>\n<blockquote>\n<p><strong>Decision: Last-Observed-Value vs. Linear Interpolation</strong></p>\n<ul>\n<li><strong>Context</strong>: Choose interpolation strategy for aligning samples to query timestamps</li>\n<li><strong>Options Considered</strong>: Nearest neighbor, last-observed-value, linear interpolation, no interpolation</li>\n<li><strong>Decision</strong>: Last-observed-value with 5-minute staleness threshold</li>\n<li><strong>Rationale</strong>: Reflects semantic meaning of monitoring metrics which represent current state until updated; linear interpolation implies continuous change which is incorrect for many metrics like error counts</li>\n<li><strong>Consequences</strong>: More accurate representation of monitoring data but can show &quot;stair-step&quot; graphs instead of smooth curves</li>\n</ul>\n</blockquote>\n<p><strong>Performance optimizations</strong> for range queries include:</p>\n<ul>\n<li><strong>Parallel series processing</strong>: Retrieve data for multiple series concurrently when series count is high</li>\n<li><strong>Chunk boundary alignment</strong>: Align query ranges with storage chunk boundaries to minimize decompression overhead</li>\n<li><strong>Result streaming</strong>: Begin returning results before all data is processed for large queries</li>\n<li><strong>Memory management</strong>: Process series in batches to avoid loading entire result set into memory</li>\n<li><strong>Query caching</strong>: Cache results for repeated range queries over the same time periods</li>\n</ul>\n<p><strong>Query complexity management</strong> prevents resource exhaustion from overly broad range queries. Limits include maximum time range duration, maximum number of series that can be processed, and maximum number of output data points. A query requesting 1-second resolution over 30 days would produce 2.6 million data points per series, potentially overwhelming the system.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Limit</th>\n<th>Default Value</th>\n<th>Purpose</th>\n<th>Enforcement Point</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Max Range Duration</td>\n<td>365 days</td>\n<td>Prevent excessive time spans</td>\n<td>Query validation</td>\n</tr>\n<tr>\n<td>Max Series Count</td>\n<td>10,000</td>\n<td>Prevent high-cardinality explosions</td>\n<td>After label selection</td>\n</tr>\n<tr>\n<td>Max Sample Count</td>\n<td>50 million</td>\n<td>Prevent memory exhaustion</td>\n<td>Before data retrieval</td>\n</tr>\n<tr>\n<td>Query Timeout</td>\n<td>30 seconds</td>\n<td>Prevent long-running queries</td>\n<td>HTTP handler level</td>\n</tr>\n</tbody></table>\n<p><strong>Range query processing pipeline</strong> coordinates these steps:</p>\n<ol>\n<li><strong>Parse and validate parameters</strong> - extract start/end/step, validate ranges and limits</li>\n<li><strong>Execute instant query parsing</strong> - parse PromQL expression into AST</li>\n<li><strong>Generate evaluation timestamps</strong> - create array of timestamps at step intervals</li>\n<li><strong>For each timestamp</strong>: execute instant query evaluation to get point-in-time results</li>\n<li><strong>Collect timestamp-value matrices</strong> - accumulate results across all evaluation points</li>\n<li><strong>Format as range query response</strong> - structure output with series metadata and value matrices</li>\n</ol>\n<p><strong>Error handling</strong> in range queries must address scenarios like storage unavailability during long-running queries, memory exhaustion from overly large result sets, and timeout expiration during processing. The executor should provide partial results when possible rather than failing entirely.</p>\n<p>⚠️ <strong>Pitfall: Step Interval Too Small</strong>\nSetting step intervals much smaller than the underlying scrape interval (e.g., 1-second steps when data is scraped every 15 seconds) produces misleading results with excessive interpolation. The step should generally be no smaller than the scrape interval to avoid artificial data smoothing. Always validate that step &gt;= scrape_interval for meaningful results.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The PromQL query engine requires careful coordination between parsing, execution, and result formatting components. This implementation focuses on building a working query engine that can handle the core PromQL operations while maintaining good performance characteristics.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Expression Parsing</td>\n<td>Hand-written recursive descent</td>\n<td>ANTLR grammar generator</td>\n</tr>\n<tr>\n<td>Regular Expressions</td>\n<td>Go&#39;s <code>regexp</code> package</td>\n<td>RE2 library for better performance</td>\n</tr>\n<tr>\n<td>JSON Serialization</td>\n<td>Go&#39;s <code>encoding/json</code></td>\n<td>Custom JSON streaming for large results</td>\n</tr>\n<tr>\n<td>HTTP API</td>\n<td>Go&#39;s <code>net/http</code> with custom handlers</td>\n<td>Gorilla Mux for advanced routing</td>\n</tr>\n<tr>\n<td>Concurrent Processing</td>\n<td>Go routines with sync primitives</td>\n<td>Worker pool pattern with channels</td>\n</tr>\n<tr>\n<td>Caching</td>\n<td>Simple <code>sync.Map</code> for query cache</td>\n<td>LRU cache with TTL expiration</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/query/\n  engine.go              ← main QueryEngine type and coordination\n  engine_test.go         ← end-to-end query tests\n  parser/\n    parser.go            ← PromQL expression parsing\n    ast.go               ← AST node type definitions\n    lexer.go             ← tokenization and lexical analysis\n    parser_test.go       ← parsing unit tests\n  selector/\n    selector.go          ← label matching and series selection\n    matcher.go           ← individual label matcher implementations\n    selector_test.go     ← selection logic tests\n  aggregation/\n    aggregator.go        ← aggregation function implementations\n    grouping.go          ← series grouping logic\n    functions.go         ← built-in aggregation functions\n    aggregation_test.go  ← aggregation correctness tests\n  execution/\n    instant.go           ← instant query execution\n    range.go             ← range query execution\n    interpolation.go     ← value interpolation and alignment\n    execution_test.go    ← query execution tests</code></pre></div>\n\n<p><strong>Core Query Engine Infrastructure (Complete):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryEngine coordinates parsing, execution, and formatting of PromQL queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage      </span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExpressionParser</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    selector     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LabelSelector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    aggregator   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Aggregator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config       </span><span style=\"color:#B392F0\">QueryConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryResult represents the output of query execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResultType </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"resultType\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Result     </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}           </span><span style=\"color:#9ECBFF\">`json:\"result\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Warnings   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"warnings,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InstantQueryResult represents point-in-time query results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> InstantQueryResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metric </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"metric\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value  [</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}    </span><span style=\"color:#9ECBFF\">`json:\"value\"`</span><span style=\"color:#6A737D\"> // [timestamp, value]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RangeQueryResult represents time-windowed query results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RangeQueryResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metric </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">   `json:\"metric\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Values [][]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}     </span><span style=\"color:#9ECBFF\">`json:\"values\"`</span><span style=\"color:#6A737D\"> // [[timestamp, value], ...]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewQueryEngine creates a configured query engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewQueryEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> QueryConfig</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:      storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser:       </span><span style=\"color:#B392F0\">NewExpressionParser</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        selector:     </span><span style=\"color:#B392F0\">NewLabelSelector</span><span style=\"color:#E1E4E8\">(storage),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        aggregator:   </span><span style=\"color:#B392F0\">NewAggregator</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:       config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryTimeout: config.query_timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteInstantQuery processes point-in-time PromQL queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteInstantQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">evalTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(ctx, e.queryTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse expression into AST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expr, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> e.parser.</span><span style=\"color:#B392F0\">ParseExpression</span><span style=\"color:#E1E4E8\">(query)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"parse error: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Execute expression evaluation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">evaluateExpression</span><span style=\"color:#E1E4E8\">(ctx, expr, evalTime)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"evaluation error: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Format results for API response</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">formatInstantResult</span><span style=\"color:#E1E4E8\">(result), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteRangeQuery processes time-windowed PromQL queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteRangeQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">step</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(ctx, e.queryTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Validate range query parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">validateRangeParams</span><span style=\"color:#E1E4E8\">(start, end, step); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Parse expression into AST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expr, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> e.parser.</span><span style=\"color:#B392F0\">ParseExpression</span><span style=\"color:#E1E4E8\">(query)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"parse error: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Generate evaluation timeline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamps </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">generateTimeline</span><span style=\"color:#E1E4E8\">(start, end, step)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Execute expression at each timestamp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    results </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, ts </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> timestamps {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">evaluateExpression</span><span style=\"color:#E1E4E8\">(ctx, expr, ts)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"evaluation error at </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, ts, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        e.</span><span style=\"color:#B392F0\">accumulateRangeResults</span><span style=\"color:#E1E4E8\">(results, result, ts)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> e.</span><span style=\"color:#B392F0\">formatRangeResult</span><span style=\"color:#E1E4E8\">(results), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Expression Parser Implementation (Core Logic Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> parser</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExpressionParser handles PromQL expression parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ExpressionParser</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lexer </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Lexer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ASTNode represents nodes in the abstract syntax tree</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ASTNode</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Accept</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">visitor</span><span style=\"color:#B392F0\"> ASTVisitor</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricSelectorNode represents metric selection with label matchers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricSelectorNode</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MetricName    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LabelMatchers []</span><span style=\"color:#B392F0\">LabelMatcher</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ParseExpression converts PromQL text into executable AST</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExpressionParser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ParseExpression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">input</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">ASTNode</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p.lexer </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> NewLexer</span><span style=\"color:#E1E4E8\">(input)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize lexer with input string and advance to first token</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse primary expression (metric selector, number, or parenthesized expression)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check for range selector [5m] after metric selector</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle function calls if expression followed by (</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse binary operators according to precedence rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle aggregation operations (sum, avg, etc.) with grouping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Validate semantic constraints (range selectors only on metric selectors)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return completed AST or detailed parsing error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use recursive descent with separate methods for each precedence level</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> p.</span><span style=\"color:#B392F0\">parseExpression</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// parseMetricSelector handles metric{label=\"value\"} syntax</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExpressionParser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">parseMetricSelector</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricSelectorNode</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse metric name identifier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check for opening brace { to start label selector</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse label matcher list: label op \"value\", ...</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate label names and values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Ensure closing brace } terminates label selector</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return MetricSelectorNode with parsed components</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// parseLabelMatchers handles {job=\"api\", instance!=\"web-1\"} syntax</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExpressionParser</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">parseLabelMatchers</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#B392F0\">LabelMatcher</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> matchers []</span><span style=\"color:#B392F0\">LabelMatcher</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse comma-separated list of label matchers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each matcher: parse label name, operator, and value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate operator types (=, !=, =~, !~)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle quoted string values with escape sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate regex patterns for =~ and !~ operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return completed matcher list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> matchers, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Label Selector Implementation (Core Logic Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> selector</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LabelSelector filters time series based on label criteria</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LabelSelector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage       </span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    regexCache    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Regexp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxCacheSize  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LabelMatcher represents a single label filtering criterion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LabelMatcher</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type     </span><span style=\"color:#B392F0\">MatchType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Regex    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Regexp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MatchType defines the label matching operation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MatchType</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MatchEqual</span><span style=\"color:#B392F0\"> MatchType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MatchNotEqual</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MatchRegex</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MatchNotRegex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SelectSeries finds time series matching all provided label matchers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LabelSelector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SelectSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">matchers</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">LabelMatcher</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Handle empty matcher list (return all series)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Sort matchers by estimated selectivity (exact matches first)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Evaluate most selective matcher to get initial candidate set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For remaining matchers, intersect with candidates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle regex matchers by compiling patterns and testing values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return sorted list of matching series IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use storage.indexes.labelValues for efficient exact match lookups</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evaluateExactMatch finds series with specific label value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LabelSelector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluateExactMatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">matcher</span><span style=\"color:#B392F0\"> LabelMatcher</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Look up label-value combination in inverted index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return series ID list or empty slice if not found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle case where label name doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evaluateRegexMatch finds series matching regex pattern</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">LabelSelector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluateRegexMatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">matcher</span><span style=\"color:#B392F0\"> LabelMatcher</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get compiled regex from cache or compile new pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Retrieve all values for the label name from index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Test each value against regex pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Collect series IDs for all matching values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update regex cache if pattern was newly compiled</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Aggregation Engine Implementation (Core Logic Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> aggregation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Aggregator handles grouping and mathematical aggregation of time series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Aggregator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxGroupSize </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AggregationRequest specifies how to aggregate time series data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AggregationRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Function   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GroupBy    []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Without    []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Series     []</span><span style=\"color:#B392F0\">SeriesData</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesData represents a time series with its labels and current value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SeriesData</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels </span><span style=\"color:#B392F0\">Labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value  </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Valid  </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AggregateSeries applies aggregation function to grouped time series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Aggregator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AggregateSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">req</span><span style=\"color:#B392F0\"> AggregationRequest</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">SeriesData</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create aggregation groups based on groupBy/without labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Partition input series into groups by label values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply aggregation function within each group</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle missing/invalid values appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Construct output series with group labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Sort output series for consistent results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use createAggregationGroups helper to handle grouping logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// createAggregationGroups partitions series based on grouping specification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Aggregator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">createAggregationGroups</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">series</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">SeriesData</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">groupBy</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">without</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">SeriesData</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    groups </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">SeriesData</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: For each input series, compute group key based on labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Handle \"by\" grouping: include only specified labels in key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle \"without\" grouping: exclude specified labels from key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add series to appropriate group bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return map from group key to series list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> groups</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// applyAggregationFunction computes result for values within a group</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Aggregator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">applyAggregationFunction</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">function</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">values</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(values) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> function {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"sum\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Sum all values in the group</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"avg\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Calculate arithmetic mean</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"max\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Find maximum value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"min\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Find minimum value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"count\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Return count of valid values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(values)), </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"unknown aggregation function: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, function)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Range Query Execution (Core Logic Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> execution</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RangeExecutor handles time-windowed query processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RangeExecutor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage          </span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stalenessThreshold </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteRangeQuery processes queries over time windows</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RangeExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteRangeQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expr</span><span style=\"color:#B392F0\"> ASTNode</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">step</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">RangeQueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate array of query timestamps from start to end at step intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each timestamp, execute instant query evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Collect results into time series matrix format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle series that appear/disappear during time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply staleness detection for missing data points</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Format results with consistent timestamp alignment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// generateTimeline creates timestamp sequence for range query</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RangeExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">generateTimeline</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">step</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> timestamps []</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start at aligned timestamp (round start time to step boundary)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate timestamps at step intervals until end time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Ensure final timestamp doesn't exceed end time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return array of query evaluation timestamps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> timestamps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// interpolateValue finds appropriate value for query timestamp</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">r </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RangeExecutor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">interpolateValue</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Find sample closest to query timestamp within staleness threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return sample value if within threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return invalid if no recent sample available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle edge cases (no samples, samples after query time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints:</strong></p>\n<p>After implementing the core query engine components, verify functionality with these checkpoints:</p>\n<ol>\n<li><p><strong>Parser Testing</strong>: Run <code>go test ./internal/query/parser/...</code> - should parse basic metric selectors, label matchers, and mathematical expressions without errors.</p>\n</li>\n<li><p><strong>Label Selection</strong>: Test with <code>curl &quot;http://localhost:8080/api/v1/query?query=up{job=\\&quot;prometheus\\&quot;}&quot;</code> - should return series matching the label selector.</p>\n</li>\n<li><p><strong>Aggregation</strong>: Test with <code>curl &quot;http://localhost:8080/api/v1/query?query=sum(up)&quot;</code> - should return single aggregated value across all series.</p>\n</li>\n<li><p><strong>Range Queries</strong>: Test with <code>curl &quot;http://localhost:8080/api/v1/query_range?query=up&amp;start=1609459200&amp;end=1609462800&amp;step=60&quot;</code> - should return time series matrix with consistent timestamp alignment.</p>\n</li>\n</ol>\n<p><strong>Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parse errors on valid PromQL</td>\n<td>Incorrect operator precedence or tokenization</td>\n<td>Add debug logging to lexer token output</td>\n<td>Fix precedence in recursive descent methods</td>\n</tr>\n<tr>\n<td>Empty query results</td>\n<td>Label matcher not finding series</td>\n<td>Check inverted index contents and matcher logic</td>\n<td>Verify label matcher evaluation against actual stored labels</td>\n</tr>\n<tr>\n<td>Memory exhaustion on aggregation</td>\n<td>High cardinality grouping labels</td>\n<td>Monitor group count and series per group</td>\n<td>Add cardinality limits and validate grouping labels</td>\n</tr>\n<tr>\n<td>Slow range queries</td>\n<td>Inefficient timestamp generation or storage access</td>\n<td>Profile CPU and measure storage query time</td>\n<td>Optimize timeline generation and batch storage requests</td>\n</tr>\n<tr>\n<td>Inconsistent results</td>\n<td>Race conditions in concurrent processing</td>\n<td>Test with single-threaded execution</td>\n<td>Add proper synchronization around shared data structures</td>\n</tr>\n</tbody></table>\n<h2 id=\"component-interactions-and-data-flow\">Component Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section integrates all four milestones by showing how the Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4) components communicate and coordinate to form a complete metrics collection system.</p>\n</blockquote>\n<p>The metrics collection system operates as a <strong>synchronized orchestra</strong> where multiple components must coordinate their activities without blocking each other. Think of it like a busy restaurant kitchen: the servers (scrape engine) continuously bring in orders from customers (targets), the prep cooks (storage engine) process and organize ingredients (metrics) into proper containers (compressed chunks), while the head chef (query engine) fulfills requests by combining ingredients from storage. All three activities happen simultaneously, requiring careful coordination to avoid collisions and ensure freshness.</p>\n<p>Understanding these interactions is critical because the performance characteristics of your entire monitoring system depend on how efficiently these components communicate. Poor coordination leads to backpressure where slow queries block metric ingestion, or where high scraping volume overwhelms storage and degrades query performance. The key insight is that <strong>each pipeline must be designed with independent flow control</strong> while sharing the underlying data structures safely.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Architecture Overview\"></p>\n<h3 id=\"ingestion-pipeline-flow-from-scraped-metrics-to-indexed-storage\">Ingestion Pipeline: Flow from Scraped Metrics to Indexed Storage</h3>\n<p>The ingestion pipeline represents the <strong>forward flow</strong> of data through the system, from external targets to persistent storage. This pipeline must handle continuous high-volume writes while maintaining durability guarantees and enabling efficient querying. Think of it as a <strong>mail sorting facility</strong> that receives letters (metrics) from many post offices (targets), validates addressing (labels), sorts them into appropriate bins (time series), and files them in the archive (compressed storage) with proper indexing for later retrieval.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fscrape-sequence.svg\" alt=\"Scrape Operation Sequence\"></p>\n<h4 id=\"scrape-collection-phase\">Scrape Collection Phase</h4>\n<p>The ingestion pipeline begins when the <code>ScrapeEngine</code> identifies targets for metric collection. The scrape scheduler maintains an internal priority queue of upcoming scrape operations, ordered by target URL and next scrape time. This scheduler must balance scraping load across time to avoid thundering herd effects where all targets are scraped simultaneously.</p>\n<table>\n<thead>\n<tr>\n<th>Scrape Phase Step</th>\n<th>Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>Duration Limit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target Selection</td>\n<td><code>ScrapeEngine</code></td>\n<td>Current time, target configurations</td>\n<td>Targets ready for scraping</td>\n<td>&lt; 1ms</td>\n</tr>\n<tr>\n<td>HTTP Request</td>\n<td><code>HTTPClient</code></td>\n<td>Target URL, headers, timeout</td>\n<td>Raw metrics text or error</td>\n<td><code>ScrapeTimeout</code> (default 10s)</td>\n</tr>\n<tr>\n<td>Format Parsing</td>\n<td>Metrics Parser</td>\n<td>Prometheus exposition format text</td>\n<td><code>Sample</code> structs with labels</td>\n<td>&lt; 100ms per MB</td>\n</tr>\n<tr>\n<td>Label Validation</td>\n<td><code>LabelValidator</code></td>\n<td>Parsed samples with labels</td>\n<td>Valid samples or rejection errors</td>\n<td>&lt; 10ms per sample</td>\n</tr>\n<tr>\n<td>Batch Preparation</td>\n<td><code>ScrapeEngine</code></td>\n<td>Validated samples</td>\n<td>Batched samples for storage</td>\n<td>&lt; 50ms</td>\n</tr>\n</tbody></table>\n<p>The scrape collection process follows these steps:</p>\n<ol>\n<li><p><strong>Target prioritization</strong>: The scheduler examines all configured targets and identifies which ones are due for scraping based on their individual <code>scrape_interval</code> settings and last successful scrape time.</p>\n</li>\n<li><p><strong>Concurrent scrape initiation</strong>: The engine creates goroutines for each ready target, up to a maximum concurrent scrape limit to prevent resource exhaustion. Each goroutine gets its own <code>HTTPClient</code> context with the target&#39;s specific <code>scrape_timeout</code>.</p>\n</li>\n<li><p><strong>HTTP metrics retrieval</strong>: The <code>HTTPClient.ScrapeTarget()</code> method performs an HTTP GET request to the target&#39;s <code>/metrics</code> endpoint, following redirects and handling authentication if configured. The response body is limited to prevent memory exhaustion attacks.</p>\n</li>\n<li><p><strong>Exposition format parsing</strong>: The raw HTTP response is parsed using the Prometheus exposition format parser, which converts text lines like <code>http_requests_total{method=&quot;GET&quot;,status=&quot;200&quot;} 1234 1609459200000</code> into structured <code>Sample</code> objects.</p>\n</li>\n<li><p><strong>Label cardinality validation</strong>: Each parsed sample&#39;s labels are validated against cardinality limits using the <code>CardinalityTracker</code> to prevent label explosion attacks that could exhaust system memory.</p>\n</li>\n<li><p><strong>Batch accumulation</strong>: Valid samples are accumulated into batches of 1000-10000 samples to amortize the cost of storage operations while keeping memory usage bounded.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Batch Size for Storage Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Individual sample writes are inefficient due to lock contention and WAL sync overhead</li>\n<li><strong>Options Considered</strong>: Per-sample writes, fixed 1000-sample batches, adaptive batching based on memory pressure</li>\n<li><strong>Decision</strong>: Fixed 10000-sample batches with memory-based early flushing</li>\n<li><strong>Rationale</strong>: Fixed batching provides predictable memory usage and good write throughput, while early flushing prevents OOM during high-cardinality scrapes</li>\n<li><strong>Consequences</strong>: Storage write operations are more efficient, but samples experience up to 10000-sample buffering delay before persistence</li>\n</ul>\n</blockquote>\n<h4 id=\"storage-ingestion-phase\">Storage Ingestion Phase</h4>\n<p>Once the scrape engine has collected and validated metric samples, they must be ingested into the time series storage engine. This phase converts the batch of samples into compressed, indexed time series data while maintaining ACID durability guarantees through write-ahead logging.</p>\n<p>The <code>StorageEngine.Append()</code> method coordinates this complex process:</p>\n<table>\n<thead>\n<tr>\n<th>Storage Ingestion Step</th>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>WAL Write</td>\n<td><code>WriteAheadLog</code></td>\n<td>Durably record intended writes</td>\n<td>Retry with backoff, alert on disk full</td>\n</tr>\n<tr>\n<td>Series Resolution</td>\n<td><code>InvertedIndexes</code></td>\n<td>Map metric+labels to series ID</td>\n<td>Create new series if not found</td>\n</tr>\n<tr>\n<td>Chunk Allocation</td>\n<td><code>CompressedChunk</code></td>\n<td>Find or create chunk for timestamp</td>\n<td>Create new chunk if current full</td>\n</tr>\n<tr>\n<td>Gorilla Compression</td>\n<td><code>GorillaCompressor</code></td>\n<td>Compress sample using deltas/XOR</td>\n<td>Store uncompressed on compression failure</td>\n</tr>\n<tr>\n<td>Index Updates</td>\n<td><code>InvertedIndexes</code></td>\n<td>Update metric and label indexes</td>\n<td>Rebuild indexes on corruption</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td><code>StorageEngine</code></td>\n<td>Enforce memory limits, trigger compaction</td>\n<td>Reject writes, compact old chunks</td>\n</tr>\n</tbody></table>\n<p>The storage ingestion process operates as follows:</p>\n<ol>\n<li><p><strong>Write-ahead logging</strong>: Before any in-memory state changes, the entire sample batch is serialized and written to the <code>WriteAheadLog</code> with an <code>fsync()</code> to ensure durability. This guarantees that even if the process crashes during ingestion, the samples can be replayed from the WAL on restart.</p>\n</li>\n<li><p><strong>Series identification</strong>: For each sample, the storage engine computes a hash of the metric name plus sorted labels to generate a unique series ID. The <code>InvertedIndexes</code> structure maintains mappings from this hash to the actual <code>SeriesMetadata</code> containing chunk references.</p>\n</li>\n<li><p><strong>Chunk location</strong>: Each time series is stored as a sequence of chunks covering different time ranges. The storage engine finds the appropriate chunk for each sample&#39;s timestamp, creating new chunks when the current chunk is full (typically 120 samples or 2 hours of data).</p>\n</li>\n<li><p><strong>Compression application</strong>: Samples are compressed using the Gorilla compression algorithm implemented in <code>GorillaCompressor</code>. This applies delta-of-delta encoding to timestamps and XOR encoding to float values, typically achieving 1.5 bytes per sample.</p>\n</li>\n<li><p><strong>Index maintenance</strong>: The inverted indexes are updated to reflect the new samples. This includes updating the metric name index, each label name/value index, and the series metadata with new chunk references.</p>\n</li>\n<li><p><strong>Memory pressure monitoring</strong>: The storage engine tracks total memory usage across all chunks and indexes. When memory usage exceeds configured limits, background compaction is triggered to compress older chunks and potentially evict cold data.</p>\n</li>\n</ol>\n<blockquote>\n<p>The critical insight is that <strong>durability and consistency are maintained through WAL-first writes</strong>, while performance is optimized through batched operations and compression. The WAL acts as the single source of truth during crash recovery.</p>\n</blockquote>\n<h4 id=\"error-handling-and-backpressure\">Error Handling and Backpressure</h4>\n<p>The ingestion pipeline must gracefully handle various failure scenarios without losing data or blocking the entire system. The key principle is <strong>graceful degradation</strong> where individual target failures don&#39;t impact the broader system&#39;s health.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Method</th>\n<th>Recovery Action</th>\n<th>User Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target HTTP timeout</td>\n<td><code>context.Context</code> deadline exceeded</td>\n<td>Mark target as <code>HealthDown</code>, continue other targets</td>\n<td>Missing metrics for one target</td>\n</tr>\n<tr>\n<td>Storage disk full</td>\n<td><code>WriteAheadLog.AppendSamples()</code> returns <code>ENOSPC</code></td>\n<td>Reject new samples, alert operators</td>\n<td>Metrics ingestion stops</td>\n</tr>\n<tr>\n<td>High cardinality attack</td>\n<td><code>CardinalityTracker.RecordSeries()</code> exceeds limit</td>\n<td>Reject samples with new label combinations</td>\n<td>New series creation blocked</td>\n</tr>\n<tr>\n<td>Parser format errors</td>\n<td>Malformed Prometheus exposition format</td>\n<td>Skip invalid lines, log warnings</td>\n<td>Partial metrics loss for target</td>\n</tr>\n<tr>\n<td>Memory exhaustion</td>\n<td><code>StorageEngine</code> memory usage exceeds limit</td>\n<td>Trigger emergency compaction, reject writes</td>\n<td>Temporary ingestion backpressure</td>\n</tr>\n</tbody></table>\n<p>The backpressure mechanism works through a <strong>token bucket</strong> system where the storage engine issues tokens at a rate proportional to its available capacity. The scrape engine consumes tokens before attempting to append samples, naturally slowing down ingestion when storage is overwhelmed.</p>\n<p>⚠️ <strong>Pitfall: Blocking Scrapes on Storage Pressure</strong>\nA common mistake is making scrape operations synchronously wait for storage availability, which can cause all scraping to stop during storage issues. Instead, scrapes should buffer samples in memory with bounded queues and drop oldest samples when buffers fill. This preserves recent data while preventing memory exhaustion.</p>\n<h3 id=\"query-processing-pipeline-steps-from-promql-input-to-aggregated-results\">Query Processing Pipeline: Steps from PromQL Input to Aggregated Results</h3>\n<p>The query processing pipeline represents the <strong>reverse flow</strong> of data through the system, from high-level PromQL expressions to specific time series values retrieved from storage. Unlike the ingestion pipeline which handles continuous writes, the query pipeline processes discrete read requests that may touch large portions of stored data. Think of it as a <strong>research librarian</strong> who receives complex questions, breaks them down into specific book and page lookups, retrieves the relevant information, and synthesizes it into a coherent answer.</p>\n<p><img src=\"/api/project/metrics-collector/architecture-doc/asset?path=diagrams%2Fquery-execution.svg\" alt=\"Query Execution Flow\"></p>\n<h4 id=\"promql-parsing-and-planning-phase\">PromQL Parsing and Planning Phase</h4>\n<p>Query processing begins when the HTTP API receives a PromQL expression and converts it into an executable query plan. This phase must validate syntax, optimize execution order, and estimate resource requirements to prevent resource exhaustion attacks.</p>\n<table>\n<thead>\n<tr>\n<th>Parsing Phase Step</th>\n<th>Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lexical Analysis</td>\n<td><code>ExpressionParser</code></td>\n<td>Raw PromQL string</td>\n<td>Token stream</td>\n<td>O(n) where n = query length</td>\n</tr>\n<tr>\n<td>Syntax Parsing</td>\n<td><code>ExpressionParser</code></td>\n<td>Token stream</td>\n<td>Abstract Syntax Tree</td>\n<td>O(n log n) for expression depth</td>\n</tr>\n<tr>\n<td>Semantic Analysis</td>\n<td><code>QueryEngine</code></td>\n<td>AST nodes</td>\n<td>Validated query plan</td>\n<td>O(m) where m = number of selectors</td>\n</tr>\n<tr>\n<td>Optimization</td>\n<td><code>QueryEngine</code></td>\n<td>Query plan</td>\n<td>Optimized execution order</td>\n<td>O(m²) for selector optimization</td>\n</tr>\n<tr>\n<td>Resource Estimation</td>\n<td><code>QueryEngine</code></td>\n<td>Optimized plan</td>\n<td>Memory and time estimates</td>\n<td>O(m) for cardinality estimation</td>\n</tr>\n</tbody></table>\n<p>The parsing process transforms PromQL text through these stages:</p>\n<ol>\n<li><p><strong>Tokenization</strong>: The raw PromQL string is broken into tokens representing metric names, operators, functions, and literals. For example, <code>rate(http_requests_total[5m])</code> becomes tokens <code>FUNCTION(rate)</code>, <code>IDENTIFIER(http_requests_total)</code>, <code>RANGE[5m]</code>.</p>\n</li>\n<li><p><strong>AST construction</strong>: Tokens are parsed into a tree structure representing the expression&#39;s hierarchical structure. Function calls become <code>FunctionCallNode</code> instances with child nodes for their arguments, while metric selectors become <code>MetricSelectorNode</code> instances containing label matchers.</p>\n</li>\n<li><p><strong>Type checking</strong>: Each AST node is validated to ensure type compatibility. For example, the <code>rate()</code> function requires a range vector argument, so applying it to an instant vector would generate a type error.</p>\n</li>\n<li><p><strong>Query optimization</strong>: The query planner analyzes the AST to optimize execution. This includes pushing label filters down to storage selectors, reordering operations to minimize intermediate result sizes, and identifying opportunities for parallel execution.</p>\n</li>\n<li><p><strong>Resource estimation</strong>: Based on the optimized plan, the query engine estimates memory requirements and execution time by consulting cardinality statistics from the storage engine. Queries exceeding configured limits are rejected.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Decision: AST-Based Query Execution</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support complex PromQL expressions with nested functions and operations</li>\n<li><strong>Options Considered</strong>: Direct interpretation, bytecode compilation, AST walking with visitors</li>\n<li><strong>Decision</strong>: AST walking with visitor pattern for execution</li>\n<li><strong>Rationale</strong>: AST provides flexibility for optimization passes while visitor pattern enables clean separation of parsing and execution logic</li>\n<li><strong>Consequences</strong>: Enables sophisticated query optimization but requires more complex parsing infrastructure than simple interpretation</li>\n</ul>\n</blockquote>\n<h4 id=\"series-selection-and-filtering-phase\">Series Selection and Filtering Phase</h4>\n<p>Once the query is parsed and planned, the execution engine must identify which time series match the query&#39;s label selectors and retrieve their data from storage. This phase often dominates query performance since it involves index lookups and potentially large data retrievals.</p>\n<p>The <code>LabelSelector.SelectSeries()</code> method coordinates series selection:</p>\n<table>\n<thead>\n<tr>\n<th>Selection Step</th>\n<th>Component</th>\n<th>Operation</th>\n<th>Data Structure</th>\n<th>Time Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric Name Lookup</td>\n<td><code>InvertedIndexes</code></td>\n<td>Find all series for metric</td>\n<td>Hash table</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>Label Filter Application</td>\n<td><code>LabelSelector</code></td>\n<td>Apply each label matcher</td>\n<td>Sorted arrays with binary search</td>\n<td>O(log n) per matcher</td>\n</tr>\n<tr>\n<td>Series Intersection</td>\n<td><code>LabelSelector</code></td>\n<td>Combine multiple label results</td>\n<td>Sorted array merge</td>\n<td>O(n + m)</td>\n</tr>\n<tr>\n<td>Cardinality Validation</td>\n<td><code>QueryEngine</code></td>\n<td>Check result set size</td>\n<td>Count operation</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>Series Metadata Retrieval</td>\n<td><code>InvertedIndexes</code></td>\n<td>Get chunk refs for matching series</td>\n<td>Hash table lookups</td>\n<td>O(k) where k = matching series</td>\n</tr>\n</tbody></table>\n<p>The series selection algorithm works as follows:</p>\n<ol>\n<li><p><strong>Metric name filtering</strong>: If the query specifies a metric name (e.g., <code>http_requests_total</code>), the inverted index is consulted to get the set of all series IDs that have this metric name. This typically eliminates 99%+ of series immediately.</p>\n</li>\n<li><p><strong>Label matcher evaluation</strong>: Each label matcher in the query (e.g., <code>{status=&quot;200&quot;}</code>, <code>{method=~&quot;GET|POST&quot;}</code>) is evaluated against the remaining series. Exact matchers use hash lookups while regex matchers require iteration with pattern matching.</p>\n</li>\n<li><p><strong>Result set intersection</strong>: Multiple label matchers are combined using set intersection operations. Since series IDs are stored in sorted arrays, this uses efficient merge algorithms to find the intersection of all matching conditions.</p>\n</li>\n<li><p><strong>Cardinality enforcement</strong>: The final result set is checked against the query engine&#39;s <code>max_series</code> limit. Queries matching too many series are rejected to prevent memory exhaustion and ensure bounded query execution time.</p>\n</li>\n<li><p><strong>Chunk reference collection</strong>: For each matching series, the storage engine retrieves the <code>SeriesMetadata</code> containing references to all chunks that store data for this series, along with time range information for efficient chunk selection.</p>\n</li>\n</ol>\n<h4 id=\"data-retrieval-and-aggregation-phase\">Data Retrieval and Aggregation Phase</h4>\n<p>With the matching series identified, the query engine retrieves the actual time series data and applies any aggregation functions specified in the PromQL expression. This phase must handle large data volumes efficiently while supporting various aggregation semantics.</p>\n<table>\n<thead>\n<tr>\n<th>Aggregation Phase Step</th>\n<th>Component</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk Loading</td>\n<td><code>StorageEngine</code></td>\n<td>Chunk references + time range</td>\n<td>Decompress matching chunks</td>\n<td>Raw sample arrays</td>\n</tr>\n<tr>\n<td>Sample Extraction</td>\n<td><code>CompressedChunk</code></td>\n<td>Compressed chunk data</td>\n<td>Gorilla decompression</td>\n<td><code>Sample</code> structs</td>\n</tr>\n<tr>\n<td>Time Range Filtering</td>\n<td><code>RangeExecutor</code></td>\n<td>Samples + query time bounds</td>\n<td>Filter by timestamp</td>\n<td>Relevant samples only</td>\n</tr>\n<tr>\n<td>Interpolation</td>\n<td><code>RangeExecutor</code></td>\n<td>Sparse samples + query timestamps</td>\n<td>Fill gaps for step alignment</td>\n<td>Dense sample matrix</td>\n</tr>\n<tr>\n<td>Grouping</td>\n<td><code>Aggregator</code></td>\n<td>Samples + group-by labels</td>\n<td>Group by label combinations</td>\n<td>Sample groups</td>\n</tr>\n<tr>\n<td>Function Application</td>\n<td><code>Aggregator</code></td>\n<td>Sample groups + aggregation function</td>\n<td>Apply sum/avg/max/etc</td>\n<td>Aggregated results</td>\n</tr>\n</tbody></table>\n<p>The aggregation process handles different query types:</p>\n<p><strong>For instant queries</strong> (<code>http_requests_total</code> at a specific timestamp):</p>\n<ol>\n<li><p><strong>Point-in-time lookup</strong>: Find the sample closest to the query timestamp for each matching series, using staleness rules to determine if samples are too old to be valid.</p>\n</li>\n<li><p><strong>Label grouping</strong>: If the query includes aggregation (e.g., <code>sum by (status)</code>), group series by the specified label dimensions, creating separate result groups for each unique label combination.</p>\n</li>\n<li><p><strong>Aggregation function</strong>: Apply the requested function (<code>sum</code>, <code>avg</code>, <code>max</code>, <code>min</code>, <code>count</code>, <code>quantile</code>) to each group, producing a single value per group.</p>\n</li>\n</ol>\n<p><strong>For range queries</strong> (e.g., <code>rate(http_requests_total[5m])</code> over a time window):</p>\n<ol>\n<li><p><strong>Range vector construction</strong>: For each series and each query step timestamp, collect all samples within the specified range window (e.g., 5 minutes).</p>\n</li>\n<li><p><strong>Function application</strong>: Apply range functions like <code>rate()</code>, <code>increase()</code>, or <code>avg_over_time()</code> to each range vector, producing one value per series per step.</p>\n</li>\n<li><p><strong>Step alignment</strong>: Interpolate or aggregate results to align with the query&#39;s step interval, ensuring consistent output timestamps across all series.</p>\n</li>\n<li><p><strong>Multi-step aggregation</strong>: If instant aggregation is also requested (e.g., <code>sum(rate(...))</code>), group and aggregate the results from each step timestamp.</p>\n</li>\n</ol>\n<blockquote>\n<p><strong>The key insight is that range queries are essentially many instant queries executed in parallel</strong>, with additional complexity for handling time-based functions and step alignment. Efficient execution requires careful memory management to avoid loading unnecessary data.</p>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Loading Entire Series for Range Queries</strong>\nA common mistake is loading all samples for a time series when only a small time range is needed. Always filter chunks by time range before decompression, and use the chunk time bounds (<code>mint</code>/<code>maxt</code>) to skip chunks that don&#39;t overlap with the query range.</p>\n<h3 id=\"concurrency-control-managing-concurrent-reads-writes-and-background-operations\">Concurrency Control: Managing Concurrent Reads, Writes, and Background Operations</h3>\n<p>The metrics collection system must handle simultaneous ingestion, querying, and maintenance operations without data corruption or performance degradation. This requires careful <strong>concurrency control</strong> that allows maximum parallelism while maintaining data consistency. Think of it as <strong>air traffic control</strong> for a busy airport: multiple planes (operations) must use shared runways (data structures) simultaneously, requiring precise coordination to prevent collisions while maximizing throughput.</p>\n<p>The fundamental challenge is that writes (ingestion) and reads (queries) access the same underlying data structures (<code>InvertedIndexes</code>, <code>CompressedChunk</code>, series metadata) with very different access patterns and performance requirements. Writes are typically small and frequent, while reads may access large portions of data. Background operations like compaction need exclusive access to reorganize data efficiently.</p>\n<h4 id=\"read-write-concurrency-model\">Read-Write Concurrency Model</h4>\n<p>The storage engine uses a <strong>multi-granularity locking scheme</strong> that provides fine-grained concurrency control without sacrificing correctness. This approach recognizes that different data structures have different concurrency requirements and optimizes each accordingly.</p>\n<table>\n<thead>\n<tr>\n<th>Data Structure</th>\n<th>Concurrency Model</th>\n<th>Lock Type</th>\n<th>Typical Hold Time</th>\n<th>Contention Level</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>InvertedIndexes</code></td>\n<td>Readers-writer lock</td>\n<td><code>sync.RWMutex</code></td>\n<td>&lt; 1ms for reads, &lt; 10ms for writes</td>\n<td>Low (mostly reads)</td>\n</tr>\n<tr>\n<td><code>CompressedChunk</code></td>\n<td>Per-chunk mutexes</td>\n<td><code>sync.Mutex</code></td>\n<td>&lt; 100μs</td>\n<td>Very low (chunks rarely shared)</td>\n</tr>\n<tr>\n<td><code>SeriesMetadata</code></td>\n<td>Copy-on-write</td>\n<td>Atomic pointer swap</td>\n<td>0 (lockless reads)</td>\n<td>None</td>\n</tr>\n<tr>\n<td><code>WriteAheadLog</code></td>\n<td>Sequential writes only</td>\n<td><code>sync.Mutex</code></td>\n<td>&lt; 1ms</td>\n<td>Medium (write bottleneck)</td>\n</tr>\n<tr>\n<td>Target health state</td>\n<td>Atomic operations</td>\n<td><code>sync/atomic</code></td>\n<td>0 (lockless)</td>\n<td>None</td>\n</tr>\n</tbody></table>\n<p><strong>Index Concurrency</strong>: The <code>InvertedIndexes</code> structure uses a readers-writer lock (<code>sync.RWMutex</code>) that allows multiple concurrent readers but exclusive writer access. This works well because queries (readers) vastly outnumber ingestion operations (writers), and index lookups are typically very fast.</p>\n<p><strong>Chunk-Level Isolation</strong>: Each <code>CompressedChunk</code> has its own mutex, allowing concurrent access to different chunks while serializing access to individual chunks. Since chunks represent distinct time ranges, most operations naturally access different chunks, minimizing contention.</p>\n<p><strong>Series Metadata Atomicity</strong>: Series metadata uses a <strong>copy-on-write</strong> pattern where updates create a new metadata structure and atomically swap the pointer. Readers get a consistent snapshot without locking, while writers coordinate through a single writer lock.</p>\n<blockquote>\n<p><strong>Decision: Fine-Grained Locking Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support concurrent reads and writes without blocking each other</li>\n<li><strong>Options Considered</strong>: Global read-write lock, lock-free data structures, fine-grained per-component locking</li>\n<li><strong>Decision</strong>: Fine-grained locking with readers-writer locks for indexes and per-chunk mutexes</li>\n<li><strong>Rationale</strong>: Balances implementation complexity with performance - simpler than lock-free but much better concurrency than global locking</li>\n<li><strong>Consequences</strong>: Good read concurrency with acceptable write performance, but requires careful lock ordering to prevent deadlocks</li>\n</ul>\n</blockquote>\n<h4 id=\"write-coordination-and-batching\">Write Coordination and Batching</h4>\n<p>Multiple scrape targets generate samples simultaneously, requiring coordination to prevent write conflicts and optimize storage throughput. The ingestion system uses a <strong>channel-based coordination</strong> model where scrapers send samples through buffered channels to a smaller number of storage writers.</p>\n<p>The write coordination architecture:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Goroutines</th>\n<th>Responsibility</th>\n<th>Buffer Size</th>\n<th>Backpressure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scrape Workers</td>\n<td>50-100</td>\n<td>HTTP scraping and parsing</td>\n<td>N/A</td>\n<td>Block on channel send</td>\n</tr>\n<tr>\n<td>Sample Channels</td>\n<td>10</td>\n<td>Buffer samples between scraping and storage</td>\n<td>10,000 samples</td>\n<td>Drop oldest samples</td>\n</tr>\n<tr>\n<td>Storage Writers</td>\n<td>2-5</td>\n<td>Batch samples and write to storage</td>\n<td>N/A</td>\n<td>Apply write rate limiting</td>\n</tr>\n<tr>\n<td>Background Compactor</td>\n<td>1</td>\n<td>Compress old chunks, update indexes</td>\n<td>N/A</td>\n<td>Sleep when no work</td>\n</tr>\n</tbody></table>\n<p>This coordination model provides several benefits:</p>\n<ol>\n<li><p><strong>Write batching</strong>: Storage writers collect samples from multiple scrapers into large batches, amortizing the cost of WAL sync operations and lock acquisition.</p>\n</li>\n<li><p><strong>Load smoothing</strong>: Buffered channels absorb bursts of scraping activity and smooth them into steady storage write rates, preventing storage overwhelm during synchronized scrapes.</p>\n</li>\n<li><p><strong>Backpressure propagation</strong>: When storage becomes overwhelmed, channel buffers fill up, causing scrapers to block on channel sends, naturally throttling ingestion rate.</p>\n</li>\n<li><p><strong>Failure isolation</strong>: If one scraper encounters errors, other scrapers continue operating normally since each uses independent goroutines and error handling.</p>\n</li>\n</ol>\n<p>The write batching algorithm works as follows:</p>\n<ol>\n<li><p><strong>Sample accumulation</strong>: Storage writers continuously read from sample channels, accumulating samples into batches of 1000-10000 samples or until a time threshold (100ms) is reached.</p>\n</li>\n<li><p><strong>WAL writing</strong>: Complete batches are serialized and written to the WAL with <code>fsync()</code> to ensure durability before any in-memory state changes.</p>\n</li>\n<li><p><strong>Concurrent storage</strong>: Multiple storage writers can process different batches in parallel since the WAL provides ordering guarantees and each batch targets different series/chunks.</p>\n</li>\n<li><p><strong>Error handling</strong>: If WAL writing fails (e.g., disk full), the batch is either retried with exponential backoff or dropped with alerting, depending on the error type.</p>\n</li>\n</ol>\n<h4 id=\"background-operations-coordination\">Background Operations Coordination</h4>\n<p>The storage engine runs several background operations that must coordinate with foreground ingestion and querying to maintain system health without impacting user-facing performance. These operations include chunk compaction, index optimization, and old data deletion.</p>\n<table>\n<thead>\n<tr>\n<th>Background Operation</th>\n<th>Frequency</th>\n<th>Duration</th>\n<th>Resources Used</th>\n<th>Coordination Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chunk Compaction</td>\n<td>Every 2 hours</td>\n<td>10-60 seconds</td>\n<td>CPU, temporary memory</td>\n<td>Readers-writer lock during index updates</td>\n</tr>\n<tr>\n<td>Index Defragmentation</td>\n<td>Daily</td>\n<td>1-10 minutes</td>\n<td>CPU, disk I/O</td>\n<td>Copy-on-write index replacement</td>\n</tr>\n<tr>\n<td>WAL Truncation</td>\n<td>Every 15 minutes</td>\n<td>&lt; 1 second</td>\n<td>Disk I/O</td>\n<td>WAL mutex during file operations</td>\n</tr>\n<tr>\n<td>Old Data Deletion</td>\n<td>Hourly</td>\n<td>5-30 seconds</td>\n<td>Disk I/O</td>\n<td>Series-level locking during deletion</td>\n</tr>\n<tr>\n<td>Memory Pressure Compaction</td>\n<td>When memory &gt; 80%</td>\n<td>1-5 seconds</td>\n<td>CPU, memory</td>\n<td>Emergency write throttling</td>\n</tr>\n</tbody></table>\n<p><strong>Chunk Compaction Coordination</strong>: The background compactor identifies chunks that can be merged (adjacent time ranges, same series) or compressed further (old chunks with space to reclaim). During compaction, it:</p>\n<ol>\n<li><p><strong>Identifies compaction candidates</strong> by scanning series metadata for adjacent chunks or chunks with low compression ratios.</p>\n</li>\n<li><p><strong>Acquires read locks</strong> on source chunks to prevent concurrent modifications during compaction.</p>\n</li>\n<li><p><strong>Creates new compacted chunks</strong> in temporary storage, using improved compression parameters optimized for the specific data patterns observed.</p>\n</li>\n<li><p><strong>Atomically updates indexes</strong> to point to new chunks, acquiring brief write locks only during the pointer swap operations.</p>\n</li>\n<li><p><strong>Releases old chunks</strong> for garbage collection after a grace period to ensure no in-flight queries are using them.</p>\n</li>\n</ol>\n<p><strong>Memory Pressure Handling</strong>: When memory usage exceeds configured thresholds, the system triggers emergency compaction and write throttling:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Memory Usage Thresholds:\n- 70%: Begin background compaction of oldest chunks\n- 80%: Throttle ingestion rate to 50% of normal\n- 90%: Reject new writes, compact aggressively\n- 95%: Emergency mode - drop incoming samples, compact everything possible</code></pre></div>\n\n<p>This multi-tier approach provides <strong>graceful degradation</strong> under memory pressure while maintaining system availability.</p>\n<p>⚠️ <strong>Pitfall: Deadlock in Lock Acquisition</strong>\nBackground operations must acquire locks in a consistent order to prevent deadlocks. Always acquire series-level locks before chunk-level locks, and index locks before series locks. Use timeout-based lock acquisition (<code>TryLock</code> with timeouts) for background operations to prevent indefinite blocking.</p>\n<blockquote>\n<p>The critical insight is that <strong>coordination overhead must be minimized in the common case</strong> (concurrent reads, occasional writes) while providing strong guarantees in edge cases (memory pressure, failures). This is achieved through careful lock granularity choices and optimistic concurrency control where possible.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical implementation guidance for building the component interaction and data flow coordination systems described above.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommended for Beginners</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inter-component Communication</td>\n<td>Buffered channels (<code>make(chan Sample, 10000)</code>)</td>\n<td>Lock-free ring buffers</td>\n<td>Buffered channels</td>\n</tr>\n<tr>\n<td>Concurrency Control</td>\n<td><code>sync.RWMutex</code> and <code>sync.Mutex</code></td>\n<td>Lock-free data structures with <code>sync/atomic</code></td>\n<td>Standard library mutexes</td>\n</tr>\n<tr>\n<td>Background Task Scheduling</td>\n<td><code>time.Ticker</code> with goroutines</td>\n<td>Custom scheduler with priority queues</td>\n<td><code>time.Ticker</code></td>\n</tr>\n<tr>\n<td>Error Propagation</td>\n<td>Error channels and context cancellation</td>\n<td>Structured error types with retry policies</td>\n<td>Error channels</td>\n</tr>\n<tr>\n<td>Resource Monitoring</td>\n<td>Simple counters with <code>sync/atomic</code></td>\n<td>Prometheus metrics with histograms</td>\n<td>Atomic counters</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  coordinator/\n    coordinator.go          ← main coordination logic\n    pipeline.go            ← ingestion pipeline implementation\n    query_coordinator.go   ← query processing coordination\n    background.go          ← background operation management\n    coordinator_test.go    ← integration tests\n  \n  storage/\n    engine.go              ← storage engine with concurrency control\n    chunk.go               ← compressed chunk with per-chunk locking\n    index.go               ← inverted indexes with RWMutex\n    wal.go                 ← write-ahead log with sequential writes\n  \n  scrape/\n    engine.go              ← scrape engine with worker pools\n    target.go              ← individual target management\n    client.go              ← HTTP client with timeout handling\n  \n  query/\n    engine.go              ← query engine with resource limits\n    executor.go            ← query execution with series selection\n    aggregator.go          ← aggregation with grouping logic</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Pipeline Coordination Infrastructure</strong> (Complete, ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync/atomic</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PipelineCoordinator manages the flow of samples from scraping to storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PipelineCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sampleChannels    []</span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageWriters    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    channelBufferSize </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batchSize         </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batchTimeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Metrics for monitoring pipeline health</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samplesReceived   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samplesStored     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samplesBatched    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backpressureEvents </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Coordination</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx    </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancel </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wg     </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewPipelineCoordinator creates a coordinator with the specified configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewPipelineCoordinator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">writers</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">channelBufferSize</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">batchSize</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">batchTimeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithCancel</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    channels </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, writers)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> channels {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        channels[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, channelBufferSize)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">PipelineCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sampleChannels:    channels,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storageWriters:    writers,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        channelBufferSize: channelBufferSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batchSize:         batchSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        batchTimeout:      batchTimeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctx:               ctx,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cancel:            cancel,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendSamples distributes samples across writer channels with load balancing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(samples) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Round-robin distribution across channels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    channelIndex </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">pc.samplesReceived, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(samples)))) </span><span style=\"color:#F97583\">%</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(pc.sampleChannels)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> pc.sampleChannels[channelIndex] </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> samples:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">pc.ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> pc.ctx.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Channel is full - apply backpressure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">pc.backpressureEvents, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ErrBackpressure</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins the storage writer goroutines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> pc.storageWriters; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pc.wg.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        go</span><span style=\"color:#E1E4E8\"> pc.</span><span style=\"color:#B392F0\">runStorageWriter</span><span style=\"color:#E1E4E8\">(i, storage)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Shutdown gracefully stops all writers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pc.</span><span style=\"color:#B392F0\">cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pc.wg.</span><span style=\"color:#B392F0\">Wait</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetMetrics returns pipeline health metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetMetrics</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">PipelineMetrics</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> PipelineMetrics</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SamplesReceived:    atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">pc.samplesReceived),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SamplesStored:      atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">pc.samplesStored),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        SamplesBatched:     atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">pc.samplesBatched),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        BackpressureEvents: atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">pc.backpressureEvents),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// runStorageWriter processes samples from a channel and batches them for storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">runStorageWriter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">writerID</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> pc.wg.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batch </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, pc.batchSize)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    batchTimer </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTimer</span><span style=\"color:#E1E4E8\">(pc.batchTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> batchTimer.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> samples </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">pc.sampleChannels[writerID]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            batch </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(batch, samples</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">pc.samplesBatched, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(samples)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Flush batch if it reaches target size</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(batch) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> pc.batchSize {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                pc.</span><span style=\"color:#B392F0\">flushBatch</span><span style=\"color:#E1E4E8\">(storage, batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                batch </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> batch[:</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#6A737D\">// Reset slice while keeping capacity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                batchTimer.</span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">(pc.batchTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">batchTimer.C:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Flush partial batch on timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(batch) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                pc.</span><span style=\"color:#B392F0\">flushBatch</span><span style=\"color:#E1E4E8\">(storage, batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                batch </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> batch[:</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            batchTimer.</span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">(pc.batchTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">pc.ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // Flush remaining samples before shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(batch) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                pc.</span><span style=\"color:#B392F0\">flushBatch</span><span style=\"color:#E1E4E8\">(storage, batch)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// flushBatch writes a batch of samples to storage with error handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">flushBatch</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">batch</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> storage.</span><span style=\"color:#B392F0\">Append</span><span style=\"color:#E1E4E8\">(batch)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Log error but don't crash - implement retry logic here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Storage write failed for batch of </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> samples: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(batch), err)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">pc.samplesStored, </span><span style=\"color:#F97583\">int64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(batch)))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PipelineMetrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SamplesReceived    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SamplesStored      </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SamplesBatched     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BackpressureEvents </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> ErrBackpressure </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"pipeline backpressure - channel full\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Query Coordination Infrastructure</strong> (Complete, ready to use):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryCoordinator manages concurrent query execution with resource limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxConcurrentQueries </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryTimeout        </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSeriesPerQuery   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Semaphore for limiting concurrent queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    querySemaphore </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    activeQueries   </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    completedQueries </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeoutQueries  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errorQueries    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewQueryCoordinator creates a coordinator with resource limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewQueryCoordinator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">maxConcurrent</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">maxSeries</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxConcurrentQueries: maxConcurrent,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryTimeout:        timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxSeriesPerQuery:   maxSeries,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        querySemaphore:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}, maxConcurrent),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteQuery coordinates the execution of a single query with resource management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">evalTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">engine</span><span style=\"color:#B392F0\"> QueryEngine</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Acquire semaphore slot for concurrency control</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> qc.querySemaphore </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}{}:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\">qc.querySemaphore }()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ctx.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    atomic.</span><span style=\"color:#B392F0\">AddInt32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.activeQueries, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> atomic.</span><span style=\"color:#B392F0\">AddInt32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.activeQueries, </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Create query-specific context with timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryCtx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(ctx, qc.queryTimeout)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Execute query with resource monitoring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> qc.</span><span style=\"color:#B392F0\">executeWithMonitoring</span><span style=\"color:#E1E4E8\">(queryCtx, query, evalTime, engine)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Update metrics based on result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> context.DeadlineExceeded {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.timeoutQueries, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.errorQueries, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        atomic.</span><span style=\"color:#B392F0\">AddInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.completedQueries, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteRangeQuery coordinates range query execution with memory management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteRangeQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">step</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">engine</span><span style=\"color:#B392F0\"> QueryEngine</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Calculate expected result size for memory estimation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stepCount </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">(end.</span><span style=\"color:#B392F0\">Sub</span><span style=\"color:#E1E4E8\">(start) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> step)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> stepCount </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#6A737D\">// Arbitrary limit to prevent massive queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"range query too large: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> steps exceeds limit of 10000\"</span><span style=\"color:#E1E4E8\">, stepCount)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Use same concurrency control as instant queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> qc.querySemaphore </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}{}:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() { </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\">qc.querySemaphore }()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, ctx.</span><span style=\"color:#B392F0\">Err</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryCtx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(ctx, qc.queryTimeout</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(stepCount</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#B392F0\"> cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> engine.</span><span style=\"color:#B392F0\">ExecuteRangeQuery</span><span style=\"color:#E1E4E8\">(queryCtx, query, start, end, step)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// executeWithMonitoring wraps query execution with resource monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">executeWithMonitoring</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">evalTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">engine</span><span style=\"color:#B392F0\"> QueryEngine</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Pre-execution validation would go here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Query complexity analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Cardinality estimation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Resource availability check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> engine.</span><span style=\"color:#B392F0\">ExecuteInstantQuery</span><span style=\"color:#E1E4E8\">(ctx, query, evalTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(startTime)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Post-execution monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Validate result size doesn't exceed limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seriesCount </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> qc.</span><span style=\"color:#B392F0\">countResultSeries</span><span style=\"color:#E1E4E8\">(result)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> seriesCount </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> qc.maxSeriesPerQuery {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"query returned </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\"> series, exceeds limit of </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, seriesCount, qc.maxSeriesPerQuery)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Log slow queries for optimization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> duration </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> qc.queryTimeout</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Slow query detected: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> took </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, query, duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result, err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// countResultSeries counts the number of time series in a query result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">countResultSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> result.ResultType {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"vector\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> vector, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> result.Result.([]</span><span style=\"color:#B392F0\">InstantQueryResult</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(vector)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#9ECBFF\"> \"matrix\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> matrix, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> result.Result.([]</span><span style=\"color:#B392F0\">RangeQueryResult</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(matrix)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetMetrics returns query coordination metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetMetrics</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">QueryCoordinatorMetrics</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> QueryCoordinatorMetrics</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ActiveQueries:    atomic.</span><span style=\"color:#B392F0\">LoadInt32</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.activeQueries),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CompletedQueries: atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.completedQueries),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        TimeoutQueries:   atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.timeoutQueries),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ErrorQueries:     atomic.</span><span style=\"color:#B392F0\">LoadInt64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">qc.errorQueries),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryCoordinatorMetrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ActiveQueries    </span><span style=\"color:#F97583\">int32</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompletedQueries </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeoutQueries   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorQueries     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Main System Coordinator</strong> (Skeleton for implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SystemCoordinator manages the interaction between scraping, storage, and querying</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SystemCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scrapeEngine    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageEngine   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryEngine     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipelineCoord   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PipelineCoordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryCoord      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCoordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backgroundMgr   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BackgroundManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Coordination channels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shutdownCh      </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    healthCh        </span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> ComponentHealth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger          </span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewSystemCoordinator creates a fully configured system coordinator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSystemCoordinator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#B392F0\"> Logger</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemCoordinator</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create storage engine with concurrency-safe configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create scrape engine that will send samples to pipeline coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create query engine that reads from storage with coordination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create pipeline coordinator with appropriate buffer sizes and batching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Create query coordinator with resource limits from config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Create background manager for maintenance operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Set up health monitoring channels and coordination</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">SystemCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:        config,</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Initialize components here</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        shutdownCh:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        healthCh:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:        logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins coordinated operation of all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start storage engine and wait for ready signal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start pipeline coordinator with storage engine reference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start scrape engine with pipeline coordinator for sample delivery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start background manager for maintenance operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Start health monitoring goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Register HTTP handlers for queries using query coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Signal that system is ready for traffic</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sc.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"System coordinator started successfully\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Shutdown gracefully stops all components in reverse dependency order</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    close</span><span style=\"color:#E1E4E8\">(sc.shutdownCh)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Stop accepting new queries (HTTP handlers)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Wait for active queries to complete with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Stop scrape engine (no new samples)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Wait for pipeline to flush remaining samples</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Stop background operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Perform final storage sync and close</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Log shutdown completion with component status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleScrapeResults processes samples from the scrape engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleScrapeResults</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate samples are not nil/empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if system is shutting down (return error if so)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Send samples to pipeline coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle backpressure errors by updating scrape engine metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log any coordination failures for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteQuery processes queries through the coordinated query pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryReq</span><span style=\"color:#B392F0\"> QueryRequest</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate query request (syntax, resource limits)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check system health - reject queries if storage unhealthy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Route to appropriate query coordinator method (instant vs range)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply any system-level query transformations or optimizations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update system-level query metrics and logging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> sc.queryCoord.</span><span style=\"color:#B392F0\">ExecuteQuery</span><span style=\"color:#E1E4E8\">(ctx, queryReq.Query, queryReq.EvalTime, sc.queryEngine)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// monitorComponentHealth runs background health monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">monitorComponentHealth</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 1: Check scrape engine health (targets responding, sample rate)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 2: Check storage engine health (disk space, memory usage, WAL status)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 3: Check query engine health (response times, error rates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 4: Check pipeline health (backpressure events, batch efficiency)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 5: Update overall system health status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO 6: Trigger alerting if any component is unhealthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#E1E4E8\"> health </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">sc.healthCh:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            // TODO: Process component health updates from other goroutines</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">sc.shutdownCh:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status    </span><span style=\"color:#B392F0\">HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EvalTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\">  // For range queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#6A737D\">  // For range queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Step     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\"> // For range queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Go Concurrency Best Practices for Metrics Systems:</strong></p>\n<ol>\n<li><p><strong>Channel sizing</strong>: Use buffered channels with sizes based on expected throughput. For sample channels, buffer size should handle 10-30 seconds of peak sample rate to absorb bursts.</p>\n</li>\n<li><p><strong>Goroutine lifecycle</strong>: Always use <code>sync.WaitGroup</code> to coordinate goroutine shutdown. Each component should have a <code>Start()</code> method that launches goroutines and a <code>Shutdown()</code> method that signals stop and waits for completion.</p>\n</li>\n<li><p><strong>Context propagation</strong>: Pass <code>context.Context</code> through all operations to support timeouts and cancellation. Query operations especially need this for resource control.</p>\n</li>\n<li><p><strong>Atomic operations</strong>: Use <code>sync/atomic</code> for metrics counters and simple state flags. This avoids mutex contention for frequently updated values.</p>\n</li>\n<li><p><strong>Lock ordering</strong>: When acquiring multiple locks, always use consistent ordering (e.g., series-level before chunk-level) to prevent deadlocks.</p>\n</li>\n</ol>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the component interaction and data flow coordination:</p>\n<p><strong>Test Command</strong>: <code>go test ./internal/coordinator/... -v</code></p>\n<p><strong>Expected Behavior</strong>:</p>\n<ul>\n<li>Pipeline coordinator should handle 10,000 samples/second without backpressure</li>\n<li>Query coordinator should support 50 concurrent queries with proper resource limits</li>\n<li>System coordinator should gracefully start all components and coordinate shutdown</li>\n<li>Health monitoring should detect and report component failures within 30 seconds</li>\n</ul>\n<p><strong>Manual Verification</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start the system</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Generate sample load</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/samples</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> '[{\"metric\":\"test\",\"value\":1,\"timestamp\":\"2023-01-01T00:00:00Z\"}]'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Execute query while samples are being ingested</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> 'http://localhost:8080/api/v1/query?query=test&#x26;time=2023-01-01T00:00:00Z'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify coordination by checking metrics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/metrics</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> grep</span><span style=\"color:#79B8FF\"> -E</span><span style=\"color:#9ECBFF\"> \"(samples_received|samples_stored|queries_executed)\"</span></span></code></pre></div>\n\n<p><strong>Signs of Problems</strong>:</p>\n<ul>\n<li>Pipeline backpressure events increase continuously → increase channel buffer sizes or add more storage writers</li>\n<li>Query timeouts during ingestion → storage engine locks are held too long</li>\n<li>Memory usage grows without bounds → background compaction not running or ineffective</li>\n<li>Deadlocks during shutdown → lock ordering violations in component coordination</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Samples lost during ingestion</td>\n<td>Channel buffer overflow</td>\n<td>Check <code>BackpressureEvents</code> metric, monitor channel lengths</td>\n<td>Increase channel buffer size or add more storage writers</td>\n</tr>\n<tr>\n<td>Queries fail with timeout</td>\n<td>Lock contention between reads/writes</td>\n<td>Add mutex profiling, check lock hold times</td>\n<td>Implement read-mostly optimizations, reduce lock scope</td>\n</tr>\n<tr>\n<td>Memory usage grows continuously</td>\n<td>Background compaction not running</td>\n<td>Check compaction goroutine status, memory pressure triggers</td>\n<td>Fix compaction scheduling, add emergency memory limits</td>\n</tr>\n<tr>\n<td>System hangs during shutdown</td>\n<td>Goroutines not respecting context cancellation</td>\n<td>Use <code>go tool trace</code> to find blocked goroutines</td>\n<td>Add context checks in long-running operations</td>\n</tr>\n<tr>\n<td>Inconsistent query results</td>\n<td>Race conditions in concurrent access</td>\n<td>Run with <code>-race</code> flag, add data consistency checks</td>\n<td>Add proper synchronization around shared data structures</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section spans all four milestones by providing comprehensive error handling strategies for the Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4). Robust error handling is essential for production deployment of each milestone.</p>\n</blockquote>\n<p>Think of error handling in a metrics collection system like designing a hospital&#39;s emergency response protocols. Just as a hospital must continue operating even when individual departments face equipment failures, network outages, or staff shortages, our metrics system must gracefully handle target unavailability, storage corruption, and resource exhaustion while maintaining service for healthy components. The key insight is that metrics collection is inherently about observability—if our observability system itself becomes unreliable, we lose visibility into the systems we&#39;re monitoring, creating a dangerous blind spot.</p>\n<p>Effective error handling in distributed systems requires a layered approach. Each component must handle its local failure modes while contributing to system-wide resilience patterns. This means implementing circuit breakers to isolate failing targets, graceful degradation to maintain partial functionality under stress, and comprehensive recovery mechanisms that can rebuild consistent state after crashes.</p>\n<h3 id=\"target-unavailability\">Target Unavailability</h3>\n<p>The <strong>scrape engine</strong> operates in an inherently unreliable environment where targets may be temporarily unreachable due to network partitions, service restarts, configuration changes, or resource exhaustion. Unlike traditional request-response systems where failures are immediately visible to users, metrics collection operates on a background schedule where failures accumulate silently until noticed through missing data or monitoring alerts.</p>\n<p>Think of target health management like a cardiac monitor in an intensive care unit. The monitor must distinguish between genuine cardiac events (target is actually down) and sensor failures (network timeout, probe malformation), while maintaining a clear history of both successful and failed readings. A single missed heartbeat doesn&#39;t indicate cardiac arrest, but a pattern of missed beats requires immediate attention.</p>\n<p>The <code>TargetHealth</code> component tracks the availability and reliability status of each scrape endpoint through state transitions based on success and failure patterns. This health tracking serves multiple purposes: it prevents wasted resources on consistently failing targets, provides visibility into target reliability, and enables adaptive scraping strategies that reduce load on struggling services.</p>\n<table>\n<thead>\n<tr>\n<th>Health State</th>\n<th>Entry Condition</th>\n<th>Scraping Behavior</th>\n<th>Transition Triggers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HealthUp</code></td>\n<td>3 consecutive successful scrapes</td>\n<td>Normal interval scraping</td>\n<td>2 consecutive failures → HealthDegraded</td>\n</tr>\n<tr>\n<td><code>HealthDegraded</code></td>\n<td>2 consecutive failures from HealthUp</td>\n<td>Reduced frequency (2x interval)</td>\n<td>3 consecutive successes → HealthUp, 3 more failures → HealthDown</td>\n</tr>\n<tr>\n<td><code>HealthDown</code></td>\n<td>3 consecutive failures from HealthDegraded</td>\n<td>Exponential backoff (max 5min intervals)</td>\n<td>5 consecutive successes → HealthDegraded</td>\n</tr>\n</tbody></table>\n<h4 id=\"network-timeout-handling\">Network Timeout Handling</h4>\n<p>Network timeouts represent the most common failure mode in distributed scraping systems. The challenge lies in distinguishing between genuinely slow targets that need more time and unresponsive targets that should be abandoned quickly to prevent resource exhaustion.</p>\n<blockquote>\n<p><strong>Decision: Multi-Layered Timeout Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Single timeout values either cause false failures for slow targets or waste resources on dead targets</li>\n<li><strong>Options Considered</strong>: Fixed timeout, adaptive timeout based on target history, layered timeouts with different retry behavior</li>\n<li><strong>Decision</strong>: Implement layered timeouts with connection timeout (5s), first-byte timeout (10s), and total request timeout (30s)</li>\n<li><strong>Rationale</strong>: This provides fast failure detection for network issues while allowing reasonable time for slow but healthy targets</li>\n<li><strong>Consequences</strong>: More complex timeout logic but better balance between reliability and resource usage</li>\n</ul>\n</blockquote>\n<p>The <code>HTTPClient</code> implements three distinct timeout layers that work together to provide fast failure detection while accommodating legitimate performance variations:</p>\n<table>\n<thead>\n<tr>\n<th>Timeout Layer</th>\n<th>Duration</th>\n<th>Purpose</th>\n<th>Failure Indication</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Connection Timeout</td>\n<td>5 seconds</td>\n<td>Detect network connectivity issues</td>\n<td>Target unreachable or network partition</td>\n</tr>\n<tr>\n<td>First Byte Timeout</td>\n<td>10 seconds</td>\n<td>Detect application-level hangs</td>\n<td>Target process hung or severely overloaded</td>\n</tr>\n<tr>\n<td>Total Request Timeout</td>\n<td>30 seconds</td>\n<td>Prevent resource exhaustion</td>\n<td>Large response or very slow processing</td>\n</tr>\n</tbody></table>\n<p>The scrape engine handles timeout failures by updating target health state and implementing exponential backoff for consistently failing targets. This prevents the scraper from overwhelming targets that are experiencing temporary resource constraints while quickly returning to normal scraping frequency once targets recover.</p>\n<h4 id=\"http-error-response-handling\">HTTP Error Response Handling</h4>\n<p>HTTP error responses provide valuable diagnostic information about target state and should be handled differently based on their semantic meaning. Unlike timeouts, which may indicate transient network issues, HTTP errors often indicate configuration problems or application-level failures that require different recovery strategies.</p>\n<p>The scrape engine categorizes HTTP errors into permanent failures (4xx client errors) and temporary failures (5xx server errors), applying different retry strategies and health state transitions for each category:</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Status Codes</th>\n<th>Retry Strategy</th>\n<th>Health Impact</th>\n<th>Diagnostic Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Authentication/Authorization</td>\n<td>401, 403</td>\n<td>No retry - requires configuration fix</td>\n<td>Immediate HealthDown</td>\n<td>Log configuration error, alert operator</td>\n</tr>\n<tr>\n<td>Client Errors</td>\n<td>400, 404, 405</td>\n<td>No retry - indicates configuration problem</td>\n<td>Immediate HealthDown</td>\n<td>Validate target URL and endpoint configuration</td>\n</tr>\n<tr>\n<td>Server Overload</td>\n<td>429, 503</td>\n<td>Exponential backoff with jitter</td>\n<td>Gradual degradation</td>\n<td>Reduce scraping frequency, implement circuit breaker</td>\n</tr>\n<tr>\n<td>Server Errors</td>\n<td>500, 502, 504</td>\n<td>Standard retry with timeout</td>\n<td>Follow normal health transitions</td>\n<td>Monitor for pattern indicating systemic issues</td>\n</tr>\n</tbody></table>\n<h4 id=\"malformed-metrics-handling\">Malformed Metrics Handling</h4>\n<p>The Prometheus exposition format parsing must handle malformed metrics data gracefully while preserving successfully parsed metrics from the same scrape operation. Think of this like a data quality inspector at a manufacturing plant—defective items should be rejected and logged, but the entire batch shouldn&#39;t be discarded if most items are acceptable.</p>\n<p>Parse errors fall into several categories that require different handling strategies:</p>\n<p><strong>Metric Name Validation Errors</strong>: Metric names that violate naming conventions (contain invalid characters, use reserved prefixes, or exceed length limits) are rejected at parse time. The parser logs the specific validation failure and continues processing remaining metrics from the same scrape.</p>\n<p><strong>Label Format Errors</strong>: Labels with invalid names, missing values, or encoding issues are handled by either dropping the problematic label (if it&#39;s not critical) or dropping the entire metric (if the label is required for uniqueness). The decision depends on whether the remaining labels provide sufficient identity for the time series.</p>\n<p><strong>Value Format Errors</strong>: Timestamps or values that cannot be parsed as valid numbers cause the specific sample to be dropped while preserving other samples from the same metric. This is particularly important for histogram metrics where individual bucket counts may be malformed while others are valid.</p>\n<p><strong>Timestamp Consistency Errors</strong>: Samples with timestamps significantly in the past or future (outside a configurable staleness threshold) are dropped to prevent storage corruption and query inconsistencies. The parser maintains a window of acceptable timestamps based on scrape time.</p>\n<blockquote>\n<p><strong>Decision: Partial Scrape Success Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Scrapes often contain mix of valid and invalid metrics, binary success/failure loses valuable data</li>\n<li><strong>Options Considered</strong>: All-or-nothing parsing, best-effort parsing with warnings, configurable error tolerance thresholds</li>\n<li><strong>Decision</strong>: Implement best-effort parsing that preserves valid metrics while logging specific parse failures</li>\n<li><strong>Rationale</strong>: Metrics collection should be resilient to individual metric formatting issues without losing all observability</li>\n<li><strong>Consequences</strong>: More complex error reporting but better data availability and easier debugging of format issues</li>\n</ul>\n</blockquote>\n<h4 id=\"circuit-breaker-implementation\">Circuit Breaker Implementation</h4>\n<p>Circuit breakers prevent the scrape engine from wasting resources on consistently failing targets while enabling rapid recovery when targets become healthy again. The implementation tracks failure patterns and automatically transitions between closed (normal operation), open (failing fast), and half-open (testing recovery) states.</p>\n<p>The circuit breaker maintains failure statistics over a sliding time window and makes state transition decisions based on failure rate thresholds rather than absolute failure counts. This provides more robust behavior under varying traffic patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Circuit State</th>\n<th>Scraping Behavior</th>\n<th>Failure Threshold</th>\n<th>Success Requirement</th>\n<th>Transition Logic</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>Normal scraping at configured interval</td>\n<td>50% failures over 5 minutes</td>\n<td>N/A</td>\n<td>Failure rate exceeds threshold → Open</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>No scraping, immediate failure response</td>\n<td>N/A</td>\n<td>N/A</td>\n<td>After 1 minute timeout → Half-Open</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Single test scrape allowed</td>\n<td>N/A</td>\n<td>1 successful scrape</td>\n<td>Success → Closed, Failure → Open</td>\n</tr>\n</tbody></table>\n<h3 id=\"storage-errors\">Storage Errors</h3>\n<p>The <strong>time series storage engine</strong> must handle various failure modes while maintaining data consistency and preventing corruption. Unlike stateless components that can simply restart after failures, the storage engine maintains persistent state that requires careful recovery procedures and consistency guarantees.</p>\n<p>Think of storage error handling like a bank&#39;s vault system. The vault must protect deposits even during power outages, hardware failures, or software crashes. Every transaction must be logged before execution (write-ahead logging), and after any disruption, the bank must verify the vault&#39;s contents match the transaction log exactly. Similarly, our storage engine uses write-ahead logging and consistency checks to ensure no data is lost or corrupted during failures.</p>\n<h4 id=\"disk-space-exhaustion\">Disk Space Exhaustion</h4>\n<p>Disk space exhaustion represents one of the most critical failure modes because it can cause data loss and prevent normal operation. The storage engine must detect approaching disk exhaustion, implement emergency data retention policies, and gracefully degrade functionality to preserve the most important data.</p>\n<p>The storage engine implements a multi-level disk space monitoring system that triggers different responses based on available space thresholds:</p>\n<table>\n<thead>\n<tr>\n<th>Available Space</th>\n<th>Action Taken</th>\n<th>Data Retention Policy</th>\n<th>Write Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&gt; 20%</td>\n<td>Normal operation</td>\n<td>Standard retention period</td>\n<td>All writes accepted</td>\n</tr>\n<tr>\n<td>10-20%</td>\n<td>Warning logs, accelerate cleanup</td>\n<td>Reduce retention to 75% of configured</td>\n<td>All writes accepted</td>\n</tr>\n<tr>\n<td>5-10%</td>\n<td>Emergency retention, pause compaction</td>\n<td>Reduce retention to 50% of configured</td>\n<td>High-priority metrics only</td>\n</tr>\n<tr>\n<td>&lt; 5%</td>\n<td>Read-only mode, aggressive cleanup</td>\n<td>Reduce retention to 25% of configured</td>\n<td>No new writes accepted</td>\n</tr>\n</tbody></table>\n<p>The emergency retention system prioritizes metrics based on configured importance levels and access patterns. Critical infrastructure metrics (CPU, memory, disk usage) receive highest priority, while application-specific metrics may be discarded first during space constraints.</p>\n<blockquote>\n<p><strong>Decision: Graceful Degradation Over Hard Failures</strong></p>\n<ul>\n<li><strong>Context</strong>: Disk exhaustion traditionally causes complete storage system failure and data loss</li>\n<li><strong>Options Considered</strong>: Fail fast when disk full, emergency retention policies, offload data to remote storage</li>\n<li><strong>Decision</strong>: Implement layered retention policies that preserve most important data while maintaining read access</li>\n<li><strong>Rationale</strong>: Partial observability is much more valuable than complete loss of metrics collection during emergencies</li>\n<li><strong>Consequences</strong>: More complex retention logic but maintains system availability during resource constraints</li>\n</ul>\n</blockquote>\n<h4 id=\"write-ahead-log-corruption-recovery\">Write-Ahead Log Corruption Recovery</h4>\n<p>The <code>WriteAheadLog</code> provides durability guarantees by recording all intended writes before they&#39;re applied to the main storage indexes. WAL corruption can occur due to hardware failures, power outages during writes, or filesystem issues. Recovery procedures must detect corruption and rebuild consistent state without losing committed data.</p>\n<p>WAL recovery follows a structured approach that validates log integrity and replays valid entries while handling various corruption scenarios:</p>\n<ol>\n<li><p><strong>Log Segment Validation</strong>: Each WAL segment begins with a header containing a checksum of the segment&#39;s contents. During recovery, the system validates each segment header and marks corrupted segments for special handling.</p>\n</li>\n<li><p><strong>Entry Checksum Verification</strong>: Individual log entries contain CRC32 checksums that detect corruption within valid segments. Corrupted entries are skipped during replay, and their absence is logged for operator investigation.</p>\n</li>\n<li><p><strong>Partial Write Detection</strong>: Power failures can cause partial writes where only part of a log entry is written to disk. The recovery system detects these by checking entry length headers against available data.</p>\n</li>\n<li><p><strong>Timestamp Consistency Checking</strong>: Replayed entries must have timestamps consistent with the recovery point. Entries with timestamps far in the future or past are considered suspect and may indicate clock issues or corruption.</p>\n</li>\n<li><p><strong>Index Rebuilding</strong>: After replaying all valid WAL entries, the system rebuilds its in-memory indexes and validates them against the recovered data. Any inconsistencies trigger a full reindex operation.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Corruption Type</th>\n<th>Detection Method</th>\n<th>Recovery Action</th>\n<th>Data Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Segment Header Corruption</td>\n<td>Header checksum mismatch</td>\n<td>Skip entire segment, log corruption</td>\n<td>Loss of data in corrupted segment</td>\n</tr>\n<tr>\n<td>Entry Corruption</td>\n<td>Entry checksum failure</td>\n<td>Skip corrupted entry, continue replay</td>\n<td>Loss of individual corrupted entries</td>\n</tr>\n<tr>\n<td>Partial Write</td>\n<td>Length header mismatch</td>\n<td>Truncate at last valid entry</td>\n<td>Loss of incomplete final entry</td>\n</tr>\n<tr>\n<td>Timestamp Inconsistency</td>\n<td>Time bounds checking</td>\n<td>Skip suspicious entries with warnings</td>\n<td>Potential loss of out-of-order data</td>\n</tr>\n</tbody></table>\n<h4 id=\"index-inconsistency-recovery\">Index Inconsistency Recovery</h4>\n<p>The <code>InvertedIndexes</code> maintain mappings from metric names and label combinations to time series identifiers. Index corruption can cause queries to return incorrect results or fail to find existing data. The storage engine detects index inconsistencies during normal operations and can rebuild indexes from the authoritative chunk data.</p>\n<p>Index consistency checking operates continuously during normal operations, validating that index entries correspond to actual stored data and that all stored data has appropriate index entries:</p>\n<p><strong>Forward Consistency Checking</strong>: For each index entry pointing to a time series, verify that the referenced time series actually exists in storage and contains the expected metric name and labels.</p>\n<p><strong>Reverse Consistency Checking</strong>: For each stored time series, verify that appropriate index entries exist and point to the correct series identifier.</p>\n<p><strong>Cross-Index Consistency</strong>: Verify that the metric name index, label value indexes, and series metadata index all contain consistent information about the same time series.</p>\n<p><strong>Cardinality Validation</strong>: Compare the number of unique time series found in storage against cardinality tracking counters to detect missing or extra index entries.</p>\n<p>The recovery system can rebuild indexes in several modes depending on the severity of detected inconsistencies:</p>\n<table>\n<thead>\n<tr>\n<th>Inconsistency Type</th>\n<th>Rebuild Strategy</th>\n<th>Downtime Required</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing Index Entries</td>\n<td>Incremental rebuild of missing entries</td>\n<td>None</td>\n<td>Temporary query slowdown</td>\n</tr>\n<tr>\n<td>Extra Index Entries</td>\n<td>Remove orphaned entries during background cleanup</td>\n<td>None</td>\n<td>Minimal</td>\n</tr>\n<tr>\n<td>Cross-Index Mismatch</td>\n<td>Full reindex of affected metric names</td>\n<td>Read-only mode during rebuild</td>\n<td>Significant temporary impact</td>\n</tr>\n<tr>\n<td>Systematic Corruption</td>\n<td>Complete index rebuild from chunk data</td>\n<td>Full downtime during rebuild</td>\n<td>Complete rebuild required</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-consistency-validation\">Data Consistency Validation</h4>\n<p>The storage engine implements continuous data consistency checking that validates the integrity of compressed chunks, the accuracy of series metadata, and the correctness of time ordering within series data.</p>\n<p>Consistency validation operates at multiple levels:</p>\n<p><strong>Chunk-Level Validation</strong>: Each compressed chunk maintains metadata about its time range, sample count, and compression parameters. The validation system periodically decompresses chunks and verifies that the contained samples match the metadata.</p>\n<p><strong>Series-Level Validation</strong>: Time series must maintain strict time ordering of samples and consistent label sets across all chunks. The validation system checks for timestamp inversions, duplicate timestamps, and label mutations within series.</p>\n<p><strong>Storage-Level Validation</strong>: The overall storage system maintains invariants about total series count, disk space usage, and index sizes. Background validation jobs verify these invariants and alert operators to systematic issues.</p>\n<p><strong>Cross-Component Validation</strong>: Consistency checks verify that WAL entries, compressed chunks, and index entries all describe the same underlying data consistently.</p>\n<h3 id=\"query-errors\">Query Errors</h3>\n<p>The <strong>query engine</strong> must handle malformed queries, missing data scenarios, and resource exhaustion while providing useful error messages that help users diagnose and fix their queries. Query error handling is particularly challenging because users may not understand the underlying data model or system limitations.</p>\n<p>Think of query error handling like a reference librarian helping researchers find information. When a researcher asks for something that doesn&#39;t exist or is unclear in their request, the librarian doesn&#39;t simply say &quot;not found&quot;—they explain what resources are available, suggest alternatives, and help reformulate the request to find relevant information.</p>\n<h4 id=\"invalid-expression-parsing\">Invalid Expression Parsing</h4>\n<p>PromQL expression parsing must handle syntax errors gracefully while providing specific error messages that help users correct their queries. The parser encounters various error categories that require different diagnostic approaches:</p>\n<p><strong>Lexical Errors</strong>: Invalid characters, unterminated strings, or malformed numbers in the query text. These errors include specific position information and suggest valid alternatives.</p>\n<p><strong>Syntax Errors</strong>: Grammatically incorrect expressions such as mismatched parentheses, invalid operator precedence, or incomplete function calls. The parser provides context about expected tokens and suggests corrections.</p>\n<p><strong>Semantic Errors</strong>: Syntactically correct expressions that violate PromQL semantic rules, such as applying vector operators to scalar values or using undefined functions. These errors explain the type mismatch and suggest valid operations.</p>\n<p><strong>Label Selector Errors</strong>: Invalid label matching expressions, malformed regular expressions, or label names that violate naming conventions. The parser validates regex patterns and provides specific regex error messages.</p>\n<p>The <code>ExpressionParser</code> maintains an error collection that accumulates multiple parsing errors and provides comprehensive feedback rather than stopping at the first error encountered:</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Example Query</th>\n<th>Error Message</th>\n<th>Suggested Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lexical</td>\n<td><code>http_requests_@total</code></td>\n<td>Invalid character &#39;@&#39; at position 13</td>\n<td>Use valid metric name characters: [a-zA-Z0-9_:]</td>\n</tr>\n<tr>\n<td>Syntax</td>\n<td><code>http_requests_total +</code></td>\n<td>Unexpected end of expression, expected right operand</td>\n<td>Complete the addition operation: <code>+ &lt;expression&gt;</code></td>\n</tr>\n<tr>\n<td>Semantic</td>\n<td><code>rate(&quot;string&quot;)</code></td>\n<td>Function rate() expects vector argument, got string</td>\n<td>Use vector selector: <code>rate(http_requests_total[5m])</code></td>\n</tr>\n<tr>\n<td>Label Selector</td>\n<td><code>{__name__=~&quot;[invalid&quot;</code></td>\n<td>Invalid regex pattern: missing closing bracket</td>\n<td>Fix regex pattern: <code>{__name__=~&quot;valid.*&quot;}</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Detailed Error Context Over Simple Error Messages</strong></p>\n<ul>\n<li><strong>Context</strong>: Generic error messages like &quot;parse error&quot; don&#39;t help users fix their queries effectively</li>\n<li><strong>Options Considered</strong>: Simple error codes, detailed position-specific messages, suggested corrections with examples</li>\n<li><strong>Decision</strong>: Provide detailed error messages with position information, context, and suggested corrections</li>\n<li><strong>Rationale</strong>: Query languages are complex and users need specific guidance to construct valid expressions</li>\n<li><strong>Consequences</strong>: More complex error handling code but much better user experience and faster debugging</li>\n</ul>\n</blockquote>\n<h4 id=\"missing-data-handling\">Missing Data Handling</h4>\n<p>Time series data is inherently sparse—not all metrics are available at all times, and query time ranges may extend beyond available data. The query engine must handle these scenarios gracefully while providing clear indication of data availability issues.</p>\n<p>Missing data scenarios require different handling strategies based on their cause and the query type:</p>\n<p><strong>Series Not Found</strong>: When a query references metric names or label combinations that don&#39;t exist in storage, the query engine returns empty results rather than errors. This matches Prometheus behavior where missing metrics are treated as empty result sets rather than error conditions.</p>\n<p><strong>Time Range Gaps</strong>: Queries may request data from time ranges where no samples were collected due to scraping failures or target downtime. The query engine interpolates across small gaps (less than 2 scrape intervals) but returns empty values for larger gaps.</p>\n<p><strong>Staleness Handling</strong>: Samples older than the configured staleness threshold (typically 5 minutes) are considered stale and excluded from instant queries. Range queries may include stale samples with appropriate warnings.</p>\n<p><strong>Partial Data Availability</strong>: When some series in an aggregation query have data while others don&#39;t, the query engine includes warnings about partial results and indicates which series contributed to the final result.</p>\n<p>The query engine provides comprehensive warnings about data availability issues:</p>\n<table>\n<thead>\n<tr>\n<th>Missing Data Type</th>\n<th>Query Behavior</th>\n<th>Warning Message</th>\n<th>User Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric Not Found</td>\n<td>Return empty result</td>\n<td>No time series found for selector</td>\n<td>Verify metric name and labels exist</td>\n</tr>\n<tr>\n<td>Time Range Gap</td>\n<td>Return empty for gap period</td>\n<td>Data gap detected from X to Y</td>\n<td>Check target availability during gap</td>\n</tr>\n<tr>\n<td>Stale Data</td>\n<td>Exclude from instant queries</td>\n<td>Using stale data older than threshold</td>\n<td>Check recent scraping status</td>\n</tr>\n<tr>\n<td>Partial Aggregation</td>\n<td>Include available series only</td>\n<td>Aggregation includes only N of M series</td>\n<td>Investigate missing series</td>\n</tr>\n</tbody></table>\n<h4 id=\"resource-exhaustion-protection\">Resource Exhaustion Protection</h4>\n<p>Query execution can consume significant memory and CPU resources, particularly for queries over long time ranges or high-cardinality metrics. The query engine implements multiple layers of resource protection to prevent individual queries from affecting system stability.</p>\n<p>Think of query resource limits like admission control at a concert venue. The venue has a maximum capacity that ensures safety and comfort for all attendees. When capacity is reached, additional people must wait rather than creating an unsafe overcrowding situation. Similarly, the query engine limits concurrent queries and per-query resource usage to maintain stability for all users.</p>\n<p>The <code>QueryCoordinator</code> implements several resource protection mechanisms:</p>\n<p><strong>Concurrent Query Limiting</strong>: A semaphore limits the number of queries executing simultaneously to prevent CPU and memory exhaustion. Queries that exceed the limit are queued or rejected with appropriate error messages.</p>\n<p><strong>Per-Query Memory Limits</strong>: Each query has a maximum memory allocation for storing intermediate results and final output. Queries that exceed this limit are terminated with resource exhaustion errors.</p>\n<p><strong>Time Range Restrictions</strong>: Very long time range queries can consume excessive resources and are limited to configurable maximum durations (typically 30 days). Users must break long-range queries into smaller segments.</p>\n<p><strong>Series Count Limits</strong>: Queries that would process more than a maximum number of time series (typically 10,000) are rejected to prevent memory exhaustion from high-cardinality selectors.</p>\n<p><strong>Query Timeout Enforcement</strong>: All queries have maximum execution time limits (typically 30 seconds) after which they&#39;re terminated to prevent resource hoarding.</p>\n<p>The resource protection system provides specific error messages that help users understand the limit violation and suggest query modifications:</p>\n<table>\n<thead>\n<tr>\n<th>Resource Limit</th>\n<th>Threshold</th>\n<th>Error Message</th>\n<th>Suggested Solution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent Queries</td>\n<td>10 simultaneous</td>\n<td>Maximum concurrent queries exceeded, try again later</td>\n<td>Wait for other queries to complete or increase limit</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>1GB per query</td>\n<td>Query memory limit exceeded at N GB</td>\n<td>Reduce time range or use more specific label selectors</td>\n</tr>\n<tr>\n<td>Time Range</td>\n<td>30 days</td>\n<td>Query time range exceeds maximum of 30 days</td>\n<td>Break query into smaller time segments</td>\n</tr>\n<tr>\n<td>Series Count</td>\n<td>10,000 series</td>\n<td>Query would examine N series, limit is 10,000</td>\n<td>Use more specific label selectors to reduce cardinality</td>\n</tr>\n<tr>\n<td>Execution Time</td>\n<td>30 seconds</td>\n<td>Query timeout after 30 seconds</td>\n<td>Simplify query or reduce time range</td>\n</tr>\n</tbody></table>\n<h3 id=\"resource-protection\">Resource Protection</h3>\n<p>System-wide resource protection ensures that no single component or operation can compromise overall system stability. This requires coordination between all components and global limits that prevent resource exhaustion at the system level.</p>\n<p>Think of system resource protection like air traffic control at a major airport. Individual flights (queries, scrapes, storage operations) may have their own requirements, but the control system must manage overall airport capacity, runway utilization, and airspace congestion to keep all flights operating safely. No single flight is allowed to disrupt the entire airport&#39;s operation.</p>\n<h4 id=\"memory-management-strategy\">Memory Management Strategy</h4>\n<p>Memory usage in metrics collection systems can grow rapidly due to high-cardinality metrics, large query results, or accumulated metadata. The system implements a comprehensive memory management strategy that monitors usage, enforces limits, and provides graceful degradation when approaching limits.</p>\n<p>The memory management system operates at three levels:</p>\n<p><strong>Component-Level Limits</strong>: Each major component (scrape engine, storage engine, query engine) has dedicated memory quotas that prevent any single component from consuming all available memory.</p>\n<p><strong>Operation-Level Limits</strong>: Individual operations within components have specific memory limits that prevent single large operations from affecting other operations.</p>\n<p><strong>System-Level Monitoring</strong>: Global memory usage monitoring triggers emergency responses when total system memory usage approaches dangerous levels.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Memory Quota</th>\n<th>Enforcement Mechanism</th>\n<th>Degradation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scrape Engine</td>\n<td>25% of system memory</td>\n<td>Limit concurrent scrapes</td>\n<td>Reduce scraping parallelism</td>\n</tr>\n<tr>\n<td>Storage Engine</td>\n<td>50% of system memory</td>\n<td>Limit chunk cache and WAL buffer size</td>\n<td>Flush caches more aggressively</td>\n</tr>\n<tr>\n<td>Query Engine</td>\n<td>20% of system memory</td>\n<td>Per-query limits and queue depth</td>\n<td>Reject or queue new queries</td>\n</tr>\n<tr>\n<td>System Overhead</td>\n<td>5% reserved</td>\n<td>OS and other processes</td>\n<td>Emergency garbage collection</td>\n</tr>\n</tbody></table>\n<h4 id=\"cardinality-explosion-prevention\">Cardinality Explosion Prevention</h4>\n<p>Label cardinality explosion represents one of the most dangerous failure modes in metrics systems because it can cause exponential growth in memory usage and storage requirements. The system implements multiple layers of cardinality control that detect, prevent, and mitigate high-cardinality scenarios.</p>\n<p>The <code>CardinalityTracker</code> monitors series creation rates and label combination patterns to detect cardinality explosions early:</p>\n<p><strong>Per-Metric Cardinality Limits</strong>: Each metric name has a maximum number of allowed label combinations (typically 10,000 series). New series exceeding this limit are rejected with specific error messages.</p>\n<p><strong>Label Value Limits</strong>: Individual labels have maximum numbers of allowed distinct values (typically 1,000 values per label name). This prevents single labels from creating excessive cardinality.</p>\n<p><strong>Series Creation Rate Limits</strong>: The system monitors the rate of new time series creation and temporarily blocks new series when creation rates exceed thresholds that indicate cardinality explosions.</p>\n<p><strong>High-Cardinality Detection</strong>: Background monitoring identifies metrics and labels contributing most to total cardinality, providing operators with actionable information about cardinality sources.</p>\n<p>The cardinality protection system provides detailed error messages that help users understand the cardinality violation and suggest alternatives:</p>\n<table>\n<thead>\n<tr>\n<th>Cardinality Limit</th>\n<th>Threshold</th>\n<th>Error Message</th>\n<th>Remediation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Per-Metric Series</td>\n<td>10,000 series</td>\n<td>Metric &#39;X&#39; exceeds 10,000 series limit</td>\n<td>Use fewer label dimensions or aggregate at source</td>\n</tr>\n<tr>\n<td>Label Value Count</td>\n<td>1,000 values</td>\n<td>Label &#39;Y&#39; has too many distinct values</td>\n<td>Use label value prefixes or reduce granularity</td>\n</tr>\n<tr>\n<td>Series Creation Rate</td>\n<td>1,000/minute</td>\n<td>New series creation rate too high</td>\n<td>Review metric instrumentation for cardinality bugs</td>\n</tr>\n<tr>\n<td>Total System Series</td>\n<td>1,000,000 series</td>\n<td>System approaching maximum series capacity</td>\n<td>Review all metrics for optimization opportunities</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Cardinality Limits Too Restrictive</strong>\nSetting cardinality limits too low can prevent legitimate use cases where high cardinality is necessary for proper observability. The limits should be based on actual system capacity and usage patterns rather than arbitrary conservative values. Monitor actual cardinality usage and adjust limits based on system performance and storage capacity.</p>\n<h4 id=\"graceful-degradation-mechanisms\">Graceful Degradation Mechanisms</h4>\n<p>When the system approaches resource limits, graceful degradation maintains partial functionality rather than complete system failure. This requires careful prioritization of operations and temporary reduction in quality or completeness of service.</p>\n<p>The system implements several graceful degradation strategies:</p>\n<p><strong>Scraping Degradation</strong>: When memory or CPU usage is high, the scrape engine reduces scraping frequency for less critical targets while maintaining normal intervals for high-priority metrics.</p>\n<p><strong>Storage Degradation</strong>: The storage engine may temporarily disable compression or reduce cache sizes to free memory for critical operations like query processing.</p>\n<p><strong>Query Degradation</strong>: The query engine may impose stricter limits on query complexity, reduce the maximum number of returned data points, or provide approximate results instead of exact calculations.</p>\n<p><strong>Service Shedding</strong>: In extreme overload situations, the system may temporarily reject new requests (queries or metric ingestion) while processing the existing workload.</p>\n<p>Each degradation mechanism provides clear indication of reduced service levels and estimates of when normal operation will resume:</p>\n<table>\n<thead>\n<tr>\n<th>Degradation Type</th>\n<th>Trigger Condition</th>\n<th>Service Impact</th>\n<th>Recovery Condition</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Reduced Scraping</td>\n<td>CPU &gt; 80% for 5 minutes</td>\n<td>50% longer intervals for low-priority targets</td>\n<td>CPU &lt; 60% for 2 minutes</td>\n</tr>\n<tr>\n<td>Storage Throttling</td>\n<td>Memory &gt; 85%</td>\n<td>Compression disabled, smaller write batches</td>\n<td>Memory &lt; 75%</td>\n</tr>\n<tr>\n<td>Query Limiting</td>\n<td>Query queue &gt; 50 requests</td>\n<td>Stricter per-query limits, longer timeouts</td>\n<td>Queue &lt; 10 requests</td>\n</tr>\n<tr>\n<td>Request Rejection</td>\n<td>System overload detected</td>\n<td>HTTP 503 responses with retry-after headers</td>\n<td>Load drops below threshold</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Graceful Degradation Over Complete Failure</strong></p>\n<ul>\n<li><strong>Context</strong>: Resource exhaustion traditionally causes complete system failure and loss of observability</li>\n<li><strong>Options Considered</strong>: Fail fast when resources exhausted, graceful degradation with reduced service, horizontal scaling requirements</li>\n<li><strong>Decision</strong>: Implement graceful degradation that maintains critical functionality while reducing less important operations</li>\n<li><strong>Rationale</strong>: Partial observability during high load is much more valuable than complete loss of metrics during peak demand</li>\n<li><strong>Consequences</strong>: More complex resource management but maintains system availability during stress conditions</li>\n</ul>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Handling</td>\n<td>Standard Go error values with fmt.Errorf</td>\n<td>Structured errors with github.com/pkg/errors for stack traces</td>\n</tr>\n<tr>\n<td>Circuit Breaker</td>\n<td>Custom implementation with simple counters</td>\n<td>github.com/sony/gobreaker for production features</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>Basic success/failure counters</td>\n<td>Exponential moving averages for failure rates</td>\n</tr>\n<tr>\n<td>Resource Limits</td>\n<td>Simple sync.WaitGroup semaphores</td>\n<td>Context-based cancellation with resource tracking</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Standard log package</td>\n<td>Structured logging with github.com/sirupsen/logrus</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure\">File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n├── errors/\n│   ├── errors.go              ← Custom error types and utilities\n│   ├── circuit_breaker.go     ← Circuit breaker implementation\n│   └── health.go              ← Target health tracking\n├── scrape/\n│   ├── health.go              ← Target health management\n│   ├── timeout.go             ← Multi-layer timeout handling\n│   └── parser_errors.go       ← Metrics parsing error handling\n├── storage/\n│   ├── consistency.go         ← Data consistency validation\n│   ├── recovery.go            ← WAL and index recovery\n│   └── disk_monitor.go        ← Disk space monitoring\n├── query/\n│   ├── resource_limits.go     ← Query resource protection\n│   ├── parse_errors.go        ← Query parsing error handling\n│   └── coordinator.go         ← Query concurrency management\n└── system/\n    ├── resource_monitor.go    ← System-wide resource tracking\n    └── degradation.go         ← Graceful degradation logic</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Complete error handling infrastructure that provides structured error types and utilities:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/errors/errors.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ErrorType categorizes different kinds of errors for handling decisions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ErrorType</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeTransient</span><span style=\"color:#B392F0\">    ErrorType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"transient\"</span><span style=\"color:#6A737D\">    // Retry may succeed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypePermanent</span><span style=\"color:#B392F0\">    ErrorType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"permanent\"</span><span style=\"color:#6A737D\">    // No point retrying</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeRateLimit</span><span style=\"color:#B392F0\">    ErrorType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"rate_limit\"</span><span style=\"color:#6A737D\">   // Need backoff</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeResource</span><span style=\"color:#B392F0\">     ErrorType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"resource\"</span><span style=\"color:#6A737D\">     // Resource exhaustion</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricsError provides structured error information with context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricsError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type       </span><span style=\"color:#B392F0\">ErrorType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cause      </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Context    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> e.Cause </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> (caused by: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">, e.Component, e.Operation, e.Message, e.Cause)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, e.Component, e.Operation, e.Message)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTransientError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cause</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsError</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MetricsError</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Type:      ErrorTypeTransient,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Component: component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Operation: operation,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Message:   message,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cause:     cause,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Context:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewPermanentError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cause</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsError</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MetricsError</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Type:      ErrorTypePermanent,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Component: component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Operation: operation,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Message:   message,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Cause:     cause,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Context:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IsRetryable determines if an error should trigger retry logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> IsRetryable</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> metricsErr, ok </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> err.(</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsError</span><span style=\"color:#E1E4E8\">); ok {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> metricsErr.Type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> ErrorTypeTransient </span><span style=\"color:#F97583\">||</span><span style=\"color:#E1E4E8\"> metricsErr.Type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> ErrorTypeRateLimit</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Complete circuit breaker implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/errors/circuit_breaker.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitClosed</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitOpen</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CircuitHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu                  </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state              </span><span style=\"color:#B392F0\">CircuitState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureCount       </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successCount       </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFailureTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastSuccessTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failureThreshold   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successThreshold   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout           </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    onStateChange     </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">from</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">to</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCircuitBreaker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">failureThreshold</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">successThreshold</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        state:            CircuitClosed,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        failureThreshold: failureThreshold,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        successThreshold: successThreshold,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout:          timeout,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Call</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">fn</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">cb.</span><span style=\"color:#B392F0\">allowRequest</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> NewPermanentError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"circuit_breaker\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"call\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"circuit breaker is open\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> fn</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.</span><span style=\"color:#B392F0\">recordFailure</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.</span><span style=\"color:#B392F0\">recordSuccess</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">allowRequest</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> cb.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> cb.state {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> CircuitClosed:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> CircuitOpen:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(cb.lastFailureTime) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> cb.timeout</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> CircuitHalfOpen:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recordSuccess</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> cb.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.successCount</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.lastSuccessTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> cb.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitHalfOpen </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> cb.successCount </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> cb.successThreshold {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.</span><span style=\"color:#B392F0\">setState</span><span style=\"color:#E1E4E8\">(CircuitClosed)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.failureCount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.successCount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">recordFailure</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> cb.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.failureCount</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.lastFailureTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> cb.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitClosed </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> cb.failureCount </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> cb.failureThreshold {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.</span><span style=\"color:#B392F0\">setState</span><span style=\"color:#E1E4E8\">(CircuitOpen)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> cb.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitHalfOpen {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.</span><span style=\"color:#B392F0\">setState</span><span style=\"color:#E1E4E8\">(CircuitOpen)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cb.successCount </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CircuitBreaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">setState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">newState</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    oldState </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> cb.state</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cb.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> newState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> cb.onStateChange </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> oldState </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> newState {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        go</span><span style=\"color:#E1E4E8\"> cb.</span><span style=\"color:#B392F0\">onStateChange</span><span style=\"color:#E1E4E8\">(oldState, newState)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p>Target health management with state transitions:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/scrape/health.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">th </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TargetHealth</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordScrapeResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">success</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    th.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> th.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    th.lastScrapeTime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> success {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Reset consecutive failure counter to 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Check if we should transition from HealthDown or HealthDegraded to better state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Update lastSuccessTime and record scrape duration for metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Hint: Use transition thresholds from health state table above</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Increment consecutive failure counter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Check if failure count crosses threshold for state degradation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Store error information for diagnostic purposes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Update exponential backoff interval if in HealthDown state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Hint: Max backoff should be 5 minutes, use exponential growth with jitter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">th </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TargetHealth</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ShouldScrape</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    th.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> th.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check current health state and return appropriate scrape decision</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For HealthUp: return true with normal interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For HealthDegraded: return true with 2x normal interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For HealthDown: return true only if backoff period has elapsed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Calculate next scrape time based on state and backoff strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Since(th.lastScrapeTime) to check if enough time has passed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Storage disk space monitoring with emergency retention:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/storage/disk_monitor.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiskMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckDiskSpace</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiskSpaceStatus</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Use syscall.Statfs (Unix) or similar to get filesystem stats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate available space percentage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Determine alert level based on thresholds from disk space table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If below emergency threshold, trigger immediate retention cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return status with recommended actions for storage engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Available space = (free blocks * block size) / (total blocks * block size)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">dm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DiskMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">TriggerEmergencyRetention</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">targetSpacePercent</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate how much data needs to be deleted to reach target space</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Identify oldest data chunks that can be safely deleted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Delete chunks in order of age, prioritizing low-importance metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update indexes to remove references to deleted chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Force garbage collection and filesystem sync to reclaim space immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use priority levels stored in SeriesMetadata to determine deletion order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>Query resource limiting with graceful degradation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/query/resource_limits.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ql </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteWithLimits</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fn</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if query exceeds complexity limits (time range, series count estimate)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Acquire semaphore slot for concurrent query limiting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set up memory tracking for this query execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create timeout context if none provided or if shorter than configured limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Execute query function with resource monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Check memory usage during execution, terminate if limit exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Release resources and update usage statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context.WithTimeout and context.WithCancel for resource control</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ql </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryLimiter</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EstimateQueryComplexity</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">query</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComplexityEstimate</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse query to identify metric selectors and operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Estimate number of time series that would be examined</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Calculate memory requirements based on time range and series count  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Identify expensive operations (regex matching, aggregation functions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return complexity estimate with resource requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use simple heuristics like series_count * time_range_hours * 8 bytes per sample</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Target Health Management Verification</strong>:</p>\n<ul>\n<li>Run scrape engine with mix of healthy and failing targets</li>\n<li>Verify state transitions follow the health state table exactly</li>\n<li>Check that backoff intervals increase exponentially for failing targets</li>\n<li>Confirm circuit breaker prevents wasted requests to consistently failing targets</li>\n</ul>\n<p><strong>Storage Error Recovery Testing</strong>:</p>\n<ul>\n<li>Simulate disk full condition and verify graceful degradation</li>\n<li>Kill storage process during writes and verify WAL recovery</li>\n<li>Corrupt index files and verify automatic rebuild from chunk data  </li>\n<li>Verify consistency checker detects and reports data inconsistencies</li>\n</ul>\n<p><strong>Query Resource Protection Validation</strong>:</p>\n<ul>\n<li>Submit queries exceeding memory limits and verify they&#39;re terminated</li>\n<li>Run many concurrent queries and verify queueing/rejection behavior</li>\n<li>Test very long time range queries get appropriate error messages</li>\n<li>Verify high-cardinality queries are rejected with specific feedback</li>\n</ul>\n<p><strong>System-Wide Degradation Testing</strong>:</p>\n<ul>\n<li>Load system to resource limits and verify graceful degradation activates</li>\n<li>Confirm partial functionality maintained during resource exhaustion</li>\n<li>Verify system automatically recovers when load decreases</li>\n<li>Check that degradation status is clearly visible in system metrics</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Targets show as down but are accessible</td>\n<td>Circuit breaker stuck open</td>\n<td>Check circuit breaker state and failure history</td>\n<td>Reset circuit breaker or adjust thresholds</td>\n</tr>\n<tr>\n<td>Storage consuming excessive disk space</td>\n<td>Retention policy not running</td>\n<td>Check disk monitor logs and retention job status</td>\n<td>Manually trigger retention cleanup</td>\n</tr>\n<tr>\n<td>Queries timing out frequently</td>\n<td>Resource limits too restrictive</td>\n<td>Check query complexity estimates and actual resource usage</td>\n<td>Increase limits or optimize queries</td>\n</tr>\n<tr>\n<td>Memory usage growing indefinitely</td>\n<td>Cardinality explosion not detected</td>\n<td>Check series creation rates and label cardinality</td>\n<td>Implement stricter cardinality limits</td>\n</tr>\n<tr>\n<td>WAL recovery taking very long</td>\n<td>Large WAL files with many segments</td>\n<td>Check WAL segment sizes and rotation frequency</td>\n<td>Adjust WAL rotation settings</td>\n</tr>\n<tr>\n<td>Parse errors for valid metrics format</td>\n<td>Strict validation rejecting valid data</td>\n<td>Check parsing error logs for specific validation failures</td>\n<td>Relax validation rules or fix data format</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-and-validation-strategy\">Testing and Validation Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides comprehensive testing approaches for all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4), along with end-to-end validation of their integrated operation.</p>\n</blockquote>\n<h3 id=\"the-quality-assurance-lighthouse-mental-model\">The Quality Assurance Lighthouse Mental Model</h3>\n<p>Think of our testing strategy like a lighthouse system guiding ships safely to shore. Just as lighthouses provide multiple layers of navigation safety—from close-in harbor lights to distant beacon warnings—our testing strategy provides multiple concentric layers of validation. The innermost ring consists of <strong>component testing</strong> that validates individual pieces work correctly in isolation, like harbor lights that help boats navigate the immediate docking area. The middle ring contains <strong>integration testing</strong> that verifies components work together properly, like channel markers that guide ships through connected waterways. The outermost ring encompasses <strong>end-to-end testing</strong> that validates complete user workflows, like the main lighthouse beacon that provides navigation from miles offshore. Performance validation acts as the weather monitoring system, ensuring our lighthouse remains operational under all conditions—calm seas and storms alike.</p>\n<p>This layered approach ensures that problems are caught as early and specifically as possible. A unit test failure tells us exactly which component is broken, like a harbor light malfunction points to a specific dock. An integration test failure indicates communication problems between components, like misaligned channel markers. An end-to-end test failure suggests workflow problems that might only appear under realistic usage patterns, like a lighthouse whose beam is blocked by unexpected fog.</p>\n<h3 id=\"component-testing\">Component Testing</h3>\n<p>Component testing validates each system component in isolation, using test doubles to eliminate external dependencies and focus on the component&#39;s core logic and behavior. This isolation allows us to test error conditions, edge cases, and performance characteristics that would be difficult to reproduce in integrated scenarios.</p>\n<h4 id=\"metrics-data-model-testing\">Metrics Data Model Testing</h4>\n<p>The metrics data model components require thorough testing of their semantic behaviors, thread safety, and cardinality management capabilities. Each metric type has specific behavioral contracts that must be validated under both normal and stress conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Scope</th>\n<th>Key Validations</th>\n<th>Test Techniques</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Counter Semantics</td>\n<td>Counter increment behavior</td>\n<td>Monotonic increase, no decreases allowed, overflow handling</td>\n<td>Property-based testing with random increments</td>\n</tr>\n<tr>\n<td>Gauge Semantics</td>\n<td>Gauge set/add behavior</td>\n<td>Arbitrary values allowed, thread-safe updates</td>\n<td>Concurrent goroutines setting values</td>\n</tr>\n<tr>\n<td>Histogram Semantics</td>\n<td>Bucket observation behavior</td>\n<td>Correct bucket assignment, sum/count accuracy</td>\n<td>Statistical validation of distributions</td>\n</tr>\n<tr>\n<td>Label Validation</td>\n<td>Label cardinality control</td>\n<td>Name/value length limits, character restrictions, cardinality tracking</td>\n<td>Fuzzing with invalid characters and long strings</td>\n</tr>\n<tr>\n<td>Thread Safety</td>\n<td>Concurrent metric updates</td>\n<td>No data races, consistent reads during writes</td>\n<td>Race detector with high concurrency</td>\n</tr>\n</tbody></table>\n<p><strong>Counter Behavior Testing:</strong> Counter tests verify that increment operations maintain monotonic behavior and that attempts to decrease values are properly rejected. The test suite generates random sequences of increment operations and validates that the final value equals the sum of all increments. Thread safety testing spawns multiple goroutines performing concurrent increments and verifies that no increments are lost due to race conditions.</p>\n<p><strong>Gauge Behavior Testing:</strong> Gauge tests validate that set and add operations work correctly with both positive and negative values. Unlike counters, gauges must accept any floating-point value including negative numbers, zero, and special values like positive/negative infinity. Concurrent testing verifies that rapid updates from multiple goroutines don&#39;t corrupt the stored value.</p>\n<p><strong>Histogram Distribution Testing:</strong> Histogram tests validate that observed values are assigned to the correct buckets and that sum and count fields maintain accuracy. Statistical testing generates known distributions and verifies that the histogram buckets capture the distribution shape correctly. Edge case testing validates behavior with extreme values, NaN, and infinity.</p>\n<p><strong>Label Cardinality Testing:</strong> Label validation testing ensures that the cardinality tracking system correctly prevents label explosion attacks. Tests attempt to create series with high-cardinality label combinations and verify that appropriate limits are enforced. Performance testing measures memory usage as cardinality increases to validate that growth remains within acceptable bounds.</p>\n<blockquote>\n<p><strong>Decision: Property-Based Testing for Metric Semantics</strong></p>\n<ul>\n<li><strong>Context</strong>: Metric types have mathematical properties that must hold across all possible input sequences</li>\n<li><strong>Options Considered</strong>: Example-based unit tests, property-based testing, formal verification</li>\n<li><strong>Decision</strong>: Property-based testing with example-based edge cases</li>\n<li><strong>Rationale</strong>: Properties like counter monotonicity are universal invariants that should hold for any valid input sequence. Property-based testing explores the input space more thoroughly than hand-written examples while remaining practical to implement</li>\n<li><strong>Consequences</strong>: Tests catch more edge cases but may be harder to debug when they fail. Requires careful property specification to avoid vacuous tests</li>\n</ul>\n</blockquote>\n<h4 id=\"scrape-engine-testing\">Scrape Engine Testing</h4>\n<p>The scrape engine requires testing of HTTP operations, parsing logic, service discovery, and error handling. Testing focuses on isolation using HTTP test servers and mock service discovery backends to create deterministic test conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Scope</th>\n<th>Key Validations</th>\n<th>Test Techniques</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Scraping</td>\n<td>Target endpoint communication</td>\n<td>Timeout handling, response parsing, error recovery</td>\n<td>HTTP test servers with controllable responses</td>\n</tr>\n<tr>\n<td>Format Parsing</td>\n<td>Prometheus exposition format</td>\n<td>Correct metric extraction, malformed input handling</td>\n<td>Corpus-based testing with valid/invalid samples</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Target discovery and updates</td>\n<td>Dynamic target list updates, health tracking</td>\n<td>Mock discovery backends with scripted changes</td>\n</tr>\n<tr>\n<td>Concurrency Control</td>\n<td>Parallel scraping operations</td>\n<td>No resource leaks, proper cleanup, backpressure handling</td>\n<td>Load testing with many concurrent targets</td>\n</tr>\n<tr>\n<td>Target Health</td>\n<td>Health state transitions</td>\n<td>Accurate failure detection, recovery detection</td>\n<td>Fault injection with network simulation</td>\n</tr>\n</tbody></table>\n<p><strong>HTTP Transport Testing:</strong> HTTP scraping tests use Go&#39;s <code>httptest</code> package to create controllable HTTP servers that can simulate various response conditions. Test servers can introduce delays to validate timeout handling, return malformed responses to test error recovery, and simulate network failures to verify retry logic. Connection pooling and resource cleanup testing ensures that scraping doesn&#39;t leak HTTP connections or goroutines.</p>\n<p><strong>Exposition Format Parsing:</strong> Parser testing validates correct extraction of metrics from Prometheus exposition format text. A comprehensive test corpus includes well-formed metrics, malformed input that should be rejected, edge cases like empty values or unusual characters, and large payloads that test memory usage. Fuzzing techniques generate random input to discover parsing crashes or hangs.</p>\n<p><strong>Service Discovery Testing:</strong> Service discovery testing uses mock backends that implement the <code>TargetDiscoverer</code> interface to provide scripted target updates. Tests validate that target additions, removals, and label updates are processed correctly and that the scrape engine adapts its scraping schedule accordingly. Performance testing measures the time to detect and process large target set changes.</p>\n<p><strong>Concurrent Scraping Testing:</strong> Concurrency testing spawns hundreds of mock targets and validates that the scrape engine can handle the load without resource exhaustion. Tests monitor goroutine counts, memory usage, and file descriptor usage to detect leaks. Backpressure testing validates that the scrape engine gracefully handles storage slowdowns without dropping data or consuming excessive memory.</p>\n<p>⚠️ <strong>Pitfall: Testing Against Real HTTP Endpoints</strong>\nUsing real HTTP endpoints in component tests makes tests non-deterministic and dependent on external services. The tests become slow, flaky, and may fail due to network issues unrelated to the code being tested. Instead, use <code>httptest.Server</code> to create local test servers that can simulate any response pattern needed for testing. This makes tests fast, reliable, and completely controllable.</p>\n<h4 id=\"time-series-storage-testing\">Time Series Storage Testing</h4>\n<p>Storage engine testing focuses on data durability, compression correctness, query performance, and recovery behavior. Testing emphasizes validation of complex concurrent scenarios where reads and writes happen simultaneously.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Scope</th>\n<th>Key Validations</th>\n<th>Test Techniques</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Durability</td>\n<td>Write-ahead logging</td>\n<td>WAL recovery, corruption detection, consistency</td>\n<td>Process restart simulation, disk failure injection</td>\n</tr>\n<tr>\n<td>Compression Accuracy</td>\n<td>Gorilla compression algorithm</td>\n<td>Lossless compression, decompression accuracy</td>\n<td>Statistical validation with real-world data patterns</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td>Series selection and retrieval</td>\n<td>Index efficiency, memory usage, query timeouts</td>\n<td>Benchmark testing with varying cardinality levels</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Read/write coordination</td>\n<td>Data consistency, no corruption, fair scheduling</td>\n<td>High-concurrency stress testing</td>\n</tr>\n<tr>\n<td>Storage Limits</td>\n<td>Disk usage and retention</td>\n<td>Cleanup effectiveness, emergency procedures</td>\n<td>Disk space simulation with controlled limits</td>\n</tr>\n</tbody></table>\n<p><strong>Write-Ahead Log Testing:</strong> WAL testing validates that all writes are durably recorded before being applied to the main storage. Recovery testing simulates process crashes at various points during write operations and verifies that recovery produces consistent state. Corruption detection testing deliberately corrupts WAL files and validates that the recovery process detects and handles the corruption appropriately.</p>\n<p><strong>Compression Algorithm Testing:</strong> Gorilla compression testing validates that the delta-of-delta timestamp encoding and XOR value encoding produce bit-accurate results. Test data includes regular timestamp intervals, irregular intervals, and pathological cases like duplicate timestamps or wildly varying values. Decompression testing ensures that compressed data can be accurately reconstructed without any data loss.</p>\n<p><strong>Index Performance Testing:</strong> Index testing validates that the inverted indexes provide efficient series lookup across different cardinality levels. Benchmark tests measure lookup performance as the number of series and labels grows. Memory usage testing tracks index memory consumption and validates that it remains within expected bounds as data volume increases.</p>\n<p><strong>Concurrent Access Testing:</strong> Concurrency testing spawns multiple goroutines performing simultaneous reads and writes and validates that the data remains consistent. Testing uses techniques like read-after-write validation to ensure that writes are immediately visible to subsequent reads. Lock contention testing measures performance under high concurrency and validates that the system provides fair access to both readers and writers.</p>\n<p>⚠️ <strong>Pitfall: Insufficient WAL Recovery Testing</strong>\nMany storage systems fail catastrophically because WAL recovery wasn&#39;t tested thoroughly. It&#39;s not enough to test that recovery works when the process shuts down cleanly—you must test recovery after crashes during critical operations like WAL rotation, index updates, and chunk compression. Use tools like <code>kill -9</code> or controlled process termination to simulate realistic crash scenarios.</p>\n<h4 id=\"query-engine-testing\">Query Engine Testing</h4>\n<p>Query engine testing validates PromQL parsing correctness, execution accuracy, and resource management. Testing emphasizes mathematical accuracy of aggregations and proper handling of time-based operations.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Scope</th>\n<th>Key Validations</th>\n<th>Test Techniques</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PromQL Parsing</td>\n<td>Expression parsing accuracy</td>\n<td>AST correctness, syntax error handling, operator precedence</td>\n<td>Grammar-based test generation</td>\n</tr>\n<tr>\n<td>Label Matching</td>\n<td>Series filtering logic</td>\n<td>Regex performance, exact matching, negative selectors</td>\n<td>Large-scale cardinality testing</td>\n</tr>\n<tr>\n<td>Aggregation Math</td>\n<td>Mathematical correctness</td>\n<td>Sum, average, quantile accuracy, grouping behavior</td>\n<td>Statistical validation against known results</td>\n</tr>\n<tr>\n<td>Range Queries</td>\n<td>Time-based operations</td>\n<td>Interpolation accuracy, staleness handling, step alignment</td>\n<td>Time series simulation with known patterns</td>\n</tr>\n<tr>\n<td>Resource Limits</td>\n<td>Memory and execution time</td>\n<td>Query timeouts, memory limits, complexity estimation</td>\n<td>Adversarial query construction</td>\n</tr>\n</tbody></table>\n<p><strong>PromQL Parser Testing:</strong> Parser testing validates that PromQL expressions are correctly converted into executable abstract syntax trees. Test cases include all supported operators, functions, and syntax constructs. Error handling testing ensures that invalid queries produce helpful error messages with specific locations of syntax problems. Precedence testing validates that complex expressions with multiple operators are parsed with correct operator precedence.</p>\n<p><strong>Label Selector Testing:</strong> Label matching testing validates that label selectors correctly filter time series based on label values. Performance testing measures regex matching speed with complex patterns and large label sets. Edge case testing validates behavior with empty labels, special characters, and Unicode content in label values.</p>\n<p><strong>Mathematical Accuracy Testing:</strong> Aggregation testing validates that mathematical operations produce numerically accurate results. Statistical testing compares query results against independently computed expected values. Edge case testing validates handling of special floating-point values like NaN, infinity, and denormal numbers. Precision testing ensures that aggregations maintain accuracy even with large numbers of input values.</p>\n<p><strong>Time Range Query Testing:</strong> Range query testing validates that queries over time windows produce correct results with proper interpolation and staleness handling. Test data includes regular and irregular timestamp patterns. Boundary condition testing validates behavior at query start and end times, including proper handling of data points that fall exactly on boundaries.</p>\n<blockquote>\n<p><strong>Decision: Statistical Validation for Mathematical Operations</strong></p>\n<ul>\n<li><strong>Context</strong>: Aggregation functions must produce mathematically correct results but floating-point arithmetic introduces precision issues</li>\n<li><strong>Options Considered</strong>: Exact arithmetic libraries, statistical validation with tolerance, property-based testing</li>\n<li><strong>Decision</strong>: Statistical validation with appropriate tolerance levels for floating-point operations</li>\n<li><strong>Rationale</strong>: Exact arithmetic is too slow for production use, but we must validate that results are within acceptable precision bounds. Different operations have different precision characteristics that require specific tolerance levels</li>\n<li><strong>Consequences</strong>: Tests must specify appropriate tolerance levels for each operation type. Enables fast floating-point operations while catching significant mathematical errors</li>\n</ul>\n</blockquote>\n<h3 id=\"end-to-end-testing\">End-to-End Testing</h3>\n<p>End-to-end testing validates complete workflows from metrics scraping through storage to querying, using realistic data patterns and operational scenarios. These tests verify that components work together correctly and that the system provides the expected user experience.</p>\n<h4 id=\"complete-scrape-to-query-workflows\">Complete Scrape-to-Query Workflows</h4>\n<p>End-to-end workflow testing validates the entire pipeline from target discovery through data storage to query execution. These tests use real HTTP servers exposing Prometheus-format metrics and execute actual PromQL queries against the stored data.</p>\n<p><strong>Workflow Test Scenarios:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Description</th>\n<th>Validation Points</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Scraping</td>\n<td>Single target with simple metrics</td>\n<td>Target discovery, scraping, storage, basic queries</td>\n<td>All metrics stored correctly, queries return expected values</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Dynamic target addition/removal</td>\n<td>Target list updates, scraping adaptation, data consistency</td>\n<td>New targets automatically discovered and scraped</td>\n</tr>\n<tr>\n<td>High Cardinality</td>\n<td>Many series with complex labels</td>\n<td>Memory usage, query performance, storage efficiency</td>\n<td>System remains responsive under high cardinality load</td>\n</tr>\n<tr>\n<td>Long-Running</td>\n<td>Extended operation over hours/days</td>\n<td>Data retention, resource stability, query consistency</td>\n<td>No memory leaks, stable performance over time</td>\n</tr>\n<tr>\n<td>Failure Recovery</td>\n<td>Network failures and target outages</td>\n<td>Error handling, recovery behavior, data consistency</td>\n<td>Graceful degradation and recovery without data loss</td>\n</tr>\n</tbody></table>\n<p><strong>Basic Scraping Workflow Testing:</strong> Tests start with a simple scenario involving one target exposing counter, gauge, and histogram metrics. The test validates that metrics are discovered, scraped at the correct interval, stored durably, and can be queried accurately. Timing validation ensures that scrapes happen at the expected intervals and that query results reflect the most recent scraped data.</p>\n<p><strong>Service Discovery Integration Testing:</strong> Tests validate dynamic target management by starting with an empty target list and then adding targets through service discovery. The test monitors how quickly new targets are detected and begin being scraped. Target removal testing validates that removed targets stop being scraped and that their data remains queryable for the retention period.</p>\n<p><strong>High Cardinality Stress Testing:</strong> Tests create scenarios with thousands of unique time series by using high-cardinality labels like user IDs or request paths. The test validates that the system can handle the load without excessive memory usage or query performance degradation. Resource monitoring throughout the test ensures that memory growth remains bounded and that garbage collection remains effective.</p>\n<p><strong>Long-Running Stability Testing:</strong> Extended tests run for hours or days to validate system stability under continuous operation. Tests monitor memory usage, file descriptor counts, and query performance over time to detect resource leaks or performance degradation. Data consistency checks validate that older data remains accurate and queryable throughout the extended run.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Long-Running Test Duration</strong>\nMany systems appear stable in short tests but develop problems over longer periods due to resource leaks or gradual performance degradation. End-to-end tests should run for at least several hours, preferably overnight, to detect these issues. Monitor system resources throughout the test and validate that performance remains stable over the entire duration.</p>\n<h4 id=\"multi-component-integration-scenarios\">Multi-Component Integration Scenarios</h4>\n<p>Integration testing validates the interfaces and communication patterns between major system components. These tests focus on data flow, error propagation, and coordination between components.</p>\n<p><strong>Integration Test Categories:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Integration</th>\n<th>Components</th>\n<th>Focus Areas</th>\n<th>Key Validations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scrape-Storage</td>\n<td>ScrapeEngine, StorageEngine</td>\n<td>Data ingestion pipeline</td>\n<td>Sample batching, backpressure handling, durability</td>\n</tr>\n<tr>\n<td>Storage-Query</td>\n<td>StorageEngine, QueryEngine</td>\n<td>Data retrieval pipeline</td>\n<td>Index usage, query optimization, result accuracy</td>\n</tr>\n<tr>\n<td>Cross-Component Error Handling</td>\n<td>All components</td>\n<td>Error propagation and recovery</td>\n<td>Graceful degradation, error isolation, recovery coordination</td>\n</tr>\n<tr>\n<td>Resource Coordination</td>\n<td>All components</td>\n<td>Resource management</td>\n<td>Memory limits, concurrency control, fair resource allocation</td>\n</tr>\n</tbody></table>\n<p><strong>Scrape-Storage Integration:</strong> Tests validate the data flow from scraping to storage, focusing on the sample ingestion pipeline. Backpressure testing validates that storage slowdowns are properly communicated back to the scrape engine to prevent memory exhaustion. Batching tests verify that samples are efficiently grouped for storage operations. Durability testing validates that scraped data survives process restarts and storage failures.</p>\n<p><strong>Storage-Query Integration:</strong> Tests validate that queries can efficiently retrieve data stored by the scrape engine. Index usage testing ensures that label selectors use indexes effectively rather than scanning all series. Query optimization testing validates that the query engine chooses efficient execution plans for complex queries. Cache coherency testing ensures that query results reflect recent writes from the scrape engine.</p>\n<p><strong>Error Handling Coordination:</strong> Integration error testing validates that component failures are properly isolated and don&#39;t cascade to other components. Tests simulate various failure modes like storage disk full, query timeouts, and scrape target failures. Recovery coordination testing validates that components restart in the correct order and re-establish communication properly.</p>\n<p><strong>Resource Management Integration:</strong> Resource coordination testing validates that components cooperate effectively in resource usage. Memory limit testing validates that components respect overall system memory limits rather than competing for resources. Concurrency testing validates that components don&#39;t create excessive goroutines or other concurrent resources that could overwhelm the system.</p>\n<h4 id=\"data-consistency-and-accuracy-validation\">Data Consistency and Accuracy Validation</h4>\n<p>Data accuracy testing validates that the complete system preserves data integrity throughout the scrape-store-query pipeline. These tests focus on numerical accuracy, timestamp preservation, and metadata consistency.</p>\n<p><strong>Accuracy Validation Approaches:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Validation Type</th>\n<th>Methodology</th>\n<th>Test Data</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Numerical Accuracy</td>\n<td>Compare scraped values to query results</td>\n<td>Known metric values from test targets</td>\n<td>Zero data loss, accurate aggregations</td>\n</tr>\n<tr>\n<td>Timestamp Preservation</td>\n<td>Validate timestamp consistency through pipeline</td>\n<td>Synthetic time series with known timestamps</td>\n<td>Timestamps preserved through storage and retrieval</td>\n</tr>\n<tr>\n<td>Metadata Consistency</td>\n<td>Verify label and metric name preservation</td>\n<td>Complex label combinations</td>\n<td>All metadata preserved accurately</td>\n</tr>\n<tr>\n<td>Aggregation Accuracy</td>\n<td>Compare aggregation results to mathematical truth</td>\n<td>Statistical distributions with known properties</td>\n<td>Aggregations match expected mathematical results</td>\n</tr>\n</tbody></table>\n<p><strong>End-to-End Numerical Accuracy:</strong> Tests expose known metric values through test HTTP endpoints and validate that queries return identical values. Counter testing validates that increments are preserved accurately without any loss. Histogram testing validates that bucket counts and sum/count values are preserved correctly. Gauge testing validates that the most recent values are returned accurately by queries.</p>\n<p><strong>Timestamp Precision Testing:</strong> Tests validate that timestamps are preserved accurately throughout the system. Test targets expose metrics with known timestamps and queries validate that the timestamps are stored and retrieved without modification. Time zone testing validates that UTC timestamps are handled consistently regardless of the local system timezone.</p>\n<p><strong>Label and Metadata Preservation:</strong> Tests validate that metric names, label names, label values, and metric metadata are preserved accurately throughout the system. Unicode testing validates proper handling of international characters in labels. Special character testing validates handling of characters that might have special meaning in various system components.</p>\n<p><strong>Mathematical Aggregation Validation:</strong> Tests validate that PromQL aggregations produce mathematically correct results. Statistical testing uses metrics with known distributions and validates that aggregation functions like <code>sum()</code>, <code>avg()</code>, and <code>quantile()</code> produce results within acceptable precision bounds. Cross-validation testing compares query results against independently computed expected values.</p>\n<blockquote>\n<p><strong>Decision: Reference Implementation Validation</strong></p>\n<ul>\n<li><strong>Context</strong>: End-to-end accuracy testing needs ground truth to compare against actual system results</li>\n<li><strong>Options Considered</strong>: Hand-computed expected results, reference implementation, statistical validation</li>\n<li><strong>Decision</strong>: Hybrid approach using reference implementation for complex scenarios and hand-computed results for simple cases</li>\n<li><strong>Rationale</strong>: Hand computation works for simple scenarios but becomes impractical for complex queries. A reference implementation provides automated ground truth generation but may have its own bugs</li>\n<li><strong>Consequences</strong>: Requires maintaining a separate reference implementation but enables comprehensive accuracy validation</li>\n</ul>\n</blockquote>\n<h3 id=\"performance-validation\">Performance Validation</h3>\n<p>Performance validation ensures that the system meets scalability, latency, and resource usage requirements under realistic load conditions. Testing focuses on identifying bottlenecks and validating that performance characteristics meet operational requirements.</p>\n<h4 id=\"load-testing-and-capacity-planning\">Load Testing and Capacity Planning</h4>\n<p>Load testing validates system behavior under realistic operational loads and identifies performance bottlenecks before they impact production usage. Tests progressively increase load while monitoring system behavior and resource usage.</p>\n<p><strong>Load Testing Dimensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Load Dimension</th>\n<th>Test Parameters</th>\n<th>Scaling Range</th>\n<th>Key Metrics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scraping Scale</td>\n<td>Number of targets and metrics per target</td>\n<td>10 to 10,000 targets</td>\n<td>Scrape completion rate, memory usage, CPU usage</td>\n</tr>\n<tr>\n<td>Storage Throughput</td>\n<td>Samples per second ingestion rate</td>\n<td>1K to 1M samples/sec</td>\n<td>Write latency, compression ratio, disk usage</td>\n</tr>\n<tr>\n<td>Query Concurrency</td>\n<td>Simultaneous query execution</td>\n<td>1 to 1,000 concurrent queries</td>\n<td>Query response time, memory per query, throughput</td>\n</tr>\n<tr>\n<td>Data Volume</td>\n<td>Total time series and time range</td>\n<td>1K to 10M series over weeks</td>\n<td>Index performance, query latency, storage efficiency</td>\n</tr>\n</tbody></table>\n<p><strong>Scraping Load Testing:</strong> Scraping load tests progressively increase the number of scrape targets while monitoring scrape completion rates and resource usage. Tests validate that the scrape engine can handle the target number of endpoints within the configured scrape intervals. Memory usage monitoring ensures that high target counts don&#39;t cause memory exhaustion. Network resource testing validates that the system doesn&#39;t exhaust network connections or file descriptors.</p>\n<p><strong>Storage Throughput Testing:</strong> Storage load tests measure how many samples per second the storage engine can handle sustainably. Tests generate synthetic time series data at various ingestion rates while monitoring write latency and resource usage. Compression efficiency testing validates that storage space usage remains within expected bounds as ingestion rate increases. WAL performance testing ensures that write-ahead logging doesn&#39;t become a bottleneck at high ingestion rates.</p>\n<p><strong>Query Concurrency Testing:</strong> Query load tests execute multiple simultaneous queries while measuring response times and resource usage. Tests include a mix of instant queries, range queries, and complex aggregations to simulate realistic query patterns. Memory usage per query is monitored to validate that concurrent queries don&#39;t cause memory exhaustion. Query isolation testing ensures that expensive queries don&#39;t significantly impact the performance of simple queries.</p>\n<p><strong>Long-Term Data Volume Testing:</strong> Volume testing validates performance with large amounts of historical data. Tests create months or years of synthetic time series data and measure how query performance changes as data volume increases. Index scalability testing validates that label lookup performance remains acceptable as the number of unique series grows. Retention policy testing validates that data cleanup operations don&#39;t significantly impact query performance.</p>\n<h4 id=\"bottleneck-identification-and-resource-monitoring\">Bottleneck Identification and Resource Monitoring</h4>\n<p>Performance testing includes comprehensive monitoring to identify bottlenecks and validate that resources are used efficiently. Monitoring focuses on the critical resources that typically limit performance in metrics systems.</p>\n<p><strong>Resource Monitoring Categories:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Resource Category</th>\n<th>Key Metrics</th>\n<th>Monitoring Tools</th>\n<th>Bottleneck Indicators</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Usage</td>\n<td>Heap size, GC frequency, allocation rate</td>\n<td>Runtime profiling, memory profilers</td>\n<td>High GC overhead, allocation spikes</td>\n</tr>\n<tr>\n<td>CPU Utilization</td>\n<td>CPU usage per component, goroutine counts</td>\n<td>CPU profiling, runtime metrics</td>\n<td>High CPU usage, goroutine leaks</td>\n</tr>\n<tr>\n<td>Disk I/O</td>\n<td>Read/write throughput, latency, queue depth</td>\n<td>I/O monitoring, disk stats</td>\n<td>High I/O wait, storage latency spikes</td>\n</tr>\n<tr>\n<td>Network I/O</td>\n<td>Connection counts, bandwidth usage, error rates</td>\n<td>Network monitoring, connection pooling stats</td>\n<td>Connection exhaustion, bandwidth saturation</td>\n</tr>\n</tbody></table>\n<p><strong>Memory Performance Analysis:</strong> Memory monitoring tracks heap usage, garbage collection frequency, and allocation patterns throughout performance tests. Profiling identifies which components and operations consume the most memory. Memory leak detection validates that memory usage remains bounded during extended operation. Gorilla compression efficiency is validated by measuring the compression ratio achieved on realistic data patterns.</p>\n<p><strong>CPU Performance Analysis:</strong> CPU profiling identifies which operations consume the most processing time. Goroutine monitoring ensures that the system doesn&#39;t create excessive concurrent operations. Lock contention analysis identifies synchronization bottlenecks that prevent efficient CPU utilization. Query parsing and execution profiling validates that PromQL operations perform efficiently.</p>\n<p><strong>Disk I/O Performance Analysis:</strong> Disk monitoring measures read and write throughput during various operations. WAL write performance is critical for ingestion throughput. Index read performance affects query latency. Storage compaction operations are monitored to ensure they don&#39;t interfere with normal operations. Disk space usage is tracked to validate that retention policies work effectively.</p>\n<p><strong>Network I/O Performance Analysis:</strong> Network monitoring tracks HTTP connection usage during scraping operations. Connection pooling efficiency is validated to ensure that scraping doesn&#39;t create excessive network overhead. DNS resolution performance is monitored since service discovery can generate significant DNS traffic. Network timeout and retry behavior is validated under various network conditions.</p>\n<h4 id=\"scalability-requirements-verification\">Scalability Requirements Verification</h4>\n<p>Scalability testing validates that the system can handle the target operational scale defined in the system requirements. Tests specifically target the scalability limits identified in the goals and non-goals section.</p>\n<p><strong>Scalability Test Targets:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scalability Requirement</th>\n<th>Target Scale</th>\n<th>Test Approach</th>\n<th>Acceptance Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent Targets</td>\n<td>1,000 scrape targets</td>\n<td>Progressive load increase</td>\n<td>All targets scraped within interval</td>\n</tr>\n<tr>\n<td>Series Cardinality</td>\n<td>1M unique time series</td>\n<td>High-cardinality label generation</td>\n<td>Query performance remains acceptable</td>\n</tr>\n<tr>\n<td>Query Throughput</td>\n<td>100 queries/second</td>\n<td>Concurrent query execution</td>\n<td>Mean response time under 1 second</td>\n</tr>\n<tr>\n<td>Data Retention</td>\n<td>30 days of historical data</td>\n<td>Long-running data accumulation</td>\n<td>Storage usage within 2x of raw data size</td>\n</tr>\n</tbody></table>\n<p><strong>Target Scale Verification:</strong> Scalability tests progressively increase system load until reaching the target scale requirements. Each test level is sustained for sufficient time to validate stability at that scale. Resource usage is monitored throughout scaling tests to identify when resources become constrained. Performance characteristics like query latency and scrape completion rates are tracked to validate that they remain within acceptable bounds at target scale.</p>\n<p><strong>Cardinality Limit Testing:</strong> High-cardinality testing generates time series with label combinations that approach the target cardinality limits. Memory usage is carefully monitored to validate that the system can handle high cardinality without excessive memory consumption. Query performance testing validates that label selectors remain efficient even with high cardinality. Index scalability is validated by measuring lookup performance as cardinality increases.</p>\n<p><strong>Throughput Sustainability Testing:</strong> Throughput tests validate that the system can sustain target throughput rates over extended periods without performance degradation. Load balancing effectiveness is validated by measuring how evenly work is distributed across system resources. Backpressure handling is tested by temporarily constraining resources and validating that the system handles the constraint gracefully without data loss.</p>\n<p>⚠️ <strong>Pitfall: Testing Only Peak Performance</strong>\nMany performance tests only measure short-term peak performance rather than sustained throughput over realistic time periods. Real systems must handle continuous load for hours or days, not just brief spikes. Performance tests should sustain target loads for at least 30 minutes to validate that resource usage remains stable and that performance doesn&#39;t degrade over time.</p>\n<h3 id=\"milestone-verification\">Milestone Verification</h3>\n<p>Milestone verification provides concrete checkpoints to validate successful completion of each development phase. Each milestone includes specific behavioral verification, performance benchmarks, and integration validation steps.</p>\n<h4 id=\"milestone-1-metrics-data-model-verification\">Milestone 1: Metrics Data Model Verification</h4>\n<p>Milestone 1 verification validates that the metrics data model correctly implements counter, gauge, and histogram semantics with proper labeling and cardinality control.</p>\n<p><strong>Verification Checklist:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Verification Category</th>\n<th>Test Procedures</th>\n<th>Expected Results</th>\n<th>Validation Commands</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric Type Behavior</td>\n<td>Create and manipulate each metric type</td>\n<td>Counters only increase, gauges accept any value, histograms distribute correctly</td>\n<td>Unit test suite execution</td>\n</tr>\n<tr>\n<td>Label Functionality</td>\n<td>Create metrics with various label combinations</td>\n<td>Labels attached correctly, cardinality tracking works</td>\n<td>Label validation test execution</td>\n</tr>\n<tr>\n<td>Thread Safety</td>\n<td>Concurrent access to metric instances</td>\n<td>No race conditions, consistent values</td>\n<td>Race detector test execution</td>\n</tr>\n<tr>\n<td>Cardinality Control</td>\n<td>Attempt to create high-cardinality combinations</td>\n<td>Limits enforced, memory usage bounded</td>\n<td>Cardinality stress test execution</td>\n</tr>\n</tbody></table>\n<p><strong>Counter Behavior Verification:</strong> Counter testing validates that increment operations work correctly and that attempts to decrease values are rejected. Overflow testing validates behavior when counter values approach floating-point limits. Thread safety testing uses concurrent goroutines to validate that increments are atomic and no updates are lost.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example verification commands</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#9ECBFF\"> ./internal/metrics/counter_test.go</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -bench=BenchmarkCounterIncrement</span><span style=\"color:#9ECBFF\"> ./internal/metrics/</span></span></code></pre></div>\n\n<p><strong>Gauge Behavior Verification:</strong> Gauge testing validates that both set and add operations work with positive and negative values. Special value testing validates handling of infinity, NaN, and zero values. Concurrent testing validates that rapid updates don&#39;t corrupt the stored value.</p>\n<p><strong>Histogram Behavior Verification:</strong> Histogram testing validates that observations are assigned to correct buckets and that sum and count fields are maintained accurately. Distribution testing uses known statistical distributions and validates that histogram buckets correctly represent the distribution shape.</p>\n<p><strong>Label System Verification:</strong> Label validation testing creates metrics with various label combinations and validates that labels are stored and retrieved correctly. Cardinality testing attempts to create high-cardinality label combinations and validates that appropriate limits are enforced. Unicode testing validates that international characters in label names and values are handled correctly.</p>\n<h4 id=\"milestone-2-scrape-engine-verification\">Milestone 2: Scrape Engine Verification</h4>\n<p>Milestone 2 verification validates that the scrape engine can discover targets, scrape metrics via HTTP, and handle various failure modes gracefully.</p>\n<p><strong>Verification Checklist:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Verification Category</th>\n<th>Test Procedures</th>\n<th>Expected Results</th>\n<th>Validation Commands</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target Discovery</td>\n<td>Configure static and dynamic targets</td>\n<td>Targets discovered and added to scrape list</td>\n<td>Discovery integration test</td>\n</tr>\n<tr>\n<td>HTTP Scraping</td>\n<td>Scrape from test HTTP endpoints</td>\n<td>Metrics successfully retrieved and parsed</td>\n<td>HTTP scraping test</td>\n</tr>\n<tr>\n<td>Error Handling</td>\n<td>Simulate network failures and timeouts</td>\n<td>Graceful failure handling, retry logic works</td>\n<td>Fault injection test</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Add/remove targets dynamically</td>\n<td>Target list updates automatically</td>\n<td>Service discovery test</td>\n</tr>\n</tbody></table>\n<p><strong>Target Discovery Verification:</strong> Discovery testing configures both static targets and mock service discovery and validates that all targets are discovered and added to the scrape schedule. Update testing validates that target list changes are processed correctly. Health tracking testing validates that target health status is maintained accurately.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example verification commands</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/scraper/discovery_test.go</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -timeout=30s</span><span style=\"color:#9ECBFF\"> ./internal/scraper/integration_test.go</span></span></code></pre></div>\n\n<p><strong>HTTP Scraping Verification:</strong> HTTP testing uses <code>httptest.Server</code> to create controllable test endpoints that expose Prometheus-format metrics. Parsing verification validates that various metric types are correctly extracted from the HTTP response. Timeout testing validates that scrapes that exceed the timeout are cancelled properly.</p>\n<p><strong>Error Handling Verification:</strong> Fault injection testing simulates various error conditions including network timeouts, HTTP errors, malformed responses, and DNS failures. Recovery testing validates that the scrape engine recovers correctly when failed targets become available again. Circuit breaker testing validates that consistently failing targets are temporarily bypassed.</p>\n<p><strong>Service Discovery Integration Verification:</strong> Service discovery testing uses mock discovery backends to simulate target additions, removals, and metadata updates. Latency testing measures how quickly target changes are detected and processed. Consistency testing validates that target metadata is correctly propagated from service discovery to scraping operations.</p>\n<h4 id=\"milestone-3-time-series-storage-verification\">Milestone 3: Time Series Storage Verification</h4>\n<p>Milestone 3 verification validates that the storage engine correctly stores time series data with compression, provides durable persistence, and supports efficient queries.</p>\n<p><strong>Verification Checklist:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Verification Category</th>\n<th>Test Procedures</th>\n<th>Expected Results</th>\n<th>Validation Commands</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Persistence</td>\n<td>Write data and restart process</td>\n<td>All data recovered correctly after restart</td>\n<td>WAL recovery test</td>\n</tr>\n<tr>\n<td>Compression Efficiency</td>\n<td>Store various data patterns</td>\n<td>Compression ratios within expected range</td>\n<td>Compression benchmark test</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td>Execute queries on stored data</td>\n<td>Query response times within limits</td>\n<td>Query performance test</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Simultaneous reads and writes</td>\n<td>No data corruption, consistent results</td>\n<td>Concurrency test</td>\n</tr>\n</tbody></table>\n<p><strong>Data Persistence Verification:</strong> Persistence testing writes time series data and then simulates process crashes at various points. Recovery testing validates that all durably committed data is correctly restored after restart. WAL integrity testing validates that write-ahead log recovery produces consistent results.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example verification commands  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/wal_test.go</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -bench=BenchmarkStorageWrite</span><span style=\"color:#9ECBFF\"> ./internal/storage/</span></span></code></pre></div>\n\n<p><strong>Compression Efficiency Verification:</strong> Compression testing stores various time series patterns including regular intervals, irregular intervals, and pathological cases. Compression ratio measurement validates that Gorilla compression achieves expected space savings. Decompression accuracy testing validates that compressed data can be reconstructed without any loss.</p>\n<p><strong>Query Performance Verification:</strong> Query testing executes various query patterns against stored data and measures response times. Index efficiency testing validates that label selectors use indexes effectively. Memory usage testing validates that queries don&#39;t consume excessive memory even with large result sets.</p>\n<p><strong>Concurrent Access Verification:</strong> Concurrency testing spawns multiple goroutines performing simultaneous reads and writes. Data consistency testing validates that concurrent operations don&#39;t corrupt stored data. Lock contention testing measures performance under high concurrency and validates that the system provides fair access to resources.</p>\n<h4 id=\"milestone-4-query-engine-verification\">Milestone 4: Query Engine Verification</h4>\n<p>Milestone 4 verification validates that the query engine correctly parses PromQL expressions, executes queries accurately, and provides expected aggregation functionality.</p>\n<p><strong>Verification Checklist:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Verification Category</th>\n<th>Test Procedures</th>\n<th>Expected Results</th>\n<th>Validation Commands</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PromQL Parsing</td>\n<td>Parse various PromQL expressions</td>\n<td>Correct AST generation, proper error messages</td>\n<td>Parser test suite</td>\n</tr>\n<tr>\n<td>Query Execution</td>\n<td>Execute instant and range queries</td>\n<td>Accurate results, proper error handling</td>\n<td>Query execution test</td>\n</tr>\n<tr>\n<td>Aggregation Functions</td>\n<td>Test sum, avg, max, min, count operations</td>\n<td>Mathematically correct aggregation results</td>\n<td>Aggregation test suite</td>\n</tr>\n<tr>\n<td>Label Matching</td>\n<td>Test exact, regex, and inequality selectors</td>\n<td>Correct series filtering, efficient execution</td>\n<td>Label selector test</td>\n</tr>\n</tbody></table>\n<p><strong>PromQL Parsing Verification:</strong> Parser testing validates that various PromQL expressions are correctly converted to abstract syntax trees. Error handling testing validates that invalid expressions produce helpful error messages. Precedence testing validates that complex expressions are parsed with correct operator precedence.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Example verification commands</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/query/parser_test.go</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -bench=BenchmarkQueryExecution</span><span style=\"color:#9ECBFF\"> ./internal/query/</span></span></code></pre></div>\n\n<p><strong>Query Execution Verification:</strong> Query execution testing validates that both instant and range queries produce accurate results. Time range testing validates that queries over various time windows work correctly. Interpolation testing validates that query results are correctly interpolated between actual data points.</p>\n<p><strong>Aggregation Function Verification:</strong> Mathematical testing validates that aggregation functions produce numerically correct results. Statistical testing uses known distributions and validates that aggregation results match expected mathematical values. Edge case testing validates handling of special values like NaN and infinity in aggregations.</p>\n<p><strong>Label Matching Verification:</strong> Label selector testing validates that exact matching, regex matching, and inequality operators correctly filter time series. Performance testing validates that complex label selectors execute efficiently. Cardinality testing validates that label selectors work correctly even with high-cardinality label sets.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical approaches for building a comprehensive testing framework that validates all aspects of the metrics collection system.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Testing Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing Framework</td>\n<td>Go standard testing package with testify assertions</td>\n<td>Ginkgo BDD framework with Gomega matchers</td>\n</tr>\n<tr>\n<td>HTTP Testing</td>\n<td>httptest package for mock servers</td>\n<td>WireMock or similar for complex HTTP scenarios</td>\n</tr>\n<tr>\n<td>Property-Based Testing</td>\n<td>rapid testing library</td>\n<td>gopter property-based testing framework</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Simple goroutine-based load generation</td>\n<td>k6 or Artillery for sophisticated load patterns</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td>Go built-in pprof</td>\n<td>Continuous profiling with pprof integration</td>\n</tr>\n<tr>\n<td>Test Data Management</td>\n<td>In-memory test fixtures</td>\n<td>Testcontainers for external dependencies</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<p>The testing code should be organized to support both component-level and integration testing with clear separation of concerns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    metrics/\n      counter.go\n      counter_test.go          ← Unit tests for counter behavior\n      gauge_test.go           ← Unit tests for gauge behavior  \n      histogram_test.go       ← Unit tests for histogram behavior\n      labels_test.go          ← Unit tests for label validation\n    scraper/\n      scraper.go\n      scraper_test.go         ← Unit tests for scraping logic\n      discovery_test.go       ← Unit tests for service discovery\n      integration_test.go     ← Integration tests with HTTP servers\n    storage/\n      storage.go\n      storage_test.go         ← Unit tests for storage operations\n      wal_test.go            ← Unit tests for write-ahead log\n      compression_test.go     ← Unit tests for compression algorithms\n    query/\n      parser_test.go          ← Unit tests for PromQL parsing\n      executor_test.go        ← Unit tests for query execution\n  test/\n    integration/\n      end_to_end_test.go      ← Complete workflow tests\n      performance_test.go     ← Load and performance tests\n    fixtures/\n      metrics_samples.txt     ← Sample Prometheus format data\n      test_queries.promql     ← Sample PromQL queries for testing\n    helpers/\n      test_server.go          ← HTTP test server utilities\n      data_generator.go       ← Synthetic data generation</code></pre></div>\n\n<h4 id=\"testing-infrastructure-starter-code\">Testing Infrastructure Starter Code</h4>\n<p>Here&#39;s a complete testing infrastructure that provides the foundation for comprehensive testing:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> testhelpers</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http/httptest</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MockHTTPTarget provides a controllable HTTP endpoint for scraping tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockHTTPTarget</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">httptest</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics    []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    delay      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errorRate  </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMockHTTPTarget creates a new mock target with configurable behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMockHTTPTarget</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics: []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"test_counter 42\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"test_gauge 3.14\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"test_histogram_bucket{le=</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">1.0</span><span style=\"color:#79B8FF\">\\\"</span><span style=\"color:#9ECBFF\">} 10\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target.server </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> httptest.</span><span style=\"color:#B392F0\">NewServer</span><span style=\"color:#E1E4E8\">(http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(target.handleMetrics))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> target</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// URL returns the target's HTTP URL for scraping</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">URL</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> t.server.URL </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"/metrics\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetMetrics updates the metrics exposed by this target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> t.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetDelay configures response delay for timeout testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetDelay</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">delay</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> t.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetErrorRate configures the probability of HTTP errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetErrorRate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rate</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> t.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.errorRate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    delay </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errorRate </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.errorRate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simulate network delay</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> delay </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time.</span><span style=\"color:#B392F0\">Sleep</span><span style=\"color:#E1E4E8\">(delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simulate random errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> rand.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> errorRate {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Simulated server error\"</span><span style=\"color:#E1E4E8\">, http.StatusInternalServerError)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"text/plain\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, metric </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> metrics {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Fprintln</span><span style=\"color:#E1E4E8\">(w, metric)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close shuts down the mock target server</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">t </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.server.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeSeriesGenerator creates synthetic time series data for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeSeriesGenerator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rand </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">rand</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Rand</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTimeSeriesGenerator creates a generator with a fixed seed for reproducible tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTimeSeriesGenerator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seed</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rand: rand.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(rand.</span><span style=\"color:#B392F0\">NewSource</span><span style=\"color:#E1E4E8\">(seed)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateCounterSeries creates a monotonically increasing counter series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateCounterSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    start</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samples </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, count)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> count; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Counters can only increase</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        increment </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> g.rand.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> increment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        samples[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> Sample</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timestamp: start.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(i) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> interval),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Value:     value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> samples</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateGaugeSeries creates a gauge series with random walk behavior  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeSeriesGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateGaugeSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    start</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">count</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samples </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, count)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> g.rand.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> count; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Gauges can increase or decrease</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        change </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> (g.rand.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 20</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> change</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        samples[i] </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> Sample</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timestamp: start.</span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(i) </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> interval),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Value:     value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> samples</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PerformanceMonitor tracks resource usage during tests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PerformanceMonitor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startMemory    </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    measurements   []</span><span style=\"color:#B392F0\">ResourceMeasurement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu             </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ResourceMeasurement</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryUsage   </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GoroutineCount </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CPUUsage      </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewPerformanceMonitor creates a monitor for tracking test performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewPerformanceMonitor</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMonitor</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">PerformanceMonitor</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        startTime: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        measurements: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">ResourceMeasurement</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartMonitoring begins periodic resource measurement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartMonitoring</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(interval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        select</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ctx.</span><span style=\"color:#B392F0\">Done</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#F97583\"> &#x3C;-</span><span style=\"color:#E1E4E8\">ticker.C:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            m.</span><span style=\"color:#B392F0\">takeMeasurement</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">takeMeasurement</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement actual resource measurement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // This would use runtime.ReadMemStats(), runtime.NumGoroutine(), etc.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // For now, provide placeholder implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    measurement </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> ResourceMeasurement</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp:      time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MemoryUsage:    </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// TODO: Get actual memory usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        GoroutineCount: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// TODO: Get actual goroutine count  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CPUUsage:       </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// TODO: Get actual CPU usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.measurements </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(m.measurements, measurement)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetSummary returns performance statistics from monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMonitor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetSummary</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">PerformanceSummary</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(m.measurements) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#B392F0\"> PerformanceSummary</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Calculate actual performance statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> PerformanceSummary</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Duration:       time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(m.startTime),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        PeakMemory:     </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// TODO: Calculate from measurements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        AvgGoroutines:  </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// TODO: Calculate from measurements  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        MaxGoroutines:  </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\">// TODO: Calculate from measurements</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PerformanceSummary</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PeakMemory    </span><span style=\"color:#F97583\">uint64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AvgGoroutines </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxGoroutines </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-testing-logic-skeletons\">Core Testing Logic Skeletons</h4>\n<p>Here are the essential test function signatures with detailed TODO comments for implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> metrics_test</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestCounterBehavior validates counter semantic correctness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestCounterBehavior</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create a new counter instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify initial value is 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call Inc() and verify value increases to 1  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call Add(5) and verify value increases to 6</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Attempt Add(-1) and verify it returns an error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify final value is still 6 (no decrease occurred)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestCounterConcurrency validates thread-safe counter operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestCounterConcurrency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create a new counter instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start 10 goroutines, each incrementing counter 100 times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use sync.WaitGroup to wait for all goroutines to complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify final counter value equals 1000 (10 * 100)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Run test with -race flag to detect race conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use atomic operations or mutex in counter implementation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestHistogramDistribution validates histogram bucket assignment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestHistogramDistribution</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    buckets </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10.0</span><span style=\"color:#E1E4E8\">, math.</span><span style=\"color:#B392F0\">Inf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create histogram with specified bucket boundaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Observe values: 0.5, 2.0, 7.0, 15.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify bucket counts: [1, 1, 1, 1] </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify total count equals 4</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify sum equals 24.5 (0.5 + 2.0 + 7.0 + 15.0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Values should be assigned to first bucket where value &#x3C;= bucket</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestLabelCardinality validates cardinality explosion prevention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestLabelCardinality</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tracker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewCardinalityTracker</span><span style=\"color:#E1E4E8\">(maxSeries </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create metrics with normal cardinality (should succeed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Attempt to create metrics exceeding cardinality limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify that excess metrics are rejected with appropriate error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify that memory usage remains bounded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Test cleanup of expired series to free cardinality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use combination of metric name + label set to identify unique series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestScrapeTargetDiscovery validates service discovery integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestScrapeTargetDiscovery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mockDiscovery </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewMockServiceDiscovery</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scrapeEngine </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewScrapeEngine</span><span style=\"color:#E1E4E8\">(config, storage, logger)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start scrape engine with empty target list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add targets via mock service discovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify targets are discovered within discovery interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove targets via service discovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify removed targets stop being scraped</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify target metadata is correctly propagated</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Mock service discovery should implement TargetDiscoverer interface</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestScrapeErrorHandling validates graceful failure handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestScrapeErrorHandling</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewMockHTTPTarget</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> target.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Configure target to return HTTP 500 errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify scrape engine records failure and continues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Configure target to timeout on requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify scrape engine cancels request and marks target down</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Configure target to return malformed metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify partial success - valid metrics stored, invalid rejected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use target.SetErrorRate() and target.SetDelay() for fault injection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestStorageCompression validates Gorilla compression accuracy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestStorageCompression</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate time series with regular 15-second intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Compress samples using GorillaCompressor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Decompress and verify all samples match exactly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Measure compression ratio (should be &#x3C; 2 bytes/sample)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Test with irregular timestamps and verify accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test with pathological data (constant values, huge jumps)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Gorilla compression works best with regular intervals and smooth changes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestStorageRecovery validates WAL recovery after crash</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestStorageRecovery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tempDir </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> t.</span><span style=\"color:#B392F0\">TempDir</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create storage engine and write test data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Simulate crash by closing storage without clean shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create new storage engine instance with same data directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify all committed data is recovered correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify uncommitted data in WAL is replayed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test recovery with corrupted WAL entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: WAL should be replayed in order, corrupt entries should be detected</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestQueryParsing validates PromQL expression parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestQueryParsing</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewExpressionParser</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse simple metric selector: `http_requests_total`</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify AST contains MetricSelectorNode with correct name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse selector with labels: `http_requests_total{method=\"GET\"}`</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify label matchers are parsed correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Parse complex expression with functions: `rate(http_requests_total[5m])`</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Parse invalid expressions and verify helpful error messages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Build AST recursively, respect operator precedence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestQueryExecution validates query result accuracy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestQueryExecution</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Store known time series data in storage engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute instant query and verify results match expected values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute range query and verify all timestamps are covered</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test label selector queries with exact and regex matching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Test aggregation queries (sum, avg, max, min, count)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Verify mathematical accuracy of aggregation results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use statistical validation with appropriate floating-point tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestEndToEndWorkflow validates complete scrape-to-query pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestEndToEndWorkflow</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start mock HTTP target exposing known metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Configure scrape engine to scrape the target</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Wait for metrics to be scraped and stored</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute queries against stored metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify query results match original metrics from target</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test with target failures and recovery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use eventually assertions for asynchronous scraping operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BenchmarkStorageWrite measures storage write performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> BenchmarkStorageWrite</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">b</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">B</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewStorageEngine</span><span style=\"color:#E1E4E8\">(config, logger)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samples </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> generateTestSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    b.</span><span style=\"color:#B392F0\">ResetTimer</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Measure samples per second write throughput</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Measure write latency distribution (p50, p95, p99)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Measure memory allocation per write operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test with varying batch sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Report compression ratio achieved</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use b.N for iteration count, benchmark multiple scenarios</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestResourceLimits validates system behavior under resource constraints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestResourceLimits</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Configure system with low memory limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate high-cardinality metrics to approach limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify system applies backpressure before exhausting memory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test query limits with complex queries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify system remains responsive under resource pressure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Test disk space limits and emergency retention</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Monitor resource usage throughout test, set aggressive limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>Each milestone completion should be validated with these specific checkpoints:</p>\n<p><strong>Milestone 1 Checkpoint - Metrics Data Model:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run all metrics tests with race detection</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -race</span><span style=\"color:#9ECBFF\"> ./internal/metrics/...</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Benchmark metric operations performance  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -bench=.</span><span style=\"color:#9ECBFF\"> ./internal/metrics/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected output: All tests pass, no race conditions detected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Benchmark results should show sub-microsecond operation times</span></span></code></pre></div>\n\n<p><strong>Milestone 2 Checkpoint - Scrape Engine:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run scrape engine tests</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -timeout=30s</span><span style=\"color:#9ECBFF\"> ./internal/scraper/...</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run integration test with HTTP targets</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -run=TestScrapeIntegration</span><span style=\"color:#9ECBFF\"> ./test/integration/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Targets discovered, metrics scraped, HTTP errors handled gracefully</span></span></code></pre></div>\n\n<p><strong>Milestone 3 Checkpoint - Time Series Storage:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test storage with crash recovery</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -run=TestStorageRecovery</span><span style=\"color:#9ECBFF\"> ./internal/storage/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Benchmark write performance</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -bench=BenchmarkStorageWrite</span><span style=\"color:#9ECBFF\"> ./internal/storage/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Recovery works correctly, write throughput > 10K samples/sec</span></span></code></pre></div>\n\n<p><strong>Milestone 4 Checkpoint - Query Engine:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test PromQL parsing and execution</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/query/...</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># End-to-end workflow test</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#79B8FF\"> -run=TestEndToEndWorkflow</span><span style=\"color:#9ECBFF\"> ./test/integration/</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: PromQL queries execute correctly, results mathematically accurate</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tests hang indefinitely</td>\n<td>Deadlock in concurrent code</td>\n<td>Use <code>go test -timeout=30s</code>, check goroutine dumps</td>\n<td>Review lock ordering, use channels instead of mutexes where possible</td>\n</tr>\n<tr>\n<td>Race detector failures</td>\n<td>Unsynchronized access to shared data</td>\n<td>Run with <code>-race</code>, examine race reports</td>\n<td>Add proper synchronization with mutexes or atomic operations</td>\n</tr>\n<tr>\n<td>Memory usage grows unbounded</td>\n<td>Memory leaks or missing cleanup</td>\n<td>Profile with <code>go test -memprofile=mem.prof</code></td>\n<td>Implement proper cleanup, check for goroutine leaks</td>\n</tr>\n<tr>\n<td>Inconsistent test failures</td>\n<td>Timing-dependent test logic</td>\n<td>Add logging, use deterministic test data</td>\n<td>Use mock clocks, add proper synchronization</td>\n</tr>\n<tr>\n<td>Compression tests fail</td>\n<td>Incorrect Gorilla algorithm implementation</td>\n<td>Compare bit-by-bit with reference implementation</td>\n<td>Debug delta-of-delta calculation, check XOR logic</td>\n</tr>\n<tr>\n<td>Query results incorrect</td>\n<td>Mathematical precision issues</td>\n<td>Compare with independently computed expected values</td>\n<td>Use appropriate floating-point tolerance, check aggregation logic</td>\n</tr>\n</tbody></table>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides systematic debugging approaches for all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4). It helps developers diagnose and fix issues across the entire metrics collection pipeline.</p>\n</blockquote>\n<p>Building a metrics collection system introduces numerous failure modes that can be difficult to diagnose. Unlike traditional CRUD applications where failures are often obvious and immediate, metrics systems fail in subtle ways that may not surface until production load or specific edge cases occur. The distributed nature of scraping, the time-series characteristics of the data, and the complex interactions between compression, indexing, and querying create a debugging landscape that requires systematic approaches and specialized tools.</p>\n<p>This section provides a comprehensive guide for diagnosing and fixing issues that commonly arise during development and operation of the metrics collection system. The debugging strategies are organized around observable symptoms rather than internal implementation details, because developers typically encounter problems through external manifestations like slow queries, missing data, or crashed processes.</p>\n<h3 id=\"symptom-based-troubleshooting\">Symptom-Based Troubleshooting</h3>\n<p><strong>Mental Model: The Detective&#39;s Methodology</strong></p>\n<p>Think of debugging a metrics system like investigating a crime scene. You start with observable symptoms (the crime), gather evidence through logs and metrics (witness testimony and physical evidence), form hypotheses about root causes (suspect theories), test those hypotheses with additional investigation (interrogation and forensics), and finally implement fixes (arrest and prosecution). The key is following the evidence systematically rather than jumping to conclusions based on assumptions.</p>\n<p>The symptom-based approach works because it mirrors how problems actually manifest in production. A user reports &quot;queries are slow&quot; or &quot;metrics are missing&quot; - they don&#39;t report &quot;the Gorilla compressor has a bug in delta calculation.&quot; By starting with symptoms and working backward to root causes, we build a systematic investigation process that works regardless of the specific implementation details.</p>\n<h4 id=\"high-level-system-symptoms\">High-Level System Symptoms</h4>\n<p>These symptoms affect the overall system behavior and typically indicate problems in component coordination or resource exhaustion.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Causes</th>\n<th>Investigation Steps</th>\n<th>Resolution Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>System consumes excessive memory</td>\n<td>Label explosion, inefficient compression, query result sets too large</td>\n<td>Check cardinality metrics, analyze heap dumps, review query patterns</td>\n<td>Implement cardinality limits, fix compression bugs, add query result size limits</td>\n</tr>\n<tr>\n<td>Queries return no data despite active scraping</td>\n<td>Index corruption, time zone mismatches, retention policy too aggressive</td>\n<td>Verify scrape success rates, check timestamp alignment, inspect index consistency</td>\n<td>Rebuild indexes, fix timestamp handling, adjust retention settings</td>\n</tr>\n<tr>\n<td>System becomes unresponsive under load</td>\n<td>Resource exhaustion, deadlocks, inefficient algorithms</td>\n<td>Monitor resource usage, check for lock contention, profile CPU usage</td>\n<td>Add resource limits, fix deadlocks, optimize hot paths</td>\n</tr>\n<tr>\n<td>Data appears and disappears intermittently</td>\n<td>Race conditions in storage, inconsistent reads, partial failures</td>\n<td>Enable race detector, add consistency checks, review concurrent access patterns</td>\n<td>Fix race conditions, add proper locking, implement read consistency</td>\n</tr>\n<tr>\n<td>System fails to start after restart</td>\n<td>Corrupted WAL, configuration errors, dependency unavailability</td>\n<td>Check WAL recovery logs, validate configuration, verify dependencies</td>\n<td>Repair or rebuild WAL, fix configuration, ensure dependencies are available</td>\n</tr>\n</tbody></table>\n<h4 id=\"scraping-engine-symptoms\">Scraping Engine Symptoms</h4>\n<p>Scraping problems often manifest as missing or stale data, and can be particularly tricky because network issues create intermittent failures.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Causes</th>\n<th>Investigation Steps</th>\n<th>Resolution Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Targets show as &quot;down&quot; but are actually healthy</td>\n<td>Network connectivity issues, DNS resolution problems, authentication failures</td>\n<td>Test manual HTTP requests, check DNS resolution, verify credentials</td>\n<td>Fix network configuration, update DNS settings, correct authentication</td>\n</tr>\n<tr>\n<td>Scraping succeeds but no metrics are stored</td>\n<td>Parsing errors, timestamp issues, storage failures</td>\n<td>Check parsing error rates, verify timestamp formats, monitor storage health</td>\n<td>Fix parsing logic, standardize timestamp handling, resolve storage issues</td>\n</tr>\n<tr>\n<td>Some metrics missing from multi-metric endpoints</td>\n<td>Partial parsing failures, label validation errors, metric name conflicts</td>\n<td>Enable detailed parsing logs, check label validation rules, inspect metric naming</td>\n<td>Handle partial failures gracefully, fix validation rules, resolve naming conflicts</td>\n</tr>\n<tr>\n<td>Scraping performance degrades over time</td>\n<td>Connection pool exhaustion, memory leaks, increasing target latency</td>\n<td>Monitor connection pool usage, check memory growth, measure target response times</td>\n<td>Implement connection pooling, fix memory leaks, adjust timeout settings</td>\n</tr>\n<tr>\n<td>Targets oscillate between healthy and unhealthy</td>\n<td>Network instability, target overload, circuit breaker misconfiguration</td>\n<td>Analyze target health history, check target resource usage, review circuit breaker settings</td>\n<td>Adjust health check thresholds, reduce target load, tune circuit breaker parameters</td>\n</tr>\n</tbody></table>\n<h4 id=\"storage-engine-symptoms\">Storage Engine Symptoms</h4>\n<p>Storage problems often create subtle data corruption or performance issues that compound over time.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Causes</th>\n<th>Investigation Steps</th>\n<th>Resolution Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Write performance degrades significantly</td>\n<td>WAL bottlenecks, compression inefficiency, disk I/O limits</td>\n<td>Monitor WAL write rates, profile compression performance, check disk I/O patterns</td>\n<td>Optimize WAL batching, fix compression algorithms, add faster storage</td>\n</tr>\n<tr>\n<td>Queries return incorrect aggregated values</td>\n<td>Compression errors, timestamp alignment issues, counter reset handling</td>\n<td>Verify raw sample values, check timestamp ordering, test counter reset detection</td>\n<td>Fix compression bugs, align timestamps properly, improve counter reset logic</td>\n</tr>\n<tr>\n<td>Storage space usage grows faster than expected</td>\n<td>Poor compression ratios, retention policy not working, high cardinality explosion</td>\n<td>Analyze compression effectiveness, verify retention execution, audit label cardinality</td>\n<td>Optimize compression parameters, fix retention policies, implement cardinality controls</td>\n</tr>\n<tr>\n<td>Data corruption detected in stored chunks</td>\n<td>Concurrent write bugs, filesystem issues, incomplete writes</td>\n<td>Enable data integrity checks, review concurrent access patterns, check filesystem health</td>\n<td>Fix concurrent write bugs, repair filesystem issues, add write atomicity</td>\n</tr>\n<tr>\n<td>Recovery from WAL fails after crashes</td>\n<td>WAL corruption, incomplete transaction records, ordering violations</td>\n<td>Examine WAL contents, verify transaction boundaries, check record ordering</td>\n<td>Implement WAL validation, add transaction atomicity, improve ordering guarantees</td>\n</tr>\n</tbody></table>\n<h4 id=\"query-engine-symptoms\">Query Engine Symptoms</h4>\n<p>Query problems can be particularly frustrating because they often work correctly on small datasets but fail at scale.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Causes</th>\n<th>Investigation Steps</th>\n<th>Resolution Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queries timeout frequently</td>\n<td>Inefficient query plans, high cardinality selectors, resource limits too low</td>\n<td>Analyze query execution plans, check selector cardinality, monitor resource usage</td>\n<td>Optimize query execution, add cardinality limits, increase resource limits</td>\n</tr>\n<tr>\n<td>PromQL parsing errors for valid expressions</td>\n<td>Parser bugs, unsupported features, operator precedence issues</td>\n<td>Test expressions in isolation, check parser grammar, verify operator precedence</td>\n<td>Fix parser bugs, implement missing features, correct precedence rules</td>\n</tr>\n<tr>\n<td>Aggregation functions return wrong results</td>\n<td>Grouping logic errors, timestamp alignment issues, sample interpolation bugs</td>\n<td>Test with known datasets, verify grouping behavior, check timestamp handling</td>\n<td>Fix grouping algorithms, align timestamps consistently, correct interpolation logic</td>\n</tr>\n<tr>\n<td>Range queries miss data points</td>\n<td>Time window calculations wrong, staleness threshold too strict, index selection issues</td>\n<td>Verify time window boundaries, check staleness detection, review index selection logic</td>\n<td>Fix time window calculations, adjust staleness thresholds, optimize index selection</td>\n</tr>\n<tr>\n<td>Memory usage spikes during large queries</td>\n<td>Result set too large, inefficient data structures, memory leaks in query engine</td>\n<td>Monitor query result sizes, profile memory allocations, check for memory leaks</td>\n<td>Add result size limits, optimize data structures, fix memory leaks</td>\n</tr>\n</tbody></table>\n<h4 id=\"performance-and-scalability-symptoms\">Performance and Scalability Symptoms</h4>\n<p>These symptoms typically appear under load and indicate architectural limitations or resource bottlenecks.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Root Causes</th>\n<th>Investigation Steps</th>\n<th>Resolution Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scraping falls behind target intervals</td>\n<td>Too many targets per scraper, slow target responses, processing bottlenecks</td>\n<td>Monitor scraping queue depth, measure target response times, check processing latency</td>\n<td>Add more scrapers, optimize target endpoints, parallelize processing</td>\n</tr>\n<tr>\n<td>Query latency increases with data retention period</td>\n<td>Index inefficiency, poor data locality, excessive disk I/O</td>\n<td>Analyze query execution times by time range, check index hit rates, monitor disk I/O</td>\n<td>Optimize index structures, improve data locality, add data tiering</td>\n</tr>\n<tr>\n<td>System cannot handle target cardinality growth</td>\n<td>Memory exhaustion from indexes, storage I/O limits, query complexity explosion</td>\n<td>Monitor memory usage by component, check storage I/O patterns, analyze query complexity</td>\n<td>Implement cardinality limits, add horizontal scaling, optimize query complexity</td>\n</tr>\n<tr>\n<td>Background operations impact foreground performance</td>\n<td>WAL flushing blocks writes, compaction affects reads, GC pauses affect queries</td>\n<td>Monitor background operation timing, check resource contention, measure GC impact</td>\n<td>Optimize background scheduling, reduce resource contention, tune GC parameters</td>\n</tr>\n<tr>\n<td>Resource usage is uneven across system components</td>\n<td>Load imbalance, inefficient resource allocation, component bottlenecks</td>\n<td>Analyze resource usage by component, check load distribution, identify bottlenecks</td>\n<td>Rebalance load, optimize resource allocation, scale bottlenecked components</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Debugging Insight</strong></p>\n<p>The most effective debugging approach for metrics systems is to maintain end-to-end observability of the system itself. Every component should emit metrics about its own behavior, creating a &quot;metrics system monitoring itself&quot; capability. This recursive observability is essential because metrics system failures often have cascading effects that obscure the original root cause.</p>\n</blockquote>\n<h3 id=\"debugging-tools-and-techniques\">Debugging Tools and Techniques</h3>\n<p><strong>Mental Model: The Medical Diagnostic Toolkit</strong></p>\n<p>Think of debugging tools like medical diagnostic equipment. Just as doctors use different instruments for different symptoms (stethoscope for heart issues, X-ray for bone problems, blood tests for infections), metrics system debugging requires specialized tools for different types of problems. Some tools provide continuous monitoring (like vital sign monitors), others require active investigation (like MRI scans), and some are only used in emergency situations (like defibrillators).</p>\n<p>The key principle is using the right tool for the type of problem you&#39;re investigating, starting with non-invasive continuous monitoring and escalating to more intensive diagnostic techniques only when necessary.</p>\n<h4 id=\"logging-infrastructure\">Logging Infrastructure</h4>\n<p>Effective logging is the foundation of debugging any distributed system, but metrics systems have specific logging requirements due to their high-throughput, time-sensitive nature.</p>\n<p><strong>Structured Logging Configuration</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Log Level</th>\n<th>Key Fields</th>\n<th>Sampling Strategy</th>\n<th>Retention Period</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ScrapeEngine</code></td>\n<td>INFO for successes, WARN for failures</td>\n<td>target_url, scrape_duration, sample_count, error_message</td>\n<td>100% for failures, 1% for successes</td>\n<td>7 days</td>\n</tr>\n<tr>\n<td><code>StorageEngine</code></td>\n<td>DEBUG for writes, ERROR for corruption</td>\n<td>series_count, sample_count, compression_ratio, wal_sync_duration</td>\n<td>0.1% for writes, 100% for errors</td>\n<td>3 days</td>\n</tr>\n<tr>\n<td><code>QueryEngine</code></td>\n<td>INFO for slow queries, DEBUG for execution</td>\n<td>query_text, execution_time, series_selected, result_size</td>\n<td>100% for &gt;1s queries, 1% for fast queries</td>\n<td>1 day</td>\n</tr>\n<tr>\n<td><code>SystemCoordinator</code></td>\n<td>INFO for lifecycle, ERROR for failures</td>\n<td>component_name, operation, duration, resource_usage</td>\n<td>100% for all events</td>\n<td>30 days</td>\n</tr>\n</tbody></table>\n<p><strong>Contextual Logging Best Practices</strong></p>\n<p>Every log entry should include sufficient context to understand the operation being performed. For metrics systems, this context typically includes temporal information (timestamps, time ranges), dimensional information (metric names, label sets), and operational information (component state, resource usage).</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>// Example of well-structured log entries:\n{\n  &quot;timestamp&quot;: &quot;2024-01-15T10:30:45.123Z&quot;,\n  &quot;level&quot;: &quot;ERROR&quot;,\n  &quot;component&quot;: &quot;ScrapeEngine&quot;,\n  &quot;operation&quot;: &quot;scrapeTarget&quot;,\n  &quot;target_url&quot;: &quot;http://api-server:8080/metrics&quot;,\n  &quot;scrape_interval&quot;: &quot;15s&quot;,\n  &quot;attempt_number&quot;: 3,\n  &quot;error&quot;: &quot;context deadline exceeded&quot;,\n  &quot;consecutive_failures&quot;: 5,\n  &quot;target_labels&quot;: {&quot;job&quot;: &quot;api-server&quot;, &quot;instance&quot;: &quot;api-server:8080&quot;},\n  &quot;trace_id&quot;: &quot;abc123def456&quot;\n}\n\n{\n  &quot;timestamp&quot;: &quot;2024-01-15T10:30:45.456Z&quot;,\n  &quot;level&quot;: &quot;WARN&quot;,\n  &quot;component&quot;: &quot;StorageEngine&quot;,\n  &quot;operation&quot;: &quot;compressChunk&quot;,\n  &quot;series_id&quot;: 12345,\n  &quot;chunk_samples&quot;: 240,\n  &quot;compression_ratio&quot;: 0.85,\n  &quot;warning&quot;: &quot;compression ratio below threshold&quot;,\n  &quot;metric_name&quot;: &quot;http_requests_total&quot;,\n  &quot;label_hash&quot;: &quot;def456abc123&quot;\n}</code></pre></div>\n\n<h4 id=\"system-metrics-and-observability\">System Metrics and Observability</h4>\n<p>The metrics collection system must monitor itself comprehensively. This self-monitoring provides real-time visibility into system health and performance characteristics.</p>\n<p><strong>Core System Metrics</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Metric Name</th>\n<th>Labels</th>\n<th>Description</th>\n<th>Alert Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scraping Health</td>\n<td><code>scrape_targets_up</code></td>\n<td>job, instance</td>\n<td>Number of healthy targets per job</td>\n<td>&lt; 90% of targets</td>\n</tr>\n<tr>\n<td>Scraping Performance</td>\n<td><code>scrape_duration_seconds</code></td>\n<td>job, instance, quantile</td>\n<td>Scrape operation duration distribution</td>\n<td>p99 &gt; scrape_timeout</td>\n</tr>\n<tr>\n<td>Storage Throughput</td>\n<td><code>samples_ingested_total</code></td>\n<td>component</td>\n<td>Total samples successfully stored</td>\n<td>Rate decreasing</td>\n</tr>\n<tr>\n<td>Storage Errors</td>\n<td><code>storage_errors_total</code></td>\n<td>component, error_type</td>\n<td>Storage operation failures by type</td>\n<td>&gt; 0.1% error rate</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td><code>query_duration_seconds</code></td>\n<td>query_type, quantile</td>\n<td>Query execution time distribution</td>\n<td>p95 &gt; 10 seconds</td>\n</tr>\n<tr>\n<td>Query Load</td>\n<td><code>concurrent_queries</code></td>\n<td>none</td>\n<td>Number of actively executing queries</td>\n<td>&gt; max_concurrent_queries</td>\n</tr>\n<tr>\n<td>Resource Usage</td>\n<td><code>memory_usage_bytes</code></td>\n<td>component</td>\n<td>Memory usage by system component</td>\n<td>&gt; 80% of limit</td>\n</tr>\n<tr>\n<td>Resource Usage</td>\n<td><code>goroutine_count</code></td>\n<td>component</td>\n<td>Active goroutines by component</td>\n<td>Continuously increasing</td>\n</tr>\n</tbody></table>\n<p><strong>Component Health Indicators</strong></p>\n<p>Each system component should provide standardized health indicators that can be monitored independently and in aggregate.</p>\n<table>\n<thead>\n<tr>\n<th>Health Indicator</th>\n<th>Measurement Method</th>\n<th>Healthy Range</th>\n<th>Warning Range</th>\n<th>Critical Range</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scrape Success Rate</td>\n<td>Successful scrapes / Total scrape attempts</td>\n<td>&gt; 95%</td>\n<td>90-95%</td>\n<td>&lt; 90%</td>\n</tr>\n<tr>\n<td>Storage Write Success Rate</td>\n<td>Successful writes / Total write attempts</td>\n<td>&gt; 99%</td>\n<td>95-99%</td>\n<td>&lt; 95%</td>\n</tr>\n<tr>\n<td>Query Success Rate</td>\n<td>Successful queries / Total queries</td>\n<td>&gt; 99%</td>\n<td>95-99%</td>\n<td>&lt; 95%</td>\n</tr>\n<tr>\n<td>WAL Sync Latency</td>\n<td>Time to fsync WAL entries</td>\n<td>&lt; 10ms</td>\n<td>10-50ms</td>\n<td>&gt; 50ms</td>\n</tr>\n<tr>\n<td>Index Lookup Latency</td>\n<td>Time to resolve label selectors</td>\n<td>&lt; 1ms</td>\n<td>1-10ms</td>\n<td>&gt; 10ms</td>\n</tr>\n<tr>\n<td>Compression Efficiency</td>\n<td>Compressed size / Raw size</td>\n<td>&lt; 0.2</td>\n<td>0.2-0.5</td>\n<td>&gt; 0.5</td>\n</tr>\n</tbody></table>\n<h4 id=\"interactive-debugging-tools\">Interactive Debugging Tools</h4>\n<p>For active investigation of problems, several interactive tools provide detailed system inspection capabilities.</p>\n<p><strong>Component State Inspection</strong></p>\n<table>\n<thead>\n<tr>\n<th>Tool</th>\n<th>Command Format</th>\n<th>Information Provided</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Target Health Inspector</td>\n<td><code>GET /debug/targets</code></td>\n<td>Current health status, last scrape results, failure reasons</td>\n<td>Diagnosing scraping issues</td>\n</tr>\n<tr>\n<td>Series Cardinality Inspector</td>\n<td><code>GET /debug/cardinality?metric=&lt;name&gt;</code></td>\n<td>Label combination counts, high-cardinality series</td>\n<td>Investigating memory usage</td>\n</tr>\n<tr>\n<td>Storage Consistency Checker</td>\n<td><code>GET /debug/storage/check</code></td>\n<td>Index-data consistency, corruption detection</td>\n<td>Validating storage integrity</td>\n</tr>\n<tr>\n<td>Query Execution Planner</td>\n<td><code>GET /debug/query/explain?query=&lt;promql&gt;</code></td>\n<td>Execution plan, estimated resource usage</td>\n<td>Optimizing slow queries</td>\n</tr>\n<tr>\n<td>WAL Content Inspector</td>\n<td><code>GET /debug/wal/entries?limit=&lt;n&gt;</code></td>\n<td>Recent WAL entries, transaction boundaries</td>\n<td>Debugging storage issues</td>\n</tr>\n</tbody></table>\n<p><strong>Profiling and Performance Analysis</strong></p>\n<table>\n<thead>\n<tr>\n<th>Profiling Tool</th>\n<th>Activation Method</th>\n<th>Data Collected</th>\n<th>Analysis Focus</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU Profiler</td>\n<td><code>GET /debug/pprof/profile</code></td>\n<td>CPU usage by function</td>\n<td>Hot paths, algorithmic efficiency</td>\n</tr>\n<tr>\n<td>Memory Profiler</td>\n<td><code>GET /debug/pprof/heap</code></td>\n<td>Memory allocations by location</td>\n<td>Memory leaks, allocation patterns</td>\n</tr>\n<tr>\n<td>Goroutine Profiler</td>\n<td><code>GET /debug/pprof/goroutine</code></td>\n<td>Goroutine stacks and states</td>\n<td>Deadlocks, resource contention</td>\n</tr>\n<tr>\n<td>Mutex Profiler</td>\n<td><code>GET /debug/pprof/mutex</code></td>\n<td>Lock contention by location</td>\n<td>Synchronization bottlenecks</td>\n</tr>\n<tr>\n<td>Block Profiler</td>\n<td><code>GET /debug/pprof/block</code></td>\n<td>Blocking operations by location</td>\n<td>I/O bottlenecks, channel contention</td>\n</tr>\n</tbody></table>\n<h4 id=\"distributed-tracing-integration\">Distributed Tracing Integration</h4>\n<p>For complex issues that span multiple components, distributed tracing provides end-to-end visibility into request processing.</p>\n<p><strong>Trace Instrumentation Points</strong></p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Trace Span Name</th>\n<th>Key Attributes</th>\n<th>Child Spans</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Scrape</td>\n<td><code>scrape_target</code></td>\n<td>target_url, scrape_interval</td>\n<td>http_request, parse_metrics, store_samples</td>\n</tr>\n<tr>\n<td>Sample Storage</td>\n<td><code>store_samples</code></td>\n<td>sample_count, series_count</td>\n<td>wal_write, compress_chunk, update_index</td>\n</tr>\n<tr>\n<td>Query Execution</td>\n<td><code>execute_query</code></td>\n<td>query_type, time_range</td>\n<td>parse_expression, select_series, aggregate_results</td>\n</tr>\n<tr>\n<td>Series Selection</td>\n<td><code>select_series</code></td>\n<td>selector_count, series_matched</td>\n<td>index_lookup, label_matching</td>\n</tr>\n<tr>\n<td>Range Query</td>\n<td><code>range_query</code></td>\n<td>start_time, end_time, step</td>\n<td>instant_query (multiple)</td>\n</tr>\n</tbody></table>\n<p><strong>Trace Analysis Techniques</strong></p>\n<p>When investigating distributed issues, trace analysis reveals timing relationships and failure propagation patterns that are invisible in individual component logs.</p>\n<p>Critical timing relationships to analyze include scrape-to-storage latency (how long samples take to become queryable), query-to-result latency (end-to-end query performance), and error propagation delays (how long failures take to surface). Performance bottleneck identification focuses on the longest spans in the trace, resource contention points where spans block waiting for resources, and cascade failure patterns where one component failure triggers failures in dependent components.</p>\n<blockquote>\n<p><strong>Debugging Insight: The Observer Effect</strong></p>\n<p>Remember that debugging tools themselves consume system resources and can change system behavior. Heavy logging, frequent profiling, and detailed tracing all introduce overhead that may mask or alter the problems you&#39;re trying to investigate. Use sampling and conditional instrumentation to minimize observer effects during production debugging.</p>\n</blockquote>\n<h3 id=\"implementation-pitfalls\">Implementation Pitfalls</h3>\n<p><strong>Mental Model: The Software Engineering Minefield</strong></p>\n<p>Think of common implementation pitfalls like a minefield that every developer must navigate. Experienced engineers know where the mines are buried based on previous painful explosions. By documenting these pitfalls explicitly, we create a map that helps new developers avoid stepping on the same mines that have injured others.</p>\n<p>The key insight is that pitfalls in metrics systems often appear as &quot;working&quot; code that fails under specific conditions like high cardinality, time zone changes, or concurrent access patterns. These delayed failures make pitfalls particularly dangerous because they pass initial testing but create production incidents.</p>\n<h4 id=\"metrics-data-model-pitfalls\">Metrics Data Model Pitfalls</h4>\n<p>These pitfalls relate to the fundamental data structures and semantic behaviors that form the foundation of the metrics system.</p>\n<p>⚠️ <strong>Pitfall: Counter Reset Handling</strong></p>\n<p>Many developers implement counters as simple incrementing values without considering that counter resets (when a process restarts) require special handling for rate calculations to remain accurate.</p>\n<p><strong>Why This Fails:</strong> When a counter resets to zero, naive rate calculations produce large negative values because <code>current_value - previous_value</code> becomes negative. This creates obviously wrong metrics and breaks downstream alerting and dashboards.</p>\n<p><strong>Detection:</strong> Monitor for negative rate values in counter-based metrics. Implement validation that rejects negative rates from counter calculations.</p>\n<p><strong>Correct Implementation:</strong> Track counter resets by detecting when the current value is less than the previous value, and handle resets by either skipping the rate calculation for that interval or estimating the true rate by assuming the counter accumulated <code>current_value</code> since the reset.</p>\n<p>⚠️ <strong>Pitfall: High Cardinality Label Design</strong></p>\n<p>Developers often add labels with high cardinality (user IDs, request IDs, timestamps) without understanding the exponential memory impact of label combinations.</p>\n<p><strong>Why This Fails:</strong> Each unique combination of metric name and label values creates a separate time series. Labels with thousands of values create millions of time series that exhaust system memory. For example, a metric with labels <code>{user_id, endpoint, status}</code> where user_id has 10,000 values, endpoint has 50 values, and status has 5 values creates up to 2.5 million time series.</p>\n<p><strong>Detection:</strong> Monitor total series count and memory usage. Implement cardinality reporting that shows series count per metric name and per label combination.</p>\n<p><strong>Correct Implementation:</strong> Use high-cardinality information in log entries rather than metric labels. Design label schemas with bounded cardinality - prefer categorical labels like <code>{status=&quot;success|failure&quot;, tier=&quot;premium|standard&quot;}</code> over unbounded labels like <code>{user_id=&quot;12345&quot;, request_id=&quot;abc-def&quot;}</code>.</p>\n<p>⚠️ <strong>Pitfall: Histogram Bucket Boundaries</strong></p>\n<p>Developers often choose histogram bucket boundaries arbitrarily or try to change them after the histogram has been created and is collecting data.</p>\n<p><strong>Why This Fails:</strong> Histogram bucket boundaries must remain consistent throughout the lifetime of the metric. Changing boundaries makes historical data incomparable and breaks percentile calculations. Poor bucket boundary choices (too few buckets, wrong ranges) provide insufficient resolution for meaningful percentile analysis.</p>\n<p><strong>Detection:</strong> Monitor histogram bucket distribution - if most values fall into only one or two buckets, the boundaries are poorly chosen. Check for bucket boundary inconsistencies in historical data.</p>\n<p><strong>Correct Implementation:</strong> Choose histogram buckets based on the expected distribution of values you&#39;re measuring. Use exponential bucket spacing (1ms, 2ms, 5ms, 10ms, 25ms, 50ms, 100ms...) for latency measurements. Once chosen, never modify bucket boundaries - create a new histogram metric instead.</p>\n<h4 id=\"scraping-engine-pitfalls\">Scraping Engine Pitfalls</h4>\n<p>These pitfalls involve HTTP operations, timing, and concurrency patterns specific to pull-based metrics collection.</p>\n<p>⚠️ <strong>Pitfall: Scrape Timeout Implementation</strong></p>\n<p>Many developers implement scrape timeouts using <code>time.After()</code> or similar mechanisms without properly canceling the underlying HTTP request, leading to resource leaks.</p>\n<p><strong>Why This Fails:</strong> Even when the scrape operation times out from the coordinator&#39;s perspective, the underlying HTTP request continues consuming connection pool resources, goroutines, and network bandwidth. Over time, these leaked resources accumulate and eventually exhaust system capacity.</p>\n<p><strong>Detection:</strong> Monitor goroutine count and HTTP connection pool usage. Look for continuously increasing resource usage even when scrape timeouts are occurring.</p>\n<p><strong>Correct Implementation:</strong> Use context-based cancellation with <code>context.WithTimeout()</code> and pass the context to <code>http.Request.WithContext()</code>. This ensures that timing out the scrape operation also cancels the underlying HTTP request and releases all associated resources.</p>\n<p>⚠️ <strong>Pitfall: Concurrent Scraping Without Rate Limiting</strong></p>\n<p>Developers often implement concurrent scraping by launching goroutines for each target without considering the aggregate load this places on target services.</p>\n<p><strong>Why This Fails:</strong> Launching hundreds of concurrent HTTP requests can overwhelm target services, creating cascading failures. Target services may implement their own rate limiting or simply crash under the load, making metrics collection counterproductively impact the systems being monitored.</p>\n<p><strong>Detection:</strong> Monitor target response times and error rates. Look for correlation between scraper concurrency levels and target service health.</p>\n<p><strong>Correct Implementation:</strong> Implement scraper worker pools with bounded concurrency. Use semaphores or buffered channels to limit the maximum number of concurrent scrape operations. Consider implementing adaptive rate limiting that backs off when target services show signs of stress.</p>\n<p>⚠️ <strong>Pitfall: Service Discovery Race Conditions</strong></p>\n<p>Developers often implement service discovery updates by directly modifying the target list without considering concurrent access from scraping goroutines.</p>\n<p><strong>Why This Fails:</strong> Concurrent modification of the target list during scraping operations creates race conditions that can cause panics, data corruption, or targets being scraped multiple times simultaneously.</p>\n<p><strong>Detection:</strong> Run with the Go race detector enabled (<code>go run -race</code>). Monitor for panics or unexpected behavior during service discovery updates.</p>\n<p><strong>Correct Implementation:</strong> Use copy-on-write semantics for target list updates. Create a new target list and atomically replace the reference, allowing in-flight scrape operations to complete with the old target list before switching to the new one.</p>\n<h4 id=\"storage-engine-pitfalls\">Storage Engine Pitfalls</h4>\n<p>Storage engine pitfalls often involve data corruption, concurrency issues, and compression edge cases that only surface under specific conditions.</p>\n<p>⚠️ <strong>Pitfall: WAL Corruption During Concurrent Writes</strong></p>\n<p>Developers often implement write-ahead logging with multiple goroutines writing directly to the WAL file without proper synchronization.</p>\n<p><strong>Why This Fails:</strong> Multiple concurrent writers can interleave bytes from different log entries, creating corrupted WAL entries that cannot be parsed during recovery. This makes the entire WAL unusable and causes data loss during crash recovery.</p>\n<p><strong>Detection:</strong> Implement WAL validation during recovery that checks entry boundaries and checksums. Monitor for WAL recovery failures after system restarts.</p>\n<p><strong>Correct Implementation:</strong> Serialize all WAL writes through a single goroutine using a channel-based queue, or implement file-level locking to ensure exclusive access during write operations. Always fsync after writing complete log entries to ensure durability.</p>\n<p>⚠️ <strong>Pitfall: Gorilla Compression Edge Cases</strong></p>\n<p>Developers often implement Gorilla compression by following the basic algorithm without handling edge cases like irregular timestamp intervals, floating point special values, or compression ratio degradation.</p>\n<p><strong>Why This Fails:</strong> The Gorilla compression algorithm assumes relatively regular timestamp intervals and normal floating point values. Irregular timestamps reduce compression effectiveness, while special values like NaN or infinity can break the XOR-based value compression. When compression ratios degrade, the system uses much more memory than expected.</p>\n<p><strong>Detection:</strong> Monitor compression ratios per chunk and overall system memory usage. Implement validation for special floating point values in incoming samples.</p>\n<p><strong>Correct Implementation:</strong> Handle irregular timestamps by falling back to absolute timestamp encoding when delta-of-delta compression becomes ineffective. Validate floating point values and reject or special-case NaN and infinity. Monitor compression ratios and implement fallback storage for chunks that don&#39;t compress well.</p>\n<p>⚠️ <strong>Pitfall: Index Consistency Under Concurrent Access</strong></p>\n<p>Developers often implement inverted indexes with concurrent read and write access without proper synchronization, leading to index corruption or inconsistent query results.</p>\n<p><strong>Why This Fails:</strong> Concurrent modification of index data structures can leave them in inconsistent states where some series are visible through some label combinations but not others, or where index entries point to non-existent series data.</p>\n<p><strong>Detection:</strong> Implement index consistency checks that verify all index entries point to valid series data and that series data can be found through expected index paths. Run these checks periodically and after any index corruption reports.</p>\n<p><strong>Correct Implementation:</strong> Use readers-writer mutexes to allow concurrent reads while serializing writes. Implement index updates as atomic operations that either complete entirely or leave the index unchanged. Consider using copy-on-write strategies for large index updates.</p>\n<h4 id=\"query-engine-pitfalls\">Query Engine Pitfalls</h4>\n<p>Query engine pitfalls often involve mathematical edge cases, performance characteristics that don&#39;t scale, and semantic misunderstandings of PromQL operations.</p>\n<p>⚠️ <strong>Pitfall: Rate Calculation Across Counter Resets</strong></p>\n<p>Developers often implement rate calculations using simple arithmetic without detecting and handling counter resets properly.</p>\n<p><strong>Why This Fails:</strong> Counter resets create negative rate values that are mathematically incorrect and break visualizations and alerting. The <code>rate()</code> function should calculate the true rate of increase, which requires detecting resets and handling them appropriately.</p>\n<p><strong>Detection:</strong> Monitor for negative values in rate calculation results. Implement validation that rejects impossible rate values based on the known characteristics of the underlying counter.</p>\n<p><strong>Correct Implementation:</strong> Detect counter resets by checking if the current counter value is less than the previous value. When a reset is detected, calculate the rate as <code>current_value / time_interval</code>, assuming the counter accumulated its current value since the reset. For partial intervals after resets, either skip the rate calculation or pro-rate based on the partial interval.</p>\n<p>⚠️ <strong>Pitfall: Query Resource Exhaustion</strong></p>\n<p>Developers often implement query execution without considering resource limits, allowing queries over high-cardinality metrics to exhaust system memory.</p>\n<p><strong>Why This Fails:</strong> Queries that select thousands of time series can consume gigabytes of memory for intermediate results and final output. Without resource limits, a single poorly-written query can crash the entire metrics system.</p>\n<p><strong>Detection:</strong> Monitor memory usage during query execution and track query result sizes. Implement timeouts and memory limits for query operations.</p>\n<p><strong>Correct Implementation:</strong> Implement query resource limits including maximum series per query, maximum query execution time, and maximum result set size. Use streaming processing where possible to avoid loading entire result sets into memory. Provide query cost estimation to help users understand the resource impact of their queries.</p>\n<p>⚠️ <strong>Pitfall: Aggregation Function Semantic Errors</strong></p>\n<p>Developers often implement aggregation functions like <code>avg()</code> or <code>sum()</code> without properly handling missing data points, different timestamp alignments, or grouping semantics.</p>\n<p><strong>Why This Fails:</strong> Different time series may have samples at different timestamps, and aggregation functions must handle these alignment issues correctly. Simply averaging all available values without considering temporal alignment produces mathematically meaningless results.</p>\n<p><strong>Detection:</strong> Test aggregation functions with time series that have different sampling intervals and missing data points. Verify that aggregation results match hand-calculated expected values.</p>\n<p><strong>Correct Implementation:</strong> Implement timestamp alignment by interpolating or extrapolating values to common evaluation timestamps before applying aggregation functions. Handle missing data appropriately - some functions should skip missing values while others should treat them as zero. Clearly document the temporal semantics of each aggregation function.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical tools and code structures for implementing effective debugging capabilities throughout the metrics collection system.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Structured Logging</td>\n<td><code>log/slog</code> with JSON formatting</td>\n<td><code>go.uber.org/zap</code> with sampling and async writing</td>\n</tr>\n<tr>\n<td>HTTP Debugging Endpoints</td>\n<td><code>net/http/pprof</code> + custom handlers</td>\n<td><code>go.uber.org/fx</code> + <code>github.com/gorilla/mux</code> for organized routing</td>\n</tr>\n<tr>\n<td>Metrics Self-Monitoring</td>\n<td>Manual metrics with <code>expvar</code></td>\n<td><code>github.com/prometheus/client_golang</code> for full compatibility</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Manual trace context propagation</td>\n<td><code>go.opentelemetry.io/otel</code> with Jaeger or Zipkin</td>\n</tr>\n<tr>\n<td>Health Checking</td>\n<td>Simple HTTP endpoint with JSON status</td>\n<td>Full health check framework with dependencies</td>\n</tr>\n<tr>\n<td>Configuration Validation</td>\n<td>Manual validation with error accumulation</td>\n<td><code>github.com/go-playground/validator/v10</code> for declarative rules</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n  debug/\n    health.go              ← component health checking\n    profiling.go           ← performance profiling endpoints  \n    inspection.go          ← system state inspection tools\n    tracing.go             ← distributed tracing utilities\n    debug_test.go          ← debugging tool tests\n  monitoring/\n    metrics.go             ← self-monitoring metrics definitions\n    alerts.go              ← alerting rule definitions\n    dashboards.go          ← dashboard configuration export\n  validation/\n    config_validator.go    ← configuration validation\n    data_validator.go      ← data consistency checking\n    performance_validator.go ← performance requirement validation\ncmd/\n  debug-tool/\n    main.go               ← standalone debugging CLI tool\n  health-check/\n    main.go               ← health checking utility</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Structured Logger Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debug</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Logger provides structured logging with different levels and context support</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Logger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLogger creates a logger for a specific component with JSON formatting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    handler </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> slog.</span><span style=\"color:#B392F0\">NewJSONHandler</span><span style=\"color:#E1E4E8\">(os.Stdout, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerOptions</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Level: slog.LevelDebug,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        AddSource: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> slog.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(handler).</span><span style=\"color:#B392F0\">With</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"component\"</span><span style=\"color:#E1E4E8\">, component,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"pid\"</span><span style=\"color:#E1E4E8\">, os.</span><span style=\"color:#B392F0\">Getpid</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger: logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component: component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Info logs informational messages with structured fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> ...</span><span style=\"color:#B392F0\">any</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(msg, fields</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error logs error messages with structured fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> ...</span><span style=\"color:#B392F0\">any</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    allFields </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(fields, </span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">, err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.logger.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(msg, allFields</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Debug logs debug messages with structured fields (only in debug builds)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Debug</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> ...</span><span style=\"color:#B392F0\">any</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.logger.</span><span style=\"color:#B392F0\">Debug</span><span style=\"color:#E1E4E8\">(msg, fields</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WithContext returns a logger that includes trace information from context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WithContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Extract trace ID from context if tracing is enabled</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> l</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WithFields returns a logger with additional fields included in all log entries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WithFields</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> ...</span><span style=\"color:#B392F0\">any</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger: l.logger.</span><span style=\"color:#B392F0\">With</span><span style=\"color:#E1E4E8\">(fields</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component: l.component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Complete Health Checking System</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debug</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthStatus represents the health state of a component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthUp</span><span style=\"color:#B392F0\">       HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"up\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthDown</span><span style=\"color:#B392F0\">     HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"down\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthDegraded</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ComponentHealth represents the health of a single system component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"component\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status    </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\"> `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"message,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">    `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metrics   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"metrics,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SystemHealth aggregates health information from all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SystemHealth</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    OverallStatus </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\">                   `json:\"overall_status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Components    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentHealth</span><span style=\"color:#9ECBFF\">    `json:\"components\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">                      `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Uptime        </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">                  `json:\"uptime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthChecker manages health checking for all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthChecker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    components </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentHealth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHealthChecker creates a new health checker instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHealthChecker</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        components: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentHealth</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        startTime:  time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterComponent registers a component for health checking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterComponent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hc.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hc.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hc.components[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ComponentHealth</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Component: name,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Status:    HealthDown,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Message:   </span><span style=\"color:#9ECBFF\">\"Component not yet initialized\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateComponentHealth updates the health status of a component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateComponentHealth</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">status</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hc.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hc.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> component, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hc.components[name]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component.Status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component.Message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component.Timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component.Metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetSystemHealth returns the overall system health status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetSystemHealth</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SystemHealth</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hc.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hc.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    overallStatus </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> HealthUp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    componentCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentHealth</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, component </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hc.components {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Create a copy to avoid race conditions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        componentCopy[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ComponentHealth</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Component: component.Component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Status:    component.Status,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Message:   component.Message,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Timestamp: component.Timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Metrics:   component.Metrics,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Determine overall status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> component.Status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthDown {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            overallStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthDown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> component.Status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthDegraded </span><span style=\"color:#F97583\">&#x26;&#x26;</span><span style=\"color:#E1E4E8\"> overallStatus </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthUp {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            overallStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthDegraded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">SystemHealth</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        OverallStatus: overallStatus,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Components:    componentCopy,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp:     time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Uptime:        time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(hc.startTime),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ServeHTTP implements http.Handler to expose health information via HTTP</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthChecker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    health </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hc.</span><span style=\"color:#B392F0\">GetSystemHealth</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Set HTTP status code based on overall health</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> health.OverallStatus {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthUp:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusOK)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthDegraded:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusOK) </span><span style=\"color:#6A737D\">// Still serving requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthDown:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusServiceUnavailable)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w).</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(health)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Component State Inspector</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ComponentStateInspector provides detailed inspection of internal component state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ComponentStateInspector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scrapeEngine  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ScrapeEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storageEngine </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryEngine   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewComponentStateInspector creates an inspector with access to all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewComponentStateInspector</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">scrapeEngine</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ScrapeEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">storageEngine</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryEngine</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentStateInspector</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ComponentStateInspector</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        scrapeEngine:  scrapeEngine,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storageEngine: storageEngine,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryEngine:   queryEngine,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:        logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InspectScrapeTargets returns detailed information about all scrape targets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">csi </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentStateInspector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InspectScrapeTargets</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Lock scrape engine to prevent concurrent modification during inspection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through all configured targets and collect their current state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each target, include: URL, last scrape time, consecutive failures, health status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Calculate aggregate statistics: total targets, healthy targets, failure rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return structured data suitable for JSON serialization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use read locks to allow continued operation while inspecting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InspectStorageState returns information about storage engine internal state  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">csi </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentStateInspector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InspectStorageState</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock on storage engine to ensure consistent state view</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Count total number of time series and chunks across all indexes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Calculate storage efficiency metrics: compression ratio, samples per chunk</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Inspect WAL state: current segment, pending entries, last sync time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check index consistency: verify all series have corresponding chunks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Return comprehensive storage health and efficiency data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InspectQueryPerformance returns query engine performance and resource usage data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">csi </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentStateInspector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InspectQueryPerformance</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Collect query execution statistics from query engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate percentiles for query duration and result set sizes  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Identify slow queries and high-cardinality selectors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Report resource usage: memory per query, concurrent query count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return performance data suitable for optimization analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Distributed Tracing Integration</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// TracingCoordinator manages distributed tracing across all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TracingCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    enabled    </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sampler    </span><span style=\"color:#B392F0\">Sampler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tracer     </span><span style=\"color:#B392F0\">Tracer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTracingCoordinator creates a tracing coordinator with sampling configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTracingCoordinator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">enabled</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">samplingRate</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TracingCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize tracer with appropriate backend (Jaeger, Zipkin, or no-op)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Configure sampler with specified sampling rate and adaptive algorithms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set up trace context propagation for HTTP requests and internal operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return configured coordinator ready for use across system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StartSpan creates a new tracing span for an operation with specified attributes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TracingCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">StartSpan</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operationName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">attributes</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) (</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">Span</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if tracing is enabled and sampling decision allows this trace</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Extract parent span context from incoming context if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create new span with operation name and configured attributes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add standard attributes: component name, operation timestamp, correlation ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return new context containing span and span object for lifecycle management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> ctx, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InjectTraceHeaders adds tracing information to HTTP request headers for propagation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TracingCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InjectTraceHeaders</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">headers</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract active span from context if present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Serialize span context into trace propagation headers (B3, Jaeger, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add headers to outgoing HTTP request for downstream trace correlation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Handle case where no active span exists gracefully</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExtractTraceContext extracts tracing information from incoming HTTP request headers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">tc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TracingCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExtractTraceContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">headers</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse trace propagation headers from incoming HTTP request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Reconstruct span context from header information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create new context containing extracted trace information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return context suitable for starting child spans</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 Debugging Verification: Metrics Data Model</strong></p>\n<p>After implementing the metrics data model with debugging support:</p>\n<ol>\n<li><strong>Run Component Tests</strong>: Execute <code>go test ./internal/metrics/... -v</code> to verify all metric types handle edge cases correctly</li>\n<li><strong>Verify Cardinality Tracking</strong>: Create metrics with high-cardinality labels and confirm the system detects and reports the cardinality explosion</li>\n<li><strong>Test Counter Reset Handling</strong>: Simulate counter resets and verify that subsequent rate calculations handle them appropriately</li>\n<li><strong>Check Label Validation</strong>: Attempt to create metrics with invalid label names and values, confirming that validation prevents high-cardinality labels</li>\n</ol>\n<p><strong>Expected Behavior</strong>: Cardinality tracking reports series counts accurately, counter resets are detected and logged, label validation prevents problematic metric creation, and all metric types expose their internal state for debugging.</p>\n<p><strong>Milestone 2 Debugging Verification: Scrape Engine</strong></p>\n<p>After implementing the scrape engine with debugging support:</p>\n<ol>\n<li><strong>Run Scraping Tests</strong>: Execute <code>go test ./internal/scraping/... -v</code> with mock HTTP targets to verify scraping logic</li>\n<li><strong>Test Timeout Handling</strong>: Configure short scrape timeouts and verify that timeouts cancel HTTP requests properly without leaking resources</li>\n<li><strong>Verify Target Health Tracking</strong>: Start and stop mock target services and confirm that target health status updates correctly</li>\n<li><strong>Check Service Discovery</strong>: Modify service discovery configuration and verify that target list updates without disrupting ongoing scrapes</li>\n</ol>\n<p><strong>Expected Behavior</strong>: Scrape operations respect timeouts and cancel underlying resources, target health accurately reflects service availability, service discovery updates are applied safely, and detailed scrape metrics are available for debugging.</p>\n<p><strong>Milestone 3 Debugging Verification: Time Series Storage</strong></p>\n<p>After implementing the time series storage with debugging support:</p>\n<ol>\n<li><strong>Run Storage Tests</strong>: Execute <code>go test ./internal/storage/... -v</code> to verify compression, indexing, and WAL functionality</li>\n<li><strong>Test WAL Recovery</strong>: Kill the storage process during write operations and verify that WAL recovery restores consistent state</li>\n<li><strong>Verify Compression Effectiveness</strong>: Store time series with different patterns and confirm that compression ratios meet expectations</li>\n<li><strong>Check Index Consistency</strong>: Run index consistency checks and verify that all time series can be found through expected label selectors</li>\n</ol>\n<p><strong>Expected Behavior</strong>: WAL recovery restores all committed data after crashes, compression ratios achieve expected efficiency, index consistency checks pass, and storage state can be inspected comprehensively.</p>\n<p><strong>Milestone 4 Debugging Verification: Query Engine</strong></p>\n<p>After implementing the query engine with debugging support:</p>\n<ol>\n<li><strong>Run Query Tests</strong>: Execute <code>go test ./internal/query/... -v</code> to verify PromQL parsing and execution</li>\n<li><strong>Test Resource Limits</strong>: Execute queries that exceed configured limits and verify that they are rejected appropriately</li>\n<li><strong>Verify Aggregation Correctness</strong>: Run aggregation queries with known expected results and confirm mathematical accuracy</li>\n<li><strong>Check Performance Monitoring</strong>: Execute slow queries and verify that performance metrics accurately capture execution characteristics</li>\n</ol>\n<p><strong>Expected Behavior</strong>: Query resource limits prevent system overload, aggregation functions produce mathematically correct results, query performance is monitored and reported accurately, and query execution plans can be inspected for optimization.</p>\n<h4 id=\"language-specific-debugging-hints\">Language-Specific Debugging Hints</h4>\n<p><strong>Go Race Detection and Concurrency Debugging</strong></p>\n<ul>\n<li>Always run tests with <code>-race</code> flag during development: <code>go test -race ./...</code></li>\n<li>Use <code>go run -race</code> when testing manually to catch race conditions early</li>\n<li>Monitor goroutine count with <code>runtime.NumGoroutine()</code> to detect goroutine leaks</li>\n<li>Use <code>runtime.GC(); runtime.GC()</code> to force garbage collection before measuring memory usage</li>\n<li>Use buffered channels with explicit capacity to avoid deadlocks: <code>make(chan Sample, 100)</code></li>\n</ul>\n<p><strong>Memory Profiling and Optimization</strong></p>\n<ul>\n<li>Use <code>go tool pprof http://localhost:8080/debug/pprof/heap</code> for interactive memory analysis</li>\n<li>Check for memory leaks by comparing heap profiles over time: <code>go tool pprof -base profile1.pb.gz profile2.pb.gz</code></li>\n<li>Use <code>runtime/debug.SetGCPercent()</code> to tune garbage collection frequency based on workload</li>\n<li>Implement object pooling with <code>sync.Pool</code> for frequently allocated objects like samples and labels</li>\n<li>Use <code>unsafe.Sizeof()</code> to understand memory layout of critical data structures</li>\n</ul>\n<p><strong>HTTP Client Configuration for Reliability</strong></p>\n<ul>\n<li>Set reasonable timeouts: <code>http.Client{Timeout: 30 * time.Second}</code></li>\n<li>Configure connection pooling: <code>http.Transport{MaxIdleConns: 100, MaxIdleConnsPerHost: 10}</code></li>\n<li>Use context cancellation: <code>req.WithContext(ctx)</code> for all HTTP requests</li>\n<li>Implement exponential backoff for retries using <code>time.Sleep(time.Duration(attempt*attempt) * time.Second)</code></li>\n<li>Monitor connection pool usage with custom <code>http.Transport</code> metrics</li>\n</ul>\n<p><strong>Error Handling and Recovery Patterns</strong></p>\n<ul>\n<li>Use typed errors for different failure modes: <code>type NetworkError struct { ... }</code></li>\n<li>Implement circuit breakers for external dependencies to prevent cascade failures</li>\n<li>Log errors with full context including operation, input parameters, and system state</li>\n<li>Use <code>defer</code> statements for cleanup that must happen regardless of success or failure</li>\n<li>Implement graceful degradation where partial functionality is better than complete failure</li>\n</ul>\n<h2 id=\"future-extensions-and-scalability\">Future Extensions and Scalability</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section demonstrates how the Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4) architecture supports future enhancements without major redesigns.</p>\n</blockquote>\n<p>Think of our metrics collection system as a city&#39;s infrastructure. We&#39;ve built the fundamental utilities - the power grid (<code>StorageEngine</code>), water system (<code>ScrapeEngine</code>), and communication network (<code>QueryEngine</code>). Now we need to plan for future growth: adding new neighborhoods (federation), emergency services (alerting), and express highways (query optimizations). The key architectural insight is that extensible systems are designed with <strong>composition over inheritance</strong> - new capabilities are added by combining existing components rather than rewriting them.</p>\n<p>The extensibility of our metrics system relies on three core architectural principles that we established in earlier sections. First, our <strong>component isolation</strong> ensures that new features can be added without disrupting existing functionality - the <code>ScrapeEngine</code>, <code>StorageEngine</code>, and <code>QueryEngine</code> communicate through well-defined interfaces rather than tight coupling. Second, our <strong>data model completeness</strong> means that metric labels, timestamps, and metadata provide sufficient information for advanced features like rule evaluation and cross-instance queries. Third, our <strong>pipeline architecture</strong> with channels and coordinators creates natural extension points where new processing stages can be inserted.</p>\n<h3 id=\"alerting-system\">Alerting System</h3>\n<p>The alerting system represents the first major extension that transforms our passive metrics collection into an active monitoring platform. Think of alerting as adding a security system to our infrastructure city - it continuously watches for specific conditions and triggers responses when thresholds are crossed.</p>\n<h4 id=\"rule-evaluation-engine\">Rule Evaluation Engine</h4>\n<p>The <strong>Rule Evaluation Engine</strong> extends our existing <code>QueryEngine</code> by adding continuous evaluation of PromQL expressions against incoming time series data. Rather than building a separate system, we leverage the query parsing and execution infrastructure we already have.</p>\n<p><strong>AlertRule Data Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>Unique identifier for the alert rule</td>\n</tr>\n<tr>\n<td><code>Query</code></td>\n<td><code>string</code></td>\n<td>PromQL expression to evaluate</td>\n</tr>\n<tr>\n<td><code>Duration</code></td>\n<td><code>time.Duration</code></td>\n<td>How long condition must be true before firing</td>\n</tr>\n<tr>\n<td><code>Labels</code></td>\n<td><code>Labels</code></td>\n<td>Additional labels attached to alert instances</td>\n</tr>\n<tr>\n<td><code>Annotations</code></td>\n<td><code>map[string]string</code></td>\n<td>Human-readable descriptions and runbook links</td>\n</tr>\n<tr>\n<td><code>EvaluationInterval</code></td>\n<td><code>time.Duration</code></td>\n<td>How frequently to evaluate the rule</td>\n</tr>\n<tr>\n<td><code>State</code></td>\n<td><code>AlertState</code></td>\n<td>Current state: pending, firing, or resolved</td>\n</tr>\n<tr>\n<td><code>ActiveSince</code></td>\n<td><code>time.Time</code></td>\n<td>When the alert first became active</td>\n</tr>\n<tr>\n<td><code>ResolvedAt</code></td>\n<td><code>*time.Time</code></td>\n<td>When the alert was resolved (nil if still active)</td>\n</tr>\n</tbody></table>\n<p>The rule evaluation process follows a continuous assessment cycle:</p>\n<ol>\n<li>The <code>RuleEvaluator</code> maintains a schedule of all active alert rules with their next evaluation times</li>\n<li>At each evaluation interval, it executes the rule&#39;s PromQL query using our existing <code>QueryEngine.ExecuteInstantQuery</code> method</li>\n<li>For each time series returned by the query, it checks if the result value meets the alert condition</li>\n<li>If the condition is met, it starts tracking the duration - alerts only fire after being active for the specified <code>Duration</code></li>\n<li>When an alert transitions from pending to firing, it generates an <code>AlertInstance</code> with the current timestamp and label set</li>\n<li>The evaluator continues checking resolved conditions - when the query returns no results or false values, it marks alerts as resolved</li>\n</ol>\n<blockquote>\n<p><strong>Design Insight</strong>: By reusing our <code>QueryEngine</code> for rule evaluation, we automatically inherit all the query optimization, caching, and error handling that we built for user queries. This demonstrates the power of composable architecture - new features leverage existing robust components.</p>\n</blockquote>\n<p><strong>RuleEvaluator Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AddRule</code></td>\n<td><code>rule *AlertRule</code></td>\n<td><code>error</code></td>\n<td>Registers a new alert rule for evaluation</td>\n</tr>\n<tr>\n<td><code>RemoveRule</code></td>\n<td><code>ruleName string</code></td>\n<td><code>error</code></td>\n<td>Stops evaluating and removes an alert rule</td>\n</tr>\n<tr>\n<td><code>EvaluateAll</code></td>\n<td><code>ctx context.Context, evalTime time.Time</code></td>\n<td><code>[]AlertInstance, error</code></td>\n<td>Evaluates all rules at the specified time</td>\n</tr>\n<tr>\n<td><code>GetAlertState</code></td>\n<td><code>ruleName string</code></td>\n<td><code>AlertState, error</code></td>\n<td>Returns current state of a specific alert rule</td>\n</tr>\n<tr>\n<td><code>ListActiveAlerts</code></td>\n<td><code>labels LabelMatcher</code></td>\n<td><code>[]AlertInstance, error</code></td>\n<td>Returns all currently firing alerts matching label filters</td>\n</tr>\n</tbody></table>\n<h4 id=\"notification-manager\">Notification Manager</h4>\n<p>The <strong>Notification Manager</strong> handles the delivery of alert notifications through multiple channels. Think of it as the emergency dispatch system - when the security system (rule evaluator) detects a problem, the dispatcher determines who to notify and how.</p>\n<p><strong>NotificationChannel Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Send</code></td>\n<td><code>ctx context.Context, alert AlertInstance</code></td>\n<td><code>error</code></td>\n<td>Delivers alert notification through this channel</td>\n</tr>\n<tr>\n<td><code>Test</code></td>\n<td><code>ctx context.Context</code></td>\n<td><code>error</code></td>\n<td>Verifies channel configuration and connectivity</td>\n</tr>\n<tr>\n<td><code>GetType</code></td>\n<td></td>\n<td><code>string</code></td>\n<td>Returns channel type (email, slack, webhook, etc.)</td>\n</tr>\n<tr>\n<td><code>IsHealthy</code></td>\n<td></td>\n<td><code>bool</code></td>\n<td>Indicates if channel is currently operational</td>\n</tr>\n</tbody></table>\n<p>The notification pipeline implements sophisticated routing and de-duplication:</p>\n<ol>\n<li><strong>Alert Grouping</strong>: Multiple related alerts are combined into a single notification to prevent spam - alerts with identical label sets (excluding instance-specific labels) are grouped together</li>\n<li><strong>Rate Limiting</strong>: Each notification channel has configurable rate limits to prevent overwhelming external systems during large-scale outages</li>\n<li><strong>Retry Logic</strong>: Failed notifications are queued for retry with exponential backoff - critical alerts get more aggressive retry attempts</li>\n<li><strong>Escalation Chains</strong>: After a specified time without acknowledgment, alerts can escalate to additional notification channels or contacts</li>\n<li><strong>Maintenance Windows</strong>: Notifications can be suppressed during scheduled maintenance periods based on label matching</li>\n</ol>\n<blockquote>\n<p><strong>Decision: Notification Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Alert notifications need to be reliable, fast, and handle various delivery failures</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Direct synchronous delivery from rule evaluator</li>\n<li>Asynchronous queue-based delivery with persistence</li>\n<li>Hybrid approach with immediate delivery plus persistent retry queue</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Asynchronous queue-based delivery with WAL persistence</li>\n<li><strong>Rationale</strong>: Rule evaluation must continue even if notification delivery fails. Persistent queue ensures alerts aren&#39;t lost during system restarts</li>\n<li><strong>Consequences</strong>: Adds complexity but provides notification reliability and system resilience</li>\n</ul>\n</blockquote>\n<h4 id=\"alert-state-management\">Alert State Management</h4>\n<p>Alert state management tracks the lifecycle of alert instances from creation through resolution. This extends our existing <code>StorageEngine</code> with specialized alert state persistence.</p>\n<p><strong>AlertInstance Data Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>RuleName</code></td>\n<td><code>string</code></td>\n<td>Name of the alert rule that generated this instance</td>\n</tr>\n<tr>\n<td><code>Labels</code></td>\n<td><code>Labels</code></td>\n<td>Complete label set including rule labels and metric labels</td>\n</tr>\n<tr>\n<td><code>State</code></td>\n<td><code>AlertState</code></td>\n<td>Current state: pending, firing, resolved</td>\n</tr>\n<tr>\n<td><code>Value</code></td>\n<td><code>float64</code></td>\n<td>The metric value that triggered the alert</td>\n</tr>\n<tr>\n<td><code>StartsAt</code></td>\n<td><code>time.Time</code></td>\n<td>When the alert condition first became true</td>\n</tr>\n<tr>\n<td><code>EndsAt</code></td>\n<td><code>*time.Time</code></td>\n<td>When the alert condition resolved (nil if still active)</td>\n</tr>\n<tr>\n<td><code>GeneratorURL</code></td>\n<td><code>string</code></td>\n<td>URL to query that generated this alert</td>\n</tr>\n<tr>\n<td><code>Fingerprint</code></td>\n<td><code>uint64</code></td>\n<td>Hash of label set for efficient deduplication</td>\n</tr>\n</tbody></table>\n<p>Alert state transitions follow a strict finite state machine:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Event</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>None</td>\n<td>Query returns true</td>\n<td>Pending</td>\n<td>Record start time, begin duration tracking</td>\n</tr>\n<tr>\n<td>Pending</td>\n<td>Duration exceeded</td>\n<td>Firing</td>\n<td>Generate notification, mark as active</td>\n</tr>\n<tr>\n<td>Pending</td>\n<td>Query returns false</td>\n<td>None</td>\n<td>Clear tracking, no notification</td>\n</tr>\n<tr>\n<td>Firing</td>\n<td>Query returns false</td>\n<td>Resolved</td>\n<td>Send resolution notification, record end time</td>\n</tr>\n<tr>\n<td>Resolved</td>\n<td>Query returns true</td>\n<td>Pending</td>\n<td>Start new alert instance cycle</td>\n</tr>\n</tbody></table>\n<h3 id=\"multi-instance-federation\">Multi-Instance Federation</h3>\n<p>Federation enables horizontal scaling by connecting multiple independent metrics collection instances into a coordinated cluster. Think of federation as connecting multiple city infrastructures into a metropolitan area - each city maintains its own services while sharing critical information across the region.</p>\n<h4 id=\"hierarchical-federation-model\">Hierarchical Federation Model</h4>\n<p>Our federation model follows a <strong>hierarchical pull-based approach</strong> that aligns with our existing scrape engine architecture. Rather than building complex consensus protocols, we extend the scraping concept to pull metrics from other Prometheus-compatible instances.</p>\n<p><strong>Federation Configuration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>FederationConfig</code></td>\n<td><code>struct</code></td>\n<td>Top-level federation configuration</td>\n</tr>\n<tr>\n<td><code>UpstreamTargets</code></td>\n<td><code>[]FederationTarget</code></td>\n<td>List of upstream instances to scrape from</td>\n</tr>\n<tr>\n<td><code>MatchRules</code></td>\n<td><code>[]FederationRule</code></td>\n<td>Rules for selecting which metrics to federate</td>\n</tr>\n<tr>\n<td><code>ScrapInterval</code></td>\n<td><code>time.Duration</code></td>\n<td>How often to scrape upstream instances</td>\n</tr>\n<tr>\n<td><code>ExternalLabels</code></td>\n<td><code>Labels</code></td>\n<td>Labels added to all metrics from this instance</td>\n</tr>\n</tbody></table>\n<p>The federation process extends our existing target discovery and scraping:</p>\n<ol>\n<li><strong>Target Discovery</strong>: Federation targets are configured as static targets in the <code>ScrapeEngine</code> with special federation endpoints (<code>/federate</code>)</li>\n<li><strong>Metric Selection</strong>: Each federation target specifies <code>MatchRules</code> that determine which metrics to pull - this prevents recursive federation and controls data volume</li>\n<li><strong>Label Rewriting</strong>: Federated metrics receive additional external labels that identify their source instance and prevent label conflicts</li>\n<li><strong>Conflict Resolution</strong>: When multiple instances provide the same time series (same metric name and labels), the most recent timestamp wins</li>\n<li><strong>Topology Management</strong>: The federation hierarchy is maintained through configuration - each instance knows its role (leaf, intermediate, root) and scrapes accordingly</li>\n</ol>\n<blockquote>\n<p><strong>Design Insight</strong>: By implementing federation as an extension of scraping rather than a separate mechanism, we reuse all the existing HTTP client code, retry logic, target health checking, and metrics parsing. This is a prime example of architectural composition.</p>\n</blockquote>\n<p><strong>FederationRule Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MatchMetrics</code></td>\n<td><code>[]string</code></td>\n<td>Metric name patterns to include (supports regex)</td>\n</tr>\n<tr>\n<td><code>MatchLabels</code></td>\n<td><code>[]LabelMatcher</code></td>\n<td>Label conditions that must be satisfied</td>\n</tr>\n<tr>\n<td><code>ExcludeMetrics</code></td>\n<td><code>[]string</code></td>\n<td>Metric name patterns to exclude</td>\n</tr>\n<tr>\n<td><code>SampleLimit</code></td>\n<td><code>int</code></td>\n<td>Maximum samples per scrape to prevent overload</td>\n</tr>\n</tbody></table>\n<h4 id=\"cross-instance-querying\">Cross-Instance Querying</h4>\n<p>Cross-instance querying allows PromQL queries to span multiple federation instances, providing a global view of metrics across the entire infrastructure. This extends our <code>QueryEngine</code> with <strong>query federation capabilities</strong>.</p>\n<p>The federated query process works through query distribution and result merging:</p>\n<ol>\n<li><strong>Query Analysis</strong>: The <code>QueryPlanner</code> analyzes incoming PromQL queries to determine which federated instances might contain relevant data</li>\n<li><strong>Query Distribution</strong>: Queries are sent to relevant upstream instances in parallel using HTTP requests to their <code>/query</code> endpoints</li>\n<li><strong>Result Merging</strong>: Partial results from multiple instances are combined using the same aggregation logic as our local <code>Aggregator</code></li>\n<li><strong>Deduplication</strong>: Time series with identical label sets from multiple sources are deduplicated based on external labels and timestamps</li>\n<li><strong>Error Handling</strong>: Partial failures from some instances don&#39;t prevent returning results from available instances</li>\n</ol>\n<p><strong>QueryFederator Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ExecuteFederatedQuery</code></td>\n<td><code>ctx context.Context, query string, evalTime time.Time</code></td>\n<td><code>*QueryResult, error</code></td>\n<td>Executes query across federated instances</td>\n</tr>\n<tr>\n<td><code>GetAvailableInstances</code></td>\n<td><code>labels LabelMatcher</code></td>\n<td><code>[]FederationTarget, error</code></td>\n<td>Returns instances that might contain matching data</td>\n</tr>\n<tr>\n<td><code>MergeResults</code></td>\n<td><code>results []QueryResult</code></td>\n<td><code>*QueryResult, error</code></td>\n<td>Combines partial results from multiple instances</td>\n</tr>\n<tr>\n<td><code>AddFederationTarget</code></td>\n<td><code>target FederationTarget</code></td>\n<td><code>error</code></td>\n<td>Registers new instance for federated queries</td>\n</tr>\n</tbody></table>\n<h4 id=\"global-view-consistency\">Global View Consistency</h4>\n<p>Maintaining consistency across federated instances requires careful handling of clock skew, network partitions, and instance failures. Our approach prioritizes <strong>availability over strict consistency</strong> - we provide eventually consistent global views rather than strong consistency guarantees.</p>\n<p><strong>Clock Skew Handling</strong>: Federation instances may have slightly different system clocks, causing timestamp misalignment. We address this through:</p>\n<ul>\n<li>Configurable tolerance windows that accept samples within a reasonable time range (typically ±1 minute)</li>\n<li>Timestamp normalization during federation that adjusts for known clock skew between instances</li>\n<li>Warning alerts when clock skew exceeds acceptable thresholds</li>\n</ul>\n<p><strong>Network Partition Resilience</strong>: When federation links fail, each instance continues operating independently:</p>\n<ul>\n<li>Local queries continue working against locally stored data</li>\n<li>Federation queries return partial results with warnings about unavailable instances  </li>\n<li>Automatic reconnection attempts restore federation links when network connectivity returns</li>\n<li>Backfill mechanisms can catch up on missed data after reconnection</li>\n</ul>\n<h3 id=\"advanced-query-features\">Advanced Query Features</h3>\n<p>Advanced query features extend our basic PromQL implementation with performance optimizations, computed metrics, and sophisticated analytical capabilities that would typically be added after the core system proves itself in production.</p>\n<h4 id=\"recording-rules\">Recording Rules</h4>\n<p><strong>Recording Rules</strong> pre-compute expensive queries and store the results as new time series, dramatically improving dashboard and alert performance for complex aggregations. Think of recording rules as creating express highway routes through our query system - frequently used complex paths get dedicated infrastructure for faster transit.</p>\n<p>Recording rules extend our alert rule evaluation infrastructure by treating computed metrics as first-class time series:</p>\n<p><strong>RecordingRule Structure:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>Unique identifier for the recording rule</td>\n</tr>\n<tr>\n<td><code>Query</code></td>\n<td><code>string</code></td>\n<td>PromQL expression to evaluate and store</td>\n</tr>\n<tr>\n<td><code>MetricName</code></td>\n<td><code>string</code></td>\n<td>Name for the generated time series</td>\n</tr>\n<tr>\n<td><code>Labels</code></td>\n<td><code>Labels</code></td>\n<td>Additional labels attached to computed metrics</td>\n</tr>\n<tr>\n<td><code>EvaluationInterval</code></td>\n<td><code>time.Duration</code></td>\n<td>How often to evaluate and update the recording</td>\n</tr>\n<tr>\n<td><code>LastEvaluation</code></td>\n<td><code>time.Time</code></td>\n<td>Timestamp of most recent rule execution</td>\n</tr>\n<tr>\n<td><code>EvaluationDuration</code></td>\n<td><code>time.Duration</code></td>\n<td>How long the last evaluation took</td>\n</tr>\n<tr>\n<td><code>SamplesProduced</code></td>\n<td><code>int64</code></td>\n<td>Number of time series points generated</td>\n</tr>\n</tbody></table>\n<p>The recording rule evaluation process follows a continuous computation cycle:</p>\n<ol>\n<li><strong>Rule Scheduling</strong>: The <code>RuleEvaluator</code> schedules recording rules alongside alert rules, maintaining separate evaluation intervals for each</li>\n<li><strong>Query Execution</strong>: At each interval, the rule&#39;s PromQL query executes using our standard <code>QueryEngine.ExecuteInstantQuery</code> method</li>\n<li><strong>Result Transformation</strong>: Query results are converted into new time series with the specified <code>MetricName</code> and additional labels</li>\n<li><strong>Storage Integration</strong>: Generated samples are fed back into our <code>StorageEngine</code> through the same <code>Append</code> interface used by the scrape engine</li>\n<li><strong>Metadata Management</strong>: Recording rule metadata (evaluation time, sample count) is tracked for monitoring rule performance</li>\n</ol>\n<blockquote>\n<p><strong>Architecture Insight</strong>: Recording rules create a feedback loop where the query engine feeds computed results back into storage, which can then be queried again. This requires careful cycle detection to prevent infinite recursion in rule dependencies.</p>\n</blockquote>\n<p><strong>Common Recording Rule Patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Example Query</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rate Calculation</td>\n<td><code>rate(http_requests_total[5m])</code></td>\n<td>Pre-compute request rates for dashboards</td>\n</tr>\n<tr>\n<td>Quantile Aggregation</td>\n<td><code>histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))</code></td>\n<td>Expensive percentile calculations</td>\n</tr>\n<tr>\n<td>Cross-Service Aggregation</td>\n<td><code>sum by (service) (rate(errors_total[1m])) / sum by (service) (rate(requests_total[1m]))</code></td>\n<td>Service-level error rates</td>\n</tr>\n<tr>\n<td>Resource Utilization</td>\n<td><code>avg by (cluster) ((cpu_usage / cpu_limit) * 100)</code></td>\n<td>Cluster-wide resource metrics</td>\n</tr>\n</tbody></table>\n<h4 id=\"query-optimization-engine\">Query Optimization Engine</h4>\n<p>The <strong>Query Optimization Engine</strong> analyzes PromQL queries to identify performance improvements through query rewriting, caching, and execution plan optimization. This extends our <code>QueryEngine</code> with intelligent query planning capabilities.</p>\n<p>Query optimization operates through multiple analysis phases:</p>\n<ol>\n<li><strong>Syntax Tree Analysis</strong>: The query&#39;s AST is analyzed to identify expensive operations like large time range scans or high-cardinality aggregations</li>\n<li><strong>Series Cardinality Estimation</strong>: Before execution, the optimizer estimates how many time series will be involved based on label selector specificity</li>\n<li><strong>Execution Plan Generation</strong>: Multiple query execution strategies are generated and their estimated costs are compared</li>\n<li><strong>Cache Utilization</strong>: The optimizer checks if partial results for subexpressions are available in the query result cache</li>\n<li><strong>Query Rewriting</strong>: Equivalent but more efficient query forms are substituted (e.g., using recording rules instead of complex aggregations)</li>\n</ol>\n<p><strong>QueryOptimizer Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>OptimizeQuery</code></td>\n<td><code>ctx context.Context, query string, timeRange TimeRange</code></td>\n<td><code>*OptimizedQuery, error</code></td>\n<td>Returns optimized execution plan</td>\n</tr>\n<tr>\n<td><code>EstimateComplexity</code></td>\n<td><code>query string, timeRange TimeRange</code></td>\n<td><code>*ComplexityEstimate, error</code></td>\n<td>Predicts query resource requirements</td>\n</tr>\n<tr>\n<td><code>SuggestRecordingRules</code></td>\n<td><code>queries []string, frequency time.Duration</code></td>\n<td><code>[]RecordingRuleSuggestion, error</code></td>\n<td>Identifies queries that would benefit from pre-computation</td>\n</tr>\n<tr>\n<td><code>UpdateStatistics</code></td>\n<td><code>query string, duration time.Duration, seriesCount int</code></td>\n<td><code>error</code></td>\n<td>Records query performance for future optimization</td>\n</tr>\n</tbody></table>\n<p><strong>Query Result Caching</strong> significantly improves repeated query performance by storing computed results with cache keys based on query text, time range, and evaluation timestamp:</p>\n<ul>\n<li><strong>Immutable Range Caching</strong>: Queries for historical time ranges (older than the staleness threshold) return identical results and can be cached indefinitely</li>\n<li><strong>Partial Result Caching</strong>: Large time range queries are broken into smaller chunks, with completed chunks cached while only the most recent chunk is recomputed</li>\n<li><strong>Cache Invalidation</strong>: Recording rule updates and data ingestion trigger selective cache invalidation based on affected metric names and label sets</li>\n<li><strong>Memory Management</strong>: LRU eviction prevents cache from consuming excessive memory, with configurable size limits and TTL policies</li>\n</ul>\n<h4 id=\"advanced-aggregation-functions\">Advanced Aggregation Functions</h4>\n<p>Advanced aggregation functions extend our basic <code>sum</code>, <code>avg</code>, <code>max</code>, <code>min</code>, and <code>count</code> operations with sophisticated statistical and mathematical capabilities commonly needed in production monitoring environments.</p>\n<p><strong>Statistical Aggregations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>stddev_over_time</code></td>\n<td><code>vector, duration</code></td>\n<td><code>vector</code></td>\n<td>Standard deviation of values over time window</td>\n</tr>\n<tr>\n<td><code>quantile_over_time</code></td>\n<td><code>quantile, vector, duration</code></td>\n<td><code>vector</code></td>\n<td>Arbitrary quantile calculation over time</td>\n</tr>\n<tr>\n<td><code>mad_over_time</code></td>\n<td><code>vector, duration</code></td>\n<td><code>vector</code></td>\n<td>Median absolute deviation for outlier detection</td>\n</tr>\n<tr>\n<td><code>predict_linear</code></td>\n<td><code>vector, duration</code></td>\n<td><code>vector</code></td>\n<td>Linear regression prediction of future values</td>\n</tr>\n<tr>\n<td><code>deriv</code></td>\n<td><code>vector</code></td>\n<td><code>vector</code></td>\n<td>Per-second derivative calculation</td>\n</tr>\n</tbody></table>\n<p><strong>Advanced Grouping Operations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>topk</code></td>\n<td><code>k, vector</code></td>\n<td><code>vector</code></td>\n<td>Top K series by value</td>\n</tr>\n<tr>\n<td><code>bottomk</code></td>\n<td><code>k, vector</code></td>\n<td><code>vector</code></td>\n<td>Bottom K series by value</td>\n</tr>\n<tr>\n<td><code>count_values</code></td>\n<td><code>string, vector</code></td>\n<td><code>vector</code></td>\n<td>Count occurrences of each distinct value</td>\n</tr>\n<tr>\n<td><code>group_by_interval</code></td>\n<td><code>vector, duration</code></td>\n<td><code>vector</code></td>\n<td>Time-based grouping for irregular series</td>\n</tr>\n</tbody></table>\n<p>These advanced functions are implemented as extensions to our existing <code>Aggregator</code> component, following the same interface patterns but with more complex mathematical operations:</p>\n<ol>\n<li><strong>Streaming Computation</strong>: Statistical functions process samples in streaming fashion to handle large time ranges without loading entire datasets into memory</li>\n<li><strong>Numerical Stability</strong>: Implementations use numerically stable algorithms (e.g., Welford&#39;s method for standard deviation) to prevent precision loss</li>\n<li><strong>Missing Data Handling</strong>: Advanced functions include configurable strategies for handling missing or null values in time series</li>\n<li><strong>Performance Optimization</strong>: Expensive computations are candidates for automatic recording rule generation when used frequently</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Query Complexity Explosion</strong></p>\n<p>Advanced query features can easily create queries that consume excessive system resources. A common mistake is allowing unrestricted use of expensive functions like <code>quantile_over_time</code> over large time ranges with high-cardinality label sets. This can consume gigabytes of memory and take minutes to execute.</p>\n<p><strong>Prevention Strategy</strong>: Implement query complexity estimation that considers the number of time series, time range duration, and computational complexity of functions. Reject or warn about queries that exceed resource limits:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Estimated complexity: 50GB memory, 120 seconds execution\nThis query spans 10,000 time series over 7 days with quantile calculation.\nConsider: reducing time range, adding more specific label filters, or creating a recording rule.</code></pre></div>\n\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The extensibility features build upon our existing component architecture through composition and interface extension rather than core system modification. This section provides practical guidance for implementing these advanced capabilities.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Storage</td>\n<td>YAML files + file watching</td>\n<td>etcd cluster with version control</td>\n</tr>\n<tr>\n<td>Notification Delivery</td>\n<td>HTTP webhooks + retry queues</td>\n<td>Message queue system (RabbitMQ/Kafka)</td>\n</tr>\n<tr>\n<td>Federation Transport</td>\n<td>HTTP scraping (existing)</td>\n<td>gRPC streaming for high-volume federation</td>\n</tr>\n<tr>\n<td>Query Caching</td>\n<td>In-memory LRU cache</td>\n<td>Redis cluster with persistence</td>\n</tr>\n<tr>\n<td>Result Storage</td>\n<td>Extend existing StorageEngine</td>\n<td>Separate OLAP system for recording rules</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure for Extensions:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  internal/\n    alerting/\n      rule_evaluator.go           ← Alert rule evaluation engine\n      notification_manager.go     ← Multi-channel notification delivery  \n      alert_state.go             ← Alert lifecycle management\n      rule_evaluator_test.go     ← Comprehensive rule testing\n    federation/\n      federation_scraper.go       ← Extends ScrapeEngine for federation\n      query_federator.go         ← Cross-instance query distribution\n      topology_manager.go        ← Federation hierarchy management\n    query_advanced/\n      recording_rules.go         ← Pre-computed metrics engine\n      query_optimizer.go         ← Query performance optimization\n      advanced_aggregations.go   ← Statistical and mathematical functions\n      query_cache.go             ← Result caching with invalidation\n  configs/\n    alerting_rules.yml           ← Alert rule definitions\n    recording_rules.yml          ← Recording rule definitions\n    federation.yml               ← Federation topology configuration</code></pre></div>\n\n<p><strong>Alert Rule Evaluator Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> alerting</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/to/project/internal/query</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/to/project/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertState represents the current state of an alert rule</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStateInactive</span><span style=\"color:#B392F0\"> AlertState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStatePending</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStateFiring</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStateResolved</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertRule defines a condition to monitor and alert on</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name               </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration           </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">     `yaml:\"for\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels             </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Annotations        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"annotations\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EvaluationInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">     `yaml:\"interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Internal state tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    State        </span><span style=\"color:#B392F0\">AlertState</span><span style=\"color:#9ECBFF\">    `yaml:\"-\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ActiveSince  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">     `yaml:\"-\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResolvedAt   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">    `yaml:\"-\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span><span style=\"color:#9ECBFF\">  `yaml:\"-\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertInstance represents a specific firing alert</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertInstance</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RuleName     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">             `json:\"rule_name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">  `json:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    State        </span><span style=\"color:#B392F0\">AlertState</span><span style=\"color:#9ECBFF\">         `json:\"state\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value        </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">            `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartsAt     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">          `json:\"starts_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EndsAt       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">         `json:\"ends_at,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GeneratorURL </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">             `json:\"generator_url\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Fingerprint  </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">             `json:\"fingerprint\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RuleEvaluator manages alert rule evaluation and state tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RuleEvaluator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rules       </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertRule</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryEngine </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    activeAlerts    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#6A737D\">  // fingerprint -> alert</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    notificationCh  </span><span style=\"color:#F97583\">chan&#x3C;-</span><span style=\"color:#B392F0\"> AlertInstance</span><span style=\"color:#6A737D\">       // channel for sending notifications</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    evaluationTicker </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Ticker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh          </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewRuleEvaluator creates a rule evaluator with notification channel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewRuleEvaluator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">queryEngine</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                      notificationCh</span><span style=\"color:#F97583\"> chan&#x3C;-</span><span style=\"color:#B392F0\"> AlertInstance</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleEvaluator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">RuleEvaluator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rules:           </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryEngine:     queryEngine,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:         storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        activeAlerts:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">uint64</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        notificationCh:  notificationCh,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stopCh:          </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadRulesFromFile loads alert rules from YAML configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">re </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LoadRulesFromFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Read YAML file containing alert rule definitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse YAML into []AlertRule using yaml.Unmarshal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate each rule: check query syntax, ensure positive duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call AddRule for each valid rule to register it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return error if any rule validation fails</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AddRule registers a new alert rule for evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">re </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AddRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate rule.Query by parsing it with queryEngine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Ensure rule.Name is unique among existing rules  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set default EvaluationInterval if not specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Store rule in re.rules map with name as key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log successful rule registration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins periodic rule evaluation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">re </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">evaluationInterval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create ticker with specified evaluation interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start goroutine that calls EvaluateAll on each tick</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle context cancellation to stop evaluation loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Ensure proper cleanup of ticker and goroutines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EvaluateAll evaluates all registered rules at the specified time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">re </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EvaluateAll</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">evalTime</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Iterate through all rules in re.rules map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each rule, execute rule.Query using queryEngine.ExecuteInstantQuery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Process query results to determine if alert condition is met</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update rule state (inactive -> pending -> firing) based on duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate AlertInstance for newly firing alerts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Send new/resolved alerts to notification channel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return list of all currently active alert instances</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Federation Target Discovery (Core Logic Skeleton):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> federation</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/to/project/internal/scrape</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FederationTarget represents an upstream metrics instance to federate from</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FederationTarget</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    URL           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `yaml:\"url\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MatchRules    []</span><span style=\"color:#B392F0\">FederationRule</span><span style=\"color:#9ECBFF\">    `yaml:\"match_rules\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScrapeInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">      `yaml:\"scrape_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExternalLabels </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">  `yaml:\"external_labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FederationRule defines which metrics to pull from upstream instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FederationRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MatchMetrics   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                    `yaml:\"match_metrics\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MatchLabels    []</span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">LabelMatcher</span><span style=\"color:#9ECBFF\">        `yaml:\"match_labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExcludeMetrics []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                    `yaml:\"exclude_metrics\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SampleLimit    </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">                         `yaml:\"sample_limit\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FederationScraper extends ScrapeEngine to handle federation endpoints</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FederationScraper</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    baseScraper </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">scrape</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ScrapeEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpClient  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    targets     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">FederationTarget</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScrapeFederationTarget pulls metrics from upstream federation endpoint</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fs </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FederationScraper</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ScrapeFederationTarget</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">target</span><span style=\"color:#B392F0\"> FederationTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Build federation URL with match[] parameters from target.MatchRules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create HTTP request with proper timeout and user-agent headers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute HTTP GET request to /federate endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse response body as Prometheus exposition format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply external labels from target.ExternalLabels to all metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Filter metrics based on target.MatchRules inclusion/exclusion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Send filtered samples to storage engine through existing pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateFederationTargets refreshes the list of upstream instances</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">fs </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FederationScraper</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateFederationTargets</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">targets</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">FederationTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate each target URL and match rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check for target URL uniqueness to prevent duplicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update fs.targets map with new target configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Schedule scraping for new targets using existing scheduler</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Remove scraping for targets no longer in the list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Query Result Caching Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query_advanced</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha256</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/hex</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">container/list</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CacheKey uniquely identifies a cached query result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheKey</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"start\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End       </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"end\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EvalTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"eval_time\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String returns string representation for hashing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ck </span><span style=\"color:#B392F0\">CacheKey</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, ck.Query, ck.Start.</span><span style=\"color:#B392F0\">Unix</span><span style=\"color:#E1E4E8\">(), ck.End.</span><span style=\"color:#B392F0\">Unix</span><span style=\"color:#E1E4E8\">(), ck.EvalTime.</span><span style=\"color:#B392F0\">Unix</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Hash returns SHA256 hash of cache key for map storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ck </span><span style=\"color:#B392F0\">CacheKey</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Hash</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> sha256.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hasher.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">(ck.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">()))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hex.</span><span style=\"color:#B392F0\">EncodeToString</span><span style=\"color:#E1E4E8\">(hasher.</span><span style=\"color:#B392F0\">Sum</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CacheEntry stores cached query results with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Key        </span><span style=\"color:#B392F0\">CacheKey</span><span style=\"color:#9ECBFF\">              `json:\"key\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Result     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#9ECBFF\">    `json:\"result\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CachedAt   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">            `json:\"cached_at\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">            `json:\"last_access\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TTL        </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">        `json:\"ttl\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Size       </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">                `json:\"size_bytes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // LRU list element for eviction tracking</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    element </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Element</span><span style=\"color:#9ECBFF\"> `json:\"-\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryCache provides LRU caching for query results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryCache</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entries    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#6A737D\">  // hash -> entry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lruList    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">list</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">List</span><span style=\"color:#6A737D\">              // LRU eviction order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSize    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                   // maximum cache size in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    currentSize </span><span style=\"color:#F97583\">int64</span><span style=\"color:#6A737D\">                  // current cache size in bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutex      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span><span style=\"color:#6A737D\">            // concurrent access protection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Metrics for cache performance monitoring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hits       </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    misses     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    evictions  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewQueryCache creates cache with specified maximum size</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewQueryCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">maxSizeBytes</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        entries:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lruList:  list.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxSize:  maxSizeBytes,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Get retrieves cached result if available and not expired</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#B392F0\"> CacheKey</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate hash of cache key for map lookup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if entry exists and is not expired (cachedAt + TTL > now)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update entry.AccessTime and move to front of LRU list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Increment hit/miss counters for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return deep copy of cached result to prevent modification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Put stores query result in cache with automatic eviction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Put</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">key</span><span style=\"color:#B392F0\"> CacheKey</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">result</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ttl</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate size of result for memory tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if adding entry would exceed maxSize limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Evict LRU entries until sufficient space is available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create CacheEntry with current timestamp and TTL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Store entry in map and add to front of LRU list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update currentSize and evictions counter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InvalidateByMetric removes cached entries that depend on specific metric</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">InvalidateByMetric</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metricName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">int</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Iterate through all cached entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse each entry's query to extract referenced metric names  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Remove entries whose queries reference the invalidated metric</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update cache size counters and LRU list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return number of entries invalidated</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints for Extensions:</strong></p>\n<p><strong>Alerting System Checkpoint:</strong></p>\n<ul>\n<li>Load sample alert rules from YAML: <code>go run cmd/server/main.go --config=configs/alerting_test.yml</code></li>\n<li>Expected: Server starts and logs &quot;Loaded 3 alert rules&quot; </li>\n<li>Verify rule evaluation: Send test metrics that trigger conditions, check alert state API</li>\n<li>Notification testing: Configure webhook endpoint, verify alert notifications are delivered</li>\n<li>Signs of problems: Rules not loading (check YAML syntax), queries failing (validate PromQL), notifications not sending (check webhook URL)</li>\n</ul>\n<p><strong>Federation Checkpoint:</strong></p>\n<ul>\n<li>Configure federation target: Add federation section to config pointing to existing Prometheus</li>\n<li>Expected: Federated metrics appear in local storage with external labels</li>\n<li>Verify query spanning: Execute queries that combine local and federated metrics</li>\n<li>Performance check: Monitor federation scrape duration and memory usage</li>\n<li>Signs of problems: No federated metrics (check upstream /federate endpoint), label conflicts (verify external labels), high memory usage (reduce match rules scope)</li>\n</ul>\n<p><strong>Query Optimization Checkpoint:</strong></p>\n<ul>\n<li>Enable query caching: Set cache_enabled=true in query configuration</li>\n<li>Execute expensive query twice: Second execution should be significantly faster</li>\n<li>Recording rule test: Create recording rule, verify pre-computed metrics are stored</li>\n<li>Cache invalidation: Ingest new data, verify cache entries are properly invalidated</li>\n<li>Performance monitoring: Check cache hit ratio, query duration improvements</li>\n<li>Signs of problems: No cache hits (check TTL settings), memory growth (verify eviction), incorrect results (cache invalidation bugs)</li>\n</ul>\n<p><strong>Debugging Tips for Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Alert rules not firing</td>\n<td>Query returns no results</td>\n<td>Check rule query in query UI</td>\n<td>Verify label selectors match actual metrics</td>\n</tr>\n<tr>\n<td>Notifications not delivered</td>\n<td>Webhook endpoint unreachable</td>\n<td>Check notification manager logs</td>\n<td>Verify webhook URL, check network connectivity</td>\n</tr>\n<tr>\n<td>Federation not working</td>\n<td>Upstream instance not accessible</td>\n<td>Test federation URL manually</td>\n<td>Check upstream /federate endpoint, verify network</td>\n</tr>\n<tr>\n<td>Cached queries wrong results</td>\n<td>Cache not invalidated on data ingestion</td>\n<td>Check cache invalidation logs</td>\n<td>Ensure cache invalidation on metric writes</td>\n</tr>\n<tr>\n<td>Recording rules consuming memory</td>\n<td>Rule generates high-cardinality metrics</td>\n<td>Monitor rule evaluation metrics</td>\n<td>Add more specific label selectors to rule query</td>\n</tr>\n<tr>\n<td>Federation causing high memory</td>\n<td>Too many metrics being federated</td>\n<td>Check federation match rules scope</td>\n<td>Restrict match rules to essential metrics only</td>\n</tr>\n</tbody></table>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides comprehensive definitions for technical terms used throughout all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4).</p>\n</blockquote>\n<p>This glossary serves as the definitive reference for technical terminology, concepts, and vocabulary used throughout the metrics collection system design. Understanding these terms is essential for implementing and maintaining the system effectively. Each term includes its definition, context of use, and relationships to other concepts where applicable.</p>\n<h3 id=\"core-concepts-and-terminology\">Core Concepts and Terminology</h3>\n<p>The metrics collection system introduces several domain-specific concepts that may be unfamiliar to developers coming from other backgrounds. This section establishes a common vocabulary for discussing system behavior, implementation details, and operational characteristics.</p>\n<p><strong>Time Series and Data Model Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>time series</strong></td>\n<td>A sequence of timestamped values identified by a unique combination of metric name and label set</td>\n<td>The fundamental data structure stored and queried by the system</td>\n</tr>\n<tr>\n<td><strong>cardinality</strong></td>\n<td>The number of unique time series created by all combinations of metric name and label values</td>\n<td>Critical metric for memory usage and performance planning</td>\n</tr>\n<tr>\n<td><strong>label explosion</strong></td>\n<td>Exponential growth in cardinality when high-cardinality labels create excessive time series combinations</td>\n<td>Primary cause of memory exhaustion and query performance degradation</td>\n</tr>\n<tr>\n<td><strong>metric type</strong></td>\n<td>The semantic category that defines how a metric behaves: counter, gauge, histogram, or summary</td>\n<td>Determines valid operations and query interpretations</td>\n</tr>\n<tr>\n<td><strong>observability</strong></td>\n<td>The ability to understand system behavior and health through external metrics and monitoring</td>\n<td>The broader goal that metrics collection systems enable</td>\n</tr>\n<tr>\n<td><strong>exposition format</strong></td>\n<td>The Prometheus text-based format used to expose metrics over HTTP endpoints</td>\n<td>Standard format parsed by the scrape engine</td>\n</tr>\n<tr>\n<td><strong>retention period</strong></td>\n<td>The duration for which historical time series data is preserved before automatic deletion</td>\n<td>Balances storage costs with historical analysis needs</td>\n</tr>\n</tbody></table>\n<p><strong>Scraping and Collection Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>pull-based scraping</strong></td>\n<td>A metrics collection model where the monitoring system actively retrieves metrics from targets</td>\n<td>Contrasts with push-based systems where applications send metrics</td>\n</tr>\n<tr>\n<td><strong>scrape target</strong></td>\n<td>An HTTP endpoint that exposes metrics in the exposition format for collection</td>\n<td>The source of all metrics data in the system</td>\n</tr>\n<tr>\n<td><strong>service discovery</strong></td>\n<td>Automatic detection and configuration of scrape targets from dynamic sources like Kubernetes</td>\n<td>Enables monitoring of dynamic environments without manual configuration</td>\n</tr>\n<tr>\n<td><strong>target health</strong></td>\n<td>The availability and response status of a scrape endpoint based on recent collection attempts</td>\n<td>Used for alerting and operational visibility</td>\n</tr>\n<tr>\n<td><strong>scrape interval</strong></td>\n<td>The frequency at which metrics are collected from each target</td>\n<td>Balances data freshness with system load</td>\n</tr>\n<tr>\n<td><strong>scrape timeout</strong></td>\n<td>The maximum duration allowed for a single HTTP metrics collection request</td>\n<td>Prevents slow targets from blocking the scrape engine</td>\n</tr>\n</tbody></table>\n<p><strong>Storage and Compression Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Gorilla compression</strong></td>\n<td>A specialized time series compression algorithm using delta-of-delta timestamp encoding and XOR value compression</td>\n<td>Reduces storage requirements to approximately 1.37 bytes per sample</td>\n</tr>\n<tr>\n<td><strong>WAL</strong></td>\n<td>Write-Ahead Log - a durability mechanism that records intended operations before they are applied</td>\n<td>Enables crash recovery without data loss</td>\n</tr>\n<tr>\n<td><strong>chunk</strong></td>\n<td>A compressed block of time series samples with fixed time boundaries and maximum sample counts</td>\n<td>The unit of storage and compression for time series data</td>\n</tr>\n<tr>\n<td><strong>index consistency</strong></td>\n<td>The property that all indexes correctly map to existing time series data without orphaned references</td>\n<td>Critical for query correctness and system reliability</td>\n</tr>\n<tr>\n<td><strong>downsampling</strong></td>\n<td>The process of reducing data resolution by aggregating high-frequency samples into lower-frequency summaries</td>\n<td>Enables long-term storage with reduced space requirements</td>\n</tr>\n</tbody></table>\n<p><strong>Querying and Analysis Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>PromQL</strong></td>\n<td>Prometheus Query Language - a functional query language for selecting and aggregating time series data</td>\n<td>The primary interface for data analysis and alerting</td>\n</tr>\n<tr>\n<td><strong>AST</strong></td>\n<td>Abstract Syntax Tree - the parsed representation of a PromQL query used for execution</td>\n<td>Internal structure created by the query parser</td>\n</tr>\n<tr>\n<td><strong>label selector</strong></td>\n<td>Filter criteria that match time series based on label name and value patterns</td>\n<td>Fundamental mechanism for narrowing query scope</td>\n</tr>\n<tr>\n<td><strong>aggregation</strong></td>\n<td>Mathematical combination of multiple time series values into summary statistics</td>\n<td>Enables analysis across multiple dimensions and instances</td>\n</tr>\n<tr>\n<td><strong>range query</strong></td>\n<td>A query that returns multiple data points across a specified time window</td>\n<td>Used for graphing and trend analysis</td>\n</tr>\n<tr>\n<td><strong>instant query</strong></td>\n<td>A query that returns a single point-in-time value for each matching time series</td>\n<td>Used for alerting and current state monitoring</td>\n</tr>\n<tr>\n<td><strong>interpolation</strong></td>\n<td>The process of estimating values at query timestamps when no exact sample exists</td>\n<td>Handles alignment between sample timestamps and query evaluation times</td>\n</tr>\n<tr>\n<td><strong>staleness</strong></td>\n<td>The threshold beyond which a data point is considered too old to use in query results</td>\n<td>Prevents outdated values from appearing in current analysis</td>\n</tr>\n</tbody></table>\n<h3 id=\"system-architecture-and-component-terms\">System Architecture and Component Terms</h3>\n<p>The metrics system consists of several interconnected components that coordinate to provide end-to-end metrics collection and analysis capabilities. Understanding the role and terminology of each component is essential for system implementation and maintenance.</p>\n<p><strong>Component and Coordination Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>backpressure</strong></td>\n<td>A flow control mechanism that slows down producers when consumers cannot keep up with the data rate</td>\n<td>Prevents memory exhaustion during high-volume ingestion</td>\n</tr>\n<tr>\n<td><strong>pipeline coordination</strong></td>\n<td>Managing the flow of data between system components with proper synchronization and error handling</td>\n<td>Ensures reliable data flow from scraping through storage to querying</td>\n</tr>\n<tr>\n<td><strong>concurrency control</strong></td>\n<td>Managing simultaneous access to shared resources without data corruption or race conditions</td>\n<td>Critical for multi-threaded components like the storage engine</td>\n</tr>\n<tr>\n<td><strong>graceful degradation</strong></td>\n<td>Maintaining system availability with reduced functionality during overload or partial failures</td>\n<td>Allows continued operation when some components are impaired</td>\n</tr>\n<tr>\n<td><strong>write batching</strong></td>\n<td>Combining multiple small write operations into larger, more efficient batch operations</td>\n<td>Improves storage throughput and reduces overhead</td>\n</tr>\n<tr>\n<td><strong>lock granularity</strong></td>\n<td>The scope and size of data protected by each synchronization primitive</td>\n<td>Affects both performance and correctness in concurrent systems</td>\n</tr>\n<tr>\n<td><strong>copy-on-write</strong></td>\n<td>An optimization where reads access shared data while writes create private copies</td>\n<td>Reduces contention between readers and writers</td>\n</tr>\n<tr>\n<td><strong>readers-writer lock</strong></td>\n<td>A synchronization primitive allowing multiple concurrent readers or a single writer</td>\n<td>Optimizes for read-heavy workloads common in time series systems</td>\n</tr>\n<tr>\n<td><strong>channel-based coordination</strong></td>\n<td>Using Go channels to coordinate communication and synchronization between goroutines</td>\n<td>Primary concurrency pattern in Go-based implementations</td>\n</tr>\n</tbody></table>\n<p><strong>Operational and Monitoring Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>resource monitoring</strong></td>\n<td>Tracking system resource usage to prevent exhaustion and enable proactive limits</td>\n<td>Includes memory, disk space, CPU, and network bandwidth monitoring</td>\n</tr>\n<tr>\n<td><strong>resource exhaustion</strong></td>\n<td>The depletion of system resources that can cause component failures or degraded performance</td>\n<td>Common operational issue requiring monitoring and response procedures</td>\n</tr>\n<tr>\n<td><strong>emergency retention</strong></td>\n<td>Aggressive data deletion triggered automatically during disk space crises</td>\n<td>Last-resort mechanism to prevent complete system failure</td>\n</tr>\n<tr>\n<td><strong>consistency validation</strong></td>\n<td>Verification that stored data matches expected invariants and relationships</td>\n<td>Includes index-data alignment and checksum validation</td>\n</tr>\n<tr>\n<td><strong>partial scrape success</strong></td>\n<td>The ability to preserve valid metrics while rejecting malformed ones from the same target</td>\n<td>Improves system resilience to target-side issues</td>\n</tr>\n<tr>\n<td><strong>circuit breaker</strong></td>\n<td>A failure protection pattern that temporarily blocks requests to failing dependencies</td>\n<td>Prevents cascade failures and enables faster recovery</td>\n</tr>\n</tbody></table>\n<h3 id=\"testing-and-development-terms\">Testing and Development Terms</h3>\n<p>Building a reliable metrics collection system requires comprehensive testing strategies and development practices. These terms describe the approaches and techniques used to validate system correctness and performance.</p>\n<p><strong>Testing Methodology Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>property-based testing</strong></td>\n<td>Testing approach using automatically generated inputs that satisfy specified properties</td>\n<td>Discovers edge cases that manual test cases might miss</td>\n</tr>\n<tr>\n<td><strong>fault injection</strong></td>\n<td>Deliberately introducing errors and failures to test error handling and recovery mechanisms</td>\n<td>Validates system behavior under adverse conditions</td>\n</tr>\n<tr>\n<td><strong>statistical validation</strong></td>\n<td>Comparing computed results against mathematically expected values using statistical methods</td>\n<td>Ensures aggregation functions and calculations produce correct results</td>\n</tr>\n<tr>\n<td><strong>race detector</strong></td>\n<td>A tool for detecting unsynchronized access to shared memory in concurrent programs</td>\n<td>Essential for validating thread safety in Go programs</td>\n</tr>\n<tr>\n<td><strong>benchmark testing</strong></td>\n<td>Measuring performance characteristics like throughput, latency, and resource usage</td>\n<td>Validates that system meets performance requirements</td>\n</tr>\n<tr>\n<td><strong>mock objects</strong></td>\n<td>Test doubles that simulate external dependencies with controllable, predictable behavior</td>\n<td>Enables isolated testing of individual components</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-features-and-extensions\">Advanced Features and Extensions</h3>\n<p>The metrics system architecture supports advanced capabilities that extend beyond basic collection and querying. Understanding these concepts is important for system evolution and operational sophistication.</p>\n<p><strong>Advanced System Capabilities</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>distributed tracing</strong></td>\n<td>End-to-end request tracking that follows operations across multiple system components</td>\n<td>Enables debugging and performance analysis of complex operations</td>\n</tr>\n<tr>\n<td><strong>structured logging</strong></td>\n<td>Machine-readable log format with consistent fields and structured data</td>\n<td>Improves observability and enables automated log analysis</td>\n</tr>\n<tr>\n<td><strong>health checking</strong></td>\n<td>Systematic monitoring of component status with standardized health reporting</td>\n<td>Provides operational visibility and enables automated response</td>\n</tr>\n<tr>\n<td><strong>federation</strong></td>\n<td>Connecting multiple metrics instances into a coordinated cluster for scalability</td>\n<td>Enables horizontal scaling beyond single-instance limits</td>\n</tr>\n<tr>\n<td><strong>recording rules</strong></td>\n<td>Pre-computed expensive queries that are stored as new time series</td>\n<td>Improves query performance for commonly used aggregations</td>\n</tr>\n<tr>\n<td><strong>query optimization</strong></td>\n<td>Analyzing queries to identify performance improvements and suggest alternatives</td>\n<td>Reduces resource consumption and improves user experience</td>\n</tr>\n<tr>\n<td><strong>hierarchical federation</strong></td>\n<td>A pull-based federation approach that extends the scraping concept to other metrics instances</td>\n<td>Maintains consistency with the core pull-based architecture</td>\n</tr>\n<tr>\n<td><strong>cross-instance querying</strong></td>\n<td>PromQL queries that span multiple federated instances and merge results</td>\n<td>Provides unified view of metrics across distributed deployments</td>\n</tr>\n</tbody></table>\n<p><strong>Alerting and Notification Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>alert rule evaluation</strong></td>\n<td>Continuous assessment of PromQL expressions against time series data to detect alert conditions</td>\n<td>Core function of alerting systems built on the metrics platform</td>\n</tr>\n<tr>\n<td><strong>notification manager</strong></td>\n<td>System responsible for delivering alert notifications through multiple channels</td>\n<td>Handles routing, rate limiting, and delivery confirmation</td>\n</tr>\n<tr>\n<td><strong>rule evaluator</strong></td>\n<td>Component that manages alert rule execution, state tracking, and notification generation</td>\n<td>Coordinates between query engine and notification systems</td>\n</tr>\n<tr>\n<td><strong>alert state</strong></td>\n<td>The current condition of an alert rule: inactive, pending, firing, or resolved</td>\n<td>Determines notification behavior and operational response</td>\n</tr>\n</tbody></table>\n<p><strong>Performance and Caching Terms</strong></p>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>query result caching</strong></td>\n<td>Storing computed PromQL results with appropriate cache keys for repeated queries</td>\n<td>Improves response times and reduces computational load</td>\n</tr>\n<tr>\n<td><strong>cache invalidation</strong></td>\n<td>Removing cached entries when underlying time series data changes</td>\n<td>Maintains cache consistency while preserving performance benefits</td>\n</tr>\n<tr>\n<td><strong>query complexity estimation</strong></td>\n<td>Predicting resource requirements before query execution to enable limits and optimization</td>\n<td>Prevents resource exhaustion from expensive queries</td>\n</tr>\n<tr>\n<td><strong>LRU eviction</strong></td>\n<td>Least Recently Used cache entry removal strategy for managing memory usage</td>\n<td>Balances cache hit rates with memory consumption</td>\n</tr>\n<tr>\n<td><strong>query federation</strong></td>\n<td>Distributing queries across multiple instances and merging partial results</td>\n<td>Enables querying of datasets larger than single-instance capacity</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-types-and-structures\">Data Types and Structures</h3>\n<p>The metrics system defines specific data structures and types that represent different aspects of the system. Understanding these types and their relationships is crucial for implementation.</p>\n<p><strong>Core Data Type Categories</strong></p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Purpose</th>\n<th>Key Types</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Configuration Types</strong></td>\n<td>System and component configuration</td>\n<td><code>Config</code>, <code>ScrapeConfig</code>, <code>StorageConfig</code>, <code>QueryConfig</code></td>\n</tr>\n<tr>\n<td><strong>Metric Types</strong></td>\n<td>Time series data representation</td>\n<td><code>Counter</code>, <code>Gauge</code>, <code>Histogram</code>, <code>Sample</code>, <code>Labels</code></td>\n</tr>\n<tr>\n<td><strong>Storage Types</strong></td>\n<td>Data persistence and compression</td>\n<td><code>CompressedChunk</code>, <code>WriteAheadLog</code>, <code>InvertedIndexes</code>, <code>GorillaCompressor</code></td>\n</tr>\n<tr>\n<td><strong>Query Types</strong></td>\n<td>Query execution and results</td>\n<td><code>QueryEngine</code>, <code>ExpressionParser</code>, <code>ASTNode</code>, <code>QueryResult</code></td>\n</tr>\n<tr>\n<td><strong>Coordination Types</strong></td>\n<td>Component interaction and flow control</td>\n<td><code>PipelineCoordinator</code>, <code>QueryCoordinator</code>, <code>SystemCoordinator</code></td>\n</tr>\n<tr>\n<td><strong>Health and Monitoring Types</strong></td>\n<td>System status and diagnostics</td>\n<td><code>ComponentHealth</code>, <code>HealthChecker</code>, <code>SystemHealth</code>, <code>PerformanceMonitor</code></td>\n</tr>\n<tr>\n<td><strong>Error and Control Types</strong></td>\n<td>Error handling and flow control</td>\n<td><code>MetricsError</code>, <code>CircuitBreaker</code>, <code>QueryLimiter</code></td>\n</tr>\n<tr>\n<td><strong>Testing Types</strong></td>\n<td>Development and validation support</td>\n<td><code>MockHTTPTarget</code>, <code>TimeSeriesGenerator</code>, <code>PerformanceMonitor</code></td>\n</tr>\n</tbody></table>\n<h3 id=\"constants-and-configuration-values\">Constants and Configuration Values</h3>\n<p>The system defines standard constants for timeouts, limits, and default behaviors that ensure consistent operation across different deployment environments.</p>\n<p><strong>System Default Constants</strong></p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Value</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>DEFAULT_SCRAPE_INTERVAL</code></td>\n<td>15 seconds</td>\n<td>Standard frequency for metrics collection when not otherwise specified</td>\n</tr>\n<tr>\n<td><code>DEFAULT_SCRAPE_TIMEOUT</code></td>\n<td>10 seconds</td>\n<td>Maximum duration for HTTP scrape requests to prevent blocking</td>\n</tr>\n<tr>\n<td><code>DEFAULT_RETENTION_PERIOD</code></td>\n<td>30 days</td>\n<td>Standard data retention duration balancing storage cost and utility</td>\n</tr>\n<tr>\n<td><code>DEFAULT_QUERY_TIMEOUT</code></td>\n<td>30 seconds</td>\n<td>Maximum query execution time to prevent resource exhaustion</td>\n</tr>\n</tbody></table>\n<p><strong>Health and Status Constants</strong></p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Description</th>\n<th>Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>HealthUp</code></td>\n<td>Component is healthy and operational</td>\n<td>Indicates normal component function</td>\n</tr>\n<tr>\n<td><code>HealthDown</code></td>\n<td>Component has failed or is unreachable</td>\n<td>Indicates complete component failure</td>\n</tr>\n<tr>\n<td><code>HealthDegraded</code></td>\n<td>Component is operational but experiencing issues</td>\n<td>Indicates partial functionality or performance problems</td>\n</tr>\n</tbody></table>\n<p><strong>Query and Matching Constants</strong></p>\n<table>\n<thead>\n<tr>\n<th>Constant</th>\n<th>Description</th>\n<th>Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MatchEqual</code></td>\n<td>Exact string equality matching</td>\n<td>Standard label value filtering</td>\n</tr>\n<tr>\n<td><code>MatchNotEqual</code></td>\n<td>String inequality matching</td>\n<td>Exclusion-based label filtering</td>\n</tr>\n<tr>\n<td><code>MatchRegex</code></td>\n<td>Regular expression matching</td>\n<td>Pattern-based label filtering</td>\n</tr>\n<tr>\n<td><code>MatchNotRegex</code></td>\n<td>Negative regular expression matching</td>\n<td>Pattern-based label exclusion</td>\n</tr>\n</tbody></table>\n<h3 id=\"error-types-and-handling-categories\">Error Types and Handling Categories</h3>\n<p>The system categorizes errors to enable appropriate handling strategies and recovery mechanisms. Understanding error classifications helps in building robust error handling logic.</p>\n<p><strong>Error Classification System</strong></p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>Description</th>\n<th>Handling Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ErrorTypeTransient</code></td>\n<td>Temporary errors that may succeed on retry with backoff</td>\n<td>Implement exponential backoff retry logic</td>\n</tr>\n<tr>\n<td><code>ErrorTypePermanent</code></td>\n<td>Permanent errors that will not succeed on retry</td>\n<td>Log error and fail fast without retry attempts</td>\n</tr>\n<tr>\n<td><code>ErrorTypeRateLimit</code></td>\n<td>Rate limiting errors requiring backoff</td>\n<td>Implement longer backoff periods and reduce request rate</td>\n</tr>\n<tr>\n<td><code>ErrorTypeResource</code></td>\n<td>Resource exhaustion requiring different handling</td>\n<td>Implement circuit breakers and shed load</td>\n</tr>\n</tbody></table>\n<p><strong>Circuit Breaker States</strong></p>\n<table>\n<thead>\n<tr>\n<th>State</th>\n<th>Description</th>\n<th>Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>CircuitClosed</code></td>\n<td>Normal operation allowing all requests</td>\n<td>Monitor failure rate and transition to open on threshold</td>\n</tr>\n<tr>\n<td><code>CircuitOpen</code></td>\n<td>Failure protection mode rejecting all requests</td>\n<td>Block requests and transition to half-open after timeout</td>\n</tr>\n<tr>\n<td><code>CircuitHalfOpen</code></td>\n<td>Testing mode allowing limited requests to test recovery</td>\n<td>Allow single request to test if service has recovered</td>\n</tr>\n</tbody></table>\n<h3 id=\"alert-and-rule-management\">Alert and Rule Management</h3>\n<p>The alerting system introduces additional terminology for rule management, notification handling, and alert state tracking.</p>\n<p><strong>Alert State Management</strong></p>\n<table>\n<thead>\n<tr>\n<th>State</th>\n<th>Description</th>\n<th>Transitions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AlertStateInactive</code></td>\n<td>Alert rule condition is not currently met</td>\n<td>Transitions to Pending when condition becomes true</td>\n</tr>\n<tr>\n<td><code>AlertStatePending</code></td>\n<td>Condition is met but duration threshold not yet exceeded</td>\n<td>Transitions to Firing after duration or Inactive if condition clears</td>\n</tr>\n<tr>\n<td><code>AlertStateFiring</code></td>\n<td>Condition has been met for the required duration</td>\n<td>Transitions to Resolved when condition clears</td>\n</tr>\n<tr>\n<td><code>AlertStateResolved</code></td>\n<td>Previously firing alert where condition is no longer met</td>\n<td>Transitions to Inactive after notification sent</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for implementing the metrics collection system using the defined terminology and concepts.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP Server</strong></td>\n<td>Go net/http with ServeMux</td>\n<td>Gin or Echo framework with middleware</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td>YAML with gopkg.in/yaml.v3</td>\n<td>Viper with environment variable override</td>\n</tr>\n<tr>\n<td><strong>Logging</strong></td>\n<td>Standard log/slog package</td>\n<td>Structured logging with zerolog or logrus</td>\n</tr>\n<tr>\n<td><strong>Testing</strong></td>\n<td>Standard testing package</td>\n<td>Testify for assertions and table-driven tests</td>\n</tr>\n<tr>\n<td><strong>Benchmarking</strong></td>\n<td>Go built-in benchmarks</td>\n<td>Continuous benchmarking with benchstat</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure-organization\">File Structure Organization</h4>\n<p>The metrics collection system should be organized into clear modules that separate concerns and enable independent development of each component:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>metrics-system/\n  cmd/\n    metrics-server/\n      main.go                    ← Application entry point\n  internal/\n    config/\n      config.go                  ← Configuration loading and validation\n      defaults.go                ← Default constant definitions\n    metrics/\n      types.go                   ← Metric type definitions (Counter, Gauge, Histogram)\n      labels.go                  ← Label handling and validation\n      validator.go               ← Cardinality and label validation\n    scrape/\n      engine.go                  ← Main scrape engine coordination\n      target.go                  ← Target health and state management\n      discovery.go               ← Service discovery implementations\n      parser.go                  ← Metrics exposition format parsing\n    storage/\n      engine.go                  ← Storage engine coordination\n      compression.go             ← Gorilla compression implementation\n      wal.go                     ← Write-ahead log implementation\n      index.go                   ← Inverted index management\n      chunk.go                   ← Compressed chunk handling\n    query/\n      engine.go                  ← Query engine coordination\n      parser.go                  ← PromQL parsing and AST construction\n      selector.go                ← Label selector implementation\n      aggregator.go              ← Aggregation function implementation\n      executor.go                ← Range query execution\n    coordination/\n      pipeline.go                ← Pipeline coordination between components\n      system.go                  ← System-level coordination\n      health.go                  ← Health checking and status reporting\n    errors/\n      types.go                   ← Error type definitions\n      circuit.go                 ← Circuit breaker implementation\n  pkg/\n    client/                      ← Public client libraries (if needed)\n  test/\n    integration/                 ← End-to-end integration tests\n    testdata/                    ← Test fixtures and sample data\n  docs/\n    glossary.md                  ← This document</code></pre></div>\n\n<h4 id=\"core-infrastructure-implementation\">Core Infrastructure Implementation</h4>\n<p><strong>Logger Implementation Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> internal</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Logger provides structured logging with component identification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Logger</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLogger creates a structured logger for a specific component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    handler </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> slog.</span><span style=\"color:#B392F0\">NewJSONHandler</span><span style=\"color:#E1E4E8\">(os.Stdout, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerOptions</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Level: slog.LevelInfo,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> slog.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(handler).</span><span style=\"color:#B392F0\">With</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"component\"</span><span style=\"color:#E1E4E8\">, component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component: component,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:    logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Info logs informational messages with structured fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> ...</span><span style=\"color:#B392F0\">any</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(msg, fields</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error logs error messages with structured fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> ...</span><span style=\"color:#B392F0\">any</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.logger.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(msg, fields</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Debug logs debug messages with structured fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Debug</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">msg</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">fields</span><span style=\"color:#F97583\"> ...</span><span style=\"color:#B392F0\">any</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l.logger.</span><span style=\"color:#B392F0\">Debug</span><span style=\"color:#E1E4E8\">(msg, fields</span><span style=\"color:#F97583\">...</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WithContext adds trace context to logger if available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WithContext</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Extract trace ID from context if tracing is enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Add trace ID as a structured field to logger</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> l</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Configuration Loading Implementation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> config</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">gopkg.in/yaml.v3</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config represents the complete system configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Scrape  </span><span style=\"color:#B392F0\">ScrapeConfig</span><span style=\"color:#9ECBFF\">  `yaml:\"scrape\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query   </span><span style=\"color:#B392F0\">QueryConfig</span><span style=\"color:#9ECBFF\">   `yaml:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ScrapeConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScrapeInterval  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">  `yaml:\"scrape_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScrapeTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">  `yaml:\"scrape_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StaticConfigs   []</span><span style=\"color:#B392F0\">StaticConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"static_configs\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataDirectory     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"data_directory\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionPeriod   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"retention_period\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompressionEnabled </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">         `yaml:\"compression_enabled\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryTimeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"query_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxSeries        </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"max_series\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxRangeDuration </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"max_range_duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StaticConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Targets []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">          `yaml:\"targets\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Constants for default values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_SCRAPE_INTERVAL</span><span style=\"color:#F97583\">   =</span><span style=\"color:#79B8FF\"> 15</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_SCRAPE_TIMEOUT</span><span style=\"color:#F97583\">    =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_RETENTION_PERIOD</span><span style=\"color:#F97583\">  =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Hour</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEFAULT_QUERY_TIMEOUT</span><span style=\"color:#F97583\">     =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadFromFile reads configuration from YAML file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadFromFile</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">filename</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(filename)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"reading config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> yaml.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"parsing config YAML: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config.</span><span style=\"color:#B392F0\">SetDefaults</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetDefaults populates default values for unspecified configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetDefaults</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Scrape.ScrapeInterval </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Scrape.ScrapeInterval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DEFAULT_SCRAPE_INTERVAL</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Scrape.ScrapeTimeout </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Scrape.ScrapeTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DEFAULT_SCRAPE_TIMEOUT</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Storage.RetentionPeriod </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Storage.RetentionPeriod </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DEFAULT_RETENTION_PERIOD</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Query.QueryTimeout </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Query.QueryTimeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DEFAULT_QUERY_TIMEOUT</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Storage.DataDirectory </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Storage.DataDirectory </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"./data\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Query.MaxSeries </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        c.Query.MaxSeries </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeletons\">Core Logic Skeletons</h4>\n<p><strong>Health Status Enumeration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> health</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthStatus represents the operational state of system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthUp</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthDown</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthDegraded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> h {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthUp:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"UP\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthDown:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"DOWN\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthDegraded:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"DEGRADED\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"UNKNOWN\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Error Type System:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ErrorType categorizes errors for appropriate handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ErrorType</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeTransient</span><span style=\"color:#B392F0\"> ErrorType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypePermanent</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeRateLimit</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ErrorTypeResource</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricsError provides structured error information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricsError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#B392F0\">ErrorType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Component </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cause     </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"[</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">] </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, e.Component, e.Operation, e.Message, e.Cause)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Unwrap</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> e.Cause</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"testing-infrastructure\">Testing Infrastructure</h4>\n<p><strong>Mock HTTP Target for Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> testing</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http/httptest</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MockHTTPTarget provides controllable HTTP endpoint for testing scrape engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockHTTPTarget</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">httptest</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics   []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    delay     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errorRate </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMockHTTPTarget creates a new mock target with default behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMockHTTPTarget</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metrics:   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        delay:     </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errorRate: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target.server </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> httptest.</span><span style=\"color:#B392F0\">NewServer</span><span style=\"color:#E1E4E8\">(http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(target.handleMetrics))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> target</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// URL returns the HTTP URL of the mock target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">URL</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> m.server.URL </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"/metrics\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetMetrics configures the metrics exposed by this target</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(metrics))</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(m.metrics, metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetDelay sets artificial response delay for timeout testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetDelay</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">delay</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetErrorRate sets probability of HTTP 500 errors (0.0 to 1.0)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetErrorRate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">rate</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.errorRate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Close shuts down the mock HTTP server</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.server.</span><span style=\"color:#B392F0\">Close</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockHTTPTarget</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">handleMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    delay </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errorRate </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> m.errorRate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metrics </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(m.metrics))</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(metrics, m.metrics)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simulate delay if configured</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> delay </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time.</span><span style=\"color:#B392F0\">Sleep</span><span style=\"color:#E1E4E8\">(delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Simulate errors if configured</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> errorRate </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> &#x26;&#x26;</span><span style=\"color:#E1E4E8\"> rand.</span><span style=\"color:#B392F0\">Float64</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> errorRate {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        http.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"Simulated server error\"</span><span style=\"color:#E1E4E8\">, http.StatusInternalServerError)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"text/plain\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusOK)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write metrics in Prometheus exposition format</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, metric </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> metrics {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Fprintf</span><span style=\"color:#E1E4E8\">(w, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, metric)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Milestone 1 - Metrics Data Model Checkpoint:</strong>\nAfter implementing the metrics data model, verify functionality with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/metrics/...</span></span></code></pre></div>\n\n<p>Expected behavior:</p>\n<ul>\n<li>Counter values increase monotonically and reject negative increments</li>\n<li>Gauge values can be set to any value and read back correctly</li>\n<li>Histogram observations are recorded in appropriate buckets</li>\n<li>Label validation rejects high-cardinality combinations</li>\n<li>Metric metadata is stored and retrievable</li>\n</ul>\n<p><strong>Milestone 2 - Scrape Engine Checkpoint:</strong>\nAfter implementing the scrape engine, test with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/metrics-server/main.go</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:9090/targets</span><span style=\"color:#6A737D\">  # Should show discovered targets</span></span></code></pre></div>\n\n<p>Expected behavior:</p>\n<ul>\n<li>Targets are discovered from configuration</li>\n<li>HTTP scrapes retrieve metrics successfully</li>\n<li>Target health reflects scrape success/failure</li>\n<li>Malformed metrics are rejected without affecting valid ones</li>\n</ul>\n<p><strong>Milestone 3 - Storage Engine Checkpoint:</strong>\nAfter implementing storage, verify with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/storage/...</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check data directory contains WAL and chunk files</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">ls</span><span style=\"color:#79B8FF\"> -la</span><span style=\"color:#9ECBFF\"> ./data/</span></span></code></pre></div>\n\n<p>Expected behavior:</p>\n<ul>\n<li>Samples are compressed using Gorilla algorithm</li>\n<li>WAL provides durability for writes</li>\n<li>Indexes enable fast series lookup</li>\n<li>Old data is deleted according to retention policy</li>\n</ul>\n<p><strong>Milestone 4 - Query Engine Checkpoint:</strong>\nAfter implementing the query engine, test with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> \"http://localhost:9090/api/v1/query?query=up\"</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> \"http://localhost:9090/api/v1/query_range?query=rate(requests_total[5m])&#x26;start=1609459200&#x26;end=1609462800&#x26;step=60\"</span></span></code></pre></div>\n\n<p>Expected behavior:</p>\n<ul>\n<li>PromQL expressions parse correctly</li>\n<li>Label selectors filter time series appropriately</li>\n<li>Aggregation functions produce correct results</li>\n<li>Range queries return properly interpolated data points</li>\n</ul>\n","toc":[{"level":1,"text":"Prometheus-Like Metrics Collection System: Design Document","id":"prometheus-like-metrics-collection-system-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"The Observatory Mental Model","id":"the-observatory-mental-model"},{"level":3,"text":"Existing Solutions Analysis","id":"existing-solutions-analysis"},{"level":3,"text":"Core Technical Challenges","id":"core-technical-challenges"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"The Observatory Charter Mental Model","id":"the-observatory-charter-mental-model"},{"level":3,"text":"Functional Requirements","id":"functional-requirements"},{"level":3,"text":"Quality Attributes","id":"quality-attributes"},{"level":3,"text":"Scope Limitations","id":"scope-limitations"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Responsibilities","id":"component-responsibilities"},{"level":4,"text":"Scrape Engine Component","id":"scrape-engine-component"},{"level":4,"text":"Time Series Storage Component","id":"time-series-storage-component"},{"level":4,"text":"Query Engine Component","id":"query-engine-component"},{"level":4,"text":"HTTP API Server Component","id":"http-api-server-component"},{"level":3,"text":"Data Flow Overview","id":"data-flow-overview"},{"level":4,"text":"Metrics Collection Flow","id":"metrics-collection-flow"},{"level":4,"text":"Query Processing Flow","id":"query-processing-flow"},{"level":3,"text":"Deployment Architecture","id":"deployment-architecture"},{"level":4,"text":"Single-Instance Deployment","id":"single-instance-deployment"},{"level":4,"text":"Configuration Management","id":"configuration-management"},{"level":4,"text":"Process Management and Health Monitoring","id":"process-management-and-health-monitoring"},{"level":4,"text":"Resource Requirements and Capacity Planning","id":"resource-requirements-and-capacity-planning"},{"level":4,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Metrics Data Model","id":"metrics-data-model"},{"level":3,"text":"Metric Type Semantics","id":"metric-type-semantics"},{"level":4,"text":"Counter Semantics and Behavior","id":"counter-semantics-and-behavior"},{"level":4,"text":"Gauge Semantics and Flexibility","id":"gauge-semantics-and-flexibility"},{"level":4,"text":"Histogram Design and Bucket Strategy","id":"histogram-design-and-bucket-strategy"},{"level":4,"text":"Summary Metrics and Client-Side Quantiles","id":"summary-metrics-and-client-side-quantiles"},{"level":3,"text":"Multi-Dimensional Labeling","id":"multi-dimensional-labeling"},{"level":4,"text":"Label Structure and Naming Conventions","id":"label-structure-and-naming-conventions"},{"level":4,"text":"Cardinality Mathematics and Memory Impact","id":"cardinality-mathematics-and-memory-impact"},{"level":4,"text":"Label Best Practices and Anti-Patterns","id":"label-best-practices-and-anti-patterns"},{"level":3,"text":"Time Series Identity","id":"time-series-identity"},{"level":4,"text":"Identity Composition and Uniqueness","id":"identity-composition-and-uniqueness"},{"level":4,"text":"Identity Lifecycle and Creation","id":"identity-lifecycle-and-creation"},{"level":4,"text":"Identity Normalization and Canonical Form","id":"identity-normalization-and-canonical-form"},{"level":3,"text":"Cardinality Control","id":"cardinality-control"},{"level":4,"text":"Cardinality Explosion Detection","id":"cardinality-explosion-detection"},{"level":4,"text":"Enforcement Strategies and Policies","id":"enforcement-strategies-and-policies"},{"level":4,"text":"Label Value Validation and Sanitization","id":"label-value-validation-and-sanitization"},{"level":4,"text":"Memory Management and Series Eviction","id":"memory-management-and-series-eviction"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Metric Type Infrastructure","id":"core-metric-type-infrastructure"},{"level":4,"text":"Label Management and Validation","id":"label-management-and-validation"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Scrape Engine Design","id":"scrape-engine-design"},{"level":3,"text":"The Observatory Network Mental Model","id":"the-observatory-network-mental-model"},{"level":3,"text":"Target Discovery","id":"target-discovery"},{"level":3,"text":"Scrape Scheduling","id":"scrape-scheduling"},{"level":3,"text":"Metrics Parsing","id":"metrics-parsing"},{"level":3,"text":"Health and Error Handling","id":"health-and-error-handling"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeletons","id":"core-logic-skeletons"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Time Series Storage Engine","id":"time-series-storage-engine"},{"level":3,"text":"Library Archive Mental Model","id":"library-archive-mental-model"},{"level":3,"text":"Gorilla Compression","id":"gorilla-compression"},{"level":4,"text":"Delta-of-Delta Timestamp Encoding","id":"delta-of-delta-timestamp-encoding"},{"level":4,"text":"XOR Value Encoding","id":"xor-value-encoding"},{"level":4,"text":"Compression Implementation Strategy","id":"compression-implementation-strategy"},{"level":3,"text":"Series Indexing","id":"series-indexing"},{"level":4,"text":"Primary Metric Index","id":"primary-metric-index"},{"level":4,"text":"Label Value Indexes","id":"label-value-indexes"},{"level":4,"text":"Index Intersection Algorithms","id":"index-intersection-algorithms"},{"level":4,"text":"Cardinality Management","id":"cardinality-management"},{"level":3,"text":"Data Lifecycle Management","id":"data-lifecycle-management"},{"level":4,"text":"Retention Policy Configuration","id":"retention-policy-configuration"},{"level":4,"text":"Automated Downsampling Process","id":"automated-downsampling-process"},{"level":4,"text":"Garbage Collection and Compaction","id":"garbage-collection-and-compaction"},{"level":3,"text":"Persistence and Recovery","id":"persistence-and-recovery"},{"level":4,"text":"Write-Ahead Log Structure","id":"write-ahead-log-structure"},{"level":4,"text":"Checkpoint and Recovery Process","id":"checkpoint-and-recovery-process"},{"level":4,"text":"Failure Scenarios and Recovery","id":"failure-scenarios-and-recovery"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Storage Infrastructure Code","id":"storage-infrastructure-code"},{"level":4,"text":"Core Storage Logic Skeletons","id":"core-storage-logic-skeletons"},{"level":4,"text":"Gorilla Compression Implementation Skeleton","id":"gorilla-compression-implementation-skeleton"},{"level":4,"text":"WAL and Recovery Implementation Skeleton","id":"wal-and-recovery-implementation-skeleton"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"PromQL Query Engine","id":"promql-query-engine"},{"level":3,"text":"SQL for Time Series Mental Model: Understanding PromQL through database query analogies","id":"sql-for-time-series-mental-model-understanding-promql-through-database-query-analogies"},{"level":3,"text":"Query Parsing: Lexical analysis and AST construction for PromQL expressions","id":"query-parsing-lexical-analysis-and-ast-construction-for-promql-expressions"},{"level":3,"text":"Label Selector Engine: Exact, regex, and inequality matching for filtering time series","id":"label-selector-engine-exact-regex-and-inequality-matching-for-filtering-time-series"},{"level":3,"text":"Aggregation Operations: Sum, average, percentile, and grouping operations across label dimensions","id":"aggregation-operations-sum-average-percentile-and-grouping-operations-across-label-dimensions"},{"level":3,"text":"Range Query Execution: Retrieving and interpolating data points across time windows","id":"range-query-execution-retrieving-and-interpolating-data-points-across-time-windows"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Component Interactions and Data Flow","id":"component-interactions-and-data-flow"},{"level":3,"text":"Ingestion Pipeline: Flow from Scraped Metrics to Indexed Storage","id":"ingestion-pipeline-flow-from-scraped-metrics-to-indexed-storage"},{"level":4,"text":"Scrape Collection Phase","id":"scrape-collection-phase"},{"level":4,"text":"Storage Ingestion Phase","id":"storage-ingestion-phase"},{"level":4,"text":"Error Handling and Backpressure","id":"error-handling-and-backpressure"},{"level":3,"text":"Query Processing Pipeline: Steps from PromQL Input to Aggregated Results","id":"query-processing-pipeline-steps-from-promql-input-to-aggregated-results"},{"level":4,"text":"PromQL Parsing and Planning Phase","id":"promql-parsing-and-planning-phase"},{"level":4,"text":"Series Selection and Filtering Phase","id":"series-selection-and-filtering-phase"},{"level":4,"text":"Data Retrieval and Aggregation Phase","id":"data-retrieval-and-aggregation-phase"},{"level":3,"text":"Concurrency Control: Managing Concurrent Reads, Writes, and Background Operations","id":"concurrency-control-managing-concurrent-reads-writes-and-background-operations"},{"level":4,"text":"Read-Write Concurrency Model","id":"read-write-concurrency-model"},{"level":4,"text":"Write Coordination and Batching","id":"write-coordination-and-batching"},{"level":4,"text":"Background Operations Coordination","id":"background-operations-coordination"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Target Unavailability","id":"target-unavailability"},{"level":4,"text":"Network Timeout Handling","id":"network-timeout-handling"},{"level":4,"text":"HTTP Error Response Handling","id":"http-error-response-handling"},{"level":4,"text":"Malformed Metrics Handling","id":"malformed-metrics-handling"},{"level":4,"text":"Circuit Breaker Implementation","id":"circuit-breaker-implementation"},{"level":3,"text":"Storage Errors","id":"storage-errors"},{"level":4,"text":"Disk Space Exhaustion","id":"disk-space-exhaustion"},{"level":4,"text":"Write-Ahead Log Corruption Recovery","id":"write-ahead-log-corruption-recovery"},{"level":4,"text":"Index Inconsistency Recovery","id":"index-inconsistency-recovery"},{"level":4,"text":"Data Consistency Validation","id":"data-consistency-validation"},{"level":3,"text":"Query Errors","id":"query-errors"},{"level":4,"text":"Invalid Expression Parsing","id":"invalid-expression-parsing"},{"level":4,"text":"Missing Data Handling","id":"missing-data-handling"},{"level":4,"text":"Resource Exhaustion Protection","id":"resource-exhaustion-protection"},{"level":3,"text":"Resource Protection","id":"resource-protection"},{"level":4,"text":"Memory Management Strategy","id":"memory-management-strategy"},{"level":4,"text":"Cardinality Explosion Prevention","id":"cardinality-explosion-prevention"},{"level":4,"text":"Graceful Degradation Mechanisms","id":"graceful-degradation-mechanisms"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure","id":"file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Testing and Validation Strategy","id":"testing-and-validation-strategy"},{"level":3,"text":"The Quality Assurance Lighthouse Mental Model","id":"the-quality-assurance-lighthouse-mental-model"},{"level":3,"text":"Component Testing","id":"component-testing"},{"level":4,"text":"Metrics Data Model Testing","id":"metrics-data-model-testing"},{"level":4,"text":"Scrape Engine Testing","id":"scrape-engine-testing"},{"level":4,"text":"Time Series Storage Testing","id":"time-series-storage-testing"},{"level":4,"text":"Query Engine Testing","id":"query-engine-testing"},{"level":3,"text":"End-to-End Testing","id":"end-to-end-testing"},{"level":4,"text":"Complete Scrape-to-Query Workflows","id":"complete-scrape-to-query-workflows"},{"level":4,"text":"Multi-Component Integration Scenarios","id":"multi-component-integration-scenarios"},{"level":4,"text":"Data Consistency and Accuracy Validation","id":"data-consistency-and-accuracy-validation"},{"level":3,"text":"Performance Validation","id":"performance-validation"},{"level":4,"text":"Load Testing and Capacity Planning","id":"load-testing-and-capacity-planning"},{"level":4,"text":"Bottleneck Identification and Resource Monitoring","id":"bottleneck-identification-and-resource-monitoring"},{"level":4,"text":"Scalability Requirements Verification","id":"scalability-requirements-verification"},{"level":3,"text":"Milestone Verification","id":"milestone-verification"},{"level":4,"text":"Milestone 1: Metrics Data Model Verification","id":"milestone-1-metrics-data-model-verification"},{"level":4,"text":"Milestone 2: Scrape Engine Verification","id":"milestone-2-scrape-engine-verification"},{"level":4,"text":"Milestone 3: Time Series Storage Verification","id":"milestone-3-time-series-storage-verification"},{"level":4,"text":"Milestone 4: Query Engine Verification","id":"milestone-4-query-engine-verification"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Testing Infrastructure Starter Code","id":"testing-infrastructure-starter-code"},{"level":4,"text":"Core Testing Logic Skeletons","id":"core-testing-logic-skeletons"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Symptom-Based Troubleshooting","id":"symptom-based-troubleshooting"},{"level":4,"text":"High-Level System Symptoms","id":"high-level-system-symptoms"},{"level":4,"text":"Scraping Engine Symptoms","id":"scraping-engine-symptoms"},{"level":4,"text":"Storage Engine Symptoms","id":"storage-engine-symptoms"},{"level":4,"text":"Query Engine Symptoms","id":"query-engine-symptoms"},{"level":4,"text":"Performance and Scalability Symptoms","id":"performance-and-scalability-symptoms"},{"level":3,"text":"Debugging Tools and Techniques","id":"debugging-tools-and-techniques"},{"level":4,"text":"Logging Infrastructure","id":"logging-infrastructure"},{"level":4,"text":"System Metrics and Observability","id":"system-metrics-and-observability"},{"level":4,"text":"Interactive Debugging Tools","id":"interactive-debugging-tools"},{"level":4,"text":"Distributed Tracing Integration","id":"distributed-tracing-integration"},{"level":3,"text":"Implementation Pitfalls","id":"implementation-pitfalls"},{"level":4,"text":"Metrics Data Model Pitfalls","id":"metrics-data-model-pitfalls"},{"level":4,"text":"Scraping Engine Pitfalls","id":"scraping-engine-pitfalls"},{"level":4,"text":"Storage Engine Pitfalls","id":"storage-engine-pitfalls"},{"level":4,"text":"Query Engine Pitfalls","id":"query-engine-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Language-Specific Debugging Hints","id":"language-specific-debugging-hints"},{"level":2,"text":"Future Extensions and Scalability","id":"future-extensions-and-scalability"},{"level":3,"text":"Alerting System","id":"alerting-system"},{"level":4,"text":"Rule Evaluation Engine","id":"rule-evaluation-engine"},{"level":4,"text":"Notification Manager","id":"notification-manager"},{"level":4,"text":"Alert State Management","id":"alert-state-management"},{"level":3,"text":"Multi-Instance Federation","id":"multi-instance-federation"},{"level":4,"text":"Hierarchical Federation Model","id":"hierarchical-federation-model"},{"level":4,"text":"Cross-Instance Querying","id":"cross-instance-querying"},{"level":4,"text":"Global View Consistency","id":"global-view-consistency"},{"level":3,"text":"Advanced Query Features","id":"advanced-query-features"},{"level":4,"text":"Recording Rules","id":"recording-rules"},{"level":4,"text":"Query Optimization Engine","id":"query-optimization-engine"},{"level":4,"text":"Advanced Aggregation Functions","id":"advanced-aggregation-functions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Core Concepts and Terminology","id":"core-concepts-and-terminology"},{"level":3,"text":"System Architecture and Component Terms","id":"system-architecture-and-component-terms"},{"level":3,"text":"Testing and Development Terms","id":"testing-and-development-terms"},{"level":3,"text":"Advanced Features and Extensions","id":"advanced-features-and-extensions"},{"level":3,"text":"Data Types and Structures","id":"data-types-and-structures"},{"level":3,"text":"Constants and Configuration Values","id":"constants-and-configuration-values"},{"level":3,"text":"Error Types and Handling Categories","id":"error-types-and-handling-categories"},{"level":3,"text":"Alert and Rule Management","id":"alert-and-rule-management"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure Organization","id":"file-structure-organization"},{"level":4,"text":"Core Infrastructure Implementation","id":"core-infrastructure-implementation"},{"level":4,"text":"Core Logic Skeletons","id":"core-logic-skeletons"},{"level":4,"text":"Testing Infrastructure","id":"testing-infrastructure"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"}],"title":"Prometheus-Like Metrics Collection System: Design Document","markdown":"# Prometheus-Like Metrics Collection System: Design Document\n\n\n## Overview\n\nA distributed metrics collection system that scrapes time-series data from service endpoints, stores it with efficient compression, and provides a query engine for monitoring and alerting. The key architectural challenge is handling high-cardinality labeled metrics at scale while maintaining low latency for queries and minimal storage overhead.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** This section establishes the foundation for Milestones 1-4 by explaining why we need a metrics collection system and what technical challenges it must solve.\n\nModern distributed systems have transformed from simple monolithic applications into complex networks of microservices, containers, and cloud-native components. This architectural evolution has created an unprecedented need for comprehensive observability — the ability to understand system behavior, performance, and health from the outside by examining the data it emits. At the heart of observability lies metrics collection: the systematic gathering, storage, and analysis of quantitative measurements that tell the story of how our systems are performing over time.\n\nThe challenge we face is not merely collecting numbers from our applications. Any system can log metrics to files or send them to a database. The real complexity emerges when we consider the scale, velocity, and dimensional richness of modern metrics data. A typical microservices deployment might generate millions of time series, each representing a unique combination of metric name and label values, with new data points arriving every few seconds. These metrics must be stored efficiently, queried quickly, and remain available even as the underlying infrastructure scales and evolves.\n\nExisting monitoring solutions often fall short in one or more critical areas: they either cannot handle the scale of modern systems, lack the flexibility to model complex multi-dimensional data, or require architectural compromises that limit their effectiveness. The goal of this project is to build a Prometheus-like metrics collection system that addresses these shortcomings while providing the foundation for robust observability in distributed environments.\n\n### The Observatory Mental Model\n\nTo understand the architecture and challenges of a metrics collection system, consider the analogy of a **weather monitoring network**. Imagine you are tasked with building a system to monitor weather conditions across an entire continent. You need to track temperature, humidity, wind speed, and atmospheric pressure at thousands of locations, with measurements taken every few minutes, 24 hours a day.\n\nYour weather monitoring network would require several key components, each with specific responsibilities and challenges:\n\n**Weather Stations (Metrics Sources):** Thousands of automated weather stations scattered across the continent continuously measure environmental conditions. Each station has a unique identifier (location) and can measure multiple phenomena simultaneously. The stations must be robust, autonomous, and capable of operating in diverse conditions. In our metrics system, these weather stations correspond to **application instances, services, and infrastructure components** that expose metrics through HTTP endpoints.\n\n**Data Collectors (Scrape Engine):** Rather than having each weather station independently transmit its data, you deploy a network of data collectors that periodically visit stations to retrieve measurements. This pull-based approach offers several advantages: collectors can detect when stations are offline, they can apply consistent formatting and validation, and they can handle network issues gracefully. In our system, the **scrape engine** acts as these data collectors, periodically fetching metrics from configured targets.\n\n**Central Archives (Time Series Storage):** All collected measurements must be stored in a central archive system that can handle the massive volume of time-stamped data points. The archive must compress data efficiently (weather measurements follow predictable patterns), provide fast access for analysis, and automatically manage data retention as storage fills up. Our **time series storage engine** serves this archival function, using specialized compression techniques designed for temporal data.\n\n**Research Interface (Query Engine):** Scientists and analysts need to query the archived weather data to identify patterns, trends, and anomalies. They might ask questions like \"What was the average temperature in the Pacific Northwest during July?\" or \"Show me all locations where humidity exceeded 90% in the past week.\" The query interface must support complex filtering, aggregation, and time-based operations. Our **PromQL query engine** provides this analytical capability for metrics data.\n\nThis mental model illuminates several key insights about metrics collection systems:\n\n**Pull vs Push Architecture:** Just as weather stations don't individually transmit data to every interested party, metrics systems benefit from a pull-based architecture where a central collector retrieves data from sources. This approach provides better failure detection, reduces network overhead, and enables consistent data formatting.\n\n**Dimensional Data Challenges:** Weather measurements aren't just numbers — they have dimensions (location, altitude, measurement type, instrument ID). Similarly, modern metrics are multi-dimensional, with labels that provide rich context. Managing the combinatorial explosion of unique dimension combinations becomes a critical scalability challenge.\n\n**Time Series Characteristics:** Weather data exhibits temporal patterns that enable efficient compression — temperatures change gradually, measurements are taken at regular intervals, and similar conditions tend to cluster in time. Metrics data shares these characteristics, allowing specialized storage techniques that wouldn't work for general-purpose databases.\n\n**Query Pattern Optimization:** Weather researchers typically ask time-based questions about recent data, with occasional historical analysis. Metrics queries follow similar patterns — most focus on recent time windows, with aggregation across multiple dimensions. Storage and indexing strategies can optimize for these access patterns.\n\nThe weather monitoring analogy also reveals why building an effective metrics collection system is challenging. The scale is enormous (potentially millions of time series), the data arrives continuously, storage efficiency is crucial, and query performance must remain fast even as the dataset grows. These constraints shape every architectural decision in our system.\n\n### Existing Solutions Analysis\n\nThe metrics collection landscape includes several established approaches, each with distinct architectural philosophies and trade-offs. Understanding these existing solutions helps clarify the design space and motivates our architectural choices.\n\n**Push-Based Systems (StatsD, Graphite, InfluxDB):**\n\nPush-based systems follow a model where application instances actively send their metrics to a central collector. Applications use client libraries to emit metrics via UDP packets (StatsD) or HTTP requests, and a central aggregation service receives, processes, and stores the data.\n\n| Aspect | Advantages | Disadvantages |\n|--------|------------|---------------|\n| Network Overhead | Applications control transmission timing | High network traffic from many sources |\n| Failure Detection | Immediate feedback on transmission errors | Cannot distinguish between application down vs network issues |\n| Configuration | No need to configure scrape targets centrally | Each application must know collector endpoints |\n| Scaling | Horizontal scaling of collectors is straightforward | Network storms during traffic spikes |\n| Service Discovery | Applications handle their own endpoint resolution | Difficult to ensure all services are monitored |\n\nThe fundamental challenge with push-based systems is **observability of the monitoring system itself**. When metrics stop arriving, it's difficult to determine whether the application has failed, the network path is broken, or the application simply has no activity to report. This ambiguity complicates alerting and incident response.\n\n**Pull-Based Systems (Prometheus, DataDog Agent):**\n\nPull-based systems invert the responsibility: a central collector actively retrieves metrics from application endpoints. Applications expose metrics via HTTP endpoints in a standardized format, and the collector periodically scrapes these endpoints according to a configured schedule.\n\n| Aspect | Advantages | Disadvantages |\n|--------|------------|---------------|\n| Failure Detection | Clear distinction between target down vs no data | Applications must maintain HTTP endpoints |\n| Network Control | Collector controls scrape timing and concurrency | Requires network reachability from collector to targets |\n| Configuration | Centralized target configuration with service discovery | Initial setup complexity for service discovery |\n| Debugging | Easy to manually inspect target endpoints | Firewall and network policy complexity |\n| Consistency | Uniform scrape intervals and timeout handling | Potential polling overhead for idle applications |\n\n**Hybrid Approaches (Prometheus Push Gateway, Vector):**\n\nSome systems attempt to combine both approaches, typically by providing a push-to-pull bridge. Applications push metrics to an intermediate gateway, which exposes them via pull endpoints for the main collector.\n\n> **Key Insight:** The choice between push and pull fundamentally affects system observability, failure modes, and operational complexity. Pull-based systems provide better visibility into the health of the monitoring system itself, at the cost of additional networking complexity.\n\n**Architectural Decision Record:**\n\n> **Decision: Pull-Based Scraping Architecture**\n> - **Context**: Need to choose between push-based (applications send metrics) vs pull-based (collector retrieves metrics) architecture for our metrics collection system\n> - **Options Considered**: \n>   1. Pure push-based with UDP/HTTP transmission from applications\n>   2. Pull-based scraping with HTTP endpoints on applications  \n>   3. Hybrid push-to-pull gateway model\n> - **Decision**: Pull-based scraping with HTTP endpoints\n> - **Rationale**: Pull-based systems provide superior observability of the monitoring infrastructure itself. When scraping fails, we know definitively that a target is unreachable or unresponsive, enabling precise alerting. Centralized configuration simplifies service discovery integration and reduces per-application configuration burden. The HTTP endpoint model also enables manual debugging and testing of individual services.\n> - **Consequences**: Applications must implement HTTP metrics endpoints and handle scraping load. Network policies must allow collector-to-target connectivity. However, we gain clear failure attribution, centralized configuration management, and better debugging capabilities.\n\n**Storage Engine Comparison:**\n\nDifferent metrics systems employ varying storage strategies, each optimized for different trade-offs:\n\n| System | Storage Approach | Compression | Query Performance | Operational Complexity |\n|--------|------------------|-------------|-------------------|----------------------|\n| Graphite | Whisper files (RRD-style) | Fixed retention buckets | Fast for pre-aggregated data | Medium - file system management |\n| InfluxDB | Custom time series engine | Snappy + series compression | Variable based on cardinality | High - clustering and consistency |\n| Prometheus | Block-based with Gorilla compression | Delta-of-delta + XOR | Excellent for recent data | Low - single node simplicity |\n| VictoriaMetrics | Compressed column storage | Multiple algorithms | Optimized for high cardinality | Medium - configuration complexity |\n\nOur system will adopt Prometheus's approach of **block-based storage with Gorilla compression** because it provides excellent compression ratios for typical metrics workloads while maintaining query performance for the most common use case: analyzing recent data.\n\n### Core Technical Challenges\n\nBuilding an effective metrics collection system requires solving several interconnected technical challenges. Each challenge involves fundamental trade-offs that shape the system's architecture and performance characteristics.\n\n**Scale and Throughput Management:**\n\nModern distributed systems can generate enormous volumes of metrics data. Consider a microservices deployment with 100 services, each running 10 instances, exposing 50 metrics each, scraped every 15 seconds. This generates 100 × 10 × 50 × (3600/15) = 1.2 million data points per hour, or approximately 330 samples per second just for the base metrics. Real deployments often exceed this by orders of magnitude when including infrastructure metrics, custom application metrics, and higher scrape frequencies.\n\nThe scale challenge manifests in several dimensions:\n\n**Ingestion Rate:** The system must sustainably ingest hundreds of thousands to millions of samples per second without dropping data or introducing excessive latency. This requires efficient parsing, concurrent processing, and careful memory management to avoid garbage collection pressure.\n\n**Storage Volume:** With retention periods measured in weeks or months, the total storage requirements can reach terabytes. Each sample consists of a timestamp (8 bytes), a float64 value (8 bytes), plus the overhead of series identification and indexing. Without compression, a billion samples would require at least 16 GB of raw storage.\n\n**Query Latency:** Despite the massive data volumes, queries must complete within seconds to support interactive dashboards and real-time alerting. This requires intelligent indexing, data locality optimization, and query execution strategies that minimize disk I/O.\n\n**Cardinality Explosion Problem:**\n\nThe **cardinality** of a metrics system refers to the number of unique time series, where each series is defined by a unique combination of metric name and label values. This represents perhaps the most insidious scaling challenge in metrics systems.\n\nConsider a simple HTTP request counter with labels for method, status code, and endpoint:\n\n```\nhttp_requests_total{method=\"GET\", status=\"200\", endpoint=\"/api/users\"}\nhttp_requests_total{method=\"POST\", status=\"201\", endpoint=\"/api/users\"}\nhttp_requests_total{method=\"GET\", status=\"404\", endpoint=\"/api/orders\"}\n```\n\nIf your system has 10 HTTP methods, 20 possible status codes, and 100 endpoints, the potential cardinality is 10 × 20 × 100 = 20,000 unique time series for this single metric. Add labels for instance ID, deployment version, and geographic region, and the cardinality explodes exponentially.\n\nHigh cardinality creates multiple problems:\n\n| Problem Area | Impact | Root Cause |\n|--------------|--------|------------|\n| Memory Usage | Each series requires indexing structures in RAM | Series metadata and recent samples kept in memory |\n| Query Performance | Label matching becomes expensive | Must scan large label indexes for selector evaluation |\n| Storage Overhead | Small series create inefficient storage chunks | Compression algorithms work poorly on short time series |\n| Ingestion Latency | New series creation becomes a bottleneck | Index updates and memory allocation under write load |\n\n**Storage Efficiency and Compression:**\n\nTime series data exhibits characteristics that enable sophisticated compression, but achieving optimal compression ratios requires careful algorithm selection and implementation. The challenge is balancing compression effectiveness with query performance and computational overhead.\n\n**Timestamp Compression:** Metrics are typically collected at regular intervals, creating predictable timestamp patterns. The Gorilla compression algorithm exploits this by storing timestamps as delta-of-deltas: instead of storing absolute timestamps, it stores the difference between consecutive timestamp differences. For regular scrape intervals, this often compresses timestamps to just a few bits per sample.\n\n**Value Compression:** Metric values often change gradually or remain constant for extended periods. Gorilla compression uses XOR-based encoding where each value is XORed with its predecessor, and only the differing bits are stored. When values are stable, this can compress 64-bit floats to just a few bits.\n\nHowever, compression introduces complexity:\n\n**Write Amplification:** Compressed blocks must be periodically finalized and written to disk, creating bursty I/O patterns that can interfere with query performance.\n\n**Query Overhead:** Reading compressed data requires decompression, adding CPU overhead to query execution. The system must balance compression ratios against query latency requirements.\n\n**Memory Pressure:** Compression algorithms maintain state for active series, and this metadata can consume significant memory in high-cardinality environments.\n\n**Query Performance Under Load:**\n\nThe query engine must support complex analytical operations across massive datasets while maintaining interactive response times. This challenge is complicated by the multi-dimensional nature of metrics data and the variety of query patterns users employ.\n\n**Range Query Scalability:** Range queries that retrieve data across long time windows or high-cardinality label combinations can potentially scan terabytes of data. The query engine must use indexing, pruning, and parallel execution strategies to make such queries feasible.\n\n**Aggregation Efficiency:** PromQL queries often perform aggregation operations (sum, average, percentile) across thousands of time series. These operations require careful memory management and algorithmic optimization to avoid excessive memory usage or computation time.\n\n**Concurrent Query Load:** Production metrics systems serve multiple concurrent users running dashboards, alerts, and ad-hoc queries. The system must manage resource allocation to prevent expensive queries from interfering with critical alerting queries.\n\n**Index Maintenance:** As new series are created and old series expire, the label indexes that enable fast series selection must be updated consistently without blocking ongoing queries.\n\n> **Critical Design Principle:** Every architectural decision in our metrics system must consider its impact on cardinality scaling. Features that seem innocent in small deployments can become system-breaking bottlenecks when cardinality grows from thousands to millions of series.\n\n**Real-Time vs Historical Query Patterns:**\n\nMetrics queries exhibit distinct patterns that the system can optimize for:\n\n**Hot Data Access:** Most queries focus on recent data (last few hours to days), which should be kept in memory or fast storage for immediate access.\n\n**Cold Data Access:** Historical queries are less frequent but often span longer time ranges, requiring different optimization strategies focused on I/O efficiency rather than memory access.\n\n**Alert Query Priority:** Alerting queries must complete quickly and reliably, potentially requiring resource reservation or priority queuing mechanisms.\n\n**Dashboard Query Batching:** Dashboard refreshes often trigger multiple related queries that could benefit from shared computation or caching strategies.\n\nUnderstanding these challenges provides the foundation for our architectural decisions. Each component of our metrics collection system — the data model, scrape engine, storage layer, and query engine — must be designed to address these fundamental scaling and performance constraints.\n\n⚠️ **Pitfall: Ignoring Cardinality Early**\nMany developers building metrics systems focus initially on throughput and storage optimization while treating cardinality as a later concern. This approach leads to systems that work well in testing but fail catastrophically in production when label combinations proliferate. From the initial design phase, every data structure and algorithm must account for high-cardinality scenarios. For example, using hash maps keyed by series labels seems efficient until you have millions of series, at which point the hash map itself becomes a memory bottleneck and garbage collection issue.\n\n⚠️ **Pitfall: Optimizing for Average Case**\nMetrics systems exhibit highly variable load patterns — scraping creates periodic ingestion bursts, queries arrive in waves during incident response, and certain time periods (deployments, traffic spikes) generate disproportionate data volumes. Designing for average-case performance results in systems that become unresponsive precisely when reliability is most critical. Instead, all components must be designed to handle 99th percentile loads gracefully, with explicit backpressure and resource protection mechanisms.\n\n### Implementation Guidance\n\nBuilding a metrics collection system requires careful technology selection and project organization. This guidance helps you make practical implementation decisions and avoid common pitfalls.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|-----------------|-----------|\n| HTTP Server | `net/http` with standard mux | `gorilla/mux` or `gin` | Standard library sufficient for metrics endpoints |\n| Time Series Storage | File-based chunks with `os.File` | Embedded key-value store like `bbolt` | File system provides simple persistence model |\n| Compression | Basic delta encoding | Full Gorilla algorithm | Start simple, optimize when cardinality grows |\n| Service Discovery | Static YAML configuration | Kubernetes API integration | Static config easier to debug and understand |\n| Parsing | Custom text parser | `prometheus/common/expfmt` | Custom parser teaches format understanding |\n| Concurrency | `goroutines` with channels | Worker pool patterns | Go's concurrency primitives handle most cases |\n\n**Recommended Project Structure:**\n\nUnderstanding how to organize your codebase prevents the common mistake of creating a monolithic main.go file that becomes unmaintainable:\n\n```\nmetrics-system/\n├── cmd/\n│   ├── collector/           ← Main collector binary\n│   │   └── main.go\n│   └── query/              ← Query API server\n│       └── main.go\n├── internal/\n│   ├── model/              ← Metrics data model (Milestone 1)\n│   │   ├── metric.go       ← Counter, Gauge, Histogram types\n│   │   ├── labels.go       ← Label handling and validation\n│   │   ├── sample.go       ← Sample and timestamp types\n│   │   └── metadata.go     ← Metric metadata storage\n│   ├── scrape/             ← Scrape engine (Milestone 2)\n│   │   ├── discovery.go    ← Target discovery logic\n│   │   ├── scraper.go      ← HTTP scraping implementation\n│   │   ├── scheduler.go    ← Scrape interval management\n│   │   └── parser.go       ← Metrics format parsing\n│   ├── storage/            ← Time series storage (Milestone 3)\n│   │   ├── engine.go       ← Main storage engine\n│   │   ├── compression.go  ← Gorilla compression implementation\n│   │   ├── index.go        ← Series indexing\n│   │   ├── retention.go    ← Data lifecycle management\n│   │   └── wal.go          ← Write-ahead logging\n│   ├── query/              ← Query engine (Milestone 4)\n│   │   ├── parser.go       ← PromQL parsing\n│   │   ├── executor.go     ← Query execution\n│   │   ├── aggregation.go  ← Aggregation functions\n│   │   └── matcher.go      ← Label matching logic\n│   └── api/                ← HTTP API handlers\n│       ├── scrape.go       ← Metrics exposition endpoints\n│       └── query.go        ← Query API endpoints\n├── pkg/                    ← Public interfaces (if building libraries)\n├── configs/                ← Example configuration files\n├── scripts/               ← Build and deployment scripts\n└── test/                  ← Integration tests and test data\n    ├── integration/\n    └── testdata/\n```\n\n**Infrastructure Starter Code:**\n\nHere's a complete HTTP server foundation that handles the basic networking and routing for both scrape targets and the query API:\n\n```go\n// internal/api/server.go\npackage api\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"time\"\n)\n\n// Server wraps HTTP server functionality for metrics collection\ntype Server struct {\n    httpServer *http.Server\n    mux        *http.ServeMux\n}\n\n// NewServer creates a new API server instance\nfunc NewServer(port int) *Server {\n    mux := http.NewServeMux()\n    \n    server := &http.Server{\n        Addr:         fmt.Sprintf(\":%d\", port),\n        Handler:      mux,\n        ReadTimeout:  30 * time.Second,\n        WriteTimeout: 30 * time.Second,\n        IdleTimeout:  120 * time.Second,\n    }\n    \n    return &Server{\n        httpServer: server,\n        mux:        mux,\n    }\n}\n\n// RegisterScrapeEndpoint adds a metrics exposition endpoint\nfunc (s *Server) RegisterScrapeEndpoint(path string, handler http.HandlerFunc) {\n    s.mux.HandleFunc(path, handler)\n}\n\n// RegisterQueryEndpoint adds a query API endpoint  \nfunc (s *Server) RegisterQueryEndpoint(path string, handler http.HandlerFunc) {\n    s.mux.HandleFunc(path, handler)\n}\n\n// Start begins serving HTTP requests\nfunc (s *Server) Start() error {\n    log.Printf(\"Starting metrics server on %s\", s.httpServer.Addr)\n    return s.httpServer.ListenAndServe()\n}\n\n// Shutdown gracefully stops the server\nfunc (s *Server) Shutdown(ctx context.Context) error {\n    log.Println(\"Shutting down metrics server...\")\n    return s.httpServer.Shutdown(ctx)\n}\n```\n\n**Configuration Management Infrastructure:**\n\n```go\n// internal/config/config.go\npackage config\n\nimport (\n    \"fmt\"\n    \"time\"\n    \"gopkg.in/yaml.v2\"\n    \"os\"\n)\n\n// Config holds all system configuration\ntype Config struct {\n    Scrape   ScrapeConfig   `yaml:\"scrape\"`\n    Storage  StorageConfig  `yaml:\"storage\"`\n    Query    QueryConfig    `yaml:\"query\"`\n}\n\n// ScrapeConfig configures the scraping engine\ntype ScrapeConfig struct {\n    Interval        time.Duration `yaml:\"interval\"`\n    Timeout         time.Duration `yaml:\"timeout\"`\n    MaxConcurrency  int          `yaml:\"max_concurrency\"`\n    StaticTargets   []string     `yaml:\"static_targets\"`\n}\n\n// StorageConfig configures time series storage\ntype StorageConfig struct {\n    DataDir         string        `yaml:\"data_dir\"`\n    RetentionPeriod time.Duration `yaml:\"retention_period\"`\n    ChunkSize       int           `yaml:\"chunk_size\"`\n    EnableCompression bool        `yaml:\"enable_compression\"`\n}\n\n// QueryConfig configures the query engine\ntype QueryConfig struct {\n    MaxConcurrency int           `yaml:\"max_concurrency\"`\n    QueryTimeout   time.Duration `yaml:\"query_timeout\"`\n    MaxSamples     int           `yaml:\"max_samples\"`\n}\n\n// LoadFromFile reads configuration from YAML file\nfunc LoadFromFile(filename string) (*Config, error) {\n    data, err := os.ReadFile(filename)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to read config file: %w\", err)\n    }\n    \n    var config Config\n    if err := yaml.Unmarshal(data, &config); err != nil {\n        return nil, fmt.Errorf(\"failed to parse config: %w\", err)\n    }\n    \n    return &config, nil\n}\n\n// SetDefaults populates default configuration values\nfunc (c *Config) SetDefaults() {\n    if c.Scrape.Interval == 0 {\n        c.Scrape.Interval = 15 * time.Second\n    }\n    if c.Scrape.Timeout == 0 {\n        c.Scrape.Timeout = 10 * time.Second\n    }\n    if c.Scrape.MaxConcurrency == 0 {\n        c.Scrape.MaxConcurrency = 50\n    }\n    if c.Storage.DataDir == \"\" {\n        c.Storage.DataDir = \"./data\"\n    }\n    if c.Storage.RetentionPeriod == 0 {\n        c.Storage.RetentionPeriod = 30 * 24 * time.Hour // 30 days\n    }\n    if c.Query.MaxConcurrency == 0 {\n        c.Query.MaxConcurrency = 10\n    }\n    if c.Query.QueryTimeout == 0 {\n        c.Query.QueryTimeout = 30 * time.Second\n    }\n}\n```\n\n**Core Logging Infrastructure:**\n\n```go\n// internal/logging/logger.go\npackage logging\n\nimport (\n    \"log\"\n    \"os\"\n    \"fmt\"\n)\n\ntype Logger struct {\n    infoLogger  *log.Logger\n    errorLogger *log.Logger\n    debugLogger *log.Logger\n}\n\nfunc NewLogger() *Logger {\n    return &Logger{\n        infoLogger:  log.New(os.Stdout, \"INFO: \", log.Ldate|log.Ltime|log.Lshortfile),\n        errorLogger: log.New(os.Stderr, \"ERROR: \", log.Ldate|log.Ltime|log.Lshortfile),\n        debugLogger: log.New(os.Stdout, \"DEBUG: \", log.Ldate|log.Ltime|log.Lshortfile),\n    }\n}\n\nfunc (l *Logger) Info(v ...interface{}) {\n    l.infoLogger.Println(v...)\n}\n\nfunc (l *Logger) Infof(format string, v ...interface{}) {\n    l.infoLogger.Printf(format, v...)\n}\n\nfunc (l *Logger) Error(v ...interface{}) {\n    l.errorLogger.Println(v...)\n}\n\nfunc (l *Logger) Errorf(format string, v ...interface{}) {\n    l.errorLogger.Printf(format, v...)\n}\n\nfunc (l *Logger) Debug(v ...interface{}) {\n    l.debugLogger.Println(v...)\n}\n\nfunc (l *Logger) Debugf(format string, v ...interface{}) {\n    l.debugLogger.Printf(format, v...)\n}\n```\n\n**Development Workflow Tips:**\n\n1. **Start with Static Configuration:** Begin with YAML configuration files before implementing dynamic service discovery. This allows you to test the core functionality without network complexity.\n\n2. **Use Table-Driven Tests:** Go's table-driven test pattern works excellently for metrics systems where you need to test many label combinations and edge cases:\n\n```go\nfunc TestLabelMatching(t *testing.T) {\n    tests := []struct {\n        name     string\n        labels   map[string]string\n        matcher  string\n        expected bool\n    }{\n        {\"exact match\", map[string]string{\"job\": \"api\"}, `job=\"api\"`, true},\n        {\"regex match\", map[string]string{\"instance\": \"web-01\"}, `instance=~\"web-.*\"`, true},\n        // Add more test cases...\n    }\n    // Test implementation...\n}\n```\n\n3. **Implement Graceful Shutdown:** Metrics systems often run as long-lived services. Implement proper shutdown handling to avoid data loss:\n\n```go\n// In your main.go\nc := make(chan os.Signal, 1)\nsignal.Notify(c, os.Interrupt, syscall.SIGTERM)\n<-c\n\nctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\ndefer cancel()\n\nif err := server.Shutdown(ctx); err != nil {\n    log.Fatal(\"Server forced to shutdown:\", err)\n}\n```\n\n**Language-Specific Hints for Go:**\n\n- **Memory Management:** Use `sync.Pool` for frequently allocated objects like sample slices to reduce GC pressure\n- **Goroutine Management:** Always use `context.Context` for cancellation in long-running operations like scraping and queries\n- **Time Handling:** Use `time.Unix()` for timestamp conversions and always store timestamps in UTC\n- **Error Handling:** Wrap errors with `fmt.Errorf(\"operation failed: %w\", err)` to maintain error context through the call stack\n- **Concurrent Maps:** Use `sync.RWMutex` to protect shared data structures like series indexes\n- **File I/O:** Always call `file.Sync()` after writing critical data like WAL entries to ensure durability\n\n**Common Development Pitfalls:**\n\n⚠️ **Pitfall: Blocking Operations in Hot Paths**\nNever perform I/O operations directly in scraping or query processing goroutines without proper timeouts. Always use `context.WithTimeout()` and handle cancellation appropriately.\n\n⚠️ **Pitfall: Unbounded Memory Growth**\nMetrics systems can accumulate memory leaks through retained references to old samples or series. Use profiling tools like `go tool pprof` regularly during development to identify memory growth patterns.\n\n⚠️ **Pitfall: Ignoring Graceful Degradation**\nBuild backpressure and circuit breaker patterns from the beginning. When storage is full or queries are slow, the system should reject new work rather than becoming unresponsive.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** This section establishes the scope and requirements that guide all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4).\n\n### The Observatory Charter Mental Model\n\nThink of this goals section as the charter document for a national weather observatory network. Just as meteorologists must decide whether their observatory will track temperature and rainfall (essential) versus tracking every atmospheric particle (impossible), we must define exactly what our metrics collection system will and won't accomplish. The charter prevents scope creep—when stakeholders later ask \"can it also do real-time alerting?\" we can point to this document and explain why alerting is explicitly out of scope for this implementation.\n\nThis boundary-setting is crucial for complex systems. Without clear goals, developers often build everything they can imagine rather than building the core functionality excellently. A well-scoped metrics system that handles scraping, storage, and querying perfectly is far more valuable than a system that attempts alerting, dashboards, and federation but does none of them well.\n\nThe goals also serve as acceptance criteria for the project. Each requirement must be measurable and testable—we can't claim success with vague objectives like \"good performance.\" Instead, we specify concrete targets: sub-second query response times, specific storage compression ratios, and exact retention capabilities.\n\n### Functional Requirements\n\nThe functional requirements define the core capabilities our metrics system must provide to be considered complete. These map directly to our four major milestones and represent the essential features that distinguish a metrics collection system from a generic time-series database.\n\n**Metrics Data Model Requirements**\n\nOur system must implement a comprehensive metrics data model that supports the standard observability metric types used in modern monitoring systems. The `Counter` type must enforce monotonic increase semantics, meaning values can only go up or reset to zero (typically on process restart). The `Gauge` type must allow arbitrary value changes to represent measurements like memory usage or queue depth that can increase or decrease freely. The `Histogram` type must track value distributions using configurable buckets, enabling percentile calculations and distribution analysis.\n\n| Metric Type | Semantic Behavior | Example Use Case | Key Constraint |\n|-------------|------------------|------------------|----------------|\n| Counter | Monotonic increase, resets on restart | HTTP requests served | Value ≥ previous value or reset to 0 |\n| Gauge | Arbitrary value changes | Memory usage bytes | No constraints on value changes |\n| Histogram | Distribution tracking in buckets | Request latency distribution | Bucket boundaries fixed after creation |\n| Summary | Client-side quantile calculation | Response time percentiles | Quantiles calculated at source |\n\nThe labeling system must support multi-dimensional metrics where each time series is uniquely identified by its metric name plus label set. Labels enable powerful filtering and aggregation—a single `http_requests_total` metric with `method`, `status_code`, and `handler` labels can answer questions like \"What's the error rate for POST requests to the login endpoint?\" The system must validate label names and values, rejecting reserved prefixes like `__` and ensuring label values don't contain characters that break the exposition format.\n\n> **Decision: Multi-Dimensional Labeling**\n> - **Context**: Need to support filtering and aggregation across different metric dimensions\n> - **Options Considered**: Single-dimensional metrics with encoded names, hierarchical metric names, multi-dimensional labels\n> - **Decision**: Multi-dimensional labels attached to each metric\n> - **Rationale**: Labels provide flexibility for ad-hoc queries without predefined metric names, enable efficient storage of related time series, and match Prometheus compatibility\n> - **Consequences**: Enables powerful querying but introduces cardinality explosion risks that must be managed\n\n**Scrape Engine Requirements**\n\nThe scrape engine must implement pull-based metrics collection that actively retrieves metrics from configured targets. Static configuration must support defining scrape targets through YAML configuration files, specifying endpoints, intervals, and timeouts. The system must parse the Prometheus exposition format, handling both the standard text format and optional metric metadata including help text and type information.\n\nService discovery integration must automatically update the target list when services are added or removed. The system must support at minimum static file-based service discovery, where external systems can update JSON or YAML files containing current service endpoints. The scrape scheduler must respect per-target intervals, ensuring targets are scraped consistently without drift or overlap.\n\n| Scrape Component | Responsibility | Configuration | Error Handling |\n|-----------------|---------------|---------------|----------------|\n| Target Discovery | Find scrape endpoints | Static config, service discovery files | Log discovery errors, continue with known targets |\n| HTTP Client | Fetch metrics from endpoints | Timeout, retry count, headers | Mark target down, continue scrape cycle |\n| Parser | Parse exposition format text | Format validation, metadata extraction | Skip malformed metrics, log parse errors |\n| Scheduler | Trigger scrapes at intervals | Per-target intervals, jitter | Compensate for missed scrapes, prevent overlap |\n\nThe scrape engine must handle target failures gracefully. Network timeouts must not block other targets' scrape cycles. HTTP errors (4xx, 5xx) must be logged and reported but not crash the scraper. Malformed exposition format must be handled by skipping unparseable lines while processing valid metrics from the same target.\n\n**Storage Engine Requirements**\n\nThe storage engine must provide efficient time-series storage with compression achieving less than 2 bytes per sample on average. The system must implement Gorilla-style compression using delta-of-delta encoding for timestamps and XOR encoding for floating-point values. This compression is essential for handling high-cardinality metrics at scale without exhausting storage capacity.\n\nThe storage must support configurable retention periods, automatically deleting data older than the specified duration. The default retention period must be 30 days, but the system must support retention periods from hours to years. Data deletion must be efficient, removing entire blocks rather than individual samples to avoid fragmentation.\n\n| Storage Feature | Requirement | Performance Target | Implementation Note |\n|----------------|-------------|-------------------|-------------------|\n| Compression Ratio | < 2 bytes per sample | 1.3 bytes average | Gorilla delta-of-delta + XOR |\n| Retention | Configurable age-based deletion | 30 days default | Block-based deletion |\n| Write Throughput | Handle scrape ingestion | 100k samples/sec | Batch writes, async commits |\n| Read Latency | Query response time | < 1 second typical | Efficient indexing |\n\nThe indexing system must enable fast lookup of time series by metric name and label combinations. The index must support exact label matching, regex matching, and negative matching (labels that don't equal a value). Query performance must remain reasonable even with high-cardinality label combinations, though the system may reject queries that would examine excessive numbers of series.\n\n**Query Engine Requirements**\n\nThe query engine must implement a PromQL-compatible query language supporting instant queries (single timestamp), range queries (time window), and basic aggregation functions. The system must support label selectors using exact match (`=`), not-equal (`!=`), regex match (`=~`), and negative regex (`!~`) operators.\n\nAggregation functions must include `sum`, `avg`, `max`, `min`, and `count` operations that can group results by specified label dimensions. Range queries must return data points at regular step intervals within the specified time window, interpolating missing values when necessary.\n\n| Query Type | Input | Output | Example |\n|-----------|-------|--------|---------|\n| Instant | Metric selector + timestamp | Vector of current values | `up` at now |\n| Range | Metric selector + time window + step | Matrix of time series | `cpu_usage[5m]` |\n| Aggregation | Vector + grouping labels | Grouped vector | `sum by (instance) (cpu_usage)` |\n| Rate Calculation | Counter range | Rate vector | `rate(http_requests[5m])` |\n\nThe query engine must handle counter resets correctly when calculating rates and derivatives. When a counter value decreases (indicating a reset), the rate calculation must treat the reset as the beginning of a new monotonic sequence rather than a negative rate.\n\n### Quality Attributes\n\nThe quality attributes define the performance, scalability, and reliability characteristics our metrics system must achieve. These non-functional requirements are often more critical than features—a metrics system that loses data or responds slowly becomes unusable regardless of its feature completeness.\n\n**Performance Requirements**\n\nQuery response time must remain under one second for typical queries examining up to 10,000 time series. Range queries spanning one week with five-minute resolution must complete within five seconds. These performance targets ensure the system remains usable for operational monitoring where slow queries impede incident response.\n\nStorage ingestion must handle sustained write rates of 100,000 samples per second without falling behind or dropping data. This throughput supports monitoring environments with thousands of services each exposing hundreds of metrics. Write latency must remain low enough that scraped metrics appear in queries within the scrape interval.\n\n| Performance Metric | Target | Measurement Method | Degradation Threshold |\n|-------------------|--------|-------------------|----------------------|\n| Query Response | < 1 second typical | 95th percentile latency | > 5 seconds |\n| Range Query | < 5 seconds | One week window | > 30 seconds |\n| Write Throughput | 100k samples/sec | Sustained ingestion | Falls behind scrape rate |\n| Storage Efficiency | < 2 bytes/sample | Compression ratio | > 4 bytes/sample |\n\nMemory usage must remain bounded even with high-cardinality metrics. The system must reject queries or scrape configurations that would consume excessive memory, providing clear error messages about cardinality limits. Disk usage must grow predictably based on ingestion rate and retention period.\n\n**Scalability Requirements**\n\nThe system must support monitoring environments with up to 1,000 scrape targets and 1 million active time series. While this implementation focuses on single-instance deployment, the architecture must not preclude future horizontal scaling through techniques like sharding or federation.\n\nCardinality must be controlled to prevent label explosion that could exhaust memory or storage. The system must provide mechanisms to limit the number of unique label value combinations, either through validation rules or runtime limits. High-cardinality label combinations (> 10,000 series per metric name) should trigger warnings or rejections.\n\n| Scalability Dimension | Limit | Rationale | Failure Mode |\n|----------------------|--------|-----------|--------------|\n| Scrape Targets | 1,000 targets | Single-instance HTTP client limits | Connection exhaustion |\n| Active Series | 1M series | Memory for index and active chunks | Out of memory errors |\n| Label Cardinality | 10k series per metric | Prevent exponential explosion | Query performance degradation |\n| Retention Data | 30 days × ingestion rate | Disk capacity planning | Disk full, query slowness |\n\n**Reliability Requirements**\n\nThe system must provide durability guarantees ensuring that successfully scraped metrics are not lost due to process crashes or system failures. Write-ahead logging must persist ingested samples before acknowledging success. Recovery after crashes must restore the system to a consistent state without data loss.\n\nTarget scraping must be resilient to individual target failures. Network partitions, service restarts, or malformed responses from one target must not affect scraping of other targets. The scrape engine must continue operating with degraded target coverage rather than failing completely.\n\n> **Decision: Pull-Based Scraping Model**\n> - **Context**: Need to choose between push-based (targets send metrics) and pull-based (collector fetches metrics) architectures\n> - **Options Considered**: Push-based with HTTP POST, pull-based with HTTP GET, hybrid push/pull\n> - **Decision**: Pull-based scraping with HTTP GET requests\n> - **Rationale**: Pull model provides better failure isolation (target failures don't crash collector), enables service discovery integration, matches Prometheus ecosystem, and simplifies target authentication\n> - **Consequences**: Requires targets to expose HTTP endpoints, may have higher network overhead, but provides better operational control and debugging\n\nData consistency must be maintained under concurrent access. Multiple queries must be able to execute simultaneously without corrupting results or crashing the system. Write operations must not interfere with concurrent reads beyond brief locking periods.\n\nThe system must handle resource exhaustion gracefully rather than crashing or corrupting data. When memory usage approaches limits, the system should reject new queries or reduce cache sizes while continuing to serve existing requests. Disk full conditions should pause ingestion while preserving existing data.\n\n### Scope Limitations\n\nThe scope limitations define features explicitly excluded from this implementation. These boundaries prevent scope creep and ensure we build the core metrics collection capabilities excellently rather than attempting everything mediocrely.\n\n**Alerting System Exclusion**\n\nThis implementation does not include alerting capabilities such as rule evaluation, threshold monitoring, or notification delivery. While alerting is a natural extension of metrics collection, implementing it properly requires additional components for rule management, evaluation scheduling, notification routing, and alert state tracking.\n\nBuilding alerting well requires solving problems orthogonal to metrics collection: template rendering for notifications, integration with external systems (email, Slack, PagerDuty), alert deduplication and grouping, and escalation policies. Including alerting would double the system complexity without improving the core collection, storage, and querying capabilities.\n\n| Alerting Feature | Excluded Reason | Alternative Approach |\n|------------------|-----------------|---------------------|\n| Rule Evaluation | Complex scheduling and state management | External alerting system queries our API |\n| Notification Delivery | Requires integration with many external services | Use dedicated alerting tools |\n| Alert State Tracking | Complex state machine for firing/resolved states | Stateless query-based detection |\n| Escalation Policies | Complex workflow management | External incident management tools |\n\nFuture implementations can add alerting by building a separate alert manager that queries this metrics system's API, maintaining clean separation of concerns.\n\n**Dashboard and Visualization Exclusion**\n\nThe system does not include built-in dashboards, graphing capabilities, or web-based visualization tools. While metrics are most valuable when visualized, building excellent charting and dashboard functionality requires deep frontend expertise and extensive JavaScript development.\n\nVisualization tools have different requirements than metrics collection: real-time updates, interactive charts, dashboard templating, user authentication, and responsive design. These concerns are better addressed by specialized tools like Grafana that can query our system's API.\n\n**Advanced Query Functions Exclusion**\n\nThe PromQL implementation includes only basic aggregation functions (`sum`, `avg`, `max`, `min`, `count`) and excludes advanced functions like `predict_linear`, `histogram_quantile`, or complex mathematical operations. These advanced functions require sophisticated algorithms and extensive testing to implement correctly.\n\nSimilarly, the system excludes recording rules (pre-computed queries stored as new metrics) and complex rate calculations beyond basic `rate()` function. These features add significant complexity to the query engine without being essential for basic monitoring.\n\n| Advanced Feature | Excluded Reason | Basic Alternative |\n|------------------|-----------------|-------------------|\n| `predict_linear` | Requires linear regression algorithms | Use external analysis tools |\n| `histogram_quantile` | Complex quantile calculation from buckets | Use summary metrics instead |\n| Recording Rules | Requires rule management and scheduling | Run queries manually or externally |\n| Complex Math Functions | Requires extensive function library | Perform calculations in analysis tools |\n\n**Multi-Instance Federation Exclusion**\n\nThis implementation targets single-instance deployment and excludes federation capabilities that would allow multiple metrics instances to share data or provide hierarchical aggregation. Federation requires solving distributed systems problems like data replication, conflict resolution, and cross-instance querying.\n\nBuilding federation properly requires consensus protocols, network partition handling, and complex query routing logic. These distributed systems challenges are significant projects themselves and would overshadow the core metrics collection functionality.\n\n**Enterprise Features Exclusion**\n\nThe system excludes enterprise-oriented features like user authentication, multi-tenancy, access control, and audit logging. These features require extensive security implementation and user management capabilities that are orthogonal to metrics collection.\n\nSimilarly, the system excludes advanced operational features like automatic backup/restore, cluster management, or sophisticated monitoring of the metrics system itself. These features are valuable for production deployment but not essential for understanding how metrics collection works.\n\n> **Design Insight: Scope Boundaries Enable Excellence**\n> By explicitly excluding advanced features, we can focus engineering effort on making the core metrics collection, storage, and querying capabilities excellent. A metrics system that handles these fundamentals perfectly provides a solid foundation for adding excluded features later, while a system that attempts everything often does nothing well.\n\nThe excluded features can be addressed through external integration rather than built-in functionality. Alerting systems can query our API, visualization tools can display our data, and operational tools can manage our deployment. This approach follows the Unix philosophy of building tools that do one thing excellently and compose well with other tools.\n\n**⚠️ Common Pitfalls in Scope Definition**\n\nDevelopers often make these mistakes when defining project scope:\n\n**Pitfall: Vague Non-Goals** - Writing \"won't include advanced features\" without specifying exactly which features. This leads to scope creep when stakeholders assume their favorite feature isn't \"advanced.\" Instead, explicitly list each excluded capability with reasoning.\n\n**Pitfall: Feature Creep During Implementation** - Adding \"quick features\" during development because they seem easy. A simple dashboard or basic alerting might seem straightforward, but each addition introduces new failure modes, test requirements, and maintenance burden. Stick to the defined scope religiously.\n\n**Pitfall: Unrealistic Performance Targets** - Setting performance requirements without understanding the underlying constraints. Claiming sub-millisecond query times while implementing complex aggregations over millions of series. Base performance targets on realistic measurements from similar systems.\n\n**Pitfall: Missing Quality Attributes** - Focusing only on functional requirements while ignoring performance, reliability, and scalability needs. A metrics system that handles all the required features but responds slowly or loses data is unusable for monitoring production systems.\n\n### Implementation Guidance\n\nThe requirements established in this section drive architectural decisions throughout the remaining design. Each functional requirement maps to specific implementation choices, while quality attributes establish measurable targets for validation.\n\n**Requirements Traceability Matrix**\n\n| Requirement Category | Implementation Component | Validation Method | Success Criteria |\n|---------------------|-------------------------|-------------------|------------------|\n| Metrics Data Model | `Counter`, `Gauge`, `Histogram` types | Unit tests with semantic validation | Monotonic counter behavior, flexible gauge updates |\n| Multi-dimensional Labels | Label map with validation | Integration tests with high-cardinality scenarios | Supports 10k series per metric name |\n| Pull-based Scraping | HTTP client with service discovery | Load testing with 1000 targets | Maintains scrape intervals under load |\n| Gorilla Compression | Delta-of-delta and XOR encoding | Compression ratio measurement | < 2 bytes per sample average |\n| PromQL Queries | Parser and execution engine | Query correctness and performance tests | < 1 second response for typical queries |\n\n**Technology Stack Recommendations**\n\nFor implementing the requirements defined in this section, the technology choices directly impact our ability to meet performance and scalability targets:\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|-----------------|-----------|\n| HTTP Server | `net/http` standard library | `fasthttp` or `gin` framework | Standard library sufficient for scraping loads |\n| Configuration | `yaml.v3` for static config | `viper` for dynamic config | YAML parsing meets static config requirements |\n| Time Series Storage | Append-only files with custom format | Embedded database like BadgerDB | Custom format provides compression control |\n| Indexing | In-memory maps with periodic snapshots | LSM-tree based indexes | Memory indexes meet single-instance scale |\n| Compression | Custom Gorilla implementation | Existing libraries like `tsz` | Custom implementation for educational value |\n\n**Configuration Structure Foundation**\n\nThe requirements drive a specific configuration structure that supports all functional needs while maintaining simplicity:\n\n```go\n// Config represents the complete system configuration matching our functional requirements\ntype Config struct {\n    // Scrape configuration supports pull-based collection requirement\n    Scrape ScrapeConfig `yaml:\"scrape\"`\n    \n    // Storage configuration meets retention and compression requirements  \n    Storage StorageConfig `yaml:\"storage\"`\n    \n    // Query configuration enforces performance and resource limits\n    Query QueryConfig `yaml:\"query\"`\n    \n    // Server configuration for HTTP endpoints\n    Server ServerConfig `yaml:\"server\"`\n}\n\n// ScrapeConfig implements target discovery and interval management requirements\ntype ScrapeConfig struct {\n    // Global defaults applied to all targets\n    ScrapeInterval time.Duration `yaml:\"scrape_interval\"`\n    ScrapeTimeout  time.Duration `yaml:\"scrape_timeout\"`\n    \n    // Static target configuration\n    StaticConfigs []StaticConfig `yaml:\"static_configs\"`\n    \n    // Service discovery configuration  \n    FileServiceDiscovery []FileSDConfig `yaml:\"file_sd_configs\"`\n}\n\n// StorageConfig meets retention and compression requirements\ntype StorageConfig struct {\n    // Data directory for time series storage\n    DataDirectory string `yaml:\"data_directory\"`\n    \n    // Retention period for automatic cleanup\n    RetentionPeriod time.Duration `yaml:\"retention_period\"`\n    \n    // Compression settings for Gorilla algorithm\n    CompressionEnabled bool `yaml:\"compression_enabled\"`\n    \n    // Write-ahead log configuration for durability\n    WALEnabled bool `yaml:\"wal_enabled\"`\n}\n\n// QueryConfig enforces performance and resource limits\ntype QueryConfig struct {\n    // Maximum query execution time\n    QueryTimeout time.Duration `yaml:\"query_timeout\"`\n    \n    // Maximum number of series a query can examine\n    MaxSeries int `yaml:\"max_series\"`\n    \n    // Maximum range query duration\n    MaxRangeDuration time.Duration `yaml:\"max_range_duration\"`\n}\n```\n\n**Requirements Validation Framework**\n\nEach milestone should validate that requirements are being met through specific tests and measurements:\n\n```go\n// RequirementsValidator provides methods to verify system meets defined requirements\ntype RequirementsValidator struct {\n    config *Config\n    logger Logger\n}\n\n// ValidatePerformanceRequirements checks that system meets performance targets\nfunc (v *RequirementsValidator) ValidatePerformanceRequirements() error {\n    // TODO: Implement query latency measurement\n    // TODO: Validate write throughput under load\n    // TODO: Check compression ratio achievement\n    // TODO: Verify memory usage remains bounded\n    return nil\n}\n\n// ValidateScalabilityRequirements verifies system handles target scale\nfunc (v *RequirementsValidator) ValidateScalabilityRequirements() error {\n    // TODO: Test with 1000 scrape targets\n    // TODO: Verify 1M active time series support\n    // TODO: Check cardinality limit enforcement\n    // TODO: Validate retention period handling\n    return nil\n}\n\n// ValidateReliabilityRequirements ensures durability and fault tolerance\nfunc (v *RequirementsValidator) ValidateReliabilityRequirements() error {\n    // TODO: Test crash recovery with WAL\n    // TODO: Verify target failure isolation\n    // TODO: Check graceful resource exhaustion handling\n    // TODO: Validate concurrent access safety\n    return nil\n}\n```\n\n**Milestone Checkpoint: Requirements Verification**\n\nAfter implementing each milestone, verify it meets the requirements established in this section:\n\n**Milestone 1 Checkpoint - Metrics Data Model:**\n- Run: `go test ./internal/metrics/... -v`\n- Expected: All metric type tests pass, counter monotonicity enforced, labels validate correctly\n- Manual verification: Create metrics with various label combinations, confirm cardinality limits work\n- Performance check: Create 10k series, verify memory usage remains reasonable\n\n**Milestone 2 Checkpoint - Scrape Engine:**\n- Run: `go test ./internal/scraper/... -v` and start test HTTP targets\n- Expected: Targets discovered from config, scraped at intervals, parse errors handled gracefully\n- Manual verification: Configure 10 targets, observe scrape success rates and timing\n- Performance check: Scrape 100 targets simultaneously, verify no blocking or missed intervals\n\n**Milestone 3 Checkpoint - Storage Engine:**\n- Run: `go test ./internal/storage/... -v` and measure compression ratios\n- Expected: Data persisted durably, Gorilla compression achieves < 2 bytes/sample, retention works\n- Manual verification: Insert sample data, restart system, verify data recovers correctly\n- Performance check: Sustain 10k samples/sec writes, measure query response times\n\n**Milestone 4 Checkpoint - Query Engine:**\n- Run: `go test ./internal/query/... -v` and execute sample PromQL queries\n- Expected: Basic PromQL parsing works, aggregations produce correct results, range queries function\n- Manual verification: Run queries against stored data, verify results match expectations\n- Performance check: Query 1000 series over one week, verify sub-second response\n\nThe requirements defined in this section provide the acceptance criteria for each milestone and the overall system success. Every implementation decision should trace back to these requirements, and every feature should be validated against these targets.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** This section establishes the architectural foundation for all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4).\n\nThink of our metrics collection system like a modern weather monitoring network. Weather stations (scrape targets) continuously measure temperature, humidity, and wind speed. Data collection trucks (scrape engine) visit each station on a schedule to gather measurements. The measurements are transported to a central archive (storage engine) where they're organized, compressed, and indexed for long-term preservation. Scientists and meteorologists (users) can then query the archive to analyze weather patterns, generate reports, and make predictions about future conditions.\n\nThis mental model captures the essential flow of our system: autonomous data generation at distributed endpoints, scheduled collection via pull-based mechanisms, efficient storage with temporal organization, and flexible querying for analysis and monitoring. Just as weather data becomes valuable when aggregated and analyzed over time, metrics data provides observability insights when collected systematically and made queryable.\n\nOur architecture consists of four primary components that work together to implement this observability pipeline. Each component has distinct responsibilities and interfaces, but they're designed to work together seamlessly to provide a complete metrics collection and analysis solution.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n### Component Responsibilities\n\nThe system is decomposed into four major components, each with clearly defined responsibilities and boundaries. This separation of concerns enables independent development, testing, and scaling of each component while maintaining clean interfaces between them.\n\n#### Scrape Engine Component\n\nThe **Scrape Engine** serves as the data acquisition layer of our system, responsible for discovering targets and collecting metrics from distributed endpoints. Think of it as a fleet of postal workers who know every address in the city, visit each location on a precise schedule, and collect all the mail waiting for pickup.\n\nThe scrape engine operates on a pull-based model, meaning it actively reaches out to configured targets rather than waiting for them to push data. This design choice provides several advantages: targets remain stateless and don't need to know about the collector, network failures are handled centrally, and the collector maintains complete control over data collection timing and frequency.\n\n| Responsibility | Description | Key Operations |\n|---|---|---|\n| Target Discovery | Locate scrape endpoints through static config or service discovery | Parse configuration files, query DNS records, interact with Kubernetes API |\n| Scrape Scheduling | Execute HTTP requests to targets at configured intervals | Manage per-target timers, handle concurrent scraping, track scrape success/failure |\n| Metrics Parsing | Parse Prometheus exposition format from HTTP response bodies | Tokenize text format, validate metric names and labels, extract timestamps |\n| Health Monitoring | Track target availability and scrape success rates | Record response times, detect timeouts, maintain target status |\n| Backpressure Control | Prevent overwhelming targets with too-frequent requests | Implement exponential backoff, respect rate limits, queue scrape requests |\n\nThe scrape engine maintains its own configuration system with support for dynamic updates. When service discovery detects new targets, the engine automatically begins scraping them without requiring a restart. Similarly, when targets disappear, scraping stops and resources are cleaned up.\n\n> **Decision: Pull-Based Collection Model**\n> - **Context**: Metrics can be collected via push (targets send data) or pull (collector retrieves data) models\n> - **Options Considered**: Push-based with target-initiated connections, pull-based with collector-initiated connections, hybrid approach\n> - **Decision**: Pull-based collection with HTTP scraping\n> - **Rationale**: Pull model provides better failure isolation (targets can't overwhelm collector), simpler target implementation (no need to know collector address), and centralized control over collection frequency and timeout handling\n> - **Consequences**: Requires targets to expose HTTP endpoints, collector must maintain target inventory, network failures affect data collection\n\n| Collection Model | Advantages | Disadvantages | Chosen? |\n|---|---|---|---|\n| Push-based | Targets control timing, works behind firewalls | Targets need collector address, backpressure harder to manage | No |\n| Pull-based | Centralized control, simpler targets, better failure isolation | Requires HTTP endpoints, collector needs target discovery | **Yes** |\n| Hybrid | Flexibility for different target types | Increased complexity, two codepaths to maintain | No |\n\n#### Time Series Storage Component  \n\nThe **Storage Engine** functions as the system's memory and archive, responsible for persisting collected metrics efficiently and making them available for queries. Imagine a specialized library that stores millions of temperature readings, where each book represents a time series and pages contain chronologically ordered measurements compressed to save space.\n\nThe storage layer implements several sophisticated optimizations to handle the unique characteristics of time series data. Unlike traditional databases that optimize for random access patterns, time series data is almost always written in chronological order and queried by time ranges. This access pattern enables aggressive compression and specialized indexing strategies.\n\n| Responsibility | Description | Key Operations |\n|---|---|---|\n| Data Ingestion | Accept scraped metrics and store them durably | Validate metric format, assign timestamps, write to WAL, index updates |\n| Compression | Reduce storage footprint using time series algorithms | Apply Gorilla compression, manage chunk boundaries, optimize encoding |\n| Indexing | Maintain lookups for metric names and label combinations | Build inverted indexes, handle label cardinality, optimize query performance |\n| Retention Management | Delete old data based on configured policies | Scan for expired data, compact storage files, update indexes |\n| Query Processing | Retrieve time series data for specified time ranges and labels | Seek to time ranges, decompress data, filter by labels, return results |\n\nThe storage engine uses a **Write-Ahead Log (WAL)** to ensure durability. Every metric write is first recorded in the WAL before being applied to the main storage structures. This guarantees that even if the system crashes during a write operation, the data can be recovered by replaying the WAL on startup.\n\nStorage is organized into **time-based chunks** that contain compressed time series data for a specific time window (typically 2 hours). This chunking strategy optimizes both compression ratio and query performance by ensuring that data accessed together is stored together physically.\n\n> **Decision: Gorilla-Style Compression**\n> - **Context**: Time series data has high redundancy that can be exploited for compression\n> - **Options Considered**: Generic compression (gzip), column-oriented compression, Gorilla delta-of-delta compression\n> - **Decision**: Implement Gorilla compression with delta-of-delta timestamps and XOR value encoding\n> - **Rationale**: Gorilla compression achieves 12x space reduction on typical time series workloads, optimized specifically for time series patterns, and provides good query performance without full decompression\n> - **Consequences**: More complex implementation than generic compression, requires careful chunk boundary management, but provides significant storage savings\n\n| Compression Approach | Compression Ratio | Query Performance | Implementation Complexity | Chosen? |\n|---|---|---|---|---|\n| No compression | 1x (baseline) | Fastest | Simplest | No |\n| Generic (gzip) | 3-5x | Slow (full decompression) | Simple | No |\n| Gorilla algorithm | 10-15x | Fast (partial decompression) | Complex | **Yes** |\n\n#### Query Engine Component\n\nThe **Query Engine** serves as the system's analytical brain, translating user queries into efficient data retrieval and processing operations. Think of it as a research librarian who understands both what researchers are looking for and the most efficient way to locate and combine information from the archive.\n\nThe query engine implements a PromQL-compatible query language that supports both instant queries (single point in time) and range queries (time series over a window). The engine's job is to parse these queries, plan efficient execution, retrieve the necessary data from storage, and perform any required aggregations or mathematical operations.\n\n| Responsibility | Description | Key Operations |\n|---|---|---|\n| Query Parsing | Convert PromQL text into executable query plans | Tokenize expressions, build AST, validate syntax, optimize plans |\n| Series Selection | Find time series matching label selectors | Query storage indexes, apply label filters, handle regex matching |\n| Data Retrieval | Fetch time series data from storage for specified time ranges | Issue storage queries, handle pagination, manage memory usage |\n| Aggregation | Perform mathematical operations across time series | Implement sum/avg/max/min/count, handle grouping, compute rates |\n| Result Formatting | Convert internal data structures to API response format | Serialize to JSON, apply time formatting, handle large result sets |\n\nThe query engine is designed to be **stateless** - each query execution is independent and doesn't rely on cached state from previous queries. This design simplifies scaling and debugging but requires careful optimization to avoid repeatedly processing the same data.\n\nQuery execution follows a **pipeline model** where data flows through a series of processing stages: series selection, data retrieval, time alignment, aggregation, and result formatting. Each stage can be optimized independently and may process data in streaming fashion to reduce memory usage.\n\n> **Decision: AST-Based Query Execution**\n> - **Context**: PromQL queries need to be parsed and executed efficiently with proper operator precedence\n> - **Options Considered**: Direct interpreter, AST with visitor pattern, compiled query plans\n> - **Decision**: Abstract Syntax Tree with recursive evaluation\n> - **Rationale**: AST provides clear separation between parsing and execution, supports complex nested expressions, and enables query optimization passes\n> - **Consequences**: More complex than direct interpretation but provides better extensibility and optimization opportunities\n\n#### HTTP API Server Component\n\nThe **HTTP Server** acts as the system's front door, providing REST endpoints for queries, configuration, and system status. It handles authentication, request routing, response formatting, and protocol translation between external HTTP clients and internal components.\n\nThe server component is relatively lightweight since most of the heavy lifting is done by the specialized engines. Its primary responsibilities center around protocol handling, request validation, and response formatting rather than core metrics processing.\n\n| Responsibility | Description | Key Operations |\n|---|---|---|\n| Request Routing | Direct incoming HTTP requests to appropriate handlers | Parse URLs, validate methods, route to query/config/status handlers |\n| Query API | Expose PromQL query interface via HTTP endpoints | Parse query parameters, invoke query engine, format responses |\n| Configuration API | Allow runtime configuration updates | Accept new scrape configs, validate settings, notify components |\n| Metrics Exposition | Expose system's own metrics for monitoring | Generate internal metrics, format in Prometheus format |\n| Error Handling | Provide consistent error responses and logging | Map internal errors to HTTP status codes, log requests |\n\n### Data Flow Overview\n\nUnderstanding how data moves through our system is crucial for both implementation and debugging. The data flow follows a clear pipeline from external targets through our components and back to users, with each stage transforming and enriching the data.\n\n#### Metrics Collection Flow\n\nThe metrics collection flow begins when the scrape engine identifies targets through service discovery and executes HTTP requests to gather metrics data. This process runs continuously in the background according to configured intervals.\n\n1. **Target Discovery Phase**: The scrape engine queries configured service discovery backends (static files, DNS, Kubernetes API) to build a current list of scrape targets. Each target includes an endpoint URL, scrape interval, timeout, and optional labels.\n\n2. **Scrape Scheduling Phase**: A scheduler component maintains timing wheels for each target, triggering scrape operations according to their individual intervals. Multiple scrapes execute concurrently using worker pools to maximize throughput.\n\n3. **HTTP Collection Phase**: For each scrape, the engine sends an HTTP GET request to the target's metrics endpoint (typically `/metrics`). The request includes appropriate headers and handles authentication if configured.\n\n4. **Metrics Parsing Phase**: The response body contains metrics in Prometheus exposition format (text-based). The parser tokenizes this content, extracting metric names, labels, values, and timestamps. Invalid metrics are logged and discarded.\n\n5. **Storage Ingestion Phase**: Parsed metrics are sent to the storage engine, which validates them, assigns canonical timestamps, and writes them to the WAL. The data is then indexed and added to the appropriate time series chunks.\n\n6. **Health Tracking Phase**: The scrape engine records success/failure status, response times, and error details for each scrape operation. This information is used for target health monitoring and alerting.\n\n| Stage | Input | Processing | Output | Failure Mode |\n|---|---|---|---|---|\n| Target Discovery | Service discovery configs | Query APIs, parse responses | Target list with endpoints | Service discovery unavailable |\n| Scrape Scheduling | Target list, intervals | Manage timers, worker pools | Scrape tasks | Resource exhaustion |\n| HTTP Collection | Target endpoints | HTTP GET requests | Raw metrics text | Network timeout, HTTP errors |\n| Metrics Parsing | Metrics text | Tokenize, validate format | Structured metrics | Parse errors, invalid format |\n| Storage Ingestion | Structured metrics | Write WAL, update indexes | Persisted time series | Disk full, corruption |\n| Health Tracking | Scrape results | Record status, compute metrics | Target health status | Monitoring system failure |\n\n#### Query Processing Flow\n\nThe query processing flow is triggered when users submit PromQL queries via the HTTP API. This flow involves parsing the query, planning execution, retrieving data, and formatting results.\n\n1. **Query Reception Phase**: The HTTP server receives a query request containing a PromQL expression, time range (for range queries), and optional parameters like timeout limits.\n\n2. **Query Parsing Phase**: The query engine tokenizes the PromQL expression and builds an Abstract Syntax Tree (AST) representing the query structure. Syntax errors are detected and reported at this stage.\n\n3. **Query Planning Phase**: The engine analyzes the AST to determine which time series need to be retrieved from storage, what time ranges are required, and what aggregations need to be performed.\n\n4. **Series Selection Phase**: Using the storage engine's indexes, the system identifies all time series that match the query's label selectors. This may involve regex matching and label combination lookups.\n\n5. **Data Retrieval Phase**: For each selected series, the storage engine retrieves data points within the query's time range. Gorilla compression is applied in reverse to decompress the stored data.\n\n6. **Query Execution Phase**: The engine applies aggregation functions, mathematical operations, and grouping logic to compute the final results according to the PromQL expression.\n\n7. **Result Formatting Phase**: The computed results are serialized into the appropriate response format (JSON for API queries) and returned to the client.\n\n| Stage | Input | Processing | Output | Performance Considerations |\n|---|---|---|---|---|\n| Query Reception | HTTP request | Validate parameters, parse time ranges | Query parameters | Request validation overhead |\n| Query Parsing | PromQL text | Tokenize, build AST | Query execution plan | Parser complexity scales with query complexity |\n| Query Planning | AST | Analyze requirements, optimize | Execution strategy | Planning time vs execution efficiency tradeoff |\n| Series Selection | Label selectors | Index lookups, regex matching | Matching series list | Index efficiency critical for high cardinality |\n| Data Retrieval | Series list, time range | Storage queries, decompression | Raw data points | I/O bound, compression CPU cost |\n| Query Execution | Raw data, operations | Aggregation, math functions | Computed results | Memory usage scales with result size |\n| Result Formatting | Computed results | Serialize to JSON | HTTP response | Serialization cost for large result sets |\n\n### Deployment Architecture\n\nThe deployment architecture defines how our components are distributed across machines and how they scale to handle production workloads. We support multiple deployment patterns depending on scale requirements and operational constraints.\n\n#### Single-Instance Deployment\n\nFor development, testing, and smaller production workloads, all components can be deployed within a single process on one machine. This deployment pattern minimizes operational complexity while providing full functionality.\n\nIn single-instance deployment, all components share the same process space and communicate via direct function calls rather than network protocols. The HTTP server, scrape engine, storage engine, and query engine all run as goroutines within the same binary.\n\n| Component | Resource Usage | Scaling Limit | Failure Impact |\n|---|---|---|---|\n| Scrape Engine | CPU-bound (HTTP clients) | ~1000 targets per instance | All scraping stops |\n| Storage Engine | Disk I/O and memory | ~1M active series | All data inaccessible |\n| Query Engine | CPU and memory | ~100 concurrent queries | All queries fail |\n| HTTP Server | Network and CPU | ~1000 QPS | All API access lost |\n\nThe single instance stores all data locally using disk-based storage with configurable data directories. Configuration is provided via YAML files that specify scrape targets, retention policies, and server settings.\n\n> **Decision: Single-Process Architecture**\n> - **Context**: Need to balance simplicity with scalability for initial implementation\n> - **Options Considered**: Microservices from the start, single process with internal interfaces, monolithic design\n> - **Decision**: Single process with clean component interfaces that can be split later\n> - **Rationale**: Reduces operational complexity, simplifies debugging, enables easier testing while maintaining clean boundaries for future scaling\n> - **Consequences**: Limited horizontal scalability but much simpler deployment and development\n\n#### Configuration Management\n\nThe system uses a hierarchical configuration approach with YAML files, environment variables, and command-line flags. Configuration can be reloaded at runtime without requiring a restart, enabling dynamic updates to scrape targets and other settings.\n\nThe `Config` struct serves as the root configuration object, containing nested configuration for each component:\n\n| Configuration Section | Key Settings | Reload Behavior |\n|---|---|---|\n| `ScrapeConfig` | scrape_interval, scrape_timeout, static_configs | Dynamic reload, affects next scrape cycle |\n| `StorageConfig` | data_directory, retention_period, compression_enabled | Partial reload, retention changes apply immediately |\n| `QueryConfig` | query_timeout, max_series, max_range_duration | Dynamic reload, affects new queries |\n| `ServerConfig` | port, read_timeout, write_timeout | Requires restart |\n\nConfiguration validation occurs at startup and during reload operations. The `RequirementsValidator` checks that the system can meet specified performance, scalability, and reliability requirements given the current configuration and available resources.\n\n#### Process Management and Health Monitoring  \n\nThe system exposes its own metrics via HTTP endpoints, enabling monitoring of internal performance and health. Key metrics include scrape success rates, query latency, storage utilization, and error rates.\n\nHealth checks are available at multiple levels: individual component health (can the storage engine accept writes?), system health (are all components functioning?), and operational health (are performance targets being met?).\n\n| Health Check | Endpoint | Success Criteria | Failure Response |\n|---|---|---|---|\n| Liveness | `/health/live` | Process responding | HTTP 503, restart container |\n| Readiness | `/health/ready` | All components initialized | HTTP 503, remove from load balancer |\n| Component Health | `/health/components` | Each component passes self-check | HTTP 200 with component status |\n\nThe system implements graceful shutdown procedures that allow in-flight operations to complete before termination. Query processing completes current requests, scraping finishes active scrapes, and storage flushes pending writes to disk.\n\n#### Resource Requirements and Capacity Planning\n\nResource requirements scale primarily with the number of active time series (cardinality) and query load rather than the volume of individual metrics. A time series that receives one sample per minute versus one sample per second requires similar memory and storage resources due to compression efficiency.\n\nMemory usage is dominated by the time series index and uncompressed data in active chunks. Each active time series requires approximately 1KB of memory for index structures plus a proportional share of chunk overhead.\n\n| Workload Characteristic | Memory Impact | Storage Impact | CPU Impact |\n|---|---|---|---|\n| High cardinality (many series) | Linear growth in index size | Linear growth in compressed data | Minimal |\n| High sample rate | Minimal (better compression) | Sublinear growth | Linear in ingestion CPU |\n| Complex queries | Temporary growth during execution | None | High CPU usage |\n| Large time ranges | Memory for decompressed data | None | Decompression CPU cost |\n\nStorage requirements depend on retention period, compression efficiency, and cardinality. Gorilla compression typically achieves 12:1 reduction, so 1 million series with 1-minute resolution retained for 30 days requires approximately 180GB of disk space.\n\n#### Common Pitfalls\n\n⚠️ **Pitfall: Underestimating Label Cardinality Impact**\nLabel combinations multiply to create the total number of time series. Adding labels like `instance_id` or `user_id` can explode cardinality exponentially. Monitor the number of active series and set cardinality limits to prevent memory exhaustion.\n\n⚠️ **Pitfall: Inadequate Resource Allocation for Query Engine**\nComplex PromQL queries can consume significant memory and CPU, especially when aggregating across high cardinality labels. Set query timeouts and memory limits, and monitor query performance to identify expensive patterns.\n\n⚠️ **Pitfall: Ignoring Scrape Target Health**\nFailed scrapes result in data gaps that affect query results and alerting. Implement proper monitoring of scrape success rates and set up alerting when targets become unavailable.\n\n⚠️ **Pitfall: Insufficient Storage Planning**\nTime series data grows continuously, and running out of disk space causes immediate system failure. Plan for at least 20% overhead beyond calculated requirements and implement monitoring of disk usage with automated alerting.\n\n### Implementation Guidance\n\nThe implementation approach balances simplicity for learning purposes with realistic production patterns. We'll use Go as the primary language due to its excellent concurrency support and HTTP capabilities.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| HTTP Server | net/http with custom routing | Gorilla Mux or Gin framework |\n| Configuration | encoding/json with structs | YAML parsing with gopkg.in/yaml.v2 |\n| Storage Backend | Local filesystem with os package | Embedded database like BadgerDB |\n| Compression | Custom Gorilla implementation | Existing library like prometheus/tsdb |\n| Service Discovery | Static file-based configuration | Kubernetes client-go integration |\n| Logging | Standard log package | Structured logging with logrus/zap |\n\n#### Recommended File Structure\n\nOrganize the codebase to reflect the component boundaries and support independent development:\n\n```\nmetrics-collector/\n├── cmd/\n│   └── collector/\n│       └── main.go                 ← Entry point and CLI parsing\n├── internal/\n│   ├── config/\n│   │   ├── config.go               ← Config structs and loading\n│   │   └── validator.go            ← RequirementsValidator implementation\n│   ├── scrape/\n│   │   ├── engine.go               ← Scrape scheduling and execution\n│   │   ├── target.go               ← Target management and service discovery\n│   │   └── parser.go               ← Prometheus format parsing\n│   ├── storage/\n│   │   ├── engine.go               ← Main storage interface\n│   │   ├── series.go               ← Time series data structures\n│   │   ├── compression.go          ← Gorilla compression implementation\n│   │   └── index.go                ← Series indexing and lookup\n│   ├── query/\n│   │   ├── engine.go               ← Query planning and execution\n│   │   ├── parser.go               ← PromQL parsing and AST\n│   │   └── functions.go            ← Aggregation and math functions\n│   └── server/\n│       ├── server.go               ← HTTP server and routing\n│       └── handlers.go             ← API endpoint handlers\n├── pkg/\n│   ├── metrics/                    ← Shared metric type definitions\n│   └── model/                      ← Common data structures\n└── configs/\n    └── example.yaml                ← Example configuration file\n```\n\n#### Infrastructure Starter Code\n\nHere's the basic HTTP server infrastructure that handles routing and graceful shutdown:\n\n```go\n// internal/server/server.go\npackage server\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\n// Server wraps HTTP server for metrics endpoints\ntype Server struct {\n    httpServer *http.Server\n    mux        *http.ServeMux\n}\n\n// NewServer creates HTTP server on specified port\nfunc NewServer(port int) *Server {\n    mux := http.NewServeMux()\n    return &Server{\n        httpServer: &http.Server{\n            Addr:         fmt.Sprintf(\":%d\", port),\n            Handler:      mux,\n            ReadTimeout:  30 * time.Second,\n            WriteTimeout: 30 * time.Second,\n        },\n        mux: mux,\n    }\n}\n\n// RegisterScrapeEndpoint adds metrics exposition endpoint\nfunc (s *Server) RegisterScrapeEndpoint(path string, handler http.HandlerFunc) {\n    s.mux.HandleFunc(path, handler)\n}\n\n// Start begins serving HTTP requests\nfunc (s *Server) Start() error {\n    return s.httpServer.ListenAndServe()\n}\n\n// Shutdown gracefully stops server\nfunc (s *Server) Shutdown(ctx context.Context) error {\n    return s.httpServer.Shutdown(ctx)\n}\n```\n\nConfiguration loading infrastructure with validation:\n\n```go\n// internal/config/config.go  \npackage config\n\nimport (\n    \"fmt\"\n    \"io/ioutil\"\n    \"time\"\n    \"gopkg.in/yaml.v2\"\n)\n\n// Default constants for configuration\nconst (\n    DEFAULT_SCRAPE_INTERVAL   = 15 * time.Second\n    DEFAULT_SCRAPE_TIMEOUT    = 10 * time.Second\n    DEFAULT_RETENTION_PERIOD  = 30 * 24 * time.Hour  // 30 days\n    DEFAULT_QUERY_TIMEOUT     = 30 * time.Second\n)\n\n// Config is the root configuration structure\ntype Config struct {\n    Scrape  ScrapeConfig  `yaml:\"scrape\"`\n    Storage StorageConfig `yaml:\"storage\"`\n    Query   QueryConfig   `yaml:\"query\"`\n    Server  ServerConfig  `yaml:\"server\"`\n}\n\n// ScrapeConfig defines scraping parameters\ntype ScrapeConfig struct {\n    ScrapeInterval time.Duration    `yaml:\"scrape_interval\"`\n    ScrapeTimeout  time.Duration    `yaml:\"scrape_timeout\"`\n    StaticConfigs  []StaticConfig   `yaml:\"static_configs\"`\n}\n\n// StorageConfig defines storage parameters  \ntype StorageConfig struct {\n    DataDirectory     string        `yaml:\"data_directory\"`\n    RetentionPeriod   time.Duration `yaml:\"retention_period\"`\n    CompressionEnabled bool         `yaml:\"compression_enabled\"`\n}\n\n// QueryConfig defines query engine parameters\ntype QueryConfig struct {\n    QueryTimeout      time.Duration `yaml:\"query_timeout\"`\n    MaxSeries         int           `yaml:\"max_series\"`\n    MaxRangeDuration  time.Duration `yaml:\"max_range_duration\"`\n}\n\n// ServerConfig defines HTTP server parameters\ntype ServerConfig struct {\n    Port int `yaml:\"port\"`\n}\n\n// StaticConfig defines a static scrape target\ntype StaticConfig struct {\n    Targets []string          `yaml:\"targets\"`\n    Labels  map[string]string `yaml:\"labels\"`\n}\n\n// LoadFromFile reads YAML configuration\nfunc LoadFromFile(filename string) (*Config, error) {\n    // TODO: Read file content using ioutil.ReadFile\n    // TODO: Parse YAML using yaml.Unmarshal  \n    // TODO: Call SetDefaults to populate missing values\n    // TODO: Return parsed config or error\n}\n\n// SetDefaults populates default config values\nfunc (c *Config) SetDefaults() {\n    // TODO: Set default values for all duration and numeric fields\n    // TODO: Ensure required string fields have sensible defaults\n    // TODO: Apply defaults recursively to nested config structs\n}\n```\n\n#### Core Logic Skeleton Code\n\nHere are the method signatures for the core components that learners should implement:\n\n```go\n// internal/scrape/engine.go\npackage scrape\n\nimport (\n    \"context\"\n    \"time\"\n    \"net/http\"\n)\n\n// ScrapeEngine manages target discovery and metric collection\ntype ScrapeEngine struct {\n    client   *http.Client\n    targets  map[string]*Target\n    // TODO: Add fields for worker pools, scheduling, health tracking\n}\n\n// NewScrapeEngine creates a new scrape engine\nfunc NewScrapeEngine() *ScrapeEngine {\n    return &ScrapeEngine{\n        client:  &http.Client{Timeout: DEFAULT_SCRAPE_TIMEOUT},\n        targets: make(map[string]*Target),\n    }\n}\n\n// UpdateTargets refreshes the target list from configuration\nfunc (e *ScrapeEngine) UpdateTargets(configs []StaticConfig) error {\n    // TODO 1: Parse static configs into Target structs with URLs and labels\n    // TODO 2: Compare new targets with existing targets map\n    // TODO 3: Add new targets and remove obsolete targets  \n    // TODO 4: Schedule scraping for new targets\n    // TODO 5: Update target labels for existing targets\n}\n\n// scrapeTarget performs HTTP request to collect metrics from one target\nfunc (e *ScrapeEngine) scrapeTarget(target *Target) error {\n    // TODO 1: Create HTTP GET request with proper headers and timeout\n    // TODO 2: Execute request and handle network errors\n    // TODO 3: Read response body and validate HTTP status code\n    // TODO 4: Parse metrics from response body using exposition format\n    // TODO 5: Send parsed metrics to storage engine\n    // TODO 6: Update target health status based on success/failure\n}\n```\n\n```go\n// internal/storage/engine.go  \npackage storage\n\nimport (\n    \"time\"\n)\n\n// StorageEngine manages time series persistence and retrieval\ntype StorageEngine struct {\n    dataDir   string\n    retention time.Duration\n    // TODO: Add fields for WAL, indexes, compression, chunks\n}\n\n// NewStorageEngine creates a storage engine\nfunc NewStorageEngine(config StorageConfig) *StorageEngine {\n    // TODO: Initialize storage directories, WAL, indexes\n}\n\n// Append adds new samples to time series\nfunc (e *StorageEngine) Append(samples []Sample) error {\n    // TODO 1: Validate samples have valid timestamps and metric names\n    // TODO 2: Write samples to WAL for durability\n    // TODO 3: Group samples by time series (metric name + labels)\n    // TODO 4: Add samples to appropriate time series chunks\n    // TODO 5: Update inverted indexes for new label combinations\n    // TODO 6: Trigger compression for completed chunks\n}\n\n// Select retrieves time series data matching label selectors\nfunc (e *StorageEngine) Select(start, end time.Time, matchers []LabelMatcher) (SeriesSet, error) {\n    // TODO 1: Use inverted indexes to find series matching label selectors\n    // TODO 2: Determine which chunks overlap with [start, end] time range  \n    // TODO 3: Read and decompress relevant chunks from disk\n    // TODO 4: Filter samples to exact time range\n    // TODO 5: Return SeriesSet iterator over matching time series\n}\n```\n\n#### Language-Specific Hints\n\n**Go-Specific Implementation Tips:**\n\n- Use `sync.RWMutex` for concurrent access to the target map in the scrape engine\n- Implement graceful shutdown using `context.Context` and `sync.WaitGroup` for worker goroutines  \n- Use `time.Ticker` for scrape scheduling rather than `time.Sleep` in loops\n- Apply `os.File.Sync()` after WAL writes to ensure durability\n- Use `encoding/binary` for efficient serialization of timestamps and float values\n- Implement the `sort.Interface` for time series to enable efficient range queries\n- Use `regexp.Compile` once at startup for label regex matchers, not on every query\n\n**Error Handling Patterns:**\n\n- Wrap errors with context using `fmt.Errorf(\"scraping target %s: %w\", url, err)`\n- Use sentinel errors like `var ErrTargetTimeout = errors.New(\"target timeout\")` for specific failures\n- Implement retry logic with exponential backoff for transient network failures\n- Log errors at appropriate levels: network timeouts as warnings, configuration errors as errors\n\n#### Milestone Checkpoints\n\n**After Milestone 1 (Metrics Data Model):**\n- Run: `go test ./internal/metrics/...` \n- Expected: All metric type tests pass, Counter increases monotonically, Gauge accepts any value\n- Manual verification: Create Counter and Gauge, observe values, confirm label attachment works\n- Success indicator: Can create metrics with labels and retrieve current values\n\n**After Milestone 2 (Scrape Engine):**\n- Run: `go run cmd/collector/main.go --config=configs/example.yaml`\n- Expected: See periodic scrape logs, target health status updates\n- Manual verification: Configure a target, observe HTTP requests in target logs\n- Success indicator: Metrics are successfully parsed from HTTP endpoints\n\n**After Milestone 3 (Time Series Storage):**  \n- Run: `go test ./internal/storage/... -v`\n- Expected: Compression tests show >10x space reduction, retention policies delete old data\n- Manual verification: Write samples, restart process, verify data persists\n- Success indicator: Data survives restarts and compression reduces storage size\n\n**After Milestone 4 (Query Engine):**\n- Run: Query API endpoint: `curl \"http://localhost:9090/api/v1/query?query=up\"`\n- Expected: JSON response with metric values and timestamps  \n- Manual verification: Try range queries, aggregation functions, label filtering\n- Success indicator: PromQL queries return correct results matching stored data\n\n\n## Metrics Data Model\n\n> **Milestone(s):** This section directly corresponds to Milestone 1 (Metrics Data Model) and establishes the foundational data structures that will be used throughout Milestones 2-4.\n\nThe metrics data model forms the conceptual and technical foundation upon which the entire monitoring system is built. Think of it as the vocabulary and grammar of our observability language - just as human language needs nouns, verbs, and sentence structure to convey meaning, our metrics system needs well-defined data types, labeling semantics, and identification rules to effectively communicate system behavior over time.\n\nUnderstanding the metrics data model is crucial because every decision made here ripples through the entire system architecture. The choice of metric types determines what kinds of mathematical operations the query engine can perform. The labeling system directly impacts storage cardinality and memory usage. The time series identity model affects indexing strategies and query performance. Get these fundamentals wrong, and the entire system becomes either unusable due to poor performance or unreliable due to semantic inconsistencies.\n\n![Metrics Data Model](./diagrams/metrics-data-model.svg)\n\n### Metric Type Semantics\n\nThink of metric types as different kinds of measuring instruments in a scientific laboratory. A thermometer measures absolute temperature at a point in time (like a gauge), while a Geiger counter accumulates radiation exposure over time (like a counter). Each instrument has specific mathematical properties that determine what calculations make sense - you can subtract two temperature readings to find a delta, but subtracting Geiger counter readings would be meaningless without considering the time periods involved.\n\nOur metrics system supports four fundamental metric types, each with distinct semantic behaviors that enable different kinds of analysis and alerting patterns.\n\n> **Decision: Four-Type Metric Model**\n> - **Context**: Need to support diverse monitoring use cases from simple resource metrics to complex latency distributions while maintaining clear semantic boundaries\n> - **Options Considered**: \n>   1. Simple gauge-only model (Graphite-style)\n>   2. Four-type model (Counter, Gauge, Histogram, Summary)\n>   3. Extended model with additional types (Sets, Traces)\n> - **Decision**: Four-type model matching Prometheus specification\n> - **Rationale**: Balances expressiveness with implementation complexity, provides clear semantic guarantees for rate calculations and aggregations, widely understood by practitioners\n> - **Consequences**: Query engine must understand type-specific operations, storage must preserve type information, scraping must validate type consistency\n\n| Metric Type | Mathematical Properties | Use Cases | Invalid Operations |\n|-------------|------------------------|-----------|-------------------|\n| `Counter` | Monotonically increasing, resets to zero | Request counts, error counts, bytes transferred | Decrease operations, negative values, arbitrary sets |\n| `Gauge` | Arbitrary value, can increase/decrease | CPU usage, memory usage, queue depth, temperature | Rate calculations without smoothing |\n| `Histogram` | Distribution with predefined buckets | Request latency, response sizes, batch sizes | Bucket boundary changes after creation |\n| `Summary` | Distribution with calculated quantiles | Similar to histogram but client-calculated percentiles | Server-side aggregation across instances |\n\n#### Counter Semantics and Behavior\n\n**Counters** represent cumulative totals that only increase over time, with the critical exception of resets to zero when the monitored process restarts. The mathematical foundation of counters enables powerful rate calculations - the derivative of a counter represents the instantaneous rate of change, which is often more meaningful than the absolute value for operational monitoring.\n\nThe semantic contract of a `Counter` includes several critical guarantees that both the client instrumenting code and the query engine must respect. First, values must never decrease except during resets to zero. Second, counter resets must be detectable by the monitoring system to avoid incorrect rate calculations. Third, the rate of change is more operationally significant than the absolute value in most use cases.\n\n| Counter Operation | Input | Behavior | Semantic Guarantee |\n|------------------|-------|----------|-------------------|\n| `Inc()` | None | Increment by 1 | Value increases monotonically |\n| `Add(value)` | Positive number | Increment by value | Rejects negative values |\n| `Reset()` | None | Set to zero | Marks reset boundary for rate calculations |\n| `Value()` | None | Return current total | Read-only access to current state |\n\nCounter resets present a particular challenge for rate calculations. When a process restarts, the counter begins again from zero, creating an artificial negative spike in the rate calculation if handled naively. The storage and query engine must detect these reset conditions and handle them appropriately by treating the first sample after a reset as a new baseline rather than calculating a rate from the previous higher value.\n\nConsider a web server request counter that increments with each HTTP request. Over one hour, it increases from 1000 to 1500 requests, indicating a rate of 500 requests per hour. If the server restarts and the counter resets to zero, then increases to 100 over the next hour, the rate should be calculated as 100 requests per hour, not as -1400 requests per hour based on the naive difference from the pre-restart value.\n\n> The fundamental insight with counters is that the absolute value is rarely interesting - you care about the rate of change over time. A counter showing 1,847,392 total requests tells you nothing actionable, but knowing that requests are arriving at 50 per second tells you everything about current system load.\n\n#### Gauge Semantics and Flexibility\n\n**Gauges** represent point-in-time measurements that can fluctuate arbitrarily in either direction. Unlike counters, gauges have no mathematical constraints on their values - they can increase, decrease, or remain constant between observations. This flexibility makes them suitable for representing resource levels, percentages, temperatures, and any other measurement where the current absolute value is meaningful.\n\nThe semantic contract of a `Gauge` emphasizes current state over historical accumulation. Operations focus on setting, adjusting, and observing the current value rather than accumulating changes over time. This distinction is crucial for query operations - while rate calculations on gauges are mathematically valid, they often require smoothing or windowing to be operationally useful due to the potentially noisy nature of gauge values.\n\n| Gauge Operation | Input | Behavior | Use Case |\n|----------------|-------|----------|----------|\n| `Set(value)` | Any number | Replace current value | Setting absolute measurements |\n| `Inc()` | None | Increment by 1 | Simple upward adjustments |\n| `Dec()` | None | Decrement by 1 | Simple downward adjustments |\n| `Add(value)` | Any number (including negative) | Adjust by delta | Relative adjustments |\n| `Value()` | None | Return current value | Current state queries |\n\nGauges excel at representing system state that fluctuates around operational ranges. CPU utilization naturally varies between 0% and 100%, memory usage grows and shrinks with allocation patterns, and queue depths rise and fall with load patterns. These measurements have meaningful absolute values at any point in time, unlike counters where only the rate of change provides operational insight.\n\nConsider monitoring database connection pool usage. The gauge might show 15 active connections out of a maximum 50, providing immediate insight into resource utilization. As queries complete and new requests arrive, the gauge fluctuates, with both the current absolute value (15 connections) and the trend over time (increasing, decreasing, or stable) providing valuable operational information.\n\n#### Histogram Design and Bucket Strategy\n\n**Histograms** capture the distribution of observed values by maintaining counts in predefined buckets, enabling calculation of percentiles, averages, and distribution shapes without storing individual samples. This aggregation approach provides powerful statistical insights while maintaining bounded memory usage regardless of observation volume.\n\nThe core design challenge with histograms lies in choosing appropriate bucket boundaries. These boundaries must be defined when the histogram is created and cannot be changed later, as the bucketing strategy affects all subsequent statistical calculations. The bucket boundaries determine the precision and range of percentile calculations - finer buckets provide higher precision but consume more storage, while coarser buckets reduce storage at the cost of statistical accuracy.\n\n| Histogram Component | Purpose | Storage Requirement | Query Capability |\n|-------------------|---------|-------------------|------------------|\n| Bucket counters | Count observations in each range | One counter per bucket | Percentile estimation, distribution shape |\n| Total count | Count all observations | One counter | Sample count for averaging |\n| Total sum | Sum all observed values | One counter | Mean calculation |\n| Bucket boundaries | Define ranges for categorization | Metadata only | Query planning and validation |\n\nThe histogram bucket strategy directly impacts the accuracy of percentile calculations. Consider measuring HTTP response latency with buckets at [0.1s, 0.5s, 1.0s, 2.0s, 5.0s, +Inf]. This configuration provides good resolution for typical web response times but would poorly serve a system where most responses complete in microseconds. The query engine estimates percentiles using bucket interpolation - if the 95th percentile falls within the 1.0s-2.0s bucket, the exact value is estimated based on the distribution assumption within that range.\n\n> **Decision: Cumulative Histogram Buckets**\n> - **Context**: Need efficient percentile calculations while supporting aggregation across multiple instances\n> - **Options Considered**:\n>   1. Individual bucket counts (each bucket independent)\n>   2. Cumulative bucket counts (each bucket includes all smaller values)\n>   3. Sparse histogram with dynamic bucket boundaries\n> - **Decision**: Cumulative bucket counts following Prometheus model\n> - **Rationale**: Enables efficient percentile calculation via bucket search, supports mathematical aggregation across instances, simplifies query engine implementation\n> - **Consequences**: Slightly more complex bucket increment logic, but dramatically simplified percentile queries and cross-instance aggregation\n\nHistogram aggregation across multiple instances provides one of the most powerful features of this metric type. Unlike gauges or counters where cross-instance aggregation requires careful consideration of meaning (average of averages vs. total counts), histogram buckets aggregate naturally - the sum of bucket counts across instances represents the combined distribution. This property enables fleet-wide latency analysis and capacity planning based on aggregate behavior patterns.\n\n#### Summary Metrics and Client-Side Quantiles\n\n**Summaries** provide an alternative approach to distribution tracking by calculating quantiles (percentiles) on the client side and transmitting pre-calculated statistical values rather than bucket counts. This approach trades some query flexibility for reduced bandwidth and storage requirements, particularly valuable in high-volume environments where network efficiency is paramount.\n\nThe fundamental difference between summaries and histograms lies in where the statistical calculation occurs. Histograms preserve the raw distribution information (via bucket counts) and calculate percentiles during query execution, while summaries calculate percentiles during observation and store only the results. This trade-off has profound implications for aggregation capabilities and query flexibility.\n\n| Summary Component | Purpose | Client Calculation | Server Storage |\n|------------------|---------|-------------------|----------------|\n| Quantile values | Pre-calculated percentiles (e.g., 0.5, 0.9, 0.99) | Sliding window quantile estimation | Direct storage of calculated values |\n| Total count | Count all observations | Simple counter increment | Counter storage and rate calculations |\n| Total sum | Sum all observed values | Running sum accumulation | Mean calculation support |\n| Observation window | Time window for quantile calculation | Sliding window management | Window metadata only |\n\nThe critical limitation of summaries becomes apparent during aggregation operations. Since quantiles are non-additive mathematical functions, combining the 95th percentile from multiple instances does not yield the fleet-wide 95th percentile. If instance A reports a 95th percentile latency of 500ms and instance B reports 750ms, the combined fleet 95th percentile could be anywhere from 500ms to 750ms (or even outside this range) depending on the distribution of requests across instances.\n\n> Summaries excel in bandwidth-constrained environments where you need specific quantiles from individual instances but don't require fleet-wide distribution analysis. Histograms provide superior query flexibility and aggregation capabilities at the cost of higher bandwidth and storage requirements.\n\n### Multi-Dimensional Labeling\n\nThink of labels as the coordinate system that transforms flat metrics into a multi-dimensional space where you can slice, dice, and aggregate data along any combination of dimensions. Just as a GPS coordinate becomes meaningful only when you know it refers to latitude and longitude, a metric value becomes operationally useful only when you know the service, environment, region, and other contextual dimensions that produced it.\n\nLabels enable the transformation from simple time series (metric_name -> value over time) to multi-dimensional time series (metric_name{label1=value1, label2=value2} -> value over time). This dimensionality is what makes modern monitoring systems powerful - instead of creating separate metrics for each combination of conditions, you create one metric with appropriate labels and query across dimensions.\n\nThe labeling system must balance expressiveness with performance. Each unique combination of label values creates a distinct time series, directly impacting memory usage, storage requirements, and query performance. Understanding this relationship is crucial for designing sustainable monitoring instrumentation that scales with system complexity.\n\n#### Label Structure and Naming Conventions\n\nLabels consist of key-value pairs where both keys and values are strings, attached to metric observations to provide dimensional context. The label key represents the dimension name (such as \"method\", \"status_code\", or \"region\") while the label value represents the specific instance of that dimension (such as \"GET\", \"200\", or \"us-west-2\").\n\nLabel naming conventions significantly impact long-term maintainability and query ergonomics. Well-chosen label names create intuitive query patterns and support natural aggregation operations, while poor label naming leads to confusion and complex query logic. The naming strategy should reflect the operational questions you need to answer rather than the technical implementation details of how metrics are collected.\n\n| Label Category | Examples | Purpose | Cardinality Impact |\n|---------------|----------|---------|-------------------|\n| Service Identity | `service`, `instance`, `job` | Identify metric source | Linear with service count |\n| Request Context | `method`, `status_code`, `endpoint` | Categorize individual operations | Multiplicative across dimensions |\n| Infrastructure | `region`, `availability_zone`, `datacenter` | Physical deployment context | Linear with infrastructure diversity |\n| Application State | `version`, `environment`, `feature_flag` | Application configuration context | Linear with deployment variations |\n\nThe hierarchical nature of many label dimensions enables powerful aggregation patterns. Consider HTTP request metrics labeled with `{service=\"api\", method=\"GET\", endpoint=\"/users\", status_code=\"200\"}`. You can aggregate across all endpoints to see service-level request rates, across all status codes to see endpoint-level traffic patterns, or across all methods to analyze API endpoint popularity. Each aggregation operation reduces dimensionality while preserving the ability to drill down into specific label combinations.\n\nLabel value consistency across the system requires careful coordination between instrumentation and operational practices. The same conceptual entity must use identical label values across all metrics - a service identified as \"user-service\" in one metric and \"userservice\" in another creates artificial separation in queries and dashboards. This consistency extends beyond naming to include value normalization (lowercase vs. uppercase, hyphen vs. underscore) and handling of dynamic values (user IDs, request IDs) that create unbounded cardinality.\n\n#### Cardinality Mathematics and Memory Impact\n\nLabel cardinality represents the number of unique time series created by all possible combinations of label values for a given metric. Understanding cardinality mathematics is essential for predicting memory usage, storage requirements, and query performance as the system scales.\n\nThe cardinality of a metric equals the cartesian product of all label value sets. A metric with labels `{service, method, status_code}` where service has 10 possible values, method has 4 values, and status_code has 15 values creates a maximum cardinality of 10 × 4 × 15 = 600 unique time series. In practice, not all combinations may exist (some services might not support all methods), but the maximum provides the upper bound for capacity planning.\n\n| Cardinality Factor | Low Impact (1-10 values) | Medium Impact (10-100 values) | High Impact (100+ values) | Unbounded (avoid) |\n|-------------------|-------------------------|----------------------------|--------------------------|------------------|\n| Examples | HTTP methods, status code classes | Status codes, service names | Instance IDs, container names | User IDs, request IDs, timestamps |\n| Memory per series | ~1KB baseline + samples | ~1KB baseline + samples | ~1KB baseline + samples | Unbounded growth |\n| Query performance | Minimal impact | Linear degradation | Significant index overhead | System instability |\n| Storage growth | Predictable | Manageable with planning | Requires careful monitoring | Leads to system failure |\n\nThe memory impact of cardinality extends beyond simple multiplication due to indexing overhead. Each unique time series requires index entries for fast lookup during queries, and these indexes must support efficient filtering across multiple label dimensions simultaneously. The storage engine maintains inverted indexes mapping label values to time series identifiers, creating memory overhead that scales with both cardinality and label diversity.\n\n![Label Cardinality Impact](./diagrams/label-cardinality.svg)\n\nConsider a real-world example: instrumenting HTTP request duration with labels for service, method, endpoint, and status_code. With 5 services, 4 HTTP methods, 20 endpoints per service, and 10 status codes, the theoretical maximum cardinality is 5 × 4 × 20 × 10 = 4,000 time series. If each time series consumes approximately 1KB of memory for metadata plus sample storage, this single metric could consume 4MB of memory just for the index structures, before considering the actual time series data.\n\n> **Decision: Label Cardinality Limits**\n> - **Context**: Need to prevent unbounded memory growth while supporting necessary operational dimensions\n> - **Options Considered**:\n>   1. No limits (trust users to instrument responsibly)\n>   2. Hard limits per metric (e.g., max 1000 series per metric name)\n>   3. Soft limits with warnings and graduated enforcement\n> - **Decision**: Soft limits with configurable enforcement thresholds\n> - **Rationale**: Provides safety against cardinality explosion while allowing legitimate high-cardinality use cases with explicit acknowledgment\n> - **Consequences**: Requires monitoring of cardinality growth, adds complexity to ingestion pipeline, enables sustainable scaling\n\n#### Label Best Practices and Anti-Patterns\n\nEffective label design requires understanding the difference between dimensions that add operational value and those that add only noise. The goal is to create labelsets that enable meaningful aggregation and filtering operations while maintaining reasonable cardinality bounds.\n\nHigh-value labels represent dimensions along which you regularly need to aggregate, filter, or alert. These typically correspond to operational boundaries (services, environments, regions) or request characteristics that affect system behavior (HTTP methods, cache hit/miss status, error types). Low-value labels often represent implementation details that don't align with operational questions or create unnecessarily high cardinality without proportional insight.\n\n| Pattern Type | Good Practice | Anti-Pattern | Impact |\n|--------------|---------------|--------------|---------|\n| Service Identity | `{service=\"user-api\", environment=\"prod\"}` | `{hostname=\"server-17-prod-usw2.internal\"}` | Service focus vs. infrastructure focus |\n| Request Classification | `{method=\"GET\", status_class=\"2xx\"}` | `{full_url=\"/api/users/12345/profile\"}` | Bounded vs. unbounded cardinality |\n| Error Categorization | `{error_type=\"timeout\", subsystem=\"database\"}` | `{error_message=\"connection refused to 10.0.0.1:5432\"}` | Actionable categories vs. specific instances |\n| Version Tracking | `{version=\"1.2.3\", deployment_id=\"abc123\"}` | `{build_timestamp=\"2023-10-15T14:30:22Z\"}` | Discrete versions vs. continuous values |\n\nThe temporal aspect of label values requires special consideration. Labels that change frequently create natural time series boundaries - when a label value changes, a new time series begins and the old one effectively ends. This behavior is correct and desired for legitimate operational dimensions (like application version during deployments) but problematic for high-frequency changes (like current timestamp or active user count).\n\nCommon anti-patterns include using user IDs, request IDs, or timestamps as label values. These create unbounded cardinality that grows continuously with system usage rather than stabilizing at a level proportional to system complexity. Instead, these high-cardinality identifiers should be either excluded from metrics entirely or aggregated into bounded categories (e.g., user_type instead of user_id, request_size_bucket instead of request_id).\n\n> The litmus test for label appropriateness is simple: \"Will I ever want to aggregate or filter metrics along this dimension?\" If you can't imagine writing a query that groups by or filters on a label, it probably shouldn't be a label.\n\n### Time Series Identity\n\nThink of time series identity as the unique \"address\" that allows the storage and query engines to locate specific metric streams within the vast multidimensional space of all possible measurements. Just as a postal address must uniquely identify a specific building, a time series identity must uniquely identify a specific sequence of timestamped values among potentially millions of similar sequences.\n\nThe time series identity model determines how the system partitions the continuous stream of metric observations into discrete, queryable sequences. This partitioning directly affects storage layout, query performance, and cardinality management. Understanding identity semantics is crucial because it defines the granularity at which the system can filter, aggregate, and analyze metric data.\n\n#### Identity Composition and Uniqueness\n\nA time series identity consists of the metric name combined with the complete set of label key-value pairs. Two time series are considered identical if and only if they have the same metric name and exactly the same set of labels with exactly the same values. Any difference in metric name, label keys, or label values creates a distinct time series identity.\n\nThis strict equality requirement has important implications for instrumentation consistency. A metric observation with labels `{service=\"api\", method=\"GET\"}` creates a different time series than an observation with labels `{method=\"GET\", service=\"api\"}` even though the label content is semantically identical - the system treats these as separate identities. Most implementations normalize label ordering to avoid this pitfall, but the fundamental principle remains: identity requires exact matching.\n\n| Identity Component | Contribution | Example | Uniqueness Impact |\n|-------------------|--------------|---------|-------------------|\n| Metric name | Primary classification | `http_requests_total` | Separates different measurement types |\n| Label keys | Dimensional structure | `method`, `status_code` | Defines available aggregation dimensions |\n| Label values | Specific instances | `GET`, `200` | Creates actual time series instances |\n| Label ordering | Normalized during ingestion | Consistent regardless of input order | Implementation detail, not semantic |\n\nThe mathematical relationship between labels and time series count becomes clear through the identity model. Each unique combination of label values creates one time series identity. If you have a metric `http_requests_total{method, status_code}` and observe requests with methods [GET, POST] and status codes [200, 404, 500], you create 2 × 3 = 6 distinct time series identities: `{method=\"GET\", status_code=\"200\"}`, `{method=\"GET\", status_code=\"404\"}`, `{method=\"GET\", status_code=\"500\"}`, `{method=\"POST\", status_code=\"200\"}`, `{method=\"POST\", status_code=\"404\"}`, `{method=\"POST\", status_code=\"500\"}`.\n\n#### Identity Lifecycle and Creation\n\nTime series identities come into existence dynamically as the system observes new combinations of metric names and label values. Unlike traditional databases where schema defines the available table and column structure upfront, metrics systems create new time series identities on-demand as applications emit previously unseen labelset combinations.\n\nThis dynamic creation model provides tremendous flexibility for evolving systems - new services, endpoints, or operational dimensions automatically create appropriate time series without schema migration or configuration changes. However, it also creates the risk of cardinality explosion if instrumentation code generates unbounded label values or fails to normalize label naming.\n\n| Lifecycle Stage | Trigger | System Action | Performance Impact |\n|----------------|---------|---------------|-------------------|\n| Creation | First sample with new identity | Allocate storage, create index entries | Memory allocation, index update overhead |\n| Active | Ongoing sample ingestion | Append samples to existing series | Minimal per-sample overhead |\n| Inactive | No samples for retention period | Mark for deletion, preserve for queries | Index overhead without storage growth |\n| Deletion | Retention policy expiration | Remove from storage and indexes | Memory reclamation, index cleanup |\n\nThe moment of time series creation represents the highest overhead in the identity lifecycle. The storage engine must allocate memory structures, create index entries mapping labels to series identifiers, and update various metadata structures to track the new series. This creation overhead motivates batching strategies where multiple samples for the same identity are grouped together during ingestion.\n\nConsider the lifecycle of a time series tracking HTTP requests for a new API endpoint. The first request to `/api/users` with method GET creates the time series identity `http_requests_total{method=\"GET\", endpoint=\"/api/users\"}`. Subsequent requests to the same endpoint with the same method append samples to this existing time series. If the endpoint is later deprecated and receives no traffic, the time series becomes inactive but remains queryable for historical analysis until the retention policy removes it entirely.\n\n#### Identity Normalization and Canonical Form\n\nTo ensure consistent identity matching across system components, the metrics system must establish a canonical form for time series identities. This normalization process converts various equivalent representations into a single, standard format that enables reliable identity comparison and lookup operations.\n\nLabel ordering normalization represents the most common identity canonicalization requirement. Since labels are conceptually an unordered set of key-value pairs, the system must establish a consistent ordering (typically lexicographic by key name) to ensure that `{method=\"GET\", service=\"api\"}` and `{service=\"api\", method=\"GET\"}` resolve to the same canonical identity.\n\n| Normalization Type | Input Variation | Canonical Form | Purpose |\n|-------------------|-----------------|----------------|---------|\n| Label ordering | `{b=\"2\", a=\"1\"}` | `{a=\"1\", b=\"2\"}` | Consistent identity hashing and comparison |\n| Label key casing | `{Method=\"GET\"}` | `{method=\"GET\"}` | Case-insensitive label key matching |\n| Value whitespace | `{status=\" 200 \"}` | `{status=\"200\"}` | Trim accidental whitespace |\n| Empty labels | `{method=\"GET\", unused=\"\"}` | `{method=\"GET\"}` | Remove labels with empty values |\n\nThe canonical identity form enables efficient storage and lookup operations through consistent hashing. The storage engine can hash the canonical identity string to determine storage location, index bucket, and cache keys without worrying about equivalent representations creating different hash values. This consistency is crucial for performance as the system scales to millions of time series.\n\nIdentity normalization must balance consistency with preservation of meaningful distinctions. While trimming whitespace from label values usually represents error correction, case-sensitive label values might be semantically important (distinguishing between SQL table names \"Users\" and \"users\" in case-sensitive databases). The normalization rules should reflect the operational reality of the monitored systems rather than imposing arbitrary formatting requirements.\n\n> **Decision: Strict Identity Immutability**\n> - **Context**: Need to ensure consistent time series identification across storage, indexing, and query operations\n> - **Options Considered**:\n>   1. Mutable identities (allow label value changes for existing series)\n>   2. Immutable identities (label changes create new time series)\n>   3. Partial mutability (allow changes to designated \"mutable\" labels)\n> - **Decision**: Strict immutability - any label change creates a new time series\n> - **Rationale**: Simplifies storage engine design, ensures query consistency, prevents data corruption from identity conflicts\n> - **Consequences**: Application label changes require explicit time series migration, but system behavior remains predictable and reliable\n\n### Cardinality Control\n\nThink of cardinality control as the immune system of the metrics infrastructure - it protects the overall system health by identifying and containing threats to stability before they can cause widespread damage. Just as a biological immune system must distinguish between beneficial bacteria and harmful pathogens, cardinality control must differentiate between legitimate high-dimensional metrics and pathological cardinality explosions.\n\nUncontrolled cardinality growth represents one of the most common causes of metrics system failure in production environments. A single misbehaving application or poorly designed instrumentation can generate millions of unique time series in minutes, consuming all available memory and rendering the entire monitoring system unusable. Effective cardinality control requires both preventive measures (limits and validation) and reactive measures (detection and mitigation).\n\n#### Cardinality Explosion Detection\n\nCardinality explosion typically manifests as rapid, unexpected growth in the number of unique time series, often accompanied by degraded query performance and memory pressure. Early detection requires monitoring the rate of new time series creation and identifying patterns that indicate problematic instrumentation rather than legitimate system growth.\n\nThe challenge in explosion detection lies in distinguishing between normal system evolution (new services, features, or infrastructure) and pathological growth (unbounded label values, instrumentation bugs). Normal growth typically correlates with planned deployments or infrastructure changes and exhibits predictable patterns. Pathological growth often appears sudden, accelerating, and disproportionate to actual system complexity changes.\n\n| Detection Signal | Normal Growth Pattern | Explosion Pattern | Response Required |\n|-----------------|----------------------|------------------|-------------------|\n| New series rate | Gradual, correlated with deployments | Sudden spike, continuously accelerating | Immediate investigation and mitigation |\n| Label value diversity | Bounded growth in known dimensions | New dimensions or unbounded values | Label audit and validation |\n| Memory usage growth | Linear with feature complexity | Exponential growth unrelated to features | Emergency cardinality limiting |\n| Query performance | Stable or gradually degrading | Rapid degradation, timeout increases | Query load balancing and limiting |\n\nAutomated detection systems should monitor both absolute cardinality levels and growth rates across multiple time horizons. A metric that creates 1000 new time series in one minute might represent normal behavior for a high-traffic service during deployment, but the same rate sustained over an hour indicates a serious problem requiring immediate intervention.\n\nThe distribution of cardinality across metrics provides additional detection signals. In healthy systems, most metrics have relatively low cardinality (10-100 time series), with a few high-cardinality metrics (1000+ series) that represent well-understood, business-critical dimensions. An explosion often manifests as a single metric suddenly dominating the cardinality budget, indicating a specific instrumentation problem rather than general system growth.\n\n#### Enforcement Strategies and Policies\n\nEffective cardinality enforcement requires a graduated response system that can provide early warnings, impose soft limits during normal operation, and implement hard limits during emergency conditions. This approach balances the need for operational safety with the flexibility to support legitimate high-cardinality use cases when properly justified and monitored.\n\nSoft enforcement mechanisms focus on visibility and warnings rather than blocking operations. These approaches work well during normal operations when development teams can respond to notifications and adjust instrumentation practices. Hard enforcement mechanisms prioritize system stability over metric completeness, appropriate during crisis situations where the monitoring system's survival takes precedence over comprehensive data collection.\n\n| Enforcement Level | Trigger Conditions | Actions Taken | Operational Impact |\n|------------------|-------------------|---------------|-------------------|\n| Warning | 50% of cardinality budget used | Log warnings, send alerts | No impact on data collection |\n| Soft limiting | 80% of cardinality budget used | Rate limit new series creation | Delayed ingestion, potential data loss |\n| Hard limiting | 95% of cardinality budget used | Reject new series, shed existing series | Guaranteed data loss, preserved system stability |\n| Emergency mode | Memory pressure or system instability | Aggressive series eviction, ingestion throttling | Significant data loss, system preservation |\n\nThe enforcement policy must define clear cardinality budgets and allocation strategies across different metric types and operational contexts. Production services might receive larger cardinality budgets than development environments, and business-critical metrics might be protected from enforcement actions that could affect their availability during incidents.\n\nSample-based enforcement provides a middle ground between complete rejection and unlimited acceptance. When soft limits are exceeded, the system can randomly sample new time series creation, preserving statistical representativeness while controlling absolute cardinality growth. This approach works particularly well for metrics where complete enumeration isn't required for operational insight.\n\n> **Decision: Hierarchical Cardinality Budgets**\n> - **Context**: Need to prevent system-wide cardinality explosion while allowing different services and metrics to have different cardinality requirements\n> - **Options Considered**:\n>   1. Global cardinality limit (single system-wide limit)\n>   2. Per-metric cardinality limits (each metric has independent limit)\n>   3. Hierarchical budgets (service -> metric -> label dimension limits)\n> - **Decision**: Hierarchical budgets with inheritance and override capabilities\n> - **Rationale**: Provides granular control while enabling reasonable defaults, supports organizational responsibility boundaries, enables emergency override for critical metrics\n> - **Consequences**: More complex configuration and monitoring, but much better operational control and blast radius limitation\n\n#### Label Value Validation and Sanitization\n\nProactive label value validation provides the first line of defense against cardinality explosion by identifying and rejecting problematic label values before they create persistent time series. Effective validation requires understanding common patterns that lead to unbounded cardinality and implementing sanitization rules that preserve operational meaning while enforcing cardinality bounds.\n\nHigh-risk label patterns include sequential identifiers (user IDs, request IDs, timestamps), unbounded categorical values (error messages, URLs with parameters), and accidentally dynamic values (configuration changes, memory addresses). Validation rules should detect these patterns and either reject the labels entirely or transform them into bounded categories.\n\n| Validation Rule | Pattern Detected | Action Taken | Example |\n|----------------|------------------|--------------|---------|\n| Sequential numbers | Label values matching `/^\\d+$/` | Reject or convert to ranges | user_id=\"12345\" → user_type=\"premium\" |\n| Timestamp values | ISO timestamp patterns | Extract time component | timestamp=\"2023-10-15T14:30:22Z\" → hour=\"14\" |\n| URL paths | HTTP URL patterns with parameters | Extract endpoint pattern | path=\"/users/123/profile\" → endpoint=\"/users/{id}/profile\" |\n| Error messages | Long strings with variable content | Extract error category | message=\"Connection failed to 10.0.0.1\" → error_type=\"connection_failed\" |\n| Excessive length | Label values over N characters | Truncate or reject | Very long values usually indicate misuse |\n\nLabel value allowlists and denylists provide explicit control over acceptable values for high-risk dimensions. Critical operational labels like service names, environments, or regions benefit from explicit allowlists that prevent typos and unauthorized values from creating unexpected cardinality. Dynamic labels that are known to be problematic can be explicitly blocked through denylists.\n\nThe sanitization process must balance data preservation with cardinality control. Overly aggressive sanitization can remove legitimate operational dimensions, reducing the system's monitoring effectiveness. Conversely, insufficient sanitization allows cardinality explosions that threaten system stability. The validation rules should be tunable based on operational experience and regularly reviewed as instrumentation practices evolve.\n\n#### Memory Management and Series Eviction\n\nWhen cardinality control measures fail to prevent memory pressure, the system must implement series eviction strategies that preserve the most operationally valuable time series while reclaiming memory from less critical ones. Effective eviction requires understanding the relative importance of different time series and implementing policies that maintain monitoring effectiveness during resource constraints.\n\nEviction strategies must consider both recency and operational importance when selecting series for removal. Recently active time series are more likely to be relevant for current operational questions, but historical data for critical business metrics may be more valuable than recent data for debugging metrics. The eviction policy should reflect organizational priorities and monitoring use cases.\n\n| Eviction Strategy | Selection Criteria | Advantages | Disadvantages |\n|------------------|-------------------|------------|---------------|\n| Least Recently Used (LRU) | Time since last sample | Simple to implement, preserves active series | May evict important historical data |\n| Least Frequently Used (LFU) | Sample count over time window | Preserves high-volume series | Complex tracking, biased toward noisy metrics |\n| Priority-based | Explicit priority labels or metric patterns | Aligns with business priorities | Requires manual priority assignment |\n| Random eviction | Random selection among candidates | Unbiased, statistically representative | May evict critical series by chance |\n\nThe eviction process should be gradual and observable to prevent sudden monitoring capability loss during incidents. Aggressive eviction during a production outage could remove exactly the time series needed for root cause analysis, creating a double failure where the monitoring system fails simultaneously with the monitored system.\n\nMemory management extends beyond series eviction to include sample retention policies within individual time series. High-cardinality metrics might benefit from shorter sample retention periods, preserving the ability to create new time series while limiting the historical depth available for queries. This approach maintains monitoring coverage while managing memory growth over time.\n\n> ⚠️ **Pitfall: Emergency Eviction During Incidents**\n> During production incidents, memory pressure often triggers aggressive eviction exactly when monitoring data is most critical. Design eviction policies to preserve incident-relevant metrics (error rates, latency, resource usage) even under extreme memory pressure. Consider pre-defining \"incident mode\" eviction policies that protect critical operational metrics at the expense of development or experimental metrics.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: User ID as Labels**\nAdding user IDs, request IDs, or other unbounded identifiers as metric labels creates unlimited cardinality that grows continuously with system usage. A metric labeled with `{user_id=\"user_12345\"}` creates one time series per user, potentially millions of series that consume memory and degrade query performance. Instead, use bounded categorical labels like `{user_type=\"premium\", region=\"us-west\"}` that provide operational insight without explosive cardinality growth.\n\n⚠️ **Pitfall: Inconsistent Label Naming**\nUsing different label names or values for the same conceptual entity across metrics creates artificial separation during queries and aggregation. If one metric uses `{service=\"api\"}` while another uses `{svc=\"api\"}` or `{service=\"API\"}`, queries cannot correlate the metrics without complex label transformation. Establish and enforce consistent label naming conventions across all instrumentation to enable natural metric correlation and aggregation.\n\n⚠️ **Pitfall: Counter Reset Handling**\nFailing to properly handle counter resets leads to incorrect rate calculations that show impossible negative rates or dramatic spikes. When a monitored process restarts, counters reset to zero, but naive rate calculations compare the new zero value against the previous higher value. Implement counter reset detection in the query engine to treat post-reset samples as new baselines rather than continuing from pre-reset values.\n\n⚠️ **Pitfall: Histogram Bucket Changes**\nChanging histogram bucket boundaries after deployment creates inconsistent percentile calculations and breaks historical trend analysis. If you initially configure latency buckets as [0.1s, 0.5s, 1.0s] but later realize you need finer resolution and change to [0.05s, 0.1s, 0.2s, 0.5s, 1.0s], the old and new data become incomparable. Plan histogram bucket boundaries carefully based on expected data distributions and avoid changes that break historical continuity.\n\n⚠️ **Pitfall: High-Cardinality Error Messages**\nUsing complete error messages as label values creates unbounded cardinality since error messages often contain variable information like timestamps, IDs, or network addresses. Instead of `{error=\"Connection timeout to server-17 at 14:32:15\"}`, use categorized error types like `{error_type=\"connection_timeout\", subsystem=\"database\"}` that provide actionable operational insight without cardinality explosion.\n\n### Implementation Guidance\n\nThis implementation guidance provides concrete Go code structures and examples to transform the design concepts into working software. The focus is on creating type-safe, efficient implementations that enforce the semantic guarantees described in the design while providing clear interfaces for the scraping and query engines.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|----------------|\n| Metric Storage | In-memory maps with mutex protection | Lock-free concurrent data structures |\n| Label Validation | Regex-based pattern matching | Compiled finite state automata |\n| Identity Hashing | Standard library SHA-256 | xxhash or similar fast hash functions |\n| Memory Management | Go garbage collector with manual monitoring | Custom memory pools with explicit lifecycle |\n| Serialization | JSON for simplicity | Protocol Buffers for efficiency |\n\n#### Recommended File Structure\n\n```\ninternal/metrics/\n  types.go              ← Core metric type definitions\n  labels.go             ← Label handling and validation\n  identity.go           ← Time series identity management\n  cardinality.go        ← Cardinality control and enforcement\n  registry.go           ← Metric registration and lookup\n  types_test.go         ← Comprehensive type behavior tests\n  examples/\n    instrumentation.go  ← Example usage patterns\n```\n\n#### Core Metric Type Infrastructure\n\n```go\n// Package metrics provides the foundational data model for time series metrics\n// with support for counters, gauges, histograms and multi-dimensional labeling.\npackage metrics\n\nimport (\n    \"fmt\"\n    \"math\"\n    \"sort\"\n    \"sync\"\n    \"time\"\n)\n\n// Sample represents a timestamped value in a time series\ntype Sample struct {\n    Timestamp time.Time `json:\"timestamp\"`\n    Value     float64   `json:\"value\"`\n}\n\n// Labels represents the multi-dimensional label set that identifies a time series.\n// Labels are stored as a sorted slice of key-value pairs for efficient comparison\n// and consistent iteration order.\ntype Labels []LabelPair\n\ntype LabelPair struct {\n    Name  string `json:\"name\"`\n    Value string `json:\"value\"`\n}\n\n// String returns the canonical string representation of the labelset\nfunc (ls Labels) String() string {\n    // TODO 1: Sort labels by name to ensure consistent representation\n    // TODO 2: Format as {name1=\"value1\", name2=\"value2\"} \n    // TODO 3: Escape quotes and backslashes in values\n    // TODO 4: Return empty string for empty labelset\n}\n\n// Hash returns a consistent hash of the labelset for use in maps and indexing\nfunc (ls Labels) Hash() uint64 {\n    // TODO 1: Create canonical string representation\n    // TODO 2: Use a fast hash function (xxhash recommended)\n    // TODO 3: Ensure consistent hash values for equivalent labelsets\n}\n\n// Counter represents a monotonically increasing metric that only goes up\ntype Counter struct {\n    mu    sync.RWMutex\n    value float64\n    // TODO: Add creation timestamp for reset detection\n}\n\n// Inc increments the counter by 1\nfunc (c *Counter) Inc() {\n    // TODO 1: Acquire write lock\n    // TODO 2: Increment value by 1\n    // TODO 3: Release lock\n}\n\n// Add increments the counter by the given value\nfunc (c *Counter) Add(value float64) error {\n    // TODO 1: Validate value is non-negative\n    // TODO 2: Acquire write lock  \n    // TODO 3: Add value to current total\n    // TODO 4: Release lock\n    // TODO 5: Return error for negative values\n}\n\n// Value returns the current counter value\nfunc (c *Counter) Value() float64 {\n    // TODO 1: Acquire read lock\n    // TODO 2: Read current value\n    // TODO 3: Release lock and return value\n}\n\n// Gauge represents a metric that can go up or down arbitrarily\ntype Gauge struct {\n    mu    sync.RWMutex\n    value float64\n}\n\n// Set sets the gauge to the given value\nfunc (g *Gauge) Set(value float64) {\n    // TODO 1: Acquire write lock\n    // TODO 2: Set value to provided input\n    // TODO 3: Release lock\n}\n\n// Inc increments the gauge by 1\nfunc (g *Gauge) Inc() {\n    // TODO 1: Acquire write lock\n    // TODO 2: Increment value by 1\n    // TODO 3: Release lock\n}\n\n// Dec decrements the gauge by 1  \nfunc (g *Gauge) Dec() {\n    // TODO 1: Acquire write lock\n    // TODO 2: Decrement value by 1\n    // TODO 3: Release lock\n}\n\n// Add adds the given value to the gauge (can be negative)\nfunc (g *Gauge) Add(value float64) {\n    // TODO 1: Acquire write lock\n    // TODO 2: Add value to current gauge value\n    // TODO 3: Release lock\n}\n\n// Value returns the current gauge value\nfunc (g *Gauge) Value() float64 {\n    // TODO 1: Acquire read lock\n    // TODO 2: Read current value\n    // TODO 3: Release lock and return value\n}\n\n// Histogram tracks the distribution of observations in predefined buckets\ntype Histogram struct {\n    mu      sync.RWMutex\n    buckets []float64 // Bucket upper bounds (sorted)\n    counts  []uint64  // Observation counts per bucket\n    sum     float64   // Sum of all observed values\n    count   uint64    // Total number of observations\n}\n\n// NewHistogram creates a histogram with the specified bucket boundaries\nfunc NewHistogram(buckets []float64) *Histogram {\n    // TODO 1: Validate buckets are sorted and finite\n    // TODO 2: Add +Inf bucket if not present\n    // TODO 3: Initialize count slice to match bucket count\n    // TODO 4: Return configured histogram\n}\n\n// Observe records an observation in the appropriate bucket\nfunc (h *Histogram) Observe(value float64) {\n    // TODO 1: Acquire write lock\n    // TODO 2: Find appropriate bucket using binary search\n    // TODO 3: Increment bucket count and total count\n    // TODO 4: Add value to running sum\n    // TODO 5: Release lock\n}\n\n// GetBuckets returns the current bucket counts and boundaries\nfunc (h *Histogram) GetBuckets() ([]float64, []uint64) {\n    // TODO 1: Acquire read lock\n    // TODO 2: Copy bucket boundaries and counts to avoid race conditions\n    // TODO 3: Release lock and return copies\n}\n```\n\n#### Label Management and Validation\n\n```go\n// LabelValidator enforces cardinality control and naming conventions\ntype LabelValidator struct {\n    maxLabelLength    int\n    maxValueLength    int\n    allowedPatterns   map[string]*regexp.Regexp\n    forbiddenPatterns []*regexp.Regexp\n}\n\n// NewLabelValidator creates a validator with default rules\nfunc NewLabelValidator() *LabelValidator {\n    // TODO 1: Initialize with reasonable defaults (label length < 64, value length < 256)\n    // TODO 2: Compile common forbidden patterns (sequential IDs, timestamps)\n    // TODO 3: Set up standard allowed patterns for common label names\n    // TODO 4: Return configured validator\n}\n\n// ValidateLabels checks a labelset for cardinality and naming violations\nfunc (lv *LabelValidator) ValidateLabels(labels Labels) error {\n    // TODO 1: Check total number of labels against limit\n    // TODO 2: Validate each label name and value length\n    // TODO 3: Check for forbidden patterns (user IDs, timestamps, etc.)\n    // TODO 4: Verify label names match allowed character sets\n    // TODO 5: Return descriptive errors for violations\n}\n\n// SanitizeLabels attempts to fix common labeling mistakes\nfunc (lv *LabelValidator) SanitizeLabels(labels Labels) Labels {\n    // TODO 1: Trim whitespace from names and values\n    // TODO 2: Convert label names to lowercase\n    // TODO 3: Remove empty label values\n    // TODO 4: Truncate overly long values with warning\n    // TODO 5: Sort labels for canonical form\n}\n\n// CardinalityTracker monitors time series creation and enforces limits\ntype CardinalityTracker struct {\n    mu              sync.RWMutex\n    seriesCounts    map[string]int  // metric name -> series count\n    totalSeries     int\n    maxTotalSeries  int\n    maxPerMetric    int\n    warningCallback func(string, int, int) // metric, current, limit\n}\n\n// RecordSeries notifies the tracker of a new time series creation\nfunc (ct *CardinalityTracker) RecordSeries(metricName string, labels Labels) error {\n    // TODO 1: Acquire write lock\n    // TODO 2: Check against per-metric and total limits\n    // TODO 3: Increment appropriate counters\n    // TODO 4: Trigger warnings if thresholds exceeded\n    // TODO 5: Return error if hard limits violated\n}\n```\n\n#### Milestone Checkpoints\n\n**Checkpoint 1: Basic Metric Types**\nRun `go test ./internal/metrics/types_test.go` to verify:\n- Counter increments correctly and rejects negative values\n- Gauge accepts arbitrary values and supports increment/decrement operations\n- Histogram correctly categorizes observations into buckets\n- All operations are thread-safe under concurrent access\n\nExpected test output:\n```\n=== RUN TestCounterIncrement\n--- PASS: TestCounterIncrement (0.00s)\n=== RUN TestGaugeOperations  \n--- PASS: TestGaugeOperations (0.00s)\n=== RUN TestHistogramBuckets\n--- PASS: TestHistogramBuckets (0.01s)\n```\n\n**Checkpoint 2: Label Handling**\nCreate a simple program that demonstrates label cardinality:\n```go\nfunc main() {\n    // Create metrics with different label combinations\n    // Verify that {service=\"api\", method=\"GET\"} and {method=\"GET\", service=\"api\"} \n    // create the same time series identity\n    // Show cardinality explosion with a loop creating user_id labels\n}\n```\n\nYou should see consistent identity hashing regardless of label order, and cardinality tracking should detect rapid series creation.\n\n**Checkpoint 3: Validation and Limits**\nTest cardinality enforcement by instrumenting a metric with rapidly changing label values:\n```bash\ncurl -X POST http://localhost:8080/metrics \\\n  -d 'metric_name=test_metric&user_id=12345&timestamp=2023-10-15T14:30:22Z'\n```\n\nThe system should reject this request with a cardinality violation error, demonstrating effective protection against unbounded label values.\n\n\n## Scrape Engine Design\n\n> **Milestone(s):** This section directly corresponds to Milestone 2 (Scrape Engine) and provides the HTTP-based metrics collection system that feeds data into the storage layer from Milestone 3.\n\n### The Observatory Network Mental Model\n\nBefore diving into the technical details of scraping, consider how the National Weather Service operates thousands of weather stations across a country. Each station is a **scrape target** that measures temperature, humidity, and wind speed at regular intervals. The central weather service doesn't wait for stations to call in—instead, it actively contacts each station every hour to collect readings. Some stations are permanently configured (static discovery), while others are mobile units that register and deregister dynamically (service discovery). When a station goes offline due to equipment failure or network issues, the central service marks it as unavailable but continues attempting to reconnect. This is exactly how our scrape engine works: it maintains a registry of metric-producing targets, actively pulls data from each one on a schedule, and gracefully handles failures while preserving the overall collection process.\n\nThe scrape engine serves as the **data ingestion coordinator** that bridges the gap between distributed services exposing metrics and our centralized time series storage. Unlike push-based systems where applications actively send metrics to a collector, our pull-based approach gives the metrics system complete control over collection timing, failure handling, and resource management. This architectural choice provides several key advantages: the scrape engine can implement sophisticated retry logic without overwhelming targets, it can discover new targets automatically through service discovery, and it can apply consistent labeling and metadata enrichment across all collected metrics.\n\n![Scrape Operation Sequence](./diagrams/scrape-sequence.svg)\n\nThe scrape engine operates through four tightly coordinated subsystems that work together to provide reliable metrics collection. The **target discovery system** maintains an up-to-date registry of all metric endpoints, automatically adding newly deployed services and removing decommissioned ones. The **scrape scheduler** manages the timing and concurrency of collection operations, ensuring each target is scraped at its configured interval without overwhelming either the scrape engine or the target services. The **metrics parser** handles the complex task of converting HTTP response bodies in Prometheus exposition format into structured time series samples that can be stored efficiently. Finally, the **health management system** tracks the availability of each target, implements retry logic for transient failures, and provides observability into the scraping process itself.\n\n> The critical insight for pull-based scraping is that the metrics system becomes the **authoritative source of timing**. Unlike push-based systems where applications control when metrics are sent, our scrape engine determines exactly when each measurement is taken. This provides much stronger guarantees about data consistency and collection reliability.\n\n### Target Discovery\n\nTarget discovery solves the fundamental question of \"which endpoints should I scrape for metrics?\" In modern distributed systems, services are constantly being deployed, scaled, and decommissioned across multiple hosts and containers. Static configuration files become outdated quickly and create operational overhead. Our target discovery system supports both static configuration for stable infrastructure and dynamic service discovery for ephemeral workloads.\n\n> **Decision: Hybrid Discovery Model**\n> - **Context**: Modern deployments mix stable infrastructure (databases, load balancers) with dynamic workloads (microservices, containers). Pure static configuration requires manual updates, while pure dynamic discovery lacks control over stable services.\n> - **Options Considered**: Static-only configuration, dynamic-only service discovery, hybrid approach supporting both\n> - **Decision**: Implement hybrid discovery supporting both static targets and pluggable service discovery backends\n> - **Rationale**: Static configuration provides explicit control and reliability for infrastructure components, while dynamic discovery automatically handles ephemeral services without operational overhead\n> - **Consequences**: Increased complexity in target management but operational flexibility for mixed environments\n\n| Discovery Method | Use Cases | Update Frequency | Configuration Complexity |\n|------------------|-----------|------------------|-------------------------|\n| Static Configuration | Databases, load balancers, core infrastructure | Manual updates only | Low - direct endpoint lists |\n| DNS Service Discovery | Services with stable DNS names | DNS TTL intervals | Medium - DNS queries and caching |\n| Kubernetes Service Discovery | Containerized microservices | Real-time via API | High - API authentication and filtering |\n\nThe **static configuration system** reads target lists from YAML files that specify exact endpoints, scrape intervals, and additional labels to attach to all metrics from each target. This approach works well for infrastructure components that have predictable network addresses and don't frequently change. The configuration supports grouping targets with similar characteristics and applying common labels that help identify the service, environment, or datacenter in query results.\n\nStatic configuration follows this structure for maximum flexibility:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `targets` | []string | List of host:port endpoints to scrape |\n| `labels` | map[string]string | Additional labels attached to all metrics from these targets |\n| `scrape_interval` | duration | How frequently to collect metrics (defaults to system setting) |\n| `scrape_timeout` | duration | Maximum time to wait for HTTP response |\n| `metrics_path` | string | HTTP path for metrics endpoint (default: /metrics) |\n| `scheme` | string | HTTP or HTTPS protocol (default: http) |\n\nThe **dynamic service discovery system** integrates with external service registries to automatically detect new targets and remove decommissioned ones. Each service discovery backend runs as an independent goroutine that maintains its own view of available targets and publishes changes through a unified target update interface. This design allows multiple discovery mechanisms to operate simultaneously—for example, DNS discovery for legacy services and Kubernetes API discovery for containerized workloads.\n\nService discovery implementations must satisfy the `TargetDiscoverer` interface:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `Discover` | ctx context.Context | <-chan []*Target | Returns channel streaming target updates |\n| `Stop` | none | none | Gracefully shuts down discovery process |\n\n**DNS-based service discovery** queries SRV records to find service endpoints automatically. Many service mesh and load balancer systems publish SRV records that contain both the service port and priority information. The DNS discoverer performs periodic queries based on configurable intervals and TTL values, automatically adding new instances when they appear in DNS and removing them when they're no longer returned. This approach works particularly well for services that register themselves in DNS or are managed by orchestration systems that update DNS records.\n\n**Kubernetes service discovery** uses the Kubernetes API to watch for pod and service changes in real-time. The discoverer connects to the Kubernetes API server and establishes watch streams for pods with specific annotations or labels that indicate they expose metrics. When new pods are scheduled or existing pods terminate, the API server immediately pushes updates through the watch stream. This provides much faster target updates compared to polling-based approaches and reduces the delay between service deployment and metrics collection.\n\nThe Kubernetes discoverer supports sophisticated **target filtering** through label selectors and namespace restrictions:\n\n| Filter Type | Configuration | Purpose |\n|-------------|---------------|---------|\n| Label Selector | `kubernetes_sd_configs.selector` | Only discover pods/services matching specific labels |\n| Namespace Filter | `kubernetes_sd_configs.namespaces` | Restrict discovery to specific Kubernetes namespaces |\n| Port Filter | `kubernetes_sd_configs.port_name` | Select specific named ports from multi-port pods |\n| Annotation Requirements | Custom annotations | Require specific annotations to enable scraping |\n\nAll discovered targets flow through a **target consolidation process** that merges static and dynamic sources into a unified target registry. The consolidator handles conflicts when the same endpoint appears in multiple sources, applies target-specific configuration overrides, and maintains a consistent view of active targets across the entire scrape engine. This process runs continuously, updating the active target list whenever any discovery source reports changes.\n\n⚠️ **Pitfall: Target Flapping**\nWhen service discovery systems report rapid add/remove cycles for the same endpoint (often due to health check failures or network issues), the scrape engine can waste resources constantly starting and stopping scrape goroutines. The consolidator should implement **target stability windows** that require a target to remain stable for a minimum duration before activating scraping. This prevents resource thrashing while still responding quickly to legitimate topology changes.\n\n### Scrape Scheduling\n\nThe scrape scheduler orchestrates the complex task of collecting metrics from potentially thousands of targets simultaneously while respecting individual scrape intervals, timeouts, and resource constraints. Unlike simple cron-style scheduling, metrics scraping requires **adaptive scheduling** that can handle varying response times, target failures, and system load while maintaining consistent collection intervals for accurate time series analysis.\n\n> Think of the scrape scheduler as an **air traffic control system** managing hundreds of flights (scrape operations) that must take off (start) at precise times, follow specific routes (HTTP collection), and land (complete) within strict deadlines (timeouts). Just as air traffic control prevents collisions and manages delays, the scrape scheduler prevents resource conflicts and manages scrape timing to optimize both accuracy and system performance.\n\nThe core scheduling challenge is **interval drift prevention**. If a target is configured for 15-second scrapes but the HTTP request takes 2 seconds to complete, the next scrape should start 15 seconds after the previous scrape began, not 15 seconds after it completed. This ensures consistent sampling intervals that preserve the mathematical properties required for rate calculations and trend analysis. Naive implementations that wait for completion before scheduling the next scrape gradually drift away from their intended intervals.\n\n> **Decision: Per-Target Goroutine Model**\n> - **Context**: Need to scrape hundreds of targets concurrently while maintaining precise intervals and independent timeout handling for each target\n> - **Options Considered**: Global worker pool with shared queue, per-target goroutines, hybrid approach with target groups\n> - **Decision**: Dedicate one goroutine per active target for independent scheduling and lifecycle management\n> - **Rationale**: Goroutines are lightweight (8KB stack), provide natural isolation for timeouts and cancellation, and eliminate complex queue management logic\n> - **Consequences**: Higher memory usage with many targets but much simpler concurrent programming model and better isolation\n\nEach target gets its own **scrape goroutine** that manages the complete lifecycle of that target's metric collection. The goroutine maintains a timer for the next scrape interval, handles HTTP requests with proper timeout context, parses the response, and forwards samples to storage. When a target is removed from discovery, its goroutine receives a cancellation signal and terminates cleanly. This model provides excellent isolation—a hanging HTTP request to one target cannot block scraping of other targets.\n\nThe per-target scheduling algorithm follows this precise sequence:\n\n1. **Initialize interval timer** using the target's configured scrape interval (e.g., 15 seconds)\n2. **Wait for timer expiration** or cancellation signal from target discovery updates  \n3. **Record scrape start time** to maintain consistent interval timing regardless of request duration\n4. **Create HTTP request context** with scrape timeout deadline (e.g., 10 seconds maximum)\n5. **Execute HTTP GET request** to the target's metrics endpoint with timeout context\n6. **Parse response body** into time series samples if HTTP request succeeds\n7. **Forward samples to storage** with additional target labels and scrape timestamp\n8. **Update target health metrics** based on success/failure outcome and response time\n9. **Calculate next scrape time** by adding interval to start time (not completion time)\n10. **Reset interval timer** to maintain consistent scheduling and repeat the cycle\n\n**Timeout handling** deserves special attention because it directly impacts both data quality and resource utilization. Each scrape operation runs within a context that automatically cancels after the configured timeout period. When cancellation occurs, the HTTP client immediately closes the connection and returns an error. The scrape goroutine records this as a timeout failure, updates the target's health status, and continues with its normal scheduling cycle. Importantly, timeouts do not delay subsequent scrapes—the next scrape timer is based on the start time, not the timeout completion time.\n\n| Timeout Scenario | Behavior | Next Scrape Timing | Health Impact |\n|------------------|----------|-------------------|---------------|\n| Request completes in 2s | Normal processing, forward samples to storage | 15s after start time | Mark target healthy |\n| Request times out after 10s | Cancel HTTP context, record timeout error | 15s after start time | Mark target unhealthy |\n| Target unreachable | Immediate connection error | 15s after start time | Mark target unreachable |\n| Invalid metrics format | HTTP succeeds but parsing fails | 15s after start time | Mark target returning bad data |\n\n**Concurrency control** prevents the scrape engine from overwhelming either itself or target services with too many simultaneous requests. The scheduler implements several layers of protection: a global semaphore limits total concurrent scrapes across all targets, per-target state tracking prevents multiple simultaneous scrapes of the same endpoint, and adaptive backoff reduces scrape frequency for consistently failing targets.\n\nThe global concurrency limiter uses a **weighted semaphore** that accounts for the expected resource cost of different types of scrapes:\n\n```\nMax Concurrent Scrapes = min(\n    configured_max_concurrent,\n    available_memory / avg_scrape_memory,\n    network_connections / 2  // leave headroom for other operations\n)\n```\n\n**Adaptive backoff** helps manage failing targets without completely abandoning them. When a target fails multiple consecutive scrapes, the scheduler gradually increases the time between scrape attempts while still respecting the configured minimum interval. This reduces resource waste on broken targets while ensuring they're automatically rediscovered when they recover.\n\nThe backoff algorithm follows this progression:\n\n1. **First failure**: Continue normal interval (temporary network glitch)\n2. **Second consecutive failure**: Add 10% jitter to interval (reduce thundering herd)\n3. **Third consecutive failure**: Double the interval up to maximum backoff limit\n4. **Continued failures**: Maintain maximum interval with exponential decay attempts\n5. **First success**: Immediately return to normal configured interval\n\n⚠️ **Pitfall: Scrape Time Drift**\nA common mistake is calculating the next scrape time as `time.Now() + interval` instead of `scrape_start_time + interval`. This causes gradual drift where scrapes happen later and later over time as request processing duration accumulates. Always base the next scrape time on when the current scrape started, not when it completed. This maintains consistent sampling intervals essential for accurate rate calculations.\n\n### Metrics Parsing\n\nThe metrics parsing subsystem transforms HTTP response bodies in Prometheus exposition format into structured `Sample` objects that can be efficiently stored and queried. This involves lexical analysis of text-based metric data, validation of metric names and label formats, type inference, and timestamp assignment. The parser must handle malformed data gracefully while preserving as much valid information as possible from each scrape response.\n\n> Think of metrics parsing like **translating documents** from one language (text exposition format) to another (structured time series data). A good translator preserves the original meaning while adapting to the target language's grammar rules. Similarly, our parser preserves metric semantics while converting to our internal data structures. When encountering unclear passages (malformed metrics), a translator makes the best interpretation possible and continues rather than abandoning the entire document.\n\nPrometheus exposition format uses a simple text-based protocol that balances human readability with parsing efficiency. Each metric family begins with optional `# HELP` and `# TYPE` comments that provide metadata, followed by one or more sample lines containing the metric name, optional labels, value, and optional timestamp. The parser must handle this streaming format incrementally since response bodies can contain thousands of metrics from complex applications.\n\nA typical exposition format response looks like this structure:\n\n```\n# HELP http_requests_total Total number of HTTP requests\n# TYPE http_requests_total counter\nhttp_requests_total{method=\"GET\",status=\"200\"} 1234 1609459200000\nhttp_requests_total{method=\"POST\",status=\"201\"} 56 1609459200000\n\n# HELP process_cpu_seconds_total Total user and system CPU seconds\n# TYPE process_cpu_seconds_total counter\nprocess_cpu_seconds_total 123.45\n```\n\nThe **lexical analyzer** processes the input stream character by character, identifying tokens like metric names, label keys, label values, numeric values, and timestamps. This component must handle several parsing challenges: quoted label values may contain escaped characters, numeric values can be integers, floats, or special values like `+Inf` and `NaN`, and timestamp values are optional Unix milliseconds that default to scrape time when absent.\n\n| Token Type | Pattern | Examples | Special Handling |\n|------------|---------|----------|------------------|\n| Metric Name | `[a-zA-Z_:][a-zA-Z0-9_:]*` | `http_requests_total`, `cpu:usage_rate` | Must start with letter, underscore, or colon |\n| Label Key | `[a-zA-Z_][a-zA-Z0-9_]*` | `method`, `status_code` | Cannot start with `__` (reserved prefix) |\n| Label Value | `\"...\"` with escape sequences | `\"GET\"`, `\"HTTP/1.1\"` | Handle `\\\"`, `\\\\`, `\\n` escape sequences |\n| Numeric Value | Float or special constants | `123.45`, `+Inf`, `NaN` | IEEE 754 compliance for special values |\n| Timestamp | Integer milliseconds | `1609459200000` | Optional, defaults to scrape time |\n\n**Metric family grouping** collects related metrics that share the same base name but have different label combinations or suffixes. For histogram metrics, the parser must recognize and group together the `_bucket`, `_sum`, and `_count` series that represent different aspects of the same histogram. This grouping enables proper validation—for example, ensuring histogram bucket boundaries are monotonically increasing and that all required series are present.\n\nThe parser implements **streaming validation** that checks metric names, label formats, and values as they're encountered rather than buffering the entire response first. This approach provides better memory efficiency for large responses and enables early error detection. When validation failures occur, the parser logs the error with sufficient context for debugging but continues processing the remaining metrics in the response.\n\n| Validation Rule | Check | Error Handling |\n|-----------------|-------|----------------|\n| Metric name format | Matches `[a-zA-Z_:][a-zA-Z0-9_:]*` | Skip metric, log error with line number |\n| Label key format | No `__` prefix, valid identifier | Skip sample, preserve other labels |\n| Label value encoding | Valid UTF-8, proper escaping | Replace invalid chars, log warning |\n| Numeric value parsing | Valid float or special constant | Skip sample, increment parse error counter |\n| Histogram consistency | Monotonic buckets, sum/count present | Accept partial data, log inconsistency |\n\n**Type inference** determines whether each metric represents a counter, gauge, histogram, or summary based on `# TYPE` comments and naming conventions. When type comments are missing, the parser uses heuristics like metric name suffixes (`_total` suggests counter, `_bucket` suggests histogram) and value patterns (monotonically increasing suggests counter). Accurate type inference is crucial because it affects how the storage engine handles the data and how query functions like `rate()` operate.\n\nThe **sample construction process** converts parsed tokens into `Sample` objects with proper timestamps and labels:\n\n1. **Parse metric name and labels** from each sample line using the lexical analyzer\n2. **Apply target labels** from service discovery configuration (instance, job, environment)  \n3. **Validate label cardinality** against configured limits to prevent memory explosion\n4. **Assign timestamp** using provided value or current scrape time with millisecond precision\n5. **Convert numeric value** to internal float64 representation, handling special constants\n6. **Create Sample object** with metric identifier (name + labels), timestamp, and value\n7. **Forward to storage engine** through the ingestion pipeline\n\n**Error recovery** strategies ensure that parsing errors don't cause complete scrape failure. The parser maintains error counters for different failure modes and implements **best-effort processing** that extracts valid metrics even from responses with some malformed data. This resilience is essential in production environments where applications may generate imperfect exposition format due to bugs or configuration issues.\n\n⚠️ **Pitfall: Label Cardinality Explosion**\nApplications sometimes generate labels with unbounded values like user IDs, request IDs, or timestamps. A single misbehaving service can create millions of unique time series, consuming all available memory. The parser must implement **cardinality protection** that limits the number of unique label combinations per metric name and rejects samples that would exceed these limits. Always validate cardinality before creating new time series, not after.\n\n![Scrape Target State Machine](./diagrams/target-state-machine.svg)\n\n### Health and Error Handling\n\nThe health and error handling subsystem monitors scrape operations, classifies failures, implements recovery strategies, and provides observability into the scraping process. This component ensures that transient network issues don't cause permanent data loss, provides operators with visibility into collection problems, and maintains system stability under adverse conditions.\n\n> Think of target health management like **managing a fleet of field reporters** who gather information from remote locations. Some reporters occasionally miss check-ins due to bad weather (network issues), others might send garbled reports (parsing errors), and some might go completely silent (service failures). A good news editor tracks which reporters are reliable, follows up on missed check-ins, and adjusts expectations based on each reporter's track record. Similarly, our health system tracks target reliability and adapts behavior accordingly.\n\nTarget health exists in multiple dimensions that require independent tracking and different response strategies. **Network health** indicates whether the scrape engine can successfully connect to a target's endpoint. **Application health** shows whether the target service is running and responding to requests. **Data health** reflects whether the metrics data being returned is valid and parseable. A target might have good network and application health but poor data health due to bugs in its metrics exposition code.\n\nThe health tracking system maintains state for each target using this comprehensive model:\n\n| Health Dimension | States | Transition Triggers | Impact on Scheduling |\n|------------------|--------|-------------------|---------------------|\n| Network | `Reachable`, `Unreachable`, `Timeout` | TCP connection success/failure | Unreachable targets get exponential backoff |\n| Application | `Healthy`, `Error`, `Degraded` | HTTP status codes (200 vs 4xx/5xx) | Error responses increase scrape interval |\n| Data | `Valid`, `Partial`, `Invalid` | Parsing success/failure rates | Invalid data triggers diagnostic logging |\n| Overall | `Up`, `Down`, `Warning` | Combination of above dimensions | Used for alerting and dashboard display |\n\n**Failure classification** determines the appropriate response to different types of errors encountered during scraping. Not all failures are equal—a temporary DNS resolution failure should be handled differently than an HTTP 404 response, which should be handled differently than a timeout. The classification system groups errors into categories that each have specific retry strategies and escalation procedures.\n\n| Error Category | Examples | Retry Strategy | Escalation |\n|----------------|----------|---------------|------------|\n| Transient Network | DNS timeout, connection refused | Immediate retry with jitter | Exponential backoff after 3 failures |\n| Service Error | HTTP 500, service unavailable | Brief delay then retry | Reduce scrape frequency after 5 failures |\n| Configuration Error | HTTP 404, invalid endpoint | Log error, continue normal interval | Alert operator after 10 consecutive failures |\n| Data Format Error | Malformed exposition format | Process partial data, log details | Report parsing statistics |\n\n**Adaptive retry logic** balances the need for reliable data collection against the risk of overwhelming failing services. When targets experience failures, the retry system implements **exponential backoff with jitter** to reduce load while maintaining the possibility of recovery detection. The jitter component prevents **thundering herd effects** where many scrape engines simultaneously retry the same failed targets.\n\nThe retry algorithm follows this progression for consecutive failures:\n\n1. **First failure**: Record error, maintain normal scrape interval\n2. **Second failure**: Add 10-30% random jitter to next scrape time  \n3. **Third failure**: Double the scrape interval (15s becomes 30s)\n4. **Fourth failure**: Double again with maximum cap (30s becomes 60s, capped at 5min)\n5. **Continued failures**: Maintain maximum interval with occasional probe attempts\n6. **First success**: Immediately return to configured normal interval\n7. **Partial success**: Reduce interval by half until back to normal\n\n**Circuit breaker patterns** protect both the scrape engine and target services from cascading failures. When a target fails consistently, the circuit breaker opens and stops sending requests temporarily. This prevents resource waste on the scrape engine side and reduces load on the failing service, potentially allowing it to recover. The circuit breaker periodically sends probe requests to detect recovery.\n\n| Circuit State | Behavior | Transition Condition | Probe Frequency |\n|---------------|----------|---------------------|-----------------|\n| Closed | Normal scraping at configured interval | Failure rate < 50% over 10 scrapes | N/A |\n| Open | Block all scrape attempts | After 60 seconds or manual reset | Every 60 seconds |\n| Half-Open | Allow single probe scrape | Successful probe or probe failure | Single attempt |\n\n**Resource protection** mechanisms prevent failing targets from consuming excessive scrape engine resources. The protection system implements **per-target resource limits** on memory usage, connection time, and response body size. When targets exceed these limits, the scrape engine terminates the request and marks the target as misbehaving.\n\n| Resource | Limit | Protection Mechanism | Action on Violation |\n|----------|-------|---------------------|-------------------|\n| Response Body Size | 10MB default | Stream reader with size limit | Truncate response, process partial data |\n| Connection Time | Scrape timeout (10s default) | Context cancellation | Close connection, mark as timeout |\n| Memory per Target | 50MB default | Sample buffer size limits | Drop oldest samples, log warning |\n| Concurrent Connections | Global semaphore | Weighted semaphore acquisition | Queue scrape, apply backpressure |\n\n**Health metrics collection** provides observability into the scraping process itself through internal metrics that track success rates, error distributions, response times, and resource usage. These metrics are essential for operators to understand collection health and optimize scrape configurations. The health system exposes these metrics through the same HTTP endpoint used by external scrapers.\n\nKey internal metrics include:\n\n| Metric Name | Type | Labels | Purpose |\n|-------------|------|--------|---------|\n| `scrape_duration_seconds` | Histogram | `job`, `instance` | Track response time distribution |\n| `scrape_samples_scraped` | Gauge | `job`, `instance` | Number of samples per scrape |\n| `scrape_series_added` | Counter | `job`, `instance` | New time series creation rate |\n| `scrape_health` | Gauge | `job`, `instance` | Binary health indicator (1=up, 0=down) |\n| `scrape_timeout_seconds` | Gauge | `job`, `instance` | Configured timeout for target |\n\n**Error aggregation and reporting** collects detailed error information across all targets and provides structured access for debugging and alerting. Rather than logging each individual failure, the system aggregates errors by type, target, and time window to provide meaningful insights without overwhelming operators with noise.\n\n⚠️ **Pitfall: Health Check Feedback Loops**\nWhen scrape targets expose their own health status as metrics, failures can create confusing feedback loops. If a service reports itself as unhealthy in its metrics but still responds to HTTP requests, should the scrape engine consider it healthy or unhealthy? Design clear separation between **collection health** (can we scrape it?) and **application health** (what does it report about itself?). Never use application-reported health metrics to control scraping behavior.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Client | `net/http` with default client | Custom client with connection pooling |\n| Service Discovery | Static file-based configuration | Kubernetes API client (`k8s.io/client-go`) |\n| Concurrency Control | Basic goroutines with sync.WaitGroup | Worker pools with semaphores |\n| Configuration | YAML files with `gopkg.in/yaml.v2` | Configuration hot-reload with file watching |\n| Metrics Parsing | Text scanner with regular expressions | Custom lexer with finite state machine |\n\n#### Recommended File Structure\n\n```\nproject-root/\n├── cmd/scraper/\n│   └── main.go                    ← Entry point for scrape engine\n├── internal/scrape/\n│   ├── engine.go                  ← Main scrape coordinator\n│   ├── target.go                  ← Target management and state\n│   ├── scheduler.go               ← Per-target scrape scheduling  \n│   ├── parser.go                  ← Prometheus format parser\n│   ├── discovery/\n│   │   ├── static.go              ← Static file-based discovery\n│   │   ├── dns.go                 ← DNS SRV record discovery\n│   │   └── kubernetes.go          ← Kubernetes API discovery\n│   └── health/\n│       ├── tracker.go             ← Target health monitoring\n│       └── circuit_breaker.go     ← Circuit breaker implementation\n├── internal/config/\n│   └── scrape_config.go           ← Scrape configuration structures\n└── internal/storage/\n    └── ingestion.go               ← Interface to storage layer\n```\n\n#### Infrastructure Starter Code\n\n**HTTP Client with Timeouts (internal/scrape/client.go):**\n\n```go\npackage scrape\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n    \"time\"\n)\n\n// HTTPClient wraps net/http.Client with scraping-specific configuration\ntype HTTPClient struct {\n    client *http.Client\n    userAgent string\n    maxResponseSize int64\n}\n\n// NewHTTPClient creates a configured HTTP client for scraping\nfunc NewHTTPClient(timeout time.Duration) *HTTPClient {\n    return &HTTPClient{\n        client: &http.Client{\n            Timeout: timeout,\n            Transport: &http.Transport{\n                MaxIdleConns:        100,\n                MaxIdleConnsPerHost: 10,\n                IdleConnTimeout:     30 * time.Second,\n            },\n        },\n        userAgent:       \"prometheus-scraper/1.0\",\n        maxResponseSize: 10 * 1024 * 1024, // 10MB limit\n    }\n}\n\n// ScrapeTarget performs HTTP GET request with size limits and timeout\nfunc (c *HTTPClient) ScrapeTarget(ctx context.Context, url string) (io.Reader, error) {\n    req, err := http.NewRequestWithContext(ctx, \"GET\", url, nil)\n    if err != nil {\n        return nil, fmt.Errorf(\"creating request: %w\", err)\n    }\n    \n    req.Header.Set(\"User-Agent\", c.userAgent)\n    req.Header.Set(\"Accept\", \"text/plain;version=0.0.4\")\n    \n    resp, err := c.client.Do(req)\n    if err != nil {\n        return nil, fmt.Errorf(\"HTTP request failed: %w\", err)\n    }\n    defer resp.Body.Close()\n    \n    if resp.StatusCode != http.StatusOK {\n        return nil, fmt.Errorf(\"HTTP %d: %s\", resp.StatusCode, resp.Status)\n    }\n    \n    // Limit response body size to prevent memory exhaustion\n    limitedReader := &io.LimitedReader{\n        R: resp.Body,\n        N: c.maxResponseSize,\n    }\n    \n    // Read entire response into memory for parsing\n    body, err := io.ReadAll(limitedReader)\n    if err != nil {\n        return nil, fmt.Errorf(\"reading response body: %w\", err)\n    }\n    \n    return bytes.NewReader(body), nil\n}\n```\n\n**Target Health Tracker (internal/scrape/health/tracker.go):**\n\n```go\npackage health\n\nimport (\n    \"sync\"\n    \"time\"\n)\n\n// HealthStatus represents the current health state of a scrape target\ntype HealthStatus int\n\nconst (\n    HealthUnknown HealthStatus = iota\n    HealthUp\n    HealthDown\n    HealthDegraded\n)\n\n// TargetHealth tracks health metrics for a single scrape target\ntype TargetHealth struct {\n    mutex            sync.RWMutex\n    status           HealthStatus\n    lastScrapeTime   time.Time\n    lastScrapeError  error\n    consecutiveFailures int\n    totalScrapes     int64\n    successfulScrapes int64\n    lastSuccessTime  time.Time\n}\n\n// NewTargetHealth creates a new health tracker for a target\nfunc NewTargetHealth() *TargetHealth {\n    return &TargetHealth{\n        status: HealthUnknown,\n    }\n}\n\n// RecordSuccess updates health status for successful scrape\nfunc (h *TargetHealth) RecordSuccess(scrapeDuration time.Duration) {\n    h.mutex.Lock()\n    defer h.mutex.Unlock()\n    \n    h.status = HealthUp\n    h.lastScrapeTime = time.Now()\n    h.lastScrapeError = nil\n    h.consecutiveFailures = 0\n    h.totalScrapes++\n    h.successfulScrapes++\n    h.lastSuccessTime = time.Now()\n}\n\n// RecordFailure updates health status for failed scrape\nfunc (h *TargetHealth) RecordFailure(err error) {\n    h.mutex.Lock()\n    defer h.mutex.Unlock()\n    \n    h.lastScrapeTime = time.Now()\n    h.lastScrapeError = err\n    h.consecutiveFailures++\n    h.totalScrapes++\n    \n    if h.consecutiveFailures >= 3 {\n        h.status = HealthDown\n    } else if h.consecutiveFailures >= 1 {\n        h.status = HealthDegraded\n    }\n}\n\n// GetStatus returns current health status thread-safely\nfunc (h *TargetHealth) GetStatus() (HealthStatus, error, time.Time) {\n    h.mutex.RLock()\n    defer h.mutex.RUnlock()\n    \n    return h.status, h.lastScrapeError, h.lastScrapeTime\n}\n\n// SuccessRate calculates the percentage of successful scrapes\nfunc (h *TargetHealth) SuccessRate() float64 {\n    h.mutex.RLock()\n    defer h.mutex.RUnlock()\n    \n    if h.totalScrapes == 0 {\n        return 0.0\n    }\n    \n    return float64(h.successfulScrapes) / float64(h.totalScrapes)\n}\n```\n\n#### Core Logic Skeletons\n\n**Main Scrape Engine (internal/scrape/engine.go):**\n\n```go\n// ScrapeEngine coordinates target discovery, scheduling, and metrics collection\ntype ScrapeEngine struct {\n    config          *Config\n    targets         map[string]*Target\n    targetsMutex    sync.RWMutex\n    discoverers     []TargetDiscoverer\n    storage         StorageEngine\n    httpClient      *HTTPClient\n    logger          Logger\n    stopChan        chan struct{}\n    wg              sync.WaitGroup\n}\n\n// NewScrapeEngine creates and configures a new scrape engine\nfunc NewScrapeEngine(config *Config, storage StorageEngine, logger Logger) *ScrapeEngine {\n    // TODO 1: Initialize ScrapeEngine struct with provided parameters\n    // TODO 2: Create HTTP client with configured timeout from config.ScrapeTimeout\n    // TODO 3: Initialize empty targets map and stop channel\n    // TODO 4: Create discoverers list based on config (static, DNS, K8s)\n    // Hint: Use sync.RWMutex for targets map since it's read frequently but written rarely\n}\n\n// Start begins target discovery and scraping operations\nfunc (e *ScrapeEngine) Start(ctx context.Context) error {\n    // TODO 1: Start all configured target discoverers in separate goroutines\n    // TODO 2: Launch target discovery consolidation loop to merge discovered targets\n    // TODO 3: Start metrics collection goroutines for each active target\n    // TODO 4: Set up signal handling for graceful shutdown\n    // Hint: Use errgroup.Group to manage multiple goroutines with error propagation\n}\n\n// UpdateTargets processes target changes from service discovery\nfunc (e *ScrapeEngine) UpdateTargets(newTargets []*Target) error {\n    // TODO 1: Acquire write lock on targets map\n    // TODO 2: Compare new target list with existing targets to find additions/removals\n    // TODO 3: Start scrape goroutines for newly discovered targets\n    // TODO 4: Stop scrape goroutines for removed targets by cancelling their contexts\n    // TODO 5: Update internal targets map with new target set\n    // Hint: Use target.URL as unique identifier for comparison\n}\n```\n\n**Target Scraper (internal/scrape/target.go):**\n\n```go\n// scrapeTarget performs a single scrape operation against a target\nfunc (e *ScrapeEngine) scrapeTarget(ctx context.Context, target *Target) error {\n    // TODO 1: Create HTTP request context with scrape timeout deadline\n    // TODO 2: Record scrape start time for consistent interval calculation\n    // TODO 3: Perform HTTP GET request to target.URL + target.MetricsPath\n    // TODO 4: Check HTTP response status and handle non-200 responses\n    // TODO 5: Parse response body using Prometheus exposition format parser\n    // TODO 6: Add target labels (job, instance) to all parsed samples\n    // TODO 7: Forward samples to storage engine via Append() method\n    // TODO 8: Update target health status based on success/failure\n    // TODO 9: Record scrape metrics (duration, sample count, error status)\n    // Hint: Always update health status even if storage append fails\n}\n\n// runTargetScrapeLoop manages the continuous scraping lifecycle for one target\nfunc (e *ScrapeEngine) runTargetScrapeLoop(ctx context.Context, target *Target) {\n    // TODO 1: Create ticker with target.ScrapeInterval duration\n    // TODO 2: Initialize target health tracker\n    // TODO 3: Enter infinite loop listening for ticker and context cancellation\n    // TODO 4: On each tick, call scrapeTarget() and handle any errors\n    // TODO 5: Calculate next scrape time based on start time, not completion time\n    // TODO 6: Implement exponential backoff for consecutive failures\n    // TODO 7: Clean up ticker and update target registry on context cancellation\n    // Hint: Use time.NewTicker and defer ticker.Stop() for proper cleanup\n}\n```\n\n**Prometheus Format Parser (internal/scrape/parser.go):**\n\n```go\n// ParsePrometheusFormat converts exposition format text to structured samples\nfunc ParsePrometheusFormat(reader io.Reader, defaultTimestamp time.Time) ([]*Sample, error) {\n    // TODO 1: Create scanner to read input line by line\n    // TODO 2: Initialize empty samples slice and current metric metadata\n    // TODO 3: Process each line: comments (HELP/TYPE) vs sample lines\n    // TODO 4: For comment lines, extract metric name and type information\n    // TODO 5: For sample lines, parse metric name, labels, value, optional timestamp\n    // TODO 6: Validate metric names match [a-zA-Z_:][a-zA-Z0-9_:]* pattern\n    // TODO 7: Parse label sets with proper quote handling and escape sequences\n    // TODO 8: Convert string values to float64, handle +Inf/-Inf/NaN special cases\n    // TODO 9: Use provided timestamp if sample doesn't include one\n    // TODO 10: Return accumulated samples slice or first parsing error encountered\n    // Hint: Use regexp for metric name validation, manual parsing for labels is more efficient\n}\n\n// parseSampleLine extracts components from a single metric sample line\nfunc parseSampleLine(line string) (metricName string, labels Labels, value float64, timestamp *time.Time, err error) {\n    // TODO 1: Find metric name at start of line (ends at '{' or whitespace)\n    // TODO 2: If '{' present, parse label set until matching '}' \n    // TODO 3: Parse numeric value after labels, handle special float constants\n    // TODO 4: Check for optional timestamp at end of line (Unix milliseconds)\n    // TODO 5: Return parsed components or detailed error with line context\n    // Hint: Be careful with quoted label values containing '{', '}', or whitespace\n}\n```\n\n#### Language-Specific Hints\n\n- **Goroutine Management**: Use `sync.WaitGroup` to track scrape goroutines and `context.Context` for graceful cancellation. Each target gets its own goroutine for isolation.\n\n- **HTTP Timeouts**: Set timeouts at multiple levels: `http.Client.Timeout` for overall request timeout, `context.WithTimeout` for individual scrape operations, and `io.LimitedReader` for response size limits.\n\n- **Memory Management**: Pre-allocate slices for samples when possible (`make([]*Sample, 0, estimatedCount)`). Use object pooling for frequently allocated parser structures.\n\n- **Error Handling**: Distinguish between permanent errors (HTTP 404) and transient errors (network timeout). Use `errors.Is()` and `errors.As()` for proper error classification.\n\n- **Configuration Reloading**: Watch configuration files with `fsnotify` package and reload target lists without stopping active scrapes.\n\n#### Milestone Checkpoints\n\n**After implementing target discovery:**\n```bash\ngo test ./internal/scrape/discovery/...\ngo run cmd/scraper/main.go -config=test-config.yaml\n```\n\nExpected behavior: Scraper should log discovered targets from static configuration and start attempting to scrape them (even if they fail). Check logs for \"discovered target\" messages.\n\n**After implementing scrape scheduling:**  \nConfigure a target pointing to `http://localhost:8080/metrics` and run a simple HTTP server that returns basic Prometheus format. Verify scrapes happen at the configured interval by checking timestamps in logs.\n\n**After implementing metrics parsing:**\nTest with malformed exposition format to verify parser handles errors gracefully and extracts valid metrics while skipping invalid ones.\n\n**Signs something is wrong:**\n- Scrape times drift further apart → Check interval calculation logic\n- Memory usage grows constantly → Check for target goroutine leaks  \n- Targets always show as \"down\" → Verify HTTP client timeout configuration\n- Parser crashes on certain inputs → Add more input validation and error recovery\n\n\n## Time Series Storage Engine\n\n> **Milestone(s):** This section directly corresponds to Milestone 3 (Time Series Storage) and provides the efficient storage foundation that will be queried by Milestone 4 (Query Engine). The storage engine receives scraped metrics from Milestone 2 (Scrape Engine) and persists the time series data defined in Milestone 1 (Metrics Data Model).\n\nThe time series storage engine serves as the memory and archive of our metrics collection system. It must handle the unique challenges of time series data: high write throughput from continuous scraping, efficient storage of timestamped numeric values, fast retrieval for queries, and automatic lifecycle management as data ages. Unlike traditional databases optimized for random access patterns, time series workloads exhibit distinct characteristics that demand specialized storage techniques.\n\n![Storage Architecture](./diagrams/storage-layout.svg)\n\nThe storage engine operates under several constraints that shape its design. Time series data arrives continuously with timestamps that are always increasing within each series, creating an append-only write pattern. Query patterns typically request recent data or ranges of historical data, rarely accessing individual points randomly. The volume of incoming data grows linearly with the number of monitored targets and their label cardinality, making storage efficiency critical for operational costs. Finally, old data has diminishing value and must be automatically removed to prevent unbounded storage growth.\n\n### Library Archive Mental Model\n\nUnderstanding time series storage becomes intuitive when we think of it like a physical library archive system. Imagine a vast library that specializes in storing weather measurement records from thousands of monitoring stations worldwide. Each monitoring station represents a unique time series identified by its location and the type of measurement (temperature, humidity, pressure). The library receives new measurements every minute and must organize them efficiently for both storage and retrieval.\n\nIn our library analogy, each **monitoring station** corresponds to a time series identified by its metric name and label set. The station \"temperature.celsius{location=paris,sensor=outdoor}\" produces a continuous stream of timestamped temperature readings, just as our time series produces timestamped numeric samples. The library must create a unique storage location for each station's records, much like our storage engine creates separate storage chunks for each time series.\n\nThe library organizes records using a two-level system: a **card catalog** (our inverted indexes) and **storage boxes** (our compressed chunks). The card catalog contains index cards that list which storage boxes contain records for each station and time period. When a researcher asks for \"all temperature readings from Paris outdoor sensors between March 1-15,\" the librarian first consults the card catalog to identify relevant boxes, then retrieves only those specific boxes rather than searching the entire archive.\n\nThe storage boxes themselves use a clever compression technique. Instead of writing the full timestamp and value for each record, the librarian writes only the difference from the previous record. If yesterday's temperature was 20.5°C and today's is 21.2°C, the record stores \"+0.7\" instead of the full value. This **delta compression** dramatically reduces the space needed for each box, allowing the library to store decades of history in a reasonable amount of space.\n\nAs records age, the library applies a **retention policy** similar to how old newspapers are eventually discarded or moved to deep storage. Records newer than one month are kept in full detail. Records older than one month but newer than one year are **downsampled** - instead of minute-by-minute readings, only hourly averages are preserved. Records older than one year are deleted entirely. This automatic lifecycle management prevents the archive from growing without bound while preserving the most valuable historical data.\n\nWhen the library receives new records, they're first written to a **processing journal** (our write-ahead log) before being filed in the appropriate storage boxes. If a power outage occurs while the librarian is updating multiple boxes, the journal can be replayed upon restart to ensure no records are lost. This provides **durability** even in the face of unexpected failures.\n\nThe genius of this system is that it optimizes for the actual usage patterns of time series data: most writes are recent data appended to existing series, most reads request ranges of data from specific series, and old data becomes less valuable over time. Our storage engine implements this same organizational philosophy using modern computer science techniques.\n\n### Gorilla Compression\n\nThe Gorilla compression algorithm, developed by Facebook for their time series database, provides remarkable space efficiency by exploiting the predictable patterns inherent in time series data. Most time series data exhibits two key properties: timestamps arrive at regular intervals, and values change gradually between adjacent samples. Gorilla compression leverages both patterns to achieve compression ratios of 10:1 or better.\n\n![Gorilla Compression Process](./diagrams/compression-process.svg)\n\n> **Decision: Gorilla Compression Algorithm**\n> - **Context**: Time series data consumes enormous storage space when naively stored as timestamp-value pairs. A monitoring system collecting metrics every 15 seconds from 1000 targets generates over 5 million samples per day. Without compression, each sample requires 16 bytes (8-byte timestamp + 8-byte float64), consuming 80MB daily per target.\n> - **Options Considered**: \n>   1. Store raw timestamp-value pairs without compression\n>   2. Generic compression (gzip/lz4) applied to time series chunks  \n>   3. Gorilla-style delta-of-delta timestamp and XOR value compression\n> - **Decision**: Implement Gorilla compression with delta-of-delta timestamps and XOR value encoding\n> - **Rationale**: Gorilla compression is specifically designed for time series characteristics. Delta-of-delta encoding exploits regular timestamp intervals common in metrics collection. XOR value encoding exploits the fact that consecutive floating-point measurements often share common bit patterns. Generic compression doesn't understand time series structure and achieves lower compression ratios.\n> - **Consequences**: Achieves 10:1 compression ratios typical of Gorilla. Requires more complex encoding/decoding logic than raw storage. CPU overhead for compression/decompression during writes and reads.\n\n| Compression Option | Space Efficiency | CPU Overhead | Implementation Complexity | Chosen? |\n|-------------------|------------------|--------------|---------------------------|---------|\n| Raw Storage | 16 bytes/sample | None | Minimal | No |\n| Generic Compression | 4-8 bytes/sample | Medium | Low | No |\n| Gorilla Compression | 1.4 bytes/sample | High | High | **Yes** |\n\n#### Delta-of-Delta Timestamp Encoding\n\nTimestamp compression exploits the regularity of metrics collection intervals. Consider a time series scraped every 15 seconds starting at timestamp 1640995200:\n\n```\nTimestamp: 1640995200, 1640995215, 1640995230, 1640995245, ...\nDeltas:    [baseline],       +15,       +15,       +15, ...\nDoD:       [baseline],   [baseline],        0,        0, ...\n```\n\nThe algorithm stores the first timestamp as a 64-bit baseline. For the second timestamp, it calculates the delta (difference) from the first and stores this delta as a baseline delta. For subsequent timestamps, it calculates the \"delta-of-delta\" - the difference between the current delta and the expected delta based on the pattern.\n\nWhen the delta-of-delta is zero (indicating a perfectly regular interval), Gorilla stores just a single bit flag. When the delta-of-delta is small (within ±63 of the expected delta), it stores the value in 7 bits. Only when timestamps deviate significantly from the pattern does it fall back to larger encodings. This approach reduces regular 64-bit timestamps to often just 1 bit per sample.\n\n| Delta-of-Delta Range | Encoding | Bits Used | Common Case |\n|---------------------|----------|-----------|-------------|\n| 0 | Single '0' bit | 1 | Regular intervals |\n| -63 to +64 | '10' + 7-bit signed value | 9 | Small time drift |\n| -255 to +256 | '110' + 9-bit signed value | 12 | Clock adjustments |\n| -2047 to +2048 | '1110' + 12-bit signed value | 16 | Irregular intervals |\n| Other | '1111' + 32-bit signed value | 36 | Major time jumps |\n\n#### XOR Value Encoding\n\nValue compression leverages the observation that consecutive floating-point measurements often share common bit patterns. Temperature readings might vary from 20.1°C to 20.3°C, and the IEEE 754 binary representations of these values differ only in the least significant bits.\n\nThe algorithm XORs each value with the previous value in the series. If the XOR result is zero (values are identical), it stores a single '0' bit. If the XOR result is non-zero, it analyzes the bit pattern to determine the most efficient encoding.\n\n```\nValue 1: 20.1 (binary: 0x4034199999999998)\nValue 2: 20.3 (binary: 0x403451999999999A)\nXOR:                   0x0000408000000002\n```\n\nThe XOR result often has many leading and trailing zero bits. Gorilla stores only the significant bits between the leading and trailing zeros, along with metadata indicating their position. When consecutive values are very similar, this reduces 64-bit floating-point values to as few as 5-10 bits.\n\n| XOR Pattern | Encoding Strategy | Typical Bits | Example Scenario |\n|-------------|------------------|--------------|------------------|\n| Zero XOR | Single '0' bit | 1 | Identical consecutive values |\n| Same pattern as previous | '10' + compressed bits | 2-15 | Gradually changing values |\n| New pattern | '11' + leading zeros + length + bits | 10-65 | Value jumps or first difference |\n\n#### Compression Implementation Strategy\n\nThe compression algorithm operates on fixed-size chunks of time series data, typically containing 2-4 hours worth of samples. Each chunk begins with baseline values for the first timestamp and value, followed by compressed deltas and XOR patterns for subsequent samples. The chunk header contains metadata necessary for decompression: the baseline timestamp, baseline value, and the number of samples in the chunk.\n\nDuring compression, the algorithm maintains state about the previous timestamp delta and value to calculate deltas-of-deltas and XOR patterns. This state allows the compressor to make optimal encoding decisions based on the emerging patterns in the data. The bit-level nature of the encoding means that sample boundaries don't align with byte boundaries, requiring careful bit manipulation during both compression and decompression.\n\nDecompression reverses the process by starting with the baseline values and iteratively applying the stored deltas and XOR patterns to reconstruct the original timestamp-value pairs. The algorithm must handle variable-length encodings correctly, using the bit flags to determine how many subsequent bits to read for each sample.\n\n### Series Indexing\n\nEfficient querying requires fast lookup of time series by metric name and label combinations. With millions of active time series, a naive linear search through all series would make queries prohibitively slow. The storage engine employs inverted indexes that map from metric names and label values to the specific time series containing that metadata, enabling subsecond query response times even with high cardinality.\n\n![Label Cardinality Impact](./diagrams/label-cardinality.svg)\n\nThe indexing challenge stems from the multi-dimensional nature of labeled time series. A query like `http_requests_total{method=\"GET\", status=\"200\"}` must find all time series where the metric name matches exactly and both specified labels have the required values. The query engine may then need to aggregate across other label dimensions like `instance` or `job` that weren't specified in the query.\n\n> **Decision: Inverted Index Architecture**\n> - **Context**: Queries must efficiently find time series matching metric names and label selectors from millions of active series. Label-based queries like `{job=\"web\", method=~\"GET|POST\"}` require fast intersection of multiple label conditions.\n> - **Options Considered**:\n>   1. Single composite index mapping full series signatures to storage locations\n>   2. Separate indexes per metric name with secondary label indexes\n>   3. Inverted indexes mapping each label value to series containing that value\n> - **Decision**: Implement inverted indexes with efficient intersection algorithms\n> - **Rationale**: Inverted indexes support arbitrary label selector combinations efficiently. They enable fast intersection operations when multiple labels are specified. They scale better with high cardinality than composite indexes.\n> - **Consequences**: Requires more storage overhead for multiple indexes. Complex intersection logic for multi-label queries. Fast query performance for typical PromQL patterns.\n\n#### Primary Metric Index\n\nThe primary index maps metric names to lists of series identifiers that contain metrics with that name. This provides the first level of filtering for most queries, since PromQL queries typically start with a metric name like `http_requests_total` or `cpu_usage_seconds`. The index structure resembles a traditional database index optimized for prefix matching and range scans.\n\n| Index Component | Structure | Purpose |\n|----------------|-----------|---------|\n| Metric Name Map | `map[string][]uint64` | Maps metric names to series ID lists |\n| Series Registry | `map[uint64]*SeriesMetadata` | Maps series IDs to full label sets and storage locations |\n| Label Indexes | `map[string]map[string][]uint64` | Maps label name → value → series IDs for fast label filtering |\n\nEach series receives a unique 64-bit identifier when first created. The series registry maintains the authoritative mapping from this identifier to the complete label set and storage chunk locations for that series. This indirection allows the inverted indexes to store compact series IDs rather than full label sets, reducing memory usage as cardinality grows.\n\n#### Label Value Indexes\n\nFor each label name that appears in any time series, the storage engine maintains an inverted index mapping from label values to the series that contain those values. The label index for `method` might map `\"GET\"` to series IDs [1001, 1003, 1007, ...] and `\"POST\"` to series IDs [1002, 1005, 1008, ...]`.\n\nWhen processing a query like `http_requests_total{method=\"GET\", status=\"200\"}`, the query engine:\n\n1. Looks up the metric name `http_requests_total` in the primary index to get candidate series IDs\n2. Looks up label value `\"GET\"` in the `method` label index to get matching series IDs  \n3. Looks up label value `\"200\"` in the `status` label index to get matching series IDs\n4. Computes the intersection of all three series ID lists to find series matching all conditions\n\n| Query Processing Step | Input | Index Used | Output |\n|----------------------|-------|------------|--------|\n| Metric Name Filter | `http_requests_total{...}` | Primary metric index | Series IDs [1001, 1002, 1003, ...] |\n| Label Filter `method=\"GET\"` | Series ID list | `method` label index | Filtered series IDs [1001, 1003, ...] |\n| Label Filter `status=\"200\"` | Series ID list | `status` label index | Final series IDs [1001, ...] |\n| Series Metadata Lookup | Series ID list | Series registry | Label sets and chunk locations |\n\n#### Index Intersection Algorithms\n\nEfficiently computing intersections of series ID lists is critical for query performance, especially when multiple label conditions are specified. The storage engine implements several intersection algorithms optimized for different scenarios commonly encountered in time series queries.\n\nFor small result sets (fewer than 1000 series), the engine uses a simple hash set intersection. It loads the smallest series ID list into a hash set, then iterates through other lists checking membership. This approach provides O(n) performance and minimal memory overhead for the common case of specific label value combinations.\n\nFor larger result sets, the engine switches to a sorted list intersection algorithm. Since series IDs are assigned monotonically, the inverted index lists are naturally sorted. The algorithm uses multiple pointers to advance through the sorted lists simultaneously, similar to merging sorted arrays. This approach avoids the memory allocation overhead of hash sets when dealing with large intermediate results.\n\nThe query optimizer chooses the intersection order based on the estimated cardinality of each label condition. Label values with lower cardinality (fewer matching series) are processed first to minimize the size of intermediate results. A label like `datacenter=\"us-west\"` might match thousands of series, while `instance=\"web-01\"` matches only one, making instance-first processing much more efficient.\n\n#### Cardinality Management\n\nHigh cardinality labels pose the greatest threat to index performance and memory usage. A label like `request_id` with unique values for every request would create millions of entries in the label value index, consuming enormous memory and slowing intersection operations. The storage engine implements several strategies to detect and mitigate cardinality explosion before it degrades system performance.\n\n| Cardinality Level | Series Count | Index Memory | Query Performance | Management Strategy |\n|-------------------|--------------|--------------|-------------------|-------------------|\n| Low | < 1,000 | < 10MB | Subsecond | Normal operation |\n| Medium | 1,000 - 100,000 | 10MB - 1GB | 1-5 seconds | Monitor growth rate |\n| High | 100,000 - 1,000,000 | 1GB - 10GB | 5-30 seconds | Throttle ingestion |\n| Critical | > 1,000,000 | > 10GB | 30+ seconds | Reject new series |\n\nThe `CardinalityTracker` component monitors the number of unique values for each label name and raises alerts when cardinality grows unexpectedly. It maintains moving averages of cardinality growth rates and can predict when a label will exceed safe thresholds based on current trends.\n\nWhen cardinality limits are approached, the storage engine can reject new time series that would exceed the configured limits. This prevents a runaway cardinality explosion from bringing down the entire monitoring system. The rejected series are logged for later analysis, allowing operators to identify the source of high-cardinality labels and fix the instrumentation.\n\n### Data Lifecycle Management\n\nTime series data has a natural lifecycle where recent data is most valuable for alerting and debugging, while historical data provides context for capacity planning and trend analysis. However, storing all historical data indefinitely is neither practical nor cost-effective. The storage engine implements automated lifecycle management that balances data retention needs with storage costs through configurable retention policies and downsampling strategies.\n\nThe lifecycle management system operates on the principle that data value decreases over time, but at different rates for different use cases. Alerting requires minute-level granularity for recent data but can tolerate hour-level granularity for data older than a week. Capacity planning queries typically aggregate data over longer time periods and don't require full-resolution historical data. By automatically reducing resolution as data ages, the system maintains query capability while controlling storage growth.\n\n> **Decision: Hierarchical Data Lifecycle Management**\n> - **Context**: Time series data volume grows linearly with time, making indefinite full-resolution retention impossible. Different use cases have different precision requirements based on data age.\n> - **Options Considered**:\n>   1. Fixed retention period with complete deletion after expiration\n>   2. Uniform downsampling (e.g., keep only hourly averages for all old data)\n>   3. Hierarchical retention with multiple resolution levels based on age\n> - **Decision**: Implement hierarchical retention with automatic downsampling at configurable age thresholds\n> - **Rationale**: Provides optimal balance between storage efficiency and query utility. Recent data kept at full resolution for debugging. Historical data downsampled to enable long-term trend analysis. Configurable policies allow customization based on organizational needs.\n> - **Consequences**: Requires complex logic to manage multiple resolution levels. Queries spanning multiple retention tiers need special handling. Provides excellent storage efficiency and query flexibility.\n\n#### Retention Policy Configuration\n\nThe storage engine supports flexible retention policies that define both the total retention period and the downsampling schedule as data ages. These policies are configured per metric or per metric pattern, allowing different retention strategies for different types of monitoring data.\n\n| Retention Tier | Age Range | Resolution | Retention Period | Storage Ratio |\n|---------------|-----------|------------|------------------|---------------|\n| High Resolution | 0-7 days | Original (15s) | 7 days | 1.0x |\n| Medium Resolution | 7-30 days | 5 minutes | 23 days | 0.05x |\n| Low Resolution | 30-365 days | 1 hour | 335 days | 0.004x |\n| Archive | 1+ years | Daily | 2 years | 0.0002x |\n\nA typical retention policy for application metrics might preserve 15-second resolution data for the past week, downsample to 5-minute resolution for the past month, and keep hourly averages for historical trend analysis. Critical business metrics might have longer high-resolution periods, while debugging metrics might have shorter retention.\n\nThe retention configuration specifies not just the time boundaries but also the aggregation functions to use during downsampling. Counter metrics typically use rate calculations, gauge metrics use averages, and histogram metrics require careful handling to preserve distribution characteristics. The policy also specifies which labels to preserve during aggregation - high-cardinality labels like instance might be dropped while keeping service and datacenter labels.\n\n#### Automated Downsampling Process\n\nThe downsampling process operates as a background task that periodically identifies data chunks eligible for resolution reduction. Rather than processing individual samples, the system works with compressed chunks to maintain efficiency. When a chunk ages past a resolution threshold, the downsampling process decompresses the chunk, aggregates samples into lower-resolution buckets, and creates new compressed chunks at the target resolution.\n\nThe aggregation process must handle different metric types appropriately to preserve semantic meaning. Counter metrics represent cumulative totals that should be converted to rates during downsampling. A counter chunk containing values [100, 115, 130, 145] over four 15-second intervals would become a single 1-minute rate of 0.75 increments per second. Gauge metrics represent point-in-time values and are typically averaged, though max, min, or last-value aggregations might be more appropriate depending on the metric semantics.\n\nHistogram metrics require special handling to preserve distribution information during downsampling. The system can't simply average the bucket counts, as this would destroy the ability to calculate accurate percentiles. Instead, histogram downsampling re-bins the constituent observations into the histogram buckets at the target resolution. This preserves the distributional characteristics while reducing storage overhead.\n\nThe downsampling process maintains metadata linking the original high-resolution chunks to their downsampled derivatives. This enables the query engine to automatically select the appropriate resolution level based on the query time range and step size. A query requesting data over a 30-day period with 1-hour steps can use the low-resolution data directly rather than aggregating high-resolution samples.\n\n#### Garbage Collection and Compaction\n\nAs data moves through the retention tiers and eventually expires, the storage engine must reclaim the associated storage space and update indexes accordingly. The garbage collection process operates independently of downsampling to avoid coupling data lifecycle decisions with storage management concerns.\n\n| Garbage Collection Phase | Scope | Actions | Frequency |\n|--------------------------|-------|---------|-----------|\n| Expired Data Deletion | Chunks older than max retention | Delete chunk files, remove index entries | Daily |\n| Index Compaction | Label indexes with many deleted entries | Rebuild compact indexes, reclaim memory | Weekly |\n| Chunk Compaction | Small chunks from same series | Merge chunks, improve compression ratio | Continuous |\n| Orphan Cleanup | Index entries without corresponding chunks | Remove stale index entries | Daily |\n\nThe garbage collection process begins by identifying chunks that have exceeded their configured retention period. Rather than immediately deleting these chunks, the system marks them for deletion and continues serving queries from the remaining data. A separate cleanup process deletes the marked chunks during off-peak hours to minimize I/O impact on active queries.\n\nIndex maintenance ensures that deleted time series don't leave stale entries in the inverted indexes. When all chunks for a particular series are deleted, the series registry entry is removed and all references to that series ID are purged from the label value indexes. This prevents memory leaks and maintains query performance as the active series set changes over time.\n\nChunk compaction addresses the storage inefficiency that occurs when many small chunks exist for the same time series. These small chunks typically result from irregular scraping or brief monitoring periods. The compaction process merges consecutive chunks from the same series into larger chunks, improving compression ratios and reducing the overhead of chunk metadata.\n\n### Persistence and Recovery\n\nThe storage engine must provide durability guarantees ensuring that accepted metric samples survive system crashes and hardware failures. Time series monitoring is often critical infrastructure used for alerting and incident response, making data loss unacceptable even in the face of unexpected failures. The persistence layer implements write-ahead logging and checkpoint-based recovery to provide these guarantees while maintaining write throughput under normal operation.\n\nThe durability challenge stems from the batch-oriented nature of time series ingestion. Samples arrive continuously from scraped targets and are buffered in memory before being compressed and written to persistent chunks. A naive approach might lose several minutes of samples if the system crashed before the in-memory buffers were flushed. The write-ahead log ensures that every accepted sample is persisted before acknowledging receipt, providing recovery capability even if compression and indexing haven't completed.\n\n> **Decision: Write-Ahead Log with Periodic Checkpoints**\n> - **Context**: Time series samples must be durably stored before acknowledgment to prevent data loss. In-memory compression buffers improve throughput but create a window of vulnerability during crashes.\n> - **Options Considered**:\n>   1. Synchronous writes with immediate persistence of each sample\n>   2. Asynchronous writes with potential sample loss during crashes  \n>   3. Write-ahead log with background compression and checkpoint recovery\n> - **Decision**: Implement write-ahead logging with periodic background checkpointing\n> - **Rationale**: WAL provides durability without sacrificing write throughput. Background compression maintains storage efficiency. Checkpoint recovery enables fast restart without replaying entire WAL history.\n> - **Consequences**: Requires additional disk I/O for WAL writes. Adds complexity for crash recovery logic. Provides strong durability guarantees with good performance.\n\n#### Write-Ahead Log Structure\n\nThe write-ahead log (WAL) provides an append-only record of all samples accepted by the storage engine. Unlike the compressed chunks optimized for query performance, the WAL prioritizes write speed and simplicity. Each WAL entry contains the complete information needed to reconstruct the sample: series identifier, timestamp, value, and metadata indicating the operation type.\n\n| WAL Entry Component | Size | Purpose |\n|--------------------|----- |---------|\n| Entry Type | 1 byte | Distinguishes sample writes from metadata operations |\n| Series ID | 8 bytes | Identifies the target time series |\n| Timestamp | 8 bytes | Sample observation time |\n| Value | 8 bytes | Metric value as IEEE 754 float64 |\n| Checksum | 4 bytes | CRC32 for corruption detection |\n\nThe WAL operates as a sequence of fixed-size segment files, each containing thousands of entries. When a segment reaches its maximum size (typically 64MB), the storage engine creates a new segment and continues appending to it. This segmented approach enables parallel processing during recovery and allows old segments to be deleted once their data has been safely incorporated into compressed chunks.\n\nEach WAL entry includes a CRC32 checksum to detect corruption from incomplete writes or storage hardware failures. During recovery, entries with invalid checksums are treated as the end of valid data, preventing corrupted entries from affecting the recovered state. The WAL also includes periodic checkpoint markers that record the current state of in-memory compression buffers, enabling incremental recovery rather than full replay.\n\nWAL writes use `fsync()` system calls to ensure data reaches persistent storage before returning control to the scrape engine. This provides strong durability guarantees at the cost of additional I/O latency. The storage engine batches multiple samples into single WAL writes when possible to amortize the `fsync()` overhead across multiple samples.\n\n#### Checkpoint and Recovery Process\n\nThe checkpoint process periodically captures the complete state of in-memory compression buffers and writes this state to persistent storage. Checkpoints enable fast recovery by providing a known good state that can be augmented with subsequent WAL entries rather than replaying the entire WAL history from system startup.\n\nDuring normal operation, the storage engine maintains several data structures in memory: partially compressed chunks for each active time series, label indexes mapping from values to series IDs, and series registries tracking metadata for all known series. The checkpoint process serializes all of this state into a compact binary format that can be quickly loaded during recovery.\n\n| Recovery Phase | Input | Processing | Output |\n|---------------|-------|------------|--------|\n| Checkpoint Load | Last complete checkpoint file | Deserialize in-memory state | Active series and partial chunks |\n| WAL Replay | WAL segments after checkpoint | Apply entries to in-memory state | Current state as of last WAL entry |\n| Index Rebuild | Recovered series metadata | Reconstruct label indexes | Complete inverted indexes |\n| Background Flush | In-memory chunks | Compress and write to storage | Cleaned up WAL segments |\n\nThe recovery process begins by identifying the most recent valid checkpoint and loading it into memory. This provides a baseline state that may be several minutes or hours old, depending on checkpoint frequency. The system then replays all WAL entries written after the checkpoint timestamp, applying each sample to the appropriate in-memory compression buffer.\n\nWAL replay handles several edge cases that can occur during normal operation. If a series referenced in a WAL entry doesn't exist in the checkpoint, recovery creates a new series with the appropriate metadata. If a WAL entry has an invalid checksum or refers to an impossible timestamp, recovery logs the error but continues processing subsequent entries. This provides robustness against partial corruption while preserving as much data as possible.\n\nAfter WAL replay completes, the storage engine rebuilds any indexes or auxiliary data structures that aren't included in the checkpoint format. The label value indexes are reconstructed by iterating through all recovered series and building the inverted mappings. This process ensures that queries work correctly immediately after recovery without waiting for new data to populate the indexes.\n\n#### Failure Scenarios and Recovery\n\nThe storage engine must handle various failure scenarios gracefully, from clean shutdowns to unexpected power loss during active writes. Each scenario requires different recovery strategies to ensure data consistency and minimize data loss.\n\n**Clean Shutdown**: When the storage engine receives a shutdown signal, it completes all in-flight compression operations and writes a final checkpoint before terminating. This provides a clean recovery state where the WAL contains no uncommitted data. Recovery from clean shutdown requires only loading the final checkpoint.\n\n**Crash During Compression**: If the system crashes while compressing chunks, the compressed data may be incomplete or corrupted. Recovery detects this by validating chunk headers and checksums. Corrupted chunks are discarded, and the data is recovered from WAL entries instead. This may require replaying more WAL history but ensures data consistency.\n\n**Crash During WAL Write**: A crash during WAL writing may leave a partially written entry at the end of the current segment. Recovery detects this using the entry checksums and treats the partial entry as the end of valid data. Subsequent entries (if any) are ignored, potentially losing the data from the incomplete write.\n\n**Disk Full During Operation**: When disk space is exhausted, the storage engine stops accepting new samples and enters read-only mode. WAL writes are suspended to prevent further disk consumption. The system attempts to complete background garbage collection to free space before resuming normal operation. If garbage collection can't free sufficient space, the system remains in read-only mode until manual intervention.\n\nThe recovery process includes extensive validation to detect and handle corruption in both WAL segments and checkpoint files. Checksums verify data integrity at multiple levels: individual WAL entries, segment boundaries, and complete checkpoint files. When corruption is detected, the system attempts to recover as much valid data as possible while clearly logging what data may have been lost.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Compression State Corruption During Concurrent Access**\n\nA common mistake is allowing multiple goroutines to modify the same compression chunk simultaneously. Gorilla compression maintains internal state about the previous timestamp and value to calculate deltas, and concurrent modifications corrupt this state. The symptoms are decompression failures or wildly incorrect values after decompression.\n\n**Why it's wrong**: Compression algorithms like Gorilla rely on strict ordering of samples and consistent state across the compression sequence. Concurrent writes can interleave samples from different goroutines, violating the monotonic timestamp assumption and corrupting the delta calculations.\n\n**How to fix**: Use mutex locks around each compression chunk during writes. The `ChunkBuilder` should include a `sync.Mutex` that protects the entire compress-and-append operation. Alternatively, use single-threaded compression with channels to serialize write access.\n\n⚠️ **Pitfall: Index Cardinality Explosion from Naive Label Storage**\n\nDevelopers often create inverted indexes that store every possible label combination, leading to exponential memory growth. With labels like `{service, instance, method, status}`, the combinations explode quickly - 10 services × 100 instances × 10 methods × 5 statuses = 50,000 index entries.\n\n**Why it's wrong**: Storing composite label combinations creates indexes that grow exponentially with label cardinality. Memory usage becomes unbounded, and index lookup performance degrades as hash tables become oversized.\n\n**How to fix**: Create separate inverted indexes for each label name, not for label combinations. Store `service_name → [series IDs]` and `instance_name → [series IDs]` separately, then compute intersections during query time. This provides linear memory growth with label cardinality.\n\n⚠️ **Pitfall: WAL Recovery Assumes Perfect Ordering**\n\nA frequent mistake is assuming that WAL entries are always perfectly ordered by timestamp during recovery. Network delays, clock drift, or buffering can cause samples to arrive and be written to the WAL in slightly different order than their actual timestamps.\n\n**Why it's wrong**: Recovery code that assumes strict timestamp ordering will fail when replaying out-of-order samples. This can manifest as assertion failures, incorrect compression, or samples being assigned to wrong time windows during replay.\n\n**How to fix**: During WAL replay, buffer samples in memory and sort them by timestamp before applying to compression chunks. Only apply samples to chunks when you're confident no earlier timestamp will arrive (e.g., after a time window has passed or during final recovery).\n\n⚠️ **Pitfall: Retention Policies Delete Active Chunks**\n\nDevelopers sometimes implement retention policies that delete chunks based solely on the chunk creation time, not the timestamp of the data within the chunk. This can delete chunks containing recent samples that were written to older chunks due to delayed ingestion.\n\n**Why it's wrong**: A chunk created yesterday might contain samples with timestamps from today if data arrived late. Deleting based on chunk creation time rather than sample timestamps causes data loss for delayed metrics.\n\n**How to fix**: Examine the actual timestamp range within each chunk during retention policy evaluation. Only delete chunks where the maximum sample timestamp is older than the retention threshold. Include a safety margin to account for possible clock drift.\n\n### Implementation Guidance\n\nThe storage engine requires careful coordination between compression algorithms, indexing structures, and persistence mechanisms. This implementation guidance provides concrete starting points for the core storage components while highlighting the areas where you'll implement the compression and lifecycle logic yourself.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Compression | Custom bitstream with sync.Pool | SIMD-optimized compression with cgo |\n| Indexing | Go maps with RWMutex | Radix trees or B+ trees |\n| Persistence | Standard os.File with fsync | mmap with explicit sync control |\n| WAL Format | Binary encoding/gob | Custom binary protocol |\n| Concurrency | Mutex per chunk | Lock-free data structures |\n\n#### Recommended File Structure\n\n```go\ninternal/storage/\n  storage.go              ← Main StorageEngine implementation\n  storage_test.go         ← Integration tests\n  chunk/\n    chunk.go              ← Compression chunk implementation  \n    gorilla.go            ← Gorilla compression algorithm\n    chunk_test.go         ← Compression algorithm tests\n  index/\n    inverted.go           ← Inverted index implementation\n    cardinality.go        ← Cardinality tracking\n    index_test.go         ← Index operation tests\n  wal/\n    wal.go                ← Write-ahead log\n    recovery.go           ← Crash recovery logic\n    wal_test.go           ← WAL and recovery tests\n  retention/\n    policy.go             ← Retention policy evaluation\n    compaction.go         ← Background compaction\n    retention_test.go     ← Lifecycle management tests\n```\n\n#### Storage Infrastructure Code\n\n```go\n// storage.go - Complete infrastructure for storage operations\npackage storage\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\n// StorageEngine manages time series persistence with compression and indexing\ntype StorageEngine struct {\n    config      *StorageConfig\n    logger      Logger\n    chunks      map[uint64]*CompressedChunk  // series_id -> current chunk\n    indexes     *InvertedIndexes\n    wal         *WriteAheadLog\n    mu          sync.RWMutex\n    shutdown    chan struct{}\n}\n\n// NewStorageEngine creates a configured storage engine instance\nfunc NewStorageEngine(config *StorageConfig, logger Logger) (*StorageEngine, error) {\n    se := &StorageEngine{\n        config:   config,\n        logger:   logger,\n        chunks:   make(map[uint64]*CompressedChunk),\n        indexes:  NewInvertedIndexes(),\n        shutdown: make(chan struct{}),\n    }\n    \n    var err error\n    se.wal, err = NewWriteAheadLog(config.data_directory, logger)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to initialize WAL: %w\", err)\n    }\n    \n    if err := se.recoverFromWAL(); err != nil {\n        return nil, fmt.Errorf(\"WAL recovery failed: %w\", err)\n    }\n    \n    go se.backgroundCompaction()\n    return se, nil\n}\n\n// CompressedChunk represents a time series chunk with Gorilla compression\ntype CompressedChunk struct {\n    seriesID     uint64\n    mint, maxt   int64        // min/max timestamp in chunk\n    samples      []byte       // compressed sample data  \n    sampleCount  int\n    mu           sync.Mutex\n}\n\n// WriteAheadLog provides durable storage for incoming samples\ntype WriteAheadLog struct {\n    dir        string\n    current    *os.File\n    segments   []string\n    logger     Logger\n    mu         sync.Mutex\n}\n\n// InvertedIndexes maintains label->series mappings for fast queries\ntype InvertedIndexes struct {\n    metricNames  map[string][]uint64                    // metric -> series IDs\n    labelValues  map[string]map[string][]uint64        // label_name -> value -> series IDs  \n    series       map[uint64]*SeriesMetadata            // series_id -> metadata\n    mu           sync.RWMutex\n}\n\n// SeriesMetadata contains the complete label set and storage info for a series\ntype SeriesMetadata struct {\n    ID          uint64\n    Labels      Labels\n    MetricName  string\n    ChunkRefs   []ChunkRef  // references to storage chunks\n}\n\n// ChunkRef points to a specific chunk in persistent storage  \ntype ChunkRef struct {\n    MinTime, MaxTime int64\n    Offset          int64   // file offset\n    Length          int32   // compressed size\n}\n```\n\n#### Core Storage Logic Skeletons\n\n```go\n// Append stores samples for the specified time series\n// This is the main ingestion entry point called by the scrape engine\nfunc (se *StorageEngine) Append(samples []*Sample) error {\n    // TODO 1: Write samples to WAL before in-memory processing for durability\n    // TODO 2: Group samples by series ID to batch processing per series\n    // TODO 3: For each series, find or create the current compression chunk\n    // TODO 4: Append samples to compression chunk using Gorilla algorithm\n    // TODO 5: If chunk reaches size limit, flush to persistent storage\n    // TODO 6: Update inverted indexes with any new label values discovered\n    // TODO 7: Return error if any step fails (WAL write, compression, indexing)\n    // Hint: Use se.mu.RLock() for reading, se.mu.Lock() for writing chunk map\n    // Hint: Batch WAL writes for multiple samples to improve throughput\n}\n\n// Select retrieves time series matching the label matchers in the time range\nfunc (se *StorageEngine) Select(start, end int64, matchers []*LabelMatcher) (SeriesSet, error) {\n    se.mu.RLock()\n    defer se.mu.RUnlock()\n    \n    // TODO 1: Use metric name matcher to get candidate series IDs from primary index\n    // TODO 2: For each remaining label matcher, get matching series IDs from label indexes\n    // TODO 3: Compute intersection of all matcher results to get final series ID set\n    // TODO 4: For each matching series ID, load chunk references from series metadata\n    // TODO 5: Filter chunk references to only those overlapping [start, end] time range\n    // TODO 6: Return SeriesSet that can iterate through matching series and decompress chunks\n    // Hint: Order matchers by selectivity (lowest cardinality first) for efficiency\n    // Hint: Use sorted slice intersection for large result sets\n}\n```\n\n#### Gorilla Compression Implementation Skeleton\n\n```go\n// gorilla.go - Gorilla compression algorithm implementation\npackage chunk\n\nimport (\n    \"encoding/binary\"\n    \"math\"\n)\n\n// GorillaCompressor implements delta-of-delta timestamp and XOR value compression\ntype GorillaCompressor struct {\n    buf              []byte\n    bitPos           int\n    \n    // Timestamp compression state\n    baseTimestamp    int64\n    prevTimestamp    int64  \n    prevDelta        int64\n    \n    // Value compression state  \n    baseValue        float64\n    prevValue        float64\n    prevLeadingZeros int\n    prevTrailingZeros int\n}\n\n// NewGorillaCompressor creates a new compressor with baseline values\nfunc NewGorillaCompressor(baseTimestamp int64, baseValue float64) *GorillaCompressor {\n    // TODO 1: Initialize compressor with baseline timestamp and value\n    // TODO 2: Allocate initial buffer for compressed bits (start with 1KB)\n    // TODO 3: Write baseline timestamp and value as uncompressed 64-bit values\n    // TODO 4: Set initial state for delta-of-delta and XOR compression\n    // Hint: Store baseline values in first 16 bytes for decompression\n}\n\n// AddSample compresses and appends a timestamp-value pair\nfunc (gc *GorillaCompressor) AddSample(timestamp int64, value float64) error {\n    // TODO 1: Calculate timestamp delta from previous timestamp\n    // TODO 2: Calculate delta-of-delta from previous delta\n    // TODO 3: Encode delta-of-delta using variable-length bit patterns\n    // TODO 4: XOR current value with previous value  \n    // TODO 5: Encode XOR result using leading/trailing zero compression\n    // TODO 6: Update internal state for next sample (prev timestamp, value, etc.)\n    // TODO 7: Expand buffer if needed to accommodate new bits\n    // Hint: Use bit manipulation helpers for variable-length encoding\n    // Hint: Delta-of-delta of 0 encodes as single bit, others use longer patterns\n}\n\n// CompressedData returns the final compressed chunk bytes\nfunc (gc *GorillaCompressor) CompressedData() []byte {\n    // TODO 1: Finalize bit stream by padding to byte boundary if needed\n    // TODO 2: Return copy of buffer truncated to actual used length  \n    // TODO 3: Include header with baseline timestamp/value and sample count\n    // Hint: Byte-align the final bit position for easier storage\n}\n```\n\n#### WAL and Recovery Implementation Skeleton\n\n```go\n// wal.go - Write-ahead log for durability\npackage wal\n\n// AppendSamples writes samples to WAL before in-memory processing\nfunc (wal *WriteAheadLog) AppendSamples(samples []*Sample) error {\n    wal.mu.Lock()\n    defer wal.mu.Unlock()\n    \n    // TODO 1: Create WAL entry for each sample with type, series ID, timestamp, value\n    // TODO 2: Calculate CRC32 checksum for each entry to detect corruption\n    // TODO 3: Write entries to current WAL segment file  \n    // TODO 4: Call fsync() to ensure data reaches persistent storage\n    // TODO 5: If current segment exceeds size limit, rotate to new segment\n    // TODO 6: Return error if any write or fsync operation fails\n    // Hint: Batch multiple samples into single write() call for efficiency\n    // Hint: Use binary.Write() for consistent cross-platform encoding\n}\n\n// recoverFromWAL replays WAL entries to rebuild in-memory state\nfunc (se *StorageEngine) recoverFromWAL() error {\n    // TODO 1: Find all WAL segment files in data directory sorted by sequence number\n    // TODO 2: Load most recent checkpoint file to get baseline state \n    // TODO 3: Replay WAL entries from checkpoint timestamp to end of latest segment\n    // TODO 4: For each entry, validate checksum and skip corrupted entries\n    // TODO 5: Apply valid entries to in-memory chunks and update indexes\n    // TODO 6: Rebuild any derived state (label indexes, series registry)\n    // TODO 7: Delete WAL segments that are fully incorporated into chunks\n    // Hint: Buffer and sort samples by timestamp before applying to handle out-of-order\n    // Hint: Log recovery progress for operational visibility\n}\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the storage engine core:\n\n**Basic Functionality Test:**\n```bash\ngo test ./internal/storage/... -v\n# Should pass tests for compression, indexing, and WAL recovery\n```\n\n**Manual Verification Steps:**\n1. Start storage engine and append 1000 samples across 10 different series\n2. Verify WAL file is created and contains expected number of entries  \n3. Query samples back using label matchers - should return correct data\n4. Stop process and restart - verify recovery loads all data correctly\n5. Wait for background compaction - verify compressed chunks are created\n\n**Performance Validation:**\n- Should handle 10,000 samples/second ingestion rate\n- Gorilla compression should achieve 5:1 compression ratio or better  \n- Label queries should return results in < 100ms for up to 10,000 series\n- WAL recovery should complete in < 30 seconds for 1GB of WAL data\n\n**Warning Signs:**\n- Memory usage growing without bound (likely cardinality explosion)\n- Compression ratio worse than 2:1 (implementation bug in Gorilla algorithm)\n- Recovery taking > 5 minutes (WAL segments not being cleaned up properly)\n- Query timeouts on small datasets (inefficient index intersection)\n\n\n## PromQL Query Engine\n\n> **Milestone(s):** This section directly corresponds to Milestone 4 (Query Engine) and integrates with all previous milestones by providing the query capabilities that retrieve and process the metrics data collected by the Scrape Engine (Milestone 2) and stored by the Time Series Storage Engine (Milestone 3) using the Metrics Data Model (Milestone 1).\n\nThe PromQL query engine transforms user queries into results by parsing expressions, selecting time series, applying aggregations, and formatting output. This component sits at the top of our metrics system architecture, providing the interface through which users extract insights from collected observability data. The query engine must balance expressiveness with performance, supporting complex analytical queries while maintaining sub-second response times over millions of time series.\n\n![Query Execution Flow](./diagrams/query-execution.svg)\n\nThe query engine operates through five coordinated stages: expression parsing transforms text queries into executable plans, label selection filters the universe of available time series, aggregation operations combine data across dimensions, range processing handles time-windowed queries, and result formatting prepares output for consumption. Each stage must handle high cardinality gracefully while providing meaningful error messages when queries cannot be satisfied.\n\n### SQL for Time Series Mental Model: Understanding PromQL through database query analogies\n\nThink of PromQL as **SQL for time series data**, where instead of selecting rows from tables, you're selecting and aggregating sequences of timestamped values. Just as SQL has `SELECT`, `FROM`, `WHERE`, `GROUP BY`, and aggregate functions, PromQL has metric selection, label filtering, time range specification, grouping operations, and mathematical functions. The key difference is that PromQL operates on the time dimension as a first-class concept, making it natural to ask questions like \"what was the average CPU usage over the last hour, grouped by service?\"\n\nIn traditional SQL, you might write `SELECT AVG(cpu_percent) FROM metrics WHERE service='api' AND timestamp > NOW() - INTERVAL '1 hour'`. In PromQL, this becomes `avg_over_time(cpu_percent{service=\"api\"}[1h])`. The bracket notation `[1h]` selects a time range, the curly braces `{service=\"api\"}` provide label filtering (equivalent to WHERE clauses), and `avg_over_time()` aggregates across the time dimension rather than across rows.\n\nThe **metric name acts like a table name** in SQL - it identifies the primary data source. Labels function like **indexed columns**, allowing efficient filtering and grouping operations. When you write `http_requests_total{method=\"GET\", status=\"200\"}`, you're essentially saying \"FROM http_requests_total WHERE method='GET' AND status='200'\". The time series database uses inverted indexes on labels just like SQL databases use B-tree indexes on columns.\n\n**Aggregation in PromQL differs from SQL aggregation** because it operates across multiple dimensions simultaneously. SQL typically aggregates rows within groups, but PromQL aggregates both across time (within each series) and across series (within label groups). When you write `sum by (service) (rate(http_requests_total[5m]))`, you're first calculating the per-second rate for each individual time series over 5-minute windows, then summing those rates across all series that share the same `service` label value.\n\nThe **time dimension** is what makes PromQL unique. Every query implicitly or explicitly operates over time ranges. An \"instant query\" like `cpu_usage{instance=\"server1\"}` asks for the most recent value, while a \"range query\" like `cpu_usage{instance=\"server1\"}[30m:1m]` asks for all values in the last 30 minutes at 1-minute resolution. This is similar to SQL window functions but built into the core query language.\n\n> **Key Insight**: PromQL treats time as a fundamental dimension, not just another column. This makes temporal operations natural but requires different thinking than traditional SQL query planning.\n\n### Query Parsing: Lexical analysis and AST construction for PromQL expressions\n\nThe query parser transforms text-based PromQL expressions into an **Abstract Syntax Tree (AST)** that represents the logical structure of operations and their dependencies. This parsing process follows the standard compiler design pattern of lexical analysis (tokenization) followed by syntactic analysis (AST construction), but with domain-specific considerations for time series expressions, mathematical operators, and label selectors.\n\n**Lexical analysis** (tokenization) breaks the input string into a sequence of meaningful tokens: metric names, label selectors, operators, function names, duration literals, and numeric constants. The lexer must handle PromQL-specific syntax like label selector curly braces `{job=\"api\"}`, range selectors with square brackets `[5m]`, and mathematical operators with appropriate precedence. Duration parsing requires special attention since PromQL supports units like `5m`, `2h`, `30s` that must be normalized to consistent internal representations.\n\n| Token Type | Examples | Description |\n|------------|----------|-------------|\n| `IDENTIFIER` | `http_requests_total`, `instance`, `job` | Metric names and label names |\n| `STRING` | `\"api\"`, `\"GET\"`, `\"/health\"` | Label values in quotes |\n| `NUMBER` | `123`, `3.14`, `1e6` | Numeric literals |\n| `DURATION` | `5m`, `2h`, `30s`, `1d` | Time duration specifiers |\n| `OPERATOR` | `+`, `-`, `*`, `/`, `%`, `^` | Mathematical operators |\n| `COMPARATOR` | `==`, `!=`, `=~`, `!~` | Label matching operators |\n| `AGGREGATOR` | `sum`, `avg`, `max`, `min`, `count` | Aggregation function names |\n| `FUNCTION` | `rate`, `increase`, `histogram_quantile` | Built-in function names |\n| `LBRACE` | `{` | Start of label selector |\n| `RBRACE` | `}` | End of label selector |\n| `LBRACKET` | `[` | Start of range selector |\n| `RBRACKET` | `]` | End of range selector |\n| `LPAREN` | `(` | Function call or grouping start |\n| `RPAREN` | `)` | Function call or grouping end |\n\n**Syntactic analysis** builds the AST by recognizing grammar patterns and enforcing operator precedence. PromQL expressions follow a hierarchy where function calls bind most tightly, followed by mathematical operators (with standard precedence: `^` > `*`, `/`, `%` > `+`, `-`), then aggregation operations, and finally binary operators between entire expressions. The parser must handle both instant queries (returning single values per series) and range queries (returning time-windowed data).\n\nThe **AST node structure** represents each operation type with specific node classes that capture the operation's semantics and operands:\n\n| Node Type | Purpose | Children | Attributes |\n|-----------|---------|----------|------------|\n| `MetricSelectorNode` | Selects time series by name and labels | None | `MetricName string`, `LabelMatchers []LabelMatcher` |\n| `RangeSelectorNode` | Applies time range to metric selector | `MetricSelectorNode` | `Duration time.Duration`, `Offset time.Duration` |\n| `FunctionCallNode` | Applies function to arguments | Variable argument nodes | `FunctionName string`, `Args []ASTNode` |\n| `AggregationNode` | Groups and aggregates series | Expression node | `Operation string`, `GroupBy []string`, `Without []string` |\n| `BinaryOperationNode` | Mathematical operation between expressions | Left and right expression nodes | `Operator string`, `Matching *VectorMatching` |\n| `NumberLiteralNode` | Numeric constant | None | `Value float64` |\n| `StringLiteralNode` | String constant | None | `Value string` |\n\n**Error handling during parsing** must provide meaningful diagnostics that help users understand syntax problems. Common parsing errors include mismatched brackets in label selectors, invalid duration formats, undefined function names, and operator precedence confusion. The parser should report the exact character position and suggest corrections when possible.\n\n> **Decision: Recursive Descent vs. Parser Generator**\n> - **Context**: Need to choose parsing strategy for PromQL expressions\n> - **Options Considered**: Hand-written recursive descent, YACC/Bison parser generator, PEG parser\n> - **Decision**: Hand-written recursive descent parser\n> - **Rationale**: PromQL grammar is relatively simple, recursive descent provides better error messages and easier debugging, eliminates external tool dependencies\n> - **Consequences**: More code to maintain but complete control over error reporting and parsing behavior\n\nThe **parsing algorithm** follows these steps for each input query:\n\n1. **Initialize lexer** with input string and begin tokenization\n2. **Parse primary expression** starting with metric selector, number literal, or parenthesized expression\n3. **Handle range selector** if current token is `[` - parse duration and optional offset\n4. **Process function calls** if primary expression is followed by `(` - parse argument list recursively\n5. **Parse mathematical operators** according to precedence rules - left-associative except for `^`\n6. **Handle aggregation operations** if expression starts with aggregation keyword - parse grouping clauses\n7. **Validate semantic constraints** - ensure range selectors only applied to metric selectors, function argument types match\n8. **Return AST root node** or parsing error with location information\n\n**Common parsing pitfalls** include incorrect operator precedence handling, failure to validate semantic constraints during parsing (allowing syntactically valid but meaningless expressions), and poor error recovery that cascades single mistakes into many error messages.\n\n⚠️ **Pitfall: Precedence Confusion**\nMany developers incorrectly implement operator precedence, leading to expressions like `2 + 3 * 4` being parsed as `(2 + 3) * 4` instead of `2 + (3 * 4)`. Always implement precedence through recursive parsing methods where higher-precedence operators are parsed by deeper recursion levels, ensuring natural left-to-right evaluation with correct operator binding.\n\n### Label Selector Engine: Exact, regex, and inequality matching for filtering time series\n\nThe label selector engine filters the universe of available time series down to those matching specific label criteria, acting as the **WHERE clause** of PromQL queries. This component must efficiently handle four types of label matching: exact equality, inequality, regular expression matching, and negative regular expression matching. Performance is critical since label selection often processes millions of time series and determines the working set size for subsequent aggregation operations.\n\n**Label matching semantics** define how series are included or excluded based on their label values. The four matcher types each serve different filtering use cases and have distinct performance characteristics:\n\n| Matcher Type | Syntax | Example | Use Case | Performance Notes |\n|--------------|--------|---------|----------|------------------|\n| Exact Match | `label=\"value\"` | `job=\"api\"` | Filter by known label values | Fastest - direct index lookup |\n| Not Equal | `label!=\"value\"` | `status!=\"200\"` | Exclude specific values | Moderate - inverse index scan |\n| Regex Match | `label=~\"pattern\"` | `instance=~\"web-.*\"` | Pattern-based inclusion | Slowest - requires regex evaluation |\n| Negative Regex | `label!~\"pattern\"` | `path!~\"/debug/.*\"` | Pattern-based exclusion | Slowest - requires regex evaluation |\n\nThe **matching algorithm** processes label selectors against the inverted indexes built by the storage engine. For exact matches, the engine performs direct hash table lookups in the `labelValues` index to retrieve the list of series IDs containing that label-value pair. For inequality matches, the engine retrieves the exact match set and computes its complement against all series containing that label name.\n\n**Regular expression matching** requires compiling regex patterns and evaluating them against all possible values for the specified label name. This is inherently expensive since it cannot use pre-built indexes effectively. The selector engine maintains a **regex compilation cache** to avoid recompiling the same patterns repeatedly, and applies optimizations like prefix extraction when patterns start with literal strings.\n\n| Label Selection Step | Operation | Data Structure Used | Time Complexity |\n|---------------------|-----------|-------------------|-----------------|\n| Exact Match Lookup | `labelValues[labelName][labelValue]` | Hash map of hash maps | O(1) average case |\n| Inequality Filtering | Set complement operation | Bitmap or hash set | O(n) where n = series count |\n| Regex Compilation | `regexp.Compile(pattern)` | Cached compiled regex | O(m) where m = pattern complexity |\n| Regex Evaluation | Pattern matching against all values | Linear scan with regex | O(k*m) where k = unique values |\n\nThe **series intersection algorithm** combines multiple label matchers using set operations. When a query contains multiple label selectors like `{job=\"api\", status!=\"500\", instance=~\"web-.*\"}`, the engine must find series that satisfy ALL conditions simultaneously. The algorithm processes matchers in optimal order, starting with the most selective (smallest result set) to minimize subsequent work.\n\n**Intersection processing** follows these steps:\n\n1. **Evaluate each matcher independently** to get per-matcher series ID sets\n2. **Sort matchers by selectivity** - exact matches first, then inequalities, finally regex patterns\n3. **Initialize result set** with most selective matcher's series IDs\n4. **Intersect remaining matchers** - remove series IDs that don't appear in subsequent matcher results\n5. **Handle special cases** - empty label selectors match all series, contradictory selectors return empty sets\n6. **Return final series ID list** sorted for consistent downstream processing\n\n**Label name filtering** handles cases where queries filter on labels that may not exist on all series. A matcher like `{version=\"1.2\"}` should only consider series that actually have a `version` label, ignoring series where this label is absent entirely. The absence of a label is semantically different from a label with an empty string value.\n\n> **Decision: Bitmap vs. Hash Set for Series Intersection**\n> - **Context**: Need efficient set operations for combining label matcher results\n> - **Options Considered**: Hash sets, sorted arrays with binary search, bitmap arrays\n> - **Decision**: Hash sets for low cardinality, bitmap arrays for high cardinality with cutoff at 10,000 series\n> - **Rationale**: Hash sets provide O(1) membership testing but have memory overhead; bitmaps are more space-efficient for dense ID ranges but require more memory for sparse ranges\n> - **Consequences**: Adaptive algorithm provides good performance across different cardinality ranges but adds implementation complexity\n\n**Performance optimizations** for label selection include:\n\n- **Early termination**: Stop processing additional matchers if intermediate result set becomes empty\n- **Matcher reordering**: Process most selective matchers first to minimize subsequent intersection work\n- **Regex optimization**: Extract literal prefixes from regex patterns to use index lookups before pattern matching\n- **Cache compiled regexes**: Avoid recompiling the same patterns across multiple query executions\n- **Parallel evaluation**: Process independent matchers concurrently when result sets are large\n\n**Common label selection pitfalls** include incorrect handling of missing labels (treating absence as empty string), inefficient regex patterns that can't use index optimizations, and failure to validate regex syntax at parse time leading to runtime compilation errors.\n\n⚠️ **Pitfall: Missing Label Semantics**\nA common mistake is treating missing labels as empty strings. The query `{version=\"\"}` should match series with an empty `version` label, while `{version!=\"stable\"}` should match series with any `version` value except \"stable\" but NOT series missing the `version` label entirely. Always check for label existence before value comparison.\n\n### Aggregation Operations: Sum, average, percentile, and grouping operations across label dimensions\n\nAggregation operations combine multiple time series into fewer series by applying mathematical functions across specified label dimensions. Think of aggregation as **GROUP BY operations in SQL**, where you collapse many rows into summary values, except PromQL aggregates across both the series dimension (combining multiple time series) and optionally the time dimension (combining multiple timestamps within each series).\n\nThe **aggregation mental model** treats each time series as a vector of timestamped values, and aggregation as matrix operations that combine these vectors according to grouping rules. When you write `sum by (service) (cpu_usage)`, you're partitioning all `cpu_usage` time series into groups based on their `service` label value, then computing the element-wise sum within each group to produce one output series per unique service.\n\n**Grouping semantics** determine which series get combined together. PromQL supports two grouping modes: `by` (include specified labels in output) and `without` (exclude specified labels from output). The grouping operation creates **aggregation groups** where each group contains series that share identical values for the grouping labels.\n\n| Grouping Mode | Syntax | Example | Behavior |\n|---------------|--------|---------|----------|\n| Group By | `sum by (label1, label2) (metric)` | `sum by (service) (cpu_usage)` | Output series contain only specified labels |\n| Group Without | `sum without (label1, label2) (metric)` | `sum without (instance) (memory_usage)` | Output series exclude specified labels |\n| No Grouping | `sum(metric)` | `sum(active_connections)` | All series combined into single output series |\n\nThe **aggregation algorithm** processes series in groups, applying the aggregation function to corresponding timestamps across all series within each group. For a timestamp T, the aggregator collects all values at timestamp T from series in the group, applies the mathematical function (sum, average, max, etc.), and produces a single output value for that timestamp.\n\n| Aggregation Function | Mathematical Operation | Null Value Handling | Use Cases |\n|---------------------|----------------------|-------------------|-----------|\n| `sum` | Add all values | Skip missing values | Total resource usage, request counts |\n| `avg` | Sum divided by count | Skip missing values | Average response times, utilization rates |\n| `max` | Largest value | Skip missing values | Peak resource usage, worst-case latencies |\n| `min` | Smallest value | Skip missing values | Best-case performance, minimum availability |\n| `count` | Number of series | Count series with any value | Number of active instances, error rate calculation |\n| `stddev` | Standard deviation | Skip missing values | Performance consistency, outlier detection |\n| `quantile` | Percentile calculation | Skip missing values | SLA monitoring, latency distribution analysis |\n\n**Quantile aggregation** deserves special attention because it requires maintaining the full distribution of values rather than computing incremental statistics. The `quantile(0.95, metric)` function needs all individual values at each timestamp to determine the 95th percentile, making it more memory-intensive than functions like sum or average that can be computed incrementally.\n\n**Implementation steps** for aggregation processing:\n\n1. **Parse grouping specification** - extract `by` or `without` label lists from AST\n2. **Group series by label values** - create aggregation groups based on grouping rules\n3. **Align timestamps across series** - ensure all series in each group have consistent timestamp alignment\n4. **Apply aggregation function** - for each timestamp, collect values from all series in group and compute result\n5. **Handle missing values** - skip series with no value at specific timestamps or use last-observed-value\n6. **Construct output series** - create result time series with group labels and aggregated values\n7. **Sort output consistently** - ensure deterministic ordering for reproducible results\n\n**Memory management** during aggregation requires careful attention since large aggregation groups can consume significant memory. The aggregator processes timestamps sequentially rather than loading entire series into memory, using a sliding window approach for functions that need historical context.\n\n**Label handling** in aggregation output requires combining the labels from input series according to grouping rules. For `sum by (service) (http_requests{service=\"api\", instance=\"web-1\", method=\"GET\"})`, the output series contains only `{service=\"api\"}`, dropping the `instance` and `method` labels. The aggregator must compute the Cartesian product of all possible label value combinations for the grouping labels.\n\n| Input Series | Labels | Grouping: by (service, status) | Output Group Key |\n|--------------|--------|-------------------------------|------------------|\n| Series 1 | `{service=\"api\", status=\"200\", instance=\"web-1\"}` | service=api, status=200 | `{service=\"api\", status=\"200\"}` |\n| Series 2 | `{service=\"api\", status=\"200\", instance=\"web-2\"}` | service=api, status=200 | `{service=\"api\", status=\"200\"}` |\n| Series 3 | `{service=\"api\", status=\"500\", instance=\"web-1\"}` | service=api, status=500 | `{service=\"api\", status=\"500\"}` |\n| Series 4 | `{service=\"db\", status=\"200\", instance=\"db-1\"}` | service=db, status=200 | `{service=\"db\", status=\"200\"}` |\n\n> **Decision: Streaming vs. Batch Aggregation**\n> - **Context**: Choose processing model for aggregating large numbers of time series\n> - **Options Considered**: Load all data then aggregate, stream processing with fixed memory, hybrid approach\n> - **Decision**: Streaming aggregation with timestamp-aligned windows\n> - **Rationale**: Provides bounded memory usage regardless of input size, enables processing datasets larger than available RAM, maintains low latency for small aggregations\n> - **Consequences**: More complex implementation with windowing logic but scales to arbitrary dataset sizes without memory exhaustion\n\n**Performance considerations** for aggregation include:\n\n- **Series cardinality**: Aggregations over high-cardinality labels create many output groups, increasing memory usage\n- **Timestamp alignment**: Series with misaligned timestamps require interpolation or bucketing to aggregate consistently\n- **Lazy evaluation**: Defer aggregation computation until results are actually needed by subsequent operations\n- **Parallel processing**: Process independent aggregation groups concurrently when group count is high\n- **Memory pooling**: Reuse buffers for intermediate calculations to reduce garbage collection pressure\n\n**Common aggregation mistakes** include incorrect null value handling (treating missing data as zero instead of skipping it), memory exhaustion on high-cardinality aggregations, and timestamp alignment issues when series have different scrape intervals.\n\n⚠️ **Pitfall: High-Cardinality Aggregation Groups**\nAggregating `by (instance_id)` where `instance_id` is a UUID will create one output series per instance, potentially millions of series. This defeats the purpose of aggregation and can cause memory exhaustion. Always aggregate by low-cardinality labels like service names, not high-cardinality identifiers like instance IDs or request IDs.\n\n### Range Query Execution: Retrieving and interpolating data points across time windows\n\nRange queries retrieve time series data across specified time windows, returning multiple data points per series rather than single instant values. Think of range queries as **time-windowed SELECT statements** that scan horizontally across the time dimension, collecting all data points between start and end timestamps at regular step intervals. The range query executor must handle timestamp alignment, missing data interpolation, and efficient data retrieval from the underlying storage engine.\n\n**Range query semantics** define three key parameters: start time (beginning of time window), end time (end of time window), and step duration (resolution of output data points). A range query like `/api/v1/query_range?query=cpu_usage&start=1609459200&end=1609462800&step=60` requests CPU usage data from timestamp 1609459200 to 1609462800 with 60-second resolution, producing one output value per series every minute.\n\nThe **query execution model** operates differently from instant queries because it must process multiple timestamps sequentially. Instead of asking \"what is the current value?\", range queries ask \"what were all the values between time A and time B at intervals of N seconds?\". This requires coordinating between timestamp generation (creating the output timeline) and value retrieval (fetching data from storage).\n\n| Range Query Component | Purpose | Input | Output |\n|----------------------|---------|-------|--------|\n| Time Range Parser | Extract start/end/step from parameters | Query string parameters | Parsed time boundaries |\n| Step Generator | Create output timestamp sequence | Start, end, step duration | Array of query timestamps |\n| Series Selector | Find matching time series | Label matchers | List of series IDs |\n| Data Retriever | Fetch samples from storage | Series IDs, time range | Raw sample data |\n| Value Interpolator | Fill gaps in data | Sparse samples, query timestamps | Dense timestamp-value pairs |\n| Result Formatter | Structure output data | Dense data, metadata | JSON/HTTP response |\n\n**Timestamp alignment** ensures that output data points appear at regular intervals regardless of when the underlying samples were actually collected. If the query requests data every 60 seconds starting at 12:00:00, the output timestamps should be 12:00:00, 12:01:00, 12:02:00, etc., even if the scraped samples occurred at 12:00:15, 12:01:03, 12:02:21, etc.\n\nThe **alignment algorithm** follows these steps:\n\n1. **Generate query timeline** - create sequence of timestamps from start to end at step intervals\n2. **Retrieve raw sample data** - fetch all samples for selected series within the time range\n3. **Sort samples by timestamp** - ensure chronological ordering for interpolation\n4. **Align to query timeline** - for each query timestamp, find the closest sample within tolerance\n5. **Interpolate missing values** - use last-observed-value or linear interpolation for gaps\n6. **Handle staleness** - mark values as stale if no recent sample exists within staleness threshold\n7. **Format output matrix** - structure results as matrix with consistent timestamp alignment\n\n**Missing data handling** is crucial because real monitoring systems have gaps due to network failures, service restarts, or scraping errors. The range query executor must distinguish between three scenarios: temporarily missing data (use interpolation), permanently missing data (return null), and series that don't exist during part of the time range (return partial results).\n\n| Missing Data Scenario | Detection Criteria | Handling Strategy | Output Behavior |\n|----------------------|-------------------|------------------|-----------------|\n| Recent Gap | No sample within staleness threshold | Return null value | Gaps in output timeline |\n| Historical Gap | Sample exists before/after gap | Last-observed-value interpolation | Filled with previous value |\n| Series Creation | First sample after query start time | Partial series data | Null values before first sample |\n| Series Deletion | Last sample before query end time | Partial series data | Null values after last sample |\n| Complete Absence | No samples in entire time range | Empty series | All null values |\n\n**Storage integration** requires efficient range scans that minimize disk I/O and decompression overhead. The range query executor works with the storage engine's `Select(start, end, matchers)` method to retrieve only relevant data, avoiding the cost of scanning unrelated time series or unnecessary time ranges.\n\n**Interpolation strategies** determine how to fill gaps between actual sample timestamps and desired query timestamps. The most common approach is **last-observed-value** interpolation, where each query timestamp receives the value of the most recent actual sample. This reflects the reality that metrics typically represent ongoing states (like CPU usage) that remain valid until explicitly updated.\n\n> **Decision: Last-Observed-Value vs. Linear Interpolation**\n> - **Context**: Choose interpolation strategy for aligning samples to query timestamps\n> - **Options Considered**: Nearest neighbor, last-observed-value, linear interpolation, no interpolation\n> - **Decision**: Last-observed-value with 5-minute staleness threshold\n> - **Rationale**: Reflects semantic meaning of monitoring metrics which represent current state until updated; linear interpolation implies continuous change which is incorrect for many metrics like error counts\n> - **Consequences**: More accurate representation of monitoring data but can show \"stair-step\" graphs instead of smooth curves\n\n**Performance optimizations** for range queries include:\n\n- **Parallel series processing**: Retrieve data for multiple series concurrently when series count is high\n- **Chunk boundary alignment**: Align query ranges with storage chunk boundaries to minimize decompression overhead\n- **Result streaming**: Begin returning results before all data is processed for large queries\n- **Memory management**: Process series in batches to avoid loading entire result set into memory\n- **Query caching**: Cache results for repeated range queries over the same time periods\n\n**Query complexity management** prevents resource exhaustion from overly broad range queries. Limits include maximum time range duration, maximum number of series that can be processed, and maximum number of output data points. A query requesting 1-second resolution over 30 days would produce 2.6 million data points per series, potentially overwhelming the system.\n\n| Resource Limit | Default Value | Purpose | Enforcement Point |\n|----------------|---------------|---------|------------------|\n| Max Range Duration | 365 days | Prevent excessive time spans | Query validation |\n| Max Series Count | 10,000 | Prevent high-cardinality explosions | After label selection |\n| Max Sample Count | 50 million | Prevent memory exhaustion | Before data retrieval |\n| Query Timeout | 30 seconds | Prevent long-running queries | HTTP handler level |\n\n**Range query processing pipeline** coordinates these steps:\n\n1. **Parse and validate parameters** - extract start/end/step, validate ranges and limits\n2. **Execute instant query parsing** - parse PromQL expression into AST\n3. **Generate evaluation timestamps** - create array of timestamps at step intervals\n4. **For each timestamp**: execute instant query evaluation to get point-in-time results\n5. **Collect timestamp-value matrices** - accumulate results across all evaluation points\n6. **Format as range query response** - structure output with series metadata and value matrices\n\n**Error handling** in range queries must address scenarios like storage unavailability during long-running queries, memory exhaustion from overly large result sets, and timeout expiration during processing. The executor should provide partial results when possible rather than failing entirely.\n\n⚠️ **Pitfall: Step Interval Too Small**\nSetting step intervals much smaller than the underlying scrape interval (e.g., 1-second steps when data is scraped every 15 seconds) produces misleading results with excessive interpolation. The step should generally be no smaller than the scrape interval to avoid artificial data smoothing. Always validate that step >= scrape_interval for meaningful results.\n\n### Implementation Guidance\n\nThe PromQL query engine requires careful coordination between parsing, execution, and result formatting components. This implementation focuses on building a working query engine that can handle the core PromQL operations while maintaining good performance characteristics.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|----------------|\n| Expression Parsing | Hand-written recursive descent | ANTLR grammar generator |\n| Regular Expressions | Go's `regexp` package | RE2 library for better performance |\n| JSON Serialization | Go's `encoding/json` | Custom JSON streaming for large results |\n| HTTP API | Go's `net/http` with custom handlers | Gorilla Mux for advanced routing |\n| Concurrent Processing | Go routines with sync primitives | Worker pool pattern with channels |\n| Caching | Simple `sync.Map` for query cache | LRU cache with TTL expiration |\n\n**Recommended File Structure:**\n\n```\ninternal/query/\n  engine.go              ← main QueryEngine type and coordination\n  engine_test.go         ← end-to-end query tests\n  parser/\n    parser.go            ← PromQL expression parsing\n    ast.go               ← AST node type definitions\n    lexer.go             ← tokenization and lexical analysis\n    parser_test.go       ← parsing unit tests\n  selector/\n    selector.go          ← label matching and series selection\n    matcher.go           ← individual label matcher implementations\n    selector_test.go     ← selection logic tests\n  aggregation/\n    aggregator.go        ← aggregation function implementations\n    grouping.go          ← series grouping logic\n    functions.go         ← built-in aggregation functions\n    aggregation_test.go  ← aggregation correctness tests\n  execution/\n    instant.go           ← instant query execution\n    range.go             ← range query execution\n    interpolation.go     ← value interpolation and alignment\n    execution_test.go    ← query execution tests\n```\n\n**Core Query Engine Infrastructure (Complete):**\n\n```go\npackage query\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sort\"\n    \"time\"\n)\n\n// QueryEngine coordinates parsing, execution, and formatting of PromQL queries\ntype QueryEngine struct {\n    storage      StorageEngine\n    parser       *ExpressionParser\n    selector     *LabelSelector\n    aggregator   *Aggregator\n    config       QueryConfig\n    queryTimeout time.Duration\n}\n\n// QueryResult represents the output of query execution\ntype QueryResult struct {\n    ResultType string                 `json:\"resultType\"`\n    Result     interface{}           `json:\"result\"`\n    Warnings   []string              `json:\"warnings,omitempty\"`\n}\n\n// InstantQueryResult represents point-in-time query results\ntype InstantQueryResult struct {\n    Metric map[string]string `json:\"metric\"`\n    Value  [2]interface{}    `json:\"value\"` // [timestamp, value]\n}\n\n// RangeQueryResult represents time-windowed query results\ntype RangeQueryResult struct {\n    Metric map[string]string   `json:\"metric\"`\n    Values [][]interface{}     `json:\"values\"` // [[timestamp, value], ...]\n}\n\n// NewQueryEngine creates a configured query engine\nfunc NewQueryEngine(storage StorageEngine, config QueryConfig) *QueryEngine {\n    return &QueryEngine{\n        storage:      storage,\n        parser:       NewExpressionParser(),\n        selector:     NewLabelSelector(storage),\n        aggregator:   NewAggregator(),\n        config:       config,\n        queryTimeout: config.query_timeout,\n    }\n}\n\n// ExecuteInstantQuery processes point-in-time PromQL queries\nfunc (e *QueryEngine) ExecuteInstantQuery(ctx context.Context, query string, evalTime time.Time) (*QueryResult, error) {\n    ctx, cancel := context.WithTimeout(ctx, e.queryTimeout)\n    defer cancel()\n\n    // Parse expression into AST\n    expr, err := e.parser.ParseExpression(query)\n    if err != nil {\n        return nil, fmt.Errorf(\"parse error: %w\", err)\n    }\n\n    // Execute expression evaluation\n    result, err := e.evaluateExpression(ctx, expr, evalTime)\n    if err != nil {\n        return nil, fmt.Errorf(\"evaluation error: %w\", err)\n    }\n\n    // Format results for API response\n    return e.formatInstantResult(result), nil\n}\n\n// ExecuteRangeQuery processes time-windowed PromQL queries\nfunc (e *QueryEngine) ExecuteRangeQuery(ctx context.Context, query string, start, end time.Time, step time.Duration) (*QueryResult, error) {\n    ctx, cancel := context.WithTimeout(ctx, e.queryTimeout)\n    defer cancel()\n\n    // Validate range query parameters\n    if err := e.validateRangeParams(start, end, step); err != nil {\n        return nil, err\n    }\n\n    // Parse expression into AST\n    expr, err := e.parser.ParseExpression(query)\n    if err != nil {\n        return nil, fmt.Errorf(\"parse error: %w\", err)\n    }\n\n    // Generate evaluation timeline\n    timestamps := e.generateTimeline(start, end, step)\n    \n    // Execute expression at each timestamp\n    results := make(map[string][]interface{})\n    for _, ts := range timestamps {\n        result, err := e.evaluateExpression(ctx, expr, ts)\n        if err != nil {\n            return nil, fmt.Errorf(\"evaluation error at %v: %w\", ts, err)\n        }\n        e.accumulateRangeResults(results, result, ts)\n    }\n\n    return e.formatRangeResult(results), nil\n}\n```\n\n**Expression Parser Implementation (Core Logic Skeleton):**\n\n```go\npackage parser\n\nimport (\n    \"fmt\"\n    \"regexp\"\n    \"strconv\"\n    \"time\"\n)\n\n// ExpressionParser handles PromQL expression parsing\ntype ExpressionParser struct {\n    lexer *Lexer\n}\n\n// ASTNode represents nodes in the abstract syntax tree\ntype ASTNode interface {\n    String() string\n    Accept(visitor ASTVisitor) (interface{}, error)\n}\n\n// MetricSelectorNode represents metric selection with label matchers\ntype MetricSelectorNode struct {\n    MetricName    string\n    LabelMatchers []LabelMatcher\n}\n\n// ParseExpression converts PromQL text into executable AST\nfunc (p *ExpressionParser) ParseExpression(input string) (ASTNode, error) {\n    p.lexer = NewLexer(input)\n    \n    // TODO 1: Initialize lexer with input string and advance to first token\n    // TODO 2: Parse primary expression (metric selector, number, or parenthesized expression)\n    // TODO 3: Check for range selector [5m] after metric selector\n    // TODO 4: Handle function calls if expression followed by (\n    // TODO 5: Parse binary operators according to precedence rules\n    // TODO 6: Handle aggregation operations (sum, avg, etc.) with grouping\n    // TODO 7: Validate semantic constraints (range selectors only on metric selectors)\n    // TODO 8: Return completed AST or detailed parsing error\n    \n    // Hint: Use recursive descent with separate methods for each precedence level\n    return p.parseExpression()\n}\n\n// parseMetricSelector handles metric{label=\"value\"} syntax\nfunc (p *ExpressionParser) parseMetricSelector() (*MetricSelectorNode, error) {\n    // TODO 1: Parse metric name identifier\n    // TODO 2: Check for opening brace { to start label selector\n    // TODO 3: Parse label matcher list: label op \"value\", ...\n    // TODO 4: Validate label names and values\n    // TODO 5: Ensure closing brace } terminates label selector\n    // TODO 6: Return MetricSelectorNode with parsed components\n    \n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// parseLabelMatchers handles {job=\"api\", instance!=\"web-1\"} syntax\nfunc (p *ExpressionParser) parseLabelMatchers() ([]LabelMatcher, error) {\n    var matchers []LabelMatcher\n    \n    // TODO 1: Parse comma-separated list of label matchers\n    // TODO 2: For each matcher: parse label name, operator, and value\n    // TODO 3: Validate operator types (=, !=, =~, !~)\n    // TODO 4: Handle quoted string values with escape sequences\n    // TODO 5: Validate regex patterns for =~ and !~ operators\n    // TODO 6: Return completed matcher list\n    \n    return matchers, nil\n}\n```\n\n**Label Selector Implementation (Core Logic Skeleton):**\n\n```go\npackage selector\n\nimport (\n    \"regexp\"\n    \"sort\"\n)\n\n// LabelSelector filters time series based on label criteria\ntype LabelSelector struct {\n    storage       StorageEngine\n    regexCache    map[string]*regexp.Regexp\n    maxCacheSize  int\n}\n\n// LabelMatcher represents a single label filtering criterion\ntype LabelMatcher struct {\n    Name     string\n    Value    string\n    Type     MatchType\n    Regex    *regexp.Regexp\n}\n\n// MatchType defines the label matching operation\ntype MatchType int\n\nconst (\n    MatchEqual MatchType = iota\n    MatchNotEqual\n    MatchRegex\n    MatchNotRegex\n)\n\n// SelectSeries finds time series matching all provided label matchers\nfunc (s *LabelSelector) SelectSeries(matchers []LabelMatcher) ([]uint64, error) {\n    // TODO 1: Handle empty matcher list (return all series)\n    // TODO 2: Sort matchers by estimated selectivity (exact matches first)\n    // TODO 3: Evaluate most selective matcher to get initial candidate set\n    // TODO 4: For remaining matchers, intersect with candidates\n    // TODO 5: Handle regex matchers by compiling patterns and testing values\n    // TODO 6: Return sorted list of matching series IDs\n    \n    // Hint: Use storage.indexes.labelValues for efficient exact match lookups\n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// evaluateExactMatch finds series with specific label value\nfunc (s *LabelSelector) evaluateExactMatch(matcher LabelMatcher) ([]uint64, error) {\n    // TODO 1: Look up label-value combination in inverted index\n    // TODO 2: Return series ID list or empty slice if not found\n    // TODO 3: Handle case where label name doesn't exist\n    \n    return nil, nil\n}\n\n// evaluateRegexMatch finds series matching regex pattern\nfunc (s *LabelSelector) evaluateRegexMatch(matcher LabelMatcher) ([]uint64, error) {\n    // TODO 1: Get compiled regex from cache or compile new pattern\n    // TODO 2: Retrieve all values for the label name from index\n    // TODO 3: Test each value against regex pattern\n    // TODO 4: Collect series IDs for all matching values\n    // TODO 5: Update regex cache if pattern was newly compiled\n    \n    return nil, nil\n}\n```\n\n**Aggregation Engine Implementation (Core Logic Skeleton):**\n\n```go\npackage aggregation\n\nimport (\n    \"math\"\n    \"sort\"\n)\n\n// Aggregator handles grouping and mathematical aggregation of time series\ntype Aggregator struct {\n    maxGroupSize int\n}\n\n// AggregationRequest specifies how to aggregate time series data\ntype AggregationRequest struct {\n    Function   string\n    GroupBy    []string\n    Without    []string\n    Series     []SeriesData\n    Timestamp  time.Time\n}\n\n// SeriesData represents a time series with its labels and current value\ntype SeriesData struct {\n    Labels Labels\n    Value  float64\n    Valid  bool\n}\n\n// AggregateSeries applies aggregation function to grouped time series\nfunc (a *Aggregator) AggregateSeries(req AggregationRequest) ([]SeriesData, error) {\n    // TODO 1: Create aggregation groups based on groupBy/without labels\n    // TODO 2: Partition input series into groups by label values\n    // TODO 3: Apply aggregation function within each group\n    // TODO 4: Handle missing/invalid values appropriately\n    // TODO 5: Construct output series with group labels\n    // TODO 6: Sort output series for consistent results\n    \n    // Hint: Use createAggregationGroups helper to handle grouping logic\n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// createAggregationGroups partitions series based on grouping specification\nfunc (a *Aggregator) createAggregationGroups(series []SeriesData, groupBy, without []string) map[string][]SeriesData {\n    groups := make(map[string][]SeriesData)\n    \n    // TODO 1: For each input series, compute group key based on labels\n    // TODO 2: Handle \"by\" grouping: include only specified labels in key\n    // TODO 3: Handle \"without\" grouping: exclude specified labels from key\n    // TODO 4: Add series to appropriate group bucket\n    // TODO 5: Return map from group key to series list\n    \n    return groups\n}\n\n// applyAggregationFunction computes result for values within a group\nfunc (a *Aggregator) applyAggregationFunction(function string, values []float64) (float64, error) {\n    if len(values) == 0 {\n        return 0, nil\n    }\n    \n    switch function {\n    case \"sum\":\n        // TODO 1: Sum all values in the group\n        return 0, nil\n    case \"avg\":\n        // TODO 2: Calculate arithmetic mean\n        return 0, nil\n    case \"max\":\n        // TODO 3: Find maximum value\n        return 0, nil\n    case \"min\":\n        // TODO 4: Find minimum value\n        return 0, nil\n    case \"count\":\n        // TODO 5: Return count of valid values\n        return float64(len(values)), nil\n    default:\n        return 0, fmt.Errorf(\"unknown aggregation function: %s\", function)\n    }\n}\n```\n\n**Range Query Execution (Core Logic Skeleton):**\n\n```go\npackage execution\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// RangeExecutor handles time-windowed query processing\ntype RangeExecutor struct {\n    storage          StorageEngine\n    stalenessThreshold time.Duration\n}\n\n// ExecuteRangeQuery processes queries over time windows\nfunc (r *RangeExecutor) ExecuteRangeQuery(ctx context.Context, expr ASTNode, start, end time.Time, step time.Duration) ([]RangeQueryResult, error) {\n    // TODO 1: Generate array of query timestamps from start to end at step intervals\n    // TODO 2: For each timestamp, execute instant query evaluation\n    // TODO 3: Collect results into time series matrix format\n    // TODO 4: Handle series that appear/disappear during time range\n    // TODO 5: Apply staleness detection for missing data points\n    // TODO 6: Format results with consistent timestamp alignment\n    \n    return nil, fmt.Errorf(\"not implemented\")\n}\n\n// generateTimeline creates timestamp sequence for range query\nfunc (r *RangeExecutor) generateTimeline(start, end time.Time, step time.Duration) []time.Time {\n    var timestamps []time.Time\n    \n    // TODO 1: Start at aligned timestamp (round start time to step boundary)\n    // TODO 2: Generate timestamps at step intervals until end time\n    // TODO 3: Ensure final timestamp doesn't exceed end time\n    // TODO 4: Return array of query evaluation timestamps\n    \n    return timestamps\n}\n\n// interpolateValue finds appropriate value for query timestamp\nfunc (r *RangeExecutor) interpolateValue(samples []Sample, queryTime time.Time) (float64, bool) {\n    // TODO 1: Find sample closest to query timestamp within staleness threshold\n    // TODO 2: Return sample value if within threshold\n    // TODO 3: Return invalid if no recent sample available\n    // TODO 4: Handle edge cases (no samples, samples after query time)\n    \n    return 0, false\n}\n```\n\n**Milestone Checkpoints:**\n\nAfter implementing the core query engine components, verify functionality with these checkpoints:\n\n1. **Parser Testing**: Run `go test ./internal/query/parser/...` - should parse basic metric selectors, label matchers, and mathematical expressions without errors.\n\n2. **Label Selection**: Test with `curl \"http://localhost:8080/api/v1/query?query=up{job=\\\"prometheus\\\"}\"` - should return series matching the label selector.\n\n3. **Aggregation**: Test with `curl \"http://localhost:8080/api/v1/query?query=sum(up)\"` - should return single aggregated value across all series.\n\n4. **Range Queries**: Test with `curl \"http://localhost:8080/api/v1/query_range?query=up&start=1609459200&end=1609462800&step=60\"` - should return time series matrix with consistent timestamp alignment.\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Parse errors on valid PromQL | Incorrect operator precedence or tokenization | Add debug logging to lexer token output | Fix precedence in recursive descent methods |\n| Empty query results | Label matcher not finding series | Check inverted index contents and matcher logic | Verify label matcher evaluation against actual stored labels |\n| Memory exhaustion on aggregation | High cardinality grouping labels | Monitor group count and series per group | Add cardinality limits and validate grouping labels |\n| Slow range queries | Inefficient timestamp generation or storage access | Profile CPU and measure storage query time | Optimize timeline generation and batch storage requests |\n| Inconsistent results | Race conditions in concurrent processing | Test with single-threaded execution | Add proper synchronization around shared data structures |\n\n\n## Component Interactions and Data Flow\n\n> **Milestone(s):** This section integrates all four milestones by showing how the Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4) components communicate and coordinate to form a complete metrics collection system.\n\nThe metrics collection system operates as a **synchronized orchestra** where multiple components must coordinate their activities without blocking each other. Think of it like a busy restaurant kitchen: the servers (scrape engine) continuously bring in orders from customers (targets), the prep cooks (storage engine) process and organize ingredients (metrics) into proper containers (compressed chunks), while the head chef (query engine) fulfills requests by combining ingredients from storage. All three activities happen simultaneously, requiring careful coordination to avoid collisions and ensure freshness.\n\nUnderstanding these interactions is critical because the performance characteristics of your entire monitoring system depend on how efficiently these components communicate. Poor coordination leads to backpressure where slow queries block metric ingestion, or where high scraping volume overwhelms storage and degrades query performance. The key insight is that **each pipeline must be designed with independent flow control** while sharing the underlying data structures safely.\n\n![System Architecture Overview](./diagrams/system-architecture.svg)\n\n### Ingestion Pipeline: Flow from Scraped Metrics to Indexed Storage\n\nThe ingestion pipeline represents the **forward flow** of data through the system, from external targets to persistent storage. This pipeline must handle continuous high-volume writes while maintaining durability guarantees and enabling efficient querying. Think of it as a **mail sorting facility** that receives letters (metrics) from many post offices (targets), validates addressing (labels), sorts them into appropriate bins (time series), and files them in the archive (compressed storage) with proper indexing for later retrieval.\n\n![Scrape Operation Sequence](./diagrams/scrape-sequence.svg)\n\n#### Scrape Collection Phase\n\nThe ingestion pipeline begins when the `ScrapeEngine` identifies targets for metric collection. The scrape scheduler maintains an internal priority queue of upcoming scrape operations, ordered by target URL and next scrape time. This scheduler must balance scraping load across time to avoid thundering herd effects where all targets are scraped simultaneously.\n\n| Scrape Phase Step | Component | Input | Output | Duration Limit |\n|------------------|-----------|--------|--------|----------------|\n| Target Selection | `ScrapeEngine` | Current time, target configurations | Targets ready for scraping | < 1ms |\n| HTTP Request | `HTTPClient` | Target URL, headers, timeout | Raw metrics text or error | `ScrapeTimeout` (default 10s) |\n| Format Parsing | Metrics Parser | Prometheus exposition format text | `Sample` structs with labels | < 100ms per MB |\n| Label Validation | `LabelValidator` | Parsed samples with labels | Valid samples or rejection errors | < 10ms per sample |\n| Batch Preparation | `ScrapeEngine` | Validated samples | Batched samples for storage | < 50ms |\n\nThe scrape collection process follows these steps:\n\n1. **Target prioritization**: The scheduler examines all configured targets and identifies which ones are due for scraping based on their individual `scrape_interval` settings and last successful scrape time.\n\n2. **Concurrent scrape initiation**: The engine creates goroutines for each ready target, up to a maximum concurrent scrape limit to prevent resource exhaustion. Each goroutine gets its own `HTTPClient` context with the target's specific `scrape_timeout`.\n\n3. **HTTP metrics retrieval**: The `HTTPClient.ScrapeTarget()` method performs an HTTP GET request to the target's `/metrics` endpoint, following redirects and handling authentication if configured. The response body is limited to prevent memory exhaustion attacks.\n\n4. **Exposition format parsing**: The raw HTTP response is parsed using the Prometheus exposition format parser, which converts text lines like `http_requests_total{method=\"GET\",status=\"200\"} 1234 1609459200000` into structured `Sample` objects.\n\n5. **Label cardinality validation**: Each parsed sample's labels are validated against cardinality limits using the `CardinalityTracker` to prevent label explosion attacks that could exhaust system memory.\n\n6. **Batch accumulation**: Valid samples are accumulated into batches of 1000-10000 samples to amortize the cost of storage operations while keeping memory usage bounded.\n\n> **Decision: Batch Size for Storage Operations**\n> - **Context**: Individual sample writes are inefficient due to lock contention and WAL sync overhead\n> - **Options Considered**: Per-sample writes, fixed 1000-sample batches, adaptive batching based on memory pressure\n> - **Decision**: Fixed 10000-sample batches with memory-based early flushing\n> - **Rationale**: Fixed batching provides predictable memory usage and good write throughput, while early flushing prevents OOM during high-cardinality scrapes\n> - **Consequences**: Storage write operations are more efficient, but samples experience up to 10000-sample buffering delay before persistence\n\n#### Storage Ingestion Phase\n\nOnce the scrape engine has collected and validated metric samples, they must be ingested into the time series storage engine. This phase converts the batch of samples into compressed, indexed time series data while maintaining ACID durability guarantees through write-ahead logging.\n\nThe `StorageEngine.Append()` method coordinates this complex process:\n\n| Storage Ingestion Step | Component | Responsibility | Failure Handling |\n|------------------------|-----------|----------------|------------------|\n| WAL Write | `WriteAheadLog` | Durably record intended writes | Retry with backoff, alert on disk full |\n| Series Resolution | `InvertedIndexes` | Map metric+labels to series ID | Create new series if not found |\n| Chunk Allocation | `CompressedChunk` | Find or create chunk for timestamp | Create new chunk if current full |\n| Gorilla Compression | `GorillaCompressor` | Compress sample using deltas/XOR | Store uncompressed on compression failure |\n| Index Updates | `InvertedIndexes` | Update metric and label indexes | Rebuild indexes on corruption |\n| Memory Management | `StorageEngine` | Enforce memory limits, trigger compaction | Reject writes, compact old chunks |\n\nThe storage ingestion process operates as follows:\n\n1. **Write-ahead logging**: Before any in-memory state changes, the entire sample batch is serialized and written to the `WriteAheadLog` with an `fsync()` to ensure durability. This guarantees that even if the process crashes during ingestion, the samples can be replayed from the WAL on restart.\n\n2. **Series identification**: For each sample, the storage engine computes a hash of the metric name plus sorted labels to generate a unique series ID. The `InvertedIndexes` structure maintains mappings from this hash to the actual `SeriesMetadata` containing chunk references.\n\n3. **Chunk location**: Each time series is stored as a sequence of chunks covering different time ranges. The storage engine finds the appropriate chunk for each sample's timestamp, creating new chunks when the current chunk is full (typically 120 samples or 2 hours of data).\n\n4. **Compression application**: Samples are compressed using the Gorilla compression algorithm implemented in `GorillaCompressor`. This applies delta-of-delta encoding to timestamps and XOR encoding to float values, typically achieving 1.5 bytes per sample.\n\n5. **Index maintenance**: The inverted indexes are updated to reflect the new samples. This includes updating the metric name index, each label name/value index, and the series metadata with new chunk references.\n\n6. **Memory pressure monitoring**: The storage engine tracks total memory usage across all chunks and indexes. When memory usage exceeds configured limits, background compaction is triggered to compress older chunks and potentially evict cold data.\n\n> The critical insight is that **durability and consistency are maintained through WAL-first writes**, while performance is optimized through batched operations and compression. The WAL acts as the single source of truth during crash recovery.\n\n#### Error Handling and Backpressure\n\nThe ingestion pipeline must gracefully handle various failure scenarios without losing data or blocking the entire system. The key principle is **graceful degradation** where individual target failures don't impact the broader system's health.\n\n| Failure Mode | Detection Method | Recovery Action | User Impact |\n|--------------|------------------|-----------------|-------------|\n| Target HTTP timeout | `context.Context` deadline exceeded | Mark target as `HealthDown`, continue other targets | Missing metrics for one target |\n| Storage disk full | `WriteAheadLog.AppendSamples()` returns `ENOSPC` | Reject new samples, alert operators | Metrics ingestion stops |\n| High cardinality attack | `CardinalityTracker.RecordSeries()` exceeds limit | Reject samples with new label combinations | New series creation blocked |\n| Parser format errors | Malformed Prometheus exposition format | Skip invalid lines, log warnings | Partial metrics loss for target |\n| Memory exhaustion | `StorageEngine` memory usage exceeds limit | Trigger emergency compaction, reject writes | Temporary ingestion backpressure |\n\nThe backpressure mechanism works through a **token bucket** system where the storage engine issues tokens at a rate proportional to its available capacity. The scrape engine consumes tokens before attempting to append samples, naturally slowing down ingestion when storage is overwhelmed.\n\n⚠️ **Pitfall: Blocking Scrapes on Storage Pressure**\nA common mistake is making scrape operations synchronously wait for storage availability, which can cause all scraping to stop during storage issues. Instead, scrapes should buffer samples in memory with bounded queues and drop oldest samples when buffers fill. This preserves recent data while preventing memory exhaustion.\n\n### Query Processing Pipeline: Steps from PromQL Input to Aggregated Results\n\nThe query processing pipeline represents the **reverse flow** of data through the system, from high-level PromQL expressions to specific time series values retrieved from storage. Unlike the ingestion pipeline which handles continuous writes, the query pipeline processes discrete read requests that may touch large portions of stored data. Think of it as a **research librarian** who receives complex questions, breaks them down into specific book and page lookups, retrieves the relevant information, and synthesizes it into a coherent answer.\n\n![Query Execution Flow](./diagrams/query-execution.svg)\n\n#### PromQL Parsing and Planning Phase\n\nQuery processing begins when the HTTP API receives a PromQL expression and converts it into an executable query plan. This phase must validate syntax, optimize execution order, and estimate resource requirements to prevent resource exhaustion attacks.\n\n| Parsing Phase Step | Component | Input | Output | Complexity |\n|-------------------|-----------|--------|--------|------------|\n| Lexical Analysis | `ExpressionParser` | Raw PromQL string | Token stream | O(n) where n = query length |\n| Syntax Parsing | `ExpressionParser` | Token stream | Abstract Syntax Tree | O(n log n) for expression depth |\n| Semantic Analysis | `QueryEngine` | AST nodes | Validated query plan | O(m) where m = number of selectors |\n| Optimization | `QueryEngine` | Query plan | Optimized execution order | O(m²) for selector optimization |\n| Resource Estimation | `QueryEngine` | Optimized plan | Memory and time estimates | O(m) for cardinality estimation |\n\nThe parsing process transforms PromQL text through these stages:\n\n1. **Tokenization**: The raw PromQL string is broken into tokens representing metric names, operators, functions, and literals. For example, `rate(http_requests_total[5m])` becomes tokens `FUNCTION(rate)`, `IDENTIFIER(http_requests_total)`, `RANGE[5m]`.\n\n2. **AST construction**: Tokens are parsed into a tree structure representing the expression's hierarchical structure. Function calls become `FunctionCallNode` instances with child nodes for their arguments, while metric selectors become `MetricSelectorNode` instances containing label matchers.\n\n3. **Type checking**: Each AST node is validated to ensure type compatibility. For example, the `rate()` function requires a range vector argument, so applying it to an instant vector would generate a type error.\n\n4. **Query optimization**: The query planner analyzes the AST to optimize execution. This includes pushing label filters down to storage selectors, reordering operations to minimize intermediate result sizes, and identifying opportunities for parallel execution.\n\n5. **Resource estimation**: Based on the optimized plan, the query engine estimates memory requirements and execution time by consulting cardinality statistics from the storage engine. Queries exceeding configured limits are rejected.\n\n> **Decision: AST-Based Query Execution**\n> - **Context**: Need to support complex PromQL expressions with nested functions and operations\n> - **Options Considered**: Direct interpretation, bytecode compilation, AST walking with visitors\n> - **Decision**: AST walking with visitor pattern for execution\n> - **Rationale**: AST provides flexibility for optimization passes while visitor pattern enables clean separation of parsing and execution logic\n> - **Consequences**: Enables sophisticated query optimization but requires more complex parsing infrastructure than simple interpretation\n\n#### Series Selection and Filtering Phase\n\nOnce the query is parsed and planned, the execution engine must identify which time series match the query's label selectors and retrieve their data from storage. This phase often dominates query performance since it involves index lookups and potentially large data retrievals.\n\nThe `LabelSelector.SelectSeries()` method coordinates series selection:\n\n| Selection Step | Component | Operation | Data Structure | Time Complexity |\n|----------------|-----------|-----------|----------------|-----------------|\n| Metric Name Lookup | `InvertedIndexes` | Find all series for metric | Hash table | O(1) |\n| Label Filter Application | `LabelSelector` | Apply each label matcher | Sorted arrays with binary search | O(log n) per matcher |\n| Series Intersection | `LabelSelector` | Combine multiple label results | Sorted array merge | O(n + m) |\n| Cardinality Validation | `QueryEngine` | Check result set size | Count operation | O(1) |\n| Series Metadata Retrieval | `InvertedIndexes` | Get chunk refs for matching series | Hash table lookups | O(k) where k = matching series |\n\nThe series selection algorithm works as follows:\n\n1. **Metric name filtering**: If the query specifies a metric name (e.g., `http_requests_total`), the inverted index is consulted to get the set of all series IDs that have this metric name. This typically eliminates 99%+ of series immediately.\n\n2. **Label matcher evaluation**: Each label matcher in the query (e.g., `{status=\"200\"}`, `{method=~\"GET|POST\"}`) is evaluated against the remaining series. Exact matchers use hash lookups while regex matchers require iteration with pattern matching.\n\n3. **Result set intersection**: Multiple label matchers are combined using set intersection operations. Since series IDs are stored in sorted arrays, this uses efficient merge algorithms to find the intersection of all matching conditions.\n\n4. **Cardinality enforcement**: The final result set is checked against the query engine's `max_series` limit. Queries matching too many series are rejected to prevent memory exhaustion and ensure bounded query execution time.\n\n5. **Chunk reference collection**: For each matching series, the storage engine retrieves the `SeriesMetadata` containing references to all chunks that store data for this series, along with time range information for efficient chunk selection.\n\n#### Data Retrieval and Aggregation Phase\n\nWith the matching series identified, the query engine retrieves the actual time series data and applies any aggregation functions specified in the PromQL expression. This phase must handle large data volumes efficiently while supporting various aggregation semantics.\n\n| Aggregation Phase Step | Component | Input | Processing | Output |\n|------------------------|-----------|--------|------------|---------|\n| Chunk Loading | `StorageEngine` | Chunk references + time range | Decompress matching chunks | Raw sample arrays |\n| Sample Extraction | `CompressedChunk` | Compressed chunk data | Gorilla decompression | `Sample` structs |\n| Time Range Filtering | `RangeExecutor` | Samples + query time bounds | Filter by timestamp | Relevant samples only |\n| Interpolation | `RangeExecutor` | Sparse samples + query timestamps | Fill gaps for step alignment | Dense sample matrix |\n| Grouping | `Aggregator` | Samples + group-by labels | Group by label combinations | Sample groups |\n| Function Application | `Aggregator` | Sample groups + aggregation function | Apply sum/avg/max/etc | Aggregated results |\n\nThe aggregation process handles different query types:\n\n**For instant queries** (`http_requests_total` at a specific timestamp):\n\n1. **Point-in-time lookup**: Find the sample closest to the query timestamp for each matching series, using staleness rules to determine if samples are too old to be valid.\n\n2. **Label grouping**: If the query includes aggregation (e.g., `sum by (status)`), group series by the specified label dimensions, creating separate result groups for each unique label combination.\n\n3. **Aggregation function**: Apply the requested function (`sum`, `avg`, `max`, `min`, `count`, `quantile`) to each group, producing a single value per group.\n\n**For range queries** (e.g., `rate(http_requests_total[5m])` over a time window):\n\n1. **Range vector construction**: For each series and each query step timestamp, collect all samples within the specified range window (e.g., 5 minutes).\n\n2. **Function application**: Apply range functions like `rate()`, `increase()`, or `avg_over_time()` to each range vector, producing one value per series per step.\n\n3. **Step alignment**: Interpolate or aggregate results to align with the query's step interval, ensuring consistent output timestamps across all series.\n\n4. **Multi-step aggregation**: If instant aggregation is also requested (e.g., `sum(rate(...))`), group and aggregate the results from each step timestamp.\n\n> **The key insight is that range queries are essentially many instant queries executed in parallel**, with additional complexity for handling time-based functions and step alignment. Efficient execution requires careful memory management to avoid loading unnecessary data.\n\n⚠️ **Pitfall: Loading Entire Series for Range Queries**\nA common mistake is loading all samples for a time series when only a small time range is needed. Always filter chunks by time range before decompression, and use the chunk time bounds (`mint`/`maxt`) to skip chunks that don't overlap with the query range.\n\n### Concurrency Control: Managing Concurrent Reads, Writes, and Background Operations\n\nThe metrics collection system must handle simultaneous ingestion, querying, and maintenance operations without data corruption or performance degradation. This requires careful **concurrency control** that allows maximum parallelism while maintaining data consistency. Think of it as **air traffic control** for a busy airport: multiple planes (operations) must use shared runways (data structures) simultaneously, requiring precise coordination to prevent collisions while maximizing throughput.\n\nThe fundamental challenge is that writes (ingestion) and reads (queries) access the same underlying data structures (`InvertedIndexes`, `CompressedChunk`, series metadata) with very different access patterns and performance requirements. Writes are typically small and frequent, while reads may access large portions of data. Background operations like compaction need exclusive access to reorganize data efficiently.\n\n#### Read-Write Concurrency Model\n\nThe storage engine uses a **multi-granularity locking scheme** that provides fine-grained concurrency control without sacrificing correctness. This approach recognizes that different data structures have different concurrency requirements and optimizes each accordingly.\n\n| Data Structure | Concurrency Model | Lock Type | Typical Hold Time | Contention Level |\n|----------------|-------------------|-----------|-------------------|------------------|\n| `InvertedIndexes` | Readers-writer lock | `sync.RWMutex` | < 1ms for reads, < 10ms for writes | Low (mostly reads) |\n| `CompressedChunk` | Per-chunk mutexes | `sync.Mutex` | < 100μs | Very low (chunks rarely shared) |\n| `SeriesMetadata` | Copy-on-write | Atomic pointer swap | 0 (lockless reads) | None |\n| `WriteAheadLog` | Sequential writes only | `sync.Mutex` | < 1ms | Medium (write bottleneck) |\n| Target health state | Atomic operations | `sync/atomic` | 0 (lockless) | None |\n\n**Index Concurrency**: The `InvertedIndexes` structure uses a readers-writer lock (`sync.RWMutex`) that allows multiple concurrent readers but exclusive writer access. This works well because queries (readers) vastly outnumber ingestion operations (writers), and index lookups are typically very fast.\n\n**Chunk-Level Isolation**: Each `CompressedChunk` has its own mutex, allowing concurrent access to different chunks while serializing access to individual chunks. Since chunks represent distinct time ranges, most operations naturally access different chunks, minimizing contention.\n\n**Series Metadata Atomicity**: Series metadata uses a **copy-on-write** pattern where updates create a new metadata structure and atomically swap the pointer. Readers get a consistent snapshot without locking, while writers coordinate through a single writer lock.\n\n> **Decision: Fine-Grained Locking Strategy**\n> - **Context**: Need to support concurrent reads and writes without blocking each other\n> - **Options Considered**: Global read-write lock, lock-free data structures, fine-grained per-component locking\n> - **Decision**: Fine-grained locking with readers-writer locks for indexes and per-chunk mutexes\n> - **Rationale**: Balances implementation complexity with performance - simpler than lock-free but much better concurrency than global locking\n> - **Consequences**: Good read concurrency with acceptable write performance, but requires careful lock ordering to prevent deadlocks\n\n#### Write Coordination and Batching\n\nMultiple scrape targets generate samples simultaneously, requiring coordination to prevent write conflicts and optimize storage throughput. The ingestion system uses a **channel-based coordination** model where scrapers send samples through buffered channels to a smaller number of storage writers.\n\nThe write coordination architecture:\n\n| Component | Goroutines | Responsibility | Buffer Size | Backpressure Handling |\n|-----------|------------|----------------|-------------|----------------------|\n| Scrape Workers | 50-100 | HTTP scraping and parsing | N/A | Block on channel send |\n| Sample Channels | 10 | Buffer samples between scraping and storage | 10,000 samples | Drop oldest samples |\n| Storage Writers | 2-5 | Batch samples and write to storage | N/A | Apply write rate limiting |\n| Background Compactor | 1 | Compress old chunks, update indexes | N/A | Sleep when no work |\n\nThis coordination model provides several benefits:\n\n1. **Write batching**: Storage writers collect samples from multiple scrapers into large batches, amortizing the cost of WAL sync operations and lock acquisition.\n\n2. **Load smoothing**: Buffered channels absorb bursts of scraping activity and smooth them into steady storage write rates, preventing storage overwhelm during synchronized scrapes.\n\n3. **Backpressure propagation**: When storage becomes overwhelmed, channel buffers fill up, causing scrapers to block on channel sends, naturally throttling ingestion rate.\n\n4. **Failure isolation**: If one scraper encounters errors, other scrapers continue operating normally since each uses independent goroutines and error handling.\n\nThe write batching algorithm works as follows:\n\n1. **Sample accumulation**: Storage writers continuously read from sample channels, accumulating samples into batches of 1000-10000 samples or until a time threshold (100ms) is reached.\n\n2. **WAL writing**: Complete batches are serialized and written to the WAL with `fsync()` to ensure durability before any in-memory state changes.\n\n3. **Concurrent storage**: Multiple storage writers can process different batches in parallel since the WAL provides ordering guarantees and each batch targets different series/chunks.\n\n4. **Error handling**: If WAL writing fails (e.g., disk full), the batch is either retried with exponential backoff or dropped with alerting, depending on the error type.\n\n#### Background Operations Coordination\n\nThe storage engine runs several background operations that must coordinate with foreground ingestion and querying to maintain system health without impacting user-facing performance. These operations include chunk compaction, index optimization, and old data deletion.\n\n| Background Operation | Frequency | Duration | Resources Used | Coordination Method |\n|---------------------|-----------|----------|----------------|---------------------|\n| Chunk Compaction | Every 2 hours | 10-60 seconds | CPU, temporary memory | Readers-writer lock during index updates |\n| Index Defragmentation | Daily | 1-10 minutes | CPU, disk I/O | Copy-on-write index replacement |\n| WAL Truncation | Every 15 minutes | < 1 second | Disk I/O | WAL mutex during file operations |\n| Old Data Deletion | Hourly | 5-30 seconds | Disk I/O | Series-level locking during deletion |\n| Memory Pressure Compaction | When memory > 80% | 1-5 seconds | CPU, memory | Emergency write throttling |\n\n**Chunk Compaction Coordination**: The background compactor identifies chunks that can be merged (adjacent time ranges, same series) or compressed further (old chunks with space to reclaim). During compaction, it:\n\n1. **Identifies compaction candidates** by scanning series metadata for adjacent chunks or chunks with low compression ratios.\n\n2. **Acquires read locks** on source chunks to prevent concurrent modifications during compaction.\n\n3. **Creates new compacted chunks** in temporary storage, using improved compression parameters optimized for the specific data patterns observed.\n\n4. **Atomically updates indexes** to point to new chunks, acquiring brief write locks only during the pointer swap operations.\n\n5. **Releases old chunks** for garbage collection after a grace period to ensure no in-flight queries are using them.\n\n**Memory Pressure Handling**: When memory usage exceeds configured thresholds, the system triggers emergency compaction and write throttling:\n\n```\nMemory Usage Thresholds:\n- 70%: Begin background compaction of oldest chunks\n- 80%: Throttle ingestion rate to 50% of normal\n- 90%: Reject new writes, compact aggressively\n- 95%: Emergency mode - drop incoming samples, compact everything possible\n```\n\nThis multi-tier approach provides **graceful degradation** under memory pressure while maintaining system availability.\n\n⚠️ **Pitfall: Deadlock in Lock Acquisition**\nBackground operations must acquire locks in a consistent order to prevent deadlocks. Always acquire series-level locks before chunk-level locks, and index locks before series locks. Use timeout-based lock acquisition (`TryLock` with timeouts) for background operations to prevent indefinite blocking.\n\n> The critical insight is that **coordination overhead must be minimized in the common case** (concurrent reads, occasional writes) while providing strong guarantees in edge cases (memory pressure, failures). This is achieved through careful lock granularity choices and optimistic concurrency control where possible.\n\n### Implementation Guidance\n\nThis section provides practical implementation guidance for building the component interaction and data flow coordination systems described above.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | Recommended for Beginners |\n|-----------|---------------|-----------------|---------------------------|\n| Inter-component Communication | Buffered channels (`make(chan Sample, 10000)`) | Lock-free ring buffers | Buffered channels |\n| Concurrency Control | `sync.RWMutex` and `sync.Mutex` | Lock-free data structures with `sync/atomic` | Standard library mutexes |\n| Background Task Scheduling | `time.Ticker` with goroutines | Custom scheduler with priority queues | `time.Ticker` |\n| Error Propagation | Error channels and context cancellation | Structured error types with retry policies | Error channels |\n| Resource Monitoring | Simple counters with `sync/atomic` | Prometheus metrics with histograms | Atomic counters |\n\n#### Recommended File Structure\n\n```\ninternal/\n  coordinator/\n    coordinator.go          ← main coordination logic\n    pipeline.go            ← ingestion pipeline implementation\n    query_coordinator.go   ← query processing coordination\n    background.go          ← background operation management\n    coordinator_test.go    ← integration tests\n  \n  storage/\n    engine.go              ← storage engine with concurrency control\n    chunk.go               ← compressed chunk with per-chunk locking\n    index.go               ← inverted indexes with RWMutex\n    wal.go                 ← write-ahead log with sequential writes\n  \n  scrape/\n    engine.go              ← scrape engine with worker pools\n    target.go              ← individual target management\n    client.go              ← HTTP client with timeout handling\n  \n  query/\n    engine.go              ← query engine with resource limits\n    executor.go            ← query execution with series selection\n    aggregator.go          ← aggregation with grouping logic\n```\n\n#### Infrastructure Starter Code\n\n**Pipeline Coordination Infrastructure** (Complete, ready to use):\n\n```go\npackage coordinator\n\nimport (\n    \"context\"\n    \"sync\"\n    \"sync/atomic\"\n    \"time\"\n)\n\n// PipelineCoordinator manages the flow of samples from scraping to storage\ntype PipelineCoordinator struct {\n    sampleChannels    []chan []Sample\n    storageWriters    int\n    channelBufferSize int\n    batchSize         int\n    batchTimeout      time.Duration\n    \n    // Metrics for monitoring pipeline health\n    samplesReceived   int64\n    samplesStored     int64\n    samplesBatched    int64\n    backpressureEvents int64\n    \n    // Coordination\n    ctx    context.Context\n    cancel context.CancelFunc\n    wg     sync.WaitGroup\n}\n\n// NewPipelineCoordinator creates a coordinator with the specified configuration\nfunc NewPipelineCoordinator(writers, channelBufferSize, batchSize int, batchTimeout time.Duration) *PipelineCoordinator {\n    ctx, cancel := context.WithCancel(context.Background())\n    \n    channels := make([]chan []Sample, writers)\n    for i := range channels {\n        channels[i] = make(chan []Sample, channelBufferSize)\n    }\n    \n    return &PipelineCoordinator{\n        sampleChannels:    channels,\n        storageWriters:    writers,\n        channelBufferSize: channelBufferSize,\n        batchSize:         batchSize,\n        batchTimeout:      batchTimeout,\n        ctx:               ctx,\n        cancel:            cancel,\n    }\n}\n\n// SendSamples distributes samples across writer channels with load balancing\nfunc (pc *PipelineCoordinator) SendSamples(samples []Sample) error {\n    if len(samples) == 0 {\n        return nil\n    }\n    \n    // Round-robin distribution across channels\n    channelIndex := int(atomic.AddInt64(&pc.samplesReceived, int64(len(samples)))) % len(pc.sampleChannels)\n    \n    select {\n    case pc.sampleChannels[channelIndex] <- samples:\n        return nil\n    case <-pc.ctx.Done():\n        return pc.ctx.Err()\n    default:\n        // Channel is full - apply backpressure\n        atomic.AddInt64(&pc.backpressureEvents, 1)\n        return ErrBackpressure\n    }\n}\n\n// Start begins the storage writer goroutines\nfunc (pc *PipelineCoordinator) Start(storage StorageEngine) {\n    for i := 0; i < pc.storageWriters; i++ {\n        pc.wg.Add(1)\n        go pc.runStorageWriter(i, storage)\n    }\n}\n\n// Shutdown gracefully stops all writers\nfunc (pc *PipelineCoordinator) Shutdown() {\n    pc.cancel()\n    pc.wg.Wait()\n}\n\n// GetMetrics returns pipeline health metrics\nfunc (pc *PipelineCoordinator) GetMetrics() PipelineMetrics {\n    return PipelineMetrics{\n        SamplesReceived:    atomic.LoadInt64(&pc.samplesReceived),\n        SamplesStored:      atomic.LoadInt64(&pc.samplesStored),\n        SamplesBatched:     atomic.LoadInt64(&pc.samplesBatched),\n        BackpressureEvents: atomic.LoadInt64(&pc.backpressureEvents),\n    }\n}\n\n// runStorageWriter processes samples from a channel and batches them for storage\nfunc (pc *PipelineCoordinator) runStorageWriter(writerID int, storage StorageEngine) {\n    defer pc.wg.Done()\n    \n    batch := make([]Sample, 0, pc.batchSize)\n    batchTimer := time.NewTimer(pc.batchTimeout)\n    defer batchTimer.Stop()\n    \n    for {\n        select {\n        case samples := <-pc.sampleChannels[writerID]:\n            batch = append(batch, samples...)\n            atomic.AddInt64(&pc.samplesBatched, int64(len(samples)))\n            \n            // Flush batch if it reaches target size\n            if len(batch) >= pc.batchSize {\n                pc.flushBatch(storage, batch)\n                batch = batch[:0] // Reset slice while keeping capacity\n                batchTimer.Reset(pc.batchTimeout)\n            }\n            \n        case <-batchTimer.C:\n            // Flush partial batch on timeout\n            if len(batch) > 0 {\n                pc.flushBatch(storage, batch)\n                batch = batch[:0]\n            }\n            batchTimer.Reset(pc.batchTimeout)\n            \n        case <-pc.ctx.Done():\n            // Flush remaining samples before shutdown\n            if len(batch) > 0 {\n                pc.flushBatch(storage, batch)\n            }\n            return\n        }\n    }\n}\n\n// flushBatch writes a batch of samples to storage with error handling\nfunc (pc *PipelineCoordinator) flushBatch(storage StorageEngine, batch []Sample) {\n    err := storage.Append(batch)\n    if err != nil {\n        // Log error but don't crash - implement retry logic here\n        log.Printf(\"Storage write failed for batch of %d samples: %v\", len(batch), err)\n        return\n    }\n    \n    atomic.AddInt64(&pc.samplesStored, int64(len(batch)))\n}\n\ntype PipelineMetrics struct {\n    SamplesReceived    int64\n    SamplesStored      int64\n    SamplesBatched     int64\n    BackpressureEvents int64\n}\n\nvar ErrBackpressure = errors.New(\"pipeline backpressure - channel full\")\n```\n\n**Query Coordination Infrastructure** (Complete, ready to use):\n\n```go\npackage coordinator\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\n// QueryCoordinator manages concurrent query execution with resource limits\ntype QueryCoordinator struct {\n    maxConcurrentQueries int\n    queryTimeout        time.Duration\n    maxSeriesPerQuery   int\n    \n    // Semaphore for limiting concurrent queries\n    querySemaphore chan struct{}\n    \n    // Metrics\n    activeQueries   int32\n    completedQueries int64\n    timeoutQueries  int64\n    errorQueries    int64\n    \n    mu sync.RWMutex\n}\n\n// NewQueryCoordinator creates a coordinator with resource limits\nfunc NewQueryCoordinator(maxConcurrent int, timeout time.Duration, maxSeries int) *QueryCoordinator {\n    return &QueryCoordinator{\n        maxConcurrentQueries: maxConcurrent,\n        queryTimeout:        timeout,\n        maxSeriesPerQuery:   maxSeries,\n        querySemaphore:      make(chan struct{}, maxConcurrent),\n    }\n}\n\n// ExecuteQuery coordinates the execution of a single query with resource management\nfunc (qc *QueryCoordinator) ExecuteQuery(ctx context.Context, query string, evalTime time.Time, engine QueryEngine) (*QueryResult, error) {\n    // Acquire semaphore slot for concurrency control\n    select {\n    case qc.querySemaphore <- struct{}{}:\n        defer func() { <-qc.querySemaphore }()\n    case <-ctx.Done():\n        return nil, ctx.Err()\n    }\n    \n    atomic.AddInt32(&qc.activeQueries, 1)\n    defer atomic.AddInt32(&qc.activeQueries, -1)\n    \n    // Create query-specific context with timeout\n    queryCtx, cancel := context.WithTimeout(ctx, qc.queryTimeout)\n    defer cancel()\n    \n    // Execute query with resource monitoring\n    result, err := qc.executeWithMonitoring(queryCtx, query, evalTime, engine)\n    \n    // Update metrics based on result\n    if err != nil {\n        if err == context.DeadlineExceeded {\n            atomic.AddInt64(&qc.timeoutQueries, 1)\n        } else {\n            atomic.AddInt64(&qc.errorQueries, 1)\n        }\n    } else {\n        atomic.AddInt64(&qc.completedQueries, 1)\n    }\n    \n    return result, err\n}\n\n// ExecuteRangeQuery coordinates range query execution with memory management\nfunc (qc *QueryCoordinator) ExecuteRangeQuery(ctx context.Context, query string, start, end time.Time, step time.Duration, engine QueryEngine) (*QueryResult, error) {\n    // Calculate expected result size for memory estimation\n    stepCount := int(end.Sub(start) / step)\n    if stepCount > 10000 { // Arbitrary limit to prevent massive queries\n        return nil, fmt.Errorf(\"range query too large: %d steps exceeds limit of 10000\", stepCount)\n    }\n    \n    // Use same concurrency control as instant queries\n    select {\n    case qc.querySemaphore <- struct{}{}:\n        defer func() { <-qc.querySemaphore }()\n    case <-ctx.Done():\n        return nil, ctx.Err()\n    }\n    \n    queryCtx, cancel := context.WithTimeout(ctx, qc.queryTimeout*time.Duration(stepCount/1000+1))\n    defer cancel()\n    \n    return engine.ExecuteRangeQuery(queryCtx, query, start, end, step)\n}\n\n// executeWithMonitoring wraps query execution with resource monitoring\nfunc (qc *QueryCoordinator) executeWithMonitoring(ctx context.Context, query string, evalTime time.Time, engine QueryEngine) (*QueryResult, error) {\n    // Pre-execution validation would go here\n    // - Query complexity analysis\n    // - Cardinality estimation\n    // - Resource availability check\n    \n    startTime := time.Now()\n    result, err := engine.ExecuteInstantQuery(ctx, query, evalTime)\n    duration := time.Since(startTime)\n    \n    // Post-execution monitoring\n    if result != nil {\n        // Validate result size doesn't exceed limits\n        seriesCount := qc.countResultSeries(result)\n        if seriesCount > qc.maxSeriesPerQuery {\n            return nil, fmt.Errorf(\"query returned %d series, exceeds limit of %d\", seriesCount, qc.maxSeriesPerQuery)\n        }\n    }\n    \n    // Log slow queries for optimization\n    if duration > qc.queryTimeout/2 {\n        log.Printf(\"Slow query detected: %s took %v\", query, duration)\n    }\n    \n    return result, err\n}\n\n// countResultSeries counts the number of time series in a query result\nfunc (qc *QueryCoordinator) countResultSeries(result *QueryResult) int {\n    switch result.ResultType {\n    case \"vector\":\n        if vector, ok := result.Result.([]InstantQueryResult); ok {\n            return len(vector)\n        }\n    case \"matrix\":\n        if matrix, ok := result.Result.([]RangeQueryResult); ok {\n            return len(matrix)\n        }\n    }\n    return 0\n}\n\n// GetMetrics returns query coordination metrics\nfunc (qc *QueryCoordinator) GetMetrics() QueryCoordinatorMetrics {\n    return QueryCoordinatorMetrics{\n        ActiveQueries:    atomic.LoadInt32(&qc.activeQueries),\n        CompletedQueries: atomic.LoadInt64(&qc.completedQueries),\n        TimeoutQueries:   atomic.LoadInt64(&qc.timeoutQueries),\n        ErrorQueries:     atomic.LoadInt64(&qc.errorQueries),\n    }\n}\n\ntype QueryCoordinatorMetrics struct {\n    ActiveQueries    int32\n    CompletedQueries int64\n    TimeoutQueries   int64\n    ErrorQueries     int64\n}\n```\n\n#### Core Logic Skeleton Code\n\n**Main System Coordinator** (Skeleton for implementation):\n\n```go\npackage coordinator\n\n// SystemCoordinator manages the interaction between scraping, storage, and querying\ntype SystemCoordinator struct {\n    config          *Config\n    scrapeEngine    *ScrapeEngine\n    storageEngine   *StorageEngine\n    queryEngine     *QueryEngine\n    \n    pipelineCoord   *PipelineCoordinator\n    queryCoord      *QueryCoordinator\n    backgroundMgr   *BackgroundManager\n    \n    // Coordination channels\n    shutdownCh      chan struct{}\n    healthCh        chan ComponentHealth\n    \n    logger          Logger\n}\n\n// NewSystemCoordinator creates a fully configured system coordinator\nfunc NewSystemCoordinator(config *Config, logger Logger) (*SystemCoordinator, error) {\n    // TODO 1: Create storage engine with concurrency-safe configuration\n    // TODO 2: Create scrape engine that will send samples to pipeline coordinator\n    // TODO 3: Create query engine that reads from storage with coordination\n    // TODO 4: Create pipeline coordinator with appropriate buffer sizes and batching\n    // TODO 5: Create query coordinator with resource limits from config\n    // TODO 6: Create background manager for maintenance operations\n    // TODO 7: Set up health monitoring channels and coordination\n    \n    return &SystemCoordinator{\n        config:        config,\n        // Initialize components here\n        shutdownCh:    make(chan struct{}),\n        healthCh:      make(chan ComponentHealth, 100),\n        logger:        logger,\n    }, nil\n}\n\n// Start begins coordinated operation of all system components\nfunc (sc *SystemCoordinator) Start(ctx context.Context) error {\n    // TODO 1: Start storage engine and wait for ready signal\n    // TODO 2: Start pipeline coordinator with storage engine reference\n    // TODO 3: Start scrape engine with pipeline coordinator for sample delivery\n    // TODO 4: Start background manager for maintenance operations\n    // TODO 5: Start health monitoring goroutine\n    // TODO 6: Register HTTP handlers for queries using query coordinator\n    // TODO 7: Signal that system is ready for traffic\n    \n    sc.logger.Info(\"System coordinator started successfully\")\n    return nil\n}\n\n// Shutdown gracefully stops all components in reverse dependency order\nfunc (sc *SystemCoordinator) Shutdown(ctx context.Context) error {\n    close(sc.shutdownCh)\n    \n    // TODO 1: Stop accepting new queries (HTTP handlers)\n    // TODO 2: Wait for active queries to complete with timeout\n    // TODO 3: Stop scrape engine (no new samples)\n    // TODO 4: Wait for pipeline to flush remaining samples\n    // TODO 5: Stop background operations\n    // TODO 6: Perform final storage sync and close\n    // TODO 7: Log shutdown completion with component status\n    \n    return nil\n}\n\n// HandleScrapeResults processes samples from the scrape engine\nfunc (sc *SystemCoordinator) HandleScrapeResults(samples []Sample) error {\n    // TODO 1: Validate samples are not nil/empty\n    // TODO 2: Check if system is shutting down (return error if so)\n    // TODO 3: Send samples to pipeline coordinator\n    // TODO 4: Handle backpressure errors by updating scrape engine metrics\n    // TODO 5: Log any coordination failures for debugging\n    \n    return nil\n}\n\n// ExecuteQuery processes queries through the coordinated query pipeline\nfunc (sc *SystemCoordinator) ExecuteQuery(ctx context.Context, queryReq QueryRequest) (*QueryResult, error) {\n    // TODO 1: Validate query request (syntax, resource limits)\n    // TODO 2: Check system health - reject queries if storage unhealthy\n    // TODO 3: Route to appropriate query coordinator method (instant vs range)\n    // TODO 4: Apply any system-level query transformations or optimizations\n    // TODO 5: Update system-level query metrics and logging\n    \n    return sc.queryCoord.ExecuteQuery(ctx, queryReq.Query, queryReq.EvalTime, sc.queryEngine)\n}\n\n// monitorComponentHealth runs background health monitoring\nfunc (sc *SystemCoordinator) monitorComponentHealth() {\n    ticker := time.NewTicker(30 * time.Second)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case <-ticker.C:\n            // TODO 1: Check scrape engine health (targets responding, sample rate)\n            // TODO 2: Check storage engine health (disk space, memory usage, WAL status)\n            // TODO 3: Check query engine health (response times, error rates)\n            // TODO 4: Check pipeline health (backpressure events, batch efficiency)\n            // TODO 5: Update overall system health status\n            // TODO 6: Trigger alerting if any component is unhealthy\n            \n        case health := <-sc.healthCh:\n            // TODO: Process component health updates from other goroutines\n            \n        case <-sc.shutdownCh:\n            return\n        }\n    }\n}\n\ntype ComponentHealth struct {\n    Component string\n    Status    HealthStatus\n    Message   string\n    Timestamp time.Time\n}\n\ntype QueryRequest struct {\n    Query    string\n    EvalTime time.Time\n    Start    time.Time  // For range queries\n    End      time.Time  // For range queries\n    Step     time.Duration // For range queries\n}\n```\n\n#### Language-Specific Hints\n\n**Go Concurrency Best Practices for Metrics Systems:**\n\n1. **Channel sizing**: Use buffered channels with sizes based on expected throughput. For sample channels, buffer size should handle 10-30 seconds of peak sample rate to absorb bursts.\n\n2. **Goroutine lifecycle**: Always use `sync.WaitGroup` to coordinate goroutine shutdown. Each component should have a `Start()` method that launches goroutines and a `Shutdown()` method that signals stop and waits for completion.\n\n3. **Context propagation**: Pass `context.Context` through all operations to support timeouts and cancellation. Query operations especially need this for resource control.\n\n4. **Atomic operations**: Use `sync/atomic` for metrics counters and simple state flags. This avoids mutex contention for frequently updated values.\n\n5. **Lock ordering**: When acquiring multiple locks, always use consistent ordering (e.g., series-level before chunk-level) to prevent deadlocks.\n\n#### Milestone Checkpoint\n\nAfter implementing the component interaction and data flow coordination:\n\n**Test Command**: `go test ./internal/coordinator/... -v`\n\n**Expected Behavior**:\n- Pipeline coordinator should handle 10,000 samples/second without backpressure\n- Query coordinator should support 50 concurrent queries with proper resource limits\n- System coordinator should gracefully start all components and coordinate shutdown\n- Health monitoring should detect and report component failures within 30 seconds\n\n**Manual Verification**:\n```bash\n# Start the system\ngo run cmd/server/main.go\n\n# Generate sample load\ncurl -X POST http://localhost:8080/api/v1/samples -d '[{\"metric\":\"test\",\"value\":1,\"timestamp\":\"2023-01-01T00:00:00Z\"}]'\n\n# Execute query while samples are being ingested\ncurl 'http://localhost:8080/api/v1/query?query=test&time=2023-01-01T00:00:00Z'\n\n# Verify coordination by checking metrics\ncurl http://localhost:8080/metrics | grep -E \"(samples_received|samples_stored|queries_executed)\"\n```\n\n**Signs of Problems**:\n- Pipeline backpressure events increase continuously → increase channel buffer sizes or add more storage writers\n- Query timeouts during ingestion → storage engine locks are held too long\n- Memory usage grows without bounds → background compaction not running or ineffective\n- Deadlocks during shutdown → lock ordering violations in component coordination\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Samples lost during ingestion | Channel buffer overflow | Check `BackpressureEvents` metric, monitor channel lengths | Increase channel buffer size or add more storage writers |\n| Queries fail with timeout | Lock contention between reads/writes | Add mutex profiling, check lock hold times | Implement read-mostly optimizations, reduce lock scope |\n| Memory usage grows continuously | Background compaction not running | Check compaction goroutine status, memory pressure triggers | Fix compaction scheduling, add emergency memory limits |\n| System hangs during shutdown | Goroutines not respecting context cancellation | Use `go tool trace` to find blocked goroutines | Add context checks in long-running operations |\n| Inconsistent query results | Race conditions in concurrent access | Run with `-race` flag, add data consistency checks | Add proper synchronization around shared data structures |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** This section spans all four milestones by providing comprehensive error handling strategies for the Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4). Robust error handling is essential for production deployment of each milestone.\n\nThink of error handling in a metrics collection system like designing a hospital's emergency response protocols. Just as a hospital must continue operating even when individual departments face equipment failures, network outages, or staff shortages, our metrics system must gracefully handle target unavailability, storage corruption, and resource exhaustion while maintaining service for healthy components. The key insight is that metrics collection is inherently about observability—if our observability system itself becomes unreliable, we lose visibility into the systems we're monitoring, creating a dangerous blind spot.\n\nEffective error handling in distributed systems requires a layered approach. Each component must handle its local failure modes while contributing to system-wide resilience patterns. This means implementing circuit breakers to isolate failing targets, graceful degradation to maintain partial functionality under stress, and comprehensive recovery mechanisms that can rebuild consistent state after crashes.\n\n### Target Unavailability\n\nThe **scrape engine** operates in an inherently unreliable environment where targets may be temporarily unreachable due to network partitions, service restarts, configuration changes, or resource exhaustion. Unlike traditional request-response systems where failures are immediately visible to users, metrics collection operates on a background schedule where failures accumulate silently until noticed through missing data or monitoring alerts.\n\nThink of target health management like a cardiac monitor in an intensive care unit. The monitor must distinguish between genuine cardiac events (target is actually down) and sensor failures (network timeout, probe malformation), while maintaining a clear history of both successful and failed readings. A single missed heartbeat doesn't indicate cardiac arrest, but a pattern of missed beats requires immediate attention.\n\nThe `TargetHealth` component tracks the availability and reliability status of each scrape endpoint through state transitions based on success and failure patterns. This health tracking serves multiple purposes: it prevents wasted resources on consistently failing targets, provides visibility into target reliability, and enables adaptive scraping strategies that reduce load on struggling services.\n\n| Health State | Entry Condition | Scraping Behavior | Transition Triggers |\n|--------------|----------------|-------------------|-------------------|\n| `HealthUp` | 3 consecutive successful scrapes | Normal interval scraping | 2 consecutive failures → HealthDegraded |\n| `HealthDegraded` | 2 consecutive failures from HealthUp | Reduced frequency (2x interval) | 3 consecutive successes → HealthUp, 3 more failures → HealthDown |\n| `HealthDown` | 3 consecutive failures from HealthDegraded | Exponential backoff (max 5min intervals) | 5 consecutive successes → HealthDegraded |\n\n#### Network Timeout Handling\n\nNetwork timeouts represent the most common failure mode in distributed scraping systems. The challenge lies in distinguishing between genuinely slow targets that need more time and unresponsive targets that should be abandoned quickly to prevent resource exhaustion.\n\n> **Decision: Multi-Layered Timeout Strategy**\n> - **Context**: Single timeout values either cause false failures for slow targets or waste resources on dead targets\n> - **Options Considered**: Fixed timeout, adaptive timeout based on target history, layered timeouts with different retry behavior\n> - **Decision**: Implement layered timeouts with connection timeout (5s), first-byte timeout (10s), and total request timeout (30s)\n> - **Rationale**: This provides fast failure detection for network issues while allowing reasonable time for slow but healthy targets\n> - **Consequences**: More complex timeout logic but better balance between reliability and resource usage\n\nThe `HTTPClient` implements three distinct timeout layers that work together to provide fast failure detection while accommodating legitimate performance variations:\n\n| Timeout Layer | Duration | Purpose | Failure Indication |\n|---------------|----------|---------|-------------------|\n| Connection Timeout | 5 seconds | Detect network connectivity issues | Target unreachable or network partition |\n| First Byte Timeout | 10 seconds | Detect application-level hangs | Target process hung or severely overloaded |\n| Total Request Timeout | 30 seconds | Prevent resource exhaustion | Large response or very slow processing |\n\nThe scrape engine handles timeout failures by updating target health state and implementing exponential backoff for consistently failing targets. This prevents the scraper from overwhelming targets that are experiencing temporary resource constraints while quickly returning to normal scraping frequency once targets recover.\n\n#### HTTP Error Response Handling\n\nHTTP error responses provide valuable diagnostic information about target state and should be handled differently based on their semantic meaning. Unlike timeouts, which may indicate transient network issues, HTTP errors often indicate configuration problems or application-level failures that require different recovery strategies.\n\nThe scrape engine categorizes HTTP errors into permanent failures (4xx client errors) and temporary failures (5xx server errors), applying different retry strategies and health state transitions for each category:\n\n| Error Category | Status Codes | Retry Strategy | Health Impact | Diagnostic Action |\n|----------------|-------------|----------------|---------------|-------------------|\n| Authentication/Authorization | 401, 403 | No retry - requires configuration fix | Immediate HealthDown | Log configuration error, alert operator |\n| Client Errors | 400, 404, 405 | No retry - indicates configuration problem | Immediate HealthDown | Validate target URL and endpoint configuration |\n| Server Overload | 429, 503 | Exponential backoff with jitter | Gradual degradation | Reduce scraping frequency, implement circuit breaker |\n| Server Errors | 500, 502, 504 | Standard retry with timeout | Follow normal health transitions | Monitor for pattern indicating systemic issues |\n\n#### Malformed Metrics Handling\n\nThe Prometheus exposition format parsing must handle malformed metrics data gracefully while preserving successfully parsed metrics from the same scrape operation. Think of this like a data quality inspector at a manufacturing plant—defective items should be rejected and logged, but the entire batch shouldn't be discarded if most items are acceptable.\n\nParse errors fall into several categories that require different handling strategies:\n\n**Metric Name Validation Errors**: Metric names that violate naming conventions (contain invalid characters, use reserved prefixes, or exceed length limits) are rejected at parse time. The parser logs the specific validation failure and continues processing remaining metrics from the same scrape.\n\n**Label Format Errors**: Labels with invalid names, missing values, or encoding issues are handled by either dropping the problematic label (if it's not critical) or dropping the entire metric (if the label is required for uniqueness). The decision depends on whether the remaining labels provide sufficient identity for the time series.\n\n**Value Format Errors**: Timestamps or values that cannot be parsed as valid numbers cause the specific sample to be dropped while preserving other samples from the same metric. This is particularly important for histogram metrics where individual bucket counts may be malformed while others are valid.\n\n**Timestamp Consistency Errors**: Samples with timestamps significantly in the past or future (outside a configurable staleness threshold) are dropped to prevent storage corruption and query inconsistencies. The parser maintains a window of acceptable timestamps based on scrape time.\n\n> **Decision: Partial Scrape Success Model**\n> - **Context**: Scrapes often contain mix of valid and invalid metrics, binary success/failure loses valuable data\n> - **Options Considered**: All-or-nothing parsing, best-effort parsing with warnings, configurable error tolerance thresholds\n> - **Decision**: Implement best-effort parsing that preserves valid metrics while logging specific parse failures\n> - **Rationale**: Metrics collection should be resilient to individual metric formatting issues without losing all observability\n> - **Consequences**: More complex error reporting but better data availability and easier debugging of format issues\n\n#### Circuit Breaker Implementation\n\nCircuit breakers prevent the scrape engine from wasting resources on consistently failing targets while enabling rapid recovery when targets become healthy again. The implementation tracks failure patterns and automatically transitions between closed (normal operation), open (failing fast), and half-open (testing recovery) states.\n\nThe circuit breaker maintains failure statistics over a sliding time window and makes state transition decisions based on failure rate thresholds rather than absolute failure counts. This provides more robust behavior under varying traffic patterns:\n\n| Circuit State | Scraping Behavior | Failure Threshold | Success Requirement | Transition Logic |\n|---------------|-------------------|-------------------|-------------------|------------------|\n| Closed | Normal scraping at configured interval | 50% failures over 5 minutes | N/A | Failure rate exceeds threshold → Open |\n| Open | No scraping, immediate failure response | N/A | N/A | After 1 minute timeout → Half-Open |\n| Half-Open | Single test scrape allowed | N/A | 1 successful scrape | Success → Closed, Failure → Open |\n\n### Storage Errors\n\nThe **time series storage engine** must handle various failure modes while maintaining data consistency and preventing corruption. Unlike stateless components that can simply restart after failures, the storage engine maintains persistent state that requires careful recovery procedures and consistency guarantees.\n\nThink of storage error handling like a bank's vault system. The vault must protect deposits even during power outages, hardware failures, or software crashes. Every transaction must be logged before execution (write-ahead logging), and after any disruption, the bank must verify the vault's contents match the transaction log exactly. Similarly, our storage engine uses write-ahead logging and consistency checks to ensure no data is lost or corrupted during failures.\n\n#### Disk Space Exhaustion\n\nDisk space exhaustion represents one of the most critical failure modes because it can cause data loss and prevent normal operation. The storage engine must detect approaching disk exhaustion, implement emergency data retention policies, and gracefully degrade functionality to preserve the most important data.\n\nThe storage engine implements a multi-level disk space monitoring system that triggers different responses based on available space thresholds:\n\n| Available Space | Action Taken | Data Retention Policy | Write Behavior |\n|----------------|--------------|----------------------|----------------|\n| > 20% | Normal operation | Standard retention period | All writes accepted |\n| 10-20% | Warning logs, accelerate cleanup | Reduce retention to 75% of configured | All writes accepted |\n| 5-10% | Emergency retention, pause compaction | Reduce retention to 50% of configured | High-priority metrics only |\n| < 5% | Read-only mode, aggressive cleanup | Reduce retention to 25% of configured | No new writes accepted |\n\nThe emergency retention system prioritizes metrics based on configured importance levels and access patterns. Critical infrastructure metrics (CPU, memory, disk usage) receive highest priority, while application-specific metrics may be discarded first during space constraints.\n\n> **Decision: Graceful Degradation Over Hard Failures**\n> - **Context**: Disk exhaustion traditionally causes complete storage system failure and data loss\n> - **Options Considered**: Fail fast when disk full, emergency retention policies, offload data to remote storage\n> - **Decision**: Implement layered retention policies that preserve most important data while maintaining read access\n> - **Rationale**: Partial observability is much more valuable than complete loss of metrics collection during emergencies\n> - **Consequences**: More complex retention logic but maintains system availability during resource constraints\n\n#### Write-Ahead Log Corruption Recovery\n\nThe `WriteAheadLog` provides durability guarantees by recording all intended writes before they're applied to the main storage indexes. WAL corruption can occur due to hardware failures, power outages during writes, or filesystem issues. Recovery procedures must detect corruption and rebuild consistent state without losing committed data.\n\nWAL recovery follows a structured approach that validates log integrity and replays valid entries while handling various corruption scenarios:\n\n1. **Log Segment Validation**: Each WAL segment begins with a header containing a checksum of the segment's contents. During recovery, the system validates each segment header and marks corrupted segments for special handling.\n\n2. **Entry Checksum Verification**: Individual log entries contain CRC32 checksums that detect corruption within valid segments. Corrupted entries are skipped during replay, and their absence is logged for operator investigation.\n\n3. **Partial Write Detection**: Power failures can cause partial writes where only part of a log entry is written to disk. The recovery system detects these by checking entry length headers against available data.\n\n4. **Timestamp Consistency Checking**: Replayed entries must have timestamps consistent with the recovery point. Entries with timestamps far in the future or past are considered suspect and may indicate clock issues or corruption.\n\n5. **Index Rebuilding**: After replaying all valid WAL entries, the system rebuilds its in-memory indexes and validates them against the recovered data. Any inconsistencies trigger a full reindex operation.\n\n| Corruption Type | Detection Method | Recovery Action | Data Impact |\n|----------------|------------------|----------------|-------------|\n| Segment Header Corruption | Header checksum mismatch | Skip entire segment, log corruption | Loss of data in corrupted segment |\n| Entry Corruption | Entry checksum failure | Skip corrupted entry, continue replay | Loss of individual corrupted entries |\n| Partial Write | Length header mismatch | Truncate at last valid entry | Loss of incomplete final entry |\n| Timestamp Inconsistency | Time bounds checking | Skip suspicious entries with warnings | Potential loss of out-of-order data |\n\n#### Index Inconsistency Recovery\n\nThe `InvertedIndexes` maintain mappings from metric names and label combinations to time series identifiers. Index corruption can cause queries to return incorrect results or fail to find existing data. The storage engine detects index inconsistencies during normal operations and can rebuild indexes from the authoritative chunk data.\n\nIndex consistency checking operates continuously during normal operations, validating that index entries correspond to actual stored data and that all stored data has appropriate index entries:\n\n**Forward Consistency Checking**: For each index entry pointing to a time series, verify that the referenced time series actually exists in storage and contains the expected metric name and labels.\n\n**Reverse Consistency Checking**: For each stored time series, verify that appropriate index entries exist and point to the correct series identifier.\n\n**Cross-Index Consistency**: Verify that the metric name index, label value indexes, and series metadata index all contain consistent information about the same time series.\n\n**Cardinality Validation**: Compare the number of unique time series found in storage against cardinality tracking counters to detect missing or extra index entries.\n\nThe recovery system can rebuild indexes in several modes depending on the severity of detected inconsistencies:\n\n| Inconsistency Type | Rebuild Strategy | Downtime Required | Performance Impact |\n|--------------------|------------------|-------------------|-------------------|\n| Missing Index Entries | Incremental rebuild of missing entries | None | Temporary query slowdown |\n| Extra Index Entries | Remove orphaned entries during background cleanup | None | Minimal |\n| Cross-Index Mismatch | Full reindex of affected metric names | Read-only mode during rebuild | Significant temporary impact |\n| Systematic Corruption | Complete index rebuild from chunk data | Full downtime during rebuild | Complete rebuild required |\n\n#### Data Consistency Validation\n\nThe storage engine implements continuous data consistency checking that validates the integrity of compressed chunks, the accuracy of series metadata, and the correctness of time ordering within series data.\n\nConsistency validation operates at multiple levels:\n\n**Chunk-Level Validation**: Each compressed chunk maintains metadata about its time range, sample count, and compression parameters. The validation system periodically decompresses chunks and verifies that the contained samples match the metadata.\n\n**Series-Level Validation**: Time series must maintain strict time ordering of samples and consistent label sets across all chunks. The validation system checks for timestamp inversions, duplicate timestamps, and label mutations within series.\n\n**Storage-Level Validation**: The overall storage system maintains invariants about total series count, disk space usage, and index sizes. Background validation jobs verify these invariants and alert operators to systematic issues.\n\n**Cross-Component Validation**: Consistency checks verify that WAL entries, compressed chunks, and index entries all describe the same underlying data consistently.\n\n### Query Errors\n\nThe **query engine** must handle malformed queries, missing data scenarios, and resource exhaustion while providing useful error messages that help users diagnose and fix their queries. Query error handling is particularly challenging because users may not understand the underlying data model or system limitations.\n\nThink of query error handling like a reference librarian helping researchers find information. When a researcher asks for something that doesn't exist or is unclear in their request, the librarian doesn't simply say \"not found\"—they explain what resources are available, suggest alternatives, and help reformulate the request to find relevant information.\n\n#### Invalid Expression Parsing\n\nPromQL expression parsing must handle syntax errors gracefully while providing specific error messages that help users correct their queries. The parser encounters various error categories that require different diagnostic approaches:\n\n**Lexical Errors**: Invalid characters, unterminated strings, or malformed numbers in the query text. These errors include specific position information and suggest valid alternatives.\n\n**Syntax Errors**: Grammatically incorrect expressions such as mismatched parentheses, invalid operator precedence, or incomplete function calls. The parser provides context about expected tokens and suggests corrections.\n\n**Semantic Errors**: Syntactically correct expressions that violate PromQL semantic rules, such as applying vector operators to scalar values or using undefined functions. These errors explain the type mismatch and suggest valid operations.\n\n**Label Selector Errors**: Invalid label matching expressions, malformed regular expressions, or label names that violate naming conventions. The parser validates regex patterns and provides specific regex error messages.\n\nThe `ExpressionParser` maintains an error collection that accumulates multiple parsing errors and provides comprehensive feedback rather than stopping at the first error encountered:\n\n| Error Category | Example Query | Error Message | Suggested Fix |\n|----------------|---------------|---------------|---------------|\n| Lexical | `http_requests_@total` | Invalid character '@' at position 13 | Use valid metric name characters: [a-zA-Z0-9_:] |\n| Syntax | `http_requests_total +` | Unexpected end of expression, expected right operand | Complete the addition operation: `+ <expression>` |\n| Semantic | `rate(\"string\")` | Function rate() expects vector argument, got string | Use vector selector: `rate(http_requests_total[5m])` |\n| Label Selector | `{__name__=~\"[invalid\"` | Invalid regex pattern: missing closing bracket | Fix regex pattern: `{__name__=~\"valid.*\"}` |\n\n> **Decision: Detailed Error Context Over Simple Error Messages**\n> - **Context**: Generic error messages like \"parse error\" don't help users fix their queries effectively\n> - **Options Considered**: Simple error codes, detailed position-specific messages, suggested corrections with examples\n> - **Decision**: Provide detailed error messages with position information, context, and suggested corrections\n> - **Rationale**: Query languages are complex and users need specific guidance to construct valid expressions\n> - **Consequences**: More complex error handling code but much better user experience and faster debugging\n\n#### Missing Data Handling\n\nTime series data is inherently sparse—not all metrics are available at all times, and query time ranges may extend beyond available data. The query engine must handle these scenarios gracefully while providing clear indication of data availability issues.\n\nMissing data scenarios require different handling strategies based on their cause and the query type:\n\n**Series Not Found**: When a query references metric names or label combinations that don't exist in storage, the query engine returns empty results rather than errors. This matches Prometheus behavior where missing metrics are treated as empty result sets rather than error conditions.\n\n**Time Range Gaps**: Queries may request data from time ranges where no samples were collected due to scraping failures or target downtime. The query engine interpolates across small gaps (less than 2 scrape intervals) but returns empty values for larger gaps.\n\n**Staleness Handling**: Samples older than the configured staleness threshold (typically 5 minutes) are considered stale and excluded from instant queries. Range queries may include stale samples with appropriate warnings.\n\n**Partial Data Availability**: When some series in an aggregation query have data while others don't, the query engine includes warnings about partial results and indicates which series contributed to the final result.\n\nThe query engine provides comprehensive warnings about data availability issues:\n\n| Missing Data Type | Query Behavior | Warning Message | User Action |\n|-------------------|----------------|----------------|-------------|\n| Metric Not Found | Return empty result | No time series found for selector | Verify metric name and labels exist |\n| Time Range Gap | Return empty for gap period | Data gap detected from X to Y | Check target availability during gap |\n| Stale Data | Exclude from instant queries | Using stale data older than threshold | Check recent scraping status |\n| Partial Aggregation | Include available series only | Aggregation includes only N of M series | Investigate missing series |\n\n#### Resource Exhaustion Protection\n\nQuery execution can consume significant memory and CPU resources, particularly for queries over long time ranges or high-cardinality metrics. The query engine implements multiple layers of resource protection to prevent individual queries from affecting system stability.\n\nThink of query resource limits like admission control at a concert venue. The venue has a maximum capacity that ensures safety and comfort for all attendees. When capacity is reached, additional people must wait rather than creating an unsafe overcrowding situation. Similarly, the query engine limits concurrent queries and per-query resource usage to maintain stability for all users.\n\nThe `QueryCoordinator` implements several resource protection mechanisms:\n\n**Concurrent Query Limiting**: A semaphore limits the number of queries executing simultaneously to prevent CPU and memory exhaustion. Queries that exceed the limit are queued or rejected with appropriate error messages.\n\n**Per-Query Memory Limits**: Each query has a maximum memory allocation for storing intermediate results and final output. Queries that exceed this limit are terminated with resource exhaustion errors.\n\n**Time Range Restrictions**: Very long time range queries can consume excessive resources and are limited to configurable maximum durations (typically 30 days). Users must break long-range queries into smaller segments.\n\n**Series Count Limits**: Queries that would process more than a maximum number of time series (typically 10,000) are rejected to prevent memory exhaustion from high-cardinality selectors.\n\n**Query Timeout Enforcement**: All queries have maximum execution time limits (typically 30 seconds) after which they're terminated to prevent resource hoarding.\n\nThe resource protection system provides specific error messages that help users understand the limit violation and suggest query modifications:\n\n| Resource Limit | Threshold | Error Message | Suggested Solution |\n|----------------|-----------|---------------|-------------------|\n| Concurrent Queries | 10 simultaneous | Maximum concurrent queries exceeded, try again later | Wait for other queries to complete or increase limit |\n| Memory Usage | 1GB per query | Query memory limit exceeded at N GB | Reduce time range or use more specific label selectors |\n| Time Range | 30 days | Query time range exceeds maximum of 30 days | Break query into smaller time segments |\n| Series Count | 10,000 series | Query would examine N series, limit is 10,000 | Use more specific label selectors to reduce cardinality |\n| Execution Time | 30 seconds | Query timeout after 30 seconds | Simplify query or reduce time range |\n\n### Resource Protection\n\nSystem-wide resource protection ensures that no single component or operation can compromise overall system stability. This requires coordination between all components and global limits that prevent resource exhaustion at the system level.\n\nThink of system resource protection like air traffic control at a major airport. Individual flights (queries, scrapes, storage operations) may have their own requirements, but the control system must manage overall airport capacity, runway utilization, and airspace congestion to keep all flights operating safely. No single flight is allowed to disrupt the entire airport's operation.\n\n#### Memory Management Strategy\n\nMemory usage in metrics collection systems can grow rapidly due to high-cardinality metrics, large query results, or accumulated metadata. The system implements a comprehensive memory management strategy that monitors usage, enforces limits, and provides graceful degradation when approaching limits.\n\nThe memory management system operates at three levels:\n\n**Component-Level Limits**: Each major component (scrape engine, storage engine, query engine) has dedicated memory quotas that prevent any single component from consuming all available memory.\n\n**Operation-Level Limits**: Individual operations within components have specific memory limits that prevent single large operations from affecting other operations.\n\n**System-Level Monitoring**: Global memory usage monitoring triggers emergency responses when total system memory usage approaches dangerous levels.\n\n| Component | Memory Quota | Enforcement Mechanism | Degradation Strategy |\n|-----------|--------------|----------------------|-------------------|\n| Scrape Engine | 25% of system memory | Limit concurrent scrapes | Reduce scraping parallelism |\n| Storage Engine | 50% of system memory | Limit chunk cache and WAL buffer size | Flush caches more aggressively |\n| Query Engine | 20% of system memory | Per-query limits and queue depth | Reject or queue new queries |\n| System Overhead | 5% reserved | OS and other processes | Emergency garbage collection |\n\n#### Cardinality Explosion Prevention\n\nLabel cardinality explosion represents one of the most dangerous failure modes in metrics systems because it can cause exponential growth in memory usage and storage requirements. The system implements multiple layers of cardinality control that detect, prevent, and mitigate high-cardinality scenarios.\n\nThe `CardinalityTracker` monitors series creation rates and label combination patterns to detect cardinality explosions early:\n\n**Per-Metric Cardinality Limits**: Each metric name has a maximum number of allowed label combinations (typically 10,000 series). New series exceeding this limit are rejected with specific error messages.\n\n**Label Value Limits**: Individual labels have maximum numbers of allowed distinct values (typically 1,000 values per label name). This prevents single labels from creating excessive cardinality.\n\n**Series Creation Rate Limits**: The system monitors the rate of new time series creation and temporarily blocks new series when creation rates exceed thresholds that indicate cardinality explosions.\n\n**High-Cardinality Detection**: Background monitoring identifies metrics and labels contributing most to total cardinality, providing operators with actionable information about cardinality sources.\n\nThe cardinality protection system provides detailed error messages that help users understand the cardinality violation and suggest alternatives:\n\n| Cardinality Limit | Threshold | Error Message | Remediation Strategy |\n|-------------------|-----------|---------------|---------------------|\n| Per-Metric Series | 10,000 series | Metric 'X' exceeds 10,000 series limit | Use fewer label dimensions or aggregate at source |\n| Label Value Count | 1,000 values | Label 'Y' has too many distinct values | Use label value prefixes or reduce granularity |\n| Series Creation Rate | 1,000/minute | New series creation rate too high | Review metric instrumentation for cardinality bugs |\n| Total System Series | 1,000,000 series | System approaching maximum series capacity | Review all metrics for optimization opportunities |\n\n⚠️ **Pitfall: Cardinality Limits Too Restrictive**\nSetting cardinality limits too low can prevent legitimate use cases where high cardinality is necessary for proper observability. The limits should be based on actual system capacity and usage patterns rather than arbitrary conservative values. Monitor actual cardinality usage and adjust limits based on system performance and storage capacity.\n\n#### Graceful Degradation Mechanisms\n\nWhen the system approaches resource limits, graceful degradation maintains partial functionality rather than complete system failure. This requires careful prioritization of operations and temporary reduction in quality or completeness of service.\n\nThe system implements several graceful degradation strategies:\n\n**Scraping Degradation**: When memory or CPU usage is high, the scrape engine reduces scraping frequency for less critical targets while maintaining normal intervals for high-priority metrics.\n\n**Storage Degradation**: The storage engine may temporarily disable compression or reduce cache sizes to free memory for critical operations like query processing.\n\n**Query Degradation**: The query engine may impose stricter limits on query complexity, reduce the maximum number of returned data points, or provide approximate results instead of exact calculations.\n\n**Service Shedding**: In extreme overload situations, the system may temporarily reject new requests (queries or metric ingestion) while processing the existing workload.\n\nEach degradation mechanism provides clear indication of reduced service levels and estimates of when normal operation will resume:\n\n| Degradation Type | Trigger Condition | Service Impact | Recovery Condition |\n|------------------|-------------------|----------------|-------------------|\n| Reduced Scraping | CPU > 80% for 5 minutes | 50% longer intervals for low-priority targets | CPU < 60% for 2 minutes |\n| Storage Throttling | Memory > 85% | Compression disabled, smaller write batches | Memory < 75% |\n| Query Limiting | Query queue > 50 requests | Stricter per-query limits, longer timeouts | Queue < 10 requests |\n| Request Rejection | System overload detected | HTTP 503 responses with retry-after headers | Load drops below threshold |\n\n> **Decision: Graceful Degradation Over Complete Failure**\n> - **Context**: Resource exhaustion traditionally causes complete system failure and loss of observability\n> - **Options Considered**: Fail fast when resources exhausted, graceful degradation with reduced service, horizontal scaling requirements\n> - **Decision**: Implement graceful degradation that maintains critical functionality while reducing less important operations\n> - **Rationale**: Partial observability during high load is much more valuable than complete loss of metrics during peak demand\n> - **Consequences**: More complex resource management but maintains system availability during stress conditions\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Error Handling | Standard Go error values with fmt.Errorf | Structured errors with github.com/pkg/errors for stack traces |\n| Circuit Breaker | Custom implementation with simple counters | github.com/sony/gobreaker for production features |\n| Health Monitoring | Basic success/failure counters | Exponential moving averages for failure rates |\n| Resource Limits | Simple sync.WaitGroup semaphores | Context-based cancellation with resource tracking |\n| Logging | Standard log package | Structured logging with github.com/sirupsen/logrus |\n\n#### File Structure\n\n```\ninternal/\n├── errors/\n│   ├── errors.go              ← Custom error types and utilities\n│   ├── circuit_breaker.go     ← Circuit breaker implementation\n│   └── health.go              ← Target health tracking\n├── scrape/\n│   ├── health.go              ← Target health management\n│   ├── timeout.go             ← Multi-layer timeout handling\n│   └── parser_errors.go       ← Metrics parsing error handling\n├── storage/\n│   ├── consistency.go         ← Data consistency validation\n│   ├── recovery.go            ← WAL and index recovery\n│   └── disk_monitor.go        ← Disk space monitoring\n├── query/\n│   ├── resource_limits.go     ← Query resource protection\n│   ├── parse_errors.go        ← Query parsing error handling\n│   └── coordinator.go         ← Query concurrency management\n└── system/\n    ├── resource_monitor.go    ← System-wide resource tracking\n    └── degradation.go         ← Graceful degradation logic\n```\n\n#### Infrastructure Starter Code\n\nComplete error handling infrastructure that provides structured error types and utilities:\n\n```go\n// internal/errors/errors.go\npackage errors\n\nimport (\n    \"fmt\"\n    \"time\"\n)\n\n// ErrorType categorizes different kinds of errors for handling decisions\ntype ErrorType string\n\nconst (\n    ErrorTypeTransient    ErrorType = \"transient\"    // Retry may succeed\n    ErrorTypePermanent    ErrorType = \"permanent\"    // No point retrying\n    ErrorTypeRateLimit    ErrorType = \"rate_limit\"   // Need backoff\n    ErrorTypeResource     ErrorType = \"resource\"     // Resource exhaustion\n)\n\n// MetricsError provides structured error information with context\ntype MetricsError struct {\n    Type       ErrorType\n    Component  string\n    Operation  string\n    Message    string\n    Cause      error\n    Timestamp  time.Time\n    Context    map[string]interface{}\n}\n\nfunc (e *MetricsError) Error() string {\n    if e.Cause != nil {\n        return fmt.Sprintf(\"%s/%s: %s (caused by: %v)\", e.Component, e.Operation, e.Message, e.Cause)\n    }\n    return fmt.Sprintf(\"%s/%s: %s\", e.Component, e.Operation, e.Message)\n}\n\nfunc NewTransientError(component, operation, message string, cause error) *MetricsError {\n    return &MetricsError{\n        Type:      ErrorTypeTransient,\n        Component: component,\n        Operation: operation,\n        Message:   message,\n        Cause:     cause,\n        Timestamp: time.Now(),\n        Context:   make(map[string]interface{}),\n    }\n}\n\nfunc NewPermanentError(component, operation, message string, cause error) *MetricsError {\n    return &MetricsError{\n        Type:      ErrorTypePermanent,\n        Component: component,\n        Operation: operation,\n        Message:   message,\n        Cause:     cause,\n        Timestamp: time.Now(),\n        Context:   make(map[string]interface{}),\n    }\n}\n\n// IsRetryable determines if an error should trigger retry logic\nfunc IsRetryable(err error) bool {\n    if metricsErr, ok := err.(*MetricsError); ok {\n        return metricsErr.Type == ErrorTypeTransient || metricsErr.Type == ErrorTypeRateLimit\n    }\n    return false\n}\n```\n\nComplete circuit breaker implementation:\n\n```go\n// internal/errors/circuit_breaker.go\npackage errors\n\nimport (\n    \"sync\"\n    \"time\"\n)\n\ntype CircuitState int\n\nconst (\n    CircuitClosed CircuitState = iota\n    CircuitOpen\n    CircuitHalfOpen\n)\n\ntype CircuitBreaker struct {\n    mu                  sync.RWMutex\n    state              CircuitState\n    failureCount       int\n    successCount       int\n    lastFailureTime    time.Time\n    lastSuccessTime    time.Time\n    \n    // Configuration\n    failureThreshold   int\n    successThreshold   int\n    timeout           time.Duration\n    onStateChange     func(from, to CircuitState)\n}\n\nfunc NewCircuitBreaker(failureThreshold, successThreshold int, timeout time.Duration) *CircuitBreaker {\n    return &CircuitBreaker{\n        state:            CircuitClosed,\n        failureThreshold: failureThreshold,\n        successThreshold: successThreshold,\n        timeout:          timeout,\n    }\n}\n\nfunc (cb *CircuitBreaker) Call(fn func() error) error {\n    if !cb.allowRequest() {\n        return NewPermanentError(\"circuit_breaker\", \"call\", \"circuit breaker is open\", nil)\n    }\n    \n    err := fn()\n    if err != nil {\n        cb.recordFailure()\n        return err\n    }\n    \n    cb.recordSuccess()\n    return nil\n}\n\nfunc (cb *CircuitBreaker) allowRequest() bool {\n    cb.mu.RLock()\n    defer cb.mu.RUnlock()\n    \n    switch cb.state {\n    case CircuitClosed:\n        return true\n    case CircuitOpen:\n        return time.Since(cb.lastFailureTime) >= cb.timeout\n    case CircuitHalfOpen:\n        return true\n    }\n    return false\n}\n\nfunc (cb *CircuitBreaker) recordSuccess() {\n    cb.mu.Lock()\n    defer cb.mu.Unlock()\n    \n    cb.successCount++\n    cb.lastSuccessTime = time.Now()\n    \n    if cb.state == CircuitHalfOpen && cb.successCount >= cb.successThreshold {\n        cb.setState(CircuitClosed)\n        cb.failureCount = 0\n        cb.successCount = 0\n    }\n}\n\nfunc (cb *CircuitBreaker) recordFailure() {\n    cb.mu.Lock()\n    defer cb.mu.Unlock()\n    \n    cb.failureCount++\n    cb.lastFailureTime = time.Now()\n    \n    if cb.state == CircuitClosed && cb.failureCount >= cb.failureThreshold {\n        cb.setState(CircuitOpen)\n    } else if cb.state == CircuitHalfOpen {\n        cb.setState(CircuitOpen)\n        cb.successCount = 0\n    }\n}\n\nfunc (cb *CircuitBreaker) setState(newState CircuitState) {\n    oldState := cb.state\n    cb.state = newState\n    \n    if cb.onStateChange != nil && oldState != newState {\n        go cb.onStateChange(oldState, newState)\n    }\n}\n```\n\n#### Core Logic Skeleton Code\n\nTarget health management with state transitions:\n\n```go\n// internal/scrape/health.go\nfunc (th *TargetHealth) RecordScrapeResult(success bool, duration time.Duration, err error) {\n    th.mu.Lock()\n    defer th.mu.Unlock()\n    \n    th.lastScrapeTime = time.Now()\n    \n    if success {\n        // TODO 1: Reset consecutive failure counter to 0\n        // TODO 2: Check if we should transition from HealthDown or HealthDegraded to better state\n        // TODO 3: Update lastSuccessTime and record scrape duration for metrics\n        // Hint: Use transition thresholds from health state table above\n    } else {\n        // TODO 1: Increment consecutive failure counter\n        // TODO 2: Check if failure count crosses threshold for state degradation\n        // TODO 3: Store error information for diagnostic purposes\n        // TODO 4: Update exponential backoff interval if in HealthDown state\n        // Hint: Max backoff should be 5 minutes, use exponential growth with jitter\n    }\n}\n\nfunc (th *TargetHealth) ShouldScrape() (bool, time.Duration) {\n    th.mu.RLock()\n    defer th.mu.RUnlock()\n    \n    // TODO 1: Check current health state and return appropriate scrape decision\n    // TODO 2: For HealthUp: return true with normal interval\n    // TODO 3: For HealthDegraded: return true with 2x normal interval\n    // TODO 4: For HealthDown: return true only if backoff period has elapsed\n    // TODO 5: Calculate next scrape time based on state and backoff strategy\n    // Hint: Use time.Since(th.lastScrapeTime) to check if enough time has passed\n}\n```\n\nStorage disk space monitoring with emergency retention:\n\n```go\n// internal/storage/disk_monitor.go\nfunc (dm *DiskMonitor) CheckDiskSpace() (*DiskSpaceStatus, error) {\n    // TODO 1: Use syscall.Statfs (Unix) or similar to get filesystem stats\n    // TODO 2: Calculate available space percentage\n    // TODO 3: Determine alert level based on thresholds from disk space table\n    // TODO 4: If below emergency threshold, trigger immediate retention cleanup\n    // TODO 5: Return status with recommended actions for storage engine\n    // Hint: Available space = (free blocks * block size) / (total blocks * block size)\n}\n\nfunc (dm *DiskMonitor) TriggerEmergencyRetention(targetSpacePercent float64) error {\n    // TODO 1: Calculate how much data needs to be deleted to reach target space\n    // TODO 2: Identify oldest data chunks that can be safely deleted\n    // TODO 3: Delete chunks in order of age, prioritizing low-importance metrics\n    // TODO 4: Update indexes to remove references to deleted chunks\n    // TODO 5: Force garbage collection and filesystem sync to reclaim space immediately\n    // Hint: Use priority levels stored in SeriesMetadata to determine deletion order\n}\n```\n\nQuery resource limiting with graceful degradation:\n\n```go\n// internal/query/resource_limits.go\nfunc (ql *QueryLimiter) ExecuteWithLimits(ctx context.Context, query string, fn func() (*QueryResult, error)) (*QueryResult, error) {\n    // TODO 1: Check if query exceeds complexity limits (time range, series count estimate)\n    // TODO 2: Acquire semaphore slot for concurrent query limiting\n    // TODO 3: Set up memory tracking for this query execution\n    // TODO 4: Create timeout context if none provided or if shorter than configured limit\n    // TODO 5: Execute query function with resource monitoring\n    // TODO 6: Check memory usage during execution, terminate if limit exceeded\n    // TODO 7: Release resources and update usage statistics\n    // Hint: Use context.WithTimeout and context.WithCancel for resource control\n}\n\nfunc (ql *QueryLimiter) EstimateQueryComplexity(query string, timeRange time.Duration) (*ComplexityEstimate, error) {\n    // TODO 1: Parse query to identify metric selectors and operations\n    // TODO 2: Estimate number of time series that would be examined\n    // TODO 3: Calculate memory requirements based on time range and series count  \n    // TODO 4: Identify expensive operations (regex matching, aggregation functions)\n    // TODO 5: Return complexity estimate with resource requirements\n    // Hint: Use simple heuristics like series_count * time_range_hours * 8 bytes per sample\n}\n```\n\n#### Milestone Checkpoints\n\n**Target Health Management Verification**:\n- Run scrape engine with mix of healthy and failing targets\n- Verify state transitions follow the health state table exactly\n- Check that backoff intervals increase exponentially for failing targets\n- Confirm circuit breaker prevents wasted requests to consistently failing targets\n\n**Storage Error Recovery Testing**:\n- Simulate disk full condition and verify graceful degradation\n- Kill storage process during writes and verify WAL recovery\n- Corrupt index files and verify automatic rebuild from chunk data  \n- Verify consistency checker detects and reports data inconsistencies\n\n**Query Resource Protection Validation**:\n- Submit queries exceeding memory limits and verify they're terminated\n- Run many concurrent queries and verify queueing/rejection behavior\n- Test very long time range queries get appropriate error messages\n- Verify high-cardinality queries are rejected with specific feedback\n\n**System-Wide Degradation Testing**:\n- Load system to resource limits and verify graceful degradation activates\n- Confirm partial functionality maintained during resource exhaustion\n- Verify system automatically recovers when load decreases\n- Check that degradation status is clearly visible in system metrics\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| Targets show as down but are accessible | Circuit breaker stuck open | Check circuit breaker state and failure history | Reset circuit breaker or adjust thresholds |\n| Storage consuming excessive disk space | Retention policy not running | Check disk monitor logs and retention job status | Manually trigger retention cleanup |\n| Queries timing out frequently | Resource limits too restrictive | Check query complexity estimates and actual resource usage | Increase limits or optimize queries |\n| Memory usage growing indefinitely | Cardinality explosion not detected | Check series creation rates and label cardinality | Implement stricter cardinality limits |\n| WAL recovery taking very long | Large WAL files with many segments | Check WAL segment sizes and rotation frequency | Adjust WAL rotation settings |\n| Parse errors for valid metrics format | Strict validation rejecting valid data | Check parsing error logs for specific validation failures | Relax validation rules or fix data format |\n\n\n## Testing and Validation Strategy\n\n> **Milestone(s):** This section provides comprehensive testing approaches for all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4), along with end-to-end validation of their integrated operation.\n\n### The Quality Assurance Lighthouse Mental Model\n\nThink of our testing strategy like a lighthouse system guiding ships safely to shore. Just as lighthouses provide multiple layers of navigation safety—from close-in harbor lights to distant beacon warnings—our testing strategy provides multiple concentric layers of validation. The innermost ring consists of **component testing** that validates individual pieces work correctly in isolation, like harbor lights that help boats navigate the immediate docking area. The middle ring contains **integration testing** that verifies components work together properly, like channel markers that guide ships through connected waterways. The outermost ring encompasses **end-to-end testing** that validates complete user workflows, like the main lighthouse beacon that provides navigation from miles offshore. Performance validation acts as the weather monitoring system, ensuring our lighthouse remains operational under all conditions—calm seas and storms alike.\n\nThis layered approach ensures that problems are caught as early and specifically as possible. A unit test failure tells us exactly which component is broken, like a harbor light malfunction points to a specific dock. An integration test failure indicates communication problems between components, like misaligned channel markers. An end-to-end test failure suggests workflow problems that might only appear under realistic usage patterns, like a lighthouse whose beam is blocked by unexpected fog.\n\n### Component Testing\n\nComponent testing validates each system component in isolation, using test doubles to eliminate external dependencies and focus on the component's core logic and behavior. This isolation allows us to test error conditions, edge cases, and performance characteristics that would be difficult to reproduce in integrated scenarios.\n\n#### Metrics Data Model Testing\n\nThe metrics data model components require thorough testing of their semantic behaviors, thread safety, and cardinality management capabilities. Each metric type has specific behavioral contracts that must be validated under both normal and stress conditions.\n\n| Test Category | Scope | Key Validations | Test Techniques |\n|---------------|-------|-----------------|-----------------|\n| Counter Semantics | Counter increment behavior | Monotonic increase, no decreases allowed, overflow handling | Property-based testing with random increments |\n| Gauge Semantics | Gauge set/add behavior | Arbitrary values allowed, thread-safe updates | Concurrent goroutines setting values |\n| Histogram Semantics | Bucket observation behavior | Correct bucket assignment, sum/count accuracy | Statistical validation of distributions |\n| Label Validation | Label cardinality control | Name/value length limits, character restrictions, cardinality tracking | Fuzzing with invalid characters and long strings |\n| Thread Safety | Concurrent metric updates | No data races, consistent reads during writes | Race detector with high concurrency |\n\n**Counter Behavior Testing:** Counter tests verify that increment operations maintain monotonic behavior and that attempts to decrease values are properly rejected. The test suite generates random sequences of increment operations and validates that the final value equals the sum of all increments. Thread safety testing spawns multiple goroutines performing concurrent increments and verifies that no increments are lost due to race conditions.\n\n**Gauge Behavior Testing:** Gauge tests validate that set and add operations work correctly with both positive and negative values. Unlike counters, gauges must accept any floating-point value including negative numbers, zero, and special values like positive/negative infinity. Concurrent testing verifies that rapid updates from multiple goroutines don't corrupt the stored value.\n\n**Histogram Distribution Testing:** Histogram tests validate that observed values are assigned to the correct buckets and that sum and count fields maintain accuracy. Statistical testing generates known distributions and verifies that the histogram buckets capture the distribution shape correctly. Edge case testing validates behavior with extreme values, NaN, and infinity.\n\n**Label Cardinality Testing:** Label validation testing ensures that the cardinality tracking system correctly prevents label explosion attacks. Tests attempt to create series with high-cardinality label combinations and verify that appropriate limits are enforced. Performance testing measures memory usage as cardinality increases to validate that growth remains within acceptable bounds.\n\n> **Decision: Property-Based Testing for Metric Semantics**\n> - **Context**: Metric types have mathematical properties that must hold across all possible input sequences\n> - **Options Considered**: Example-based unit tests, property-based testing, formal verification\n> - **Decision**: Property-based testing with example-based edge cases\n> - **Rationale**: Properties like counter monotonicity are universal invariants that should hold for any valid input sequence. Property-based testing explores the input space more thoroughly than hand-written examples while remaining practical to implement\n> - **Consequences**: Tests catch more edge cases but may be harder to debug when they fail. Requires careful property specification to avoid vacuous tests\n\n#### Scrape Engine Testing\n\nThe scrape engine requires testing of HTTP operations, parsing logic, service discovery, and error handling. Testing focuses on isolation using HTTP test servers and mock service discovery backends to create deterministic test conditions.\n\n| Test Category | Scope | Key Validations | Test Techniques |\n|---------------|-------|-----------------|-----------------|\n| HTTP Scraping | Target endpoint communication | Timeout handling, response parsing, error recovery | HTTP test servers with controllable responses |\n| Format Parsing | Prometheus exposition format | Correct metric extraction, malformed input handling | Corpus-based testing with valid/invalid samples |\n| Service Discovery | Target discovery and updates | Dynamic target list updates, health tracking | Mock discovery backends with scripted changes |\n| Concurrency Control | Parallel scraping operations | No resource leaks, proper cleanup, backpressure handling | Load testing with many concurrent targets |\n| Target Health | Health state transitions | Accurate failure detection, recovery detection | Fault injection with network simulation |\n\n**HTTP Transport Testing:** HTTP scraping tests use Go's `httptest` package to create controllable HTTP servers that can simulate various response conditions. Test servers can introduce delays to validate timeout handling, return malformed responses to test error recovery, and simulate network failures to verify retry logic. Connection pooling and resource cleanup testing ensures that scraping doesn't leak HTTP connections or goroutines.\n\n**Exposition Format Parsing:** Parser testing validates correct extraction of metrics from Prometheus exposition format text. A comprehensive test corpus includes well-formed metrics, malformed input that should be rejected, edge cases like empty values or unusual characters, and large payloads that test memory usage. Fuzzing techniques generate random input to discover parsing crashes or hangs.\n\n**Service Discovery Testing:** Service discovery testing uses mock backends that implement the `TargetDiscoverer` interface to provide scripted target updates. Tests validate that target additions, removals, and label updates are processed correctly and that the scrape engine adapts its scraping schedule accordingly. Performance testing measures the time to detect and process large target set changes.\n\n**Concurrent Scraping Testing:** Concurrency testing spawns hundreds of mock targets and validates that the scrape engine can handle the load without resource exhaustion. Tests monitor goroutine counts, memory usage, and file descriptor usage to detect leaks. Backpressure testing validates that the scrape engine gracefully handles storage slowdowns without dropping data or consuming excessive memory.\n\n⚠️ **Pitfall: Testing Against Real HTTP Endpoints**\nUsing real HTTP endpoints in component tests makes tests non-deterministic and dependent on external services. The tests become slow, flaky, and may fail due to network issues unrelated to the code being tested. Instead, use `httptest.Server` to create local test servers that can simulate any response pattern needed for testing. This makes tests fast, reliable, and completely controllable.\n\n#### Time Series Storage Testing\n\nStorage engine testing focuses on data durability, compression correctness, query performance, and recovery behavior. Testing emphasizes validation of complex concurrent scenarios where reads and writes happen simultaneously.\n\n| Test Category | Scope | Key Validations | Test Techniques |\n|---------------|-------|-----------------|-----------------|\n| Data Durability | Write-ahead logging | WAL recovery, corruption detection, consistency | Process restart simulation, disk failure injection |\n| Compression Accuracy | Gorilla compression algorithm | Lossless compression, decompression accuracy | Statistical validation with real-world data patterns |\n| Query Performance | Series selection and retrieval | Index efficiency, memory usage, query timeouts | Benchmark testing with varying cardinality levels |\n| Concurrent Access | Read/write coordination | Data consistency, no corruption, fair scheduling | High-concurrency stress testing |\n| Storage Limits | Disk usage and retention | Cleanup effectiveness, emergency procedures | Disk space simulation with controlled limits |\n\n**Write-Ahead Log Testing:** WAL testing validates that all writes are durably recorded before being applied to the main storage. Recovery testing simulates process crashes at various points during write operations and verifies that recovery produces consistent state. Corruption detection testing deliberately corrupts WAL files and validates that the recovery process detects and handles the corruption appropriately.\n\n**Compression Algorithm Testing:** Gorilla compression testing validates that the delta-of-delta timestamp encoding and XOR value encoding produce bit-accurate results. Test data includes regular timestamp intervals, irregular intervals, and pathological cases like duplicate timestamps or wildly varying values. Decompression testing ensures that compressed data can be accurately reconstructed without any data loss.\n\n**Index Performance Testing:** Index testing validates that the inverted indexes provide efficient series lookup across different cardinality levels. Benchmark tests measure lookup performance as the number of series and labels grows. Memory usage testing tracks index memory consumption and validates that it remains within expected bounds as data volume increases.\n\n**Concurrent Access Testing:** Concurrency testing spawns multiple goroutines performing simultaneous reads and writes and validates that the data remains consistent. Testing uses techniques like read-after-write validation to ensure that writes are immediately visible to subsequent reads. Lock contention testing measures performance under high concurrency and validates that the system provides fair access to both readers and writers.\n\n⚠️ **Pitfall: Insufficient WAL Recovery Testing**\nMany storage systems fail catastrophically because WAL recovery wasn't tested thoroughly. It's not enough to test that recovery works when the process shuts down cleanly—you must test recovery after crashes during critical operations like WAL rotation, index updates, and chunk compression. Use tools like `kill -9` or controlled process termination to simulate realistic crash scenarios.\n\n#### Query Engine Testing\n\nQuery engine testing validates PromQL parsing correctness, execution accuracy, and resource management. Testing emphasizes mathematical accuracy of aggregations and proper handling of time-based operations.\n\n| Test Category | Scope | Key Validations | Test Techniques |\n|---------------|-------|-----------------|-----------------|\n| PromQL Parsing | Expression parsing accuracy | AST correctness, syntax error handling, operator precedence | Grammar-based test generation |\n| Label Matching | Series filtering logic | Regex performance, exact matching, negative selectors | Large-scale cardinality testing |\n| Aggregation Math | Mathematical correctness | Sum, average, quantile accuracy, grouping behavior | Statistical validation against known results |\n| Range Queries | Time-based operations | Interpolation accuracy, staleness handling, step alignment | Time series simulation with known patterns |\n| Resource Limits | Memory and execution time | Query timeouts, memory limits, complexity estimation | Adversarial query construction |\n\n**PromQL Parser Testing:** Parser testing validates that PromQL expressions are correctly converted into executable abstract syntax trees. Test cases include all supported operators, functions, and syntax constructs. Error handling testing ensures that invalid queries produce helpful error messages with specific locations of syntax problems. Precedence testing validates that complex expressions with multiple operators are parsed with correct operator precedence.\n\n**Label Selector Testing:** Label matching testing validates that label selectors correctly filter time series based on label values. Performance testing measures regex matching speed with complex patterns and large label sets. Edge case testing validates behavior with empty labels, special characters, and Unicode content in label values.\n\n**Mathematical Accuracy Testing:** Aggregation testing validates that mathematical operations produce numerically accurate results. Statistical testing compares query results against independently computed expected values. Edge case testing validates handling of special floating-point values like NaN, infinity, and denormal numbers. Precision testing ensures that aggregations maintain accuracy even with large numbers of input values.\n\n**Time Range Query Testing:** Range query testing validates that queries over time windows produce correct results with proper interpolation and staleness handling. Test data includes regular and irregular timestamp patterns. Boundary condition testing validates behavior at query start and end times, including proper handling of data points that fall exactly on boundaries.\n\n> **Decision: Statistical Validation for Mathematical Operations**\n> - **Context**: Aggregation functions must produce mathematically correct results but floating-point arithmetic introduces precision issues\n> - **Options Considered**: Exact arithmetic libraries, statistical validation with tolerance, property-based testing\n> - **Decision**: Statistical validation with appropriate tolerance levels for floating-point operations\n> - **Rationale**: Exact arithmetic is too slow for production use, but we must validate that results are within acceptable precision bounds. Different operations have different precision characteristics that require specific tolerance levels\n> - **Consequences**: Tests must specify appropriate tolerance levels for each operation type. Enables fast floating-point operations while catching significant mathematical errors\n\n### End-to-End Testing\n\nEnd-to-end testing validates complete workflows from metrics scraping through storage to querying, using realistic data patterns and operational scenarios. These tests verify that components work together correctly and that the system provides the expected user experience.\n\n#### Complete Scrape-to-Query Workflows\n\nEnd-to-end workflow testing validates the entire pipeline from target discovery through data storage to query execution. These tests use real HTTP servers exposing Prometheus-format metrics and execute actual PromQL queries against the stored data.\n\n**Workflow Test Scenarios:**\n\n| Scenario | Description | Validation Points | Success Criteria |\n|----------|-------------|-------------------|------------------|\n| Basic Scraping | Single target with simple metrics | Target discovery, scraping, storage, basic queries | All metrics stored correctly, queries return expected values |\n| Service Discovery | Dynamic target addition/removal | Target list updates, scraping adaptation, data consistency | New targets automatically discovered and scraped |\n| High Cardinality | Many series with complex labels | Memory usage, query performance, storage efficiency | System remains responsive under high cardinality load |\n| Long-Running | Extended operation over hours/days | Data retention, resource stability, query consistency | No memory leaks, stable performance over time |\n| Failure Recovery | Network failures and target outages | Error handling, recovery behavior, data consistency | Graceful degradation and recovery without data loss |\n\n**Basic Scraping Workflow Testing:** Tests start with a simple scenario involving one target exposing counter, gauge, and histogram metrics. The test validates that metrics are discovered, scraped at the correct interval, stored durably, and can be queried accurately. Timing validation ensures that scrapes happen at the expected intervals and that query results reflect the most recent scraped data.\n\n**Service Discovery Integration Testing:** Tests validate dynamic target management by starting with an empty target list and then adding targets through service discovery. The test monitors how quickly new targets are detected and begin being scraped. Target removal testing validates that removed targets stop being scraped and that their data remains queryable for the retention period.\n\n**High Cardinality Stress Testing:** Tests create scenarios with thousands of unique time series by using high-cardinality labels like user IDs or request paths. The test validates that the system can handle the load without excessive memory usage or query performance degradation. Resource monitoring throughout the test ensures that memory growth remains bounded and that garbage collection remains effective.\n\n**Long-Running Stability Testing:** Extended tests run for hours or days to validate system stability under continuous operation. Tests monitor memory usage, file descriptor counts, and query performance over time to detect resource leaks or performance degradation. Data consistency checks validate that older data remains accurate and queryable throughout the extended run.\n\n⚠️ **Pitfall: Inadequate Long-Running Test Duration**\nMany systems appear stable in short tests but develop problems over longer periods due to resource leaks or gradual performance degradation. End-to-end tests should run for at least several hours, preferably overnight, to detect these issues. Monitor system resources throughout the test and validate that performance remains stable over the entire duration.\n\n#### Multi-Component Integration Scenarios\n\nIntegration testing validates the interfaces and communication patterns between major system components. These tests focus on data flow, error propagation, and coordination between components.\n\n**Integration Test Categories:**\n\n| Integration | Components | Focus Areas | Key Validations |\n|-------------|------------|-------------|-----------------|\n| Scrape-Storage | ScrapeEngine, StorageEngine | Data ingestion pipeline | Sample batching, backpressure handling, durability |\n| Storage-Query | StorageEngine, QueryEngine | Data retrieval pipeline | Index usage, query optimization, result accuracy |\n| Cross-Component Error Handling | All components | Error propagation and recovery | Graceful degradation, error isolation, recovery coordination |\n| Resource Coordination | All components | Resource management | Memory limits, concurrency control, fair resource allocation |\n\n**Scrape-Storage Integration:** Tests validate the data flow from scraping to storage, focusing on the sample ingestion pipeline. Backpressure testing validates that storage slowdowns are properly communicated back to the scrape engine to prevent memory exhaustion. Batching tests verify that samples are efficiently grouped for storage operations. Durability testing validates that scraped data survives process restarts and storage failures.\n\n**Storage-Query Integration:** Tests validate that queries can efficiently retrieve data stored by the scrape engine. Index usage testing ensures that label selectors use indexes effectively rather than scanning all series. Query optimization testing validates that the query engine chooses efficient execution plans for complex queries. Cache coherency testing ensures that query results reflect recent writes from the scrape engine.\n\n**Error Handling Coordination:** Integration error testing validates that component failures are properly isolated and don't cascade to other components. Tests simulate various failure modes like storage disk full, query timeouts, and scrape target failures. Recovery coordination testing validates that components restart in the correct order and re-establish communication properly.\n\n**Resource Management Integration:** Resource coordination testing validates that components cooperate effectively in resource usage. Memory limit testing validates that components respect overall system memory limits rather than competing for resources. Concurrency testing validates that components don't create excessive goroutines or other concurrent resources that could overwhelm the system.\n\n#### Data Consistency and Accuracy Validation\n\nData accuracy testing validates that the complete system preserves data integrity throughout the scrape-store-query pipeline. These tests focus on numerical accuracy, timestamp preservation, and metadata consistency.\n\n**Accuracy Validation Approaches:**\n\n| Validation Type | Methodology | Test Data | Success Criteria |\n|-----------------|-------------|-----------|------------------|\n| Numerical Accuracy | Compare scraped values to query results | Known metric values from test targets | Zero data loss, accurate aggregations |\n| Timestamp Preservation | Validate timestamp consistency through pipeline | Synthetic time series with known timestamps | Timestamps preserved through storage and retrieval |\n| Metadata Consistency | Verify label and metric name preservation | Complex label combinations | All metadata preserved accurately |\n| Aggregation Accuracy | Compare aggregation results to mathematical truth | Statistical distributions with known properties | Aggregations match expected mathematical results |\n\n**End-to-End Numerical Accuracy:** Tests expose known metric values through test HTTP endpoints and validate that queries return identical values. Counter testing validates that increments are preserved accurately without any loss. Histogram testing validates that bucket counts and sum/count values are preserved correctly. Gauge testing validates that the most recent values are returned accurately by queries.\n\n**Timestamp Precision Testing:** Tests validate that timestamps are preserved accurately throughout the system. Test targets expose metrics with known timestamps and queries validate that the timestamps are stored and retrieved without modification. Time zone testing validates that UTC timestamps are handled consistently regardless of the local system timezone.\n\n**Label and Metadata Preservation:** Tests validate that metric names, label names, label values, and metric metadata are preserved accurately throughout the system. Unicode testing validates proper handling of international characters in labels. Special character testing validates handling of characters that might have special meaning in various system components.\n\n**Mathematical Aggregation Validation:** Tests validate that PromQL aggregations produce mathematically correct results. Statistical testing uses metrics with known distributions and validates that aggregation functions like `sum()`, `avg()`, and `quantile()` produce results within acceptable precision bounds. Cross-validation testing compares query results against independently computed expected values.\n\n> **Decision: Reference Implementation Validation**\n> - **Context**: End-to-end accuracy testing needs ground truth to compare against actual system results\n> - **Options Considered**: Hand-computed expected results, reference implementation, statistical validation\n> - **Decision**: Hybrid approach using reference implementation for complex scenarios and hand-computed results for simple cases\n> - **Rationale**: Hand computation works for simple scenarios but becomes impractical for complex queries. A reference implementation provides automated ground truth generation but may have its own bugs\n> - **Consequences**: Requires maintaining a separate reference implementation but enables comprehensive accuracy validation\n\n### Performance Validation\n\nPerformance validation ensures that the system meets scalability, latency, and resource usage requirements under realistic load conditions. Testing focuses on identifying bottlenecks and validating that performance characteristics meet operational requirements.\n\n#### Load Testing and Capacity Planning\n\nLoad testing validates system behavior under realistic operational loads and identifies performance bottlenecks before they impact production usage. Tests progressively increase load while monitoring system behavior and resource usage.\n\n**Load Testing Dimensions:**\n\n| Load Dimension | Test Parameters | Scaling Range | Key Metrics |\n|----------------|-----------------|---------------|-------------|\n| Scraping Scale | Number of targets and metrics per target | 10 to 10,000 targets | Scrape completion rate, memory usage, CPU usage |\n| Storage Throughput | Samples per second ingestion rate | 1K to 1M samples/sec | Write latency, compression ratio, disk usage |\n| Query Concurrency | Simultaneous query execution | 1 to 1,000 concurrent queries | Query response time, memory per query, throughput |\n| Data Volume | Total time series and time range | 1K to 10M series over weeks | Index performance, query latency, storage efficiency |\n\n**Scraping Load Testing:** Scraping load tests progressively increase the number of scrape targets while monitoring scrape completion rates and resource usage. Tests validate that the scrape engine can handle the target number of endpoints within the configured scrape intervals. Memory usage monitoring ensures that high target counts don't cause memory exhaustion. Network resource testing validates that the system doesn't exhaust network connections or file descriptors.\n\n**Storage Throughput Testing:** Storage load tests measure how many samples per second the storage engine can handle sustainably. Tests generate synthetic time series data at various ingestion rates while monitoring write latency and resource usage. Compression efficiency testing validates that storage space usage remains within expected bounds as ingestion rate increases. WAL performance testing ensures that write-ahead logging doesn't become a bottleneck at high ingestion rates.\n\n**Query Concurrency Testing:** Query load tests execute multiple simultaneous queries while measuring response times and resource usage. Tests include a mix of instant queries, range queries, and complex aggregations to simulate realistic query patterns. Memory usage per query is monitored to validate that concurrent queries don't cause memory exhaustion. Query isolation testing ensures that expensive queries don't significantly impact the performance of simple queries.\n\n**Long-Term Data Volume Testing:** Volume testing validates performance with large amounts of historical data. Tests create months or years of synthetic time series data and measure how query performance changes as data volume increases. Index scalability testing validates that label lookup performance remains acceptable as the number of unique series grows. Retention policy testing validates that data cleanup operations don't significantly impact query performance.\n\n#### Bottleneck Identification and Resource Monitoring\n\nPerformance testing includes comprehensive monitoring to identify bottlenecks and validate that resources are used efficiently. Monitoring focuses on the critical resources that typically limit performance in metrics systems.\n\n**Resource Monitoring Categories:**\n\n| Resource Category | Key Metrics | Monitoring Tools | Bottleneck Indicators |\n|-------------------|-------------|------------------|----------------------|\n| Memory Usage | Heap size, GC frequency, allocation rate | Runtime profiling, memory profilers | High GC overhead, allocation spikes |\n| CPU Utilization | CPU usage per component, goroutine counts | CPU profiling, runtime metrics | High CPU usage, goroutine leaks |\n| Disk I/O | Read/write throughput, latency, queue depth | I/O monitoring, disk stats | High I/O wait, storage latency spikes |\n| Network I/O | Connection counts, bandwidth usage, error rates | Network monitoring, connection pooling stats | Connection exhaustion, bandwidth saturation |\n\n**Memory Performance Analysis:** Memory monitoring tracks heap usage, garbage collection frequency, and allocation patterns throughout performance tests. Profiling identifies which components and operations consume the most memory. Memory leak detection validates that memory usage remains bounded during extended operation. Gorilla compression efficiency is validated by measuring the compression ratio achieved on realistic data patterns.\n\n**CPU Performance Analysis:** CPU profiling identifies which operations consume the most processing time. Goroutine monitoring ensures that the system doesn't create excessive concurrent operations. Lock contention analysis identifies synchronization bottlenecks that prevent efficient CPU utilization. Query parsing and execution profiling validates that PromQL operations perform efficiently.\n\n**Disk I/O Performance Analysis:** Disk monitoring measures read and write throughput during various operations. WAL write performance is critical for ingestion throughput. Index read performance affects query latency. Storage compaction operations are monitored to ensure they don't interfere with normal operations. Disk space usage is tracked to validate that retention policies work effectively.\n\n**Network I/O Performance Analysis:** Network monitoring tracks HTTP connection usage during scraping operations. Connection pooling efficiency is validated to ensure that scraping doesn't create excessive network overhead. DNS resolution performance is monitored since service discovery can generate significant DNS traffic. Network timeout and retry behavior is validated under various network conditions.\n\n#### Scalability Requirements Verification\n\nScalability testing validates that the system can handle the target operational scale defined in the system requirements. Tests specifically target the scalability limits identified in the goals and non-goals section.\n\n**Scalability Test Targets:**\n\n| Scalability Requirement | Target Scale | Test Approach | Acceptance Criteria |\n|-------------------------|--------------|---------------|-------------------|\n| Concurrent Targets | 1,000 scrape targets | Progressive load increase | All targets scraped within interval |\n| Series Cardinality | 1M unique time series | High-cardinality label generation | Query performance remains acceptable |\n| Query Throughput | 100 queries/second | Concurrent query execution | Mean response time under 1 second |\n| Data Retention | 30 days of historical data | Long-running data accumulation | Storage usage within 2x of raw data size |\n\n**Target Scale Verification:** Scalability tests progressively increase system load until reaching the target scale requirements. Each test level is sustained for sufficient time to validate stability at that scale. Resource usage is monitored throughout scaling tests to identify when resources become constrained. Performance characteristics like query latency and scrape completion rates are tracked to validate that they remain within acceptable bounds at target scale.\n\n**Cardinality Limit Testing:** High-cardinality testing generates time series with label combinations that approach the target cardinality limits. Memory usage is carefully monitored to validate that the system can handle high cardinality without excessive memory consumption. Query performance testing validates that label selectors remain efficient even with high cardinality. Index scalability is validated by measuring lookup performance as cardinality increases.\n\n**Throughput Sustainability Testing:** Throughput tests validate that the system can sustain target throughput rates over extended periods without performance degradation. Load balancing effectiveness is validated by measuring how evenly work is distributed across system resources. Backpressure handling is tested by temporarily constraining resources and validating that the system handles the constraint gracefully without data loss.\n\n⚠️ **Pitfall: Testing Only Peak Performance**\nMany performance tests only measure short-term peak performance rather than sustained throughput over realistic time periods. Real systems must handle continuous load for hours or days, not just brief spikes. Performance tests should sustain target loads for at least 30 minutes to validate that resource usage remains stable and that performance doesn't degrade over time.\n\n### Milestone Verification\n\nMilestone verification provides concrete checkpoints to validate successful completion of each development phase. Each milestone includes specific behavioral verification, performance benchmarks, and integration validation steps.\n\n#### Milestone 1: Metrics Data Model Verification\n\nMilestone 1 verification validates that the metrics data model correctly implements counter, gauge, and histogram semantics with proper labeling and cardinality control.\n\n**Verification Checklist:**\n\n| Verification Category | Test Procedures | Expected Results | Validation Commands |\n|----------------------|-----------------|------------------|-------------------|\n| Metric Type Behavior | Create and manipulate each metric type | Counters only increase, gauges accept any value, histograms distribute correctly | Unit test suite execution |\n| Label Functionality | Create metrics with various label combinations | Labels attached correctly, cardinality tracking works | Label validation test execution |\n| Thread Safety | Concurrent access to metric instances | No race conditions, consistent values | Race detector test execution |\n| Cardinality Control | Attempt to create high-cardinality combinations | Limits enforced, memory usage bounded | Cardinality stress test execution |\n\n**Counter Behavior Verification:** Counter testing validates that increment operations work correctly and that attempts to decrease values are rejected. Overflow testing validates behavior when counter values approach floating-point limits. Thread safety testing uses concurrent goroutines to validate that increments are atomic and no updates are lost.\n\n```bash\n# Example verification commands\ngo test -race ./internal/metrics/counter_test.go\ngo test -bench=BenchmarkCounterIncrement ./internal/metrics/\n```\n\n**Gauge Behavior Verification:** Gauge testing validates that both set and add operations work with positive and negative values. Special value testing validates handling of infinity, NaN, and zero values. Concurrent testing validates that rapid updates don't corrupt the stored value.\n\n**Histogram Behavior Verification:** Histogram testing validates that observations are assigned to correct buckets and that sum and count fields are maintained accurately. Distribution testing uses known statistical distributions and validates that histogram buckets correctly represent the distribution shape.\n\n**Label System Verification:** Label validation testing creates metrics with various label combinations and validates that labels are stored and retrieved correctly. Cardinality testing attempts to create high-cardinality label combinations and validates that appropriate limits are enforced. Unicode testing validates that international characters in label names and values are handled correctly.\n\n#### Milestone 2: Scrape Engine Verification\n\nMilestone 2 verification validates that the scrape engine can discover targets, scrape metrics via HTTP, and handle various failure modes gracefully.\n\n**Verification Checklist:**\n\n| Verification Category | Test Procedures | Expected Results | Validation Commands |\n|----------------------|-----------------|------------------|-------------------|\n| Target Discovery | Configure static and dynamic targets | Targets discovered and added to scrape list | Discovery integration test |\n| HTTP Scraping | Scrape from test HTTP endpoints | Metrics successfully retrieved and parsed | HTTP scraping test |\n| Error Handling | Simulate network failures and timeouts | Graceful failure handling, retry logic works | Fault injection test |\n| Service Discovery | Add/remove targets dynamically | Target list updates automatically | Service discovery test |\n\n**Target Discovery Verification:** Discovery testing configures both static targets and mock service discovery and validates that all targets are discovered and added to the scrape schedule. Update testing validates that target list changes are processed correctly. Health tracking testing validates that target health status is maintained accurately.\n\n```bash\n# Example verification commands\ngo test ./internal/scraper/discovery_test.go\ngo test -timeout=30s ./internal/scraper/integration_test.go\n```\n\n**HTTP Scraping Verification:** HTTP testing uses `httptest.Server` to create controllable test endpoints that expose Prometheus-format metrics. Parsing verification validates that various metric types are correctly extracted from the HTTP response. Timeout testing validates that scrapes that exceed the timeout are cancelled properly.\n\n**Error Handling Verification:** Fault injection testing simulates various error conditions including network timeouts, HTTP errors, malformed responses, and DNS failures. Recovery testing validates that the scrape engine recovers correctly when failed targets become available again. Circuit breaker testing validates that consistently failing targets are temporarily bypassed.\n\n**Service Discovery Integration Verification:** Service discovery testing uses mock discovery backends to simulate target additions, removals, and metadata updates. Latency testing measures how quickly target changes are detected and processed. Consistency testing validates that target metadata is correctly propagated from service discovery to scraping operations.\n\n#### Milestone 3: Time Series Storage Verification\n\nMilestone 3 verification validates that the storage engine correctly stores time series data with compression, provides durable persistence, and supports efficient queries.\n\n**Verification Checklist:**\n\n| Verification Category | Test Procedures | Expected Results | Validation Commands |\n|----------------------|-----------------|------------------|-------------------|\n| Data Persistence | Write data and restart process | All data recovered correctly after restart | WAL recovery test |\n| Compression Efficiency | Store various data patterns | Compression ratios within expected range | Compression benchmark test |\n| Query Performance | Execute queries on stored data | Query response times within limits | Query performance test |\n| Concurrent Access | Simultaneous reads and writes | No data corruption, consistent results | Concurrency test |\n\n**Data Persistence Verification:** Persistence testing writes time series data and then simulates process crashes at various points. Recovery testing validates that all durably committed data is correctly restored after restart. WAL integrity testing validates that write-ahead log recovery produces consistent results.\n\n```bash\n# Example verification commands  \ngo test ./internal/storage/wal_test.go\ngo test -bench=BenchmarkStorageWrite ./internal/storage/\n```\n\n**Compression Efficiency Verification:** Compression testing stores various time series patterns including regular intervals, irregular intervals, and pathological cases. Compression ratio measurement validates that Gorilla compression achieves expected space savings. Decompression accuracy testing validates that compressed data can be reconstructed without any loss.\n\n**Query Performance Verification:** Query testing executes various query patterns against stored data and measures response times. Index efficiency testing validates that label selectors use indexes effectively. Memory usage testing validates that queries don't consume excessive memory even with large result sets.\n\n**Concurrent Access Verification:** Concurrency testing spawns multiple goroutines performing simultaneous reads and writes. Data consistency testing validates that concurrent operations don't corrupt stored data. Lock contention testing measures performance under high concurrency and validates that the system provides fair access to resources.\n\n#### Milestone 4: Query Engine Verification\n\nMilestone 4 verification validates that the query engine correctly parses PromQL expressions, executes queries accurately, and provides expected aggregation functionality.\n\n**Verification Checklist:**\n\n| Verification Category | Test Procedures | Expected Results | Validation Commands |\n|----------------------|-----------------|------------------|-------------------|\n| PromQL Parsing | Parse various PromQL expressions | Correct AST generation, proper error messages | Parser test suite |\n| Query Execution | Execute instant and range queries | Accurate results, proper error handling | Query execution test |\n| Aggregation Functions | Test sum, avg, max, min, count operations | Mathematically correct aggregation results | Aggregation test suite |\n| Label Matching | Test exact, regex, and inequality selectors | Correct series filtering, efficient execution | Label selector test |\n\n**PromQL Parsing Verification:** Parser testing validates that various PromQL expressions are correctly converted to abstract syntax trees. Error handling testing validates that invalid expressions produce helpful error messages. Precedence testing validates that complex expressions are parsed with correct operator precedence.\n\n```bash\n# Example verification commands\ngo test ./internal/query/parser_test.go  \ngo test -bench=BenchmarkQueryExecution ./internal/query/\n```\n\n**Query Execution Verification:** Query execution testing validates that both instant and range queries produce accurate results. Time range testing validates that queries over various time windows work correctly. Interpolation testing validates that query results are correctly interpolated between actual data points.\n\n**Aggregation Function Verification:** Mathematical testing validates that aggregation functions produce numerically correct results. Statistical testing uses known distributions and validates that aggregation results match expected mathematical values. Edge case testing validates handling of special values like NaN and infinity in aggregations.\n\n**Label Matching Verification:** Label selector testing validates that exact matching, regex matching, and inequality operators correctly filter time series. Performance testing validates that complex label selectors execute efficiently. Cardinality testing validates that label selectors work correctly even with high-cardinality label sets.\n\n### Implementation Guidance\n\nThis implementation guidance provides practical approaches for building a comprehensive testing framework that validates all aspects of the metrics collection system.\n\n#### Technology Recommendations\n\n| Testing Category | Simple Option | Advanced Option |\n|------------------|---------------|-----------------|\n| Unit Testing Framework | Go standard testing package with testify assertions | Ginkgo BDD framework with Gomega matchers |\n| HTTP Testing | httptest package for mock servers | WireMock or similar for complex HTTP scenarios |\n| Property-Based Testing | rapid testing library | gopter property-based testing framework |\n| Load Testing | Simple goroutine-based load generation | k6 or Artillery for sophisticated load patterns |\n| Performance Profiling | Go built-in pprof | Continuous profiling with pprof integration |\n| Test Data Management | In-memory test fixtures | Testcontainers for external dependencies |\n\n#### Recommended Project Structure\n\nThe testing code should be organized to support both component-level and integration testing with clear separation of concerns:\n\n```\nproject-root/\n  internal/\n    metrics/\n      counter.go\n      counter_test.go          ← Unit tests for counter behavior\n      gauge_test.go           ← Unit tests for gauge behavior  \n      histogram_test.go       ← Unit tests for histogram behavior\n      labels_test.go          ← Unit tests for label validation\n    scraper/\n      scraper.go\n      scraper_test.go         ← Unit tests for scraping logic\n      discovery_test.go       ← Unit tests for service discovery\n      integration_test.go     ← Integration tests with HTTP servers\n    storage/\n      storage.go\n      storage_test.go         ← Unit tests for storage operations\n      wal_test.go            ← Unit tests for write-ahead log\n      compression_test.go     ← Unit tests for compression algorithms\n    query/\n      parser_test.go          ← Unit tests for PromQL parsing\n      executor_test.go        ← Unit tests for query execution\n  test/\n    integration/\n      end_to_end_test.go      ← Complete workflow tests\n      performance_test.go     ← Load and performance tests\n    fixtures/\n      metrics_samples.txt     ← Sample Prometheus format data\n      test_queries.promql     ← Sample PromQL queries for testing\n    helpers/\n      test_server.go          ← HTTP test server utilities\n      data_generator.go       ← Synthetic data generation\n```\n\n#### Testing Infrastructure Starter Code\n\nHere's a complete testing infrastructure that provides the foundation for comprehensive testing:\n\n```go\npackage testhelpers\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"math/rand\"\n    \"net/http\"\n    \"net/http/httptest\"\n    \"strings\"\n    \"sync\"\n    \"testing\"\n    \"time\"\n)\n\n// MockHTTPTarget provides a controllable HTTP endpoint for scraping tests\ntype MockHTTPTarget struct {\n    server     *httptest.Server\n    metrics    []string\n    delay      time.Duration\n    errorRate  float64\n    mu         sync.RWMutex\n}\n\n// NewMockHTTPTarget creates a new mock target with configurable behavior\nfunc NewMockHTTPTarget() *MockHTTPTarget {\n    target := &MockHTTPTarget{\n        metrics: []string{\n            \"test_counter 42\",\n            \"test_gauge 3.14\",\n            \"test_histogram_bucket{le=\\\"1.0\\\"} 10\",\n        },\n    }\n    \n    target.server = httptest.NewServer(http.HandlerFunc(target.handleMetrics))\n    return target\n}\n\n// URL returns the target's HTTP URL for scraping\nfunc (t *MockHTTPTarget) URL() string {\n    return t.server.URL + \"/metrics\"\n}\n\n// SetMetrics updates the metrics exposed by this target\nfunc (t *MockHTTPTarget) SetMetrics(metrics []string) {\n    t.mu.Lock()\n    defer t.mu.Unlock()\n    t.metrics = metrics\n}\n\n// SetDelay configures response delay for timeout testing\nfunc (t *MockHTTPTarget) SetDelay(delay time.Duration) {\n    t.mu.Lock()\n    defer t.mu.Unlock()\n    t.delay = delay\n}\n\n// SetErrorRate configures the probability of HTTP errors\nfunc (t *MockHTTPTarget) SetErrorRate(rate float64) {\n    t.mu.Lock()\n    defer t.mu.Unlock()\n    t.errorRate = rate\n}\n\nfunc (t *MockHTTPTarget) handleMetrics(w http.ResponseWriter, r *http.Request) {\n    t.mu.RLock()\n    delay := t.delay\n    errorRate := t.errorRate\n    metrics := t.metrics\n    t.mu.RUnlock()\n    \n    // Simulate network delay\n    if delay > 0 {\n        time.Sleep(delay)\n    }\n    \n    // Simulate random errors\n    if rand.Float64() < errorRate {\n        http.Error(w, \"Simulated server error\", http.StatusInternalServerError)\n        return\n    }\n    \n    w.Header().Set(\"Content-Type\", \"text/plain\")\n    for _, metric := range metrics {\n        fmt.Fprintln(w, metric)\n    }\n}\n\n// Close shuts down the mock target server\nfunc (t *MockHTTPTarget) Close() {\n    t.server.Close()\n}\n\n// TimeSeriesGenerator creates synthetic time series data for testing\ntype TimeSeriesGenerator struct {\n    rand *rand.Rand\n}\n\n// NewTimeSeriesGenerator creates a generator with a fixed seed for reproducible tests\nfunc NewTimeSeriesGenerator(seed int64) *TimeSeriesGenerator {\n    return &TimeSeriesGenerator{\n        rand: rand.New(rand.NewSource(seed)),\n    }\n}\n\n// GenerateCounterSeries creates a monotonically increasing counter series\nfunc (g *TimeSeriesGenerator) GenerateCounterSeries(name string, labels map[string]string, \n    start time.Time, interval time.Duration, count int) []Sample {\n    \n    samples := make([]Sample, count)\n    value := float64(0)\n    \n    for i := 0; i < count; i++ {\n        // Counters can only increase\n        increment := g.rand.Float64() * 10\n        value += increment\n        \n        samples[i] = Sample{\n            Timestamp: start.Add(time.Duration(i) * interval),\n            Value:     value,\n        }\n    }\n    \n    return samples\n}\n\n// GenerateGaugeSeries creates a gauge series with random walk behavior  \nfunc (g *TimeSeriesGenerator) GenerateGaugeSeries(name string, labels map[string]string,\n    start time.Time, interval time.Duration, count int) []Sample {\n    \n    samples := make([]Sample, count)\n    value := g.rand.Float64() * 100\n    \n    for i := 0; i < count; i++ {\n        // Gauges can increase or decrease\n        change := (g.rand.Float64() - 0.5) * 20\n        value += change\n        \n        samples[i] = Sample{\n            Timestamp: start.Add(time.Duration(i) * interval),\n            Value:     value,\n        }\n    }\n    \n    return samples\n}\n\n// PerformanceMonitor tracks resource usage during tests\ntype PerformanceMonitor struct {\n    startTime      time.Time\n    startMemory    uint64\n    measurements   []ResourceMeasurement\n    mu             sync.Mutex\n}\n\ntype ResourceMeasurement struct {\n    Timestamp     time.Time\n    MemoryUsage   uint64\n    GoroutineCount int\n    CPUUsage      float64\n}\n\n// NewPerformanceMonitor creates a monitor for tracking test performance\nfunc NewPerformanceMonitor() *PerformanceMonitor {\n    return &PerformanceMonitor{\n        startTime: time.Now(),\n        measurements: make([]ResourceMeasurement, 0),\n    }\n}\n\n// StartMonitoring begins periodic resource measurement\nfunc (m *PerformanceMonitor) StartMonitoring(ctx context.Context, interval time.Duration) {\n    ticker := time.NewTicker(interval)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case <-ctx.Done():\n            return\n        case <-ticker.C:\n            m.takeMeasurement()\n        }\n    }\n}\n\nfunc (m *PerformanceMonitor) takeMeasurement() {\n    // TODO: Implement actual resource measurement\n    // This would use runtime.ReadMemStats(), runtime.NumGoroutine(), etc.\n    // For now, provide placeholder implementation\n    \n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    measurement := ResourceMeasurement{\n        Timestamp:      time.Now(),\n        MemoryUsage:    0, // TODO: Get actual memory usage\n        GoroutineCount: 0, // TODO: Get actual goroutine count  \n        CPUUsage:       0, // TODO: Get actual CPU usage\n    }\n    \n    m.measurements = append(m.measurements, measurement)\n}\n\n// GetSummary returns performance statistics from monitoring\nfunc (m *PerformanceMonitor) GetSummary() PerformanceSummary {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    \n    if len(m.measurements) == 0 {\n        return PerformanceSummary{}\n    }\n    \n    // TODO: Calculate actual performance statistics\n    return PerformanceSummary{\n        Duration:       time.Since(m.startTime),\n        PeakMemory:     0, // TODO: Calculate from measurements\n        AvgGoroutines:  0, // TODO: Calculate from measurements  \n        MaxGoroutines:  0, // TODO: Calculate from measurements\n    }\n}\n\ntype PerformanceSummary struct {\n    Duration      time.Duration\n    PeakMemory    uint64\n    AvgGoroutines int\n    MaxGoroutines int\n}\n```\n\n#### Core Testing Logic Skeletons\n\nHere are the essential test function signatures with detailed TODO comments for implementation:\n\n```go\npackage metrics_test\n\nimport (\n    \"context\"\n    \"sync\"\n    \"testing\"\n    \"time\"\n)\n\n// TestCounterBehavior validates counter semantic correctness\nfunc TestCounterBehavior(t *testing.T) {\n    // TODO 1: Create a new counter instance\n    // TODO 2: Verify initial value is 0\n    // TODO 3: Call Inc() and verify value increases to 1  \n    // TODO 4: Call Add(5) and verify value increases to 6\n    // TODO 5: Attempt Add(-1) and verify it returns an error\n    // TODO 6: Verify final value is still 6 (no decrease occurred)\n}\n\n// TestCounterConcurrency validates thread-safe counter operations\nfunc TestCounterConcurrency(t *testing.T) {\n    // TODO 1: Create a new counter instance\n    // TODO 2: Start 10 goroutines, each incrementing counter 100 times\n    // TODO 3: Use sync.WaitGroup to wait for all goroutines to complete\n    // TODO 4: Verify final counter value equals 1000 (10 * 100)\n    // TODO 5: Run test with -race flag to detect race conditions\n    // Hint: Use atomic operations or mutex in counter implementation\n}\n\n// TestHistogramDistribution validates histogram bucket assignment\nfunc TestHistogramDistribution(t *testing.T) {\n    buckets := []float64{1.0, 5.0, 10.0, math.Inf(1)}\n    // TODO 1: Create histogram with specified bucket boundaries\n    // TODO 2: Observe values: 0.5, 2.0, 7.0, 15.0\n    // TODO 3: Verify bucket counts: [1, 1, 1, 1] \n    // TODO 4: Verify total count equals 4\n    // TODO 5: Verify sum equals 24.5 (0.5 + 2.0 + 7.0 + 15.0)\n    // Hint: Values should be assigned to first bucket where value <= bucket\n}\n\n// TestLabelCardinality validates cardinality explosion prevention\nfunc TestLabelCardinality(t *testing.T) {\n    tracker := NewCardinalityTracker(maxSeries = 1000)\n    // TODO 1: Create metrics with normal cardinality (should succeed)\n    // TODO 2: Attempt to create metrics exceeding cardinality limit\n    // TODO 3: Verify that excess metrics are rejected with appropriate error\n    // TODO 4: Verify that memory usage remains bounded\n    // TODO 5: Test cleanup of expired series to free cardinality\n    // Hint: Use combination of metric name + label set to identify unique series\n}\n\n// TestScrapeTargetDiscovery validates service discovery integration\nfunc TestScrapeTargetDiscovery(t *testing.T) {\n    mockDiscovery := NewMockServiceDiscovery()\n    scrapeEngine := NewScrapeEngine(config, storage, logger)\n    \n    // TODO 1: Start scrape engine with empty target list\n    // TODO 2: Add targets via mock service discovery\n    // TODO 3: Verify targets are discovered within discovery interval\n    // TODO 4: Remove targets via service discovery\n    // TODO 5: Verify removed targets stop being scraped\n    // TODO 6: Verify target metadata is correctly propagated\n    // Hint: Mock service discovery should implement TargetDiscoverer interface\n}\n\n// TestScrapeErrorHandling validates graceful failure handling\nfunc TestScrapeErrorHandling(t *testing.T) {\n    target := NewMockHTTPTarget()\n    defer target.Close()\n    \n    // TODO 1: Configure target to return HTTP 500 errors\n    // TODO 2: Verify scrape engine records failure and continues\n    // TODO 3: Configure target to timeout on requests\n    // TODO 4: Verify scrape engine cancels request and marks target down\n    // TODO 5: Configure target to return malformed metrics\n    // TODO 6: Verify partial success - valid metrics stored, invalid rejected\n    // Hint: Use target.SetErrorRate() and target.SetDelay() for fault injection\n}\n\n// TestStorageCompression validates Gorilla compression accuracy\nfunc TestStorageCompression(t *testing.T) {\n    // TODO 1: Generate time series with regular 15-second intervals\n    // TODO 2: Compress samples using GorillaCompressor\n    // TODO 3: Decompress and verify all samples match exactly\n    // TODO 4: Measure compression ratio (should be < 2 bytes/sample)\n    // TODO 5: Test with irregular timestamps and verify accuracy\n    // TODO 6: Test with pathological data (constant values, huge jumps)\n    // Hint: Gorilla compression works best with regular intervals and smooth changes\n}\n\n// TestStorageRecovery validates WAL recovery after crash\nfunc TestStorageRecovery(t *testing.T) {\n    tempDir := t.TempDir()\n    \n    // TODO 1: Create storage engine and write test data\n    // TODO 2: Simulate crash by closing storage without clean shutdown\n    // TODO 3: Create new storage engine instance with same data directory\n    // TODO 4: Verify all committed data is recovered correctly\n    // TODO 5: Verify uncommitted data in WAL is replayed\n    // TODO 6: Test recovery with corrupted WAL entries\n    // Hint: WAL should be replayed in order, corrupt entries should be detected\n}\n\n// TestQueryParsing validates PromQL expression parsing\nfunc TestQueryParsing(t *testing.T) {\n    parser := NewExpressionParser()\n    \n    // TODO 1: Parse simple metric selector: `http_requests_total`\n    // TODO 2: Verify AST contains MetricSelectorNode with correct name\n    // TODO 3: Parse selector with labels: `http_requests_total{method=\"GET\"}`\n    // TODO 4: Verify label matchers are parsed correctly\n    // TODO 5: Parse complex expression with functions: `rate(http_requests_total[5m])`\n    // TODO 6: Parse invalid expressions and verify helpful error messages\n    // Hint: Build AST recursively, respect operator precedence\n}\n\n// TestQueryExecution validates query result accuracy\nfunc TestQueryExecution(t *testing.T) {\n    // TODO 1: Store known time series data in storage engine\n    // TODO 2: Execute instant query and verify results match expected values\n    // TODO 3: Execute range query and verify all timestamps are covered\n    // TODO 4: Test label selector queries with exact and regex matching\n    // TODO 5: Test aggregation queries (sum, avg, max, min, count)\n    // TODO 6: Verify mathematical accuracy of aggregation results\n    // Hint: Use statistical validation with appropriate floating-point tolerance\n}\n\n// TestEndToEndWorkflow validates complete scrape-to-query pipeline\nfunc TestEndToEndWorkflow(t *testing.T) {\n    // TODO 1: Start mock HTTP target exposing known metrics\n    // TODO 2: Configure scrape engine to scrape the target\n    // TODO 3: Wait for metrics to be scraped and stored\n    // TODO 4: Execute queries against stored metrics\n    // TODO 5: Verify query results match original metrics from target\n    // TODO 6: Test with target failures and recovery\n    // Hint: Use eventually assertions for asynchronous scraping operations\n}\n\n// BenchmarkStorageWrite measures storage write performance\nfunc BenchmarkStorageWrite(b *testing.B) {\n    storage := NewStorageEngine(config, logger)\n    samples := generateTestSamples(1000)\n    \n    b.ResetTimer()\n    // TODO 1: Measure samples per second write throughput\n    // TODO 2: Measure write latency distribution (p50, p95, p99)\n    // TODO 3: Measure memory allocation per write operation\n    // TODO 4: Test with varying batch sizes\n    // TODO 5: Report compression ratio achieved\n    // Hint: Use b.N for iteration count, benchmark multiple scenarios\n}\n\n// TestResourceLimits validates system behavior under resource constraints\nfunc TestResourceLimits(t *testing.T) {\n    // TODO 1: Configure system with low memory limits\n    // TODO 2: Generate high-cardinality metrics to approach limit\n    // TODO 3: Verify system applies backpressure before exhausting memory\n    // TODO 4: Test query limits with complex queries\n    // TODO 5: Verify system remains responsive under resource pressure\n    // TODO 6: Test disk space limits and emergency retention\n    // Hint: Monitor resource usage throughout test, set aggressive limits\n}\n```\n\n#### Milestone Checkpoints\n\nEach milestone completion should be validated with these specific checkpoints:\n\n**Milestone 1 Checkpoint - Metrics Data Model:**\n```bash\n# Run all metrics tests with race detection\ngo test -race ./internal/metrics/...\n\n# Benchmark metric operations performance  \ngo test -bench=. ./internal/metrics/\n\n# Expected output: All tests pass, no race conditions detected\n# Benchmark results should show sub-microsecond operation times\n```\n\n**Milestone 2 Checkpoint - Scrape Engine:**\n```bash\n# Run scrape engine tests\ngo test -timeout=30s ./internal/scraper/...\n\n# Run integration test with HTTP targets\ngo test -run=TestScrapeIntegration ./test/integration/\n\n# Expected: Targets discovered, metrics scraped, HTTP errors handled gracefully\n```\n\n**Milestone 3 Checkpoint - Time Series Storage:**\n```bash\n# Test storage with crash recovery\ngo test -run=TestStorageRecovery ./internal/storage/\n\n# Benchmark write performance\ngo test -bench=BenchmarkStorageWrite ./internal/storage/\n\n# Expected: Recovery works correctly, write throughput > 10K samples/sec\n```\n\n**Milestone 4 Checkpoint - Query Engine:**\n```bash\n# Test PromQL parsing and execution\ngo test ./internal/query/...\n\n# End-to-end workflow test\ngo test -run=TestEndToEndWorkflow ./test/integration/\n\n# Expected: PromQL queries execute correctly, results mathematically accurate\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|-------------|----------------|-----|\n| Tests hang indefinitely | Deadlock in concurrent code | Use `go test -timeout=30s`, check goroutine dumps | Review lock ordering, use channels instead of mutexes where possible |\n| Race detector failures | Unsynchronized access to shared data | Run with `-race`, examine race reports | Add proper synchronization with mutexes or atomic operations |\n| Memory usage grows unbounded | Memory leaks or missing cleanup | Profile with `go test -memprofile=mem.prof` | Implement proper cleanup, check for goroutine leaks |\n| Inconsistent test failures | Timing-dependent test logic | Add logging, use deterministic test data | Use mock clocks, add proper synchronization |\n| Compression tests fail | Incorrect Gorilla algorithm implementation | Compare bit-by-bit with reference implementation | Debug delta-of-delta calculation, check XOR logic |\n| Query results incorrect | Mathematical precision issues | Compare with independently computed expected values | Use appropriate floating-point tolerance, check aggregation logic |\n\n\n## Debugging Guide\n\n> **Milestone(s):** This section provides systematic debugging approaches for all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4). It helps developers diagnose and fix issues across the entire metrics collection pipeline.\n\nBuilding a metrics collection system introduces numerous failure modes that can be difficult to diagnose. Unlike traditional CRUD applications where failures are often obvious and immediate, metrics systems fail in subtle ways that may not surface until production load or specific edge cases occur. The distributed nature of scraping, the time-series characteristics of the data, and the complex interactions between compression, indexing, and querying create a debugging landscape that requires systematic approaches and specialized tools.\n\nThis section provides a comprehensive guide for diagnosing and fixing issues that commonly arise during development and operation of the metrics collection system. The debugging strategies are organized around observable symptoms rather than internal implementation details, because developers typically encounter problems through external manifestations like slow queries, missing data, or crashed processes.\n\n### Symptom-Based Troubleshooting\n\n**Mental Model: The Detective's Methodology**\n\nThink of debugging a metrics system like investigating a crime scene. You start with observable symptoms (the crime), gather evidence through logs and metrics (witness testimony and physical evidence), form hypotheses about root causes (suspect theories), test those hypotheses with additional investigation (interrogation and forensics), and finally implement fixes (arrest and prosecution). The key is following the evidence systematically rather than jumping to conclusions based on assumptions.\n\nThe symptom-based approach works because it mirrors how problems actually manifest in production. A user reports \"queries are slow\" or \"metrics are missing\" - they don't report \"the Gorilla compressor has a bug in delta calculation.\" By starting with symptoms and working backward to root causes, we build a systematic investigation process that works regardless of the specific implementation details.\n\n#### High-Level System Symptoms\n\nThese symptoms affect the overall system behavior and typically indicate problems in component coordination or resource exhaustion.\n\n| Symptom | Likely Root Causes | Investigation Steps | Resolution Actions |\n|---------|-------------------|-------------------|------------------- |\n| System consumes excessive memory | Label explosion, inefficient compression, query result sets too large | Check cardinality metrics, analyze heap dumps, review query patterns | Implement cardinality limits, fix compression bugs, add query result size limits |\n| Queries return no data despite active scraping | Index corruption, time zone mismatches, retention policy too aggressive | Verify scrape success rates, check timestamp alignment, inspect index consistency | Rebuild indexes, fix timestamp handling, adjust retention settings |\n| System becomes unresponsive under load | Resource exhaustion, deadlocks, inefficient algorithms | Monitor resource usage, check for lock contention, profile CPU usage | Add resource limits, fix deadlocks, optimize hot paths |\n| Data appears and disappears intermittently | Race conditions in storage, inconsistent reads, partial failures | Enable race detector, add consistency checks, review concurrent access patterns | Fix race conditions, add proper locking, implement read consistency |\n| System fails to start after restart | Corrupted WAL, configuration errors, dependency unavailability | Check WAL recovery logs, validate configuration, verify dependencies | Repair or rebuild WAL, fix configuration, ensure dependencies are available |\n\n#### Scraping Engine Symptoms\n\nScraping problems often manifest as missing or stale data, and can be particularly tricky because network issues create intermittent failures.\n\n| Symptom | Likely Root Causes | Investigation Steps | Resolution Actions |\n|---------|-------------------|-------------------|------------------- |\n| Targets show as \"down\" but are actually healthy | Network connectivity issues, DNS resolution problems, authentication failures | Test manual HTTP requests, check DNS resolution, verify credentials | Fix network configuration, update DNS settings, correct authentication |\n| Scraping succeeds but no metrics are stored | Parsing errors, timestamp issues, storage failures | Check parsing error rates, verify timestamp formats, monitor storage health | Fix parsing logic, standardize timestamp handling, resolve storage issues |\n| Some metrics missing from multi-metric endpoints | Partial parsing failures, label validation errors, metric name conflicts | Enable detailed parsing logs, check label validation rules, inspect metric naming | Handle partial failures gracefully, fix validation rules, resolve naming conflicts |\n| Scraping performance degrades over time | Connection pool exhaustion, memory leaks, increasing target latency | Monitor connection pool usage, check memory growth, measure target response times | Implement connection pooling, fix memory leaks, adjust timeout settings |\n| Targets oscillate between healthy and unhealthy | Network instability, target overload, circuit breaker misconfiguration | Analyze target health history, check target resource usage, review circuit breaker settings | Adjust health check thresholds, reduce target load, tune circuit breaker parameters |\n\n#### Storage Engine Symptoms\n\nStorage problems often create subtle data corruption or performance issues that compound over time.\n\n| Symptom | Likely Root Causes | Investigation Steps | Resolution Actions |\n|---------|-------------------|-------------------|------------------- |\n| Write performance degrades significantly | WAL bottlenecks, compression inefficiency, disk I/O limits | Monitor WAL write rates, profile compression performance, check disk I/O patterns | Optimize WAL batching, fix compression algorithms, add faster storage |\n| Queries return incorrect aggregated values | Compression errors, timestamp alignment issues, counter reset handling | Verify raw sample values, check timestamp ordering, test counter reset detection | Fix compression bugs, align timestamps properly, improve counter reset logic |\n| Storage space usage grows faster than expected | Poor compression ratios, retention policy not working, high cardinality explosion | Analyze compression effectiveness, verify retention execution, audit label cardinality | Optimize compression parameters, fix retention policies, implement cardinality controls |\n| Data corruption detected in stored chunks | Concurrent write bugs, filesystem issues, incomplete writes | Enable data integrity checks, review concurrent access patterns, check filesystem health | Fix concurrent write bugs, repair filesystem issues, add write atomicity |\n| Recovery from WAL fails after crashes | WAL corruption, incomplete transaction records, ordering violations | Examine WAL contents, verify transaction boundaries, check record ordering | Implement WAL validation, add transaction atomicity, improve ordering guarantees |\n\n#### Query Engine Symptoms\n\nQuery problems can be particularly frustrating because they often work correctly on small datasets but fail at scale.\n\n| Symptom | Likely Root Causes | Investigation Steps | Resolution Actions |\n|---------|-------------------|-------------------|------------------- |\n| Queries timeout frequently | Inefficient query plans, high cardinality selectors, resource limits too low | Analyze query execution plans, check selector cardinality, monitor resource usage | Optimize query execution, add cardinality limits, increase resource limits |\n| PromQL parsing errors for valid expressions | Parser bugs, unsupported features, operator precedence issues | Test expressions in isolation, check parser grammar, verify operator precedence | Fix parser bugs, implement missing features, correct precedence rules |\n| Aggregation functions return wrong results | Grouping logic errors, timestamp alignment issues, sample interpolation bugs | Test with known datasets, verify grouping behavior, check timestamp handling | Fix grouping algorithms, align timestamps consistently, correct interpolation logic |\n| Range queries miss data points | Time window calculations wrong, staleness threshold too strict, index selection issues | Verify time window boundaries, check staleness detection, review index selection logic | Fix time window calculations, adjust staleness thresholds, optimize index selection |\n| Memory usage spikes during large queries | Result set too large, inefficient data structures, memory leaks in query engine | Monitor query result sizes, profile memory allocations, check for memory leaks | Add result size limits, optimize data structures, fix memory leaks |\n\n#### Performance and Scalability Symptoms\n\nThese symptoms typically appear under load and indicate architectural limitations or resource bottlenecks.\n\n| Symptom | Likely Root Causes | Investigation Steps | Resolution Actions |\n|---------|-------------------|-------------------|------------------- |\n| Scraping falls behind target intervals | Too many targets per scraper, slow target responses, processing bottlenecks | Monitor scraping queue depth, measure target response times, check processing latency | Add more scrapers, optimize target endpoints, parallelize processing |\n| Query latency increases with data retention period | Index inefficiency, poor data locality, excessive disk I/O | Analyze query execution times by time range, check index hit rates, monitor disk I/O | Optimize index structures, improve data locality, add data tiering |\n| System cannot handle target cardinality growth | Memory exhaustion from indexes, storage I/O limits, query complexity explosion | Monitor memory usage by component, check storage I/O patterns, analyze query complexity | Implement cardinality limits, add horizontal scaling, optimize query complexity |\n| Background operations impact foreground performance | WAL flushing blocks writes, compaction affects reads, GC pauses affect queries | Monitor background operation timing, check resource contention, measure GC impact | Optimize background scheduling, reduce resource contention, tune GC parameters |\n| Resource usage is uneven across system components | Load imbalance, inefficient resource allocation, component bottlenecks | Analyze resource usage by component, check load distribution, identify bottlenecks | Rebalance load, optimize resource allocation, scale bottlenecked components |\n\n> **Key Debugging Insight**\n> \n> The most effective debugging approach for metrics systems is to maintain end-to-end observability of the system itself. Every component should emit metrics about its own behavior, creating a \"metrics system monitoring itself\" capability. This recursive observability is essential because metrics system failures often have cascading effects that obscure the original root cause.\n\n### Debugging Tools and Techniques\n\n**Mental Model: The Medical Diagnostic Toolkit**\n\nThink of debugging tools like medical diagnostic equipment. Just as doctors use different instruments for different symptoms (stethoscope for heart issues, X-ray for bone problems, blood tests for infections), metrics system debugging requires specialized tools for different types of problems. Some tools provide continuous monitoring (like vital sign monitors), others require active investigation (like MRI scans), and some are only used in emergency situations (like defibrillators).\n\nThe key principle is using the right tool for the type of problem you're investigating, starting with non-invasive continuous monitoring and escalating to more intensive diagnostic techniques only when necessary.\n\n#### Logging Infrastructure\n\nEffective logging is the foundation of debugging any distributed system, but metrics systems have specific logging requirements due to their high-throughput, time-sensitive nature.\n\n**Structured Logging Configuration**\n\n| Component | Log Level | Key Fields | Sampling Strategy | Retention Period |\n|-----------|-----------|------------|------------------|------------------|\n| `ScrapeEngine` | INFO for successes, WARN for failures | target_url, scrape_duration, sample_count, error_message | 100% for failures, 1% for successes | 7 days |\n| `StorageEngine` | DEBUG for writes, ERROR for corruption | series_count, sample_count, compression_ratio, wal_sync_duration | 0.1% for writes, 100% for errors | 3 days |\n| `QueryEngine` | INFO for slow queries, DEBUG for execution | query_text, execution_time, series_selected, result_size | 100% for >1s queries, 1% for fast queries | 1 day |\n| `SystemCoordinator` | INFO for lifecycle, ERROR for failures | component_name, operation, duration, resource_usage | 100% for all events | 30 days |\n\n**Contextual Logging Best Practices**\n\nEvery log entry should include sufficient context to understand the operation being performed. For metrics systems, this context typically includes temporal information (timestamps, time ranges), dimensional information (metric names, label sets), and operational information (component state, resource usage).\n\n```\n// Example of well-structured log entries:\n{\n  \"timestamp\": \"2024-01-15T10:30:45.123Z\",\n  \"level\": \"ERROR\",\n  \"component\": \"ScrapeEngine\",\n  \"operation\": \"scrapeTarget\",\n  \"target_url\": \"http://api-server:8080/metrics\",\n  \"scrape_interval\": \"15s\",\n  \"attempt_number\": 3,\n  \"error\": \"context deadline exceeded\",\n  \"consecutive_failures\": 5,\n  \"target_labels\": {\"job\": \"api-server\", \"instance\": \"api-server:8080\"},\n  \"trace_id\": \"abc123def456\"\n}\n\n{\n  \"timestamp\": \"2024-01-15T10:30:45.456Z\",\n  \"level\": \"WARN\",\n  \"component\": \"StorageEngine\",\n  \"operation\": \"compressChunk\",\n  \"series_id\": 12345,\n  \"chunk_samples\": 240,\n  \"compression_ratio\": 0.85,\n  \"warning\": \"compression ratio below threshold\",\n  \"metric_name\": \"http_requests_total\",\n  \"label_hash\": \"def456abc123\"\n}\n```\n\n#### System Metrics and Observability\n\nThe metrics collection system must monitor itself comprehensively. This self-monitoring provides real-time visibility into system health and performance characteristics.\n\n**Core System Metrics**\n\n| Metric Category | Metric Name | Labels | Description | Alert Threshold |\n|----------------|-------------|--------|-------------|----------------|\n| Scraping Health | `scrape_targets_up` | job, instance | Number of healthy targets per job | < 90% of targets |\n| Scraping Performance | `scrape_duration_seconds` | job, instance, quantile | Scrape operation duration distribution | p99 > scrape_timeout |\n| Storage Throughput | `samples_ingested_total` | component | Total samples successfully stored | Rate decreasing |\n| Storage Errors | `storage_errors_total` | component, error_type | Storage operation failures by type | > 0.1% error rate |\n| Query Performance | `query_duration_seconds` | query_type, quantile | Query execution time distribution | p95 > 10 seconds |\n| Query Load | `concurrent_queries` | none | Number of actively executing queries | > max_concurrent_queries |\n| Resource Usage | `memory_usage_bytes` | component | Memory usage by system component | > 80% of limit |\n| Resource Usage | `goroutine_count` | component | Active goroutines by component | Continuously increasing |\n\n**Component Health Indicators**\n\nEach system component should provide standardized health indicators that can be monitored independently and in aggregate.\n\n| Health Indicator | Measurement Method | Healthy Range | Warning Range | Critical Range |\n|------------------|-------------------|---------------|---------------|---------------|\n| Scrape Success Rate | Successful scrapes / Total scrape attempts | > 95% | 90-95% | < 90% |\n| Storage Write Success Rate | Successful writes / Total write attempts | > 99% | 95-99% | < 95% |\n| Query Success Rate | Successful queries / Total queries | > 99% | 95-99% | < 95% |\n| WAL Sync Latency | Time to fsync WAL entries | < 10ms | 10-50ms | > 50ms |\n| Index Lookup Latency | Time to resolve label selectors | < 1ms | 1-10ms | > 10ms |\n| Compression Efficiency | Compressed size / Raw size | < 0.2 | 0.2-0.5 | > 0.5 |\n\n#### Interactive Debugging Tools\n\nFor active investigation of problems, several interactive tools provide detailed system inspection capabilities.\n\n**Component State Inspection**\n\n| Tool | Command Format | Information Provided | Use Cases |\n|------|----------------|---------------------|-----------|\n| Target Health Inspector | `GET /debug/targets` | Current health status, last scrape results, failure reasons | Diagnosing scraping issues |\n| Series Cardinality Inspector | `GET /debug/cardinality?metric=<name>` | Label combination counts, high-cardinality series | Investigating memory usage |\n| Storage Consistency Checker | `GET /debug/storage/check` | Index-data consistency, corruption detection | Validating storage integrity |\n| Query Execution Planner | `GET /debug/query/explain?query=<promql>` | Execution plan, estimated resource usage | Optimizing slow queries |\n| WAL Content Inspector | `GET /debug/wal/entries?limit=<n>` | Recent WAL entries, transaction boundaries | Debugging storage issues |\n\n**Profiling and Performance Analysis**\n\n| Profiling Tool | Activation Method | Data Collected | Analysis Focus |\n|----------------|-------------------|----------------|----------------|\n| CPU Profiler | `GET /debug/pprof/profile` | CPU usage by function | Hot paths, algorithmic efficiency |\n| Memory Profiler | `GET /debug/pprof/heap` | Memory allocations by location | Memory leaks, allocation patterns |\n| Goroutine Profiler | `GET /debug/pprof/goroutine` | Goroutine stacks and states | Deadlocks, resource contention |\n| Mutex Profiler | `GET /debug/pprof/mutex` | Lock contention by location | Synchronization bottlenecks |\n| Block Profiler | `GET /debug/pprof/block` | Blocking operations by location | I/O bottlenecks, channel contention |\n\n#### Distributed Tracing Integration\n\nFor complex issues that span multiple components, distributed tracing provides end-to-end visibility into request processing.\n\n**Trace Instrumentation Points**\n\n| Operation | Trace Span Name | Key Attributes | Child Spans |\n|-----------|----------------|----------------|-------------|\n| HTTP Scrape | `scrape_target` | target_url, scrape_interval | http_request, parse_metrics, store_samples |\n| Sample Storage | `store_samples` | sample_count, series_count | wal_write, compress_chunk, update_index |\n| Query Execution | `execute_query` | query_type, time_range | parse_expression, select_series, aggregate_results |\n| Series Selection | `select_series` | selector_count, series_matched | index_lookup, label_matching |\n| Range Query | `range_query` | start_time, end_time, step | instant_query (multiple) |\n\n**Trace Analysis Techniques**\n\nWhen investigating distributed issues, trace analysis reveals timing relationships and failure propagation patterns that are invisible in individual component logs.\n\nCritical timing relationships to analyze include scrape-to-storage latency (how long samples take to become queryable), query-to-result latency (end-to-end query performance), and error propagation delays (how long failures take to surface). Performance bottleneck identification focuses on the longest spans in the trace, resource contention points where spans block waiting for resources, and cascade failure patterns where one component failure triggers failures in dependent components.\n\n> **Debugging Insight: The Observer Effect**\n> \n> Remember that debugging tools themselves consume system resources and can change system behavior. Heavy logging, frequent profiling, and detailed tracing all introduce overhead that may mask or alter the problems you're trying to investigate. Use sampling and conditional instrumentation to minimize observer effects during production debugging.\n\n### Implementation Pitfalls\n\n**Mental Model: The Software Engineering Minefield**\n\nThink of common implementation pitfalls like a minefield that every developer must navigate. Experienced engineers know where the mines are buried based on previous painful explosions. By documenting these pitfalls explicitly, we create a map that helps new developers avoid stepping on the same mines that have injured others.\n\nThe key insight is that pitfalls in metrics systems often appear as \"working\" code that fails under specific conditions like high cardinality, time zone changes, or concurrent access patterns. These delayed failures make pitfalls particularly dangerous because they pass initial testing but create production incidents.\n\n#### Metrics Data Model Pitfalls\n\nThese pitfalls relate to the fundamental data structures and semantic behaviors that form the foundation of the metrics system.\n\n⚠️ **Pitfall: Counter Reset Handling**\n\nMany developers implement counters as simple incrementing values without considering that counter resets (when a process restarts) require special handling for rate calculations to remain accurate.\n\n**Why This Fails:** When a counter resets to zero, naive rate calculations produce large negative values because `current_value - previous_value` becomes negative. This creates obviously wrong metrics and breaks downstream alerting and dashboards.\n\n**Detection:** Monitor for negative rate values in counter-based metrics. Implement validation that rejects negative rates from counter calculations.\n\n**Correct Implementation:** Track counter resets by detecting when the current value is less than the previous value, and handle resets by either skipping the rate calculation for that interval or estimating the true rate by assuming the counter accumulated `current_value` since the reset.\n\n⚠️ **Pitfall: High Cardinality Label Design**\n\nDevelopers often add labels with high cardinality (user IDs, request IDs, timestamps) without understanding the exponential memory impact of label combinations.\n\n**Why This Fails:** Each unique combination of metric name and label values creates a separate time series. Labels with thousands of values create millions of time series that exhaust system memory. For example, a metric with labels `{user_id, endpoint, status}` where user_id has 10,000 values, endpoint has 50 values, and status has 5 values creates up to 2.5 million time series.\n\n**Detection:** Monitor total series count and memory usage. Implement cardinality reporting that shows series count per metric name and per label combination.\n\n**Correct Implementation:** Use high-cardinality information in log entries rather than metric labels. Design label schemas with bounded cardinality - prefer categorical labels like `{status=\"success|failure\", tier=\"premium|standard\"}` over unbounded labels like `{user_id=\"12345\", request_id=\"abc-def\"}`.\n\n⚠️ **Pitfall: Histogram Bucket Boundaries**\n\nDevelopers often choose histogram bucket boundaries arbitrarily or try to change them after the histogram has been created and is collecting data.\n\n**Why This Fails:** Histogram bucket boundaries must remain consistent throughout the lifetime of the metric. Changing boundaries makes historical data incomparable and breaks percentile calculations. Poor bucket boundary choices (too few buckets, wrong ranges) provide insufficient resolution for meaningful percentile analysis.\n\n**Detection:** Monitor histogram bucket distribution - if most values fall into only one or two buckets, the boundaries are poorly chosen. Check for bucket boundary inconsistencies in historical data.\n\n**Correct Implementation:** Choose histogram buckets based on the expected distribution of values you're measuring. Use exponential bucket spacing (1ms, 2ms, 5ms, 10ms, 25ms, 50ms, 100ms...) for latency measurements. Once chosen, never modify bucket boundaries - create a new histogram metric instead.\n\n#### Scraping Engine Pitfalls\n\nThese pitfalls involve HTTP operations, timing, and concurrency patterns specific to pull-based metrics collection.\n\n⚠️ **Pitfall: Scrape Timeout Implementation**\n\nMany developers implement scrape timeouts using `time.After()` or similar mechanisms without properly canceling the underlying HTTP request, leading to resource leaks.\n\n**Why This Fails:** Even when the scrape operation times out from the coordinator's perspective, the underlying HTTP request continues consuming connection pool resources, goroutines, and network bandwidth. Over time, these leaked resources accumulate and eventually exhaust system capacity.\n\n**Detection:** Monitor goroutine count and HTTP connection pool usage. Look for continuously increasing resource usage even when scrape timeouts are occurring.\n\n**Correct Implementation:** Use context-based cancellation with `context.WithTimeout()` and pass the context to `http.Request.WithContext()`. This ensures that timing out the scrape operation also cancels the underlying HTTP request and releases all associated resources.\n\n⚠️ **Pitfall: Concurrent Scraping Without Rate Limiting**\n\nDevelopers often implement concurrent scraping by launching goroutines for each target without considering the aggregate load this places on target services.\n\n**Why This Fails:** Launching hundreds of concurrent HTTP requests can overwhelm target services, creating cascading failures. Target services may implement their own rate limiting or simply crash under the load, making metrics collection counterproductively impact the systems being monitored.\n\n**Detection:** Monitor target response times and error rates. Look for correlation between scraper concurrency levels and target service health.\n\n**Correct Implementation:** Implement scraper worker pools with bounded concurrency. Use semaphores or buffered channels to limit the maximum number of concurrent scrape operations. Consider implementing adaptive rate limiting that backs off when target services show signs of stress.\n\n⚠️ **Pitfall: Service Discovery Race Conditions**\n\nDevelopers often implement service discovery updates by directly modifying the target list without considering concurrent access from scraping goroutines.\n\n**Why This Fails:** Concurrent modification of the target list during scraping operations creates race conditions that can cause panics, data corruption, or targets being scraped multiple times simultaneously.\n\n**Detection:** Run with the Go race detector enabled (`go run -race`). Monitor for panics or unexpected behavior during service discovery updates.\n\n**Correct Implementation:** Use copy-on-write semantics for target list updates. Create a new target list and atomically replace the reference, allowing in-flight scrape operations to complete with the old target list before switching to the new one.\n\n#### Storage Engine Pitfalls\n\nStorage engine pitfalls often involve data corruption, concurrency issues, and compression edge cases that only surface under specific conditions.\n\n⚠️ **Pitfall: WAL Corruption During Concurrent Writes**\n\nDevelopers often implement write-ahead logging with multiple goroutines writing directly to the WAL file without proper synchronization.\n\n**Why This Fails:** Multiple concurrent writers can interleave bytes from different log entries, creating corrupted WAL entries that cannot be parsed during recovery. This makes the entire WAL unusable and causes data loss during crash recovery.\n\n**Detection:** Implement WAL validation during recovery that checks entry boundaries and checksums. Monitor for WAL recovery failures after system restarts.\n\n**Correct Implementation:** Serialize all WAL writes through a single goroutine using a channel-based queue, or implement file-level locking to ensure exclusive access during write operations. Always fsync after writing complete log entries to ensure durability.\n\n⚠️ **Pitfall: Gorilla Compression Edge Cases**\n\nDevelopers often implement Gorilla compression by following the basic algorithm without handling edge cases like irregular timestamp intervals, floating point special values, or compression ratio degradation.\n\n**Why This Fails:** The Gorilla compression algorithm assumes relatively regular timestamp intervals and normal floating point values. Irregular timestamps reduce compression effectiveness, while special values like NaN or infinity can break the XOR-based value compression. When compression ratios degrade, the system uses much more memory than expected.\n\n**Detection:** Monitor compression ratios per chunk and overall system memory usage. Implement validation for special floating point values in incoming samples.\n\n**Correct Implementation:** Handle irregular timestamps by falling back to absolute timestamp encoding when delta-of-delta compression becomes ineffective. Validate floating point values and reject or special-case NaN and infinity. Monitor compression ratios and implement fallback storage for chunks that don't compress well.\n\n⚠️ **Pitfall: Index Consistency Under Concurrent Access**\n\nDevelopers often implement inverted indexes with concurrent read and write access without proper synchronization, leading to index corruption or inconsistent query results.\n\n**Why This Fails:** Concurrent modification of index data structures can leave them in inconsistent states where some series are visible through some label combinations but not others, or where index entries point to non-existent series data.\n\n**Detection:** Implement index consistency checks that verify all index entries point to valid series data and that series data can be found through expected index paths. Run these checks periodically and after any index corruption reports.\n\n**Correct Implementation:** Use readers-writer mutexes to allow concurrent reads while serializing writes. Implement index updates as atomic operations that either complete entirely or leave the index unchanged. Consider using copy-on-write strategies for large index updates.\n\n#### Query Engine Pitfalls\n\nQuery engine pitfalls often involve mathematical edge cases, performance characteristics that don't scale, and semantic misunderstandings of PromQL operations.\n\n⚠️ **Pitfall: Rate Calculation Across Counter Resets**\n\nDevelopers often implement rate calculations using simple arithmetic without detecting and handling counter resets properly.\n\n**Why This Fails:** Counter resets create negative rate values that are mathematically incorrect and break visualizations and alerting. The `rate()` function should calculate the true rate of increase, which requires detecting resets and handling them appropriately.\n\n**Detection:** Monitor for negative values in rate calculation results. Implement validation that rejects impossible rate values based on the known characteristics of the underlying counter.\n\n**Correct Implementation:** Detect counter resets by checking if the current counter value is less than the previous value. When a reset is detected, calculate the rate as `current_value / time_interval`, assuming the counter accumulated its current value since the reset. For partial intervals after resets, either skip the rate calculation or pro-rate based on the partial interval.\n\n⚠️ **Pitfall: Query Resource Exhaustion**\n\nDevelopers often implement query execution without considering resource limits, allowing queries over high-cardinality metrics to exhaust system memory.\n\n**Why This Fails:** Queries that select thousands of time series can consume gigabytes of memory for intermediate results and final output. Without resource limits, a single poorly-written query can crash the entire metrics system.\n\n**Detection:** Monitor memory usage during query execution and track query result sizes. Implement timeouts and memory limits for query operations.\n\n**Correct Implementation:** Implement query resource limits including maximum series per query, maximum query execution time, and maximum result set size. Use streaming processing where possible to avoid loading entire result sets into memory. Provide query cost estimation to help users understand the resource impact of their queries.\n\n⚠️ **Pitfall: Aggregation Function Semantic Errors**\n\nDevelopers often implement aggregation functions like `avg()` or `sum()` without properly handling missing data points, different timestamp alignments, or grouping semantics.\n\n**Why This Fails:** Different time series may have samples at different timestamps, and aggregation functions must handle these alignment issues correctly. Simply averaging all available values without considering temporal alignment produces mathematically meaningless results.\n\n**Detection:** Test aggregation functions with time series that have different sampling intervals and missing data points. Verify that aggregation results match hand-calculated expected values.\n\n**Correct Implementation:** Implement timestamp alignment by interpolating or extrapolating values to common evaluation timestamps before applying aggregation functions. Handle missing data appropriately - some functions should skip missing values while others should treat them as zero. Clearly document the temporal semantics of each aggregation function.\n\n### Implementation Guidance\n\nThis section provides practical tools and code structures for implementing effective debugging capabilities throughout the metrics collection system.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Structured Logging | `log/slog` with JSON formatting | `go.uber.org/zap` with sampling and async writing |\n| HTTP Debugging Endpoints | `net/http/pprof` + custom handlers | `go.uber.org/fx` + `github.com/gorilla/mux` for organized routing |\n| Metrics Self-Monitoring | Manual metrics with `expvar` | `github.com/prometheus/client_golang` for full compatibility |\n| Distributed Tracing | Manual trace context propagation | `go.opentelemetry.io/otel` with Jaeger or Zipkin |\n| Health Checking | Simple HTTP endpoint with JSON status | Full health check framework with dependencies |\n| Configuration Validation | Manual validation with error accumulation | `github.com/go-playground/validator/v10` for declarative rules |\n\n#### Recommended File Structure\n\n```\ninternal/\n  debug/\n    health.go              ← component health checking\n    profiling.go           ← performance profiling endpoints  \n    inspection.go          ← system state inspection tools\n    tracing.go             ← distributed tracing utilities\n    debug_test.go          ← debugging tool tests\n  monitoring/\n    metrics.go             ← self-monitoring metrics definitions\n    alerts.go              ← alerting rule definitions\n    dashboards.go          ← dashboard configuration export\n  validation/\n    config_validator.go    ← configuration validation\n    data_validator.go      ← data consistency checking\n    performance_validator.go ← performance requirement validation\ncmd/\n  debug-tool/\n    main.go               ← standalone debugging CLI tool\n  health-check/\n    main.go               ← health checking utility\n```\n\n#### Infrastructure Starter Code\n\n**Complete Structured Logger Implementation**\n\n```go\npackage debug\n\nimport (\n    \"context\"\n    \"log/slog\"\n    \"os\"\n    \"time\"\n)\n\n// Logger provides structured logging with different levels and context support\ntype Logger struct {\n    logger *slog.Logger\n    component string\n}\n\n// NewLogger creates a logger for a specific component with JSON formatting\nfunc NewLogger(component string) *Logger {\n    handler := slog.NewJSONHandler(os.Stdout, &slog.HandlerOptions{\n        Level: slog.LevelDebug,\n        AddSource: true,\n    })\n    \n    logger := slog.New(handler).With(\n        \"component\", component,\n        \"pid\", os.Getpid(),\n    )\n    \n    return &Logger{\n        logger: logger,\n        component: component,\n    }\n}\n\n// Info logs informational messages with structured fields\nfunc (l *Logger) Info(msg string, fields ...any) {\n    l.logger.Info(msg, fields...)\n}\n\n// Error logs error messages with structured fields\nfunc (l *Logger) Error(msg string, err error, fields ...any) {\n    allFields := append(fields, \"error\", err.Error())\n    l.logger.Error(msg, allFields...)\n}\n\n// Debug logs debug messages with structured fields (only in debug builds)\nfunc (l *Logger) Debug(msg string, fields ...any) {\n    l.logger.Debug(msg, fields...)\n}\n\n// WithContext returns a logger that includes trace information from context\nfunc (l *Logger) WithContext(ctx context.Context) *Logger {\n    // TODO: Extract trace ID from context if tracing is enabled\n    return l\n}\n\n// WithFields returns a logger with additional fields included in all log entries\nfunc (l *Logger) WithFields(fields ...any) *Logger {\n    return &Logger{\n        logger: l.logger.With(fields...),\n        component: l.component,\n    }\n}\n```\n\n**Complete Health Checking System**\n\n```go\npackage debug\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n)\n\n// HealthStatus represents the health state of a component\ntype HealthStatus string\n\nconst (\n    HealthUp       HealthStatus = \"up\"\n    HealthDown     HealthStatus = \"down\" \n    HealthDegraded HealthStatus = \"degraded\"\n)\n\n// ComponentHealth represents the health of a single system component\ntype ComponentHealth struct {\n    Component string       `json:\"component\"`\n    Status    HealthStatus `json:\"status\"`\n    Message   string       `json:\"message,omitempty\"`\n    Timestamp time.Time    `json:\"timestamp\"`\n    Metrics   map[string]interface{} `json:\"metrics,omitempty\"`\n}\n\n// SystemHealth aggregates health information from all system components\ntype SystemHealth struct {\n    OverallStatus HealthStatus                   `json:\"overall_status\"`\n    Components    map[string]*ComponentHealth    `json:\"components\"`\n    Timestamp     time.Time                      `json:\"timestamp\"`\n    Uptime        time.Duration                  `json:\"uptime\"`\n}\n\n// HealthChecker manages health checking for all system components\ntype HealthChecker struct {\n    components map[string]*ComponentHealth\n    startTime  time.Time\n    mu         sync.RWMutex\n}\n\n// NewHealthChecker creates a new health checker instance\nfunc NewHealthChecker() *HealthChecker {\n    return &HealthChecker{\n        components: make(map[string]*ComponentHealth),\n        startTime:  time.Now(),\n    }\n}\n\n// RegisterComponent registers a component for health checking\nfunc (hc *HealthChecker) RegisterComponent(name string) {\n    hc.mu.Lock()\n    defer hc.mu.Unlock()\n    \n    hc.components[name] = &ComponentHealth{\n        Component: name,\n        Status:    HealthDown,\n        Message:   \"Component not yet initialized\",\n        Timestamp: time.Now(),\n    }\n}\n\n// UpdateComponentHealth updates the health status of a component\nfunc (hc *HealthChecker) UpdateComponentHealth(name string, status HealthStatus, message string, metrics map[string]interface{}) {\n    hc.mu.Lock()\n    defer hc.mu.Unlock()\n    \n    if component, exists := hc.components[name]; exists {\n        component.Status = status\n        component.Message = message\n        component.Timestamp = time.Now()\n        component.Metrics = metrics\n    }\n}\n\n// GetSystemHealth returns the overall system health status\nfunc (hc *HealthChecker) GetSystemHealth() *SystemHealth {\n    hc.mu.RLock()\n    defer hc.mu.RUnlock()\n    \n    overallStatus := HealthUp\n    componentCopy := make(map[string]*ComponentHealth)\n    \n    for name, component := range hc.components {\n        // Create a copy to avoid race conditions\n        componentCopy[name] = &ComponentHealth{\n            Component: component.Component,\n            Status:    component.Status,\n            Message:   component.Message,\n            Timestamp: component.Timestamp,\n            Metrics:   component.Metrics,\n        }\n        \n        // Determine overall status\n        if component.Status == HealthDown {\n            overallStatus = HealthDown\n        } else if component.Status == HealthDegraded && overallStatus == HealthUp {\n            overallStatus = HealthDegraded\n        }\n    }\n    \n    return &SystemHealth{\n        OverallStatus: overallStatus,\n        Components:    componentCopy,\n        Timestamp:     time.Now(),\n        Uptime:        time.Since(hc.startTime),\n    }\n}\n\n// ServeHTTP implements http.Handler to expose health information via HTTP\nfunc (hc *HealthChecker) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    health := hc.GetSystemHealth()\n    \n    w.Header().Set(\"Content-Type\", \"application/json\")\n    \n    // Set HTTP status code based on overall health\n    switch health.OverallStatus {\n    case HealthUp:\n        w.WriteHeader(http.StatusOK)\n    case HealthDegraded:\n        w.WriteHeader(http.StatusOK) // Still serving requests\n    case HealthDown:\n        w.WriteHeader(http.StatusServiceUnavailable)\n    }\n    \n    json.NewEncoder(w).Encode(health)\n}\n```\n\n#### Core Logic Skeleton Code\n\n**Component State Inspector**\n\n```go\n// ComponentStateInspector provides detailed inspection of internal component state\ntype ComponentStateInspector struct {\n    scrapeEngine  *ScrapeEngine\n    storageEngine *StorageEngine\n    queryEngine   *QueryEngine\n    logger        *Logger\n}\n\n// NewComponentStateInspector creates an inspector with access to all system components\nfunc NewComponentStateInspector(scrapeEngine *ScrapeEngine, storageEngine *StorageEngine, queryEngine *QueryEngine, logger *Logger) *ComponentStateInspector {\n    return &ComponentStateInspector{\n        scrapeEngine:  scrapeEngine,\n        storageEngine: storageEngine,\n        queryEngine:   queryEngine,\n        logger:        logger,\n    }\n}\n\n// InspectScrapeTargets returns detailed information about all scrape targets\nfunc (csi *ComponentStateInspector) InspectScrapeTargets() map[string]interface{} {\n    // TODO 1: Lock scrape engine to prevent concurrent modification during inspection\n    // TODO 2: Iterate through all configured targets and collect their current state\n    // TODO 3: For each target, include: URL, last scrape time, consecutive failures, health status\n    // TODO 4: Calculate aggregate statistics: total targets, healthy targets, failure rate\n    // TODO 5: Return structured data suitable for JSON serialization\n    // Hint: Use read locks to allow continued operation while inspecting\n    return nil\n}\n\n// InspectStorageState returns information about storage engine internal state  \nfunc (csi *ComponentStateInspector) InspectStorageState() map[string]interface{} {\n    // TODO 1: Acquire read lock on storage engine to ensure consistent state view\n    // TODO 2: Count total number of time series and chunks across all indexes\n    // TODO 3: Calculate storage efficiency metrics: compression ratio, samples per chunk\n    // TODO 4: Inspect WAL state: current segment, pending entries, last sync time\n    // TODO 5: Check index consistency: verify all series have corresponding chunks\n    // TODO 6: Return comprehensive storage health and efficiency data\n    return nil\n}\n\n// InspectQueryPerformance returns query engine performance and resource usage data\nfunc (csi *ComponentStateInspector) InspectQueryPerformance() map[string]interface{} {\n    // TODO 1: Collect query execution statistics from query engine\n    // TODO 2: Calculate percentiles for query duration and result set sizes  \n    // TODO 3: Identify slow queries and high-cardinality selectors\n    // TODO 4: Report resource usage: memory per query, concurrent query count\n    // TODO 5: Return performance data suitable for optimization analysis\n    return nil\n}\n```\n\n**Distributed Tracing Integration**\n\n```go\n// TracingCoordinator manages distributed tracing across all system components\ntype TracingCoordinator struct {\n    enabled    bool\n    sampler    Sampler\n    tracer     Tracer\n    logger     *Logger\n}\n\n// NewTracingCoordinator creates a tracing coordinator with sampling configuration\nfunc NewTracingCoordinator(enabled bool, samplingRate float64, logger *Logger) *TracingCoordinator {\n    // TODO 1: Initialize tracer with appropriate backend (Jaeger, Zipkin, or no-op)\n    // TODO 2: Configure sampler with specified sampling rate and adaptive algorithms\n    // TODO 3: Set up trace context propagation for HTTP requests and internal operations\n    // TODO 4: Return configured coordinator ready for use across system components\n    return nil\n}\n\n// StartSpan creates a new tracing span for an operation with specified attributes\nfunc (tc *TracingCoordinator) StartSpan(ctx context.Context, operationName string, attributes map[string]interface{}) (context.Context, Span) {\n    // TODO 1: Check if tracing is enabled and sampling decision allows this trace\n    // TODO 2: Extract parent span context from incoming context if available\n    // TODO 3: Create new span with operation name and configured attributes\n    // TODO 4: Add standard attributes: component name, operation timestamp, correlation ID\n    // TODO 5: Return new context containing span and span object for lifecycle management\n    return ctx, nil\n}\n\n// InjectTraceHeaders adds tracing information to HTTP request headers for propagation\nfunc (tc *TracingCoordinator) InjectTraceHeaders(ctx context.Context, headers http.Header) {\n    // TODO 1: Extract active span from context if present\n    // TODO 2: Serialize span context into trace propagation headers (B3, Jaeger, etc.)\n    // TODO 3: Add headers to outgoing HTTP request for downstream trace correlation\n    // Hint: Handle case where no active span exists gracefully\n}\n\n// ExtractTraceContext extracts tracing information from incoming HTTP request headers\nfunc (tc *TracingCoordinator) ExtractTraceContext(headers http.Header) context.Context {\n    // TODO 1: Parse trace propagation headers from incoming HTTP request\n    // TODO 2: Reconstruct span context from header information\n    // TODO 3: Create new context containing extracted trace information\n    // TODO 4: Return context suitable for starting child spans\n    return context.Background()\n}\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 Debugging Verification: Metrics Data Model**\n\nAfter implementing the metrics data model with debugging support:\n\n1. **Run Component Tests**: Execute `go test ./internal/metrics/... -v` to verify all metric types handle edge cases correctly\n2. **Verify Cardinality Tracking**: Create metrics with high-cardinality labels and confirm the system detects and reports the cardinality explosion\n3. **Test Counter Reset Handling**: Simulate counter resets and verify that subsequent rate calculations handle them appropriately\n4. **Check Label Validation**: Attempt to create metrics with invalid label names and values, confirming that validation prevents high-cardinality labels\n\n**Expected Behavior**: Cardinality tracking reports series counts accurately, counter resets are detected and logged, label validation prevents problematic metric creation, and all metric types expose their internal state for debugging.\n\n**Milestone 2 Debugging Verification: Scrape Engine**\n\nAfter implementing the scrape engine with debugging support:\n\n1. **Run Scraping Tests**: Execute `go test ./internal/scraping/... -v` with mock HTTP targets to verify scraping logic\n2. **Test Timeout Handling**: Configure short scrape timeouts and verify that timeouts cancel HTTP requests properly without leaking resources\n3. **Verify Target Health Tracking**: Start and stop mock target services and confirm that target health status updates correctly\n4. **Check Service Discovery**: Modify service discovery configuration and verify that target list updates without disrupting ongoing scrapes\n\n**Expected Behavior**: Scrape operations respect timeouts and cancel underlying resources, target health accurately reflects service availability, service discovery updates are applied safely, and detailed scrape metrics are available for debugging.\n\n**Milestone 3 Debugging Verification: Time Series Storage**\n\nAfter implementing the time series storage with debugging support:\n\n1. **Run Storage Tests**: Execute `go test ./internal/storage/... -v` to verify compression, indexing, and WAL functionality\n2. **Test WAL Recovery**: Kill the storage process during write operations and verify that WAL recovery restores consistent state\n3. **Verify Compression Effectiveness**: Store time series with different patterns and confirm that compression ratios meet expectations\n4. **Check Index Consistency**: Run index consistency checks and verify that all time series can be found through expected label selectors\n\n**Expected Behavior**: WAL recovery restores all committed data after crashes, compression ratios achieve expected efficiency, index consistency checks pass, and storage state can be inspected comprehensively.\n\n**Milestone 4 Debugging Verification: Query Engine**\n\nAfter implementing the query engine with debugging support:\n\n1. **Run Query Tests**: Execute `go test ./internal/query/... -v` to verify PromQL parsing and execution\n2. **Test Resource Limits**: Execute queries that exceed configured limits and verify that they are rejected appropriately\n3. **Verify Aggregation Correctness**: Run aggregation queries with known expected results and confirm mathematical accuracy\n4. **Check Performance Monitoring**: Execute slow queries and verify that performance metrics accurately capture execution characteristics\n\n**Expected Behavior**: Query resource limits prevent system overload, aggregation functions produce mathematically correct results, query performance is monitored and reported accurately, and query execution plans can be inspected for optimization.\n\n#### Language-Specific Debugging Hints\n\n**Go Race Detection and Concurrency Debugging**\n\n- Always run tests with `-race` flag during development: `go test -race ./...`\n- Use `go run -race` when testing manually to catch race conditions early\n- Monitor goroutine count with `runtime.NumGoroutine()` to detect goroutine leaks\n- Use `runtime.GC(); runtime.GC()` to force garbage collection before measuring memory usage\n- Use buffered channels with explicit capacity to avoid deadlocks: `make(chan Sample, 100)`\n\n**Memory Profiling and Optimization**\n\n- Use `go tool pprof http://localhost:8080/debug/pprof/heap` for interactive memory analysis\n- Check for memory leaks by comparing heap profiles over time: `go tool pprof -base profile1.pb.gz profile2.pb.gz`\n- Use `runtime/debug.SetGCPercent()` to tune garbage collection frequency based on workload\n- Implement object pooling with `sync.Pool` for frequently allocated objects like samples and labels\n- Use `unsafe.Sizeof()` to understand memory layout of critical data structures\n\n**HTTP Client Configuration for Reliability**\n\n- Set reasonable timeouts: `http.Client{Timeout: 30 * time.Second}`\n- Configure connection pooling: `http.Transport{MaxIdleConns: 100, MaxIdleConnsPerHost: 10}`\n- Use context cancellation: `req.WithContext(ctx)` for all HTTP requests\n- Implement exponential backoff for retries using `time.Sleep(time.Duration(attempt*attempt) * time.Second)`\n- Monitor connection pool usage with custom `http.Transport` metrics\n\n**Error Handling and Recovery Patterns**\n\n- Use typed errors for different failure modes: `type NetworkError struct { ... }`\n- Implement circuit breakers for external dependencies to prevent cascade failures\n- Log errors with full context including operation, input parameters, and system state\n- Use `defer` statements for cleanup that must happen regardless of success or failure\n- Implement graceful degradation where partial functionality is better than complete failure\n\n\n## Future Extensions and Scalability\n\n> **Milestone(s):** This section demonstrates how the Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4) architecture supports future enhancements without major redesigns.\n\nThink of our metrics collection system as a city's infrastructure. We've built the fundamental utilities - the power grid (`StorageEngine`), water system (`ScrapeEngine`), and communication network (`QueryEngine`). Now we need to plan for future growth: adding new neighborhoods (federation), emergency services (alerting), and express highways (query optimizations). The key architectural insight is that extensible systems are designed with **composition over inheritance** - new capabilities are added by combining existing components rather than rewriting them.\n\nThe extensibility of our metrics system relies on three core architectural principles that we established in earlier sections. First, our **component isolation** ensures that new features can be added without disrupting existing functionality - the `ScrapeEngine`, `StorageEngine`, and `QueryEngine` communicate through well-defined interfaces rather than tight coupling. Second, our **data model completeness** means that metric labels, timestamps, and metadata provide sufficient information for advanced features like rule evaluation and cross-instance queries. Third, our **pipeline architecture** with channels and coordinators creates natural extension points where new processing stages can be inserted.\n\n### Alerting System\n\nThe alerting system represents the first major extension that transforms our passive metrics collection into an active monitoring platform. Think of alerting as adding a security system to our infrastructure city - it continuously watches for specific conditions and triggers responses when thresholds are crossed.\n\n#### Rule Evaluation Engine\n\nThe **Rule Evaluation Engine** extends our existing `QueryEngine` by adding continuous evaluation of PromQL expressions against incoming time series data. Rather than building a separate system, we leverage the query parsing and execution infrastructure we already have.\n\n**AlertRule Data Structure:**\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `Name` | `string` | Unique identifier for the alert rule |\n| `Query` | `string` | PromQL expression to evaluate |\n| `Duration` | `time.Duration` | How long condition must be true before firing |\n| `Labels` | `Labels` | Additional labels attached to alert instances |\n| `Annotations` | `map[string]string` | Human-readable descriptions and runbook links |\n| `EvaluationInterval` | `time.Duration` | How frequently to evaluate the rule |\n| `State` | `AlertState` | Current state: pending, firing, or resolved |\n| `ActiveSince` | `time.Time` | When the alert first became active |\n| `ResolvedAt` | `*time.Time` | When the alert was resolved (nil if still active) |\n\nThe rule evaluation process follows a continuous assessment cycle:\n\n1. The `RuleEvaluator` maintains a schedule of all active alert rules with their next evaluation times\n2. At each evaluation interval, it executes the rule's PromQL query using our existing `QueryEngine.ExecuteInstantQuery` method\n3. For each time series returned by the query, it checks if the result value meets the alert condition\n4. If the condition is met, it starts tracking the duration - alerts only fire after being active for the specified `Duration`\n5. When an alert transitions from pending to firing, it generates an `AlertInstance` with the current timestamp and label set\n6. The evaluator continues checking resolved conditions - when the query returns no results or false values, it marks alerts as resolved\n\n> **Design Insight**: By reusing our `QueryEngine` for rule evaluation, we automatically inherit all the query optimization, caching, and error handling that we built for user queries. This demonstrates the power of composable architecture - new features leverage existing robust components.\n\n**RuleEvaluator Interface:**\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `AddRule` | `rule *AlertRule` | `error` | Registers a new alert rule for evaluation |\n| `RemoveRule` | `ruleName string` | `error` | Stops evaluating and removes an alert rule |\n| `EvaluateAll` | `ctx context.Context, evalTime time.Time` | `[]AlertInstance, error` | Evaluates all rules at the specified time |\n| `GetAlertState` | `ruleName string` | `AlertState, error` | Returns current state of a specific alert rule |\n| `ListActiveAlerts` | `labels LabelMatcher` | `[]AlertInstance, error` | Returns all currently firing alerts matching label filters |\n\n#### Notification Manager\n\nThe **Notification Manager** handles the delivery of alert notifications through multiple channels. Think of it as the emergency dispatch system - when the security system (rule evaluator) detects a problem, the dispatcher determines who to notify and how.\n\n**NotificationChannel Interface:**\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `Send` | `ctx context.Context, alert AlertInstance` | `error` | Delivers alert notification through this channel |\n| `Test` | `ctx context.Context` | `error` | Verifies channel configuration and connectivity |\n| `GetType` | | `string` | Returns channel type (email, slack, webhook, etc.) |\n| `IsHealthy` | | `bool` | Indicates if channel is currently operational |\n\nThe notification pipeline implements sophisticated routing and de-duplication:\n\n1. **Alert Grouping**: Multiple related alerts are combined into a single notification to prevent spam - alerts with identical label sets (excluding instance-specific labels) are grouped together\n2. **Rate Limiting**: Each notification channel has configurable rate limits to prevent overwhelming external systems during large-scale outages\n3. **Retry Logic**: Failed notifications are queued for retry with exponential backoff - critical alerts get more aggressive retry attempts\n4. **Escalation Chains**: After a specified time without acknowledgment, alerts can escalate to additional notification channels or contacts\n5. **Maintenance Windows**: Notifications can be suppressed during scheduled maintenance periods based on label matching\n\n> **Decision: Notification Architecture**\n> - **Context**: Alert notifications need to be reliable, fast, and handle various delivery failures\n> - **Options Considered**: \n>   - Direct synchronous delivery from rule evaluator\n>   - Asynchronous queue-based delivery with persistence\n>   - Hybrid approach with immediate delivery plus persistent retry queue\n> - **Decision**: Asynchronous queue-based delivery with WAL persistence\n> - **Rationale**: Rule evaluation must continue even if notification delivery fails. Persistent queue ensures alerts aren't lost during system restarts\n> - **Consequences**: Adds complexity but provides notification reliability and system resilience\n\n#### Alert State Management\n\nAlert state management tracks the lifecycle of alert instances from creation through resolution. This extends our existing `StorageEngine` with specialized alert state persistence.\n\n**AlertInstance Data Structure:**\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `RuleName` | `string` | Name of the alert rule that generated this instance |\n| `Labels` | `Labels` | Complete label set including rule labels and metric labels |\n| `State` | `AlertState` | Current state: pending, firing, resolved |\n| `Value` | `float64` | The metric value that triggered the alert |\n| `StartsAt` | `time.Time` | When the alert condition first became true |\n| `EndsAt` | `*time.Time` | When the alert condition resolved (nil if still active) |\n| `GeneratorURL` | `string` | URL to query that generated this alert |\n| `Fingerprint` | `uint64` | Hash of label set for efficient deduplication |\n\nAlert state transitions follow a strict finite state machine:\n\n| Current State | Event | Next State | Actions Taken |\n|---------------|-------|------------|---------------|\n| None | Query returns true | Pending | Record start time, begin duration tracking |\n| Pending | Duration exceeded | Firing | Generate notification, mark as active |\n| Pending | Query returns false | None | Clear tracking, no notification |\n| Firing | Query returns false | Resolved | Send resolution notification, record end time |\n| Resolved | Query returns true | Pending | Start new alert instance cycle |\n\n### Multi-Instance Federation\n\nFederation enables horizontal scaling by connecting multiple independent metrics collection instances into a coordinated cluster. Think of federation as connecting multiple city infrastructures into a metropolitan area - each city maintains its own services while sharing critical information across the region.\n\n#### Hierarchical Federation Model\n\nOur federation model follows a **hierarchical pull-based approach** that aligns with our existing scrape engine architecture. Rather than building complex consensus protocols, we extend the scraping concept to pull metrics from other Prometheus-compatible instances.\n\n**Federation Configuration:**\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `FederationConfig` | `struct` | Top-level federation configuration |\n| `UpstreamTargets` | `[]FederationTarget` | List of upstream instances to scrape from |\n| `MatchRules` | `[]FederationRule` | Rules for selecting which metrics to federate |\n| `ScrapInterval` | `time.Duration` | How often to scrape upstream instances |\n| `ExternalLabels` | `Labels` | Labels added to all metrics from this instance |\n\nThe federation process extends our existing target discovery and scraping:\n\n1. **Target Discovery**: Federation targets are configured as static targets in the `ScrapeEngine` with special federation endpoints (`/federate`)\n2. **Metric Selection**: Each federation target specifies `MatchRules` that determine which metrics to pull - this prevents recursive federation and controls data volume\n3. **Label Rewriting**: Federated metrics receive additional external labels that identify their source instance and prevent label conflicts\n4. **Conflict Resolution**: When multiple instances provide the same time series (same metric name and labels), the most recent timestamp wins\n5. **Topology Management**: The federation hierarchy is maintained through configuration - each instance knows its role (leaf, intermediate, root) and scrapes accordingly\n\n> **Design Insight**: By implementing federation as an extension of scraping rather than a separate mechanism, we reuse all the existing HTTP client code, retry logic, target health checking, and metrics parsing. This is a prime example of architectural composition.\n\n**FederationRule Structure:**\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `MatchMetrics` | `[]string` | Metric name patterns to include (supports regex) |\n| `MatchLabels` | `[]LabelMatcher` | Label conditions that must be satisfied |\n| `ExcludeMetrics` | `[]string` | Metric name patterns to exclude |\n| `SampleLimit` | `int` | Maximum samples per scrape to prevent overload |\n\n#### Cross-Instance Querying\n\nCross-instance querying allows PromQL queries to span multiple federation instances, providing a global view of metrics across the entire infrastructure. This extends our `QueryEngine` with **query federation capabilities**.\n\nThe federated query process works through query distribution and result merging:\n\n1. **Query Analysis**: The `QueryPlanner` analyzes incoming PromQL queries to determine which federated instances might contain relevant data\n2. **Query Distribution**: Queries are sent to relevant upstream instances in parallel using HTTP requests to their `/query` endpoints\n3. **Result Merging**: Partial results from multiple instances are combined using the same aggregation logic as our local `Aggregator`\n4. **Deduplication**: Time series with identical label sets from multiple sources are deduplicated based on external labels and timestamps\n5. **Error Handling**: Partial failures from some instances don't prevent returning results from available instances\n\n**QueryFederator Interface:**\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `ExecuteFederatedQuery` | `ctx context.Context, query string, evalTime time.Time` | `*QueryResult, error` | Executes query across federated instances |\n| `GetAvailableInstances` | `labels LabelMatcher` | `[]FederationTarget, error` | Returns instances that might contain matching data |\n| `MergeResults` | `results []QueryResult` | `*QueryResult, error` | Combines partial results from multiple instances |\n| `AddFederationTarget` | `target FederationTarget` | `error` | Registers new instance for federated queries |\n\n#### Global View Consistency\n\nMaintaining consistency across federated instances requires careful handling of clock skew, network partitions, and instance failures. Our approach prioritizes **availability over strict consistency** - we provide eventually consistent global views rather than strong consistency guarantees.\n\n**Clock Skew Handling**: Federation instances may have slightly different system clocks, causing timestamp misalignment. We address this through:\n- Configurable tolerance windows that accept samples within a reasonable time range (typically ±1 minute)\n- Timestamp normalization during federation that adjusts for known clock skew between instances\n- Warning alerts when clock skew exceeds acceptable thresholds\n\n**Network Partition Resilience**: When federation links fail, each instance continues operating independently:\n- Local queries continue working against locally stored data\n- Federation queries return partial results with warnings about unavailable instances  \n- Automatic reconnection attempts restore federation links when network connectivity returns\n- Backfill mechanisms can catch up on missed data after reconnection\n\n### Advanced Query Features\n\nAdvanced query features extend our basic PromQL implementation with performance optimizations, computed metrics, and sophisticated analytical capabilities that would typically be added after the core system proves itself in production.\n\n#### Recording Rules\n\n**Recording Rules** pre-compute expensive queries and store the results as new time series, dramatically improving dashboard and alert performance for complex aggregations. Think of recording rules as creating express highway routes through our query system - frequently used complex paths get dedicated infrastructure for faster transit.\n\nRecording rules extend our alert rule evaluation infrastructure by treating computed metrics as first-class time series:\n\n**RecordingRule Structure:**\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `Name` | `string` | Unique identifier for the recording rule |\n| `Query` | `string` | PromQL expression to evaluate and store |\n| `MetricName` | `string` | Name for the generated time series |\n| `Labels` | `Labels` | Additional labels attached to computed metrics |\n| `EvaluationInterval` | `time.Duration` | How often to evaluate and update the recording |\n| `LastEvaluation` | `time.Time` | Timestamp of most recent rule execution |\n| `EvaluationDuration` | `time.Duration` | How long the last evaluation took |\n| `SamplesProduced` | `int64` | Number of time series points generated |\n\nThe recording rule evaluation process follows a continuous computation cycle:\n\n1. **Rule Scheduling**: The `RuleEvaluator` schedules recording rules alongside alert rules, maintaining separate evaluation intervals for each\n2. **Query Execution**: At each interval, the rule's PromQL query executes using our standard `QueryEngine.ExecuteInstantQuery` method\n3. **Result Transformation**: Query results are converted into new time series with the specified `MetricName` and additional labels\n4. **Storage Integration**: Generated samples are fed back into our `StorageEngine` through the same `Append` interface used by the scrape engine\n5. **Metadata Management**: Recording rule metadata (evaluation time, sample count) is tracked for monitoring rule performance\n\n> **Architecture Insight**: Recording rules create a feedback loop where the query engine feeds computed results back into storage, which can then be queried again. This requires careful cycle detection to prevent infinite recursion in rule dependencies.\n\n**Common Recording Rule Patterns:**\n\n| Pattern | Example Query | Use Case |\n|---------|---------------|----------|\n| Rate Calculation | `rate(http_requests_total[5m])` | Pre-compute request rates for dashboards |\n| Quantile Aggregation | `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))` | Expensive percentile calculations |\n| Cross-Service Aggregation | `sum by (service) (rate(errors_total[1m])) / sum by (service) (rate(requests_total[1m]))` | Service-level error rates |\n| Resource Utilization | `avg by (cluster) ((cpu_usage / cpu_limit) * 100)` | Cluster-wide resource metrics |\n\n#### Query Optimization Engine\n\nThe **Query Optimization Engine** analyzes PromQL queries to identify performance improvements through query rewriting, caching, and execution plan optimization. This extends our `QueryEngine` with intelligent query planning capabilities.\n\nQuery optimization operates through multiple analysis phases:\n\n1. **Syntax Tree Analysis**: The query's AST is analyzed to identify expensive operations like large time range scans or high-cardinality aggregations\n2. **Series Cardinality Estimation**: Before execution, the optimizer estimates how many time series will be involved based on label selector specificity\n3. **Execution Plan Generation**: Multiple query execution strategies are generated and their estimated costs are compared\n4. **Cache Utilization**: The optimizer checks if partial results for subexpressions are available in the query result cache\n5. **Query Rewriting**: Equivalent but more efficient query forms are substituted (e.g., using recording rules instead of complex aggregations)\n\n**QueryOptimizer Interface:**\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `OptimizeQuery` | `ctx context.Context, query string, timeRange TimeRange` | `*OptimizedQuery, error` | Returns optimized execution plan |\n| `EstimateComplexity` | `query string, timeRange TimeRange` | `*ComplexityEstimate, error` | Predicts query resource requirements |\n| `SuggestRecordingRules` | `queries []string, frequency time.Duration` | `[]RecordingRuleSuggestion, error` | Identifies queries that would benefit from pre-computation |\n| `UpdateStatistics` | `query string, duration time.Duration, seriesCount int` | `error` | Records query performance for future optimization |\n\n**Query Result Caching** significantly improves repeated query performance by storing computed results with cache keys based on query text, time range, and evaluation timestamp:\n\n- **Immutable Range Caching**: Queries for historical time ranges (older than the staleness threshold) return identical results and can be cached indefinitely\n- **Partial Result Caching**: Large time range queries are broken into smaller chunks, with completed chunks cached while only the most recent chunk is recomputed\n- **Cache Invalidation**: Recording rule updates and data ingestion trigger selective cache invalidation based on affected metric names and label sets\n- **Memory Management**: LRU eviction prevents cache from consuming excessive memory, with configurable size limits and TTL policies\n\n#### Advanced Aggregation Functions\n\nAdvanced aggregation functions extend our basic `sum`, `avg`, `max`, `min`, and `count` operations with sophisticated statistical and mathematical capabilities commonly needed in production monitoring environments.\n\n**Statistical Aggregations:**\n\n| Function Name | Parameters | Returns | Description |\n|---------------|------------|---------|-------------|\n| `stddev_over_time` | `vector, duration` | `vector` | Standard deviation of values over time window |\n| `quantile_over_time` | `quantile, vector, duration` | `vector` | Arbitrary quantile calculation over time |\n| `mad_over_time` | `vector, duration` | `vector` | Median absolute deviation for outlier detection |\n| `predict_linear` | `vector, duration` | `vector` | Linear regression prediction of future values |\n| `deriv` | `vector` | `vector` | Per-second derivative calculation |\n\n**Advanced Grouping Operations:**\n\n| Function Name | Parameters | Returns | Description |\n|---------------|------------|---------|-------------|\n| `topk` | `k, vector` | `vector` | Top K series by value |\n| `bottomk` | `k, vector` | `vector` | Bottom K series by value |\n| `count_values` | `string, vector` | `vector` | Count occurrences of each distinct value |\n| `group_by_interval` | `vector, duration` | `vector` | Time-based grouping for irregular series |\n\nThese advanced functions are implemented as extensions to our existing `Aggregator` component, following the same interface patterns but with more complex mathematical operations:\n\n1. **Streaming Computation**: Statistical functions process samples in streaming fashion to handle large time ranges without loading entire datasets into memory\n2. **Numerical Stability**: Implementations use numerically stable algorithms (e.g., Welford's method for standard deviation) to prevent precision loss\n3. **Missing Data Handling**: Advanced functions include configurable strategies for handling missing or null values in time series\n4. **Performance Optimization**: Expensive computations are candidates for automatic recording rule generation when used frequently\n\n⚠️ **Pitfall: Query Complexity Explosion**\n\nAdvanced query features can easily create queries that consume excessive system resources. A common mistake is allowing unrestricted use of expensive functions like `quantile_over_time` over large time ranges with high-cardinality label sets. This can consume gigabytes of memory and take minutes to execute.\n\n**Prevention Strategy**: Implement query complexity estimation that considers the number of time series, time range duration, and computational complexity of functions. Reject or warn about queries that exceed resource limits:\n\n```\nEstimated complexity: 50GB memory, 120 seconds execution\nThis query spans 10,000 time series over 7 days with quantile calculation.\nConsider: reducing time range, adding more specific label filters, or creating a recording rule.\n```\n\n### Implementation Guidance\n\nThe extensibility features build upon our existing component architecture through composition and interface extension rather than core system modification. This section provides practical guidance for implementing these advanced capabilities.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Rule Storage | YAML files + file watching | etcd cluster with version control |\n| Notification Delivery | HTTP webhooks + retry queues | Message queue system (RabbitMQ/Kafka) |\n| Federation Transport | HTTP scraping (existing) | gRPC streaming for high-volume federation |\n| Query Caching | In-memory LRU cache | Redis cluster with persistence |\n| Result Storage | Extend existing StorageEngine | Separate OLAP system for recording rules |\n\n**Recommended File Structure for Extensions:**\n\n```\nproject-root/\n  internal/\n    alerting/\n      rule_evaluator.go           ← Alert rule evaluation engine\n      notification_manager.go     ← Multi-channel notification delivery  \n      alert_state.go             ← Alert lifecycle management\n      rule_evaluator_test.go     ← Comprehensive rule testing\n    federation/\n      federation_scraper.go       ← Extends ScrapeEngine for federation\n      query_federator.go         ← Cross-instance query distribution\n      topology_manager.go        ← Federation hierarchy management\n    query_advanced/\n      recording_rules.go         ← Pre-computed metrics engine\n      query_optimizer.go         ← Query performance optimization\n      advanced_aggregations.go   ← Statistical and mathematical functions\n      query_cache.go             ← Result caching with invalidation\n  configs/\n    alerting_rules.yml           ← Alert rule definitions\n    recording_rules.yml          ← Recording rule definitions\n    federation.yml               ← Federation topology configuration\n```\n\n**Alert Rule Evaluator Infrastructure (Complete Implementation):**\n\n```go\npackage alerting\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n    \"path/to/project/internal/query\"\n    \"path/to/project/internal/storage\"\n)\n\n// AlertState represents the current state of an alert rule\ntype AlertState int\n\nconst (\n    AlertStateInactive AlertState = iota\n    AlertStatePending\n    AlertStateFiring\n    AlertStateResolved\n)\n\n// AlertRule defines a condition to monitor and alert on\ntype AlertRule struct {\n    Name               string            `yaml:\"name\"`\n    Query              string            `yaml:\"query\"`\n    Duration           time.Duration     `yaml:\"for\"`\n    Labels             map[string]string `yaml:\"labels\"`\n    Annotations        map[string]string `yaml:\"annotations\"`\n    EvaluationInterval time.Duration     `yaml:\"interval\"`\n    \n    // Internal state tracking\n    State        AlertState    `yaml:\"-\"`\n    ActiveSince  time.Time     `yaml:\"-\"`\n    ResolvedAt   *time.Time    `yaml:\"-\"`\n    mutex        sync.RWMutex  `yaml:\"-\"`\n}\n\n// AlertInstance represents a specific firing alert\ntype AlertInstance struct {\n    RuleName     string             `json:\"rule_name\"`\n    Labels       map[string]string  `json:\"labels\"`\n    State        AlertState         `json:\"state\"`\n    Value        float64            `json:\"value\"`\n    StartsAt     time.Time          `json:\"starts_at\"`\n    EndsAt       *time.Time         `json:\"ends_at,omitempty\"`\n    GeneratorURL string             `json:\"generator_url\"`\n    Fingerprint  uint64             `json:\"fingerprint\"`\n}\n\n// RuleEvaluator manages alert rule evaluation and state tracking\ntype RuleEvaluator struct {\n    rules       map[string]*AlertRule\n    queryEngine *query.QueryEngine\n    storage     *storage.StorageEngine\n    \n    activeAlerts    map[uint64]*AlertInstance  // fingerprint -> alert\n    notificationCh  chan<- AlertInstance       // channel for sending notifications\n    \n    evaluationTicker *time.Ticker\n    stopCh          chan struct{}\n    mutex           sync.RWMutex\n}\n\n// NewRuleEvaluator creates a rule evaluator with notification channel\nfunc NewRuleEvaluator(queryEngine *query.QueryEngine, storage *storage.StorageEngine, \n                      notificationCh chan<- AlertInstance) *RuleEvaluator {\n    return &RuleEvaluator{\n        rules:           make(map[string]*AlertRule),\n        queryEngine:     queryEngine,\n        storage:         storage,\n        activeAlerts:    make(map[uint64]*AlertInstance),\n        notificationCh:  notificationCh,\n        stopCh:          make(chan struct{}),\n    }\n}\n\n// LoadRulesFromFile loads alert rules from YAML configuration\nfunc (re *RuleEvaluator) LoadRulesFromFile(filename string) error {\n    // TODO 1: Read YAML file containing alert rule definitions\n    // TODO 2: Parse YAML into []AlertRule using yaml.Unmarshal\n    // TODO 3: Validate each rule: check query syntax, ensure positive duration\n    // TODO 4: Call AddRule for each valid rule to register it\n    // TODO 5: Return error if any rule validation fails\n    return nil\n}\n\n// AddRule registers a new alert rule for evaluation\nfunc (re *RuleEvaluator) AddRule(rule *AlertRule) error {\n    // TODO 1: Validate rule.Query by parsing it with queryEngine\n    // TODO 2: Ensure rule.Name is unique among existing rules  \n    // TODO 3: Set default EvaluationInterval if not specified\n    // TODO 4: Store rule in re.rules map with name as key\n    // TODO 5: Log successful rule registration\n    return nil\n}\n\n// Start begins periodic rule evaluation\nfunc (re *RuleEvaluator) Start(ctx context.Context, evaluationInterval time.Duration) error {\n    // TODO 1: Create ticker with specified evaluation interval\n    // TODO 2: Start goroutine that calls EvaluateAll on each tick\n    // TODO 3: Handle context cancellation to stop evaluation loop\n    // TODO 4: Ensure proper cleanup of ticker and goroutines\n    return nil\n}\n\n// EvaluateAll evaluates all registered rules at the specified time\nfunc (re *RuleEvaluator) EvaluateAll(ctx context.Context, evalTime time.Time) ([]AlertInstance, error) {\n    // TODO 1: Iterate through all rules in re.rules map\n    // TODO 2: For each rule, execute rule.Query using queryEngine.ExecuteInstantQuery\n    // TODO 3: Process query results to determine if alert condition is met\n    // TODO 4: Update rule state (inactive -> pending -> firing) based on duration\n    // TODO 5: Generate AlertInstance for newly firing alerts\n    // TODO 6: Send new/resolved alerts to notification channel\n    // TODO 7: Return list of all currently active alert instances\n    return nil, nil\n}\n```\n\n**Federation Target Discovery (Core Logic Skeleton):**\n\n```go\npackage federation\n\nimport (\n    \"context\"\n    \"net/http\"\n    \"time\"\n    \"path/to/project/internal/scrape\"\n)\n\n// FederationTarget represents an upstream metrics instance to federate from\ntype FederationTarget struct {\n    URL           string              `yaml:\"url\"`\n    MatchRules    []FederationRule    `yaml:\"match_rules\"`\n    ScrapeInterval time.Duration      `yaml:\"scrape_interval\"`\n    ExternalLabels map[string]string  `yaml:\"external_labels\"`\n}\n\n// FederationRule defines which metrics to pull from upstream instance\ntype FederationRule struct {\n    MatchMetrics   []string                    `yaml:\"match_metrics\"`\n    MatchLabels    []query.LabelMatcher        `yaml:\"match_labels\"`\n    ExcludeMetrics []string                    `yaml:\"exclude_metrics\"`\n    SampleLimit    int                         `yaml:\"sample_limit\"`\n}\n\n// FederationScraper extends ScrapeEngine to handle federation endpoints\ntype FederationScraper struct {\n    baseScraper *scrape.ScrapeEngine\n    httpClient  *http.Client\n    targets     map[string]FederationTarget\n    mutex       sync.RWMutex\n}\n\n// ScrapeFederationTarget pulls metrics from upstream federation endpoint\nfunc (fs *FederationScraper) ScrapeFederationTarget(ctx context.Context, target FederationTarget) error {\n    // TODO 1: Build federation URL with match[] parameters from target.MatchRules\n    // TODO 2: Create HTTP request with proper timeout and user-agent headers\n    // TODO 3: Execute HTTP GET request to /federate endpoint\n    // TODO 4: Parse response body as Prometheus exposition format\n    // TODO 5: Apply external labels from target.ExternalLabels to all metrics\n    // TODO 6: Filter metrics based on target.MatchRules inclusion/exclusion\n    // TODO 7: Send filtered samples to storage engine through existing pipeline\n    return nil\n}\n\n// UpdateFederationTargets refreshes the list of upstream instances\nfunc (fs *FederationScraper) UpdateFederationTargets(targets []FederationTarget) error {\n    // TODO 1: Validate each target URL and match rules\n    // TODO 2: Check for target URL uniqueness to prevent duplicates\n    // TODO 3: Update fs.targets map with new target configuration\n    // TODO 4: Schedule scraping for new targets using existing scheduler\n    // TODO 5: Remove scraping for targets no longer in the list\n    return nil\n}\n```\n\n**Query Result Caching Infrastructure (Complete Implementation):**\n\n```go\npackage query_advanced\n\nimport (\n    \"crypto/sha256\"\n    \"encoding/hex\"\n    \"sync\"\n    \"time\"\n    \"container/list\"\n)\n\n// CacheKey uniquely identifies a cached query result\ntype CacheKey struct {\n    Query     string    `json:\"query\"`\n    Start     time.Time `json:\"start\"`\n    End       time.Time `json:\"end\"`\n    EvalTime  time.Time `json:\"eval_time\"`\n}\n\n// String returns string representation for hashing\nfunc (ck CacheKey) String() string {\n    return fmt.Sprintf(\"%s:%d:%d:%d\", ck.Query, ck.Start.Unix(), ck.End.Unix(), ck.EvalTime.Unix())\n}\n\n// Hash returns SHA256 hash of cache key for map storage\nfunc (ck CacheKey) Hash() string {\n    hasher := sha256.New()\n    hasher.Write([]byte(ck.String()))\n    return hex.EncodeToString(hasher.Sum(nil))\n}\n\n// CacheEntry stores cached query results with metadata\ntype CacheEntry struct {\n    Key        CacheKey              `json:\"key\"`\n    Result     *query.QueryResult    `json:\"result\"`\n    CachedAt   time.Time            `json:\"cached_at\"`\n    AccessTime time.Time            `json:\"last_access\"`\n    TTL        time.Duration        `json:\"ttl\"`\n    Size       int64                `json:\"size_bytes\"`\n    \n    // LRU list element for eviction tracking\n    element *list.Element `json:\"-\"`\n}\n\n// QueryCache provides LRU caching for query results\ntype QueryCache struct {\n    entries    map[string]*CacheEntry  // hash -> entry\n    lruList    *list.List              // LRU eviction order\n    maxSize    int64                   // maximum cache size in bytes\n    currentSize int64                  // current cache size in bytes\n    mutex      sync.RWMutex            // concurrent access protection\n    \n    // Metrics for cache performance monitoring\n    hits       int64\n    misses     int64\n    evictions  int64\n}\n\n// NewQueryCache creates cache with specified maximum size\nfunc NewQueryCache(maxSizeBytes int64) *QueryCache {\n    return &QueryCache{\n        entries:  make(map[string]*CacheEntry),\n        lruList:  list.New(),\n        maxSize:  maxSizeBytes,\n    }\n}\n\n// Get retrieves cached result if available and not expired\nfunc (qc *QueryCache) Get(key CacheKey) (*query.QueryResult, bool) {\n    // TODO 1: Calculate hash of cache key for map lookup\n    // TODO 2: Check if entry exists and is not expired (cachedAt + TTL > now)\n    // TODO 3: Update entry.AccessTime and move to front of LRU list\n    // TODO 4: Increment hit/miss counters for monitoring\n    // TODO 5: Return deep copy of cached result to prevent modification\n    return nil, false\n}\n\n// Put stores query result in cache with automatic eviction\nfunc (qc *QueryCache) Put(key CacheKey, result *query.QueryResult, ttl time.Duration) {\n    // TODO 1: Calculate size of result for memory tracking\n    // TODO 2: Check if adding entry would exceed maxSize limit\n    // TODO 3: Evict LRU entries until sufficient space is available\n    // TODO 4: Create CacheEntry with current timestamp and TTL\n    // TODO 5: Store entry in map and add to front of LRU list\n    // TODO 6: Update currentSize and evictions counter\n}\n\n// InvalidateByMetric removes cached entries that depend on specific metric\nfunc (qc *QueryCache) InvalidateByMetric(metricName string) int {\n    // TODO 1: Iterate through all cached entries\n    // TODO 2: Parse each entry's query to extract referenced metric names  \n    // TODO 3: Remove entries whose queries reference the invalidated metric\n    // TODO 4: Update cache size counters and LRU list\n    // TODO 5: Return number of entries invalidated\n    return 0\n}\n```\n\n**Milestone Checkpoints for Extensions:**\n\n**Alerting System Checkpoint:**\n- Load sample alert rules from YAML: `go run cmd/server/main.go --config=configs/alerting_test.yml`\n- Expected: Server starts and logs \"Loaded 3 alert rules\" \n- Verify rule evaluation: Send test metrics that trigger conditions, check alert state API\n- Notification testing: Configure webhook endpoint, verify alert notifications are delivered\n- Signs of problems: Rules not loading (check YAML syntax), queries failing (validate PromQL), notifications not sending (check webhook URL)\n\n**Federation Checkpoint:**\n- Configure federation target: Add federation section to config pointing to existing Prometheus\n- Expected: Federated metrics appear in local storage with external labels\n- Verify query spanning: Execute queries that combine local and federated metrics\n- Performance check: Monitor federation scrape duration and memory usage\n- Signs of problems: No federated metrics (check upstream /federate endpoint), label conflicts (verify external labels), high memory usage (reduce match rules scope)\n\n**Query Optimization Checkpoint:**\n- Enable query caching: Set cache_enabled=true in query configuration\n- Execute expensive query twice: Second execution should be significantly faster\n- Recording rule test: Create recording rule, verify pre-computed metrics are stored\n- Cache invalidation: Ingest new data, verify cache entries are properly invalidated\n- Performance monitoring: Check cache hit ratio, query duration improvements\n- Signs of problems: No cache hits (check TTL settings), memory growth (verify eviction), incorrect results (cache invalidation bugs)\n\n**Debugging Tips for Extensions:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Alert rules not firing | Query returns no results | Check rule query in query UI | Verify label selectors match actual metrics |\n| Notifications not delivered | Webhook endpoint unreachable | Check notification manager logs | Verify webhook URL, check network connectivity |\n| Federation not working | Upstream instance not accessible | Test federation URL manually | Check upstream /federate endpoint, verify network |\n| Cached queries wrong results | Cache not invalidated on data ingestion | Check cache invalidation logs | Ensure cache invalidation on metric writes |\n| Recording rules consuming memory | Rule generates high-cardinality metrics | Monitor rule evaluation metrics | Add more specific label selectors to rule query |\n| Federation causing high memory | Too many metrics being federated | Check federation match rules scope | Restrict match rules to essential metrics only |\n\n\n## Glossary\n\n> **Milestone(s):** This section provides comprehensive definitions for technical terms used throughout all four milestones: Metrics Data Model (1), Scrape Engine (2), Time Series Storage (3), and Query Engine (4).\n\nThis glossary serves as the definitive reference for technical terminology, concepts, and vocabulary used throughout the metrics collection system design. Understanding these terms is essential for implementing and maintaining the system effectively. Each term includes its definition, context of use, and relationships to other concepts where applicable.\n\n### Core Concepts and Terminology\n\nThe metrics collection system introduces several domain-specific concepts that may be unfamiliar to developers coming from other backgrounds. This section establishes a common vocabulary for discussing system behavior, implementation details, and operational characteristics.\n\n**Time Series and Data Model Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **time series** | A sequence of timestamped values identified by a unique combination of metric name and label set | The fundamental data structure stored and queried by the system |\n| **cardinality** | The number of unique time series created by all combinations of metric name and label values | Critical metric for memory usage and performance planning |\n| **label explosion** | Exponential growth in cardinality when high-cardinality labels create excessive time series combinations | Primary cause of memory exhaustion and query performance degradation |\n| **metric type** | The semantic category that defines how a metric behaves: counter, gauge, histogram, or summary | Determines valid operations and query interpretations |\n| **observability** | The ability to understand system behavior and health through external metrics and monitoring | The broader goal that metrics collection systems enable |\n| **exposition format** | The Prometheus text-based format used to expose metrics over HTTP endpoints | Standard format parsed by the scrape engine |\n| **retention period** | The duration for which historical time series data is preserved before automatic deletion | Balances storage costs with historical analysis needs |\n\n**Scraping and Collection Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **pull-based scraping** | A metrics collection model where the monitoring system actively retrieves metrics from targets | Contrasts with push-based systems where applications send metrics |\n| **scrape target** | An HTTP endpoint that exposes metrics in the exposition format for collection | The source of all metrics data in the system |\n| **service discovery** | Automatic detection and configuration of scrape targets from dynamic sources like Kubernetes | Enables monitoring of dynamic environments without manual configuration |\n| **target health** | The availability and response status of a scrape endpoint based on recent collection attempts | Used for alerting and operational visibility |\n| **scrape interval** | The frequency at which metrics are collected from each target | Balances data freshness with system load |\n| **scrape timeout** | The maximum duration allowed for a single HTTP metrics collection request | Prevents slow targets from blocking the scrape engine |\n\n**Storage and Compression Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **Gorilla compression** | A specialized time series compression algorithm using delta-of-delta timestamp encoding and XOR value compression | Reduces storage requirements to approximately 1.37 bytes per sample |\n| **WAL** | Write-Ahead Log - a durability mechanism that records intended operations before they are applied | Enables crash recovery without data loss |\n| **chunk** | A compressed block of time series samples with fixed time boundaries and maximum sample counts | The unit of storage and compression for time series data |\n| **index consistency** | The property that all indexes correctly map to existing time series data without orphaned references | Critical for query correctness and system reliability |\n| **downsampling** | The process of reducing data resolution by aggregating high-frequency samples into lower-frequency summaries | Enables long-term storage with reduced space requirements |\n\n**Querying and Analysis Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **PromQL** | Prometheus Query Language - a functional query language for selecting and aggregating time series data | The primary interface for data analysis and alerting |\n| **AST** | Abstract Syntax Tree - the parsed representation of a PromQL query used for execution | Internal structure created by the query parser |\n| **label selector** | Filter criteria that match time series based on label name and value patterns | Fundamental mechanism for narrowing query scope |\n| **aggregation** | Mathematical combination of multiple time series values into summary statistics | Enables analysis across multiple dimensions and instances |\n| **range query** | A query that returns multiple data points across a specified time window | Used for graphing and trend analysis |\n| **instant query** | A query that returns a single point-in-time value for each matching time series | Used for alerting and current state monitoring |\n| **interpolation** | The process of estimating values at query timestamps when no exact sample exists | Handles alignment between sample timestamps and query evaluation times |\n| **staleness** | The threshold beyond which a data point is considered too old to use in query results | Prevents outdated values from appearing in current analysis |\n\n### System Architecture and Component Terms\n\nThe metrics system consists of several interconnected components that coordinate to provide end-to-end metrics collection and analysis capabilities. Understanding the role and terminology of each component is essential for system implementation and maintenance.\n\n**Component and Coordination Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **backpressure** | A flow control mechanism that slows down producers when consumers cannot keep up with the data rate | Prevents memory exhaustion during high-volume ingestion |\n| **pipeline coordination** | Managing the flow of data between system components with proper synchronization and error handling | Ensures reliable data flow from scraping through storage to querying |\n| **concurrency control** | Managing simultaneous access to shared resources without data corruption or race conditions | Critical for multi-threaded components like the storage engine |\n| **graceful degradation** | Maintaining system availability with reduced functionality during overload or partial failures | Allows continued operation when some components are impaired |\n| **write batching** | Combining multiple small write operations into larger, more efficient batch operations | Improves storage throughput and reduces overhead |\n| **lock granularity** | The scope and size of data protected by each synchronization primitive | Affects both performance and correctness in concurrent systems |\n| **copy-on-write** | An optimization where reads access shared data while writes create private copies | Reduces contention between readers and writers |\n| **readers-writer lock** | A synchronization primitive allowing multiple concurrent readers or a single writer | Optimizes for read-heavy workloads common in time series systems |\n| **channel-based coordination** | Using Go channels to coordinate communication and synchronization between goroutines | Primary concurrency pattern in Go-based implementations |\n\n**Operational and Monitoring Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **resource monitoring** | Tracking system resource usage to prevent exhaustion and enable proactive limits | Includes memory, disk space, CPU, and network bandwidth monitoring |\n| **resource exhaustion** | The depletion of system resources that can cause component failures or degraded performance | Common operational issue requiring monitoring and response procedures |\n| **emergency retention** | Aggressive data deletion triggered automatically during disk space crises | Last-resort mechanism to prevent complete system failure |\n| **consistency validation** | Verification that stored data matches expected invariants and relationships | Includes index-data alignment and checksum validation |\n| **partial scrape success** | The ability to preserve valid metrics while rejecting malformed ones from the same target | Improves system resilience to target-side issues |\n| **circuit breaker** | A failure protection pattern that temporarily blocks requests to failing dependencies | Prevents cascade failures and enables faster recovery |\n\n### Testing and Development Terms\n\nBuilding a reliable metrics collection system requires comprehensive testing strategies and development practices. These terms describe the approaches and techniques used to validate system correctness and performance.\n\n**Testing Methodology Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **property-based testing** | Testing approach using automatically generated inputs that satisfy specified properties | Discovers edge cases that manual test cases might miss |\n| **fault injection** | Deliberately introducing errors and failures to test error handling and recovery mechanisms | Validates system behavior under adverse conditions |\n| **statistical validation** | Comparing computed results against mathematically expected values using statistical methods | Ensures aggregation functions and calculations produce correct results |\n| **race detector** | A tool for detecting unsynchronized access to shared memory in concurrent programs | Essential for validating thread safety in Go programs |\n| **benchmark testing** | Measuring performance characteristics like throughput, latency, and resource usage | Validates that system meets performance requirements |\n| **mock objects** | Test doubles that simulate external dependencies with controllable, predictable behavior | Enables isolated testing of individual components |\n\n### Advanced Features and Extensions\n\nThe metrics system architecture supports advanced capabilities that extend beyond basic collection and querying. Understanding these concepts is important for system evolution and operational sophistication.\n\n**Advanced System Capabilities**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **distributed tracing** | End-to-end request tracking that follows operations across multiple system components | Enables debugging and performance analysis of complex operations |\n| **structured logging** | Machine-readable log format with consistent fields and structured data | Improves observability and enables automated log analysis |\n| **health checking** | Systematic monitoring of component status with standardized health reporting | Provides operational visibility and enables automated response |\n| **federation** | Connecting multiple metrics instances into a coordinated cluster for scalability | Enables horizontal scaling beyond single-instance limits |\n| **recording rules** | Pre-computed expensive queries that are stored as new time series | Improves query performance for commonly used aggregations |\n| **query optimization** | Analyzing queries to identify performance improvements and suggest alternatives | Reduces resource consumption and improves user experience |\n| **hierarchical federation** | A pull-based federation approach that extends the scraping concept to other metrics instances | Maintains consistency with the core pull-based architecture |\n| **cross-instance querying** | PromQL queries that span multiple federated instances and merge results | Provides unified view of metrics across distributed deployments |\n\n**Alerting and Notification Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **alert rule evaluation** | Continuous assessment of PromQL expressions against time series data to detect alert conditions | Core function of alerting systems built on the metrics platform |\n| **notification manager** | System responsible for delivering alert notifications through multiple channels | Handles routing, rate limiting, and delivery confirmation |\n| **rule evaluator** | Component that manages alert rule execution, state tracking, and notification generation | Coordinates between query engine and notification systems |\n| **alert state** | The current condition of an alert rule: inactive, pending, firing, or resolved | Determines notification behavior and operational response |\n\n**Performance and Caching Terms**\n\n| Term | Definition | Context |\n|------|------------|---------|\n| **query result caching** | Storing computed PromQL results with appropriate cache keys for repeated queries | Improves response times and reduces computational load |\n| **cache invalidation** | Removing cached entries when underlying time series data changes | Maintains cache consistency while preserving performance benefits |\n| **query complexity estimation** | Predicting resource requirements before query execution to enable limits and optimization | Prevents resource exhaustion from expensive queries |\n| **LRU eviction** | Least Recently Used cache entry removal strategy for managing memory usage | Balances cache hit rates with memory consumption |\n| **query federation** | Distributing queries across multiple instances and merging partial results | Enables querying of datasets larger than single-instance capacity |\n\n### Data Types and Structures\n\nThe metrics system defines specific data structures and types that represent different aspects of the system. Understanding these types and their relationships is crucial for implementation.\n\n**Core Data Type Categories**\n\n| Category | Purpose | Key Types |\n|----------|---------|-----------|\n| **Configuration Types** | System and component configuration | `Config`, `ScrapeConfig`, `StorageConfig`, `QueryConfig` |\n| **Metric Types** | Time series data representation | `Counter`, `Gauge`, `Histogram`, `Sample`, `Labels` |\n| **Storage Types** | Data persistence and compression | `CompressedChunk`, `WriteAheadLog`, `InvertedIndexes`, `GorillaCompressor` |\n| **Query Types** | Query execution and results | `QueryEngine`, `ExpressionParser`, `ASTNode`, `QueryResult` |\n| **Coordination Types** | Component interaction and flow control | `PipelineCoordinator`, `QueryCoordinator`, `SystemCoordinator` |\n| **Health and Monitoring Types** | System status and diagnostics | `ComponentHealth`, `HealthChecker`, `SystemHealth`, `PerformanceMonitor` |\n| **Error and Control Types** | Error handling and flow control | `MetricsError`, `CircuitBreaker`, `QueryLimiter` |\n| **Testing Types** | Development and validation support | `MockHTTPTarget`, `TimeSeriesGenerator`, `PerformanceMonitor` |\n\n### Constants and Configuration Values\n\nThe system defines standard constants for timeouts, limits, and default behaviors that ensure consistent operation across different deployment environments.\n\n**System Default Constants**\n\n| Constant | Value | Purpose |\n|----------|--------|---------|\n| `DEFAULT_SCRAPE_INTERVAL` | 15 seconds | Standard frequency for metrics collection when not otherwise specified |\n| `DEFAULT_SCRAPE_TIMEOUT` | 10 seconds | Maximum duration for HTTP scrape requests to prevent blocking |\n| `DEFAULT_RETENTION_PERIOD` | 30 days | Standard data retention duration balancing storage cost and utility |\n| `DEFAULT_QUERY_TIMEOUT` | 30 seconds | Maximum query execution time to prevent resource exhaustion |\n\n**Health and Status Constants**\n\n| Constant | Description | Usage |\n|----------|-------------|--------|\n| `HealthUp` | Component is healthy and operational | Indicates normal component function |\n| `HealthDown` | Component has failed or is unreachable | Indicates complete component failure |\n| `HealthDegraded` | Component is operational but experiencing issues | Indicates partial functionality or performance problems |\n\n**Query and Matching Constants**\n\n| Constant | Description | Usage |\n|----------|-------------|--------|\n| `MatchEqual` | Exact string equality matching | Standard label value filtering |\n| `MatchNotEqual` | String inequality matching | Exclusion-based label filtering |\n| `MatchRegex` | Regular expression matching | Pattern-based label filtering |\n| `MatchNotRegex` | Negative regular expression matching | Pattern-based label exclusion |\n\n### Error Types and Handling Categories\n\nThe system categorizes errors to enable appropriate handling strategies and recovery mechanisms. Understanding error classifications helps in building robust error handling logic.\n\n**Error Classification System**\n\n| Error Type | Description | Handling Strategy |\n|------------|-------------|-------------------|\n| `ErrorTypeTransient` | Temporary errors that may succeed on retry with backoff | Implement exponential backoff retry logic |\n| `ErrorTypePermanent` | Permanent errors that will not succeed on retry | Log error and fail fast without retry attempts |\n| `ErrorTypeRateLimit` | Rate limiting errors requiring backoff | Implement longer backoff periods and reduce request rate |\n| `ErrorTypeResource` | Resource exhaustion requiring different handling | Implement circuit breakers and shed load |\n\n**Circuit Breaker States**\n\n| State | Description | Behavior |\n|-------|-------------|----------|\n| `CircuitClosed` | Normal operation allowing all requests | Monitor failure rate and transition to open on threshold |\n| `CircuitOpen` | Failure protection mode rejecting all requests | Block requests and transition to half-open after timeout |\n| `CircuitHalfOpen` | Testing mode allowing limited requests to test recovery | Allow single request to test if service has recovered |\n\n### Alert and Rule Management\n\nThe alerting system introduces additional terminology for rule management, notification handling, and alert state tracking.\n\n**Alert State Management**\n\n| State | Description | Transitions |\n|-------|-------------|-------------|\n| `AlertStateInactive` | Alert rule condition is not currently met | Transitions to Pending when condition becomes true |\n| `AlertStatePending` | Condition is met but duration threshold not yet exceeded | Transitions to Firing after duration or Inactive if condition clears |\n| `AlertStateFiring` | Condition has been met for the required duration | Transitions to Resolved when condition clears |\n| `AlertStateResolved` | Previously firing alert where condition is no longer met | Transitions to Inactive after notification sent |\n\n### Implementation Guidance\n\nThis section provides practical guidance for implementing the metrics collection system using the defined terminology and concepts.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **HTTP Server** | Go net/http with ServeMux | Gin or Echo framework with middleware |\n| **Configuration** | YAML with gopkg.in/yaml.v3 | Viper with environment variable override |\n| **Logging** | Standard log/slog package | Structured logging with zerolog or logrus |\n| **Testing** | Standard testing package | Testify for assertions and table-driven tests |\n| **Benchmarking** | Go built-in benchmarks | Continuous benchmarking with benchstat |\n\n#### File Structure Organization\n\nThe metrics collection system should be organized into clear modules that separate concerns and enable independent development of each component:\n\n```\nmetrics-system/\n  cmd/\n    metrics-server/\n      main.go                    ← Application entry point\n  internal/\n    config/\n      config.go                  ← Configuration loading and validation\n      defaults.go                ← Default constant definitions\n    metrics/\n      types.go                   ← Metric type definitions (Counter, Gauge, Histogram)\n      labels.go                  ← Label handling and validation\n      validator.go               ← Cardinality and label validation\n    scrape/\n      engine.go                  ← Main scrape engine coordination\n      target.go                  ← Target health and state management\n      discovery.go               ← Service discovery implementations\n      parser.go                  ← Metrics exposition format parsing\n    storage/\n      engine.go                  ← Storage engine coordination\n      compression.go             ← Gorilla compression implementation\n      wal.go                     ← Write-ahead log implementation\n      index.go                   ← Inverted index management\n      chunk.go                   ← Compressed chunk handling\n    query/\n      engine.go                  ← Query engine coordination\n      parser.go                  ← PromQL parsing and AST construction\n      selector.go                ← Label selector implementation\n      aggregator.go              ← Aggregation function implementation\n      executor.go                ← Range query execution\n    coordination/\n      pipeline.go                ← Pipeline coordination between components\n      system.go                  ← System-level coordination\n      health.go                  ← Health checking and status reporting\n    errors/\n      types.go                   ← Error type definitions\n      circuit.go                 ← Circuit breaker implementation\n  pkg/\n    client/                      ← Public client libraries (if needed)\n  test/\n    integration/                 ← End-to-end integration tests\n    testdata/                    ← Test fixtures and sample data\n  docs/\n    glossary.md                  ← This document\n```\n\n#### Core Infrastructure Implementation\n\n**Logger Implementation Starter Code:**\n\n```go\npackage internal\n\nimport (\n    \"context\"\n    \"log/slog\"\n    \"os\"\n)\n\n// Logger provides structured logging with component identification\ntype Logger struct {\n    component string\n    logger    *slog.Logger\n}\n\n// NewLogger creates a structured logger for a specific component\nfunc NewLogger(component string) *Logger {\n    handler := slog.NewJSONHandler(os.Stdout, &slog.HandlerOptions{\n        Level: slog.LevelInfo,\n    })\n    \n    logger := slog.New(handler).With(\"component\", component)\n    \n    return &Logger{\n        component: component,\n        logger:    logger,\n    }\n}\n\n// Info logs informational messages with structured fields\nfunc (l *Logger) Info(msg string, fields ...any) {\n    l.logger.Info(msg, fields...)\n}\n\n// Error logs error messages with structured fields\nfunc (l *Logger) Error(msg string, fields ...any) {\n    l.logger.Error(msg, fields...)\n}\n\n// Debug logs debug messages with structured fields\nfunc (l *Logger) Debug(msg string, fields ...any) {\n    l.logger.Debug(msg, fields...)\n}\n\n// WithContext adds trace context to logger if available\nfunc (l *Logger) WithContext(ctx context.Context) *Logger {\n    // TODO: Extract trace ID from context if tracing is enabled\n    // TODO: Add trace ID as a structured field to logger\n    return l\n}\n```\n\n**Configuration Loading Implementation:**\n\n```go\npackage config\n\nimport (\n    \"fmt\"\n    \"os\"\n    \"time\"\n    \"gopkg.in/yaml.v3\"\n)\n\n// Config represents the complete system configuration\ntype Config struct {\n    Scrape  ScrapeConfig  `yaml:\"scrape\"`\n    Storage StorageConfig `yaml:\"storage\"`\n    Query   QueryConfig   `yaml:\"query\"`\n}\n\ntype ScrapeConfig struct {\n    ScrapeInterval  time.Duration  `yaml:\"scrape_interval\"`\n    ScrapeTimeout   time.Duration  `yaml:\"scrape_timeout\"`\n    StaticConfigs   []StaticConfig `yaml:\"static_configs\"`\n}\n\ntype StorageConfig struct {\n    DataDirectory     string        `yaml:\"data_directory\"`\n    RetentionPeriod   time.Duration `yaml:\"retention_period\"`\n    CompressionEnabled bool         `yaml:\"compression_enabled\"`\n}\n\ntype QueryConfig struct {\n    QueryTimeout      time.Duration `yaml:\"query_timeout\"`\n    MaxSeries        int           `yaml:\"max_series\"`\n    MaxRangeDuration time.Duration `yaml:\"max_range_duration\"`\n}\n\ntype StaticConfig struct {\n    Targets []string          `yaml:\"targets\"`\n    Labels  map[string]string `yaml:\"labels\"`\n}\n\n// Constants for default values\nconst (\n    DEFAULT_SCRAPE_INTERVAL   = 15 * time.Second\n    DEFAULT_SCRAPE_TIMEOUT    = 10 * time.Second\n    DEFAULT_RETENTION_PERIOD  = 30 * 24 * time.Hour\n    DEFAULT_QUERY_TIMEOUT     = 30 * time.Second\n)\n\n// LoadFromFile reads configuration from YAML file\nfunc LoadFromFile(filename string) (*Config, error) {\n    data, err := os.ReadFile(filename)\n    if err != nil {\n        return nil, fmt.Errorf(\"reading config file: %w\", err)\n    }\n    \n    var config Config\n    if err := yaml.Unmarshal(data, &config); err != nil {\n        return nil, fmt.Errorf(\"parsing config YAML: %w\", err)\n    }\n    \n    config.SetDefaults()\n    return &config, nil\n}\n\n// SetDefaults populates default values for unspecified configuration\nfunc (c *Config) SetDefaults() {\n    if c.Scrape.ScrapeInterval == 0 {\n        c.Scrape.ScrapeInterval = DEFAULT_SCRAPE_INTERVAL\n    }\n    if c.Scrape.ScrapeTimeout == 0 {\n        c.Scrape.ScrapeTimeout = DEFAULT_SCRAPE_TIMEOUT\n    }\n    if c.Storage.RetentionPeriod == 0 {\n        c.Storage.RetentionPeriod = DEFAULT_RETENTION_PERIOD\n    }\n    if c.Query.QueryTimeout == 0 {\n        c.Query.QueryTimeout = DEFAULT_QUERY_TIMEOUT\n    }\n    if c.Storage.DataDirectory == \"\" {\n        c.Storage.DataDirectory = \"./data\"\n    }\n    if c.Query.MaxSeries == 0 {\n        c.Query.MaxSeries = 10000\n    }\n}\n```\n\n#### Core Logic Skeletons\n\n**Health Status Enumeration:**\n\n```go\npackage health\n\n// HealthStatus represents the operational state of system components\ntype HealthStatus int\n\nconst (\n    HealthUp HealthStatus = iota\n    HealthDown\n    HealthDegraded\n)\n\nfunc (h HealthStatus) String() string {\n    switch h {\n    case HealthUp:\n        return \"UP\"\n    case HealthDown:\n        return \"DOWN\" \n    case HealthDegraded:\n        return \"DEGRADED\"\n    default:\n        return \"UNKNOWN\"\n    }\n}\n```\n\n**Error Type System:**\n\n```go\npackage errors\n\nimport \"fmt\"\n\n// ErrorType categorizes errors for appropriate handling\ntype ErrorType int\n\nconst (\n    ErrorTypeTransient ErrorType = iota\n    ErrorTypePermanent\n    ErrorTypeRateLimit\n    ErrorTypeResource\n)\n\n// MetricsError provides structured error information\ntype MetricsError struct {\n    Type      ErrorType\n    Component string\n    Operation string\n    Message   string\n    Cause     error\n}\n\nfunc (e *MetricsError) Error() string {\n    return fmt.Sprintf(\"[%s:%s] %s: %s\", e.Component, e.Operation, e.Message, e.Cause)\n}\n\nfunc (e *MetricsError) Unwrap() error {\n    return e.Cause\n}\n```\n\n#### Testing Infrastructure\n\n**Mock HTTP Target for Testing:**\n\n```go\npackage testing\n\nimport (\n    \"fmt\"\n    \"math/rand\"\n    \"net/http\"\n    \"net/http/httptest\"\n    \"strings\"\n    \"sync\"\n    \"time\"\n)\n\n// MockHTTPTarget provides controllable HTTP endpoint for testing scrape engine\ntype MockHTTPTarget struct {\n    server    *httptest.Server\n    metrics   []string\n    delay     time.Duration\n    errorRate float64\n    mu        sync.RWMutex\n}\n\n// NewMockHTTPTarget creates a new mock target with default behavior\nfunc NewMockHTTPTarget() *MockHTTPTarget {\n    target := &MockHTTPTarget{\n        metrics:   []string{},\n        delay:     0,\n        errorRate: 0.0,\n    }\n    \n    target.server = httptest.NewServer(http.HandlerFunc(target.handleMetrics))\n    return target\n}\n\n// URL returns the HTTP URL of the mock target\nfunc (m *MockHTTPTarget) URL() string {\n    return m.server.URL + \"/metrics\"\n}\n\n// SetMetrics configures the metrics exposed by this target\nfunc (m *MockHTTPTarget) SetMetrics(metrics []string) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.metrics = make([]string, len(metrics))\n    copy(m.metrics, metrics)\n}\n\n// SetDelay sets artificial response delay for timeout testing\nfunc (m *MockHTTPTarget) SetDelay(delay time.Duration) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.delay = delay\n}\n\n// SetErrorRate sets probability of HTTP 500 errors (0.0 to 1.0)\nfunc (m *MockHTTPTarget) SetErrorRate(rate float64) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.errorRate = rate\n}\n\n// Close shuts down the mock HTTP server\nfunc (m *MockHTTPTarget) Close() {\n    m.server.Close()\n}\n\nfunc (m *MockHTTPTarget) handleMetrics(w http.ResponseWriter, r *http.Request) {\n    m.mu.RLock()\n    delay := m.delay\n    errorRate := m.errorRate\n    metrics := make([]string, len(m.metrics))\n    copy(metrics, m.metrics)\n    m.mu.RUnlock()\n    \n    // Simulate delay if configured\n    if delay > 0 {\n        time.Sleep(delay)\n    }\n    \n    // Simulate errors if configured\n    if errorRate > 0 && rand.Float64() < errorRate {\n        http.Error(w, \"Simulated server error\", http.StatusInternalServerError)\n        return\n    }\n    \n    w.Header().Set(\"Content-Type\", \"text/plain\")\n    w.WriteHeader(http.StatusOK)\n    \n    // Write metrics in Prometheus exposition format\n    for _, metric := range metrics {\n        fmt.Fprintf(w, \"%s\\n\", metric)\n    }\n}\n```\n\n#### Milestone Checkpoints\n\n**Milestone 1 - Metrics Data Model Checkpoint:**\nAfter implementing the metrics data model, verify functionality with:\n```bash\ngo test ./internal/metrics/...\n```\n\nExpected behavior:\n- Counter values increase monotonically and reject negative increments\n- Gauge values can be set to any value and read back correctly\n- Histogram observations are recorded in appropriate buckets\n- Label validation rejects high-cardinality combinations\n- Metric metadata is stored and retrievable\n\n**Milestone 2 - Scrape Engine Checkpoint:**\nAfter implementing the scrape engine, test with:\n```bash\ngo run cmd/metrics-server/main.go\ncurl http://localhost:9090/targets  # Should show discovered targets\n```\n\nExpected behavior:\n- Targets are discovered from configuration\n- HTTP scrapes retrieve metrics successfully\n- Target health reflects scrape success/failure\n- Malformed metrics are rejected without affecting valid ones\n\n**Milestone 3 - Storage Engine Checkpoint:**\nAfter implementing storage, verify with:\n```bash\ngo test ./internal/storage/... -v\n# Check data directory contains WAL and chunk files\nls -la ./data/\n```\n\nExpected behavior:\n- Samples are compressed using Gorilla algorithm\n- WAL provides durability for writes\n- Indexes enable fast series lookup\n- Old data is deleted according to retention policy\n\n**Milestone 4 - Query Engine Checkpoint:**\nAfter implementing the query engine, test with:\n```bash\ncurl \"http://localhost:9090/api/v1/query?query=up\"\ncurl \"http://localhost:9090/api/v1/query_range?query=rate(requests_total[5m])&start=1609459200&end=1609462800&step=60\"\n```\n\nExpected behavior:\n- PromQL expressions parse correctly\n- Label selectors filter time series appropriately\n- Aggregation functions produce correct results\n- Range queries return properly interpolated data points\n"}