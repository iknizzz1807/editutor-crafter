{"html":"<h1 id=\"metrics-amp-alerting-dashboard-design-document\">Metrics &amp; Alerting Dashboard: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>This system collects, stores, and visualizes time-series metrics data while providing intelligent alerting capabilities. The key architectural challenge is building a scalable time-series database that can handle high-throughput metric ingestion, efficient storage with compaction, and real-time querying for both dashboards and alert evaluation.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this foundational understanding applies throughout the project)</p>\n</blockquote>\n<p>Building a metrics and alerting system is fundamentally about solving the <strong>observability problem</strong>: how do you understand what&#39;s happening inside complex software systems that you can&#39;t directly see or touch? Modern applications are like vast, interconnected organisms with thousands of moving parts, and without proper instrumentation, diagnosing problems becomes like trying to perform surgery in complete darkness.</p>\n<p>This section establishes why metrics systems are essential, what makes them technically challenging to build correctly, and how existing solutions approach these challenges. Understanding these fundamentals will guide every architectural decision we make throughout this project.</p>\n<h3 id=\"the-hospital-monitoring-analogy\">The Hospital Monitoring Analogy</h3>\n<p>Think of a modern metrics and alerting system as the <strong>comprehensive monitoring infrastructure</strong> in a large hospital&#39;s intensive care unit. This analogy helps build intuition for the core concepts and challenges we&#39;ll encounter.</p>\n<p>In an ICU, every patient has multiple sensors continuously measuring vital signs: heart rate, blood pressure, oxygen saturation, temperature, respiratory rate, and dozens of other physiological indicators. These sensors generate <strong>time-series data</strong> — measurements that only make sense when paired with the exact timestamp they were recorded. A heart rate of 85 BPM is meaningless without knowing when it was measured, just like a CPU usage reading of 73% is useless without its timestamp.</p>\n<p>The hospital&#39;s monitoring system faces the same fundamental challenges our metrics system must solve:</p>\n<p><strong>High-Volume Data Ingestion</strong>: Across hundreds of patients, thousands of sensors generate measurements every few seconds. The monitoring system must ingest this constant stream of data without dropping measurements or falling behind. Similarly, our metrics system must handle thousands of application instances each reporting dozens of metrics every 10-30 seconds — potentially millions of data points per minute.</p>\n<p><strong>Efficient Storage with Retention Policies</strong>: The hospital can&#39;t store every heartbeat measurement forever — they need different retention strategies. Critical measurements from the last 24 hours are kept at full resolution. Data from the past week might be downsampled to one measurement per minute. Older data gets compressed into hourly or daily averages before eventually being archived or deleted. Our metrics system needs identical <strong>compaction and retention</strong> strategies to manage storage costs while preserving the right level of detail for different time ranges.</p>\n<p><strong>Real-Time Visualization</strong>: Nurses and doctors need live dashboards showing current patient status across multiple time scales — the last few minutes for immediate assessment, the last few hours to understand trends, and longer periods to track recovery progress. These dashboards must update automatically as new measurements arrive. Our system&#39;s visualization layer serves the same role for application operators and developers.</p>\n<p><strong>Intelligent Alerting</strong>: The most critical function is automated alerting when vital signs indicate danger. But this is surprisingly complex — a temporary spike in heart rate during physical therapy is normal, but the same reading at 3 AM while sleeping could indicate cardiac distress. Alert rules must consider <strong>context, duration, and patterns</strong>, not just simple thresholds. They must also avoid &quot;alert fatigue&quot; — if every minor fluctuation triggers an alarm, staff will start ignoring them, potentially missing real emergencies.</p>\n<p><strong>Multi-Dimensional Context</strong>: Each measurement needs rich context to be meaningful. A blood pressure reading must be tagged with the patient ID, measurement location (left arm vs right arm), patient position (sitting vs lying down), and the specific device used. This <strong>labeling and dimensionality</strong> allows medical staff to filter and aggregate data meaningfully. Our metrics system uses the same concept — a CPU usage metric needs labels for the server hostname, application name, data center region, and deployment version.</p>\n<p><strong>Cardinality Management</strong>: The hospital must be careful about creating too many unique combinations of patient + measurement type + device + location. With 500 patients, 20 vital signs, 50 different monitoring devices, and 10 possible measurement locations, you could theoretically have 5 million unique time series. But most combinations never exist in practice, and creating storage allocation for all possibilities would be wasteful and slow. This is the <strong>cardinality explosion</strong> problem our metrics system must solve — preventing users from accidentally creating millions of sparsely-populated time series through poorly designed label schemes.</p>\n<p>The hospital analogy illuminates why building an effective metrics system is challenging: it&#39;s not just about storing numbers, but about creating a reliable, efficient, and intelligent monitoring infrastructure that helps operators understand complex systems and respond to problems before they become catastrophic.</p>\n<h3 id=\"technical-challenges\">Technical Challenges</h3>\n<p>Building a production-ready metrics system involves solving several interconnected technical challenges that don&#39;t have obvious solutions. Each challenge influences the others, creating a web of architectural dependencies that must be carefully balanced.</p>\n<h4 id=\"high-cardinality-data-management\">High-Cardinality Data Management</h4>\n<p><strong>Cardinality</strong> refers to the number of unique time series in your system, determined by every unique combination of metric name and label values. This becomes a scalability nightmare faster than most developers anticipate.</p>\n<p>Consider a simple web application that tracks HTTP request duration with labels for <code>method</code>, <code>endpoint</code>, <code>status_code</code>, <code>datacenter</code>, and <code>version</code>. With just 5 HTTP methods, 20 endpoints, 10 possible status codes, 3 datacenters, and 5 active versions, you have 5 × 20 × 10 × 3 × 5 = 15,000 unique time series for a single metric. Add user-specific labels like <code>user_id</code> or <code>session_id</code>, and you suddenly have millions of time series, most containing only a few data points.</p>\n<p>High cardinality creates cascading problems:</p>\n<ul>\n<li><strong>Memory exhaustion</strong>: Each time series requires index entries and metadata structures in memory. With millions of series, even lightweight metadata can consume gigabytes of RAM.</li>\n<li><strong>Query performance degradation</strong>: Finding relevant time series requires scanning larger indexes. A query that should return results in milliseconds starts taking seconds.</li>\n<li><strong>Storage inefficiency</strong>: Most high-cardinality series contain sparse data, leading to poor compression ratios and wasted disk space.</li>\n<li><strong>Compaction overhead</strong>: Background processes that merge and downsample data must handle exponentially more files and indexes.</li>\n</ul>\n<p>The challenge is designing validation and limiting mechanisms that prevent cardinality explosions without restricting legitimate use cases. Users need enough dimensional flexibility to slice and dice their data meaningfully, but the system must protect itself from abuse or misconfiguration.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The cardinality problem is fundamentally about the tension between expressiveness and scalability. Every additional label dimension multiplies potential cardinality, but removing dimensions reduces the system&#39;s analytical power.</p>\n</blockquote>\n<h4 id=\"storage-efficiency-and-compression\">Storage Efficiency and Compression</h4>\n<p>Time-series data has unique storage characteristics that make traditional database approaches inefficient. The data is <strong>immutable</strong> (historical measurements never change), <strong>temporally ordered</strong> (queries almost always involve time ranges), and <strong>highly compressible</strong> (consecutive values often have small deltas).</p>\n<p>However, the data also arrives out of order — different application instances have clock skew, network delays vary, and some metrics are batch-uploaded from offline processing. The storage engine must efficiently handle both real-time ingestion and historical backfill while maintaining query performance.</p>\n<p>Compression becomes critical at scale. Raw time-series data might consume terabytes, but with proper compression (delta encoding, variable-length integers, and block compression), storage requirements can shrink by 10-20x. However, compression complicates queries — you can&#39;t seek to arbitrary time points within compressed blocks without partial decompression.</p>\n<p>The storage engine must also implement <strong>compaction</strong> — the process of merging small files into larger ones, downsampling high-resolution data to lower resolutions, and eventually deleting old data according to retention policies. Compaction runs continuously in the background but must not interfere with real-time ingestion or queries.</p>\n<h4 id=\"query-performance-at-scale\">Query Performance at Scale</h4>\n<p>Users expect sub-second query responses even when requesting data across long time ranges or high-cardinality dimensions. A dashboard loading 20 charts simultaneously might generate 20 concurrent queries, each potentially scanning millions of data points across hundreds of time series.</p>\n<p>Query performance depends on several factors:</p>\n<p><strong>Index Efficiency</strong>: Finding the right time series for a query requires efficient indexing by metric name and labels. Traditional B-tree indexes work poorly for high-cardinality label combinations. The system needs specialized indexing structures optimized for multi-dimensional filtering.</p>\n<p><strong>Data Layout</strong>: Physical storage layout dramatically affects query performance. Storing all metrics together requires extensive filtering. Storing each time series separately creates too many small files. The optimal layout depends on query patterns, which vary by application.</p>\n<p><strong>Aggregation Complexity</strong>: Most queries involve aggregation — summing values across multiple time series, computing percentiles, or calculating rates. These operations must be pushed down to the storage layer to avoid transferring massive amounts of raw data over the network.</p>\n<p><strong>Caching Strategy</strong>: Repeated queries for the same data should be cached, but time-series data is continuously growing. The caching layer must invalidate cached results appropriately while maximizing hit rates for relatively static historical data.</p>\n<h4 id=\"alert-reliability-and-accuracy\">Alert Reliability and Accuracy</h4>\n<p>Alerting is perhaps the most critical component because alert failures can mean missing production outages. However, reliable alerting involves subtle challenges that are easy to get wrong.</p>\n<p><strong>Evaluation Timing</strong>: Alert rules must be evaluated at precise intervals, but evaluation itself takes time. If evaluating all alert rules takes 45 seconds, but the evaluation interval is 30 seconds, the system falls behind and might miss brief anomalies. The evaluation scheduler must handle timing constraints gracefully.</p>\n<p><strong>State Management</strong>: Alerts transition through states (inactive → pending → firing → resolved), and these transitions must be tracked reliably across system restarts. Alert state must be persisted durably, but state updates are frequent and must not become a performance bottleneck.</p>\n<p><strong>Flapping Prevention</strong>: Metrics that oscillate around alert thresholds can cause alerts to rapidly transition between firing and resolved states, generating notification spam. The system needs hysteresis mechanisms and minimum duration requirements to prevent flapping.</p>\n<p><strong>Notification Delivery</strong>: Once an alert fires, delivering notifications reliably is surprisingly complex. External services (email, Slack, PagerDuty) can be temporarily unavailable, rate-limited, or return ambiguous error responses. The system must implement retry logic, exponential backoff, and failure detection without losing notifications or creating duplicates.</p>\n<h3 id=\"existing-solutions-comparison\">Existing Solutions Comparison</h3>\n<p>Understanding how established metrics systems approach these challenges provides valuable context for our architectural decisions. Three representative solutions illustrate different trade-offs and design philosophies.</p>\n<h4 id=\"prometheus-pull-based-open-source\">Prometheus: Pull-Based Open Source</h4>\n<p>Prometheus pioneered the modern metrics monitoring approach and remains the most influential open-source solution. Its design decisions reflect a philosophy prioritizing simplicity, reliability, and operational transparency.</p>\n<p><strong>Architecture Philosophy</strong>: Prometheus uses a <strong>pull-based</strong> model where the central server actively scrapes metrics from application endpoints. This inverts the traditional push model and provides several advantages: the monitoring system controls scraping frequency, can detect when applications become unreachable, and avoids overwhelming the central server with concurrent metric submissions.</p>\n<p><strong>Data Model</strong>: Prometheus uses a dimensional data model where each time series is identified by a metric name plus a set of key-value labels. This model is both powerful and dangerous — it enables flexible querying but makes cardinality explosions easy to create accidentally.</p>\n<p><strong>Storage Engine</strong>: The Prometheus storage engine (TSDB) uses a custom file format optimized for time-series data. Data is organized into time-based blocks, each containing compressed chunks of time series data plus indexes for efficient querying. The block-based approach enables efficient compaction and retention policy enforcement.</p>\n<p><strong>Query Language</strong>: PromQL (Prometheus Query Language) is a functional query language designed specifically for time-series data. It supports complex aggregations, mathematical operations, and time-based functions. However, PromQL has a steep learning curve and can produce queries that accidentally consume excessive resources.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Pull Model</strong></td>\n<td>Service discovery integration, failure detection, backpressure control</td>\n<td>Requires network connectivity to all targets, complex NAT/firewall traversal</td>\n</tr>\n<tr>\n<td><strong>Local Storage</strong></td>\n<td>Simple deployment, no external dependencies, predictable performance</td>\n<td>Limited scalability, no built-in high availability, single point of failure</td>\n</tr>\n<tr>\n<td><strong>PromQL</strong></td>\n<td>Powerful aggregations, mathematical flexibility, time-series specific operations</td>\n<td>Complex syntax, easy to write inefficient queries, limited statistical functions</td>\n</tr>\n<tr>\n<td><strong>Alertmanager</strong></td>\n<td>Sophisticated routing and grouping, silence management, template system</td>\n<td>Complex configuration, separate component to manage, limited notification channels</td>\n</tr>\n</tbody></table>\n<p><strong>Scalability Limitations</strong>: Prometheus is designed for single-node deployment with federation for multi-datacenter scenarios. A single Prometheus server can handle millions of time series, but scaling beyond that requires complex federation topologies or third-party solutions like Thanos or Cortex.</p>\n<h4 id=\"influxdb-purpose-built-time-series-database\">InfluxDB: Purpose-Built Time-Series Database</h4>\n<p>InfluxDB represents a different approach: building a complete database system optimized specifically for time-series workloads. This allows more sophisticated features but introduces additional complexity.</p>\n<p><strong>Architecture Philosophy</strong>: InfluxDB uses a <strong>push-based</strong> model where applications send metrics directly to the database. This approach scales more naturally for high-throughput scenarios but requires careful backpressure and rate limiting design.</p>\n<p><strong>Data Model</strong>: InfluxDB&#39;s data model includes measurements (metric names), tags (indexed labels), fields (actual values), and timestamps. The distinction between tags and fields is important — tags are indexed and support efficient filtering, while fields store the actual metric values and support mathematical operations.</p>\n<p><strong>Storage Engine</strong>: InfluxDB uses the TSM (Time-Structured Merge tree) storage engine, which combines concepts from LSM-trees with time-series-specific optimizations. Data is initially written to a WAL (Write-Ahead Log), then organized into TSM files with sophisticated compression and indexing.</p>\n<p><strong>Query Language</strong>: InfluxQL is SQL-like, making it more familiar to developers than PromQL. However, SQL wasn&#39;t designed for time-series operations, so some queries feel awkward or require complex syntax.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Push Model</strong></td>\n<td>Better for high-throughput scenarios, works behind firewalls/NAT, simpler network topology</td>\n<td>Requires backpressure handling, harder to detect application failures, potential overload scenarios</td>\n</tr>\n<tr>\n<td><strong>SQL-like Query Language</strong></td>\n<td>Familiar syntax for most developers, rich statistical functions, flexible data manipulation</td>\n<td>Some time-series operations feel unnatural, complex joins for multi-metric queries</td>\n</tr>\n<tr>\n<td><strong>Clustered Architecture</strong></td>\n<td>Built-in horizontal scaling, high availability, automatic data distribution</td>\n<td>Complex cluster management, eventual consistency trade-offs, higher operational overhead</td>\n</tr>\n<tr>\n<td><strong>Schema Flexibility</strong></td>\n<td>Dynamic schema creation, multiple field types per measurement, flexible tag structures</td>\n<td>Easy to create inefficient schemas, tag/field distinction confusion, index bloat potential</td>\n</tr>\n</tbody></table>\n<p><strong>Commercial Focus</strong>: InfluxData&#39;s business model centers on InfluxDB Enterprise and InfluxDB Cloud, with the open-source version having limitations in clustering and advanced features. This affects the development priority of community-requested features.</p>\n<h4 id=\"datadog-cloud-native-saas\">DataDog: Cloud-Native SaaS</h4>\n<p>DataDog represents the fully managed SaaS approach, where the metrics system complexity is hidden behind a service API. This illustrates how cloud providers solve scalability challenges that are difficult for individual organizations.</p>\n<p><strong>Architecture Philosophy</strong>: DataDog operates a <strong>multi-tenant</strong> metrics platform serving thousands of customers from shared infrastructure. This requires sophisticated resource isolation, security controls, and cost allocation mechanisms that single-tenant systems don&#39;t need.</p>\n<p><strong>Data Ingestion</strong>: DataDog supports multiple ingestion methods (StatsD, HTTP API, agent-based collection) and handles the complexity of routing, validation, and processing at massive scale. They can absorb traffic spikes and provide backpressure without customers needing to design these systems.</p>\n<p><strong>Storage and Retention</strong>: DataDog automatically manages retention policies, downsampling, and storage optimization. Customers specify retention requirements through pricing tiers rather than configuring technical parameters.</p>\n<p><strong>Alerting and Dashboards</strong>: The platform provides sophisticated alerting with machine learning-based anomaly detection, automatic correlation analysis, and integrated incident management workflows.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Advantages</th>\n<th>Disadvantages</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Managed Infrastructure</strong></td>\n<td>No operational overhead, automatic scaling, built-in high availability</td>\n<td>Vendor lock-in, limited customization, ongoing subscription costs</td>\n</tr>\n<tr>\n<td><strong>Advanced Analytics</strong></td>\n<td>Machine learning features, anomaly detection, correlation analysis</td>\n<td>Less control over algorithms, potential false positives, black-box analysis</td>\n</tr>\n<tr>\n<td><strong>Integrated Ecosystem</strong></td>\n<td>Unified logging/metrics/tracing, pre-built integrations, collaborative features</td>\n<td>Expensive for high-volume scenarios, feature coupling, data export limitations</td>\n</tr>\n<tr>\n<td><strong>Enterprise Features</strong></td>\n<td>SSO integration, audit trails, role-based access, compliance certifications</td>\n<td>Overkill for simple use cases, complex pricing models, feature overwhelming</td>\n</tr>\n</tbody></table>\n<p><strong>Cost Considerations</strong>: SaaS metrics platforms typically charge based on the number of custom metrics, data retention periods, and advanced feature usage. For high-cardinality applications, monthly costs can quickly reach thousands of dollars, making self-hosted solutions economically attractive.</p>\n<blockquote>\n<p><strong>Decision: Architecture Philosophy</strong></p>\n<ul>\n<li><strong>Context</strong>: We must choose between pull-based (Prometheus-style), push-based (InfluxDB-style), or hybrid approaches for metric collection</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Pure pull-based with service discovery</li>\n<li>Pure push-based with client libraries</li>\n<li>Hybrid supporting both models</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach supporting both push and pull models</li>\n<li><strong>Rationale</strong>: Different applications have different constraints — containerized services work well with pull models, while batch jobs and edge devices need push capabilities. Supporting both provides maximum flexibility for adoption.</li>\n<li><strong>Consequences</strong>: Increases implementation complexity but provides better real-world usability. Requires designing consistent APIs for both ingestion methods.</li>\n</ul>\n</blockquote>\n<p>The comparison reveals that each approach optimizes for different priorities: operational simplicity (Prometheus), database sophistication (InfluxDB), or comprehensive managed services (DataDog). Our system will need to make similar trade-offs based on the specific requirements and constraints we&#39;re targeting.</p>\n<p>Understanding these existing solutions helps us identify proven patterns worth adopting and pitfalls worth avoiding. However, building our own system allows us to make different trade-offs and potentially improve on areas where existing solutions have limitations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The context and challenges described above inform several critical technology choices for our implementation. Understanding the problem space deeply allows us to select appropriate tools and avoid common architectural mistakes.</p>\n<h4 id=\"technology-stack-recommendations\">Technology Stack Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP Server</strong></td>\n<td><code>net/http</code> with <code>gorilla/mux</code></td>\n<td><code>gin-gonic/gin</code> or <code>echo</code></td>\n<td>Standard library provides everything needed; frameworks add convenience but dependency overhead</td>\n</tr>\n<tr>\n<td><strong>Storage Format</strong></td>\n<td>JSON files with <code>encoding/json</code></td>\n<td>Protocol Buffers with <code>google.golang.org/protobuf</code></td>\n<td>JSON is human-readable for debugging; protobuf provides better performance and schema evolution</td>\n</tr>\n<tr>\n<td><strong>Time Series Storage</strong></td>\n<td>File-based with <code>os</code> package</td>\n<td>Embedded <code>badger</code> or <code>bbolt</code> database</td>\n<td>Files are simple and debuggable; embedded DBs provide better concurrency and consistency</td>\n</tr>\n<tr>\n<td><strong>Background Processing</strong></td>\n<td><code>time.Ticker</code> with goroutines</td>\n<td><code>robfig/cron</code> for scheduling</td>\n<td>Simple ticker adequate for basic needs; cron expressions provide more flexible scheduling</td>\n</tr>\n<tr>\n<td><strong>Configuration</strong></td>\n<td><code>flag</code> package</td>\n<td><code>spf13/viper</code> with YAML/TOML</td>\n<td>Command-line flags are simple; structured config files scale better for complex systems</td>\n</tr>\n</tbody></table>\n<h4 id=\"project-structure-foundation\">Project Structure Foundation</h4>\n<p>Organize the codebase to support the four major milestones while maintaining clear separation of concerns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>metrics-system/\n├── cmd/\n│   ├── server/main.go              ← HTTP server entry point\n│   └── query/main.go               ← Query CLI tool for testing\n├── internal/\n│   ├── ingestion/                  ← Milestone 1: Metrics Collection\n│   │   ├── handler.go              ← HTTP handlers for push API\n│   │   ├── scraper.go              ← Pull-based metric scraping\n│   │   ├── validator.go            ← Label validation and cardinality control\n│   │   └── processor.go            ← Metric type processing pipeline\n│   ├── storage/                    ← Milestone 2: Storage &amp; Querying\n│   │   ├── engine.go               ← Time-series storage engine\n│   │   ├── index.go                ← Label indexing and lookup\n│   │   ├── compaction.go           ← Background compaction process\n│   │   └── retention.go            ← Data retention policy enforcement\n│   ├── query/                      ← Milestone 2: Query processing\n│   │   ├── parser.go               ← Query language parsing\n│   │   ├── executor.go             ← Query execution engine\n│   │   └── aggregation.go          ← Aggregation function library\n│   ├── dashboard/                  ← Milestone 3: Visualization\n│   │   ├── server.go               ← Web dashboard HTTP handlers\n│   │   ├── websocket.go            ← Real-time data updates\n│   │   └── renderer.go             ← Chart generation logic\n│   └── alerting/                   ← Milestone 4: Alerting System\n│       ├── evaluator.go            ← Alert rule evaluation engine\n│       ├── state.go                ← Alert state management\n│       └── notifier.go             ← Notification delivery system\n├── pkg/                            ← Public APIs and shared types\n│   ├── types/                      ← Core data structures\n│   │   ├── metric.go               ← Metric, Sample, Label definitions\n│   │   ├── query.go                ← Query request/response types\n│   │   └── alert.go                ← Alert rule and state types\n│   └── client/                     ← Client library for metric submission\n│       └── client.go               ← HTTP client for push API\n├── web/                            ← Dashboard static assets\n│   ├── static/                     ← CSS, JavaScript, images\n│   └── templates/                  ← HTML templates\n└── testdata/                       ← Sample metrics and test fixtures\n    ├── metrics/                    ← Example metric data files\n    └── configs/                    ← Sample configuration files</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>HTTP Server Foundation</strong> (<code>internal/server/server.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> server</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/mux</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Server wraps the HTTP server with graceful shutdown support</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Server</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpServer </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">mux</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Router</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewServer creates a new HTTP server with default middleware</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">addr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> mux.</span><span style=\"color:#B392F0\">NewRouter</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add basic middleware</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">Use</span><span style=\"color:#E1E4E8\">(loggingMiddleware)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router.</span><span style=\"color:#B392F0\">Use</span><span style=\"color:#E1E4E8\">(corsMiddleware)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        httpServer: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Addr:         addr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Handler:      router,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            IdleTimeout:  </span><span style=\"color:#79B8FF\">120</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        router: router,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterRoutes adds all API endpoints to the router</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterRoutes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ingestionHandler</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryHandler</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">dashboardHandler</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Metrics ingestion endpoints</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">PathPrefix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/metrics\"</span><span style=\"color:#E1E4E8\">).</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">(ingestionHandler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Query API endpoints  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">PathPrefix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/query\"</span><span style=\"color:#E1E4E8\">).</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">(queryHandler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Dashboard and static assets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.router.</span><span style=\"color:#B392F0\">PathPrefix</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/\"</span><span style=\"color:#E1E4E8\">).</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">(dashboardHandler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins serving HTTP requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Starting HTTP server on </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, s.httpServer.Addr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Shutdown gracefully stops the server</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Shutting down HTTP server...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> loggingMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log.</span><span style=\"color:#B392F0\">Printf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#79B8FF\"> %s</span><span style=\"color:#79B8FF\"> %v</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, r.Method, r.URL.Path, time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> corsMiddleware</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">next</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Access-Control-Allow-Origin\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"*\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Access-Control-Allow-Methods\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"GET, POST, PUT, DELETE, OPTIONS\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Access-Control-Allow-Headers\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> r.Method </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"OPTIONS\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusOK)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        next.</span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(w, r)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Data Types</strong> (<code>pkg/types/metric.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> types</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricType represents the type of metric being stored</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricType</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeCounter</span><span style=\"color:#B392F0\"> MetricType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeGauge</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeHistogram</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String returns the string representation of the metric type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mt </span><span style=\"color:#B392F0\">MetricType</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> mt {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> MetricTypeCounter:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"counter\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> MetricTypeGauge:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"gauge\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> MetricTypeHistogram:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"histogram\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels represents a set of key-value pairs attached to a metric</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String returns a canonical string representation of labels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(l) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"{}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Sort keys for consistent output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    keys </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(l))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> l {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        keys </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(keys, k)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sort.</span><span style=\"color:#B392F0\">Strings</span><span style=\"color:#E1E4E8\">(keys)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parts </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(l))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, k </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> keys {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(parts, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">=\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"`</span><span style=\"color:#E1E4E8\">, k, l[k]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"{\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(parts, </span><span style=\"color:#9ECBFF\">\",\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Copy creates a deep copy of the labels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Copy</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(l))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> l {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Sample represents a single data point in a time series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Sample</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Metric represents a complete metric with its metadata and samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Metric</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">     `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type    </span><span style=\"color:#B392F0\">MetricType</span><span style=\"color:#9ECBFF\"> `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels  </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#9ECBFF\">     `json:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Samples []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#9ECBFF\">   `json:\"samples\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Help    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">     `json:\"help,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesID generates a unique identifier for this metric's time series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SeriesID</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> m.Name </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> m.Labels.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks if the metric has valid values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> m.Name </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"metric name cannot be empty\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(m.Samples) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"metric must have at least one sample\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Validate label keys and values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> m.Labels {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> key </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"label key cannot be empty\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">HasPrefix</span><span style=\"color:#E1E4E8\">(key, </span><span style=\"color:#9ECBFF\">\"__\"</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"label key </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: keys starting with __ are reserved\"</span><span style=\"color:#E1E4E8\">, key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(value) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"label value for key </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> exceeds maximum length\"</span><span style=\"color:#E1E4E8\">, key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Milestone 1 (Metrics Collection):</strong>\nRun the following to verify ingestion is working:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start the server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Send a test metric</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/metrics</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"name\": \"http_requests_total\",</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"type\": \"counter\", </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"labels\": {\"method\": \"GET\", \"endpoint\": \"/api\"},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"samples\": [{\"value\": 42, \"timestamp\": \"2023-01-01T12:00:00Z\"}]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  }'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should receive 200 OK response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check server logs for ingestion confirmation</span></span></code></pre></div>\n\n<p><strong>After Milestone 2 (Storage &amp; Querying):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Query the stored metric</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/query?metric=http_requests_total&#x26;start=2023-01-01T11:00:00Z&#x26;end=2023-01-01T13:00:00Z\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should return JSON with the stored sample</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify data persists across server restarts</span></span></code></pre></div>\n\n<h4 id=\"common-debugging-issues\">Common Debugging Issues</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP 500 on metric ingestion</strong></td>\n<td>JSON parsing error or validation failure</td>\n<td>Check request body format and server logs</td>\n<td>Validate JSON structure and required fields</td>\n</tr>\n<tr>\n<td><strong>Metrics disappear after restart</strong></td>\n<td>In-memory storage without persistence</td>\n<td>Check if data is being written to disk</td>\n<td>Implement file-based storage in storage engine</td>\n</tr>\n<tr>\n<td><strong>Query returns empty results</strong></td>\n<td>Label matching issues or wrong time range</td>\n<td>Verify metric name and labels match exactly</td>\n<td>Add debug logging to query execution</td>\n</tr>\n<tr>\n<td><strong>High memory usage</strong></td>\n<td>Unbounded metric storage or label cardinality explosion</td>\n<td>Monitor number of unique time series</td>\n<td>Implement retention policies and label validation</td>\n</tr>\n</tbody></table>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this section establishes the foundational scope and boundaries for the entire project)</p>\n</blockquote>\n<h3 id=\"the-goldilocks-principle-for-system-design\">The Goldilocks Principle for System Design</h3>\n<p>Building a metrics and alerting system is like designing a security system for a large office building. You need to balance comprehensive coverage with practical constraints. Install too few sensors and you miss critical events. Install too many and you&#39;re overwhelmed with false alarms, maintenance costs, and complexity. The key is finding the &quot;just right&quot; balance that meets your actual needs without overengineering.</p>\n<p>This section establishes clear boundaries for what our system will and will not do. These boundaries are crucial because metrics systems can easily become sprawling, complex beasts that try to solve every observability problem under the sun. By explicitly defining our scope, we prevent feature creep and ensure we build a focused, reliable system that excels at its core mission.</p>\n<p>The goals we set here directly drive every architectural decision throughout the project. When we face trade-offs between simplicity and features, performance and functionality, or immediate needs versus future flexibility, we&#39;ll return to these goals as our north star.</p>\n<h3 id=\"functional-goals\">Functional Goals</h3>\n<p>These are the core capabilities our system must provide to be considered successful. Each goal maps directly to one or more project milestones and represents functionality that users will interact with directly.</p>\n<h4 id=\"metric-collection-and-storage\">Metric Collection and Storage</h4>\n<p>Our system must reliably collect, validate, and store three fundamental metric types that form the foundation of observability. <strong>Counters</strong> track cumulative values that only increase over time, such as total HTTP requests or bytes processed. <strong>Gauges</strong> capture point-in-time values that can fluctuate up or down, like current memory usage or active connections. <strong>Histograms</strong> bucket observations into configurable ranges to analyze distributions, enabling percentile calculations for latency or request size analysis.</p>\n<p>The system must support both push-based and pull-based metric collection models. In the push model, applications actively send metrics to our collection endpoints, providing immediate data delivery and working well with dynamic environments like containers. In the pull model, our system scrapes metrics from application endpoints, offering better control over collection timing and reducing load on applications.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Type</th>\n<th>Purpose</th>\n<th>Example Use Cases</th>\n<th>Key Characteristics</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Counter</td>\n<td>Track cumulative events</td>\n<td>HTTP requests, errors, bytes sent</td>\n<td>Monotonically increasing, reset on restart</td>\n</tr>\n<tr>\n<td>Gauge</td>\n<td>Capture current state</td>\n<td>Memory usage, queue depth, temperature</td>\n<td>Can increase or decrease, point-in-time value</td>\n</tr>\n<tr>\n<td>Histogram</td>\n<td>Analyze distributions</td>\n<td>Request latency, response size</td>\n<td>Buckets observations, tracks count and sum</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Support Multi-Dimensional Metrics</strong></p>\n<ul>\n<li><strong>Context</strong>: Modern applications need to slice metrics by various dimensions (service, region, user type)</li>\n<li><strong>Options Considered</strong>: Flat metrics with no labels, limited predefined dimensions, flexible label system</li>\n<li><strong>Decision</strong>: Implement flexible label system similar to Prometheus</li>\n<li><strong>Rationale</strong>: Labels enable powerful querying and aggregation while maintaining storage efficiency through shared time series</li>\n<li><strong>Consequences</strong>: Adds complexity to storage and indexing but provides essential functionality for real-world use</li>\n</ul>\n</blockquote>\n<h4 id=\"time-series-data-management\">Time-Series Data Management</h4>\n<p>The system must efficiently store millions of time-series data points with high write throughput while maintaining fast query performance. Time-series data has unique characteristics: it&#39;s append-only, timestamp-ordered, and often exhibits predictable patterns that enable compression.</p>\n<p>Data retention policies automatically manage the lifecycle of stored metrics. Recent data (last 24 hours) remains at full resolution for detailed analysis. Older data gets progressively downsampled to reduce storage costs while preserving long-term trends. Data older than the configured retention period gets automatically deleted.</p>\n<table>\n<thead>\n<tr>\n<th>Data Age</th>\n<th>Resolution</th>\n<th>Purpose</th>\n<th>Storage Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>0-24 hours</td>\n<td>Original (15s intervals)</td>\n<td>Real-time monitoring</td>\n<td>High storage, fast queries</td>\n</tr>\n<tr>\n<td>1-7 days</td>\n<td>1 minute averages</td>\n<td>Recent troubleshooting</td>\n<td>Medium storage</td>\n</tr>\n<tr>\n<td>1-4 weeks</td>\n<td>5 minute averages</td>\n<td>Trend analysis</td>\n<td>Low storage</td>\n</tr>\n<tr>\n<td>&gt; 1 month</td>\n<td>Deleted or archived</td>\n<td>Compliance only</td>\n<td>Minimal storage</td>\n</tr>\n</tbody></table>\n<p>Compaction processes run automatically in the background, merging small data files into larger, more efficient structures. This reduces the number of files that queries must read and improves compression ratios by processing larger data blocks together.</p>\n<h4 id=\"query-and-visualization-capabilities\">Query and Visualization Capabilities</h4>\n<p>The system must provide a query language that allows users to select metrics by name and labels, specify time ranges, and apply aggregation functions. Queries should feel natural to operations teams familiar with tools like Prometheus or SQL.</p>\n<p>Basic aggregation functions include sum, average, maximum, minimum, and rate calculations over time windows. Rate functions are particularly important for converting counter metrics into meaningful rates (requests per second, error rate percentages).</p>\n<table>\n<thead>\n<tr>\n<th>Function</th>\n<th>Purpose</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>sum()</code></td>\n<td>Total across series</td>\n<td>Multiple time series</td>\n<td>Single aggregated series</td>\n</tr>\n<tr>\n<td><code>avg()</code></td>\n<td>Average across series</td>\n<td>Multiple time series</td>\n<td>Single averaged series</td>\n</tr>\n<tr>\n<td><code>rate()</code></td>\n<td>Change rate over time</td>\n<td>Counter series</td>\n<td>Rate per second</td>\n</tr>\n<tr>\n<td><code>quantile()</code></td>\n<td>Percentile calculation</td>\n<td>Histogram series</td>\n<td>Percentile value series</td>\n</tr>\n</tbody></table>\n<p>The web dashboard provides visual representations of metric data through line charts, bar charts, and heatmaps. Dashboard configurations persist as JSON documents, enabling sharing and version control. Real-time updates refresh charts automatically without requiring manual page reloads.</p>\n<h4 id=\"alerting-and-notification-system\">Alerting and Notification System</h4>\n<p>Alert rules define conditions that trigger notifications when metric values cross thresholds or exhibit anomalous behavior. Each rule specifies a metric query, comparison operator, threshold value, and evaluation interval.</p>\n<p>Alert state management handles transitions between inactive, pending, firing, and resolved states. Alerts enter pending state when conditions are first met, then escalate to firing after persisting for the configured duration. This prevents noisy alerts from temporary spikes.</p>\n<table>\n<thead>\n<tr>\n<th>Alert State</th>\n<th>Trigger Condition</th>\n<th>Actions Taken</th>\n<th>Next States</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inactive</td>\n<td>Normal metric values</td>\n<td>None</td>\n<td>Pending</td>\n</tr>\n<tr>\n<td>Pending</td>\n<td>Threshold crossed</td>\n<td>Start timer</td>\n<td>Inactive, Firing</td>\n</tr>\n<tr>\n<td>Firing</td>\n<td>Duration exceeded</td>\n<td>Send notifications</td>\n<td>Inactive, Resolved</td>\n</tr>\n<tr>\n<td>Resolved</td>\n<td>Condition cleared</td>\n<td>Send resolution notice</td>\n<td>Inactive</td>\n</tr>\n</tbody></table>\n<p>Notification channels deliver alert messages through multiple integrations: email, Slack, PagerDuty, and generic webhooks. Message templates allow customization of alert content with metric values, labels, and contextual information.</p>\n<h3 id=\"non-functional-goals\">Non-Functional Goals</h3>\n<p>These requirements define how well the system performs its functional capabilities. They establish the operational characteristics needed for production deployment and long-term success.</p>\n<h4 id=\"performance-requirements\">Performance Requirements</h4>\n<p>The metrics ingestion system must handle sustained write loads of 10,000 samples per second on modest hardware (4 CPU cores, 8GB RAM). This throughput supports monitoring for hundreds of applications or services in a medium-sized organization.</p>\n<p>Query response times should remain under 5 seconds for dashboard refreshes and under 30 seconds for complex analytical queries. These targets ensure dashboards feel responsive while accommodating the inherent costs of aggregating large time-series datasets.</p>\n<table>\n<thead>\n<tr>\n<th>Workload Type</th>\n<th>Target Performance</th>\n<th>Resource Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric ingestion</td>\n<td>10,000 samples/sec</td>\n<td>4 CPU cores, 8GB RAM</td>\n</tr>\n<tr>\n<td>Dashboard queries</td>\n<td>&lt; 5 seconds</td>\n<td>Standard SSD storage</td>\n</tr>\n<tr>\n<td>Analytical queries</td>\n<td>&lt; 30 seconds</td>\n<td>Network bandwidth &lt; 1Gbps</td>\n</tr>\n<tr>\n<td>Alert evaluation</td>\n<td>&lt; 60 seconds</td>\n<td>Configurable intervals</td>\n</tr>\n</tbody></table>\n<p>Storage efficiency targets aim for 80% compression ratios compared to raw metric data. Time-series data compresses well due to temporal locality and predictable patterns, making this target achievable with proper encoding techniques.</p>\n<h4 id=\"reliability-requirements\">Reliability Requirements</h4>\n<p>The system must maintain 99.9% uptime, allowing for planned maintenance windows and occasional failures. This translates to less than 9 hours of downtime per year, appropriate for internal monitoring systems that aren&#39;t customer-facing.</p>\n<p>Data durability guarantees ensure that metric data, once acknowledged as received, survives single-node failures. This requires synchronous persistence to disk before sending acknowledgments, with optional replication for higher availability environments.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Availability Target</th>\n<th>Recovery Time</th>\n<th>Data Loss Tolerance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion API</td>\n<td>99.9%</td>\n<td>&lt; 5 minutes</td>\n<td>Zero (acknowledged data)</td>\n</tr>\n<tr>\n<td>Query API</td>\n<td>99.5%</td>\n<td>&lt; 10 minutes</td>\n<td>Read-only degradation acceptable</td>\n</tr>\n<tr>\n<td>Dashboard</td>\n<td>99.5%</td>\n<td>&lt; 10 minutes</td>\n<td>Cached data acceptable</td>\n</tr>\n<tr>\n<td>Alerting</td>\n<td>99.9%</td>\n<td>&lt; 2 minutes</td>\n<td>Critical for incident response</td>\n</tr>\n</tbody></table>\n<p>Alert delivery reliability requires redundant notification channels and retry mechanisms. Critical alerts should reach on-call personnel even if primary notification channels fail.</p>\n<h4 id=\"scalability-requirements\">Scalability Requirements</h4>\n<p>The system must scale vertically on single nodes before requiring distributed deployment. This simplifies operations and reduces complexity for organizations that don&#39;t need web-scale infrastructure.</p>\n<p>Storage scalability targets support 1TB of metric data on standard hardware, sufficient for monitoring medium-sized infrastructures. Beyond this threshold, organizations typically invest in dedicated time-series databases or cloud services.</p>\n<table>\n<thead>\n<tr>\n<th>Resource</th>\n<th>Scaling Target</th>\n<th>Growth Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU utilization</td>\n<td>&lt; 70% average</td>\n<td>Vertical scaling first</td>\n</tr>\n<tr>\n<td>Memory usage</td>\n<td>&lt; 80% available</td>\n<td>Efficient data structures</td>\n</tr>\n<tr>\n<td>Storage capacity</td>\n<td>1TB total</td>\n<td>Automatic compaction/retention</td>\n</tr>\n<tr>\n<td>Network bandwidth</td>\n<td>&lt; 50% link capacity</td>\n<td>Compression and batching</td>\n</tr>\n</tbody></table>\n<p>Horizontal scaling preparation means avoiding architectural choices that prevent future distribution. While the initial implementation runs on single nodes, the design should accommodate future sharding or federation.</p>\n<h4 id=\"operational-requirements\">Operational Requirements</h4>\n<p>The system must provide comprehensive observability into its own operation through self-monitoring metrics, structured logging, and health check endpoints. Operations teams need visibility into ingestion rates, storage usage, query performance, and alert evaluation timing.</p>\n<p>Configuration management uses file-based configuration with hot-reload capabilities for non-disruptive updates. This enables GitOps workflows where configuration changes flow through version control and automated deployment pipelines.</p>\n<table>\n<thead>\n<tr>\n<th>Operational Aspect</th>\n<th>Requirement</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Self-monitoring</td>\n<td>All components instrumented</td>\n<td>Internal metrics exposure</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Hot-reload support</td>\n<td>File watching mechanisms</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Structured JSON logs</td>\n<td>Configurable log levels</td>\n</tr>\n<tr>\n<td>Health checks</td>\n<td>HTTP endpoints</td>\n<td>Dependency verification</td>\n</tr>\n</tbody></table>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>These are capabilities we explicitly choose not to build, either due to complexity constraints, timeline limitations, or because better solutions exist elsewhere. Defining non-goals is as important as defining goals because it prevents scope creep and helps resist the temptation to build unnecessary features.</p>\n<h4 id=\"advanced-analytics-and-machine-learning\">Advanced Analytics and Machine Learning</h4>\n<p>Our system will not include anomaly detection algorithms, predictive analytics, or machine learning-based alerting. These capabilities require significant expertise in data science, large amounts of historical data for training, and ongoing model maintenance.</p>\n<blockquote>\n<p>The complexity of implementing reliable machine learning features far exceeds the value for most monitoring use cases. Traditional threshold-based alerting handles 90% of operational needs effectively.</p>\n</blockquote>\n<p>Organizations needing advanced analytics should integrate dedicated ML platforms or cloud services that specialize in these capabilities. Our system can export data to these platforms through standard APIs.</p>\n<h4 id=\"multi-tenancy-and-access-control\">Multi-Tenancy and Access Control</h4>\n<p>The system will not implement user authentication, role-based access control, or tenant isolation. These features add significant complexity to every component and require expertise in security architecture.</p>\n<p>For production deployments requiring access control, the system should be deployed behind reverse proxies or API gateways that handle authentication and authorization. This separation of concerns keeps our system focused on metrics while leveraging specialized security tools.</p>\n<h4 id=\"distributed-deployment-and-high-availability\">Distributed Deployment and High Availability</h4>\n<p>The initial implementation will not support clustering, replication, or distributed deployment. Building reliable distributed systems requires expertise in consensus algorithms, failure detection, and data consistency that would dominate the project scope.</p>\n<p>Organizations requiring high availability should deploy multiple independent instances or use cloud-managed time-series databases for critical production workloads. Our system focuses on being an excellent single-node solution.</p>\n<h4 id=\"log-aggregation-and-tracing-integration\">Log Aggregation and Tracing Integration</h4>\n<p>While metrics are one pillar of observability, we will not build log aggregation or distributed tracing capabilities. These domains have different data models, storage requirements, and query patterns that warrant dedicated systems.</p>\n<p>The system should provide integration points (webhook notifications, metric export APIs) that allow correlation with external logging and tracing systems, but will not attempt to replace specialized tools like ELK stack or Jaeger.</p>\n<h4 id=\"advanced-visualization-and-dashboarding\">Advanced Visualization and Dashboarding</h4>\n<p>We will not build advanced chart types (3D visualizations, geographic maps, complex statistical plots) or dashboard features like annotation, collaboration, or advanced templating. These features require significant frontend development effort and compete with established visualization platforms.</p>\n<p>The dashboard will focus on core time-series visualizations (line charts, bar charts, simple heatmaps) that handle 80% of monitoring use cases. Users needing advanced visualizations should export data to specialized tools like Grafana or custom applications.</p>\n<table>\n<thead>\n<tr>\n<th>Non-Goal Category</th>\n<th>Rationale</th>\n<th>Alternative Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Machine Learning</td>\n<td>Complexity exceeds project scope</td>\n<td>Integration with ML platforms</td>\n</tr>\n<tr>\n<td>Multi-tenancy</td>\n<td>Security complexity too high</td>\n<td>Deploy behind auth proxy</td>\n</tr>\n<tr>\n<td>Distributed Systems</td>\n<td>Requires distributed systems expertise</td>\n<td>Multiple independent instances</td>\n</tr>\n<tr>\n<td>Log Aggregation</td>\n<td>Different data model and requirements</td>\n<td>Use dedicated logging platforms</td>\n</tr>\n<tr>\n<td>Advanced Visualization</td>\n<td>Frontend complexity</td>\n<td>Export to Grafana or similar</td>\n</tr>\n</tbody></table>\n<h4 id=\"performance-beyond-single-node-limits\">Performance Beyond Single-Node Limits</h4>\n<p>We will not optimize for ingestion rates exceeding 100,000 samples per second or storage requirements beyond 10TB. These requirements indicate need for specialized time-series databases or distributed systems.</p>\n<p>The design should avoid architectural choices that prevent future scaling, but will not implement complex optimization techniques like custom storage engines, advanced indexing structures, or distributed query processing.</p>\n<h4 id=\"complex-query-languages-and-analytics\">Complex Query Languages and Analytics</h4>\n<p>The query language will not support joins between different metric series, complex mathematical functions, or SQL-like analytical queries. These features require query optimization engines and add significant implementation complexity.</p>\n<p>Users needing complex analytics should export metric data to dedicated analytics platforms or data warehouses that provide full SQL support and analytical processing capabilities.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: By explicitly limiting scope, we can build a robust, reliable system that excels at core metrics functionality rather than a complex system that attempts everything but does nothing well.</p>\n</blockquote>\n<h3 id=\"success-criteria-and-acceptance\">Success Criteria and Acceptance</h3>\n<p>Success will be measured by the system&#39;s ability to handle realistic monitoring workloads for development and staging environments. A successful implementation should enable a small operations team to monitor a few dozen applications and services effectively.</p>\n<p>The system succeeds if it can replace simple monitoring solutions like basic Prometheus setups while providing better usability and integration capabilities. It should feel natural to operations engineers familiar with modern monitoring tools.</p>\n<table>\n<thead>\n<tr>\n<th>Success Metric</th>\n<th>Target Value</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric ingestion rate</td>\n<td>5,000+ samples/sec sustained</td>\n<td>Load testing with realistic data</td>\n</tr>\n<tr>\n<td>Query response time</td>\n<td>95th percentile &lt; 10 seconds</td>\n<td>Dashboard usage simulation</td>\n</tr>\n<tr>\n<td>Storage efficiency</td>\n<td>&gt; 70% compression ratio</td>\n<td>Compare raw vs compressed sizes</td>\n</tr>\n<tr>\n<td>Alert reliability</td>\n<td>99%+ delivery success</td>\n<td>Notification tracking logs</td>\n</tr>\n</tbody></table>\n<p>The ultimate test is whether the system provides value in real monitoring scenarios: detecting actual incidents, enabling root cause analysis through dashboard exploration, and reducing time to resolution for operational issues.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-foundation\">Technology Foundation</h4>\n<p>The system builds on proven technologies that balance simplicity with capability. These choices prioritize reliability and maintainability over cutting-edge features.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td><code>net/http</code> with standard middleware</td>\n<td>Fiber or Echo framework</td>\n</tr>\n<tr>\n<td>Storage Backend</td>\n<td>File-based with custom indexing</td>\n<td>Embedded database (BadgerDB)</td>\n</tr>\n<tr>\n<td>Data Serialization</td>\n<td>JSON for config, binary for metrics</td>\n<td>Protocol Buffers with compression</td>\n</tr>\n<tr>\n<td>Frontend Framework</td>\n<td>Vanilla JavaScript with Chart.js</td>\n<td>React or Vue.js SPA</td>\n</tr>\n</tbody></table>\n<h4 id=\"project-structure\">Project Structure</h4>\n<p>The codebase organization separates concerns clearly while maintaining simplicity for learning purposes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>metrics-dashboard/\n├── cmd/\n│   ├── server/main.go              ← HTTP server entry point\n│   └── tools/                      ← Data import/export utilities\n├── internal/\n│   ├── metrics/                    ← Core metric types and validation\n│   │   ├── types.go               ← MetricType, Labels, Sample structs\n│   │   └── validation.go          ← Validate() methods\n│   ├── ingestion/                 ← Metric collection engine\n│   │   ├── handlers.go            ← HTTP endpoint handlers\n│   │   ├── prometheus.go          ← Prometheus format support\n│   │   └── validation.go          ← Label cardinality control\n│   ├── storage/                   ← Time-series storage engine\n│   │   ├── engine.go              ← Main storage interface\n│   │   ├── compaction.go          ← Background compaction\n│   │   └── retention.go           ← Data lifecycle management\n│   ├── query/                     ← Query processing engine\n│   │   ├── parser.go              ← Query language parsing\n│   │   ├── executor.go            ← Query execution\n│   │   └── functions.go           ← Aggregation functions\n│   ├── dashboard/                 ← Web dashboard\n│   │   ├── server.go              ← Static file serving\n│   │   ├── api.go                 ← Dashboard API endpoints\n│   │   └── websocket.go           ← Real-time updates\n│   └── alerting/                  ← Alert evaluation and notification\n│       ├── evaluator.go           ← Alert rule evaluation\n│       ├── notifier.go            ← Notification delivery\n│       └── state.go               ← Alert state management\n├── web/                           ← Frontend assets\n│   ├── static/                    ← CSS, JavaScript, images\n│   └── templates/                 ← HTML templates\n├── config/                        ← Configuration files\n│   ├── server.yaml                ← Main server configuration\n│   └── alerts.yaml                ← Alert rule definitions\n└── docs/                          ← Documentation and examples</code></pre></div>\n\n<h4 id=\"core-type-definitions\">Core Type Definitions</h4>\n<p>These fundamental types establish the data model foundation that all components build upon:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package metrics defines core data structures for the metrics system.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> metrics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricType represents the type of metric being stored.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricType</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeCounter</span><span style=\"color:#B392F0\"> MetricType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeGauge</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeHistogram</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels represents key-value pairs attached to metrics for multi-dimensional querying.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String returns the canonical string representation of labels for storage keys.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract keys into slice and sort alphabetically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Build string in format \"key1=value1,key2=value2\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle special characters in keys/values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Copy creates a deep copy of the labels map.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Copy</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create new map with same capacity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Copy each key-value pair</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return new map</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Sample represents a single metric measurement at a point in time.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Sample</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Metric represents a complete metric with metadata and sample data.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Metric</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">     `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type    </span><span style=\"color:#B392F0\">MetricType</span><span style=\"color:#9ECBFF\"> `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels  </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#9ECBFF\">     `json:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Samples []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#9ECBFF\">   `json:\"samples\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesID returns a unique identifier for this metric's time series.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SeriesID</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Combine metric name with sorted labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create deterministic string representation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Consider using hash for very long IDs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks if the metric has valid fields and structure.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check metric name is non-empty and valid format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify metric type is valid enum value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate labels don't exceed cardinality limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check samples are sorted by timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate sample values for metric type (counters non-negative, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"http-server-foundation\">HTTP Server Foundation</h4>\n<p>The server infrastructure provides the foundation for all API endpoints with proper middleware and graceful shutdown:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Package server provides HTTP server infrastructure for the metrics system.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> server</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Server wraps an HTTP server with middleware and graceful shutdown capabilities.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Server</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpServer </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ServeMux</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewServer creates a new HTTP server listening on the specified address.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">addr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewServeMux</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Addr:         addr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Handler:      mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ReadTimeout:  </span><span style=\"color:#79B8FF\">15</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        WriteTimeout: </span><span style=\"color:#79B8FF\">15</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        IdleTimeout:  </span><span style=\"color:#79B8FF\">60</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        httpServer: server,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mux:        mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterRoutes adds API endpoint handlers to the server.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterRoutes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">handlers</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HandlerFunc</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Iterate through handler map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Register each route with middleware</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add logging and metrics middleware</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add CORS headers for browser requests</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins serving HTTP requests.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Log server startup information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start HTTP server</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle startup errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Shutdown performs graceful shutdown with timeout.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Log shutdown initiation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Stop accepting new connections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Wait for existing requests to complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Force close after timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing each milestone, verify the system works correctly with these concrete tests:</p>\n<p><strong>Milestone 1 - Metrics Collection:</strong></p>\n<ul>\n<li>Start the server: <code>go run cmd/server/main.go</code></li>\n<li>Send a counter metric: <code>curl -X POST localhost:8080/api/v1/metrics -d &#39;{&quot;name&quot;:&quot;http_requests&quot;,&quot;type&quot;:0,&quot;labels&quot;:{&quot;service&quot;:&quot;api&quot;},&quot;samples&quot;:[{&quot;value&quot;:1,&quot;timestamp&quot;:&quot;2024-01-01T12:00:00Z&quot;}]}&#39;</code></li>\n<li>Verify response: Should return 200 OK with no error message</li>\n<li>Check logs: Should show metric ingestion and validation messages</li>\n</ul>\n<p><strong>Milestone 2 - Storage &amp; Querying:</strong></p>\n<ul>\n<li>Query the stored metric: <code>curl &quot;localhost:8080/api/v1/query?metric=http_requests&amp;start=2024-01-01T11:00:00Z&amp;end=2024-01-01T13:00:00Z&quot;</code></li>\n<li>Verify response: Should return JSON with the stored sample</li>\n<li>Check storage files: Should see new files created in data directory</li>\n<li>Test aggregation: <code>curl &quot;localhost:8080/api/v1/query?metric=http_requests&amp;start=2024-01-01T11:00:00Z&amp;end=2024-01-01T13:00:00Z&amp;func=sum&quot;</code></li>\n</ul>\n<p><strong>Milestone 3 - Visualization Dashboard:</strong></p>\n<ul>\n<li>Open browser: Navigate to <code>http://localhost:8080/dashboard</code></li>\n<li>Create chart: Should see interface for adding metric queries</li>\n<li>View data: Should see line chart with the stored metrics</li>\n<li>Test auto-refresh: Chart should update automatically every 30 seconds</li>\n</ul>\n<p><strong>Milestone 4 - Alerting System:</strong></p>\n<ul>\n<li>Create alert rule: POST to <code>/api/v1/alerts</code> with threshold condition</li>\n<li>Trigger alert: Send metric values that exceed the threshold</li>\n<li>Verify notification: Check configured notification channel for alert message</li>\n<li>Test resolution: Send normal values and verify resolution notification</li>\n</ul>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this architectural foundation supports the entire metrics and alerting system)</p>\n</blockquote>\n<h3 id=\"the-orchestra-metaphor-understanding-system-architecture\">The Orchestra Metaphor: Understanding System Architecture</h3>\n<p>Think of a metrics and alerting system like a symphony orchestra preparing for and performing a concert. The <strong>Ingestion Engine</strong> is like the stage crew collecting sheet music from various composers (applications) and organizing it by instrument and section. The <strong>Storage Engine</strong> acts as the music library, carefully cataloging and preserving every piece of music with efficient indexing so any composition can be quickly retrieved. The <strong>Query Engine</strong> functions as the conductor&#39;s assistant, who can instantly find specific musical passages, transpose them to different keys, or create medleys by combining multiple pieces. The <strong>Dashboard</strong> serves as the concert hall&#39;s display system, showing the audience (operators) what&#39;s currently being performed with beautiful visualizations of the musical data. Finally, the <strong>Alerting System</strong> operates like the concert hall&#39;s fire safety system, continuously monitoring for problems and immediately notifying the right people when something goes wrong.</p>\n<p>Just as each orchestra section has distinct responsibilities but must coordinate seamlessly to create beautiful music, our metrics system components have clear ownership boundaries while collaborating through well-defined interfaces to provide comprehensive observability.</p>\n<h3 id=\"component-responsibilities\">Component Responsibilities</h3>\n<p>Our metrics and alerting system divides functionality across five major components, each with distinct responsibilities and clear interfaces. This separation of concerns enables independent development, testing, and scaling of each component while maintaining clean integration points.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"System Component Architecture\"></p>\n<p>The <strong>Ingestion Engine</strong> owns all aspects of receiving and preprocessing metrics data from external sources. It handles both push-based metrics (where applications actively send data to our system) and pull-based metrics (where our system scrapes metrics from application endpoints). The ingestion engine is responsible for validating incoming metric data, enforcing label cardinality limits to prevent explosion, normalizing metric names and label formats, and performing any necessary data transformations before storage. It also implements rate limiting and backpressure mechanisms to protect downstream components from being overwhelmed by metric data floods.</p>\n<table>\n<thead>\n<tr>\n<th>Component Responsibility</th>\n<th>Description</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Protocol Handling</td>\n<td>Accept HTTP POST requests with metric data, scrape Prometheus endpoints</td>\n<td><code>IngestMetrics(metrics []Metric) error</code>, <code>ScrapeEndpoint(url string) error</code></td>\n</tr>\n<tr>\n<td>Data Validation</td>\n<td>Verify metric types, validate label formats, enforce naming conventions</td>\n<td><code>ValidateMetric(metric Metric) error</code>, <code>CheckCardinality(labels Labels) error</code></td>\n</tr>\n<tr>\n<td>Preprocessing</td>\n<td>Normalize metric names, sanitize label values, apply transformations</td>\n<td><code>NormalizeMetric(metric Metric) Metric</code>, <code>SanitizeLabels(labels Labels) Labels</code></td>\n</tr>\n<tr>\n<td>Backpressure Control</td>\n<td>Implement rate limiting, queue management, graceful degradation</td>\n<td><code>CheckRateLimit(source string) bool</code>, <code>QueueMetric(metric Metric) error</code></td>\n</tr>\n</tbody></table>\n<p>The <strong>Storage Engine</strong> takes complete ownership of persisting time-series data efficiently and providing fast access patterns for both recent and historical data. It implements a sophisticated storage format optimized for time-series workloads, with separate handling for metadata (metric names and labels) and sample data (timestamps and values). The storage engine automatically manages data lifecycle through compaction processes that merge small files into larger ones and downsample older data to reduce storage requirements. It also enforces retention policies by automatically deleting data that exceeds configured age limits.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Responsibility</th>\n<th>Description</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sample Storage</td>\n<td>Persist timestamped metric values with efficient compression</td>\n<td><code>WriteSamples(samples []Sample) error</code>, <code>ReadSamples(query TimeRange) []Sample</code></td>\n</tr>\n<tr>\n<td>Metadata Management</td>\n<td>Index metric names and labels for fast lookup</td>\n<td><code>IndexMetric(name string, labels Labels) SeriesID</code>, <code>LookupSeries(query LabelQuery) []SeriesID</code></td>\n</tr>\n<tr>\n<td>Data Lifecycle</td>\n<td>Automatic compaction, downsampling, and retention enforcement</td>\n<td><code>CompactBlocks(blocks []BlockID) error</code>, <code>ApplyRetentionPolicy() error</code></td>\n</tr>\n<tr>\n<td>Query Optimization</td>\n<td>Maintain indexes and statistics for efficient query execution</td>\n<td><code>GetSeriesStats(series SeriesID) SeriesStats</code>, <code>OptimizeQuery(query Query) QueryPlan</code></td>\n</tr>\n</tbody></table>\n<p>The <strong>Query Engine</strong> serves as the intelligent data access layer, translating user queries into efficient storage operations and applying mathematical transformations to raw time-series data. It parses query expressions written in our custom query language, develops optimal execution plans that minimize storage I/O, and executes aggregation functions across multiple time series. The query engine also handles time alignment, ensuring that data points from different series are properly synchronized when performing mathematical operations.</p>\n<table>\n<thead>\n<tr>\n<th>Query Responsibility</th>\n<th>Description</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Parsing</td>\n<td>Parse query strings into structured query objects</td>\n<td><code>ParseQuery(queryString string) (Query, error)</code></td>\n</tr>\n<tr>\n<td>Execution Planning</td>\n<td>Optimize query execution order and storage access patterns</td>\n<td><code>PlanQuery(query Query) QueryPlan</code>, <code>EstimateCost(plan QueryPlan) int</code></td>\n</tr>\n<tr>\n<td>Data Aggregation</td>\n<td>Apply mathematical functions across time series</td>\n<td><code>Aggregate(function AggFunc, series []TimeSeries) TimeSeries</code></td>\n</tr>\n<tr>\n<td>Result Formatting</td>\n<td>Transform query results into client-requested formats</td>\n<td><code>FormatResults(results []TimeSeries, format OutputFormat) []byte</code></td>\n</tr>\n</tbody></table>\n<p>The <strong>Dashboard</strong> component provides the web-based user interface for visualizing metrics data and managing system configuration. It maintains a persistent store of dashboard configurations, handles user authentication and authorization, and manages real-time data updates through WebSocket connections. The dashboard component also generates shareable URLs and embeddable widgets for distributing metrics visibility across different teams and systems.</p>\n<table>\n<thead>\n<tr>\n<th>Dashboard Responsibility</th>\n<th>Description</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Configuration Management</td>\n<td>Save/load dashboard layouts and panel configurations</td>\n<td><code>SaveDashboard(config DashboardConfig) error</code>, <code>LoadDashboard(id string) DashboardConfig</code></td>\n</tr>\n<tr>\n<td>Real-Time Updates</td>\n<td>Push live metric data to browser clients via WebSockets</td>\n<td><code>SubscribeToUpdates(client ClientID, queries []Query)</code>, <code>BroadcastUpdate(data MetricData)</code></td>\n</tr>\n<tr>\n<td>Visualization Rendering</td>\n<td>Generate charts from time-series data</td>\n<td><code>RenderChart(data []TimeSeries, chartType ChartType) ChartData</code></td>\n</tr>\n<tr>\n<td>Access Control</td>\n<td>Manage user permissions for viewing and editing dashboards</td>\n<td><code>CheckPermission(user User, dashboard DashboardID, action Action) bool</code></td>\n</tr>\n</tbody></table>\n<p>The <strong>Alerting System</strong> continuously monitors metrics data against user-defined rules and manages the complete alert lifecycle from detection through resolution notification. It evaluates alert conditions at regular intervals, maintains alert state across restarts, and delivers notifications through multiple channels with proper rate limiting and escalation policies. The alerting system also provides alert silencing capabilities for maintenance windows and implements intelligent grouping to reduce notification noise.</p>\n<table>\n<thead>\n<tr>\n<th>Alerting Responsibility</th>\n<th>Description</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Evaluation</td>\n<td>Periodically check metric values against alert conditions</td>\n<td><code>EvaluateRule(rule AlertRule) AlertState</code>, <code>ScheduleEvaluation(rule AlertRule)</code></td>\n</tr>\n<tr>\n<td>State Management</td>\n<td>Track alert states and manage transitions between states</td>\n<td><code>UpdateAlertState(alert AlertID, state AlertState)</code>, <code>GetAlertHistory(alert AlertID) []StateTransition</code></td>\n</tr>\n<tr>\n<td>Notification Delivery</td>\n<td>Send alert messages through configured channels</td>\n<td><code>SendNotification(alert Alert, channel NotificationChannel) error</code></td>\n</tr>\n<tr>\n<td>Alert Grouping</td>\n<td>Combine related alerts to reduce notification volume</td>\n<td><code>GroupAlerts(alerts []Alert) []AlertGroup</code>, <code>ShouldGroup(alert1, alert2 Alert) bool</code></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight: Clear Boundaries Enable Independent Evolution</strong></p>\n<p>By defining precise component responsibilities and stable interfaces, each component can evolve independently without breaking the entire system. The ingestion engine can add new protocol support, the storage engine can optimize its internal format, and the query engine can implement new aggregation functions, all without requiring changes to other components.</p>\n</blockquote>\n<h3 id=\"data-flow-overview\">Data Flow Overview</h3>\n<p>Understanding how data flows through our metrics system is crucial for debugging issues, optimizing performance, and ensuring data consistency. The complete data journey follows several distinct paths depending on whether we&#39;re handling metric ingestion, dashboard queries, or alert evaluations.</p>\n<p><strong>Metric Ingestion Flow</strong>: The primary data flow begins when applications send metrics to our ingestion endpoints or when our system scrapes metrics from application endpoints. Raw metric data enters through HTTP requests containing JSON payloads or Prometheus exposition format text. The ingestion engine immediately performs validation, checking that metric names follow naming conventions, label values don&#39;t exceed cardinality limits, and timestamps fall within acceptable ranges. Valid metrics undergo preprocessing where label values are sanitized, metric names are normalized to lowercase, and any configured transformations are applied.</p>\n<p>Processed metrics then flow to the storage engine through an internal queue that provides buffering and backpressure protection. The storage engine writes sample data to time-series blocks while simultaneously updating metadata indexes with new metric names and label combinations. Each metric sample generates a <code>SeriesID</code> that uniquely identifies the combination of metric name and labels, enabling efficient storage and retrieval of related data points.</p>\n<table>\n<thead>\n<tr>\n<th>Ingestion Stage</th>\n<th>Input Format</th>\n<th>Processing</th>\n<th>Output Format</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Protocol Reception</td>\n<td>HTTP JSON/Prometheus text</td>\n<td>Parse request body, extract metrics</td>\n<td><code>[]Metric</code> structs</td>\n</tr>\n<tr>\n<td>Validation</td>\n<td><code>[]Metric</code></td>\n<td>Check names, labels, timestamps, cardinality</td>\n<td>Valid <code>[]Metric</code></td>\n</tr>\n<tr>\n<td>Preprocessing</td>\n<td>Valid <code>[]Metric</code></td>\n<td>Normalize names, sanitize labels</td>\n<td>Processed <code>[]Metric</code></td>\n</tr>\n<tr>\n<td>Storage Writing</td>\n<td>Processed <code>[]Metric</code></td>\n<td>Generate SeriesID, write samples and metadata</td>\n<td>Persistent storage blocks</td>\n</tr>\n</tbody></table>\n<p><strong>Dashboard Query Flow</strong>: When users view dashboards or create new visualizations, the dashboard component sends queries to the query engine requesting specific time-series data. These queries specify metric names, label filters, time ranges, and aggregation functions needed to generate chart data. The query engine parses each query string into a structured query object, then develops an execution plan that minimizes storage I/O by identifying which storage blocks contain relevant data.</p>\n<p>Query execution retrieves raw samples from storage blocks, applies any necessary filtering based on label conditions, and performs mathematical aggregations like sum, average, or rate calculations across multiple time series. Results are formatted according to the client&#39;s requirements and sent back to the dashboard component, which renders them into interactive charts and updates browser clients through WebSocket connections.</p>\n<table>\n<thead>\n<tr>\n<th>Query Stage</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Reception</td>\n<td>Query string from dashboard</td>\n<td>Parse into structured Query object</td>\n<td><code>Query</code> struct</td>\n</tr>\n<tr>\n<td>Execution Planning</td>\n<td><code>Query</code> struct</td>\n<td>Identify relevant storage blocks, optimize access order</td>\n<td><code>QueryPlan</code></td>\n</tr>\n<tr>\n<td>Data Retrieval</td>\n<td><code>QueryPlan</code></td>\n<td>Read samples from storage, apply label filters</td>\n<td>Raw <code>[]Sample</code></td>\n</tr>\n<tr>\n<td>Aggregation</td>\n<td>Raw <code>[]Sample</code></td>\n<td>Apply mathematical functions, align timestamps</td>\n<td><code>[]TimeSeries</code></td>\n</tr>\n<tr>\n<td>Result Formatting</td>\n<td><code>[]TimeSeries</code></td>\n<td>Convert to client format (JSON/CSV)</td>\n<td>Formatted response</td>\n</tr>\n</tbody></table>\n<p><strong>Alert Evaluation Flow</strong>: The alerting system runs on independent schedules, continuously evaluating alert rules against live metrics data. Each alert rule defines a query expression, comparison operator, threshold value, and evaluation frequency. The alert evaluator sends these queries to the query engine using the same interfaces as dashboard queries, but typically focuses on recent data within the last few minutes or hours.</p>\n<p>Query results undergo threshold comparison to determine if alert conditions are met. The alerting system maintains state machines for each alert rule, tracking transitions between inactive, pending, firing, and resolved states. State changes trigger notification delivery through configured channels like email, Slack, or PagerDuty, with message content generated from customizable templates that include metric values and trend information.</p>\n<table>\n<thead>\n<tr>\n<th>Alert Stage</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rule Scheduling</td>\n<td>Timer trigger</td>\n<td>Load active alert rules from configuration</td>\n<td><code>[]AlertRule</code></td>\n</tr>\n<tr>\n<td>Query Generation</td>\n<td><code>AlertRule</code></td>\n<td>Convert rule conditions into query expressions</td>\n<td><code>Query</code></td>\n</tr>\n<tr>\n<td>Condition Evaluation</td>\n<td>Query results</td>\n<td>Compare metric values against thresholds</td>\n<td><code>AlertState</code></td>\n</tr>\n<tr>\n<td>State Management</td>\n<td>Current and new <code>AlertState</code></td>\n<td>Determine state transitions, update persistence</td>\n<td>State change events</td>\n</tr>\n<tr>\n<td>Notification Delivery</td>\n<td>State change events</td>\n<td>Generate messages, send through channels</td>\n<td>External notifications</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Event-Driven Component Communication</strong></p>\n<ul>\n<li><strong>Context</strong>: Components need to coordinate without tight coupling, enabling independent scaling and deployment</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Direct method calls between components</li>\n<li>Event-driven communication with message queues</li>\n<li>Shared database with polling</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Event-driven communication with internal message queues</li>\n<li><strong>Rationale</strong>: Provides loose coupling, natural backpressure, and enables async processing without external dependencies</li>\n<li><strong>Consequences</strong>: Adds complexity but enables independent component scaling and improves system resilience</li>\n</ul>\n</blockquote>\n<h3 id=\"deployment-architecture\">Deployment Architecture</h3>\n<p>The deployment architecture determines how our components are packaged, configured, and operated in production environments. Our design supports both single-node deployments for development and testing, as well as distributed deployments for production scale and high availability.</p>\n<p><strong>Single-Node Deployment</strong>: For development, testing, and small production environments, all components run within a single process as separate goroutines communicating through Go channels. This deployment model minimizes operational complexity while providing the full functionality of our metrics system. The single binary includes HTTP servers for ingestion and dashboard endpoints, background goroutines for alert evaluation, and embedded storage using local filesystem.</p>\n<p>Configuration is provided through a single YAML file that defines ingestion endpoints, storage paths, dashboard settings, and alert rules. The process listens on multiple ports: 8080 for metric ingestion, 3000 for dashboard web interface, and 9090 for health checks and administrative operations. Internal communication between components uses Go channels for metric data flow and shared memory for configuration and state information.</p>\n<table>\n<thead>\n<tr>\n<th>Deployment Component</th>\n<th>Process</th>\n<th>Ports</th>\n<th>Storage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Engine</td>\n<td>Main process, goroutine pool</td>\n<td>8080 (HTTP)</td>\n<td>In-memory queues</td>\n</tr>\n<tr>\n<td>Storage Engine</td>\n<td>Main process, background workers</td>\n<td>N/A (internal)</td>\n<td>Local filesystem blocks</td>\n</tr>\n<tr>\n<td>Query Engine</td>\n<td>Main process, shared service</td>\n<td>N/A (internal)</td>\n<td>Shared memory caches</td>\n</tr>\n<tr>\n<td>Dashboard</td>\n<td>Main process, HTTP server</td>\n<td>3000 (HTTP/WebSocket)</td>\n<td>SQLite for configs</td>\n</tr>\n<tr>\n<td>Alerting System</td>\n<td>Main process, cron scheduler</td>\n<td>N/A (internal)</td>\n<td>Local state files</td>\n</tr>\n</tbody></table>\n<p><strong>Multi-Node Deployment</strong>: Production environments requiring higher throughput and availability can deploy components as separate services communicating over HTTP APIs. The ingestion engine runs as a stateless service that can be horizontally scaled behind a load balancer. Multiple ingestion instances write to a shared storage layer that coordinates writes through file locking and atomic operations.</p>\n<p>The storage engine operates as a separate service with dedicated high-performance storage volumes and background processes for compaction and retention enforcement. Query engines run as stateless services that connect to storage over internal network interfaces, enabling horizontal scaling of query processing. Dashboard instances share configuration through an external database, while alerting systems use distributed coordination to prevent duplicate alert evaluations.</p>\n<p>Service discovery enables components to find each other dynamically as instances are added or removed. Health checks monitor each service independently, and failures trigger automatic restarts or traffic redirection to healthy instances.</p>\n<table>\n<thead>\n<tr>\n<th>Service</th>\n<th>Scaling Model</th>\n<th>Dependencies</th>\n<th>Communication</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Service</td>\n<td>Horizontal (stateless)</td>\n<td>Storage service</td>\n<td>HTTP POST to storage</td>\n</tr>\n<tr>\n<td>Storage Service</td>\n<td>Vertical (shared state)</td>\n<td>Persistent volumes</td>\n<td>HTTP API for queries</td>\n</tr>\n<tr>\n<td>Query Service</td>\n<td>Horizontal (stateless)</td>\n<td>Storage service</td>\n<td>HTTP API calls</td>\n</tr>\n<tr>\n<td>Dashboard Service</td>\n<td>Horizontal (shared DB)</td>\n<td>External database</td>\n<td>HTTP + WebSocket</td>\n</tr>\n<tr>\n<td>Alerting Service</td>\n<td>Active/standby</td>\n<td>Query service</td>\n<td>HTTP queries, webhook notifications</td>\n</tr>\n</tbody></table>\n<p><strong>Container Deployment</strong>: Both single-node and multi-node deployments support containerization using Docker images. The single-node deployment produces one image containing all components, while multi-node deployment creates separate images for each service. Container images include all necessary dependencies and use multi-stage builds to minimize image size.</p>\n<p>Kubernetes deployment manifests define resource requirements, health checks, and scaling policies for each component. Persistent volumes ensure storage durability across container restarts, while ConfigMaps and Secrets manage configuration and sensitive credentials. Service meshes like Istio can provide additional features like traffic encryption, request tracing, and advanced load balancing.</p>\n<table>\n<thead>\n<tr>\n<th>Container Type</th>\n<th>Image Size</th>\n<th>Resource Requirements</th>\n<th>Persistent Storage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single-node</td>\n<td>~50MB</td>\n<td>512MB RAM, 1 CPU</td>\n<td>10GB volume</td>\n</tr>\n<tr>\n<td>Ingestion service</td>\n<td>~20MB</td>\n<td>256MB RAM, 0.5 CPU</td>\n<td>None (stateless)</td>\n</tr>\n<tr>\n<td>Storage service</td>\n<td>~30MB</td>\n<td>2GB RAM, 2 CPU</td>\n<td>100GB+ volume</td>\n</tr>\n<tr>\n<td>Query service</td>\n<td>~25MB</td>\n<td>1GB RAM, 1 CPU</td>\n<td>None (stateless)</td>\n</tr>\n<tr>\n<td>Dashboard service</td>\n<td>~35MB</td>\n<td>512MB RAM, 0.5 CPU</td>\n<td>External database</td>\n</tr>\n<tr>\n<td>Alerting service</td>\n<td>~20MB</td>\n<td>256MB RAM, 0.5 CPU</td>\n<td>Small state volume</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Principle: Deployment Flexibility Without Architecture Compromise</strong></p>\n<p>Our component architecture naturally supports multiple deployment models without requiring architectural changes. The same codebase can run as a single process for development or as distributed services for production, with deployment-specific configuration determining communication mechanisms.</p>\n</blockquote>\n<p><strong>Configuration Management</strong>: Deployment architecture includes comprehensive configuration management that adapts to different environments while maintaining consistency. Configuration is organized hierarchically with global defaults, environment-specific overrides, and runtime parameters. The single-node deployment uses local configuration files, while distributed deployments can integrate with configuration management systems like Consul or etcd.</p>\n<p>Environment variables provide deployment-specific parameters like network addresses, credentials, and resource limits. Configuration validation occurs at startup to catch errors before services begin processing data. Hot configuration reloading enables operational changes without service restarts for non-critical settings like dashboard themes or alert message templates.</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Layer</th>\n<th>Source</th>\n<th>Scope</th>\n<th>Reload Support</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global Defaults</td>\n<td>Compiled into binary</td>\n<td>All deployments</td>\n<td>Requires restart</td>\n</tr>\n<tr>\n<td>Environment Config</td>\n<td>YAML files or config service</td>\n<td>Per environment</td>\n<td>Hot reload</td>\n</tr>\n<tr>\n<td>Runtime Parameters</td>\n<td>Environment variables</td>\n<td>Per instance</td>\n<td>Hot reload</td>\n</tr>\n<tr>\n<td>User Preferences</td>\n<td>Database or local storage</td>\n<td>Per user</td>\n<td>Immediate</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Tight Component Coupling Through Direct Database Access</strong>\nMultiple components directly accessing the same database tables creates hidden dependencies and makes it difficult to evolve components independently. For example, if both the query engine and alerting system directly read from the same storage tables, changes to the storage schema require coordinating updates across multiple components. Instead, all data access should flow through well-defined service interfaces that can evolve their internal implementations without breaking clients.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Backpressure in Data Flow Design</strong>\nDesigning data flows without considering what happens when downstream components can&#39;t keep up leads to memory exhaustion and data loss. A common mistake is having the ingestion engine accept unlimited metrics without checking if the storage engine can handle the write load. This causes metric data to accumulate in memory until the process crashes. Always implement queue limits, circuit breakers, and graceful degradation when downstream services are overloaded.</p>\n<p>⚠️ <strong>Pitfall: Mixing Configuration and State in Component Design</strong>\nStoring configuration data (like dashboard definitions) in the same storage systems as operational data (like metric samples) creates operational complexity and scaling problems. Configuration data has different access patterns, consistency requirements, and backup needs than time-series data. Keep them separated with configuration using databases optimized for transactional consistency and operational data using storage optimized for time-series workloads.</p>\n<p>⚠️ <strong>Pitfall: Single Points of Failure in Distributed Architecture</strong>\nWhile distributed deployment provides scalability benefits, introducing single points of failure eliminates availability advantages. Common mistakes include having only one storage service instance, sharing state through a single external database, or requiring all components to register with a single service discovery instance. Design redundancy into every component that stores state, and ensure the system can operate with reduced functionality when individual components fail.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td>Go <code>net/http</code> with <code>gorilla/mux</code> router</td>\n<td>Go <code>gin-gonic/gin</code> or <code>echo</code> framework</td>\n</tr>\n<tr>\n<td>Inter-Component Communication</td>\n<td>Go channels for single-node, HTTP for multi-node</td>\n<td>gRPC with Protocol Buffers</td>\n</tr>\n<tr>\n<td>Configuration Management</td>\n<td>YAML files with <code>gopkg.in/yaml.v3</code></td>\n<td>Consul or etcd integration</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static configuration files</td>\n<td>Kubernetes DNS or Consul</td>\n</tr>\n<tr>\n<td>Health Checks</td>\n<td>Simple HTTP endpoints returning status</td>\n<td>Structured health checks with dependency status</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Go <code>log/slog</code> with structured output</td>\n<td>External logging systems like ELK or Prometheus logs</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>metrics-system/\n├── cmd/\n│   ├── single-node/\n│   │   └── main.go                 ← Single-process deployment\n│   └── distributed/\n│       ├── ingestion/main.go       ← Ingestion service\n│       ├── storage/main.go         ← Storage service\n│       ├── query/main.go           ← Query service\n│       ├── dashboard/main.go       ← Dashboard service\n│       └── alerting/main.go        ← Alerting service\n├── internal/\n│   ├── ingestion/\n│   │   ├── server.go               ← HTTP handlers and validation\n│   │   ├── processor.go            ← Metric preprocessing logic\n│   │   └── ingestion_test.go\n│   ├── storage/\n│   │   ├── engine.go               ← Storage interface implementation\n│   │   ├── blocks.go               ← Time-series block management\n│   │   └── storage_test.go\n│   ├── query/\n│   │   ├── engine.go               ← Query processing logic\n│   │   ├── parser.go               ← Query language parsing\n│   │   └── query_test.go\n│   ├── dashboard/\n│   │   ├── server.go               ← Web server and WebSocket handling\n│   │   ├── config.go               ← Dashboard configuration management\n│   │   └── dashboard_test.go\n│   ├── alerting/\n│   │   ├── evaluator.go            ← Alert rule evaluation\n│   │   ├── notifier.go             ← Notification delivery\n│   │   └── alerting_test.go\n│   └── shared/\n│       ├── types.go                ← Common data structures\n│       ├── config.go               ← Configuration loading\n│       └── health.go               ← Health check utilities\n├── web/\n│   ├── static/                     ← Dashboard HTML, CSS, JavaScript\n│   └── templates/                  ← Go templates for dashboard rendering\n├── configs/\n│   ├── single-node.yaml           ← Single-node configuration\n│   └── distributed/                ← Per-service configurations\n└── docker/\n    ├── Dockerfile.single           ← Single-node container\n    └── Dockerfile.service          ← Multi-stage service containers</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>The following code provides complete, working infrastructure that handles cross-cutting concerns so you can focus on the core metrics system logic:</p>\n<p><strong>Configuration Management (<code>internal/shared/config.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> shared</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">gopkg.in/yaml.v3</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Config represents the complete system configuration with all component settings.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Load this once at startup and pass relevant sections to each component.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Config</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Server   </span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#9ECBFF\">   `yaml:\"server\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage  </span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#9ECBFF\">  `yaml:\"storage\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Alerting </span><span style=\"color:#B392F0\">AlertingConfig</span><span style=\"color:#9ECBFF\"> `yaml:\"alerting\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ServerConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IngestionPort </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"ingestion_port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DashboardPort </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `yaml:\"dashboard_port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ReadTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"read_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteTimeout  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"write_timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    DataDir         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `yaml:\"data_dir\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetentionPeriod </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"retention_period\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompactionInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"compaction_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertingConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EvaluationInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `yaml:\"evaluation_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NotificationChannels []</span><span style=\"color:#B392F0\">NotificationChannel</span><span style=\"color:#9ECBFF\"> `yaml:\"notification_channels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NotificationChannel</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"type\"`</span><span style=\"color:#6A737D\"> // \"email\", \"slack\", \"webhook\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `yaml:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Config </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"config\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadConfig reads configuration from the specified file path with environment variable overrides.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns a complete Config struct ready for use by all components.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> LoadConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">configPath</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">ReadFile</span><span style=\"color:#E1E4E8\">(configPath)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to read config file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> config </span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> yaml.</span><span style=\"color:#B392F0\">Unmarshal</span><span style=\"color:#E1E4E8\">(data, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to parse config: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Apply environment variable overrides</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> port </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"INGESTION_PORT\"</span><span style=\"color:#E1E4E8\">); port </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        fmt.</span><span style=\"color:#B392F0\">Sscanf</span><span style=\"color:#E1E4E8\">(port, </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">config.Server.IngestionPort)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> dataDir </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">Getenv</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"STORAGE_DATA_DIR\"</span><span style=\"color:#E1E4E8\">); dataDir </span><span style=\"color:#F97583\">!=</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config.Storage.DataDir </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dataDir</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">config, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks that all required configuration values are present and valid.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Call this after loading configuration to catch errors before starting services.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Server.IngestionPort </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> ||</span><span style=\"color:#E1E4E8\"> c.Server.IngestionPort </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 65535</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"invalid ingestion port: </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, c.Server.IngestionPort)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Storage.DataDir </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"storage data directory must be specified\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.Storage.RetentionPeriod </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"retention period must be positive\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Health Check System (<code>internal/shared/health.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> shared</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthStatus represents the health state of a component or dependency.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusHealthy</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusDegraded</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusUnhealthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> h {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusHealthy:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusDegraded:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusUnhealthy:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck represents a single health check with its current status.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                                `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status      </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\">                         `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastChecked </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">                           `json:\"last_checked\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                              `json:\"message,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CheckFunc   </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#9ECBFF\">`json:\"-\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthManager coordinates health checks across all system components.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Each component registers its health checks, and the manager periodically executes them.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checks </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthCheck</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu     </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHealthManager creates a new health manager ready for check registration.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHealthManager</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checks: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterCheck adds a health check that will be executed periodically.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// The checkFunc should return the current health status and an optional message.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">checkFunc</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hm.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.checks[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Name:      name,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Status:    HealthStatusUnhealthy, </span><span style=\"color:#6A737D\">// Start unhealthy until first check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        CheckFunc: checkFunc,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RunChecks executes all registered health checks and updates their status.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Call this periodically (e.g., every 30 seconds) to keep health status current.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RunChecks</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hm.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, check </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hm.checks {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        status, message </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> check.</span><span style=\"color:#B392F0\">CheckFunc</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        check.Status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        check.Message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        check.LastChecked </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetOverallStatus returns the worst status among all checks.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// System is healthy only if all checks are healthy.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetOverallStatus</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hm.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    worst </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> HealthStatusHealthy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, check </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hm.checks {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> check.Status </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> worst {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            worst </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check.Status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> worst</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ServeHTTP implements http.Handler to provide health check endpoint.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Returns 200 for healthy, 503 for degraded/unhealthy with JSON details.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checks </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hm.checks))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, check </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hm.checks {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Copy struct to avoid exposing internal pointers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkCopy </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checkCopy.CheckFunc </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#6A737D\"> // Don't serialize function</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checks[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">checkCopy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    overall </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hm.</span><span style=\"color:#B392F0\">GetOverallStatus</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Status </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                     `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Checks </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#9ECBFF\">   `json:\"checks\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Status: overall.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Checks: checks,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> overall </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> HealthStatusHealthy {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusServiceUnavailable)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w).</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(response)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-architecture-skeleton\">Core Architecture Skeleton</h4>\n<p>The following code provides the main architectural structure with detailed TODOs mapping to the design concepts above:</p>\n<p><strong>Component Interface Definitions (<code>internal/shared/interfaces.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> shared</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricsIngester defines the interface for receiving and processing metrics data.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implement this interface in your ingestion engine component.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricsIngester</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // IngestMetrics processes a batch of metrics from push-based sources.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate each metric using Metric.Validate()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check label cardinality to prevent explosion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply any configured preprocessing transformations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Send processed metrics to storage engine via queue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return error if any metrics were rejected</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    IngestMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ScrapeEndpoint retrieves metrics from a pull-based Prometheus endpoint.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Make HTTP GET request to the provided URL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse Prometheus exposition format response</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Convert parsed data to internal Metric structs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call IngestMetrics() with converted metrics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ScrapeEndpoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">url</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeSeriesStorage defines the interface for persisting and retrieving time-series data.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implement this interface in your storage engine component.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeSeriesStorage</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // WriteSamples persists metric samples to storage blocks.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate SeriesID for each unique metric name + labels combination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Append samples to appropriate time-series blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update metadata indexes with new series information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Trigger compaction if block size thresholds exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return error if write operation fails</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    WriteSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // QueryRange retrieves samples within the specified time range.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse label selectors to identify matching series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Find storage blocks that overlap with time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Read samples from blocks, applying label filters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Sort samples by timestamp and return</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    QueryRange</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">query</span><span style=\"color:#B392F0\"> RangeQuery</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // GetSeriesMetadata returns metadata for series matching label selectors.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Consult metadata indexes for matching series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Return series names and label sets without sample data</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetSeriesMetadata</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">selectors</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">LabelSelector</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">SeriesMetadata</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryProcessor defines the interface for executing queries against time-series data.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implement this interface in your query engine component.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryProcessor</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // ExecuteQuery parses and executes a query string, returning formatted results.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse query string into Query struct</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate query syntax and semantic correctness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create execution plan optimized for storage access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute plan against storage engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply aggregation functions and mathematical operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Format results according to requested output format</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertEvaluator defines the interface for processing alert rules and managing alert state.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implement this interface in your alerting system component.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertEvaluator</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // EvaluateRules checks all active alert rules against current metric data.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load active alert rules from configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Convert each rule condition to query expression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute query using QueryProcessor interface</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Compare results against rule thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update alert states based on comparison results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Trigger notifications for state changes</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    EvaluateRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // UpdateAlertState changes the state of an alert and triggers notifications.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load current alert state from persistence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate state transition is allowed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update persistent state storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Generate notification message from template</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send notification through configured channels</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    UpdateAlertState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alertID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">newState</span><span style=\"color:#B392F0\"> AlertState</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Component Coordinator (<code>internal/coordinator/coordinator.go</code>):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">metrics-system/internal/shared</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ComponentCoordinator manages the lifecycle of all system components and coordinates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// their interactions through well-defined interfaces.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ComponentCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">shared</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    health     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">shared</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HealthManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    components </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Component</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wg         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx        </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cancel     </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">CancelFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Component represents a system component that can be started and stopped independently.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Component</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Stop</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">shared</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCoordinator creates a new component coordinator with the provided configuration.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Use this as the main orchestrator for both single-node and distributed deployments.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCoordinator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">shared</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithCancel</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:     config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:     logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        health:     shared.</span><span style=\"color:#B392F0\">NewHealthManager</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        components: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Component</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctx:        ctx,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cancel:     cancel,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterComponent adds a component to the coordinator for lifecycle management.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 1: Validate component name is unique</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 2: Store component in components map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 3: Register component's health check with health manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 4: Log component registration for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterComponent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#B392F0\"> Component</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement component registration logic here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // This should store the component and set up its health monitoring</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement me\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins all registered components in dependency order.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 1: Start components in order: Storage -> Query -> Ingestion -> Dashboard -> Alerting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 2: Wait for each component to report healthy before starting next</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 3: Start health check monitoring goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 4: Set up signal handlers for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement startup sequence here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Consider component dependencies and startup ordering</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement me\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully shuts down all components in reverse dependency order.</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 1: Cancel context to signal all components to stop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 2: Stop components in reverse order with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 3: Wait for all component goroutines to exit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TODO 4: Close any shared resources</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO: Implement shutdown sequence here</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ensure graceful cleanup of all resources</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    panic</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"implement me\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthHandler returns an HTTP handler for system health checks.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealthHandler</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Handler</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> cc.health</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing component interfaces:</strong></p>\n<ul>\n<li>Run <code>go build ./cmd/single-node/</code> - should compile without errors</li>\n<li>All interface methods should be defined with proper signatures</li>\n<li>Component registration should work without panics</li>\n</ul>\n<p><strong>After implementing basic coordinator:</strong></p>\n<ul>\n<li>Start the system with <code>./single-node -config configs/single-node.yaml</code></li>\n<li>Health check endpoint at <code>http://localhost:9090/health</code> should return component status</li>\n<li>System should shut down gracefully with CTRL+C</li>\n</ul>\n<p><strong>After connecting components:</strong></p>\n<ul>\n<li>Send a test metric: <code>curl -X POST localhost:8080/metrics -d &#39;{&quot;name&quot;:&quot;test_metric&quot;,&quot;value&quot;:42}&#39;</code></li>\n<li>Query the metric: <code>curl &quot;localhost:8080/query?query=test_metric&quot;</code></li>\n<li>Check dashboard at <code>http://localhost:3000</code> shows the metric</li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 1 (Metrics Collection), 2 (Storage &amp; Querying), 3 (Visualization Dashboard), 4 (Alerting System) - The data model provides the foundational structures used across all system components</p>\n</blockquote>\n<h3 id=\"the-blueprint-metaphor-understanding-system-data-models\">The Blueprint Metaphor: Understanding System Data Models</h3>\n<p>Think of a data model as the blueprint for a complex building project. Just as architectural blueprints define how rooms connect, what materials to use, and how utilities flow through the structure, our data model defines how metrics, time-series data, alerts, and dashboards are structured and relate to each other. A well-designed blueprint ensures that electricians, plumbers, and carpenters can work independently while their components integrate seamlessly. Similarly, our data model ensures that the ingestion engine, storage layer, query processor, and alerting system can operate independently while sharing a common understanding of data structures.</p>\n<p>The data model serves as the contract between all system components. When the ingestion engine receives a metric, it must understand the difference between a counter that only increases and a gauge that can fluctuate. When the storage engine persists this data, it must organize time-series efficiently for later retrieval. When the alerting system evaluates rules, it must understand how to extract values from different metric types. This shared understanding prevents integration headaches and ensures data consistency across the entire system.</p>\n<h3 id=\"metric-types-and-structure\">Metric Types and Structure</h3>\n<p>Understanding metric types requires thinking about what each type measures in the real world. A <strong>counter</strong> behaves like an odometer in a car - it only moves forward, tracking cumulative totals like total HTTP requests served or total bytes transmitted. A <strong>gauge</strong> behaves like a speedometer or thermometer - it shows the current value of something that can increase or decrease, like current CPU usage or active database connections. A <strong>histogram</strong> behaves like a survey response collector - it groups observations into ranges (buckets) to show distribution patterns, like response time distributions or request size distributions.</p>\n<p>Each metric type has fundamentally different mathematical properties that affect how they are stored, queried, and aggregated. Counters require rate calculations to be meaningful (requests per second rather than total requests since boot). Gauges can be averaged, but their historical values may not be meaningful for trend analysis. Histograms require special aggregation logic to merge buckets across multiple instances or time windows.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fdata-model.svg\" alt=\"Data Model Relationships\"></p>\n<p>The core metric structure must capture not just the value and timestamp, but also the rich metadata that makes metrics searchable and aggregatable. <strong>Labels</strong> provide this multi-dimensional metadata, allowing metrics to be sliced and diced across different attributes like service name, environment, region, or instance ID. However, labels create a fundamental tension: more labels provide more flexibility but exponentially increase <strong>cardinality</strong> (the number of unique time-series combinations), which directly impacts storage requirements and query performance.</p>\n<blockquote>\n<p><strong>Decision: Metric Structure Design</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to represent different types of metrics with their metadata while supporting efficient storage and querying</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Single unified metric structure with type discrimination</li>\n<li>Separate data structures for each metric type  </li>\n<li>Inheritance-based hierarchy with base metric class</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Single unified structure with <code>MetricType</code> enum discrimination</li>\n<li><strong>Rationale</strong>: Simplifies ingestion pipeline and storage layer by having consistent interfaces, while type-specific behavior is handled through switch statements rather than complex inheritance</li>\n<li><strong>Consequences</strong>: Enables polymorphic handling in storage and query layers, but requires careful validation to ensure type-specific constraints are enforced</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Name</td>\n<td>string</td>\n<td>Metric identifier following hierarchical naming convention (e.g., &quot;http_requests_total&quot;, &quot;cpu_usage_percent&quot;)</td>\n</tr>\n<tr>\n<td>Type</td>\n<td>MetricType</td>\n<td>Enum indicating Counter, Gauge, or Histogram behavior and aggregation rules</td>\n</tr>\n<tr>\n<td>Labels</td>\n<td>Labels</td>\n<td>Key-value metadata map for multi-dimensional filtering and grouping</td>\n</tr>\n<tr>\n<td>Samples</td>\n<td>[]Sample</td>\n<td>Ordered array of timestamped values representing the metric&#39;s evolution over time</td>\n</tr>\n</tbody></table>\n<p>The <code>MetricType</code> enumeration defines the fundamental behavior categories that determine how values should be interpreted and aggregated:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Type</th>\n<th>Numeric Value</th>\n<th>Aggregation Behavior</th>\n<th>Typical Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Counter</td>\n<td>0</td>\n<td>Rate calculations, cumulative sums</td>\n<td>Request counts, error counts, bytes transmitted</td>\n</tr>\n<tr>\n<td>Gauge</td>\n<td>1</td>\n<td>Point-in-time snapshots, averages</td>\n<td>CPU usage, memory consumption, queue depth</td>\n</tr>\n<tr>\n<td>Histogram</td>\n<td>2</td>\n<td>Bucket merging, percentile calculations</td>\n<td>Response times, request sizes, batch processing durations</td>\n</tr>\n</tbody></table>\n<p>The <code>Labels</code> type provides powerful multi-dimensional indexing but requires careful cardinality management. Each unique combination of label values creates a distinct time-series, so labels with high cardinality values (like user IDs or request IDs) can create storage explosions. The system must provide both flexibility for legitimate use cases and protection against accidental cardinality bombs.</p>\n<table>\n<thead>\n<tr>\n<th>Label Constraint</th>\n<th>Validation Rule</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Key Format</td>\n<td>Must match <code>^[a-zA-Z_][a-zA-Z0-9_]*$</code></td>\n<td>Ensures consistent naming and prevents injection attacks</td>\n</tr>\n<tr>\n<td>Key Length</td>\n<td>Maximum 128 characters</td>\n<td>Prevents excessive memory usage in indexes</td>\n</tr>\n<tr>\n<td>Value Length</td>\n<td>Maximum 1024 characters</td>\n<td>Balances expressiveness with storage efficiency</td>\n</tr>\n<tr>\n<td>Total Labels</td>\n<td>Maximum 32 per metric</td>\n<td>Limits combinatorial explosion while supporting rich metadata</td>\n</tr>\n<tr>\n<td>Reserved Keys</td>\n<td>Cannot start with <code>__</code> (double underscore)</td>\n<td>Reserves namespace for system-generated labels</td>\n</tr>\n</tbody></table>\n<p>The <code>Sample</code> structure captures the atomic unit of time-series data - a value paired with its observation timestamp. This pairing is fundamental to time-series analysis, enabling trend detection, rate calculations, and temporal aggregations.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Value</td>\n<td>float64</td>\n<td>Numeric measurement value with IEEE 754 double precision for wide range support</td>\n</tr>\n<tr>\n<td>Timestamp</td>\n<td>time.Time</td>\n<td>UTC timestamp with nanosecond precision for high-resolution time-series</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Timestamp Precision Inconsistency</strong>\nMany beginners mix timestamp precisions (seconds vs milliseconds vs nanoseconds) across different components, leading to data alignment issues during queries. Always normalize to nanosecond precision at ingestion time and use UTC timezone to avoid daylight savings time complications. The storage engine can downsample precision during compaction, but ingestion should preserve maximum precision.</p>\n<h3 id=\"time-series-schema\">Time-Series Schema</h3>\n<p>Time-series data organization follows the principle that <strong>storage locality drives query performance</strong>. Think of time-series storage like organizing a vast library where books (data points) must be found quickly based on multiple criteria: author (metric name), topic (labels), and publication date (timestamp). The organization scheme determines whether finding all books by a specific author from a specific year requires scanning the entire library or jumping directly to the right shelf.</p>\n<p>The fundamental insight is that time-series data exhibits strong <strong>temporal locality</strong> - queries typically request ranges of data points for the same metric and label combination. This access pattern drives the storage schema design toward grouping samples by time-series identity rather than by timestamp globally.</p>\n<blockquote>\n<p><strong>Decision: Time-Series Identification Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to uniquely identify each time-series for efficient storage and retrieval while supporting label-based filtering</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Composite primary key of (metric_name, sorted_labels, timestamp)</li>\n<li>Hash-based series ID with separate label index</li>\n<li>Hierarchical key structure with metric name prefix</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hash-based series ID with deterministic label sorting</li>\n<li><strong>Rationale</strong>: Provides O(1) lookup performance while maintaining deterministic behavior across system restarts, and allows label indexing to be optimized separately from time-series storage</li>\n<li><strong>Consequences</strong>: Enables efficient range scans within a time-series while supporting complex label queries through secondary indexes, but requires careful hash collision handling</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Schema Component</th>\n<th>Structure</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Series ID</td>\n<td>SHA-256 hash of metric name + sorted labels</td>\n<td>Provides unique, deterministic identifier for each time-series</td>\n</tr>\n<tr>\n<td>Label Index</td>\n<td>Inverted index mapping label keys/values to series IDs</td>\n<td>Enables efficient label-based filtering and selection</td>\n</tr>\n<tr>\n<td>Sample Blocks</td>\n<td>Compressed chunks of samples grouped by series ID and time range</td>\n<td>Optimizes storage efficiency and range query performance</td>\n</tr>\n<tr>\n<td>Metadata Index</td>\n<td>Series ID → metric metadata mapping</td>\n<td>Provides series discovery and label enumeration</td>\n</tr>\n</tbody></table>\n<p>The <strong>Series ID generation</strong> algorithm ensures consistent identification across system components and restarts:</p>\n<ol>\n<li>Concatenate metric name with newline separator</li>\n<li>Sort label pairs by key name lexicographically  </li>\n<li>Append each sorted label as &quot;key=value\\n&quot;</li>\n<li>Compute SHA-256 hash of resulting string</li>\n<li>Encode hash as hexadecimal string for human readability</li>\n</ol>\n<p>This deterministic approach ensures that the same metric with identical labels always produces the same series ID, regardless of label order in the original submission or which system component processes it.</p>\n<p>The <strong>Label Index</strong> provides efficient multi-dimensional filtering by maintaining inverted indexes for each label key-value pair. This allows queries like &quot;find all HTTP request metrics for the production environment&quot; to be resolved through index lookups rather than full table scans.</p>\n<table>\n<thead>\n<tr>\n<th>Index Type</th>\n<th>Key Format</th>\n<th>Value Format</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Label Key Index</td>\n<td><code>__key__:{label_key}</code></td>\n<td>Set of series IDs</td>\n<td>Find all series with specific label keys</td>\n</tr>\n<tr>\n<td>Label Value Index</td>\n<td><code>__kv__:{label_key}={label_value}</code></td>\n<td>Set of series IDs</td>\n<td>Find all series with specific label key-value pairs</td>\n</tr>\n<tr>\n<td>Metric Name Index</td>\n<td><code>__name__:{metric_name}</code></td>\n<td>Set of series IDs</td>\n<td>Find all series for a specific metric</td>\n</tr>\n<tr>\n<td>Series Metadata</td>\n<td><code>__meta__:{series_id}</code></td>\n<td>Metric name + labels JSON</td>\n<td>Resolve series ID back to human-readable metadata</td>\n</tr>\n</tbody></table>\n<p><strong>Sample Blocks</strong> organize the actual time-series data for optimal storage and retrieval. Each block contains samples for a single series within a specific time range, enabling efficient range queries and compression.</p>\n<table>\n<thead>\n<tr>\n<th>Block Component</th>\n<th>Format</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Block Header</td>\n<td>Series ID + time range + sample count</td>\n<td>Enables range filtering without decompressing block data</td>\n</tr>\n<tr>\n<td>Timestamp Array</td>\n<td>Delta-compressed timestamps</td>\n<td>Stores sample timestamps with compression for temporal locality</td>\n</tr>\n<tr>\n<td>Value Array</td>\n<td>Float64 values with optional compression</td>\n<td>Stores metric values with optional bit-packing for integer-like values</td>\n</tr>\n<tr>\n<td>Block Footer</td>\n<td>Checksum + compression metadata</td>\n<td>Ensures data integrity and provides decompression parameters</td>\n</tr>\n</tbody></table>\n<p>The storage layout prioritizes <strong>write efficiency</strong> for recent data while maintaining <strong>read efficiency</strong> for historical queries. Recent samples are accumulated in memory-based blocks that are periodically flushed to disk. Historical data is organized in immutable, compressed blocks that can be efficiently scanned for range queries.</p>\n<blockquote>\n<p>The key insight is that time-series workloads exhibit a &quot;hot-warm-cold&quot; access pattern. Recent data (last few hours) receives frequent writes and reads. Medium-aged data (days to weeks) receives occasional reads but no writes. Ancient data (months to years) receives rare reads but consumes the most storage space. The schema optimizes for each access pattern differently.</p>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Timestamp Alignment Assumptions</strong>\nBeginning developers often assume that samples from different metrics arrive with perfectly aligned timestamps, leading to complex join logic that fails in practice. Real-world metrics arrive with jittered timestamps due to network delays, processing variations, and clock skew between systems. Design aggregation and comparison logic to handle timestamp misalignment gracefully, typically by bucketing samples into time windows rather than expecting exact timestamp matches.</p>\n<h3 id=\"alert-and-dashboard-schema\">Alert and Dashboard Schema</h3>\n<p>The alerting and dashboard schemas must capture not just the static configuration but also the dynamic state transitions and user interactions that occur during system operation. Think of alert rules like smoke detectors - they must define what constitutes danger (threshold conditions), how to confirm it&#39;s not a false alarm (evaluation duration), and who to notify when action is required. Dashboard configurations are like mission control displays - they must define what data to show, how to visualize it, and how to keep the information current.</p>\n<p>Alert rule design balances expressiveness with reliability. Rules must be simple enough for operators to understand and debug, yet powerful enough to detect complex failure conditions. The schema captures both the declarative rule definition and the runtime state management required for proper alert lifecycle handling.</p>\n<blockquote>\n<p><strong>Decision: Alert Rule Expression Language</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to define alert conditions that can reference metrics, apply mathematical operations, and specify thresholds while remaining debuggable</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Simple threshold-based rules with metric name + comparison operator</li>\n<li>SQL-like query language with aggregation functions</li>\n<li>PromQL-inspired expression language with time-series functions</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: PromQL-inspired expressions with simplified function set</li>\n<li><strong>Rationale</strong>: Provides sufficient expressiveness for complex conditions while maintaining compatibility with monitoring industry standards, and allows rule expressions to be tested independently of alert evaluation</li>\n<li><strong>Consequences</strong>: Enables powerful alert conditions like rate calculations and multi-metric comparisons, but requires implementing a query parser and expression evaluator</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Alert Rule Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ID</td>\n<td>string</td>\n<td>Unique identifier for the alert rule, used for state tracking and notifications</td>\n</tr>\n<tr>\n<td>Name</td>\n<td>string</td>\n<td>Human-readable alert name displayed in notifications and dashboard</td>\n</tr>\n<tr>\n<td>Expression</td>\n<td>string</td>\n<td>Query expression that returns numeric values for threshold comparison</td>\n</tr>\n<tr>\n<td>Threshold</td>\n<td>float64</td>\n<td>Numeric value that triggers alert when expression result crosses this boundary</td>\n</tr>\n<tr>\n<td>Operator</td>\n<td>string</td>\n<td>Comparison operator: &quot;gt&quot; (greater than), &quot;lt&quot; (less than), &quot;eq&quot; (equal), &quot;ne&quot; (not equal)</td>\n</tr>\n<tr>\n<td>Duration</td>\n<td>time.Duration</td>\n<td>How long condition must persist before transitioning from pending to firing state</td>\n</tr>\n<tr>\n<td>EvaluationInterval</td>\n<td>time.Duration</td>\n<td>How frequently to evaluate the alert expression against current data</td>\n</tr>\n<tr>\n<td>Labels</td>\n<td>Labels</td>\n<td>Static labels attached to all alert instances for routing and grouping</td>\n</tr>\n<tr>\n<td>Annotations</td>\n<td>map[string]string</td>\n<td>Additional metadata like description, runbook URL, and dashboard links</td>\n</tr>\n</tbody></table>\n<p>Alert <strong>state management</strong> requires tracking the lifecycle of each alert instance as conditions change over time. The state machine ensures proper notification delivery and prevents alert flapping.</p>\n<table>\n<thead>\n<tr>\n<th>Alert State</th>\n<th>Description</th>\n<th>Entry Conditions</th>\n<th>Exit Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inactive</td>\n<td>Alert condition is not met</td>\n<td>Expression result does not cross threshold</td>\n<td>Expression result crosses threshold</td>\n</tr>\n<tr>\n<td>Pending</td>\n<td>Condition met but duration requirement not satisfied</td>\n<td>Threshold crossed but duration timer not expired</td>\n<td>Duration timer expires OR condition no longer met</td>\n</tr>\n<tr>\n<td>Firing</td>\n<td>Alert is actively triggering notifications</td>\n<td>Duration requirement satisfied while condition remains true</td>\n<td>Condition no longer met for resolution duration</td>\n</tr>\n<tr>\n<td>Resolved</td>\n<td>Alert has recovered from firing state</td>\n<td>Firing alert condition returns to normal</td>\n<td>Alert rule deleted OR condition becomes true again</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Alert Instance Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>RuleID</td>\n<td>string</td>\n<td>References the parent alert rule configuration</td>\n</tr>\n<tr>\n<td>InstanceLabels</td>\n<td>Labels</td>\n<td>Label values extracted from the metric data that triggered this instance</td>\n</tr>\n<tr>\n<td>State</td>\n<td>AlertState</td>\n<td>Current state in the alert lifecycle (Inactive, Pending, Firing, Resolved)</td>\n</tr>\n<tr>\n<td>StateChanged</td>\n<td>time.Time</td>\n<td>Timestamp when alert last transitioned between states</td>\n</tr>\n<tr>\n<td>Value</td>\n<td>float64</td>\n<td>Most recent expression result value for debugging and display</td>\n</tr>\n<tr>\n<td>FiredAt</td>\n<td>*time.Time</td>\n<td>Timestamp when alert first entered firing state (nil if never fired)</td>\n</tr>\n<tr>\n<td>ResolvedAt</td>\n<td>*time.Time</td>\n<td>Timestamp when alert was resolved (nil if still active)</td>\n</tr>\n</tbody></table>\n<p>The <strong>notification system</strong> schema defines how alerts are delivered to external systems while handling delivery failures and rate limiting.</p>\n<table>\n<thead>\n<tr>\n<th>Notification Channel Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Type</td>\n<td>string</td>\n<td>Channel type identifier: &quot;email&quot;, &quot;slack&quot;, &quot;webhook&quot;, &quot;pagerduty&quot;</td>\n</tr>\n<tr>\n<td>Name</td>\n<td>string</td>\n<td>Human-readable channel name for management and debugging</td>\n</tr>\n<tr>\n<td>Config</td>\n<td>map[string]string</td>\n<td>Channel-specific configuration like URLs, tokens, and recipient lists</td>\n</tr>\n<tr>\n<td>Enabled</td>\n<td>bool</td>\n<td>Whether this channel should receive notifications</td>\n</tr>\n<tr>\n<td>RouteFilter</td>\n<td>Labels</td>\n<td>Label selector determining which alerts use this channel</td>\n</tr>\n</tbody></table>\n<p><strong>Dashboard configuration</strong> captures the declarative specification of data visualizations while supporting real-time updates and user customization. The schema separates presentation logic from data queries to enable flexible visualization while maintaining performance.</p>\n<table>\n<thead>\n<tr>\n<th>Dashboard Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ID</td>\n<td>string</td>\n<td>Unique dashboard identifier for URL routing and sharing</td>\n</tr>\n<tr>\n<td>Title</td>\n<td>string</td>\n<td>Human-readable dashboard name displayed in browser title and navigation</td>\n</tr>\n<tr>\n<td>Description</td>\n<td>string</td>\n<td>Optional dashboard description for documentation and search</td>\n</tr>\n<tr>\n<td>Tags</td>\n<td>[]string</td>\n<td>Categorization tags for dashboard organization and discovery</td>\n</tr>\n<tr>\n<td>Panels</td>\n<td>[]Panel</td>\n<td>Array of visualization panels arranged in the dashboard layout</td>\n</tr>\n<tr>\n<td>TimeRange</td>\n<td>TimeRange</td>\n<td>Default time window for all panels unless overridden</td>\n</tr>\n<tr>\n<td>RefreshInterval</td>\n<td>time.Duration</td>\n<td>How frequently panels should update their data automatically</td>\n</tr>\n<tr>\n<td>Editable</td>\n<td>bool</td>\n<td>Whether dashboard configuration can be modified by viewers</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Panel Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ID</td>\n<td>string</td>\n<td>Unique panel identifier within the dashboard for layout and updates</td>\n</tr>\n<tr>\n<td>Title</td>\n<td>string</td>\n<td>Panel title displayed above the visualization</td>\n</tr>\n<tr>\n<td>Type</td>\n<td>string</td>\n<td>Visualization type: &quot;line&quot;, &quot;bar&quot;, &quot;stat&quot;, &quot;heatmap&quot;, &quot;table&quot;</td>\n</tr>\n<tr>\n<td>GridPos</td>\n<td>GridPosition</td>\n<td>Panel position and size within the dashboard grid layout</td>\n</tr>\n<tr>\n<td>Query</td>\n<td>PanelQuery</td>\n<td>Data source query configuration for this panel</td>\n</tr>\n<tr>\n<td>Options</td>\n<td>map[string]interface{}</td>\n<td>Panel-specific configuration like axis labels, colors, and thresholds</td>\n</tr>\n<tr>\n<td>AlertRule</td>\n<td>*PanelAlertRule</td>\n<td>Optional alert rule configuration tied to this panel&#39;s query</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>Panel Query Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Expression</td>\n<td>string</td>\n<td>Time-series query expression to retrieve data for visualization</td>\n</tr>\n<tr>\n<td>TimeRange</td>\n<td>*TimeRange</td>\n<td>Panel-specific time range override if different from dashboard default</td>\n</tr>\n<tr>\n<td>RefreshInterval</td>\n<td>*time.Duration</td>\n<td>Panel-specific refresh rate override for high-frequency data</td>\n</tr>\n<tr>\n<td>MaxDataPoints</td>\n<td>int</td>\n<td>Maximum number of data points to retrieve for performance and readability</td>\n</tr>\n<tr>\n<td>MinInterval</td>\n<td>time.Duration</td>\n<td>Minimum step size for data aggregation to prevent over-resolution</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Dashboard State Synchronization</strong>\nDevelopers often implement dashboard updates by polling for new data on a fixed interval, leading to unnecessary load and inconsistent user experience. Instead, implement event-driven updates where the query engine notifies dashboards when relevant metric data changes. This provides instant updates for rapidly changing metrics while avoiding unnecessary queries for stable metrics. Implement exponential backoff for panels that haven&#39;t changed recently to optimize resource usage.</p>\n<p>⚠️ <strong>Pitfall: Alert Flapping</strong>\nAlert rules that toggle rapidly between firing and resolved states create notification spam and erode operator confidence. This typically occurs when thresholds are set exactly at normal operating values or when evaluation intervals are too short relative to metric volatility. Implement hysteresis by requiring different thresholds for firing versus resolving (e.g., fire at 90% CPU but don&#39;t resolve until below 80%), and require minimum durations for both firing and resolution states.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The data model implementation requires careful attention to serialization performance, validation consistency, and type safety across all system components. The following code structure provides a foundation for building robust data handling while maintaining flexibility for future enhancements.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Serialization</td>\n<td>JSON with <code>encoding/json</code></td>\n<td>Protocol Buffers with <code>protobuf/proto</code></td>\n</tr>\n<tr>\n<td>Validation</td>\n<td>Manual validation functions</td>\n<td>Struct tags with <code>validator</code> library</td>\n</tr>\n<tr>\n<td>Label Storage</td>\n<td>Map with string keys/values</td>\n<td>Interned strings with reference counting</td>\n</tr>\n<tr>\n<td>Time Handling</td>\n<td><code>time.Time</code> with UTC normalization</td>\n<td>Custom timestamp type with nanosecond precision</td>\n</tr>\n<tr>\n<td>Hash Function</td>\n<td><code>crypto/sha256</code> for series IDs</td>\n<td><code>xxhash</code> for performance-critical paths</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/model/\n  types.go              ← Core data types (Metric, Sample, Labels)\n  metric_types.go       ← MetricType enum and behavior\n  series.go             ← Time-series identification and indexing\n  alerts.go             ← Alert rule and state management types  \n  dashboard.go          ← Dashboard and panel configuration types\n  validation.go         ← Input validation and constraint checking\n  serialization.go      ← JSON/binary serialization helpers\n  types_test.go         ← Comprehensive unit tests for all types</code></pre></div>\n\n<p><strong>Core Types Infrastructure (Complete Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> model</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/sha256</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sort</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricType defines the fundamental behavior of a metric</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricType</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeCounter</span><span style=\"color:#B392F0\">   MetricType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeGauge</span><span style=\"color:#B392F0\">     MetricType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MetricTypeHistogram</span><span style=\"color:#B392F0\"> MetricType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">mt </span><span style=\"color:#B392F0\">MetricType</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> mt {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> MetricTypeCounter:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"counter\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> MetricTypeGauge:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"gauge\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> MetricTypeHistogram:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"histogram\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Labels represents metric metadata with validation and indexing support</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Copy creates a deep copy of the labels map</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Copy</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> l </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    result </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(l))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> l {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result[k] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// String returns the canonical string representation for hashing and display</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(l) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"{}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pairs </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(l))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> l {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pairs </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(pairs, fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">`</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">=\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"`</span><span style=\"color:#E1E4E8\">, k, v))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sort.</span><span style=\"color:#B392F0\">Strings</span><span style=\"color:#E1E4E8\">(pairs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"{\"</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(pairs, </span><span style=\"color:#9ECBFF\">\",\"</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"}\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Sample represents a single timestamped measurement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Sample</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value     </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Metric represents a named time-series with type and metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Metric</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">     `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type    </span><span style=\"color:#B392F0\">MetricType</span><span style=\"color:#9ECBFF\"> `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels  </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#9ECBFF\">     `json:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Samples []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#9ECBFF\">   `json:\"samples\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesID generates a unique, deterministic identifier for this time-series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SeriesID</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create hash input string starting with metric name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Sort labels by key to ensure deterministic ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Append each label as \"key=value\\n\" to hash input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Compute SHA-256 hash of the complete string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return hexadecimal encoding of hash for readability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use crypto/sha256 and fmt.Sprintf for hex encoding</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate checks metric data integrity and constraint compliance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate metric name format (alphanumeric + underscore, no leading numbers)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check metric type is one of the defined enum values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate each label key/value against length and format constraints  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Ensure samples are sorted by timestamp (required for efficient storage)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check for reasonable value ranges (no NaN/Inf for counters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use regular expressions for name/label validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Alert and Dashboard Types (Skeleton Implementation):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// AlertRule defines conditions for triggering notifications</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertRule</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID                 </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name               </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Expression         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"expression\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Threshold          </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">           `json:\"threshold\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operator           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"operator\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration           </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">     `json:\"duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EvaluationInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\">     `json:\"evaluation_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels             </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#9ECBFF\">            `json:\"labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Annotations        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"annotations\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validate ensures alert rule configuration is valid and safe</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ar </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Validate</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate ID is non-empty and URL-safe</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check operator is one of: \"gt\", \"lt\", \"eq\", \"ne\"  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Ensure duration and evaluation interval are positive</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate expression syntax (implement basic PromQL parser)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check threshold is finite (no NaN/Inf values)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Dashboard represents a collection of metric visualization panels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Dashboard</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID              </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Title           </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"title\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Description     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"description\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Tags            []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"tags\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Panels          []</span><span style=\"color:#B392F0\">Panel</span><span style=\"color:#9ECBFF\">       `json:\"panels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange       </span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#9ECBFF\">     `json:\"time_range\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RefreshInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"refresh_interval\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Editable        </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">          `json:\"editable\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Panel represents a single visualization within a dashboard</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Panel</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Title     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"title\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GridPos   </span><span style=\"color:#B392F0\">GridPosition</span><span style=\"color:#9ECBFF\">          `json:\"grid_pos\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query     </span><span style=\"color:#B392F0\">PanelQuery</span><span style=\"color:#9ECBFF\">            `json:\"query\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Options   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{} </span><span style=\"color:#9ECBFF\">`json:\"options\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AlertRule </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PanelAlertRule</span><span style=\"color:#9ECBFF\">       `json:\"alert_rule,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRange specifies absolute or relative time bounds for queries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    From </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"from\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    To   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"to\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GridPosition defines panel layout within dashboard grid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> GridPosition</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    X      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `json:\"x\"`</span><span style=\"color:#6A737D\">      // Horizontal position (0-24)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Y      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `json:\"y\"`</span><span style=\"color:#6A737D\">      // Vertical position  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Width  </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `json:\"width\"`</span><span style=\"color:#6A737D\">  // Panel width (1-24)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Height </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\"> `json:\"height\"`</span><span style=\"color:#6A737D\"> // Panel height in grid units</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Validation and Serialization Helpers:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ValidationError represents constraint violations during data validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ValidationError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Field   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"field\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ve </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ValidationError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"validation failed for field '</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">': </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, ve.Field, ve.Message)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateLabels checks label constraints for cardinality and format compliance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> ValidateLabels</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check total label count against maximum limit (32)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate each key matches allowed pattern ^[a-zA-Z_][a-zA-Z0-9_]*$</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Ensure keys don't start with __ (reserved for system labels)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check key/value length limits (128/1024 characters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return ValidationError with specific constraint violations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use regexp.MustCompile for pattern validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SerializeMetric converts metric to JSON with proper timestamp formatting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> SerializeMetric</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">m</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create a serialization wrapper with RFC3339 timestamp format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Convert time.Time values to standardized string representation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use json.Marshal with proper error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Consider compression for large sample arrays</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Create intermediate struct with string timestamps for JSON compatibility</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints:</strong></p>\n<ul>\n<li>Use <code>time.Time.UTC()</code> to normalize all timestamps and avoid timezone issues</li>\n<li>Implement <code>json.Marshaler</code> and <code>json.Unmarshaler</code> interfaces for custom timestamp formatting</li>\n<li>Use <code>sync.Pool</code> for label validation to avoid allocation overhead during high-throughput ingestion</li>\n<li>Consider using <code>unsafe.Pointer</code> for zero-copy string operations in label processing (advanced optimization)</li>\n<li>Use <code>sort.Slice</code> with stable sorting for deterministic label ordering in series ID generation</li>\n</ul>\n<p><strong>Milestone Checkpoint:</strong>\nAfter implementing the data model types, verify correctness with:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/model/...</span></span></code></pre></div>\n\n<p>Expected test coverage should validate:</p>\n<ul>\n<li>Series ID generation produces identical results for same metric+labels regardless of label order</li>\n<li>Label validation rejects malformed keys/values and enforces cardinality limits  </li>\n<li>Metric validation catches type mismatches and constraint violations</li>\n<li>JSON serialization round-trips preserve all data with proper timestamp formatting</li>\n<li>Copy operations create true deep copies that don&#39;t share memory references</li>\n</ul>\n<p>Signs of implementation issues:</p>\n<ul>\n<li><strong>Series ID inconsistency</strong>: Same metric produces different IDs → check label sorting in SeriesID()</li>\n<li><strong>Validation bypass</strong>: Invalid data passes validation → tighten constraint checking in Validate() methods</li>\n<li><strong>Memory leaks</strong>: Label copying shares references → ensure Copy() creates independent map instances</li>\n<li><strong>Timestamp precision loss</strong>: Nanosecond precision not preserved → use RFC3339Nano format in JSON serialization</li>\n</ul>\n<h2 id=\"metrics-ingestion-engine\">Metrics Ingestion Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 1 (Metrics Collection) - This section details the first major component that handles incoming metrics data with both push and pull collection models</p>\n</blockquote>\n<h3 id=\"mental-model-the-data-processing-factory\">Mental Model: The Data Processing Factory</h3>\n<p>Think of the metrics ingestion engine as a sophisticated <strong>data processing factory</strong> with multiple assembly lines working in parallel. Just like a modern manufacturing plant, our factory has several key characteristics:</p>\n<p><strong>Receiving Docks</strong>: The factory has two types of receiving areas. The <strong>push dock</strong> is like a loading bay where delivery trucks (applications) arrive at any time to drop off shipments of raw materials (metrics data). The <strong>pull dock</strong> operates more like a scheduled pickup service where factory workers (scrapers) drive out on regular routes to collect materials from various suppliers (application endpoints) according to a predetermined schedule.</p>\n<p><strong>Quality Control Stations</strong>: Before any raw materials enter the main production line, they pass through rigorous <strong>quality control checkpoints</strong>. Inspectors (validators) examine each shipment to ensure it meets specifications - checking that labels are properly formatted, metric types are valid, and that we&#39;re not receiving so many unique product variations that our storage systems would overflow. Materials that don&#39;t pass inspection are either rejected with detailed error reports or sent to a preprocessing station for standardization.</p>\n<p><strong>Assembly Line Processing</strong>: Once materials pass quality control, they enter the main <strong>assembly line</strong> where workers (processors) perform standardization operations. Counter materials get special handling to ensure they&#39;re always increasing, gauge materials are processed to capture point-in-time snapshots, and histogram materials are sorted into predefined buckets. Each processed item gets a unique identifier (series ID) based on its name and characteristics.</p>\n<p><strong>Dispatch Center</strong>: Finally, processed materials are packaged and sent to the <strong>warehouse</strong> (storage engine) with proper routing information. The dispatch center maintains strict ordering and batching to ensure the warehouse can efficiently store everything without becoming overwhelmed.</p>\n<p>This factory operates 24/7 with <strong>built-in resilience mechanisms</strong>. If one assembly line gets backed up, work can be redistributed. If quality control detects a problematic supplier, it can temporarily reject their materials while alerting operations staff. The entire system is designed to handle sudden spikes in incoming materials while maintaining consistent quality and throughput.</p>\n<h3 id=\"push-vs-pull-ingestion-models\">Push vs Pull Ingestion Models</h3>\n<p>The choice between push and pull ingestion models represents one of the most fundamental architectural decisions in metrics collection systems. Each model has distinct advantages and addresses different operational patterns, making it essential to support both approaches in a comprehensive metrics platform.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fingestion-flow.svg\" alt=\"Metric Ingestion Data Flow\"></p>\n<blockquote>\n<p><strong>Decision: Support Both Push and Pull Ingestion Models</strong></p>\n<ul>\n<li><strong>Context</strong>: Applications have different operational patterns - some generate metrics continuously and prefer to send data immediately, while others benefit from having a central system collect data on a schedule. Additionally, different deployment environments favor different approaches.</li>\n<li><strong>Options Considered</strong>: Push-only (applications send data), Pull-only (server scrapes data), Hybrid approach (support both)</li>\n<li><strong>Decision</strong>: Implement hybrid approach supporting both push and pull models</li>\n<li><strong>Rationale</strong>: Push provides immediate data delivery and works well with ephemeral workloads, while pull provides centralized control and works better with service discovery. Supporting both maximizes compatibility and deployment flexibility.</li>\n<li><strong>Consequences</strong>: Increased implementation complexity but significantly broader applicability across different environments and use cases.</li>\n</ul>\n</blockquote>\n<h4 id=\"push-based-ingestion-architecture\">Push-Based Ingestion Architecture</h4>\n<p>In the <strong>push model</strong>, applications take responsibility for actively sending their metrics data to the ingestion engine. This approach treats the metrics system as a service that applications can call whenever they have data to report. The ingestion engine operates as a passive receiver, exposing HTTP endpoints that accept metric payloads and respond with success or error indicators.</p>\n<p>The push model excels in environments with <strong>dynamic or ephemeral workloads</strong>. Consider a serverless function that runs for only a few seconds - it can immediately push its execution metrics before terminating, ensuring no data is lost. Similarly, applications running in containers that scale up and down rapidly can send metrics as soon as they&#39;re generated, without requiring the metrics system to maintain awareness of their existence.</p>\n<p>Push-based ingestion also provides <strong>immediate data availability</strong>. The moment an application generates a critical metric - such as an error rate spike or a performance degradation - it can send that data directly to the ingestion engine. This immediacy is crucial for time-sensitive alerting scenarios where even a few seconds of delay could impact response times.</p>\n<p>However, the push model places <strong>operational burden on applications</strong>. Each application must implement metrics client libraries, handle network failures, implement retry logic, and manage authentication credentials for the metrics service. Applications must also be configured with the metrics service endpoint, creating operational dependencies that can complicate deployments.</p>\n<h4 id=\"pull-based-ingestion-architecture\">Pull-Based Ingestion Architecture</h4>\n<p>The <strong>pull model</strong> inverts the responsibility relationship, making the ingestion engine an active collector that periodically scrapes metrics from application endpoints. Applications expose their current metrics via HTTP endpoints (typically <code>/metrics</code>) in a standardized format, and the ingestion engine maintains a schedule of which endpoints to scrape and how frequently.</p>\n<p>This approach provides <strong>centralized operational control</strong>. The metrics system maintains the master list of what to monitor, how frequently to collect data, and how to authenticate with each endpoint. Operations teams can add new applications to monitoring, adjust collection frequencies, or temporarily disable problematic endpoints without requiring application changes or redeployments.</p>\n<p>Pull-based collection also enables <strong>sophisticated service discovery integration</strong>. The ingestion engine can automatically discover new application instances through service registries, cloud provider APIs, or orchestration platforms like Kubernetes. As new instances start up, they automatically become part of the monitoring system without manual configuration.</p>\n<p>The pull model provides <strong>built-in health checking capabilities</strong>. If an application becomes unresponsive, the scraping process will detect this condition through failed HTTP requests, enabling the metrics system to generate alerts about application availability. This creates a natural integration between metrics collection and basic health monitoring.</p>\n<p>However, pull-based collection has <strong>timing limitations</strong>. Metrics are only collected at predetermined intervals, meaning short-lived events or rapid changes might be missed between scrapes. Additionally, applications behind firewalls or NAT may not be reachable by external scrapers, limiting deployment options.</p>\n<h4 id=\"implementation-strategy-comparison\">Implementation Strategy Comparison</h4>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Push Model</th>\n<th>Pull Model</th>\n<th>Hybrid Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Data Freshness</strong></td>\n<td>Immediate (sub-second)</td>\n<td>Delayed (scrape interval)</td>\n<td>Best of both worlds</td>\n</tr>\n<tr>\n<td><strong>Operational Complexity</strong></td>\n<td>Distributed (each app)</td>\n<td>Centralized (metrics system)</td>\n<td>Higher initial complexity</td>\n</tr>\n<tr>\n<td><strong>Service Discovery</strong></td>\n<td>Manual configuration</td>\n<td>Automatic discovery</td>\n<td>Both manual and automatic</td>\n</tr>\n<tr>\n<td><strong>Network Requirements</strong></td>\n<td>Outbound from apps</td>\n<td>Inbound to apps</td>\n<td>Both directions</td>\n</tr>\n<tr>\n<td><strong>Ephemeral Workloads</strong></td>\n<td>Excellent support</td>\n<td>Poor support</td>\n<td>Excellent support</td>\n</tr>\n<tr>\n<td><strong>Debugging Visibility</strong></td>\n<td>Limited (app-side logs)</td>\n<td>Excellent (central logs)</td>\n<td>Comprehensive visibility</td>\n</tr>\n<tr>\n<td><strong>Authentication</strong></td>\n<td>Per-app credentials</td>\n<td>Central credential management</td>\n<td>Flexible auth strategies</td>\n</tr>\n</tbody></table>\n<h4 id=\"hybrid-implementation-design\">Hybrid Implementation Design</h4>\n<p>Our metrics ingestion engine implements both models through a <strong>unified processing pipeline</strong> that normalizes data from both sources before validation and storage. This design ensures consistent behavior regardless of how metrics arrive in the system.</p>\n<p>The <strong>HTTP API layer</strong> provides RESTful endpoints for push-based submissions while also hosting the scraping orchestrator that manages pull-based collection. Both pathways converge into the same validation and preprocessing pipeline, ensuring data consistency and uniform error handling.</p>\n<p><strong>Configuration management</strong> allows operators to define both push endpoints (for applications to submit data) and pull targets (for the system to scrape) in a single configuration file. This unified approach simplifies operational procedures and provides a single place to manage all metric collection policies.</p>\n<p>The system maintains <strong>separate health monitoring</strong> for push and pull pathways. Push endpoints are monitored for request rates, error rates, and processing latency. Pull operations are monitored for scrape success rates, target reachability, and scrape duration. This comprehensive monitoring ensures operators can quickly identify issues with either collection method.</p>\n<h3 id=\"validation-and-preprocessing\">Validation and Preprocessing</h3>\n<p>The validation and preprocessing stage serves as the <strong>quality gateway</strong> between raw metric submissions and the storage engine. This component must handle the inherent messiness of real-world data while protecting the system from malicious or malformed inputs that could compromise performance or reliability.</p>\n<h4 id=\"label-cardinality-control\">Label Cardinality Control</h4>\n<p><strong>Label cardinality explosion</strong> represents one of the most dangerous failure modes in metrics systems. Cardinality refers to the number of unique combinations of label key-value pairs across all metrics. When applications generate labels with unbounded or high-variability values - such as user IDs, request IDs, or timestamps - the number of unique time series grows exponentially.</p>\n<p>Consider an application that creates a metric <code>http_requests_total</code> with labels for <code>method</code>, <code>status_code</code>, and <code>user_id</code>. If the application serves 1 million users, this single metric type generates 1 million unique time series (assuming one method and status code combination per user). With multiple methods and status codes, the cardinality explodes to millions or tens of millions of series.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: Each unique time series consumes memory for indexing and storage space for samples. Cardinality explosions can quickly exhaust system resources, causing the entire metrics platform to become unresponsive or crash.</p>\n</blockquote>\n<p>The ingestion engine implements <strong>multi-layered cardinality protection</strong>:</p>\n<p><strong>Static Label Validation</strong> examines each metric submission to ensure labels conform to naming conventions and value constraints. Label names must follow DNS-like naming rules (alphanumeric characters and underscores only), and label values are checked against length limits and character restrictions.</p>\n<p><strong>Dynamic Cardinality Tracking</strong> maintains counters for unique time series being created in real-time. The system tracks cardinality per metric name, per application source, and globally. When any of these counters approaches configured thresholds, the system begins rejecting new series creation while continuing to accept samples for existing series.</p>\n<p><strong>High-Cardinality Detection</strong> uses statistical analysis to identify potentially problematic label patterns. If a metric name suddenly generates many new series in a short time period, or if specific label keys show high variability, the system flags these patterns for operator review and can automatically implement temporary restrictions.</p>\n<h4 id=\"metric-type-validation\">Metric Type Validation</h4>\n<p>Each metric type has specific <strong>semantic requirements</strong> that must be enforced during ingestion to ensure data consistency and query correctness. The validation logic differs significantly across the three primary metric types.</p>\n<p><strong>Counter Validation</strong> ensures that counter metrics behave as monotonically increasing cumulative values. The ingestion engine tracks the last seen value for each counter series and rejects samples with values lower than the previous sample (which would indicate incorrect counter usage). When counter resets occur (such as application restarts), the system detects these by identifying large decreases in value and handles them appropriately by recording reset events.</p>\n<p><strong>Gauge Validation</strong> is more permissive since gauges represent point-in-time measurements that can increase or decrease freely. However, the system still validates that gauge values fall within reasonable numeric ranges and aren&#39;t special values like NaN or infinity unless explicitly configured to allow them.</p>\n<p><strong>Histogram Validation</strong> requires the most complex logic since histograms consist of multiple related series (buckets, count, and sum). The ingestion engine validates that bucket values are monotonically increasing across bucket boundaries, that the count matches the sum of all bucket increments, and that the sum value is mathematically consistent with the observed count and bucket distributions.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Type</th>\n<th>Key Validations</th>\n<th>Failure Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Counter</strong></td>\n<td>Monotonic increase, numeric range, reset detection</td>\n<td>Reject decreasing values, log reset events</td>\n</tr>\n<tr>\n<td><strong>Gauge</strong></td>\n<td>Numeric range, finite values</td>\n<td>Reject invalid numbers, allow all valid changes</td>\n</tr>\n<tr>\n<td><strong>Histogram</strong></td>\n<td>Bucket consistency, count/sum coherence, bucket ordering</td>\n<td>Reject inconsistent histograms, validate all components</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-normalization\">Data Normalization</h4>\n<p>Raw metric data arrives in various formats and with inconsistent naming patterns. The <strong>data normalization process</strong> standardizes this data into canonical formats that the storage engine can efficiently process.</p>\n<p><strong>Metric Name Normalization</strong> converts metric names to standard formats, typically lowercase with underscores separating words. The system may also apply prefix rules (such as adding application or environment prefixes) and suffix standardization (ensuring counter metrics end with <code>_total</code> or similar conventions).</p>\n<p><strong>Label Standardization</strong> includes sorting label keys alphabetically, normalizing label values (such as converting HTTP status codes to strings or normalizing case), and applying label transformation rules. The system may also inject standard labels such as application name, environment, or collection timestamp.</p>\n<p><strong>Timestamp Processing</strong> handles the complexities of time-based data. Samples may arrive with timestamps in various formats (Unix seconds, milliseconds, nanoseconds, or ISO 8601 strings), in different time zones, or without timestamps at all. The normalization process converts all timestamps to a canonical format (typically Unix nanoseconds in UTC) and may apply clock skew detection to identify submissions with timestamps significantly different from the ingestion time.</p>\n<p><strong>Value Normalization</strong> ensures numeric consistency across different input formats. This includes handling scientific notation, different decimal precision levels, and unit conversions where configured (such as converting milliseconds to seconds for duration metrics).</p>\n<h4 id=\"error-handling-and-feedback\">Error Handling and Feedback</h4>\n<p>The validation and preprocessing pipeline must provide <strong>detailed error feedback</strong> while maintaining high throughput performance. This requires careful balance between comprehensive validation and processing speed.</p>\n<p><strong>Validation Error Classification</strong> categorizes errors into different severity levels:</p>\n<ul>\n<li><strong>Fatal errors</strong> (malformed JSON, missing required fields) that cause entire batch rejection</li>\n<li><strong>Warning errors</strong> (unusual cardinality patterns, timestamp skew) that allow processing but generate alerts  </li>\n<li><strong>Individual metric errors</strong> (single invalid metric in a batch) that reject specific metrics while processing others</li>\n</ul>\n<p><strong>Error Response Design</strong> provides structured feedback that applications can use to fix submission problems:</p>\n<table>\n<thead>\n<tr>\n<th>Error Type</th>\n<th>HTTP Status</th>\n<th>Response Format</th>\n<th>Retry Recommendation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Malformed Request</strong></td>\n<td>400 Bad Request</td>\n<td>JSON with field-specific errors</td>\n<td>Fix format, retry immediately</td>\n</tr>\n<tr>\n<td><strong>Cardinality Limit</strong></td>\n<td>429 Too Many Requests</td>\n<td>JSON with cardinality details</td>\n<td>Reduce cardinality, retry with backoff</td>\n</tr>\n<tr>\n<td><strong>Authentication</strong></td>\n<td>401 Unauthorized</td>\n<td>JSON with auth requirements</td>\n<td>Fix credentials, retry immediately</td>\n</tr>\n<tr>\n<td><strong>Rate Limiting</strong></td>\n<td>429 Too Many Requests</td>\n<td>JSON with retry-after header</td>\n<td>Respect rate limits, retry after delay</td>\n</tr>\n<tr>\n<td><strong>Server Error</strong></td>\n<td>500 Internal Server Error</td>\n<td>JSON with error tracking ID</td>\n<td>Retry with exponential backoff</td>\n</tr>\n</tbody></table>\n<p><strong>Batch Processing Logic</strong> handles mixed-success scenarios where some metrics in a batch are valid while others fail validation. The system processes all valid metrics and returns detailed error information for failed metrics, allowing applications to resubmit only the corrected data.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Ignoring Label Cardinality Limits</strong>\nMany developers create metrics with labels containing user IDs, request IDs, or other high-cardinality values without understanding the exponential impact on system resources. A single metric with a user ID label in a system with millions of users creates millions of unique time series.\n<strong>Why it&#39;s wrong</strong>: Each unique time series consumes memory for indexing and storage space for data points. High cardinality can quickly exhaust system memory and make queries extremely slow.\n<strong>How to fix</strong>: Implement strict cardinality limits per metric name (typically 1000-10000 series). Use high-cardinality values in logs, not metrics. Create aggregate metrics instead of per-entity metrics.</p>\n<p>⚠️ <strong>Pitfall: Not Handling Clock Skew in Distributed Systems</strong>\nWhen multiple applications submit metrics with their local timestamps, clock differences between systems can cause samples to arrive with timestamps in the future or significantly in the past, breaking time-series ordering assumptions.\n<strong>Why it&#39;s wrong</strong>: Storage engines often assume samples arrive in roughly chronological order. Out-of-order samples can cause index corruption or query inconsistencies.\n<strong>How to fix</strong>: Detect timestamp skew during ingestion (samples more than 5 minutes in future or 1 hour in past). Either reject skewed samples or use server-side timestamps for all ingestion.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Validation of Counter Decreases</strong>\nAllowing counter metrics to decrease without proper handling breaks the fundamental counter semantic and makes rate calculations incorrect.\n<strong>Why it&#39;s wrong</strong>: Counters represent cumulative values that should only increase. Decreases typically indicate counter resets (like application restarts) that need special handling for accurate rate calculations.\n<strong>How to fix</strong>: Track previous counter values per series. Reject samples with decreased values unless they represent valid resets (detected by large decreases). Log reset events for debugging.</p>\n<p>⚠️ <strong>Pitfall: Blocking Ingestion During Validation</strong>\nPerforming expensive validation operations (like cardinality lookups or histogram consistency checks) in the main ingestion request path creates latency spikes and reduces throughput.\n<strong>Why it&#39;s wrong</strong>: Applications submitting metrics expect fast response times. Slow validation creates backpressure that can affect application performance.\n<strong>How to fix</strong>: Use asynchronous validation where possible. Perform expensive checks in background workers. Cache validation results to avoid repeated expensive operations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>HTTP Server</strong></td>\n<td>Go net/http with middleware</td>\n<td>Fiber or Echo framework</td>\n</tr>\n<tr>\n<td><strong>Metric Parsing</strong></td>\n<td>encoding/json for JSON format</td>\n<td>Prometheus text format parser</td>\n</tr>\n<tr>\n<td><strong>Cardinality Tracking</strong></td>\n<td>In-memory maps with sync.RWMutex</td>\n<td>Redis or embedded key-value store</td>\n</tr>\n<tr>\n<td><strong>Validation Rules</strong></td>\n<td>Hard-coded validation functions</td>\n<td>Rule engine with YAML configuration</td>\n</tr>\n<tr>\n<td><strong>Background Processing</strong></td>\n<td>Goroutines with channels</td>\n<td>Worker pool with job queue</td>\n</tr>\n<tr>\n<td><strong>Health Checking</strong></td>\n<td>Simple HTTP endpoint</td>\n<td>Comprehensive health check framework</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/ingestion/\n  ingester.go              ← Main MetricsIngester implementation\n  ingester_test.go         ← Unit tests for ingestion logic\n  validator.go             ← Validation and preprocessing logic\n  validator_test.go        ← Validation tests\n  scraper.go               ← Pull-based metric collection\n  scraper_test.go          ← Scraper tests\n  handlers/\n    push_handler.go        ← HTTP handlers for push ingestion\n    health_handler.go      ← Health check endpoints\n    scrape_handler.go      ← Scrape target management\n  models/\n    metric.go              ← Core metric data structures\n    validation.go          ← Validation error types\n  config/\n    ingestion_config.go    ← Configuration structures</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>HTTP Server Infrastructure</strong> (complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ingestion</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Server wraps HTTP server with graceful shutdown and middleware</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Server</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpServer </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ingester   </span><span style=\"color:#B392F0\">MetricsIngester</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewServer creates HTTP server with configured routes and middleware</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">addr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ingester</span><span style=\"color:#B392F0\"> MetricsIngester</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> http.</span><span style=\"color:#B392F0\">NewServeMux</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        httpServer: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Addr:         addr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            Handler:      mux,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ReadTimeout:  </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            WriteTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:   logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ingester: ingester,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server.</span><span style=\"color:#B392F0\">registerRoutes</span><span style=\"color:#E1E4E8\">(mux)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">registerRoutes</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">mux</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ServeMux</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Push ingestion endpoint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/metrics\"</span><span style=\"color:#E1E4E8\">, s.handlePushMetrics)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Health check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/health\"</span><span style=\"color:#E1E4E8\">, s.handleHealth)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Scrape target management</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mux.</span><span style=\"color:#B392F0\">HandleFunc</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"/api/v1/scrape\"</span><span style=\"color:#E1E4E8\">, s.handleScrapeConfig)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"starting ingestion server\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"addr\"</span><span style=\"color:#E1E4E8\">, s.httpServer.Addr)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Server</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    s.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"shutting down ingestion server\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> s.httpServer.</span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Validation Error Infrastructure</strong> (complete implementation):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> models</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidationError represents field-specific validation failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ValidationError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Field   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"field\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#B392F0\">ValidationError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"validation failed for field '</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">' with value '</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">': </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        v.Field, v.Value, v.Message)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidationErrors represents multiple validation failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ValidationErrors</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Errors []</span><span style=\"color:#B392F0\">ValidationError</span><span style=\"color:#9ECBFF\"> `json:\"errors\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#B392F0\">ValidationErrors</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> messages []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> v.Errors {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        messages </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(messages, err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> strings.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(messages, </span><span style=\"color:#9ECBFF\">\"; \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#B392F0\">ValidationErrors</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HasErrors</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(v.Errors) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ValidationErrors</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">field</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    v.Errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(v.Errors, </span><span style=\"color:#B392F0\">ValidationError</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Field:   field,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Value:   value,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Message: message,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ToJSON serializes validation errors for HTTP responses</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#B392F0\">ValidationErrors</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ToJSON</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(v)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Main Ingestion Interface</strong> (signatures + TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ingestion</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricsIngester handles both push and pull metric collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricsIngester</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    IngestMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ScrapeEndpoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">url</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetStats</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">IngestionStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IngestionStats provides operational metrics about the ingestion process</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> IngestionStats</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MetricsReceived    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MetricsRejected    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ValidationErrors   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CardinalityViolations </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ScrapeTargets     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastScrapeTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Implementation skeleton</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ingesterImpl</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validator      </span><span style=\"color:#B392F0\">Validator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage        </span><span style=\"color:#B392F0\">TimeSeriesStorage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cardinalityMgr </span><span style=\"color:#B392F0\">CardinalityManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stats          </span><span style=\"color:#B392F0\">IngestionStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IngestMetrics processes a batch of metrics through validation and storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">i </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ingesterImpl</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IngestMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize validation errors collector</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each metric in the batch:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Validate metric name and type using i.validator.ValidateMetric()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check cardinality limits using i.cardinalityMgr.CheckCardinality()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Normalize labels and values using i.validator.NormalizeMetric()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Add to validated batch if all checks pass</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If any metrics passed validation:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Convert to storage format using models.MetricToSamples()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Call i.storage.WriteSamples() with validated metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Update ingestion statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return validation errors if any metrics failed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sync.Mutex to protect stats updates in concurrent scenarios</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Validation Logic</strong> (signatures + TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ingestion</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Validator handles metric validation and normalization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Validator</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ValidateMetric</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metric</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ValidationErrors</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    NormalizeMetric</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metric</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> validatorImpl</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metricNameRegex </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Regexp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    labelNameRegex  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Regexp</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxLabelLength  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxValueLength  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateMetric checks metric name, type, labels, and values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">v </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">validatorImpl</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateMetric</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metric</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ValidationErrors</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    errors </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ValidationErrors</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate metric name:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check if name matches regex pattern (alphanumeric + underscore)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Verify name length is within limits (1-200 characters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Ensure name doesn't start with double underscore (reserved)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate metric type:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check if Type is one of Counter, Gauge, Histogram</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For Counter: verify all sample values are >= 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For Histogram: validate bucket structure and consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate labels:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check each label name matches regex pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Verify label names don't start with double underscore</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Validate label values are within length limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check for reserved label names (__name__, job, instance)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate samples:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Ensure timestamps are within acceptable range (not too far in past/future)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - Check that sample values are finite numbers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - For counters, verify values don't decrease (unless reset detected)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use errors.Add() to collect multiple validation failures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Cardinality Management</strong> (signatures + TODOs):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ingestion</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">internal/models</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CardinalityManager tracks and limits time-series cardinality</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CardinalityManager</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CheckCardinality</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metric</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetCardinalityStats</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">CardinalityStats</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    RegisterSeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seriesID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CardinalityStats</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalSeries     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SeriesPerMetric </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxCardinality  </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> cardinalityManagerImpl</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu               </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seriesIndex      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">  // seriesID -> exists</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metricCardinality </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">int</span><span style=\"color:#6A737D\">   // metricName -> count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSeriesTotal   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSeriesPerMetric </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckCardinality verifies adding this metric won't exceed cardinality limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">cardinalityManagerImpl</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckCardinality</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metric</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">models</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> c.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate series ID using metric.SeriesID() method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if this series already exists in c.seriesIndex</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If exists, return nil (no new cardinality impact)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check global cardinality limit:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If len(c.seriesIndex) >= c.maxSeriesTotal, return cardinality error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Check per-metric cardinality limit:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   - If c.metricCardinality[metric.Name] >= c.maxSeriesPerMetric, return error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Temporarily register the series (actual registration happens after successful ingestion)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use metric name + sorted labels to create consistent series ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-hints\">Language-Specific Hints</h4>\n<p><strong>Go-Specific Implementation Details</strong>:</p>\n<ul>\n<li>Use <code>sync.RWMutex</code> for cardinality tracking to allow concurrent read access while protecting writes</li>\n<li>Implement <code>http.HandlerFunc</code> for push endpoints with proper JSON parsing using <code>json.NewDecoder(r.Body)</code></li>\n<li>Use <code>time.NewTicker</code> for scrape scheduling with proper cleanup in goroutines</li>\n<li>Handle context cancellation in long-running operations using <code>select</code> with <code>ctx.Done()</code></li>\n<li>Use <code>slog.Logger</code> for structured logging with contextual fields</li>\n<li>Implement graceful shutdown by listening for OS signals with <code>signal.Notify</code></li>\n</ul>\n<p><strong>Error Handling Patterns</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Wrap validation errors with context</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> validator.</span><span style=\"color:#B392F0\">ValidateMetric</span><span style=\"color:#E1E4E8\">(metric); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"validation failed for metric </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, metric.Name, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Use typed errors for different failure modes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">var</span><span style=\"color:#E1E4E8\"> ErrCardinalityExceeded </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> errors.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"cardinality limit exceeded\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Concurrency Patterns</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Worker pool for batch processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> numWorkers; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> batch </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> workChan {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> processBatch</span><span style=\"color:#E1E4E8\">(batch); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                errorChan </span><span style=\"color:#F97583\">&#x3C;-</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the ingestion engine, verify the following behaviors:</p>\n<p><strong>Push Ingestion Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start the server</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Submit test metrics via HTTP</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/metrics</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '[{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"name\": \"test_counter_total\",</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"type\": 0,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"labels\": {\"job\": \"test\", \"instance\": \"localhost:8080\"},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"samples\": [{\"value\": 42.0, \"timestamp\": \"'</span><span style=\"color:#E1E4E8\">$(</span><span style=\"color:#B392F0\">date</span><span style=\"color:#79B8FF\"> -u</span><span style=\"color:#9ECBFF\"> +%s</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#9ECBFF\">'\"}]</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  }]'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 200 response with empty JSON body {}</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected logs: \"ingested 1 metrics successfully\"</span></span></code></pre></div>\n\n<p><strong>Validation Error Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Submit invalid metric</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/metrics</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '[{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"name\": \"\",</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"type\": 999,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"labels\": {},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"samples\": []</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  }]'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 400 response with validation errors JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected response body contains field-specific error details</span></span></code></pre></div>\n\n<p><strong>Cardinality Limit Test</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Configure low cardinality limit in config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Submit metrics with many unique label combinations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 429 after hitting cardinality limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected logs: \"cardinality limit exceeded for metric X\"</span></span></code></pre></div>\n\n<p><strong>Health Check Verification</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> http://localhost:8080/health</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: HTTP 200 with {\"status\": \"healthy\", \"timestamp\": \"...\"}</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Metrics not appearing in storage</strong></td>\n<td>Validation failures silently dropping data</td>\n<td>Check ingestion logs for validation errors</td>\n<td>Add comprehensive logging to validation pipeline</td>\n</tr>\n<tr>\n<td><strong>High memory usage</strong></td>\n<td>Cardinality explosion from unbounded labels</td>\n<td>Monitor cardinality statistics endpoint</td>\n<td>Implement stricter label validation and limits</td>\n</tr>\n<tr>\n<td><strong>Slow ingestion performance</strong></td>\n<td>Synchronous validation blocking request processing</td>\n<td>Profile CPU usage during ingestion</td>\n<td>Move expensive validation to background workers</td>\n</tr>\n<tr>\n<td><strong>Inconsistent timestamps</strong></td>\n<td>Clock skew between client and server</td>\n<td>Compare ingestion timestamps with sample timestamps</td>\n<td>Implement timestamp normalization and skew detection</td>\n</tr>\n<tr>\n<td><strong>Counter values decreasing</strong></td>\n<td>Application restarts not handled properly</td>\n<td>Track counter resets in logs</td>\n<td>Implement reset detection and handle counter resets gracefully</td>\n</tr>\n</tbody></table>\n<h2 id=\"time-series-storage-engine\">Time-Series Storage Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 2 (Storage &amp; Querying) - This section details the storage layer that persists metrics data and enables efficient querying for both dashboards and alerting systems</p>\n</blockquote>\n<h3 id=\"mental-model-the-library-archive-system\">Mental Model: The Library Archive System</h3>\n<p>Think of our time-series storage engine as a vast, highly organized library archive system that specializes in chronological documents. Just as a library must efficiently store millions of books, organize them for quick retrieval, and manage space by moving older materials to different storage areas, our storage engine must handle millions of metric samples with similar organizational challenges.</p>\n<p>In this library analogy, each <strong>time-series</strong> is like a specific periodical or journal - say &quot;The Daily Weather Reports for New York City&quot; or &quot;Monthly Server CPU Usage for Database-01&quot;. Each <strong>sample</strong> within a time-series is like an individual issue of that periodical, with a specific date (timestamp) and content (the metric value). The <strong>labels</strong> on a metric are like the catalog classification system - they help us group related periodicals together and find exactly what we&#39;re looking for among millions of possibilities.</p>\n<p>The library&#39;s <strong>card catalog system</strong> represents our indexing structure. Just as you can look up books by author, title, subject, or publication date, we need indexes that let us quickly find time-series by metric name, label combinations, and time ranges. Without proper indexing, finding specific data would require scanning through every stored sample - like searching for a specific magazine issue by checking every shelf in the building.</p>\n<p>The library&#39;s <strong>archival policies</strong> mirror our compaction and retention strategies. Recent magazines are kept in the main reading room for immediate access (high-resolution recent data), older issues are moved to compact storage with lower accessibility (downsampled historical data), and very old materials are eventually discarded according to institutional policies (retention limits). This tiered storage approach balances accessibility, storage costs, and retrieval performance.</p>\n<p>Just as librarians periodically reorganize sections, merge duplicate materials, and transfer items between storage areas, our storage engine continuously runs background processes to compact data files, merge small storage blocks into larger ones, and apply retention policies. This maintenance work happens transparently while the library remains open for readers - our storage engine must serve queries and accept new data even while reorganizing itself.</p>\n<h3 id=\"storage-format-and-indexing\">Storage Format and Indexing</h3>\n<p>The foundation of our time-series storage system rests on a <strong>block-based architecture</strong> where metric samples are organized into immutable storage blocks on disk. This design choice provides several critical advantages: it enables efficient compression within each block, simplifies concurrent access patterns, and makes background compaction operations straightforward to implement safely.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fstorage-architecture.svg\" alt=\"Storage Engine Architecture\"></p>\n<p><strong>Storage Block Structure</strong></p>\n<p>Each storage block contains samples for multiple time-series within a specific time window, typically spanning 2-4 hours of data. Within a block, samples are stored in a columnar format that maximizes compression efficiency. The timestamp column uses delta encoding (storing differences between consecutive timestamps rather than absolute values), while the value column applies appropriate compression based on the data patterns detected during ingestion.</p>\n<table>\n<thead>\n<tr>\n<th>Block Component</th>\n<th>Data Structure</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Block Header</td>\n<td>BlockID, TimeRange, SeriesCount, CompressionType</td>\n<td>Metadata for quick block filtering and loading</td>\n</tr>\n<tr>\n<td>Series Index</td>\n<td>SeriesID → ValueOffset mapping</td>\n<td>Enables direct jump to specific time-series data within block</td>\n</tr>\n<tr>\n<td>Timestamp Column</td>\n<td>Delta-encoded timestamp array</td>\n<td>Compressed temporal data shared across all series in block</td>\n</tr>\n<tr>\n<td>Value Columns</td>\n<td>Series-specific compressed value arrays</td>\n<td>Actual metric values with optimal compression per series type</td>\n</tr>\n<tr>\n<td>Block Footer</td>\n<td>CRC checksum, Block size</td>\n<td>Data integrity verification and corruption detection</td>\n</tr>\n</tbody></table>\n<p>The <strong>series ID</strong> serves as the primary key that uniquely identifies each time-series. It&#39;s computed as a hash of the metric name combined with the lexicographically sorted label pairs. For example, the series ID for <code>http_requests_total{method=&quot;GET&quot;, status=&quot;200&quot;}</code> would be <code>hash(&quot;http_requests_total&quot; + &quot;method=GET,status=200&quot;)</code>. This deterministic approach ensures that samples belonging to the same logical time-series always map to the same series ID, regardless of the order in which labels appear in incoming metrics.</p>\n<blockquote>\n<p><strong>Decision: Block-Based vs. Row-Based Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to choose fundamental storage layout for time-series samples</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Row-based: Each sample stored as complete record with all fields</li>\n<li>Block-based: Samples grouped into columnar blocks by time window</li>\n<li>Hybrid: Combination approach with recent data row-based, historical data block-based</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Block-based columnar storage with 2-4 hour time windows</li>\n<li><strong>Rationale</strong>: Time-series data exhibits excellent compression characteristics when organized columnar, query patterns typically involve time ranges rather than individual samples, and block immutability simplifies concurrent access and compaction</li>\n<li><strong>Consequences</strong>: Enables 5-10x compression ratios, optimizes for range queries, but requires more complex indexing for point lookups</li>\n</ul>\n</blockquote>\n<p><strong>Indexing Architecture</strong></p>\n<p>The storage engine maintains multiple index structures to support different query patterns efficiently. The <strong>primary index</strong> maps series IDs to the list of blocks containing data for that time-series. This index is kept entirely in memory for fast lookups and is persisted to disk for crash recovery. Each index entry contains the series metadata (metric name and labels) plus a sorted list of block references with their time ranges.</p>\n<table>\n<thead>\n<tr>\n<th>Index Type</th>\n<th>Structure</th>\n<th>Query Pattern Supported</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Primary Index</td>\n<td>SeriesID → BlockList mapping</td>\n<td>Exact series lookup by metric name + labels</td>\n</tr>\n<tr>\n<td>Label Index</td>\n<td>LabelName → LabelValue → SeriesID list</td>\n<td>Label-based filtering and series discovery</td>\n</tr>\n<tr>\n<td>Time Index</td>\n<td>TimeRange → BlockID list</td>\n<td>Time-range queries and block pruning</td>\n</tr>\n<tr>\n<td>Inverted Index</td>\n<td>LabelValue → SeriesID list</td>\n<td>Cross-metric label searches</td>\n</tr>\n</tbody></table>\n<p>The <strong>label index</strong> enables efficient filtering by label values without scanning all series. For each unique label name that appears in the dataset, we maintain a secondary index mapping each possible value to the list of series IDs that contain that label-value pair. This structure allows queries like &quot;find all series with <code>status=&#39;500&#39;</code>&quot; to execute in logarithmic time rather than requiring a full series scan.</p>\n<p><strong>Compression Strategies</strong></p>\n<p>Different metric types exhibit distinct value patterns that benefit from specialized compression approaches. <strong>Counter metrics</strong> typically show monotonically increasing values, making them excellent candidates for delta-of-delta encoding where we store the difference between consecutive delta values. <strong>Gauge metrics</strong> often have values clustered around certain ranges, benefiting from dictionary compression that maps frequent values to short codes.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Type</th>\n<th>Compression Algorithm</th>\n<th>Typical Compression Ratio</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Counter</td>\n<td>Delta-of-delta encoding</td>\n<td>8:1 to 12:1</td>\n<td>Values increase monotonically with predictable rates</td>\n</tr>\n<tr>\n<td>Gauge</td>\n<td>Dictionary + RLE compression</td>\n<td>4:1 to 8:1</td>\n<td>Values cluster around common ranges with repetition</td>\n</tr>\n<tr>\n<td>Histogram</td>\n<td>Mixed: delta encoding for timestamps, dictionary for bucket counts</td>\n<td>6:1 to 10:1</td>\n<td>Bucket structure provides natural compression opportunities</td>\n</tr>\n</tbody></table>\n<p><strong>File Organization</strong></p>\n<p>Storage blocks are organized into a hierarchical directory structure that facilitates efficient querying and maintenance operations. The root storage directory contains subdirectories organized by day, with each day containing hour-based subdirectories that hold the actual block files. This organization allows time-based query optimizations and simplifies retention policy implementation.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>data/\n├── 2024-01-15/\n│   ├── 00/  (midnight hour)\n│   │   ├── block_001.db\n│   │   ├── block_002.db\n│   │   └── index.db\n│   ├── 01/  (1 AM hour)\n│   │   ├── block_003.db\n│   │   └── index.db\n│   └── ...\n├── 2024-01-16/\n│   └── ...\n└── meta/\n    ├── series_index.db\n    └── label_index.db</code></pre></div>\n\n<p>This hierarchical approach provides several operational benefits. Time-range queries can immediately eliminate entire directories from consideration, reducing I/O overhead. Retention policies can delete entire day directories atomically. Background maintenance tasks can process data in convenient time-based chunks without interfering with current ingestion.</p>\n<h3 id=\"compaction-and-retention-policies\">Compaction and Retention Policies</h3>\n<p>The storage engine&#39;s longevity and performance depend critically on automated data lifecycle management. Without compaction, the system would accumulate thousands of small storage blocks that degrade query performance and waste disk space. Without retention policies, historical data would consume unbounded storage and eventually exhaust available capacity.</p>\n<p><strong>Compaction Strategy</strong></p>\n<p>Our compaction system operates on multiple levels, similar to LSM-tree approaches but adapted for time-series workloads. <strong>Level 0 compaction</strong> runs continuously, merging small blocks created during active ingestion into larger, more efficiently compressed blocks. <strong>Level 1 compaction</strong> runs hourly, reorganizing data across longer time windows to optimize for common query patterns. <strong>Level 2 compaction</strong> runs daily, performing deep reorganization and applying downsampling policies to historical data.</p>\n<table>\n<thead>\n<tr>\n<th>Compaction Level</th>\n<th>Trigger</th>\n<th>Input Blocks</th>\n<th>Output Blocks</th>\n<th>Primary Goal</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Level 0</td>\n<td>Every 15 minutes</td>\n<td>5-10 small ingestion blocks</td>\n<td>1 medium block</td>\n<td>Reduce file count, improve compression</td>\n</tr>\n<tr>\n<td>Level 1</td>\n<td>Hourly</td>\n<td>All blocks in hour window</td>\n<td>1-2 optimized blocks</td>\n<td>Optimize query performance, remove duplicates</td>\n</tr>\n<tr>\n<td>Level 2</td>\n<td>Daily</td>\n<td>Day&#39;s worth of historical blocks</td>\n<td>Downsampled summary blocks</td>\n<td>Apply retention policies, reduce storage footprint</td>\n</tr>\n</tbody></table>\n<p>The compaction process must handle several complex scenarios. <strong>Overlapping time ranges</strong> occur when blocks contain data from slightly different time windows - the compaction algorithm merges samples in timestamp order while detecting and removing duplicates. <strong>Schema evolution</strong> happens when new labels appear in the dataset - the compaction process updates index structures to accommodate new label combinations without breaking existing queries.</p>\n<p><strong>Downsampling Algorithms</strong></p>\n<p>As data ages, maintaining full-resolution samples becomes increasingly expensive while providing diminishing value for most use cases. Our downsampling system applies configurable policies that reduce data resolution while preserving the statistical properties most important for analysis and alerting.</p>\n<blockquote>\n<p><strong>Decision: Fixed vs. Adaptive Downsampling Windows</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance storage efficiency with data fidelity for historical metrics</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Fixed windows: Predetermined downsampling intervals (1h → 5m, 1d → 1h, 1w → 6h)</li>\n<li>Adaptive windows: Dynamic intervals based on data volatility and query patterns  </li>\n<li>Query-driven: Downsample only when storage pressure or query performance degrades</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Fixed windows with configurable thresholds</li>\n<li><strong>Rationale</strong>: Predictable storage growth, simplified implementation, deterministic behavior for alerting rules that depend on historical data</li>\n<li><strong>Consequences</strong>: May oversample stable metrics or undersample volatile ones, but provides consistent query performance and storage characteristics</li>\n</ul>\n</blockquote>\n<p>The downsampling process preserves different statistical properties depending on the metric type. For <strong>counter metrics</strong>, we maintain the rate-of-change information by storing periodic snapshots along with minimum and maximum observed rates within each downsampling window. For <strong>gauge metrics</strong>, we preserve the average, minimum, maximum, and last observed value within each window. For <strong>histogram metrics</strong>, we merge bucket counts and recalculate percentiles at the reduced resolution.</p>\n<table>\n<thead>\n<tr>\n<th>Downsampling Window</th>\n<th>Original Resolution</th>\n<th>Target Resolution</th>\n<th>Statistical Preservation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1 hour - 24 hours</td>\n<td>15 second intervals</td>\n<td>1 minute intervals</td>\n<td>Full statistical detail maintained</td>\n</tr>\n<tr>\n<td>1 day - 7 days</td>\n<td>1 minute intervals</td>\n<td>5 minute intervals</td>\n<td>Min/max/avg/last preserved per window</td>\n</tr>\n<tr>\n<td>1 week - 30 days</td>\n<td>5 minute intervals</td>\n<td>30 minute intervals</td>\n<td>Statistical summary with confidence intervals</td>\n</tr>\n<tr>\n<td>30+ days</td>\n<td>30 minute intervals</td>\n<td>2 hour intervals</td>\n<td>Trend data only, detailed statistics discarded</td>\n</tr>\n</tbody></table>\n<p><strong>Retention Policy Implementation</strong></p>\n<p>Retention policies operate at multiple granularities to provide fine-grained control over data lifecycle management. <strong>Global policies</strong> apply default retention periods to all metrics, <strong>metric-specific policies</strong> override defaults for particular metric names, and <strong>label-based policies</strong> allow retention decisions based on label values such as environment or criticality levels.</p>\n<p>The retention engine runs as a background process that evaluates policies every hour and marks eligible data for deletion. Rather than immediately removing data, the system uses a <strong>two-phase deletion</strong> approach: first marking blocks as expired, then physically removing them after a grace period. This approach prevents accidental data loss and allows recovery from misconfigured retention policies.</p>\n<table>\n<thead>\n<tr>\n<th>Policy Type</th>\n<th>Evaluation Frequency</th>\n<th>Scope</th>\n<th>Example Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Global</td>\n<td>Every hour</td>\n<td>All metrics</td>\n<td>&quot;Delete all data older than 90 days&quot;</td>\n</tr>\n<tr>\n<td>Metric-specific</td>\n<td>Every hour</td>\n<td>Named metrics</td>\n<td>&quot;Keep http_request logs for 180 days&quot;</td>\n</tr>\n<tr>\n<td>Label-based</td>\n<td>Every hour</td>\n<td>Metrics matching label patterns</td>\n<td>&quot;Keep production env data for 1 year, dev data for 30 days&quot;</td>\n</tr>\n<tr>\n<td>Storage-based</td>\n<td>Every 30 minutes</td>\n<td>Entire system</td>\n<td>&quot;When disk usage &gt; 80%, delete oldest non-critical data&quot;</td>\n</tr>\n</tbody></table>\n<p>The policy evaluation engine maintains a priority queue of deletion candidates, allowing the system to free storage space intelligently during capacity pressure situations. Critical metrics (those referenced by active alert rules or displayed on important dashboards) receive higher retention priority than metrics that haven&#39;t been queried recently.</p>\n<p><strong>Consistency and Durability Guarantees</strong></p>\n<p>The storage engine provides <strong>eventual consistency</strong> for compaction operations while maintaining <strong>strong consistency</strong> for retention policy application. During compaction, readers may temporarily observe both the original blocks and the newly compacted blocks, but query results remain correct because the block selection logic handles overlapping time ranges properly.</p>\n<p>Write operations use a <strong>write-ahead log (WAL)</strong> to ensure durability. Incoming samples are first written to the WAL with an fsync operation, then asynchronously flushed to storage blocks. This approach provides immediate durability guarantees while allowing batched writes to optimize storage block creation. The WAL is automatically truncated as samples are successfully persisted to storage blocks.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Compaction During Development</strong>\nMany developers focus on implementing the basic read/write functionality and defer compaction implementation until later. This approach leads to severe performance degradation as the system accumulates thousands of small storage blocks. Query performance degrades exponentially with block count, and storage space usage can be 5-10x higher than necessary. Implement basic compaction early, even if it&#39;s a simple periodic merger of small blocks.</p>\n<p>⚠️ <strong>Pitfall: Retention Policies That Don&#39;t Account for Alert Dependencies</strong>\nImplementing retention policies that delete historical data without considering active alert rules can break alerting functionality. Alert rules often need historical data for baseline calculations or trend analysis. The retention system must maintain metadata about which metrics are referenced by active alerts and apply appropriate retention extensions automatically.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The time-series storage engine implementation requires careful attention to concurrent access patterns, efficient file I/O, and robust error handling. The following guidance provides the foundation for building a production-ready storage system.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>File Format</td>\n<td>JSON lines with gzip compression</td>\n<td>Custom binary format with advanced compression</td>\n</tr>\n<tr>\n<td>Indexing</td>\n<td>In-memory maps with periodic persistence</td>\n<td>B+ tree or LSM-tree based indexes</td>\n</tr>\n<tr>\n<td>Compression</td>\n<td>Go&#39;s gzip/lz4 for general compression</td>\n<td>Specialized time-series compression (Gorilla, etc.)</td>\n</tr>\n<tr>\n<td>Concurrency</td>\n<td>sync.RWMutex for index protection</td>\n<td>Lock-free data structures with atomic operations</td>\n</tr>\n<tr>\n<td>Background Tasks</td>\n<td>time.Ticker with simple scheduling</td>\n<td>Priority queue with adaptive scheduling</td>\n</tr>\n</tbody></table>\n<p><strong>File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/storage/\n├── engine.go              ← Main storage engine implementation\n├── block.go               ← Storage block read/write operations  \n├── index.go               ← In-memory and persistent indexing\n├── compaction.go          ← Background compaction processes\n├── retention.go           ← Data lifecycle management\n├── wal.go                 ← Write-ahead log for durability\n├── compression.go         ← Compression algorithm implementations\n├── query.go               ← Query execution against storage blocks\n└── storage_test.go        ← Comprehensive integration tests</code></pre></div>\n\n<p><strong>Core Data Structures</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageEngine manages time-series data with compaction and retention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dataDir       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    seriesIndex   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesIndex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    blockManager  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BlockManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    compactor     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Compactor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retentionMgr  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RetentionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal           </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WriteAheadLog</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    closed        </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesIndex maintains the mapping from series IDs to storage blocks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SeriesIndex</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    series    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesInfo</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    labels    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">  // label_name -> label_value -> series_ids</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dirty     </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesInfo contains metadata and block references for a time-series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SeriesInfo</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SeriesID    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MetricName  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Labels      </span><span style=\"color:#B392F0\">Labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Blocks      []</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BlockReference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastSample  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BlockReference points to a storage block containing series data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BlockReference</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BlockID    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FilePath   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange  </span><span style=\"color:#B392F0\">TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SeriesCount </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Compressed </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageBlock represents an immutable collection of time-series samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageBlock</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID          </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange   </span><span style=\"color:#B392F0\">TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Series      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">Sample</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Metadata    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    compressed  </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Core Storage Engine Interface</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// TimeSeriesStorage defines the primary storage interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeSeriesStorage</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // WriteSamples persists metric samples to storage with durability guarantees</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    WriteSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // QueryRange retrieves samples for specified series within time range</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    QueryRange</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // QuerySeries finds all series matching label selectors</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    QuerySeries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">matchers</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">LabelMatcher</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">SeriesInfo</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // GetSeriesInfo retrieves metadata for a specific series</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    GetSeriesInfo</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SeriesInfo</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Close gracefully shuts down storage engine and background processes</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Close</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LabelMatcher defines criteria for series selection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> LabelMatcher</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operator </span><span style=\"color:#B392F0\">MatchOperator</span><span style=\"color:#6A737D\">  // Equal, NotEqual, RegexMatch, etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Write-Ahead Log Implementation</strong></p>\n<p>The WAL ensures durability by persisting samples before acknowledging writes. This complete implementation handles the critical path for data integrity:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> storage</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bufio</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">os</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteAheadLog provides durability guarantees for metric samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WriteAheadLog</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filePath    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">os</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">File</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    writer      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">bufio</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Writer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFlush   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    flushInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WALRecord represents a single entry in the write-ahead log</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WALRecord</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"type\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data      []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#9ECBFF\">  `json:\"data\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Checksum  </span><span style=\"color:#F97583\">uint32</span><span style=\"color:#9ECBFF\">    `json:\"checksum\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewWriteAheadLog creates and initializes a new WAL instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewWriteAheadLog</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">dataDir</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">flushInterval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WriteAheadLog</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    walPath </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> filepath.</span><span style=\"color:#B392F0\">Join</span><span style=\"color:#E1E4E8\">(dataDir, </span><span style=\"color:#9ECBFF\">\"wal.log\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> os.</span><span style=\"color:#B392F0\">OpenFile</span><span style=\"color:#E1E4E8\">(walPath, os.O_CREATE</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_APPEND</span><span style=\"color:#F97583\">|</span><span style=\"color:#E1E4E8\">os.O_WRONLY, </span><span style=\"color:#79B8FF\">0644</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to open WAL file: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wal </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">WriteAheadLog</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filePath:      walPath,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        file:          file,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        writer:        bufio.</span><span style=\"color:#B392F0\">NewWriter</span><span style=\"color:#E1E4E8\">(file),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        flushInterval: flushInterval,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lastFlush:     time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start background flush routine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> wal.</span><span style=\"color:#B392F0\">backgroundFlush</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> wal, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AppendSamples writes samples to WAL with immediate durability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WriteAheadLog</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">AppendSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    record </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> WALRecord</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Type:      </span><span style=\"color:#9ECBFF\">\"samples\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Data:      samples,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Checksum:  </span><span style=\"color:#B392F0\">calculateChecksum</span><span style=\"color:#E1E4E8\">(samples),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Serialize record to JSON</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> json.</span><span style=\"color:#B392F0\">Marshal</span><span style=\"color:#E1E4E8\">(record)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to marshal WAL record: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Write to buffer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.writer.</span><span style=\"color:#B392F0\">Write</span><span style=\"color:#E1E4E8\">(data); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to write to WAL: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.writer.</span><span style=\"color:#B392F0\">WriteString</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to write newline: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Force immediate flush for durability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.writer.</span><span style=\"color:#B392F0\">Flush</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to flush WAL buffer: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> w.file.</span><span style=\"color:#B392F0\">Sync</span><span style=\"color:#E1E4E8\">(); err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"failed to sync WAL to disk: </span><span style=\"color:#79B8FF\">%w</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// backgroundFlush periodically ensures data is written to disk</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">w </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WriteAheadLog</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">backgroundFlush</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(w.flushInterval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ticker.C {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.writer.</span><span style=\"color:#B392F0\">Flush</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.file.</span><span style=\"color:#B392F0\">Sync</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.lastFlush </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Storage Engine Core Implementation Template</strong></p>\n<p>The following template provides the structure for the main storage engine. Key methods are stubbed with detailed TODOs that correspond to the design algorithms described above:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// NewStorageEngine creates a new time-series storage engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStorageEngine</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StorageConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    engine </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:       config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        dataDir:      config.DataDir,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seriesIndex:  </span><span style=\"color:#B392F0\">NewSeriesIndex</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        blockManager: </span><span style=\"color:#B392F0\">NewBlockManager</span><span style=\"color:#E1E4E8\">(config.DataDir),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create data directory if it doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize write-ahead log with configured flush interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Load existing series index from disk (if present)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start background compaction goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Start background retention policy goroutine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Register shutdown cleanup handlers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> engine, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteSamples persists metric samples with durability guarantees</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    e.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> e.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> e.closed {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"storage engine is closed\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Write samples to WAL for immediate durability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Group samples by series ID for efficient batching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update series index with new samples and time ranges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add samples to current active storage block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If block reaches size threshold, trigger compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update cardinality tracking for label explosion detection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryRange retrieves samples for specified series within time range</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">e </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">QueryRange</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    e.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> e.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Look up series info in index to get block references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Filter blocks by time range to minimize I/O</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Load samples from each relevant block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Merge samples in chronological order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply any necessary decompression</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Filter samples to exact time range requested</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Background Compaction Process</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Compactor handles background compaction of storage blocks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Compactor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    engine      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    running     </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh      </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Mutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// runCompaction executes the main compaction logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Compactor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">runCompaction</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Scan data directory for blocks eligible for compaction</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Group blocks by time range and size for optimal merging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each group, read all samples into memory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Sort samples by timestamp and remove duplicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply compression and write to new compacted block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update series index to reference new block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Mark old blocks for deletion after grace period</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Update compaction statistics and metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Language-Specific Optimization Hints</strong></p>\n<ul>\n<li><strong>File I/O</strong>: Use <code>os.File.ReadAt()</code> and <code>os.File.WriteAt()</code> for concurrent access to different file regions without seeking</li>\n<li><strong>Memory Management</strong>: Pre-allocate slices with known capacity using <code>make([]Sample, 0, expectedCount)</code> to avoid repeated allocations</li>\n<li><strong>Concurrency</strong>: Use <code>sync.RWMutex</code> for the series index since reads vastly outnumber writes in typical workloads</li>\n<li><strong>Error Handling</strong>: Wrap file system errors with context using <code>fmt.Errorf(&quot;operation failed: %w&quot;, err)</code> for better debugging</li>\n<li><strong>JSON Performance</strong>: Consider using <code>encoding/json.Encoder</code> with a pooled buffer for better performance than <code>json.Marshal</code></li>\n</ul>\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the storage engine, verify functionality with these concrete tests:</p>\n<ol>\n<li><p><strong>Basic Write/Read Test</strong>: Write 1000 samples across 10 different series, then query each series individually. Verify all samples are returned in correct chronological order.</p>\n</li>\n<li><p><strong>Compaction Test</strong>: Write enough data to trigger compaction (based on your block size threshold), wait for compaction to complete, then verify queries still return correct results and storage directory contains fewer files.</p>\n</li>\n<li><p><strong>WAL Recovery Test</strong>: Write samples, kill the process before graceful shutdown, restart and verify all samples are recovered from the WAL.</p>\n</li>\n<li><p><strong>Concurrent Access Test</strong>: Run multiple goroutines writing different series while others query existing data. Verify no race conditions using <code>go test -race</code>.</p>\n</li>\n</ol>\n<p>Expected behavior: Write operations should complete in under 10ms for batches of 100 samples, queries should return results in under 50ms for 24-hour ranges, and compaction should reduce file count by 5-10x while maintaining query correctness.</p>\n<p><strong>Common Implementation Pitfalls</strong></p>\n<p>⚠️ <strong>Pitfall: Not Handling Partial Writes</strong>\nFile system operations can be interrupted, leaving storage blocks in an inconsistent state. Always write to temporary files first, then atomically rename them to the final location using <code>os.Rename()</code>. This ensures readers never see partially written data.</p>\n<p>⚠️ <strong>Pitfall: Index Corruption During Crashes</strong>\nThe series index is critical for query performance but vulnerable to corruption if the process crashes during updates. Implement index versioning with atomic updates: write new index to <code>index.new</code>, fsync, then rename to <code>index.db</code>.</p>\n<p>⚠️ <strong>Pitfall: Memory Leaks in Long-Running Compaction</strong>\nCompaction processes can accumulate large amounts of data in memory when processing multiple blocks. Implement streaming compaction that processes blocks in chunks rather than loading entire datasets into memory simultaneously.</p>\n<h2 id=\"query-engine\">Query Engine</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 2 (Storage &amp; Querying) - This section details the query processing system that executes metric queries for both dashboard visualization and alert evaluation</p>\n</blockquote>\n<h3 id=\"mental-model-the-research-assistant\">Mental Model: The Research Assistant</h3>\n<p>Think of the Query Engine as an experienced research assistant working in a vast scientific library. When you need specific information, you don&#39;t hand the assistant a raw storage location - instead, you make a request in natural language: &quot;Find me all the temperature readings from Building A sensors between 2 PM and 4 PM yesterday, and calculate the average for each floor.&quot;</p>\n<p>The research assistant then performs a sophisticated multi-step process. First, they parse your request to understand exactly what you&#39;re looking for - which metrics, what time period, which filtering criteria, and what calculations to perform. Next, they create a research plan - determining which sections of the library to visit, in what order, and how to efficiently gather the information. Then they execute this plan, navigating the library&#39;s indexing system to locate the relevant data quickly. Finally, they process and analyze the raw information, performing the requested calculations and presenting the results in the format you need.</p>\n<p>This analogy captures the essence of query processing: translating high-level information requests into efficient data retrieval and computation operations. Just as a good research assistant knows shortcuts through the library and can optimize their research strategy based on the type of question, our Query Engine must understand how to leverage the time-series storage structure to answer metric queries efficiently.</p>\n<p>The key insight is that query processing is not just about finding data - it&#39;s about finding data <em>efficiently</em> and performing meaningful computations on it. A research assistant who checks every single book in the library for your temperature readings would eventually find the answer, but would take far too long. Similarly, a query engine that scans every stored sample to answer a simple aggregation query would be functionally useless at scale.</p>\n<h3 id=\"query-language-design\">Query Language Design</h3>\n<p>Our query language provides a simple but powerful syntax for expressing metric selection, time range filtering, and aggregation operations. The design balances expressiveness with simplicity, targeting the common use cases that appear in both dashboard visualizations and alert rule definitions.</p>\n<blockquote>\n<p><strong>Decision: Simplified Query Syntax Over Full Programming Language</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a way for users to specify what metric data they want and how to process it, with options ranging from simple metric selection to complex mathematical expressions</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Simple metric selection with basic aggregation functions</li>\n<li>SQL-like query language with time-series extensions</li>\n<li>Full expression language with custom functions and variables</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Simple metric selection syntax with built-in aggregation functions and basic arithmetic</li>\n<li><strong>Rationale</strong>: Dashboard users and alert definitions need to be readable by operations teams, not just developers. Complex query languages create a barrier to adoption and increase the chance of errors in critical alerting rules</li>\n<li><strong>Consequences</strong>: Enables quick learning curve and reduces syntax errors, but limits advanced analytical capabilities and may require multiple queries for complex scenarios</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Query Language Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Selection + Aggregation</td>\n<td>Easy to learn, hard to misuse, fast parsing</td>\n<td>Limited analytical power, may need multiple queries</td>\n<td>✓ Yes</td>\n</tr>\n<tr>\n<td>SQL-like Syntax</td>\n<td>Familiar to many users, powerful joins and subqueries</td>\n<td>Complex for time-series concepts, verbose syntax</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Full Expression Language</td>\n<td>Maximum flexibility, supports complex math</td>\n<td>High learning curve, error-prone for alerts</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<p>The query language consists of four main components that work together to specify data selection and processing:</p>\n<p><strong>Metric Selection</strong> forms the foundation of every query, specifying which time-series to include in the result set. The basic syntax uses the metric name followed by optional label matchers enclosed in curly braces. Label matchers support exact matching, regular expressions, and negative matching to provide flexible filtering capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Selector Type</th>\n<th>Syntax</th>\n<th>Example</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Basic Metric</td>\n<td><code>metric_name</code></td>\n<td><code>cpu_usage</code></td>\n<td>Selects all series for the named metric</td>\n</tr>\n<tr>\n<td>Label Exact Match</td>\n<td><code>metric_name{label=&quot;value&quot;}</code></td>\n<td><code>cpu_usage{host=&quot;web01&quot;}</code></td>\n<td>Matches series with exact label value</td>\n</tr>\n<tr>\n<td>Label Regex Match</td>\n<td><code>metric_name{label=~&quot;pattern&quot;}</code></td>\n<td><code>cpu_usage{host=~&quot;web.*&quot;}</code></td>\n<td>Matches series with label matching regex</td>\n</tr>\n<tr>\n<td>Label Not Equal</td>\n<td><code>metric_name{label!=&quot;value&quot;}</code></td>\n<td><code>cpu_usage{host!=&quot;localhost&quot;}</code></td>\n<td>Excludes series with specific label value</td>\n</tr>\n<tr>\n<td>Label Not Regex</td>\n<td><code>metric_name{label!~&quot;pattern&quot;}</code></td>\n<td><code>cpu_usage{host!~&quot;test.*&quot;}</code></td>\n<td>Excludes series matching regex pattern</td>\n</tr>\n<tr>\n<td>Multiple Labels</td>\n<td><code>metric_name{label1=&quot;value1&quot;,label2=&quot;value2&quot;}</code></td>\n<td><code>cpu_usage{host=&quot;web01&quot;,cpu=&quot;0&quot;}</code></td>\n<td>All label conditions must match</td>\n</tr>\n</tbody></table>\n<p><strong>Time Range Specification</strong> determines the temporal scope of the query, supporting both absolute timestamps and relative time expressions. The time range can be specified as part of the query string or provided separately by the query execution context (such as dashboard time controls).</p>\n<table>\n<thead>\n<tr>\n<th>Time Range Type</th>\n<th>Syntax</th>\n<th>Example</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Relative Range</td>\n<td><code>[5m]</code></td>\n<td><code>cpu_usage[5m]</code></td>\n<td>Last 5 minutes of data</td>\n</tr>\n<tr>\n<td>Absolute Range</td>\n<td><code>[2024-01-15T10:00:00Z:2024-01-15T11:00:00Z]</code></td>\n<td><code>cpu_usage[2024-01-15T10:00:00Z:2024-01-15T11:00:00Z]</code></td>\n<td>Specific time window</td>\n</tr>\n<tr>\n<td>Duration Units</td>\n<td><code>s</code>, <code>m</code>, <code>h</code>, <code>d</code></td>\n<td><code>[30s]</code>, <code>[2h]</code>, <code>[7d]</code></td>\n<td>Seconds, minutes, hours, days</td>\n</tr>\n</tbody></table>\n<p><strong>Aggregation Functions</strong> process the selected time-series data to produce meaningful results for visualization and alerting. These functions operate either across time (reducing a time-series to a single value) or across series (combining multiple time-series into fewer series).</p>\n<table>\n<thead>\n<tr>\n<th>Function Category</th>\n<th>Function Name</th>\n<th>Syntax</th>\n<th>Description</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statistical</td>\n<td><code>avg</code></td>\n<td><code>avg(cpu_usage)</code></td>\n<td>Average across all selected series</td>\n<td>Overall system load</td>\n</tr>\n<tr>\n<td>Statistical</td>\n<td><code>sum</code></td>\n<td><code>sum(http_requests_total)</code></td>\n<td>Sum across all selected series</td>\n<td>Total request rate</td>\n</tr>\n<tr>\n<td>Statistical</td>\n<td><code>min</code></td>\n<td><code>min(disk_free_bytes)</code></td>\n<td>Minimum value across series</td>\n<td>Worst-case capacity</td>\n</tr>\n<tr>\n<td>Statistical</td>\n<td><code>max</code></td>\n<td><code>max(response_time)</code></td>\n<td>Maximum value across series</td>\n<td>Worst-case latency</td>\n</tr>\n<tr>\n<td>Statistical</td>\n<td><code>count</code></td>\n<td><code>count(up)</code></td>\n<td>Number of series with data</td>\n<td>Service availability</td>\n</tr>\n<tr>\n<td>Temporal</td>\n<td><code>rate</code></td>\n<td><code>rate(http_requests_total[5m])</code></td>\n<td>Per-second rate of increase</td>\n<td>Request rate calculation</td>\n</tr>\n<tr>\n<td>Temporal</td>\n<td><code>increase</code></td>\n<td><code>increase(counter_metric[1h])</code></td>\n<td>Total increase over time range</td>\n<td>Growth measurement</td>\n</tr>\n<tr>\n<td>Percentile</td>\n<td><code>quantile</code></td>\n<td><code>quantile(0.95, response_time)</code></td>\n<td>Percentile across series</td>\n<td>SLA measurements</td>\n</tr>\n</tbody></table>\n<p><strong>Arithmetic Operations</strong> allow combining metrics and constants using basic mathematical operators, enabling the calculation of derived metrics and ratios that provide business context to raw measurements.</p>\n<table>\n<thead>\n<tr>\n<th>Operator</th>\n<th>Syntax</th>\n<th>Example</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Addition</td>\n<td><code>+</code></td>\n<td><code>cpu_usage + memory_usage</code></td>\n<td>Add metrics together</td>\n</tr>\n<tr>\n<td>Subtraction</td>\n<td><code>-</code></td>\n<td><code>disk_total - disk_used</code></td>\n<td>Calculate differences</td>\n</tr>\n<tr>\n<td>Multiplication</td>\n<td><code>*</code></td>\n<td><code>cpu_usage * 100</code></td>\n<td>Scale by constant</td>\n</tr>\n<tr>\n<td>Division</td>\n<td><code>/</code></td>\n<td><code>http_errors / http_total</code></td>\n<td>Calculate ratios</td>\n</tr>\n<tr>\n<td>Parentheses</td>\n<td><code>()</code></td>\n<td><code>(cpu_usage + memory_usage) / 2</code></td>\n<td>Control operation order</td>\n</tr>\n</tbody></table>\n<p>The query language supports function composition and chaining to build complex expressions from simple building blocks. For example, calculating an error rate percentage requires combining metric selection, rate calculation, and arithmetic: <code>rate(http_errors_total[5m]) / rate(http_requests_total[5m]) * 100</code>.</p>\n<blockquote>\n<p>The critical design insight is that query complexity should grow gradually - simple queries should have simple syntax, while complex analytics remain possible through composition rather than requiring entirely different syntax patterns.</p>\n</blockquote>\n<h3 id=\"query-execution-pipeline\">Query Execution Pipeline</h3>\n<p>The query execution pipeline transforms user query strings into efficient operations against the time-series storage engine through a carefully orchestrated sequence of parsing, planning, and execution phases. This pipeline design separates concerns cleanly, enabling optimization opportunities while maintaining correctness guarantees.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fquery-execution.svg\" alt=\"Query Processing Flow\"></p>\n<p><strong>Phase 1: Query Parsing and Validation</strong> converts the query string into a structured internal representation that can be processed programmatically. The parser employs a recursive descent approach that handles the nested nature of function calls and arithmetic expressions while providing clear error messages for syntax errors.</p>\n<p>The parsing process follows these steps:</p>\n<ol>\n<li><p><strong>Lexical Analysis</strong>: The query string is tokenized into meaningful symbols - metric names, label selectors, function names, operators, and literals. This phase handles quoted strings, regular expressions, and numeric constants while detecting malformed tokens.</p>\n</li>\n<li><p><strong>Syntax Analysis</strong>: Tokens are organized into an Abstract Syntax Tree (AST) that represents the query structure. The parser validates that function calls have the correct number of arguments, operators are used in valid contexts, and parentheses are balanced.</p>\n</li>\n<li><p><strong>Semantic Validation</strong>: The AST is analyzed for logical correctness - metric names are checked against known metrics, label names are validated, and function signatures are verified. This phase catches errors like attempting to calculate rates on gauge metrics or using undefined aggregation functions.</p>\n</li>\n<li><p><strong>Type Analysis</strong>: Each node in the AST is annotated with its expected result type (scalar, vector, or matrix) to ensure operations are compatible. For example, arithmetic operations between vectors require matching label sets.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Parsing Component</th>\n<th>Input</th>\n<th>Output</th>\n<th>Error Types Detected</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Lexer</td>\n<td>Raw query string</td>\n<td>Token stream</td>\n<td>Invalid characters, unterminated strings</td>\n</tr>\n<tr>\n<td>Parser</td>\n<td>Token stream</td>\n<td>Abstract Syntax Tree</td>\n<td>Syntax errors, malformed expressions</td>\n</tr>\n<tr>\n<td>Semantic Analyzer</td>\n<td>AST + Metric Metadata</td>\n<td>Validated AST</td>\n<td>Unknown metrics, invalid functions</td>\n</tr>\n<tr>\n<td>Type Checker</td>\n<td>Validated AST</td>\n<td>Typed AST</td>\n<td>Type mismatches, incompatible operations</td>\n</tr>\n</tbody></table>\n<p><strong>Phase 2: Query Planning and Optimization</strong> analyzes the validated query to determine the most efficient execution strategy. The planner considers factors like storage layout, available indexes, and data locality to minimize the amount of data that must be read and processed.</p>\n<p>The planning process generates an execution plan that specifies:</p>\n<ol>\n<li><p><strong>Series Selection Strategy</strong>: Determines which series indexes to consult and what filtering can be pushed down to the storage layer. For queries with highly selective label matchers, the planner can avoid reading irrelevant series entirely.</p>\n</li>\n<li><p><strong>Time Range Optimization</strong>: Analyzes the query&#39;s time requirements to determine which storage blocks need to be accessed. Queries covering short recent time periods can skip older compacted blocks entirely.</p>\n</li>\n<li><p><strong>Aggregation Scheduling</strong>: Decides whether aggregations can be performed incrementally as data is read (reducing memory usage) or need to buffer all data first (required for percentile calculations).</p>\n</li>\n<li><p><strong>Parallelization Opportunities</strong>: Identifies independent operations that can be executed concurrently, such as processing different series or time ranges in parallel.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Planning Decision</th>\n<th>Factors Considered</th>\n<th>Optimization Applied</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Index Usage</td>\n<td>Label selectivity, cardinality</td>\n<td>Choose most selective indexes first</td>\n</tr>\n<tr>\n<td>Block Selection</td>\n<td>Query time range, block boundaries</td>\n<td>Skip blocks outside time range</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Result set size, aggregation type</td>\n<td>Stream processing vs. buffering</td>\n</tr>\n<tr>\n<td>Parallelization</td>\n<td>CPU cores, I/O patterns</td>\n<td>Concurrent series processing</td>\n</tr>\n</tbody></table>\n<p><strong>Phase 3: Storage Access and Data Retrieval</strong> executes the optimized query plan against the time-series storage engine, retrieving the minimum necessary data while maintaining consistency guarantees.</p>\n<p>The execution engine coordinates several storage operations:</p>\n<ol>\n<li><p><strong>Series Discovery</strong>: Uses the series index to identify all time-series that match the query&#39;s label selectors. This phase leverages inverted indexes to quickly find candidate series without scanning all stored data.</p>\n</li>\n<li><p><strong>Block Access</strong>: Reads the storage blocks that contain samples within the query&#39;s time range. The engine uses block metadata to skip blocks that don&#39;t contain relevant data and can read multiple blocks concurrently.</p>\n</li>\n<li><p><strong>Sample Filtering</strong>: Applies additional filtering criteria that couldn&#39;t be pushed down to the storage layer, such as complex regular expression matches or range conditions.</p>\n</li>\n<li><p><strong>Data Streaming</strong>: Organizes the retrieved samples into time-ordered streams that can be processed efficiently by the aggregation functions.</p>\n</li>\n</ol>\n<p><strong>Phase 4: Aggregation and Result Computation</strong> applies the query&#39;s mathematical operations to the retrieved data, producing the final result set that will be returned to the caller.</p>\n<p>Different aggregation types require different processing strategies:</p>\n<ol>\n<li><p><strong>Series Aggregation</strong>: Functions like <code>sum()</code> and <code>avg()</code> combine multiple time-series into fewer series. These operations must align timestamps across series and handle cases where series have different sampling intervals.</p>\n</li>\n<li><p><strong>Temporal Aggregation</strong>: Functions like <code>rate()</code> and <code>increase()</code> analyze how individual time-series change over time. These operations must handle counter resets and missing data points appropriately.</p>\n</li>\n<li><p><strong>Statistical Functions</strong>: Operations like <code>quantile()</code> require collecting all relevant data points before computation, using streaming algorithms when possible to manage memory usage.</p>\n</li>\n<li><p><strong>Arithmetic Operations</strong>: Binary operations between metrics must align series by labels and handle cases where operand series don&#39;t have matching labels.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Aggregation Type</th>\n<th>Processing Strategy</th>\n<th>Memory Requirements</th>\n<th>Special Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Series Sum/Average</td>\n<td>Streaming with timestamp alignment</td>\n<td>O(time_points)</td>\n<td>Handle missing series data</td>\n</tr>\n<tr>\n<td>Rate Calculation</td>\n<td>Windowed streaming</td>\n<td>O(window_size)</td>\n<td>Detect counter resets</td>\n</tr>\n<tr>\n<td>Percentiles</td>\n<td>Collect-then-compute</td>\n<td>O(all_samples)</td>\n<td>May require approximate algorithms</td>\n</tr>\n<tr>\n<td>Binary Arithmetic</td>\n<td>Label-based matching</td>\n<td>O(series_count)</td>\n<td>Handle label mismatches</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Streaming vs. Buffering Execution</strong></p>\n<ul>\n<li><strong>Context</strong>: Query results can range from a few data points to millions of samples, and available memory may be limited</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Always buffer all data in memory before processing</li>\n<li>Always use streaming processing with fixed memory limits</li>\n<li>Choose strategy based on query characteristics and system resources</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach that uses streaming for compatible operations and buffering only when required</li>\n<li><strong>Rationale</strong>: Streaming enables processing datasets larger than available memory and provides consistent latency, while some operations (like percentiles) fundamentally require seeing all data</li>\n<li><strong>Consequences</strong>: Enables scaling to large datasets with predictable resource usage, but requires more complex execution engine implementation</li>\n</ul>\n</blockquote>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Timestamp Alignment Errors</strong>\nWhen aggregating multiple time-series, failing to properly align timestamps leads to incorrect results. This happens when developers assume all time-series have samples at identical timestamps, but in reality, metrics are collected at slightly different times. For example, averaging CPU usage across multiple hosts might miss samples or double-count values if timestamps aren&#39;t aligned to common intervals. The fix is to implement timestamp bucketing that groups nearby samples into common time slots before aggregation.</p>\n<p>⚠️ <strong>Pitfall: Counter Reset Handling</strong>\nRate calculations become incorrect when counter metrics reset to zero (usually due to process restarts). A naive rate calculation sees a large negative change and produces meaningless results. The correct approach detects when the current value is less than the previous value and treats this as a reset, calculating the rate from the reset point rather than the previous sample.</p>\n<p>⚠️ <strong>Pitfall: Memory Explosion with High Cardinality</strong>\nQueries that select high-cardinality metrics without sufficient filtering can consume enormous amounts of memory. For example, querying all HTTP request metrics without filtering by endpoint or status code might return millions of time-series. The query engine must implement memory limits and provide clear error messages when queries would exceed available resources, guiding users to add more selective label filters.</p>\n<p>⚠️ <strong>Pitfall: Inefficient Range Query Patterns</strong>\nRepeatedly executing queries with small time ranges (like dashboard panels refreshing every 10 seconds) creates unnecessary load on the storage layer. Each query incurs parsing and planning overhead, and may read overlapping data. The solution is implementing query result caching and encouraging dashboard designs that use longer refresh intervals with appropriate time range selections.</p>\n<p>⚠️ <strong>Pitfall: Missing Data Point Handling</strong>\nTime-series data often has gaps due to collection failures or network issues. Aggregation functions that don&#39;t handle missing data appropriately can produce misleading results - for example, averaging over incomplete data sets or assuming zero values where no data exists. The query engine must distinguish between explicit zero values and missing data, providing options for interpolation, forwarding previous values, or excluding incomplete time periods from calculations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Parser</td>\n<td>Hand-written recursive descent parser</td>\n<td>ANTLR or similar parser generator</td>\n</tr>\n<tr>\n<td>Expression Evaluation</td>\n<td>Direct AST traversal</td>\n<td>Bytecode compilation with virtual machine</td>\n</tr>\n<tr>\n<td>Result Serialization</td>\n<td>JSON with standard library</td>\n<td>MessagePack or Protocol Buffers</td>\n</tr>\n<tr>\n<td>Query Caching</td>\n<td>In-memory LRU cache</td>\n<td>Redis with TTL-based invalidation</td>\n</tr>\n<tr>\n<td>Concurrency Control</td>\n<td>Worker pool with channels</td>\n<td>Actor model with Akka-style messaging</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure\">File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/query/\n  engine.go              ← main QueryProcessor implementation\n  parser.go              ← query string parsing logic\n  planner.go             ← query optimization and planning\n  executor.go            ← execution engine coordination\n  aggregation.go         ← aggregation function implementations\n  ast.go                 ← abstract syntax tree definitions\n  cache.go               ← query result caching\n  engine_test.go         ← comprehensive query engine tests\n  testdata/              ← test query files and expected results\n    queries.json         ← test cases with expected outputs</code></pre></div>\n\n<h4 id=\"query-parser-infrastructure\">Query Parser Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">regexp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strconv</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TokenType represents different types of tokens in query strings</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenMetricName</span><span style=\"color:#B392F0\"> TokenType</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenLabelName</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenString</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRegex</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenNumber</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenFunction</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenOperator</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenLeftParen</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRightParen</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenLeftBrace</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRightBrace</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenLeftBracket</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRightBracket</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenComma</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenEqual</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenNotEqual</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRegexMatch</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenRegexNotMatch</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenEOF</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    TokenError</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Token represents a single token from query lexical analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Type     </span><span style=\"color:#B392F0\">TokenType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Position </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Lexer tokenizes query strings into structured tokens</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Lexer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    position </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current  </span><span style=\"color:#F97583\">rune</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewLexer creates a lexer for the given query string</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewLexer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">input</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Lexer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    l </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">Lexer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        input:    input,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        position: </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(input) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        l.current </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> rune</span><span style=\"color:#E1E4E8\">(input[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> l</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NextToken returns the next token from the input stream</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">l </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Lexer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">NextToken</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">Token</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> l.current </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">isWhitespace</span><span style=\"color:#E1E4E8\">(l.current) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            l.</span><span style=\"color:#B392F0\">skipWhitespace</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        switch</span><span style=\"color:#E1E4E8\"> l.current {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">singleCharToken</span><span style=\"color:#E1E4E8\">(TokenLeftParen)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">singleCharToken</span><span style=\"color:#E1E4E8\">(TokenRightParen)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">singleCharToken</span><span style=\"color:#E1E4E8\">(TokenLeftBrace)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">singleCharToken</span><span style=\"color:#E1E4E8\">(TokenRightBrace)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">[</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">singleCharToken</span><span style=\"color:#E1E4E8\">(TokenLeftBracket)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">]</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">singleCharToken</span><span style=\"color:#E1E4E8\">(TokenRightBracket)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">,</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">singleCharToken</span><span style=\"color:#E1E4E8\">(TokenComma)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">=</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">handleEquals</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">!</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">handleNotEquals</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">\"</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">readString</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        case</span><span style=\"color:#9ECBFF\"> '</span><span style=\"color:#79B8FF\">/</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">readRegex</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">isLetter</span><span style=\"color:#E1E4E8\">(l.current) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">readIdentifier</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">isDigit</span><span style=\"color:#E1E4E8\">(l.current) {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> l.</span><span style=\"color:#B392F0\">readNumber</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenError, Value: </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">(l.current), Position: l.position}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#B392F0\"> Token</span><span style=\"color:#E1E4E8\">{Type: TokenEOF, Position: l.position}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryAST represents the parsed structure of a query</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryAST</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Root </span><span style=\"color:#B392F0\">ASTNode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ASTNode represents a node in the query abstract syntax tree</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ASTNode</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    NodeType</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricSelectorNode represents metric selection with label matchers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricSelectorNode</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MetricName   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LabelMatchers []</span><span style=\"color:#B392F0\">LabelMatcher</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeRangeNode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricSelectorNode</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">NodeType</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"MetricSelector\"</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricSelectorNode</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation for string representation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">{...}\"</span><span style=\"color:#E1E4E8\">, n.MetricName)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FunctionCallNode represents aggregation function calls</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> FunctionCallNode</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FunctionName </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Arguments    []</span><span style=\"color:#B392F0\">ASTNode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Parameters   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FunctionCallNode</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">NodeType</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"FunctionCall\"</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FunctionCallNode</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">(...)\"</span><span style=\"color:#E1E4E8\">, n.FunctionName)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BinaryOpNode represents arithmetic operations between expressions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BinaryOpNode</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Left     </span><span style=\"color:#B392F0\">ASTNode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operator </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Right    </span><span style=\"color:#B392F0\">ASTNode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BinaryOpNode</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">NodeType</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"BinaryOp\"</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">BinaryOpNode</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"(</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#79B8FF\"> %s</span><span style=\"color:#79B8FF\"> %s</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">, n.Left, n.Operator, n.Right)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TimeRangeNode represents time range specifications</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TimeRangeNode</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Duration </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Start    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    End      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeRangeNode</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">NodeType</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"TimeRange\"</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">n </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TimeRangeNode</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> n.Duration </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"[</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">]\"</span><span style=\"color:#E1E4E8\">, n.Duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"[</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">:</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">]\"</span><span style=\"color:#E1E4E8\">, n.Start, n.End)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"query-execution-engine-core\">Query Execution Engine Core</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// QueryResult contains the result of query execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResultType </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"resultType\"`</span><span style=\"color:#6A737D\"> // \"vector\", \"matrix\", \"scalar\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Data       </span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}           </span><span style=\"color:#9ECBFF\">`json:\"data\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Warnings   []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"warnings,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Stats      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryExecutionStats</span><span style=\"color:#9ECBFF\">  `json:\"stats,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryExecutionStats provides metrics about query performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryExecutionStats</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SeriesCount      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"seriesCount\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SamplesProcessed </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"samplesProcessed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ExecutionTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"executionTime\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BlocksRead       </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"blocksRead\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteQuery processes a query string and returns results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// This is the main entry point that coordinates parsing, planning, and execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryProcessor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse the query string into an AST</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use NewParser(queryString).Parse() and handle syntax errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate the AST against available metrics and labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check metric names exist in storage, validate function signatures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create an optimized execution plan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Analyze label selectivity, determine block access patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Execute the plan against storage engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Retrieve samples, apply aggregations, handle errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Format results according to query type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Convert internal format to JSON-serializable result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Collect execution statistics for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Track series count, samples processed, execution time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PlanQuery analyzes a parsed query to create an optimized execution strategy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">qp </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryProcessor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">PlanQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ast</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">QueryAST</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ExecutionPlan</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Analyze label matchers to estimate series selectivity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Query series index to count matching series before data retrieval</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine optimal block access strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check time range against block boundaries, skip unnecessary blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Plan aggregation execution order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Some aggregations can stream, others need all data collected first</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Identify parallelization opportunities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Independent series can be processed concurrently</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Set memory limits based on estimated result size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Calculate expected series count × time range to estimate memory usage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecutionPlan contains optimized strategy for query execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ExecutionPlan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SeriesSelectors  []</span><span style=\"color:#B392F0\">SeriesSelector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange       </span><span style=\"color:#B392F0\">TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AggregationPlan []</span><span style=\"color:#B392F0\">AggregationStep</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryLimit     </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Parallelism     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SeriesSelector defines how to efficiently find matching time-series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SeriesSelector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MetricName    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LabelMatchers []</span><span style=\"color:#B392F0\">LabelMatcher</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EstimatedSeries </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AggregationStep defines one step in the aggregation pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AggregationStep</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Function    </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Parameters  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    InputType   </span><span style=\"color:#F97583\">string</span><span style=\"color:#6A737D\">  // \"vector\", \"matrix\", \"scalar\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    OutputType  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CanStream   </span><span style=\"color:#F97583\">bool</span><span style=\"color:#6A737D\">    // whether this step can process data incrementally</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"aggregation-function-library\">Aggregation Function Library</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// AggregationFunction defines the interface for all aggregation operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AggregationFunction</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Aggregate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">series</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">params</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    CanStream</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    RequiredParameters</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SumAggregation implements sum() function across multiple time-series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SumAggregation</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SumAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"sum\"</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SumAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CanStream</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SumAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RequiredParameters</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{} }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SumAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Aggregate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">series</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">params</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate input series are compatible for summation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check that series have overlapping time ranges</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create timestamp alignment buckets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Find common timestamp intervals across all input series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For each timestamp bucket, sum values from all series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Handle missing values appropriately (skip or interpolate)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Generate result time-series with combined labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Remove labels that differ across input series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle streaming execution for large datasets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Process samples in time order without buffering entire series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RateAggregation implements rate() function for counter metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> RateAggregation</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"rate\"</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CanStream</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RequiredParameters</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"range\"</span><span style=\"color:#E1E4E8\">} }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RateAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Aggregate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">series</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">params</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate this is being applied to counter metrics only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Check metric type metadata or detect always-increasing values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each series, calculate rate over the specified time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: rate = (current_value - previous_value) / time_difference</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle counter resets (when current &#x3C; previous)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: When counter resets, use current_value / time_since_reset for rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply rate calculation across the specified time window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use the time range parameter to determine window size</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return new time-series with per-second rate values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Preserve original labels but update metric name to indicate rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QuantileAggregation implements quantile() function for percentile calculations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QuantileAggregation</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QuantileAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"quantile\"</span><span style=\"color:#E1E4E8\"> }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QuantileAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CanStream</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> false</span><span style=\"color:#E1E4E8\"> } </span><span style=\"color:#6A737D\">// needs all data for sorting</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QuantileAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RequiredParameters</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"q\"</span><span style=\"color:#E1E4E8\">} }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QuantileAggregation</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Aggregate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">series</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">params</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}) ([]</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract quantile parameter (0.0 to 1.0)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Validate q parameter is valid percentile value</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each timestamp, collect all series values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Align timestamps and gather values across all series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Sort values at each timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sort.Float64s or implement streaming quantile algorithm for large datasets</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Calculate quantile value using interpolation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: For q=0.95, find value at position (count-1)*0.95 in sorted array</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate result series with quantile values over time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Create single output series with quantile labels added</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"query-result-caching-system\">Query Result Caching System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// QueryCache provides caching for expensive query results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryCache</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu       </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSize  </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxAge   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hitCount </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    missCount </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CacheEntry represents a cached query result with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> CacheEntry</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Result    </span><span style=\"color:#B392F0\">QueryResult</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryHash </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CreatedAt </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AccessCount </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Size      </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewQueryCache creates a query result cache with specified limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewQueryCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">maxSize</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">maxAge</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cache:   </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">CacheEntry</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxSize: maxSize,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxAge:  maxAge,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Get retrieves cached query result if available and fresh</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Get</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate cache key from query string and time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use hash of query + normalized time range for consistent keys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if cached entry exists and is still valid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Verify entry age is less than maxAge and data covers requested time range</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Update access statistics for cache hit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Increment hit count and entry access count atomically</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return cached result if valid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Deep copy result to prevent caller modifications</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Put stores query result in cache with size limits</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryCache</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Put</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">result</span><span style=\"color:#B392F0\"> QueryResult</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate result size for memory management</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Estimate memory usage of result data structures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if adding this entry would exceed cache limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Implement LRU eviction if cache is full</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Store entry with metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Include creation time, access count, and size information</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update cache statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Track cache size, entry count, and miss count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the Query Engine, verify the following behaviors:</p>\n<p><strong>Basic Query Execution</strong>: Start your metrics system and ingest sample data with multiple time-series. Execute a simple query like <code>cpu_usage{host=&quot;web01&quot;}</code> and verify it returns the correct samples within the requested time range.</p>\n<p><strong>Aggregation Functions</strong>: Test aggregation functions by executing <code>sum(cpu_usage)</code> and <code>avg(cpu_usage)</code> queries. Verify that the results correctly combine multiple time-series and handle timestamp alignment appropriately.</p>\n<p><strong>Rate Calculations</strong>: For counter metrics, test <code>rate(http_requests_total[5m])</code> and verify it correctly calculates per-second rates while handling counter resets appropriately.</p>\n<p><strong>Error Handling</strong>: Submit invalid queries with syntax errors, unknown metrics, and incompatible operations. Verify that the query engine returns helpful error messages rather than crashing.</p>\n<p><strong>Performance Testing</strong>: Execute queries against datasets with varying cardinalities (100 series, 1000 series, 10000 series) and measure response times. Query execution should remain responsive even with high-cardinality metrics.</p>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Queries return empty results</td>\n<td>Metric names or labels don&#39;t match stored data</td>\n<td>Check series index for available metrics and labels</td>\n<td>Verify query selectors match actual stored data</td>\n</tr>\n<tr>\n<td>Aggregation results are incorrect</td>\n<td>Timestamp alignment issues or missing data handling</td>\n<td>Log timestamp distribution across input series</td>\n<td>Implement proper timestamp bucketing and interpolation</td>\n</tr>\n<tr>\n<td>Rate calculations show negative values</td>\n<td>Counter resets not detected properly</td>\n<td>Check for decreasing values in counter series</td>\n<td>Add counter reset detection in rate function</td>\n</tr>\n<tr>\n<td>Query execution is extremely slow</td>\n<td>Reading too much data or inefficient aggregation</td>\n<td>Profile query execution and check block access patterns</td>\n<td>Add more selective label filters or optimize storage access</td>\n</tr>\n<tr>\n<td>Memory usage grows without bounds</td>\n<td>High cardinality queries without limits</td>\n<td>Monitor memory allocation during query processing</td>\n<td>Implement memory limits and streaming aggregation</td>\n</tr>\n<tr>\n<td>Cached results are stale or incorrect</td>\n<td>Cache invalidation logic has bugs</td>\n<td>Check cache hit/miss ratios and entry timestamps</td>\n<td>Fix cache key generation and expiration logic</td>\n</tr>\n</tbody></table>\n<h2 id=\"visualization-dashboard\">Visualization Dashboard</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 3 (Visualization Dashboard) - This section details the web-based dashboard system that provides real-time metric visualization with configurable panels and sharing capabilities</p>\n</blockquote>\n<h3 id=\"mental-model-the-mission-control-center\">Mental Model: The Mission Control Center</h3>\n<p>Think of a metrics dashboard as NASA&#39;s mission control center during a space launch. The mission control center has multiple large displays, each showing different aspects of the mission - rocket telemetry, weather conditions, trajectory calculations, and system health indicators. Each display is carefully positioned and sized based on its importance and the information it conveys. Mission controllers can quickly glance across all displays to understand the complete system state, and critical alerts are highlighted prominently to demand immediate attention.</p>\n<p>Your metrics dashboard operates on the same principle. Each panel acts like one of those mission control displays, showing a specific aspect of your system&#39;s health through charts and graphs. Just as mission control displays update in real-time as new telemetry data arrives from the spacecraft, your dashboard panels refresh continuously as new metric samples flow into the storage engine. The dashboard&#39;s layout and panel configuration serve as the &quot;mission plan&quot; - defining what information to display, where to position it, and how frequently to update it.</p>\n<p>The mission control analogy extends to user interaction patterns. Mission controllers don&#39;t constantly reconfigure their displays during critical operations - they rely on pre-configured views that show the right information at the right time. Similarly, effective dashboard design involves creating persistent configurations that teams can rely on during incident response. The ability to share dashboard views mirrors how mission control can broadcast their displays to other teams who need situational awareness.</p>\n<p>This mental model helps us understand why dashboard performance and reliability are critical. Just as mission control cannot afford display failures during launch sequences, your metrics dashboard must maintain consistent performance even when visualizing thousands of time-series or handling rapid data updates. The real-time nature of the system means that stale or missing data can lead to incorrect operational decisions.</p>\n<h3 id=\"dashboard-configuration-system\">Dashboard Configuration System</h3>\n<p>The dashboard configuration system serves as the blueprint that transforms raw time-series data into meaningful visual representations. This system must balance flexibility with simplicity, allowing users to create sophisticated visualizations without requiring deep technical knowledge of the underlying query language or storage internals.</p>\n<p><strong>Dashboard Entity Structure</strong></p>\n<p>The core dashboard entity encapsulates all information needed to render and maintain a complete dashboard view. The relationship between dashboards and panels follows a hierarchical ownership model where each dashboard owns its panel configurations completely.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ID</td>\n<td>string</td>\n<td>Unique identifier for the dashboard, used for persistence and sharing</td>\n</tr>\n<tr>\n<td>Title</td>\n<td>string</td>\n<td>Human-readable dashboard name displayed in the interface</td>\n</tr>\n<tr>\n<td>Description</td>\n<td>string</td>\n<td>Optional detailed description explaining the dashboard&#39;s purpose</td>\n</tr>\n<tr>\n<td>Tags</td>\n<td>[]string</td>\n<td>Searchable keywords for dashboard categorization and discovery</td>\n</tr>\n<tr>\n<td>Panels</td>\n<td>[]Panel</td>\n<td>Complete list of visualization panels with their configurations</td>\n</tr>\n<tr>\n<td>TimeRange</td>\n<td>TimeRange</td>\n<td>Global time window applied to all panels unless overridden</td>\n</tr>\n<tr>\n<td>RefreshInterval</td>\n<td>time.Duration</td>\n<td>Automatic update frequency for all panels in the dashboard</td>\n</tr>\n<tr>\n<td>Editable</td>\n<td>bool</td>\n<td>Whether users can modify this dashboard configuration</td>\n</tr>\n</tbody></table>\n<p><strong>Panel Configuration Architecture</strong></p>\n<p>Each panel represents an independent visualization component with its own query, display options, and positioning information. The panel system uses a plugin-like architecture where different panel types (line charts, bar charts, single-stat displays) share common configuration patterns while allowing type-specific customization.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>ID</td>\n<td>string</td>\n<td>Unique panel identifier within the dashboard scope</td>\n</tr>\n<tr>\n<td>Title</td>\n<td>string</td>\n<td>Panel header text displayed above the visualization</td>\n</tr>\n<tr>\n<td>Type</td>\n<td>string</td>\n<td>Panel renderer type (linechart, barchart, singlestat, table)</td>\n</tr>\n<tr>\n<td>GridPos</td>\n<td>GridPosition</td>\n<td>Panel size and position within the dashboard grid layout</td>\n</tr>\n<tr>\n<td>Query</td>\n<td>PanelQuery</td>\n<td>Metric query definition and time range specifications</td>\n</tr>\n<tr>\n<td>Options</td>\n<td>map[string]interface{}</td>\n<td>Type-specific rendering options and styling preferences</td>\n</tr>\n<tr>\n<td>AlertRule</td>\n<td>*PanelAlertRule</td>\n<td>Optional alert threshold configuration tied to this panel</td>\n</tr>\n</tbody></table>\n<p>The <code>GridPosition</code> structure implements a responsive grid system that ensures consistent layout across different screen sizes while allowing precise positioning control.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>X</td>\n<td>int</td>\n<td>Horizontal grid position (0-based, left to right)</td>\n</tr>\n<tr>\n<td>Y</td>\n<td>int</td>\n<td>Vertical grid position (0-based, top to bottom)</td>\n</tr>\n<tr>\n<td>Width</td>\n<td>int</td>\n<td>Panel width in grid units (typically 1-12 columns)</td>\n</tr>\n<tr>\n<td>Height</td>\n<td>int</td>\n<td>Panel height in grid units (affects chart detail level)</td>\n</tr>\n</tbody></table>\n<p><strong>Query Integration Architecture</strong></p>\n<p>The panel query system bridges the dashboard layer with the underlying query engine, translating user-friendly configuration into executable query plans. This abstraction allows dashboard users to work with intuitive concepts like &quot;metric name&quot; and &quot;time range&quot; without understanding the complexities of query optimization or storage access patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Expression</td>\n<td>string</td>\n<td>Query expression using the system&#39;s query language syntax</td>\n</tr>\n<tr>\n<td>TimeRange</td>\n<td>*TimeRange</td>\n<td>Panel-specific time range override (inherits from dashboard if nil)</td>\n</tr>\n<tr>\n<td>RefreshInterval</td>\n<td>*time.Duration</td>\n<td>Panel-specific refresh rate override</td>\n</tr>\n<tr>\n<td>MaxDataPoints</td>\n<td>int</td>\n<td>Maximum number of data points to retrieve (controls resolution)</td>\n</tr>\n<tr>\n<td>FormatAs</td>\n<td>string</td>\n<td>Result format preference (timeseries, table, scalar)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: JSON-Based Configuration Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Dashboard configurations need persistent storage with human-readable format for version control and manual editing</li>\n<li><strong>Options Considered</strong>: Binary protocol buffers, YAML files, JSON documents, SQL schema</li>\n<li><strong>Decision</strong>: JSON documents stored as files with optional database backend</li>\n<li><strong>Rationale</strong>: JSON provides excellent tooling support, version control compatibility, and direct JavaScript integration for the web interface</li>\n<li><strong>Consequences</strong>: Enables easy dashboard sharing and backup, but requires careful schema versioning for backward compatibility</li>\n</ul>\n</blockquote>\n<p><strong>Configuration Validation and Normalization</strong></p>\n<p>The dashboard configuration system implements comprehensive validation to prevent runtime errors and ensure consistent behavior across different deployment environments. Validation occurs at multiple layers: structural validation during JSON parsing, semantic validation during dashboard loading, and runtime validation during query execution.</p>\n<p>Dashboard validation encompasses several critical areas:</p>\n<ol>\n<li><strong>Structural Validation</strong>: Verifies that required fields are present, data types match expectations, and nested objects conform to their schemas</li>\n<li><strong>Query Validation</strong>: Ensures that panel queries use valid syntax and reference existing metrics</li>\n<li><strong>Layout Validation</strong>: Confirms that panel grid positions don&#39;t create overlapping layouts or exceed reasonable size constraints</li>\n<li><strong>Reference Validation</strong>: Checks that alert rule references point to valid notification channels and use appropriate threshold operators</li>\n</ol>\n<p>The validation system produces detailed error reports that help users identify and fix configuration problems quickly. Rather than failing silently or with cryptic error messages, the system provides specific field-level feedback with suggested corrections.</p>\n<p><strong>Template and Variable System</strong></p>\n<p>Advanced dashboard configurations benefit from templating capabilities that allow dynamic metric selection and parameterized queries. This system enables creation of reusable dashboard templates that work across different environments or services without requiring manual reconfiguration.</p>\n<p>Dashboard variables support several interpolation patterns:</p>\n<ol>\n<li><strong>Metric Name Variables</strong>: Allow selection from available metrics matching a pattern</li>\n<li><strong>Label Value Variables</strong>: Provide dropdown selection of label values for filtering</li>\n<li><strong>Time Range Variables</strong>: Enable quick switching between common time windows</li>\n<li><strong>Custom Variables</strong>: Support arbitrary string substitution for advanced use cases</li>\n</ol>\n<p>The template engine processes variable substitution during query generation, ensuring that the underlying query engine receives fully-resolved expressions while maintaining the user-friendly parameterized interface.</p>\n<h3 id=\"real-time-data-updates\">Real-Time Data Updates</h3>\n<p>Real-time dashboard updates represent one of the most technically challenging aspects of the visualization system. The architecture must efficiently push fresh data to multiple concurrent dashboard viewers while managing resource consumption and maintaining consistent user experience across varying network conditions.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fdashboard-update-flow.svg\" alt=\"Real-Time Dashboard Updates\"></p>\n<p><strong>WebSocket Communication Architecture</strong></p>\n<p>The real-time update system uses WebSocket connections to establish persistent, bidirectional communication channels between the dashboard server and client browsers. This approach provides significantly lower latency compared to HTTP polling while reducing server resource consumption through connection reuse.</p>\n<p>The WebSocket protocol implementation handles several critical responsibilities:</p>\n<ol>\n<li><strong>Connection Lifecycle Management</strong>: Establishes connections during dashboard load, maintains heartbeat monitoring, and gracefully handles disconnections with automatic reconnection logic</li>\n<li><strong>Message Routing</strong>: Distributes metric updates only to clients displaying relevant dashboards, avoiding unnecessary data transmission</li>\n<li><strong>Update Scheduling</strong>: Coordinates refresh cycles across panels to prevent overwhelming clients with excessive update frequencies</li>\n<li><strong>Backpressure Handling</strong>: Implements client-side buffering and server-side rate limiting when update rates exceed client processing capacity</li>\n</ol>\n<p><strong>Update Subscription Model</strong></p>\n<p>Each dashboard client establishes subscriptions for the specific metrics and time ranges displayed in its panels. The subscription system tracks active queries and their refresh requirements, enabling the server to batch related updates and minimize redundant query execution.</p>\n<table>\n<thead>\n<tr>\n<th>Subscription Component</th>\n<th>Purpose</th>\n<th>Key Attributes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Fingerprint</td>\n<td>Identifies unique query/time-range combinations</td>\n<td>Hash of expression, labels, time window</td>\n</tr>\n<tr>\n<td>Refresh Schedule</td>\n<td>Determines update timing for each subscription</td>\n<td>Interval, next execution time, jitter offset</td>\n</tr>\n<tr>\n<td>Client Set</td>\n<td>Tracks which clients need specific updates</td>\n<td>Connection IDs, panel mappings, active status</td>\n</tr>\n<tr>\n<td>Result Cache</td>\n<td>Stores recent query results for immediate delivery</td>\n<td>Timestamped data, expiration time, memory limit</td>\n</tr>\n</tbody></table>\n<p>The subscription model optimizes resource usage by sharing query results across multiple clients viewing identical data. When multiple users access the same dashboard, the system executes each unique query only once per refresh cycle, then distributes results to all interested clients.</p>\n<p><strong>Incremental Update Strategy</strong></p>\n<p>Rather than retransmitting complete datasets on every refresh, the system implements incremental updates that send only new or changed data points. This approach dramatically reduces bandwidth consumption and improves client-side rendering performance, especially for dashboards displaying long time ranges with high-frequency updates.</p>\n<p>The incremental update algorithm works through a multi-step process:</p>\n<ol>\n<li><strong>Baseline Establishment</strong>: New clients receive complete datasets for initial rendering, establishing a known synchronization point</li>\n<li><strong>Delta Computation</strong>: Subsequent updates calculate differences between current and previously transmitted data</li>\n<li><strong>Efficient Encoding</strong>: Delta messages use compact representation focusing on timestamp ranges and value changes</li>\n<li><strong>Client-Side Merging</strong>: Dashboard clients merge incremental updates with existing datasets, maintaining complete time-series views</li>\n<li><strong>Periodic Resynchronization</strong>: Full dataset refresh occurs periodically to prevent drift from accumulated deltas</li>\n</ol>\n<p><strong>Adaptive Update Frequency</strong></p>\n<p>The real-time update system implements adaptive refresh rates that balance data freshness with system performance. Rather than using fixed update intervals, the system monitors several factors to determine optimal refresh timing for each dashboard and panel combination.</p>\n<p>Adaptive frequency calculation considers:</p>\n<ul>\n<li><strong>Data Velocity</strong>: Metrics with higher ingestion rates benefit from more frequent updates</li>\n<li><strong>Panel Visibility</strong>: Off-screen or inactive panels receive lower-priority update scheduling</li>\n<li><strong>Client Performance</strong>: Slow client connections or processing delays trigger automatic rate reduction</li>\n<li><strong>System Load</strong>: High query engine utilization results in reduced refresh frequencies across all clients</li>\n<li><strong>User Preferences</strong>: Explicit refresh rate settings override automatic calculations when specified</li>\n</ul>\n<blockquote>\n<p><strong>Decision: WebSocket-Based Real-Time Updates</strong></p>\n<ul>\n<li><strong>Context</strong>: Dashboard users need immediate visibility into metric changes without manual refresh actions</li>\n<li><strong>Options Considered</strong>: HTTP polling, Server-Sent Events (SSE), WebSocket connections, UDP broadcast</li>\n<li><strong>Decision</strong>: WebSocket connections with intelligent subscription management</li>\n<li><strong>Rationale</strong>: WebSockets provide bidirectional communication needed for subscription management while maintaining low latency and efficient resource usage</li>\n<li><strong>Consequences</strong>: Enables true real-time experience but requires connection state management and fallback handling for network disruptions</li>\n</ul>\n</blockquote>\n<p><strong>Error Handling and Resilience</strong></p>\n<p>Real-time update systems must gracefully handle various failure modes without disrupting the overall dashboard experience. The architecture implements comprehensive error handling that maintains service availability even when individual components experience problems.</p>\n<p>Connection-level error handling addresses network disruptions, client disconnections, and server restart scenarios. The client-side implementation includes exponential backoff reconnection logic that prevents overwhelming the server during widespread connectivity issues. Server-side connection tracking removes stale subscriptions and reclaims associated resources when clients disconnect unexpectedly.</p>\n<p>Query-level error handling ensures that problems with individual panel queries don&#39;t affect other dashboard components. Failed queries trigger client-side error displays while continuing to update successful panels normally. The system maintains error state information to provide meaningful feedback about persistent query problems.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete implementation details for building the dashboard visualization system, focusing on the web server architecture, real-time communication, and client-side rendering components.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Web Server</td>\n<td>net/http with gorilla/mux router</td>\n<td>Gin or Echo framework with middleware</td>\n</tr>\n<tr>\n<td>WebSocket</td>\n<td>gorilla/websocket library</td>\n<td>Custom WebSocket with compression</td>\n</tr>\n<tr>\n<td>Frontend</td>\n<td>Vanilla JavaScript + Chart.js</td>\n<td>React/Vue with D3.js or Plotly</td>\n</tr>\n<tr>\n<td>Static Assets</td>\n<td>Embedded with go:embed</td>\n<td>CDN with build pipeline</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>JSON files with validation</td>\n<td>YAML with schema validation</td>\n</tr>\n</tbody></table>\n<p><strong>File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    dashboard-server/\n      main.go                     ← Dashboard server entry point\n  internal/\n    dashboard/\n      server.go                   ← HTTP server and WebSocket handling\n      config.go                   ← Dashboard configuration management\n      subscription.go             ← Real-time update subscriptions\n      panel.go                    ← Panel rendering and query execution\n      validation.go               ← Configuration validation logic\n      templates.go                ← Variable substitution and templating\n    dashboard/handlers/\n      dashboard_handlers.go       ← REST API for dashboard CRUD operations\n      websocket_handlers.go       ← WebSocket connection management\n      static_handlers.go          ← Static asset serving\n    dashboard/models/\n      dashboard.go                ← Dashboard and Panel data structures\n      subscription.go             ← Subscription and update models\n  web/\n    static/\n      js/\n        dashboard.js              ← Dashboard client-side logic\n        chart-renderer.js         ← Chart rendering with Chart.js\n        websocket-client.js       ← WebSocket communication handling\n      css/\n        dashboard.css             ← Dashboard styling\n      index.html                  ← Main dashboard interface\n  configs/\n    dashboard-server.yaml         ← Dashboard server configuration</code></pre></div>\n\n<p><strong>Dashboard Server Infrastructure</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> dashboard</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/mux</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/gorilla/websocket</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DashboardServer manages the web interface and real-time updates for metrics visualization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DashboardServer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ServerConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryProcessor </span><span style=\"color:#B392F0\">QueryProcessor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // WebSocket management</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    upgrader       </span><span style=\"color:#B392F0\">websocket</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Upgrader</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clients        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WebSocketClient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    subscriptions  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SubscriptionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clientsMu      </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Dashboard storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dashboards     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Dashboard</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dashboardsMu   </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    server         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Server</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    shutdown       </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WebSocketClient represents a connected dashboard client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WebSocketClient</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ID           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conn         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">websocket</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Conn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    send         </span><span style=\"color:#F97583\">chan</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    subscriptions </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PanelSubscription</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PanelSubscription tracks a client's interest in specific panel data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PanelSubscription</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    PanelID         </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RefreshInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastUpdate      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TimeRange       </span><span style=\"color:#B392F0\">TimeRange</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewDashboardServer creates a new dashboard server instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewDashboardServer</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">ServerConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryProcessor</span><span style=\"color:#B392F0\"> QueryProcessor</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:        config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:        logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryProcessor: queryProcessor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        upgrader: </span><span style=\"color:#B392F0\">websocket</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Upgrader</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            CheckOrigin: </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#79B8FF\"> true</span><span style=\"color:#E1E4E8\"> }, </span><span style=\"color:#6A737D\">// Configure appropriately for production</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        clients:       </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">WebSocketClient</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        subscriptions: </span><span style=\"color:#B392F0\">NewSubscriptionManager</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        dashboards:    </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Dashboard</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        shutdown:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start initializes the dashboard server and begins serving requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Load existing dashboard configurations from storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set up HTTP routes for dashboard CRUD operations and static assets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize WebSocket upgrade handler for real-time connections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start subscription manager for coordinating panel updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Begin periodic cleanup of stale connections and subscriptions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Start HTTP server on configured port with timeout settings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use gorilla/mux for routing and implement graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully shuts down the dashboard server</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Signal shutdown to all background goroutines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Close all active WebSocket connections with proper close frames</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Save any modified dashboard configurations to persistent storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Shutdown HTTP server with context timeout for graceful connection draining</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Wait for all goroutines to complete or context timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>WebSocket Connection Management</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// HandleWebSocketUpgrade processes WebSocket connection requests from dashboard clients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleWebSocketUpgrade</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Upgrade HTTP connection to WebSocket using ds.upgrader</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate unique client ID and create WebSocketClient instance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Register client in ds.clients map with proper locking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Start goroutines for reading client messages and writing updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle client disconnection cleanup in defer statement</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use separate goroutines for reading and writing to avoid deadlocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HandleClientMessage processes incoming messages from WebSocket clients</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandleClientMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">client</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">WebSocketClient</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">messageType</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse message JSON to determine message type (subscribe, unsubscribe, ping)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For subscribe messages, create PanelSubscription and register with SubscriptionManager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: For unsubscribe messages, remove subscription and clean up resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For ping messages, respond with pong to maintain connection health</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log any parsing errors and send error response to client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Define message types as constants and use type switches for handling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BroadcastPanelUpdate sends updated data to all clients subscribed to a specific panel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">BroadcastPanelUpdate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">panelID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">PanelUpdateMessage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Find all clients subscribed to this panel ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Serialize update message to JSON format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Send message to each client's send channel (non-blocking)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove clients with full send channels (indicates slow/disconnected clients)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log broadcast statistics for monitoring purposes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use select with default case for non-blocking channel sends</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Dashboard Configuration Management</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// LoadDashboard retrieves a dashboard configuration by ID</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LoadDashboard</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">dashboardID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Dashboard</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check in-memory cache first with read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If not cached, load from persistent storage (file or database)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate dashboard configuration using validation.go functions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Cache validated dashboard in memory with write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return dashboard or appropriate error if not found/invalid</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sync.RWMutex for efficient concurrent access to dashboard cache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SaveDashboard persists a dashboard configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SaveDashboard</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">dashboard</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Dashboard</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate complete dashboard configuration including all panels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate unique ID if this is a new dashboard</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write dashboard JSON to persistent storage with atomic operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update in-memory cache with write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Notify any clients viewing this dashboard about configuration changes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use temporary files and atomic rename for safe persistence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateDashboard checks dashboard configuration for correctness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ds </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DashboardServer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateDashboard</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">dashboard</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Dashboard</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate required fields (ID, Title) are non-empty</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check TimeRange values are reasonable (From before To, not too far in past/future)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate each panel configuration including GridPosition bounds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Parse and validate all panel queries using query engine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Check for panel ID uniqueness within the dashboard</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Return ValidationErrors with specific field-level error messages</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Client-Side Dashboard Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">javascript</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// web/static/js/dashboard.js</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DashboardClient</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    constructor</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">dashboardId</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.dashboardId </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> dashboardId;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.panels </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> new</span><span style=\"color:#B392F0\"> Map</span><span style=\"color:#E1E4E8\">();</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.websocket </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.reconnectAttempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.maxReconnectAttempts </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.reconnectDelay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">; </span><span style=\"color:#6A737D\">// Start with 1 second delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Initialize dashboard and establish WebSocket connection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Load dashboard configuration from REST API</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Create and render all panels based on configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Establish WebSocket connection for real-time updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Subscribe to updates for all visible panels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Set up periodic connection health checks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Hint: Use fetch() for REST calls and native WebSocket API</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Handle incoming WebSocket messages with panel updates</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    handleWebSocketMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">event</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Parse JSON message and determine message type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: For panel updates, find corresponding panel and update data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: For error messages, display user-friendly error in affected panel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: For connection status changes, update UI connection indicator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Log any parsing errors to browser console for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Hint: Use try-catch around JSON.parse() for robust error handling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Reconnect WebSocket with exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    reconnectWebSocket</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Check if reconnection attempts exceed maximum limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Calculate delay using exponential backoff with jitter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Schedule reconnection attempt using setTimeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Reset reconnect counter on successful connection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Update UI to show connection status during reconnection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Hint: Use Math.random() for jitter to avoid thundering herd</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DashboardPanel</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    constructor</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">panelConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">container</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> panelConfig;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.container </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> container;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.chart </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        this</span><span style=\"color:#E1E4E8\">.lastUpdate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> null</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Render panel with Chart.js or similar library</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    render</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Create canvas element for chart rendering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Initialize Chart.js with panel-specific configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Configure chart options based on panel type (line, bar, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Set up chart update methods for real-time data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Handle panel resize events for responsive layout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Hint: Use Chart.js responsive configuration for automatic resizing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Update panel with new data from WebSocket</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    updateData</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">newData</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Validate incoming data format matches panel expectations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Merge new data with existing chart dataset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Update chart using Chart.js update() method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Update last update timestamp for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Handle any rendering errors gracefully with fallback display</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Hint: Use chart.update('none') for performance when data changes frequently</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the dashboard visualization system, verify the following behaviors:</p>\n<ol>\n<li><p><strong>Dashboard Configuration</strong>: Create a simple dashboard JSON file with 2-3 panels. Load it through the REST API and verify all panels render correctly with proper positioning.</p>\n</li>\n<li><p><strong>Real-Time Updates</strong>: Start the dashboard server, open a dashboard in a browser, and inject new metrics through the ingestion API. Verify that charts update automatically without page refresh within the configured refresh interval.</p>\n</li>\n<li><p><strong>WebSocket Connectivity</strong>: Use browser developer tools to monitor WebSocket traffic. Verify that subscription messages are sent when dashboards load and that panel updates arrive as JSON messages.</p>\n</li>\n<li><p><strong>Multiple Clients</strong>: Open the same dashboard in multiple browser tabs or different browsers. Verify that all clients receive updates simultaneously and that server resource usage remains reasonable.</p>\n</li>\n<li><p><strong>Error Handling</strong>: Stop the metrics ingestion engine while keeping the dashboard open. Verify that panels display appropriate error states and recover automatically when ingestion resumes.</p>\n</li>\n</ol>\n<p>Expected command output for successful implementation:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/dashboard-server/main.go</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">2024/01/15</span><span style=\"color:#9ECBFF\"> 10:30:00</span><span style=\"color:#9ECBFF\"> INFO</span><span style=\"color:#9ECBFF\"> Dashboard</span><span style=\"color:#9ECBFF\"> server</span><span style=\"color:#9ECBFF\"> starting</span><span style=\"color:#9ECBFF\"> port=</span><span style=\"color:#79B8FF\">3000</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">2024/01/15</span><span style=\"color:#9ECBFF\"> 10:30:00</span><span style=\"color:#9ECBFF\"> INFO</span><span style=\"color:#9ECBFF\"> WebSocket</span><span style=\"color:#9ECBFF\"> handler</span><span style=\"color:#9ECBFF\"> registered</span><span style=\"color:#9ECBFF\"> endpoint=/ws</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">2024/01/15</span><span style=\"color:#9ECBFF\"> 10:30:00</span><span style=\"color:#9ECBFF\"> INFO</span><span style=\"color:#9ECBFF\"> Static</span><span style=\"color:#9ECBFF\"> assets</span><span style=\"color:#9ECBFF\"> served</span><span style=\"color:#9ECBFF\"> path=/static/</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">2024/01/15</span><span style=\"color:#9ECBFF\"> 10:30:01</span><span style=\"color:#9ECBFF\"> INFO</span><span style=\"color:#9ECBFF\"> Dashboard</span><span style=\"color:#9ECBFF\"> server</span><span style=\"color:#9ECBFF\"> ready</span><span style=\"color:#9ECBFF\"> for</span><span style=\"color:#9ECBFF\"> connections</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># In another terminal, test dashboard loading</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">$</span><span style=\"color:#9ECBFF\"> curl</span><span style=\"color:#9ECBFF\"> http://localhost:3000/api/dashboards/test-dashboard</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">{</span><span style=\"color:#B392F0\">\"id\"</span><span style=\"color:#79B8FF\">:</span><span style=\"color:#B392F0\">\"test-dashboard\"</span><span style=\"color:#B392F0\">,</span><span style=\"color:#B392F0\">\"title\"</span><span style=\"color:#79B8FF\">:</span><span style=\"color:#B392F0\">\"System Overview\"</span><span style=\"color:#B392F0\">,</span><span style=\"color:#B392F0\">\"panels\"</span><span style=\"color:#79B8FF\">:</span><span style=\"color:#9ECBFF\">[</span><span style=\"color:#79B8FF\">...</span><span style=\"color:#9ECBFF\">]}</span></span></code></pre></div>\n\n<p><strong>Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Charts not updating in real-time</td>\n<td>WebSocket connection not established</td>\n<td>Check browser console for WebSocket errors</td>\n<td>Verify WebSocket endpoint URL and CORS configuration</td>\n</tr>\n<tr>\n<td>High CPU usage during updates</td>\n<td>Too frequent refresh intervals</td>\n<td>Monitor WebSocket message frequency</td>\n<td>Implement adaptive refresh rates based on client performance</td>\n</tr>\n<tr>\n<td>Panels showing &quot;No Data&quot;</td>\n<td>Query execution errors</td>\n<td>Check server logs for query parsing failures</td>\n<td>Validate panel queries against available metrics</td>\n</tr>\n<tr>\n<td>Dashboard layout broken</td>\n<td>Grid position conflicts</td>\n<td>Check panel GridPosition values for overlaps</td>\n<td>Implement layout validation during dashboard save</td>\n</tr>\n<tr>\n<td>Memory leaks in browser</td>\n<td>Chart.js instances not destroyed</td>\n<td>Monitor browser memory usage over time</td>\n<td>Call chart.destroy() when panels are removed</td>\n</tr>\n<tr>\n<td>WebSocket connections dropping</td>\n<td>Network issues or server overload</td>\n<td>Check WebSocket close codes and server metrics</td>\n<td>Implement exponential backoff reconnection logic</td>\n</tr>\n</tbody></table>\n<h2 id=\"alerting-system\">Alerting System</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> 4 (Alerting System) - This section details the alerting system that monitors metric data in real-time, evaluates threshold conditions, manages alert states, and delivers notifications through multiple channels</p>\n</blockquote>\n<h3 id=\"mental-model-the-security-guard-system\">Mental Model: The Security Guard System</h3>\n<p>Think of the alerting system as a sophisticated security monitoring service that guards a large corporate campus. Just like a security system, our alerting infrastructure operates on multiple layers of vigilance and response.</p>\n<p>The <strong>security guards</strong> are our <code>AlertEvaluator</code> instances - they patrol their assigned zones (metric queries) on regular schedules, constantly checking for anomalies. Each guard has a <strong>patrol route</strong> (alert rule expression) that defines which areas to monitor and what constitutes suspicious activity (threshold conditions). When a guard notices something unusual - say, a door that should be locked is open (CPU usage above 90%) - they don&#39;t immediately sound the alarm. Instead, they watch carefully for a specified duration to ensure it&#39;s not a false alarm (evaluation period).</p>\n<p>The <strong>dispatch center</strong> is our alert state management system. When guards report suspicious activity, the dispatch center tracks the evolving situation through distinct phases: <strong>investigating</strong> (pending state), <strong>confirmed incident</strong> (firing state), and <strong>all clear</strong> (resolved state). The dispatch center maintains detailed incident logs, ensuring that every status change is properly documented and that appropriate personnel are notified at each stage.</p>\n<p>The <strong>communication network</strong> represents our notification channels - some incidents require immediate phone calls to on-duty personnel (PagerDuty for critical alerts), while others might warrant email reports to the facilities team (email for warnings), and some situations need real-time updates in the security chat room (Slack integration). Each communication channel has its own protocols, message formats, and delivery confirmation requirements.</p>\n<p>Most importantly, the system includes <strong>intelligent filtering</strong> to prevent alert fatigue - just like how a security system shouldn&#39;t wake the entire building every time a cat triggers a motion sensor. This involves proper alert prioritization, silence periods during maintenance, and sophisticated grouping to avoid overwhelming responders with duplicate notifications.</p>\n<p>This mental model helps us understand that alerting isn&#39;t just about detecting problems - it&#39;s about managing the entire lifecycle of incident awareness, from initial detection through resolution, while maintaining the trust and attention of the people who respond to these alerts.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Falert-state-machine.svg\" alt=\"Alert State Machine\"></p>\n<h3 id=\"alert-rule-definition-and-evaluation\">Alert Rule Definition and Evaluation</h3>\n<p>Alert rules form the foundation of our monitoring system, defining the specific conditions that warrant human attention. Each rule represents a formal specification of what constitutes an abnormal system state and how the system should respond when that condition occurs.</p>\n<blockquote>\n<p><strong>Decision: Alert Rule Data Model</strong></p>\n<ul>\n<li><strong>Context</strong>: We need a standardized way to define monitoring conditions that can be evaluated consistently across the system while providing sufficient flexibility for diverse monitoring scenarios.</li>\n<li><strong>Options Considered</strong>: Simple threshold-only rules, complex multi-condition rules with Boolean logic, time-series anomaly detection rules</li>\n<li><strong>Decision</strong>: Structured threshold-based rules with duration conditions and metadata annotations</li>\n<li><strong>Rationale</strong>: Threshold-based rules are intuitive for operators, predictable in behavior, and can handle 80% of common monitoring scenarios. Duration conditions prevent alert flapping from brief spikes.</li>\n<li><strong>Consequences</strong>: Simple to implement and understand, but may require multiple rules for complex scenarios. Future extensions can add advanced detection methods.</li>\n</ul>\n</blockquote>\n<p>The <code>AlertRule</code> structure captures all essential information needed to evaluate and manage alerts:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ID</code></td>\n<td><code>string</code></td>\n<td>Unique identifier for the alert rule, used for tracking and referencing</td>\n</tr>\n<tr>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>Human-readable name displayed in notifications and dashboards</td>\n</tr>\n<tr>\n<td><code>Expression</code></td>\n<td><code>string</code></td>\n<td>Metric query expression that returns the value to evaluate against threshold</td>\n</tr>\n<tr>\n<td><code>Threshold</code></td>\n<td><code>float64</code></td>\n<td>Numeric value that triggers the alert when comparison condition is met</td>\n</tr>\n<tr>\n<td><code>Operator</code></td>\n<td><code>string</code></td>\n<td>Comparison operator: &quot;gt&quot; (greater than), &quot;lt&quot; (less than), &quot;eq&quot; (equal), &quot;ne&quot; (not equal)</td>\n</tr>\n<tr>\n<td><code>Duration</code></td>\n<td><code>time.Duration</code></td>\n<td>Minimum time the condition must persist before transitioning to firing state</td>\n</tr>\n<tr>\n<td><code>EvaluationInterval</code></td>\n<td><code>time.Duration</code></td>\n<td>How frequently this rule should be evaluated against current metric data</td>\n</tr>\n<tr>\n<td><code>Labels</code></td>\n<td><code>Labels</code></td>\n<td>Key-value pairs attached to alerts generated by this rule for routing and grouping</td>\n</tr>\n<tr>\n<td><code>Annotations</code></td>\n<td><code>map[string]string</code></td>\n<td>Additional metadata for alert descriptions, runbook links, and context information</td>\n</tr>\n</tbody></table>\n<p>Alert rule expressions leverage our query engine to retrieve metric data for evaluation. The expression syntax follows the same patterns established in our query processor, enabling complex aggregations and transformations. For example, an expression might be <code>rate(http_requests_total[5m])</code> to monitor request rate over a 5-minute window, or <code>avg(cpu_usage) by (instance)</code> to track average CPU usage per server instance.</p>\n<blockquote>\n<p><strong>Decision: Duration-Based Alert Evaluation</strong></p>\n<ul>\n<li><strong>Context</strong>: Raw metric values can be noisy, leading to false alerts from brief spikes or temporary network issues.</li>\n<li><strong>Options Considered</strong>: Immediate alerting on threshold breach, duration-based evaluation, statistical outlier detection</li>\n<li><strong>Decision</strong>: Require conditions to persist for a specified duration before firing alerts</li>\n<li><strong>Rationale</strong>: Duration thresholds dramatically reduce false positives while adding minimal delay for genuine issues. This matches operational intuition that brief spikes are usually not actionable.</li>\n<li><strong>Consequences</strong>: Alerts may take longer to fire, but operator confidence in alert validity increases significantly.</li>\n</ul>\n</blockquote>\n<p>The alert evaluation process follows a carefully orchestrated sequence:</p>\n<ol>\n<li><p><strong>Rule Scheduling</strong>: The <code>AlertEvaluator</code> maintains an internal scheduler that tracks the next evaluation time for each active rule. Rules with shorter evaluation intervals receive priority scheduling to ensure timely detection of rapidly changing conditions.</p>\n</li>\n<li><p><strong>Query Execution</strong>: When a rule&#39;s evaluation time arrives, the evaluator constructs a query request using the rule&#39;s expression and submits it to our <code>QueryProcessor</code>. The query includes the current timestamp and any necessary lookback window for rate calculations or aggregations.</p>\n</li>\n<li><p><strong>Threshold Comparison</strong>: The evaluator extracts the numeric result from the query response and applies the specified comparison operator against the configured threshold. For queries returning multiple time series (e.g., per-instance metrics), each series is evaluated independently.</p>\n</li>\n<li><p><strong>Duration Tracking</strong>: The system maintains state for each rule-series combination, recording when threshold conditions first became true. Only after the condition persists for the full duration period does the alert transition to firing state.</p>\n</li>\n<li><p><strong>State Transition</strong>: Based on current conditions and historical state, the evaluator determines appropriate state transitions and triggers any necessary notifications or state persistence operations.</p>\n</li>\n</ol>\n<p>The <code>AlertEvaluator</code> interface defines the core evaluation contract:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>EvaluateRules</code></td>\n<td><code>ctx context.Context</code></td>\n<td><code>error</code></td>\n<td>Processes all active alert rules, evaluating conditions and updating states</td>\n</tr>\n<tr>\n<td><code>UpdateAlertState</code></td>\n<td><code>ruleID string, seriesLabels Labels, newState AlertState</code></td>\n<td><code>error</code></td>\n<td>Persists alert state changes and triggers appropriate notifications</td>\n</tr>\n<tr>\n<td><code>GetActiveAlerts</code></td>\n<td><code>ctx context.Context</code></td>\n<td><code>[]ActiveAlert, error</code></td>\n<td>Retrieves currently firing or pending alerts for dashboard display</td>\n</tr>\n<tr>\n<td><code>SilenceAlert</code></td>\n<td><code>ctx context.Context, ruleID string, duration time.Duration</code></td>\n<td><code>error</code></td>\n<td>Temporarily suppresses notifications for specified alert during maintenance</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Evaluation Interval vs Duration Confusion</strong>\nA common mistake is setting the evaluation interval longer than the alert duration. If a rule has a 30-second duration but evaluates every 60 seconds, the alert can never fire because the condition never persists long enough between evaluations. Always ensure evaluation interval ≤ duration/2 for reliable detection.</p>\n<p>⚠️ <strong>Pitfall: Query Timeout Handling</strong>\nAlert evaluation queries must complete within the evaluation interval to maintain scheduling accuracy. Implement proper timeout handling and degraded operation modes when the query engine is overloaded. Consider using cached or approximate results rather than skipping evaluations entirely.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fnotification-flow.svg\" alt=\"Alert Notification Flow\"></p>\n<h3 id=\"alert-state-management\">Alert State Management</h3>\n<p>Alert state management represents one of the most critical aspects of reliable alerting systems. Unlike simple binary on/off switches, production alerting requires sophisticated state tracking that accounts for the temporal nature of system problems and the human workflow of incident response.</p>\n<blockquote>\n<p><strong>Decision: Four-State Alert Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Alerts need to represent the lifecycle of problems from initial detection through resolution, while avoiding false positives from brief anomalies.</li>\n<li><strong>Options Considered</strong>: Binary firing/resolved model, three-state pending/firing/resolved model, four-state model with explicit inactive state</li>\n<li><strong>Decision</strong>: Four distinct states: Inactive, Pending, Firing, and Resolved with explicit transition rules</li>\n<li><strong>Rationale</strong>: Four states provide clear semantics for each phase of alert lifecycle while supporting duration-based evaluation and proper resolution tracking.</li>\n<li><strong>Consequences</strong>: More complex state management logic, but significantly better operator experience and reduced alert fatigue.</li>\n</ul>\n</blockquote>\n<p>The alert state machine governs all transitions between states with strict rules that ensure consistency and predictable behavior:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Condition</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Inactive</code></td>\n<td>Threshold breached</td>\n<td><code>Pending</code></td>\n<td>Start duration timer, no notifications sent</td>\n</tr>\n<tr>\n<td><code>Pending</code></td>\n<td>Duration timer expires while condition persists</td>\n<td><code>Firing</code></td>\n<td>Send firing notification, update alert database</td>\n</tr>\n<tr>\n<td><code>Pending</code></td>\n<td>Condition returns to normal before duration expires</td>\n<td><code>Inactive</code></td>\n<td>Reset duration timer, no notifications sent</td>\n</tr>\n<tr>\n<td><code>Firing</code></td>\n<td>Condition returns to normal</td>\n<td><code>Resolved</code></td>\n<td>Send resolution notification, log resolution timestamp</td>\n</tr>\n<tr>\n<td><code>Resolved</code></td>\n<td>Condition breaches threshold again</td>\n<td><code>Pending</code></td>\n<td>Start new duration timer for potential re-firing</td>\n</tr>\n<tr>\n<td><code>Firing</code></td>\n<td>Manual silence activated</td>\n<td><code>Silenced</code></td>\n<td>Suppress notifications while maintaining state tracking</td>\n</tr>\n<tr>\n<td><code>Silenced</code></td>\n<td>Silence period expires</td>\n<td><code>Firing</code> or <code>Resolved</code></td>\n<td>Resume normal notification behavior based on current condition</td>\n</tr>\n</tbody></table>\n<p>This state model addresses several critical operational concerns. The <strong>Pending</strong> state prevents false alarms from brief spikes while still tracking that something unusual occurred. The <strong>Resolved</strong> state provides explicit confirmation that problems have cleared, which is essential for incident tracking and post-mortem analysis. The distinction between <strong>Resolved</strong> and <strong>Inactive</strong> ensures that resolution notifications are sent appropriately.</p>\n<p>Each alert instance maintains detailed state information:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AlertID</code></td>\n<td><code>string</code></td>\n<td>Unique identifier combining rule ID and series labels hash</td>\n</tr>\n<tr>\n<td><code>RuleID</code></td>\n<td><code>string</code></td>\n<td>Reference to the alert rule that generated this alert instance</td>\n</tr>\n<tr>\n<td><code>SeriesLabels</code></td>\n<td><code>Labels</code></td>\n<td>Specific label combination for this alert instance (e.g., instance=web-01)</td>\n</tr>\n<tr>\n<td><code>State</code></td>\n<td><code>AlertState</code></td>\n<td>Current state in the alert lifecycle state machine</td>\n</tr>\n<tr>\n<td><code>LastEvaluation</code></td>\n<td><code>time.Time</code></td>\n<td>Timestamp of the most recent rule evaluation for this alert</td>\n</tr>\n<tr>\n<td><code>StateChanged</code></td>\n<td><code>time.Time</code></td>\n<td>When the alert last transitioned between states</td>\n</tr>\n<tr>\n<td><code>FiredAt</code></td>\n<td><code>*time.Time</code></td>\n<td>When alert transitioned to firing state, nil if never fired</td>\n</tr>\n<tr>\n<td><code>ResolvedAt</code></td>\n<td><code>*time.Time</code></td>\n<td>When alert transitioned to resolved state, nil if not resolved</td>\n</tr>\n<tr>\n<td><code>Value</code></td>\n<td><code>float64</code></td>\n<td>Current metric value that was compared against threshold</td>\n</tr>\n<tr>\n<td><code>ActiveSince</code></td>\n<td><code>*time.Time</code></td>\n<td>When condition first became true (pending state start)</td>\n</tr>\n<tr>\n<td><code>Annotations</code></td>\n<td><code>map[string]string</code></td>\n<td>Resolved template variables and additional context</td>\n</tr>\n</tbody></table>\n<p>State transitions are not immediate but follow a careful evaluation process that considers timing constraints and current conditions. When evaluating a rule, the system must account for:</p>\n<p><strong>Timing Considerations</strong>: Each state transition has specific timing requirements. The pending-to-firing transition requires the condition to persist for the full duration period. Resolution detection should happen quickly to minimize notification delay, but must be confirmed across multiple evaluation cycles to avoid flapping.</p>\n<p><strong>Multi-Series Handling</strong>: Rules that return multiple time series (such as per-instance metrics) require independent state tracking for each series. An alert rule monitoring CPU usage across 10 servers creates 10 separate alert instances, each with its own state machine and notification lifecycle.</p>\n<p><strong>Persistence Requirements</strong>: Alert state must survive system restarts and component failures. The state management system implements write-ahead logging for all state transitions, ensuring that no alert state is lost even if the alerting system crashes during evaluation.</p>\n<blockquote>\n<p><strong>Decision: Alert State Persistence Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Alert state must survive system restarts while maintaining high write throughput during normal operations.</li>\n<li><strong>Options Considered</strong>: In-memory only with periodic snapshots, database with transactions, append-only log with periodic compaction</li>\n<li><strong>Decision</strong>: Hybrid approach with in-memory state and write-ahead log for durability</li>\n<li><strong>Rationale</strong>: In-memory operations provide fast evaluation performance, while WAL ensures durability without requiring complex database transactions.</li>\n<li><strong>Consequences</strong>: Fast evaluation performance with strong durability guarantees, but requires careful recovery logic during startup.</li>\n</ul>\n</blockquote>\n<p>The state management system implements several sophisticated features to handle edge cases:</p>\n<p><strong>Flapping Detection</strong>: When alerts rapidly transition between firing and resolved states, the system implements exponential backoff for notifications and marks the alert as flapping. This prevents notification storms while maintaining visibility into unstable conditions.</p>\n<p><strong>Staleness Handling</strong>: If metric data becomes unavailable (e.g., due to application crashes), the system implements configurable staleness policies. Alerts can be automatically resolved after a timeout period, or marked as &quot;insufficient data&quot; to distinguish from genuine resolution.</p>\n<p><strong>Group Notifications</strong>: Related alerts can be grouped together to reduce notification volume. For example, if 50 servers all lose network connectivity simultaneously, operators receive one grouped notification rather than 50 individual alerts.</p>\n<p>⚠️ <strong>Pitfall: State Transition Concurrency</strong>\nAlert state updates must be atomic to prevent race conditions when multiple evaluation goroutines update the same alert. Always use proper locking or atomic operations when modifying alert state, and ensure state transitions are logged before notifications are sent.</p>\n<p>⚠️ <strong>Pitfall: Resolution Detection Delay</strong>\nSetting evaluation intervals too long can delay resolution detection, leaving alerts in firing state long after problems clear. For critical alerts, consider using shorter evaluation intervals or implementing separate resolution checking with faster cycles.</p>\n<h3 id=\"notification-channel-integration\">Notification Channel Integration</h3>\n<p>The notification delivery system serves as the crucial bridge between alert detection and human response. A sophisticated notification system must handle multiple delivery channels, message formatting, delivery confirmation, and failure recovery while maintaining operator trust through reliable and well-formatted communications.</p>\n<blockquote>\n<p><strong>Decision: Multi-Channel Notification Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Different alerts require different notification urgency and routing, and organizations use diverse communication tools for incident response.</li>\n<li><strong>Options Considered</strong>: Single notification channel, plug-in architecture for channels, hardcoded integrations for common services</li>\n<li><strong>Decision</strong>: Unified notification interface with pluggable channel implementations and intelligent routing</li>\n<li><strong>Rationale</strong>: Pluggable architecture allows easy extension while unified interface ensures consistent behavior. Intelligent routing enables escalation policies and channel selection based on alert severity.</li>\n<li><strong>Consequences</strong>: More complex initial implementation, but highly flexible and maintainable for diverse organizational needs.</li>\n</ul>\n</blockquote>\n<p>The <code>NotificationChannel</code> structure provides a standardized interface for all delivery methods:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Type</code></td>\n<td><code>string</code></td>\n<td>Channel type identifier: &quot;email&quot;, &quot;slack&quot;, &quot;pagerduty&quot;, &quot;webhook&quot;</td>\n</tr>\n<tr>\n<td><code>Name</code></td>\n<td><code>string</code></td>\n<td>Human-readable channel name for configuration and debugging</td>\n</tr>\n<tr>\n<td><code>Config</code></td>\n<td><code>map[string]string</code></td>\n<td>Channel-specific configuration parameters (API keys, URLs, etc.)</td>\n</tr>\n<tr>\n<td><code>Enabled</code></td>\n<td><code>bool</code></td>\n<td>Whether this channel is currently active for notification delivery</td>\n</tr>\n<tr>\n<td><code>RateLimitBurst</code></td>\n<td><code>int</code></td>\n<td>Maximum notifications that can be sent in rapid succession</td>\n</tr>\n<tr>\n<td><code>RateLimitPeriod</code></td>\n<td><code>time.Duration</code></td>\n<td>Time window for rate limiting calculations</td>\n</tr>\n<tr>\n<td><code>RetryConfig</code></td>\n<td><code>RetryConfig</code></td>\n<td>Backoff and retry policies for failed delivery attempts</td>\n</tr>\n<tr>\n<td><code>MessageTemplate</code></td>\n<td><code>string</code></td>\n<td>Go template string for formatting notification messages</td>\n</tr>\n<tr>\n<td><code>Filters</code></td>\n<td><code>[]NotificationFilter</code></td>\n<td>Rules determining which alerts should use this channel</td>\n</tr>\n</tbody></table>\n<p>Each notification channel type implements a common interface that abstracts delivery details:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SendNotification</code></td>\n<td><code>ctx context.Context, alert Alert, message NotificationMessage</code></td>\n<td><code>error</code></td>\n<td>Delivers notification through this channel with delivery confirmation</td>\n</tr>\n<tr>\n<td><code>ValidateConfig</code></td>\n<td><code>config map[string]string</code></td>\n<td><code>error</code></td>\n<td>Checks channel configuration for required parameters and connectivity</td>\n</tr>\n<tr>\n<td><code>HealthCheck</code></td>\n<td><code>ctx context.Context</code></td>\n<td><code>error</code></td>\n<td>Verifies channel availability and authentication status</td>\n</tr>\n<tr>\n<td><code>FormatMessage</code></td>\n<td><code>alert Alert, template string</code></td>\n<td><code>NotificationMessage, error</code></td>\n<td>Applies message templating to generate channel-specific content</td>\n</tr>\n</tbody></table>\n<p><strong>Email Notification Implementation</strong>: Email channels support both plain text and HTML message formats with embedded metric charts and alert context. The implementation handles SMTP authentication, TLS encryption, and delivery status tracking. Email notifications include alert details, direct links to relevant dashboards, and suggested runbook procedures.</p>\n<p><strong>Slack Integration</strong>: Slack notifications leverage webhook integrations or bot API tokens to deliver rich formatted messages with interactive elements. Messages include color-coded severity indicators, metric snapshots, and action buttons for alert acknowledgment or silencing. The system supports both channel posting and direct message delivery based on alert routing rules.</p>\n<p><strong>PagerDuty Integration</strong>: For critical alerts requiring immediate response, PagerDuty integration creates incidents with proper escalation policies. The system maps alert severity levels to PagerDuty incident priorities and includes detailed context in incident descriptions. Auto-resolution is supported when alerts return to normal state.</p>\n<p><strong>Webhook Channels</strong>: Generic webhook support enables integration with custom tooling and services not directly supported. Webhook notifications include configurable payload formatting, authentication headers, and retry logic with exponential backoff.</p>\n<p>Message templating provides flexible content customization while ensuring consistent formatting across channels:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Alert: {{ .Alert.RuleName }}\nStatus: {{ .Alert.State }}\nInstance: {{ .Alert.Labels.instance }}\nValue: {{ .Alert.Value }} (threshold: {{ .Alert.Threshold }})\nStarted: {{ .Alert.ActiveSince.Format &quot;2006-01-02 15:04:05&quot; }}\nRunbook: {{ .Alert.Annotations.runbook_url }}\nDashboard: https://dashboard.example.com/d/{{ .Alert.Annotations.dashboard_id }}</code></pre></div>\n\n<p>The notification routing system determines which channels receive each alert based on configurable rules:</p>\n<table>\n<thead>\n<tr>\n<th>Filter Type</th>\n<th>Parameters</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>LabelFilter</code></td>\n<td><code>label string, values []string</code></td>\n<td>Route alerts with specific label values to designated channels</td>\n</tr>\n<tr>\n<td><code>SeverityFilter</code></td>\n<td><code>min_severity string, max_severity string</code></td>\n<td>Channel receives alerts within specified severity range</td>\n</tr>\n<tr>\n<td><code>TimeFilter</code></td>\n<td><code>start_time string, end_time string, timezone string</code></td>\n<td>Time-based routing for business hours vs. on-call coverage</td>\n</tr>\n<tr>\n<td><code>TeamFilter</code></td>\n<td><code>teams []string</code></td>\n<td>Route alerts to channels based on team ownership labels</td>\n</tr>\n<tr>\n<td><code>EscalationFilter</code></td>\n<td><code>delay time.Duration</code></td>\n<td>Secondary channels activated after specified delay if alert persists</td>\n</tr>\n</tbody></table>\n<p><strong>Delivery Reliability</strong>: The notification system implements sophisticated retry logic to handle transient failures. Each channel maintains a delivery queue with exponential backoff for failed attempts. Critical notifications use multiple delivery attempts across different time intervals, and persistent failures trigger fallback notification channels.</p>\n<p><strong>Rate Limiting</strong>: To prevent notification storms during widespread outages, each channel implements configurable rate limiting. The system uses token bucket algorithms to allow burst notifications while preventing sustained high-volume delivery that could overwhelm recipients or trigger service quotas.</p>\n<p><strong>Delivery Confirmation</strong>: Where supported by the underlying service, the system tracks delivery confirmation and maintains metrics on notification success rates. Failed deliveries are logged for troubleshooting, and persistent delivery failures can trigger alerts about the alerting system itself.</p>\n<p>⚠️ <strong>Pitfall: Template Rendering Errors</strong>\nNotification templates can fail to render if alert data is missing expected fields or contains invalid characters. Always validate templates during configuration and implement fallback rendering with basic alert information when template processing fails.</p>\n<p>⚠️ <strong>Pitfall: Notification Loop Prevention</strong>\nBe careful not to create notification loops where alerts about the alerting system itself trigger recursive notifications. Implement special handling for infrastructure alerts and avoid routing alerting system failures through the same notification channels.</p>\n<p>⚠️ <strong>Pitfall: Credential Management</strong>\nNotification channels require API keys and credentials that must be securely stored and rotated. Never log sensitive configuration values, implement proper secret management, and provide clear error messages when authentication fails without exposing credentials.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete implementation details for building the alerting system in Go, bridging the gap between the design concepts and working code.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Alert Storage</td>\n<td>In-memory with JSON file persistence</td>\n<td>BadgerDB embedded database</td>\n</tr>\n<tr>\n<td>Notification Queue</td>\n<td>Go channels with worker pools</td>\n<td>Redis with persistent queues</td>\n</tr>\n<tr>\n<td>HTTP Client</td>\n<td>net/http with custom retry logic</td>\n<td>go-retryablehttp library</td>\n</tr>\n<tr>\n<td>Message Templating</td>\n<td>text/template standard library</td>\n<td>Advanced templating with sprig functions</td>\n</tr>\n<tr>\n<td>State Persistence</td>\n<td>JSON files with atomic writes</td>\n<td>Write-ahead log with periodic snapshots</td>\n</tr>\n<tr>\n<td>Scheduling</td>\n<td>time.Ticker with goroutines</td>\n<td>Cron-like scheduler with distributed coordination</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure\">File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/server/main.go                    ← server entry point\n  internal/alerting/                    ← alerting system implementation\n    evaluator.go                        ← core alert evaluation logic\n    evaluator_test.go                   ← evaluation engine tests\n    state_manager.go                    ← alert state tracking\n    notification_manager.go             ← notification delivery coordination\n    channels/                           ← notification channel implementations\n      email.go                          ← email notification channel\n      slack.go                          ← Slack integration\n      webhook.go                        ← generic webhook channel\n      pagerduty.go                      ← PagerDuty integration\n    rules/                              ← alert rule management\n      parser.go                         ← rule configuration parsing\n      validator.go                      ← rule validation logic\n    templates/                          ← notification templates\n      default.tmpl                      ← default message template\n      slack.tmpl                        ← Slack-specific template\n  internal/storage/                     ← shared storage interfaces\n    interfaces.go                       ← storage contracts\n  pkg/config/                           ← configuration management\n    alerting.go                         ← alerting configuration structures</code></pre></div>\n\n<h4 id=\"alert-state-management-infrastructure\">Alert State Management Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> alerting</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertState represents the current state of an alert in the lifecycle</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStateInactive</span><span style=\"color:#B392F0\"> AlertState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStatePending</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStateFiring</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStateResolved</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AlertStateSilenced</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertInstance tracks the state and history of a specific alert</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertInstance</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AlertID         </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"alert_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RuleID          </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">                 `json:\"rule_id\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SeriesLabels    </span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#9ECBFF\">                 `json:\"series_labels\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    State           </span><span style=\"color:#B392F0\">AlertState</span><span style=\"color:#9ECBFF\">             `json:\"state\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastEvaluation  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"last_evaluation\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StateChanged    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">              `json:\"state_changed\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FiredAt         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">             `json:\"fired_at,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ResolvedAt      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">             `json:\"resolved_at,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value           </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">                `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ActiveSince     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">             `json:\"active_since,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Annotations     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">      `json:\"annotations\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NotificationLog []</span><span style=\"color:#B392F0\">NotificationRecord</span><span style=\"color:#9ECBFF\">   `json:\"notification_log\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NotificationRecord tracks when and how notifications were delivered</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NotificationRecord</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Channel     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"channel\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    State       </span><span style=\"color:#B392F0\">AlertState</span><span style=\"color:#9ECBFF\"> `json:\"state\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Success     </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"success\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ErrorMsg    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"error_msg,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryCount  </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">       `json:\"retry_count\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StateManager handles alert state transitions and persistence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StateManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    alerts      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertInstance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    persistence </span><span style=\"color:#B392F0\">StatePersistence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StatePersistence interface for alert state durability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StatePersistence</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    SaveState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">alertID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">state</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    LoadState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">alertID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    LoadAllStates</span><span style=\"color:#E1E4E8\">() (</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    DeleteState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">alertID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewStateManager creates a new alert state manager with persistence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewStateManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">persistence</span><span style=\"color:#B392F0\"> StatePersistence</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StateManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">StateManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        alerts:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        persistence: persistence,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:      logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateAlertState processes alert state transitions with proper validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StateManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateAlertState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ruleID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesLabels</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">currentValue</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">threshold</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">conditionMet</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Generate unique alert ID from rule ID and series labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Acquire write lock and get current alert state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Determine new state based on current state and condition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate state transition is legal according to state machine</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update alert instance with new state and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Persist state change to storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Release lock and trigger notifications if state changed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use SeriesID() method to generate consistent alert identifiers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetActiveAlerts returns all alerts in firing or pending states</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StateManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetActiveAlerts</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock for safe concurrent access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through all alerts and filter by active states</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create slice of active alerts for return</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Release lock and return filtered results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CleanupResolvedAlerts removes old resolved alerts from memory and storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StateManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CleanupResolvedAlerts</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">maxAge</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate cutoff time based on maxAge parameter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Acquire write lock for alert map modifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Identify resolved alerts older than cutoff time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Remove alerts from memory and delete from persistence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log cleanup statistics and release lock</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"alert-evaluation-engine-core\">Alert Evaluation Engine Core</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> alerting</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertEvaluator handles the core alert evaluation logic and scheduling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertEvaluator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryProcessor   </span><span style=\"color:#B392F0\">QueryProcessor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stateManager     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StateManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    notificationMgr  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger          </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh          </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    evaluationStats </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EvaluationStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EvaluationStats tracks alert evaluation performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> EvaluationStats</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TotalEvaluations    </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SuccessfulEvaluations </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FailedEvaluations   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AverageEvaluationTime </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastEvaluation      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewAlertEvaluator creates a new alert evaluation engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewAlertEvaluator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">queryProcessor</span><span style=\"color:#B392F0\"> QueryProcessor</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">stateManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StateManager</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ruleManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">RuleManager</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">notificationMgr</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryProcessor:  queryProcessor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stateManager:   stateManager,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ruleManager:    ruleManager,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        notificationMgr: notificationMgr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:         logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stopCh:         </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        evaluationStats: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">EvaluationStats</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins the alert evaluation loop with proper scheduling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize evaluation scheduler with configurable intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Start background goroutine for continuous evaluation loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Implement graceful shutdown handling with context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Set up periodic statistics reporting and health checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return any initialization errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.NewTicker for regular evaluation scheduling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EvaluateRules processes all active alert rules against current metric data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EvaluateRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get list of active alert rules from rule manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through each rule and check if evaluation is due</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute rule query expression against query processor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Extract numeric results and compare against thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update alert state through state manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Trigger notifications for state changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update evaluation statistics and handle any errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Handle multi-series query results by evaluating each series independently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// evaluateRule processes a single alert rule against current metric data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">evaluateRule</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Build query request with rule expression and current time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Execute query and handle timeout/error conditions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse query results into individual time series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: For each series, extract current value and labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Apply threshold comparison using rule operator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update alert state with new condition and value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Log evaluation results and any errors encountered</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully shuts down the alert evaluator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ae </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Signal stop to evaluation loop via channel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Wait for current evaluations to complete</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Clean up any pending notifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Log final evaluation statistics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    close</span><span style=\"color:#E1E4E8\">(ae.stopCh)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"notification-delivery-system\">Notification Delivery System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> alerting</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">text/template</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NotificationManager coordinates notification delivery across multiple channels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NotificationManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    channels    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NotificationChannel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    templates   </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">template</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Template</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    deliveryQueue </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NotificationRequest</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    workers     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NotificationRequest represents a pending notification delivery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NotificationRequest</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Alert        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertInstance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Channels     []</span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    State        </span><span style=\"color:#B392F0\">AlertState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryCount   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxRetries   </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NotificationChannel interface for all notification delivery methods</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NotificationChannel</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    SendNotification</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alert</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    ValidateConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    FormatMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">alert</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">template</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Type</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewNotificationManager creates a notification manager with configured channels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewNotificationManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">channels</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">NotificationChannel</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">workers</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mgr </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        channels:      </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">NotificationChannel</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        templates:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">template</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Template</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        deliveryQueue: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NotificationRequest</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        workers:       workers,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:        logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Register notification channels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, channel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> channels {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        mgr.channels[channel.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">()] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> channel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> mgr</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendNotification queues alert notification for delivery through specified channels</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">nm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendNotification</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alert</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">channels</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate that requested channels are configured and healthy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create notification request with alert details and channel list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply any routing rules or filters for channel selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Queue notification request for background delivery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return immediate acknowledgment or queueing errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Non-blocking queue insertion to avoid deadlocks during high load</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins notification delivery workers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">nm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start configured number of worker goroutines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Each worker processes delivery queue continuously</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Implement graceful shutdown with context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Set up delivery retry logic with exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Track delivery statistics and health metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// deliverNotification handles actual delivery to a specific channel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">nm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">deliverNotification</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">req</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NotificationRequest</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">channelName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get notification channel by name from registered channels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Select appropriate message template based on channel type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Render notification message using template and alert data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Attempt delivery through channel with timeout handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Record delivery attempt result and implement retry logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update notification log in alert instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// LoadTemplates reads notification templates from configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">nm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">LoadTemplates</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">templateDir</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Scan template directory for .tmpl files</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse each template file using text/template</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate template syntax and required variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Store parsed templates in manager template map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Set up template reloading on configuration changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetDeliveryStats returns notification delivery statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">nm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetDeliveryStats</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DeliveryStats</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Aggregate delivery statistics from all channels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate success rates and average delivery times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return structured statistics for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"email-notification-channel-implementation\">Email Notification Channel Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> channels</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">crypto/tls</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/smtp</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">strings</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EmailChannel implements notification delivery via SMTP email</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> EmailChannel</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config   </span><span style=\"color:#B392F0\">EmailConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    auth     </span><span style=\"color:#B392F0\">smtp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Auth</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">smtp</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EmailConfig holds SMTP configuration parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> EmailConfig</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SMTPHost     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"smtp_host\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SMTPPort     </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">    `yaml:\"smtp_port\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Username     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"username\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Password     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"password\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    FromAddress  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"from_address\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ToAddresses  []</span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"to_addresses\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    UseTLS       </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">   `yaml:\"use_tls\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Subject      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `yaml:\"subject\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewEmailChannel creates a new email notification channel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewEmailChannel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> EmailConfig</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EmailChannel</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate SMTP configuration parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set up SMTP authentication with provided credentials</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Test SMTP connection to validate configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Initialize email channel with working SMTP client</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return configured channel or connection errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendNotification delivers alert notification via email</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ec </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EmailChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendNotification</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alert</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Format email subject using alert data and configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Build complete email message with headers and body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Connect to SMTP server with TLS if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Authenticate using stored credentials</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send email to all configured recipients</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle SMTP errors and implement retry logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use net/smtp package with proper TLS configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateConfig checks email channel configuration for required fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ec </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EmailChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check that SMTP host and port are provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate authentication credentials</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify from address and recipient list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Test SMTP connectivity if possible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return detailed validation errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FormatMessage renders alert data into email-formatted message</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ec </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EmailChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">FormatMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">alert</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tmplStr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse message template string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create template data context with alert information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute template rendering with alert data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle template errors gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return formatted message or rendering errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck verifies SMTP connectivity and authentication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ec </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EmailChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Establish SMTP connection with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify authentication credentials</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Test basic SMTP commands without sending email</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return health status or connectivity errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ec </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EmailChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> ec.name }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ec </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">EmailChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Type</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"email\"</span><span style=\"color:#E1E4E8\"> }</span></span></code></pre></div>\n\n<h4 id=\"slack-notification-channel-implementation\">Slack Notification Channel Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> channels</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">bytes</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SlackChannel implements notification delivery via Slack webhooks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SlackChannel</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name      </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    webhookURL </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    channel   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    username  </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    iconEmoji </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    httpClient </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SlackMessage represents a Slack webhook payload</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SlackMessage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Channel   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"channel,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Username  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"username,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IconEmoji </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"icon_emoji,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Text      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">            `json:\"text\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Attachments []</span><span style=\"color:#B392F0\">SlackAttachment</span><span style=\"color:#9ECBFF\"> `json:\"attachments,omitempty\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SlackAttachment provides rich formatting for Slack messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SlackAttachment</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Color     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"color\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Title     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"title\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Text      </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"text\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Fields    []</span><span style=\"color:#B392F0\">SlackField</span><span style=\"color:#9ECBFF\">        `json:\"fields\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">               `json:\"ts\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Footer    </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">              `json:\"footer\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SlackField represents a field in a Slack attachment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> SlackField</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Title </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"title\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Short </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">   `json:\"short\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewSlackChannel creates a new Slack notification channel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewSlackChannel</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">webhookURL</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">channel</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">username</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">iconEmoji</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlackChannel</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">SlackChannel</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        name:       name,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        webhookURL: webhookURL,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        channel:    channel,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        username:   username,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        iconEmoji:  iconEmoji,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        httpClient: </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Client</span><span style=\"color:#E1E4E8\">{Timeout: </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:     logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SendNotification delivers alert notification to Slack channel</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlackChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SendNotification</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alert</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Build Slack message with proper formatting and attachments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Set color coding based on alert state (red=firing, green=resolved)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Include alert metadata as attachment fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Serialize message to JSON payload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Send HTTP POST request to Slack webhook URL</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle HTTP errors and implement retry logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use different colors for different alert states to provide visual cues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// FormatMessage creates Slack-formatted message with rich attachments</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlackChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">FormatMessage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">alert</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertInstance</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">tmplStr</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create Slack attachment with alert details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Add fields for alert name, state, value, and duration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Include links to dashboards and runbooks if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Format timestamp and alert metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return formatted Slack message structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateConfig verifies Slack channel configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlackChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateConfig</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check that webhook URL is provided and valid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate channel name format if specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Test webhook connectivity with a simple message</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return validation errors with helpful messages</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck tests Slack webhook connectivity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlackChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Send test message to Slack webhook</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Verify HTTP response indicates successful delivery</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Handle rate limiting and authentication errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return health status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlackChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#E1E4E8\"> sc.name }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sc </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">SlackChannel</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Type</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> { </span><span style=\"color:#F97583\">return</span><span style=\"color:#9ECBFF\"> \"slack\"</span><span style=\"color:#E1E4E8\"> }</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Checkpoint 1: Alert Rule Evaluation</strong>\nAfter implementing the basic alert evaluation logic:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> test</span><span style=\"color:#9ECBFF\"> ./internal/alerting/...</span></span></code></pre></div>\n<p>Expected behavior:</p>\n<ul>\n<li>Rules evaluate against mock metric data</li>\n<li>State transitions work correctly (inactive → pending → firing → resolved)</li>\n<li>Duration-based firing prevents false alarms</li>\n<li>Multiple series from single rule create separate alert instances</li>\n</ul>\n<p><strong>Checkpoint 2: Notification Delivery</strong>\nAfter implementing notification channels:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">go</span><span style=\"color:#9ECBFF\"> run</span><span style=\"color:#9ECBFF\"> cmd/server/main.go</span><span style=\"color:#79B8FF\"> --config=test-config.yaml</span></span></code></pre></div>\n<p>Test manual notification delivery:</p>\n<ul>\n<li>Send test alert via API: <code>curl -X POST localhost:8080/api/v1/alerts/test</code></li>\n<li>Verify email delivery to configured SMTP server</li>\n<li>Check Slack webhook receives properly formatted messages</li>\n<li>Confirm notification logs are recorded in alert instances</li>\n</ul>\n<p><strong>Checkpoint 3: End-to-End Alerting</strong>\nAfter complete implementation:</p>\n<ul>\n<li>Configure alert rule monitoring CPU usage &gt; 80%</li>\n<li>Generate load to trigger threshold breach</li>\n<li>Verify alert transitions: inactive → pending → firing</li>\n<li>Confirm notifications sent during state transitions</li>\n<li>Test alert resolution when condition clears</li>\n<li>Validate silence functionality during maintenance windows</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Alerts never fire</td>\n<td>Duration longer than evaluation interval</td>\n<td>Check rule timing configuration</td>\n<td>Set evaluation interval ≤ duration/2</td>\n</tr>\n<tr>\n<td>Alert flapping</td>\n<td>Threshold too close to normal values</td>\n<td>Analyze metric variance over time</td>\n<td>Adjust threshold or use averaging</td>\n</tr>\n<tr>\n<td>Missing notifications</td>\n<td>Notification channel configuration error</td>\n<td>Check channel health endpoints</td>\n<td>Validate SMTP/webhook credentials</td>\n</tr>\n<tr>\n<td>Duplicate notifications</td>\n<td>State transition logic error</td>\n<td>Review alert state logs</td>\n<td>Fix state persistence or locking</td>\n</tr>\n<tr>\n<td>Slow evaluation</td>\n<td>Expensive query expressions</td>\n<td>Profile query execution times</td>\n<td>Optimize queries or reduce evaluation frequency</td>\n</tr>\n<tr>\n<td>Memory growth</td>\n<td>Alert instances not cleaned up</td>\n<td>Monitor resolved alert counts</td>\n<td>Implement periodic cleanup of old alerts</td>\n</tr>\n<tr>\n<td>Notification delays</td>\n<td>Delivery queue backlog</td>\n<td>Check notification worker metrics</td>\n<td>Increase worker pool size or optimize delivery</td>\n</tr>\n</tbody></table>\n<h2 id=\"component-interactions-and-data-flow\">Component Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (component interactions span the entire system architecture from metrics ingestion through storage, querying, visualization, and alerting)</p>\n</blockquote>\n<h3 id=\"the-city-infrastructure-metaphor-understanding-system-communication\">The City Infrastructure Metaphor: Understanding System Communication</h3>\n<p>Think of our metrics system as a modern city&#39;s infrastructure. The <strong>metrics ingestion engine</strong> acts like the city&#39;s logistics network, receiving shipments from various sources and routing them to appropriate destinations. The <strong>storage engine</strong> functions as the city&#39;s warehouses and archives, organizing and preserving goods for future retrieval. The <strong>query engine</strong> operates like the city&#39;s information services, helping residents find and access what they need. The <strong>dashboard</strong> serves as the city&#39;s command center, providing real-time visibility into city operations. Finally, the <strong>alerting system</strong> acts as the emergency response network, monitoring conditions and coordinating responses when problems arise.</p>\n<p>Just as a city&#39;s infrastructure components must communicate seamlessly through well-defined protocols and channels, our metrics system components interact through carefully designed APIs, message formats, and data flows. Each component has specific responsibilities but must coordinate with others to achieve the overall system goals. Understanding these interactions is crucial because the system&#39;s reliability and performance depend not just on individual component quality, but on how effectively they work together.</p>\n<p>The key insight is that <strong>data flows through the system in predictable patterns</strong>, and each component transformation adds value while maintaining data integrity. When a metric enters the system, it triggers a cascade of interactions: validation, storage, indexing, and potentially alerting. When a user queries data, it initiates a different interaction pattern: query parsing, execution planning, storage retrieval, and result formatting. Understanding these flows helps developers debug issues, optimize performance, and extend functionality.</p>\n<blockquote>\n<p><strong>Decision: Event-Driven vs Request-Response Communication</strong></p>\n<ul>\n<li><strong>Context</strong>: Components need to communicate both for real-time data flow and on-demand operations</li>\n<li><strong>Options Considered</strong>: Pure event-driven messaging, pure request-response APIs, hybrid approach</li>\n<li><strong>Decision</strong>: Hybrid approach with request-response for queries and event-driven for data flow</li>\n<li><strong>Rationale</strong>: Request-response provides immediate feedback for user interactions and error handling, while event-driven enables efficient real-time data processing and decoupling</li>\n<li><strong>Consequences</strong>: Enables both interactive user experience and high-throughput data processing, but requires managing two communication patterns</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Communication Pattern</th>\n<th>Use Cases</th>\n<th>Implementation</th>\n<th>Benefits</th>\n<th>Trade-offs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request-Response</td>\n<td>Dashboard queries, API calls, health checks</td>\n<td>HTTP REST APIs with JSON</td>\n<td>Immediate feedback, error handling, debugging</td>\n<td>Higher latency, tighter coupling</td>\n</tr>\n<tr>\n<td>Event-Driven</td>\n<td>Metric ingestion, alert notifications, real-time updates</td>\n<td>WebSocket connections, internal channels</td>\n<td>Low latency, loose coupling, scalability</td>\n<td>Complex error handling, eventual consistency</td>\n</tr>\n<tr>\n<td>Hybrid</td>\n<td>Complete system operations</td>\n<td>Both patterns appropriately applied</td>\n<td>Best of both worlds</td>\n<td>Increased complexity, multiple failure modes</td>\n</tr>\n</tbody></table>\n<h3 id=\"metric-ingestion-flow\">Metric Ingestion Flow</h3>\n<p>The metric ingestion flow represents the system&#39;s primary data entry pipeline, transforming raw metric data from external sources into properly validated, normalized, and stored time-series data. This flow must handle high throughput, validate data integrity, manage cardinality constraints, and ensure durability guarantees.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fingestion-flow.svg\" alt=\"Metric Ingestion Data Flow\"></p>\n<h4 id=\"push-based-ingestion-process\">Push-Based Ingestion Process</h4>\n<p>The push-based ingestion process begins when external applications send metric data to the system&#39;s ingestion endpoints. This follows a multi-stage pipeline designed to validate, process, and store metrics while maintaining high throughput and reliability.</p>\n<p><strong>Stage 1: HTTP Request Reception and Initial Processing</strong></p>\n<p>The ingestion process starts when the <code>MetricsIngester</code> receives an HTTP POST request containing metric data. The HTTP server performs initial request validation, checking content-type headers, request size limits, and authentication tokens if configured. The request body, typically containing a batch of metrics in JSON format, gets parsed into the internal <code>Metric</code> data structures.</p>\n<p>During this stage, the system performs basic request hygiene checks: ensuring the payload size doesn&#39;t exceed configured limits (typically 1MB to prevent memory exhaustion), validating the content-type is application/json, and verifying any required authentication headers. If these basic checks fail, the system immediately returns an HTTP 400 or 401 response without further processing.</p>\n<p><strong>Stage 2: Metric Validation and Normalization</strong></p>\n<p>Each metric in the batch undergoes comprehensive validation through the <code>Validator</code> component. This validation ensures data quality and prevents common issues that could corrupt the storage engine or cause query failures.</p>\n<p>The validation process examines multiple aspects of each metric:</p>\n<ol>\n<li><p><strong>Metric Name Validation</strong>: Confirms the metric name follows the required naming conventions (alphanumeric characters, underscores, and dots only), doesn&#39;t exceed maximum length limits (typically 256 characters), and doesn&#39;t use reserved prefixes that could conflict with system metrics.</p>\n</li>\n<li><p><strong>Metric Type Consistency</strong>: Verifies that the <code>MetricType</code> field contains a valid enum value (Counter, Gauge, or Histogram) and that the associated sample data is appropriate for the metric type. For example, counter values must be non-negative and monotonically increasing.</p>\n</li>\n<li><p><strong>Label Validation</strong>: Ensures all label keys and values in the <code>Labels</code> map follow naming conventions, don&#39;t exceed length limits, and don&#39;t contain prohibited characters that could interfere with query parsing. The system also validates that reserved label names (like <code>__name__</code> and <code>__timestamp__</code>) are not used.</p>\n</li>\n<li><p><strong>Sample Data Validation</strong>: Checks that timestamp values are reasonable (not too far in the past or future), numeric values are valid (not NaN or infinite), and the sample count is within acceptable limits.</p>\n</li>\n<li><p><strong>Cardinality Validation</strong>: The <code>CardinalityManager</code> evaluates whether accepting this metric would create excessive cardinality by generating too many unique time-series combinations. This prevents &quot;cardinality explosions&quot; that could overwhelm the storage system.</p>\n</li>\n</ol>\n<p>After validation, the <code>NormalizeMetric</code> function standardizes the metric format: converting timestamps to UTC, sorting labels alphabetically for consistent series ID generation, and applying any configured label transformations or defaults.</p>\n<p><strong>Stage 3: Series ID Generation and Indexing</strong></p>\n<p>For each validated metric, the system generates a unique series ID by computing a hash of the metric name and sorted labels. This series ID serves as the primary key for time-series data storage and enables efficient retrieval during queries.</p>\n<p>The series ID generation process combines the metric name and all label key-value pairs into a canonical string representation, then computes a cryptographic hash (typically SHA-256) to create a unique identifier. This approach ensures that metrics with identical names and labels always generate the same series ID, enabling proper time-series continuity.</p>\n<p>The <code>SeriesIndex</code> maintains mapping between series IDs and their metadata, including the metric name, full label set, and references to storage blocks containing the series data. When a new series ID is encountered, the index creates a new <code>SeriesInfo</code> entry; for existing series, it updates the last sample timestamp to track data freshness.</p>\n<p><strong>Stage 4: Write-Ahead Log Persistence</strong></p>\n<p>Before acknowledging the ingestion request, the system writes all validated samples to the <code>WriteAheadLog</code> for durability. This ensures that even if the process crashes before samples reach permanent storage, the data can be recovered and reprocessed during system restart.</p>\n<p>The WAL write process creates a <code>WALRecord</code> containing the timestamp, metric samples, and a checksum for integrity verification. The record gets appended to the current WAL file, and the system calls <code>fsync()</code> to ensure the data reaches persistent storage before proceeding. This durability guarantee is essential for preventing data loss in production environments.</p>\n<p><strong>Stage 5: Storage Engine Integration</strong></p>\n<p>After successful WAL persistence, the samples get forwarded to the <code>StorageEngine</code> through the <code>WriteSamples</code> method. The storage engine handles the complex process of organizing samples into time-based blocks, managing in-memory buffers, and scheduling background compaction operations.</p>\n<p>The storage engine may buffer samples in memory before writing them to disk blocks, depending on the configured flush intervals and memory pressure. However, since samples are already persisted in the WAL, this buffering doesn&#39;t compromise durability - it only affects query latency for the most recent data points.</p>\n<p><strong>Stage 6: Response Generation and Statistics Updates</strong></p>\n<p>Once all samples in the batch have been successfully processed and stored, the ingestion endpoint returns an HTTP 200 response to the client. The response includes statistics about the processing operation: number of metrics accepted, number rejected due to validation failures, and any warnings about potential issues.</p>\n<p>The system updates internal metrics about ingestion performance, including throughput rates, error counts, and processing latencies. These operational metrics help monitor system health and identify performance bottlenecks or data quality issues.</p>\n<h4 id=\"pull-based-scraping-process\">Pull-Based Scraping Process</h4>\n<p>The pull-based scraping process involves the metrics system actively retrieving data from external endpoints that expose metrics in standard formats like Prometheus exposition format. This process runs on configurable intervals and handles endpoint discovery, metric parsing, and error recovery.</p>\n<p><strong>Endpoint Discovery and Scheduling</strong></p>\n<p>The <code>ScrapeEndpoint</code> method manages a registry of target endpoints that should be scraped for metrics. Each target includes the endpoint URL, scraping interval, timeout settings, and any required authentication credentials. The system maintains a scheduler that triggers scrape operations at the appropriate intervals for each target.</p>\n<p>The scraping scheduler uses a priority queue to manage scrape timing, ensuring that endpoints are scraped at their configured intervals while distributing the load evenly over time. If a scrape operation takes longer than expected, the scheduler can detect this and adjust future scheduling to prevent overlapping scrapes that could overwhelm either the metrics system or the target endpoint.</p>\n<p><strong>HTTP Client Operations and Format Parsing</strong></p>\n<p>When scraping an endpoint, the system makes an HTTP GET request with appropriate timeout settings and authentication headers. The response body typically contains metrics in Prometheus exposition format, which uses a text-based representation with metric names, labels, values, and timestamps.</p>\n<p>The parsing process converts the text format into internal <code>Metric</code> data structures, handling the various metric types and label formats supported by the exposition format. This includes parsing counter metrics (with <code>_total</code> suffixes), gauge metrics, histogram metrics (with <code>_bucket</code>, <code>_count</code>, and <code>_sum</code> suffixes), and extracting label values from the metric names and label sets.</p>\n<p><strong>Error Handling and Retry Logic</strong></p>\n<p>Scraping operations can fail due to network issues, endpoint unavailability, or invalid data formats. The system implements exponential backoff retry logic to handle transient failures without overwhelming failing endpoints. For persistent failures, the system logs errors and continues attempting scrapes at reduced frequencies while alerting operators to the issue.</p>\n<h4 id=\"common-ingestion-flow-patterns\">Common Ingestion Flow Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Flow Type</th>\n<th>Trigger</th>\n<th>Processing Steps</th>\n<th>Success Outcome</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Batch Push</td>\n<td>HTTP POST with metric array</td>\n<td>Validate→Normalize→Store→Respond</td>\n<td>200 OK with statistics</td>\n<td>400/500 with error details</td>\n</tr>\n<tr>\n<td>Single Push</td>\n<td>HTTP POST with single metric</td>\n<td>Same as batch but optimized</td>\n<td>200 OK with confirmation</td>\n<td>Detailed validation errors</td>\n</tr>\n<tr>\n<td>Prometheus Scrape</td>\n<td>Timer-triggered pull</td>\n<td>Fetch→Parse→Validate→Store</td>\n<td>Updated series data</td>\n<td>Retry with backoff</td>\n</tr>\n<tr>\n<td>Health Check</td>\n<td>Monitor request</td>\n<td>Quick validation only</td>\n<td>System status</td>\n<td>Degraded/unhealthy status</td>\n</tr>\n</tbody></table>\n<h3 id=\"query-processing-flow\">Query Processing Flow</h3>\n<p>The query processing flow transforms user requests for metric data into efficient storage operations and properly formatted results. This flow must handle query parsing, execution planning, storage retrieval, data aggregation, and result formatting while maintaining good performance and accurate results.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fquery-execution.svg\" alt=\"Query Processing Flow\"></p>\n<h4 id=\"query-request-initiation\">Query Request Initiation</h4>\n<p>Query processing begins when a client submits a query request through either the dashboard API or alerting system. The request contains a query expression string, time range specification, and optional parameters like maximum data points or output format preferences.</p>\n<p>The <code>QueryProcessor</code> receives these requests through its <code>ExecuteQuery</code> method, which serves as the main entry point for all query operations. The processor immediately assigns a unique query ID for tracking and logging purposes, starts timing measurement for performance monitoring, and validates basic request parameters like time range sanity (end time after start time, reasonable date ranges).</p>\n<p><strong>Query Request Validation</strong></p>\n<p>Before parsing the query string, the system performs preliminary validation to catch obvious errors early and prevent resource waste on malformed requests. This validation checks that the time range is reasonable (not requesting data from the year 2050 or from before the system was deployed), the query string is not empty, and any optional parameters are within acceptable ranges.</p>\n<p>For queries originating from the dashboard, additional validation ensures that the requesting user has appropriate permissions to access the requested metrics, particularly if the system implements any access control policies based on metric labels or namespaces.</p>\n<h4 id=\"query-parsing-and-ast-generation\">Query Parsing and AST Generation</h4>\n<p>The query parser transforms the query string into a structured <code>QueryAST</code> that represents the query&#39;s logical structure. This parsing process uses a lexical analyzer (<code>Lexer</code>) to break the query string into tokens, followed by a recursive descent parser that builds the abstract syntax tree.</p>\n<p><strong>Lexical Analysis Phase</strong></p>\n<p>The lexer scans the query string character by character, identifying tokens like metric names, label selectors, function calls, operators, and literal values. Each token gets classified with a <code>TokenType</code> (such as <code>TokenMetricName</code>, <code>TokenLabelName</code>, <code>TokenFunction</code>) and stored with its string value and position in the original query.</p>\n<p>The lexical analysis handles various query syntax elements: quoted strings for label values containing special characters, numeric literals for threshold values and time durations, and reserved keywords for functions and operators. Error handling during lexing provides detailed feedback about syntax issues, including the character position where parsing failed.</p>\n<p><strong>Syntax Tree Construction</strong></p>\n<p>The parser consumes tokens from the lexer and constructs a hierarchical <code>QueryAST</code> representing the query&#39;s logical structure. The tree contains different node types: <code>MetricSelectorNode</code> for basic metric selection with label filters, <code>FunctionCallNode</code> for aggregation and transformation operations, and <code>BinaryOpNode</code> for arithmetic operations between query results.</p>\n<p>Each AST node implements the <code>ASTNode</code> interface with methods for type identification and string representation. This design enables the system to traverse the tree, validate query semantics, and generate execution plans while maintaining clean separation between parsing and execution concerns.</p>\n<p><strong>Semantic Validation</strong></p>\n<p>After syntax parsing, the system performs semantic validation to ensure the query makes logical sense. This validation checks that referenced metric names exist in the system, label names follow proper conventions, function calls use correct parameter types and counts, and time range specifications are consistent with the query structure.</p>\n<p>Semantic validation also detects potential performance issues like queries that would need to scan excessive amounts of data or generate extremely high-cardinality result sets. These checks help prevent queries that could overwhelm the system or consume excessive memory during execution.</p>\n<h4 id=\"query-execution-planning\">Query Execution Planning</h4>\n<p>The query planner analyzes the parsed AST and generates an optimized <code>ExecutionPlan</code> that specifies how the query should be executed against the storage engine. This planning phase is crucial for query performance because it determines the order of operations, identifies optimization opportunities, and estimates resource requirements.</p>\n<p><strong>Series Selection Optimization</strong></p>\n<p>The planner examines <code>MetricSelectorNode</code> elements in the AST to identify which time series need to be retrieved from storage. It analyzes label matchers to estimate the number of matching series and determines the most efficient retrieval strategy based on available indexes and label cardinality.</p>\n<p>For queries with multiple label filters, the planner determines the optimal order for applying filters: starting with the most selective filters to minimize the amount of data that needs to be processed by subsequent filters. This optimization can dramatically improve query performance when dealing with high-cardinality metrics.</p>\n<p><strong>Aggregation Strategy Selection</strong></p>\n<p>When the query includes aggregation functions (like sum, average, or percentile calculations), the planner determines whether aggregations can be performed incrementally as data is retrieved (streaming aggregation) or require buffering entire result sets in memory. This decision depends on the specific function requirements and available system memory.</p>\n<p>The planner also identifies opportunities for query result caching, particularly for expensive aggregations over static time ranges that are unlikely to change. Cached results can be reused for identical queries or combined with incremental calculations for queries that extend previous time ranges.</p>\n<p><strong>Parallel Execution Planning</strong></p>\n<p>For queries that need to process large amounts of data, the planner can divide the work into parallel execution phases. This might involve processing different time ranges concurrently, distributing aggregation calculations across multiple worker threads, or parallelizing series retrieval operations.</p>\n<p>The parallelization strategy considers system resources, storage architecture, and query characteristics to balance performance improvements against coordination overhead. Simple queries may execute faster with single-threaded processing, while complex aggregations over large datasets benefit significantly from parallel execution.</p>\n<h4 id=\"storage-interaction-and-data-retrieval\">Storage Interaction and Data Retrieval</h4>\n<p>The execution engine interacts with the <code>StorageEngine</code> through well-defined interfaces to retrieve the time-series data needed for query execution. This interaction must efficiently handle different query patterns while minimizing storage system load and memory usage.</p>\n<p><strong>Series Discovery and Selection</strong></p>\n<p>Using the label matchers from the execution plan, the query engine calls <code>QuerySeries</code> to identify all time series that match the query criteria. This operation returns <code>SeriesInfo</code> objects containing series metadata but not the actual sample data, allowing the system to understand the scope of data retrieval before loading samples into memory.</p>\n<p>The series selection process leverages the <code>SeriesIndex</code> to quickly identify matching series without scanning raw storage blocks. For queries with multiple label filters, the selection process can use index intersection operations to efficiently find series that match all criteria simultaneously.</p>\n<p><strong>Sample Data Retrieval</strong></p>\n<p>For each selected series, the query engine calls <code>QueryRange</code> to retrieve sample data within the specified time range. The storage engine returns arrays of <code>Sample</code> objects containing timestamps and values, which the query engine then processes according to the execution plan.</p>\n<p>To manage memory usage and processing efficiency, the query engine may retrieve sample data in batches rather than loading entire time ranges into memory simultaneously. This streaming approach enables processing of very large result sets without exhausting system memory.</p>\n<p><strong>Data Alignment and Interpolation</strong></p>\n<p>When processing multiple time series, the query engine often needs to align samples from different series to common timestamps for aggregation or arithmetic operations. This alignment process may involve interpolating missing values or selecting the most recent value before each timestamp.</p>\n<p>The alignment strategy depends on the metric types and query requirements: counter metrics typically use rate calculations that account for counter resets, gauge metrics may use linear interpolation for missing values, and histogram metrics require special handling for bucket boundaries and statistical calculations.</p>\n<h4 id=\"aggregation-and-result-processing\">Aggregation and Result Processing</h4>\n<p>After retrieving raw sample data from storage, the query engine applies aggregation functions and transformations specified in the query to compute final results. This processing phase must handle various aggregation types while maintaining numerical accuracy and handling edge cases like missing data.</p>\n<p><strong>Statistical Aggregation Functions</strong></p>\n<p>The system implements various <code>AggregationFunction</code> types for different statistical operations: sum and average for combining multiple series, min and max for finding extreme values, and percentile calculations for understanding data distributions. Each function handles streaming aggregation where possible to minimize memory usage.</p>\n<p>For histogram metrics, the system provides specialized aggregation functions that properly combine histogram buckets from multiple series, calculate quantile estimates across merged histograms, and maintain statistical accuracy even when dealing with different bucket boundaries across series.</p>\n<p><strong>Time-Based Processing</strong></p>\n<p>Many queries require processing data over time windows, such as calculating rates of change for counter metrics or computing moving averages for gauge metrics. The query engine implements these time-based operations by maintaining sliding windows of sample data and updating calculations as new samples are processed.</p>\n<p>Rate calculations for counter metrics require special handling for counter resets, where the counter value decreases (typically due to process restarts). The system detects these resets and adjusts rate calculations to maintain accuracy across reset boundaries.</p>\n<p><strong>Result Formatting and Output</strong></p>\n<p>The final query processing step formats the computed results according to the client&#39;s requirements. Dashboard queries typically receive results as time-series arrays with timestamp-value pairs, while alerting queries may only need scalar values or boolean results indicating threshold violations.</p>\n<p>The result formatting process includes unit conversion (if requested), timestamp formatting for the client&#39;s time zone preferences, and data point sampling or aggregation if the client requested a maximum number of data points. This final processing ensures that results are immediately usable by the requesting component without additional transformation.</p>\n<h4 id=\"query-processing-flow-patterns\">Query Processing Flow Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Query Type</th>\n<th>Parsing Complexity</th>\n<th>Storage Operations</th>\n<th>Aggregation Needs</th>\n<th>Result Format</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Series</td>\n<td>Low (single selector)</td>\n<td>Single series lookup</td>\n<td>None</td>\n<td>Raw time-series</td>\n</tr>\n<tr>\n<td>Multi-Series Sum</td>\n<td>Medium (aggregation function)</td>\n<td>Multiple series retrieval</td>\n<td>Sum across series</td>\n<td>Aggregated time-series</td>\n</tr>\n<tr>\n<td>Rate Calculation</td>\n<td>Medium (rate function)</td>\n<td>Series with counter handling</td>\n<td>Rate computation</td>\n<td>Derived time-series</td>\n</tr>\n<tr>\n<td>Complex Dashboard</td>\n<td>High (multiple subqueries)</td>\n<td>Optimized batch retrieval</td>\n<td>Mixed aggregations</td>\n<td>Multiple result sets</td>\n</tr>\n<tr>\n<td>Alert Evaluation</td>\n<td>Low (threshold comparison)</td>\n<td>Recent data only</td>\n<td>Threshold comparison</td>\n<td>Boolean or scalar</td>\n</tr>\n</tbody></table>\n<h3 id=\"alert-evaluation-flow\">Alert Evaluation Flow</h3>\n<p>The alert evaluation flow continuously monitors metric data against defined alert rules, manages alert state transitions, and triggers appropriate notifications when conditions change. This flow must provide reliable monitoring, minimize false positives, and ensure timely notification delivery.</p>\n<p><img src=\"/api/project/metrics-dashboard/architecture-doc/asset?path=diagrams%2Fnotification-flow.svg\" alt=\"Alert Notification Flow\"></p>\n<h4 id=\"alert-rule-evaluation-cycle\">Alert Rule Evaluation Cycle</h4>\n<p>The alert evaluation process runs on a regular schedule, typically every 15-60 seconds, to check all active alert rules against current metric data. The <code>AlertEvaluator</code> coordinates this process, managing the evaluation schedule, tracking alert states, and triggering notifications when state changes occur.</p>\n<p><strong>Evaluation Scheduling and Coordination</strong></p>\n<p>The <code>AlertEvaluator</code> maintains a scheduler that triggers evaluation cycles at the configured <code>EvaluationInterval</code>. Each evaluation cycle processes all active <code>AlertRule</code> definitions, executing their query expressions against the current metric data and comparing results to defined thresholds.</p>\n<p>The scheduling system uses a ticker mechanism to ensure consistent evaluation intervals, but includes logic to handle cases where evaluations take longer than the scheduled interval. If an evaluation cycle is still running when the next cycle should start, the system can either wait for completion or run evaluations in parallel, depending on system resources and configuration.</p>\n<p><strong>Rule Query Execution</strong></p>\n<p>For each <code>AlertRule</code>, the evaluator executes the rule&#39;s query expression using the same <code>QueryProcessor</code> used by the dashboard system. This ensures consistency between what users see in dashboards and what the alerting system monitors. The query typically retrieves recent data (last few minutes) to determine current metric values.</p>\n<p>Alert rule queries often include aggregation functions to reduce multiple time series to scalar values that can be compared against thresholds. For example, a rule monitoring average CPU usage might aggregate samples from multiple servers, while a rule monitoring error rates might calculate the ratio of error count to total request count.</p>\n<p><strong>Threshold Comparison and Condition Evaluation</strong></p>\n<p>After executing the rule query, the evaluator compares the resulting values against the rule&#39;s threshold using the specified comparison operator (greater than, less than, equals, etc.). This comparison determines whether the alert condition is currently met for each time series that matches the rule&#39;s selector.</p>\n<p>For rules that monitor multiple time series (like per-server metrics), the evaluator creates separate <code>AlertInstance</code> objects for each series that violates the threshold. Each instance tracks its own state and history independently, allowing fine-grained alerting on individual series while maintaining overall rule coherence.</p>\n<p><strong>Duration-Based Alert Logic</strong></p>\n<p>Many alert rules include a duration requirement: the condition must persist for a specified time before the alert fires. This prevents spurious alerts from brief metric spikes or temporary network issues. The evaluator tracks how long each condition has been true and only transitions alerts to firing state after the duration threshold is met.</p>\n<p>The duration tracking uses the alert instance&#39;s state history to determine when the condition first became true. If the condition stops being met before the duration expires, the instance returns to inactive state and the duration timer resets. This ensures that only sustained threshold violations trigger alerts.</p>\n<h4 id=\"alert-state-management\">Alert State Management</h4>\n<p>The <code>StateManager</code> component maintains the lifecycle of all alert instances, tracking state transitions, managing persistence, and ensuring proper notification behavior. Alert states follow a well-defined state machine that prevents inconsistent behavior and ensures reliable alerting.</p>\n<p><strong>Alert State Machine Implementation</strong></p>\n<p>Each <code>AlertInstance</code> progresses through defined states based on evaluation results and time constraints:</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Condition Met</th>\n<th>Duration Exceeded</th>\n<th>Next State</th>\n<th>Actions Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AlertStateInactive</code></td>\n<td>No</td>\n<td>N/A</td>\n<td><code>AlertStateInactive</code></td>\n<td>No action</td>\n</tr>\n<tr>\n<td><code>AlertStateInactive</code></td>\n<td>Yes</td>\n<td>No</td>\n<td><code>AlertStatePending</code></td>\n<td>Start duration timer</td>\n</tr>\n<tr>\n<td><code>AlertStatePending</code></td>\n<td>Yes</td>\n<td>Yes</td>\n<td><code>AlertStateFiring</code></td>\n<td>Send firing notification</td>\n</tr>\n<tr>\n<td><code>AlertStatePending</code></td>\n<td>No</td>\n<td>N/A</td>\n<td><code>AlertStateInactive</code></td>\n<td>Cancel duration timer</td>\n</tr>\n<tr>\n<td><code>AlertStateFiring</code></td>\n<td>Yes</td>\n<td>N/A</td>\n<td><code>AlertStateFiring</code></td>\n<td>No notification (already firing)</td>\n</tr>\n<tr>\n<td><code>AlertStateFiring</code></td>\n<td>No</td>\n<td>N/A</td>\n<td><code>AlertStateResolved</code></td>\n<td>Send resolved notification</td>\n</tr>\n<tr>\n<td><code>AlertStateResolved</code></td>\n<td>No</td>\n<td>N/A</td>\n<td><code>AlertStateInactive</code></td>\n<td>Cleanup after grace period</td>\n</tr>\n<tr>\n<td>Any</td>\n<td>Silenced</td>\n<td>N/A</td>\n<td><code>AlertStateSilenced</code></td>\n<td>Suppress notifications</td>\n</tr>\n</tbody></table>\n<p><strong>State Persistence and Recovery</strong></p>\n<p>The <code>StateManager</code> persists alert state information to survive system restarts and ensure continuity of alert monitoring. This persistence includes current state, state change timestamps, condition values, and notification history for each alert instance.</p>\n<p>During system startup, the state manager loads persisted alert states and reconciles them with current rule definitions. If rules have been modified or deleted, the system handles orphaned alert instances appropriately, either updating them to match new rule definitions or marking them for cleanup.</p>\n<p><strong>State Change Notification Triggering</strong></p>\n<p>When an alert instance transitions to a new state that requires notification (<code>AlertStateFiring</code> or <code>AlertStateResolved</code>), the state manager creates <code>NotificationRequest</code> objects and queues them for delivery through the <code>NotificationManager</code>. These requests include all necessary information for generating and sending appropriate notifications.</p>\n<p>The state manager also maintains statistics about alert behavior, tracking metrics like time spent in each state, frequency of state changes, and notification success rates. This information helps operators understand alert behavior patterns and identify rules that may need tuning.</p>\n<h4 id=\"notification-processing-and-delivery\">Notification Processing and Delivery</h4>\n<p>The notification system handles the complex process of generating appropriate messages for different channels, managing delivery queues, handling failures, and ensuring reliable notification delivery even under adverse conditions.</p>\n<p><strong>Notification Channel Selection and Routing</strong></p>\n<p>When an alert state change triggers a notification, the system determines which notification channels should receive the alert based on rule configuration, channel filters, and current system state. Different alert rules may route to different channels based on severity, affected services, or time of day.</p>\n<p>The routing logic evaluates each configured <code>NotificationChannel</code> against the alert instance to determine if the channel should receive the notification. This evaluation considers channel filters (which may match alert labels), channel availability status, and any active silencing rules that might suppress notifications for specific channels.</p>\n<p><strong>Message Templating and Formatting</strong></p>\n<p>For each selected notification channel, the system generates appropriate message content using configured message templates. The templating system has access to all alert instance data, including current values, threshold settings, alert history, and custom annotations from the alert rule definition.</p>\n<p>Different channels require different message formats: email notifications might include detailed formatting with tables and charts, Slack notifications use rich message formatting with action buttons, while SMS notifications need concise text that fits within message length limits. The templating system handles these variations while maintaining consistent information content.</p>\n<p><strong>Delivery Queue Management and Retry Logic</strong></p>\n<p>Notification delivery uses a queue-based system that can handle high notification volumes and temporary delivery failures without blocking alert evaluation or losing notifications. The <code>NotificationManager</code> maintains separate delivery queues for different channel types, allowing independent processing and retry logic.</p>\n<p>When notification delivery fails (due to network issues, service outages, or configuration problems), the system implements exponential backoff retry logic with maximum retry limits. Failed notifications are logged with detailed error information to help operators diagnose and resolve delivery issues.</p>\n<p><strong>Rate Limiting and Notification Throttling</strong></p>\n<p>To prevent notification storms that could overwhelm receiving systems or alert recipients, the notification system implements rate limiting based on channel configuration and alert frequency. This includes burst limiting (maximum notifications per minute) and sustained rate limiting (maximum notifications per hour).</p>\n<p>The rate limiting system can also implement intelligent throttling that reduces notification frequency for frequently-flapping alerts while maintaining normal delivery for stable alerts. This helps reduce alert fatigue while ensuring that genuine issues receive appropriate attention.</p>\n<h4 id=\"alert-evaluation-flow-patterns\">Alert Evaluation Flow Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Evaluation Type</th>\n<th>Trigger</th>\n<th>Query Complexity</th>\n<th>State Changes</th>\n<th>Notification Volume</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Simple Threshold</td>\n<td>Scheduled timer</td>\n<td>Single metric comparison</td>\n<td>Infrequent (stable conditions)</td>\n<td>Low (binary firing/resolved)</td>\n</tr>\n<tr>\n<td>Multi-Series Rule</td>\n<td>Scheduled timer</td>\n<td>Aggregated query</td>\n<td>Variable (per-series instances)</td>\n<td>Medium (multiple instances)</td>\n</tr>\n<tr>\n<td>Flapping Alert</td>\n<td>Scheduled timer</td>\n<td>Same query repeatedly</td>\n<td>Frequent (unstable conditions)</td>\n<td>High (requires throttling)</td>\n</tr>\n<tr>\n<td>Composite Condition</td>\n<td>Scheduled timer</td>\n<td>Multiple subqueries</td>\n<td>Complex (dependent conditions)</td>\n<td>Variable (conditional logic)</td>\n</tr>\n</tbody></table>\n<h4 id=\"integration-with-dashboard-and-query-systems\">Integration with Dashboard and Query Systems</h4>\n<p>The alert evaluation system leverages the same query processing infrastructure used by the dashboard system, ensuring consistency between what users observe in visualizations and what triggers alerts. This shared infrastructure reduces system complexity and maintains behavioral consistency.</p>\n<p><strong>Shared Query Processing Pipeline</strong></p>\n<p>Alert rule evaluation uses the same <code>QueryProcessor</code>, query parsing logic, and storage interfaces as dashboard queries. This means that alert expressions can use the same syntax and functions available in dashboard queries, and operators can test alert conditions by running equivalent queries in the dashboard interface.</p>\n<p>The shared pipeline also ensures that optimizations and bug fixes in the query processing system automatically benefit both dashboard and alerting functionality. This reduces maintenance burden and improves overall system reliability.</p>\n<p><strong>Real-Time Data Consistency</strong></p>\n<p>Since both dashboard and alerting systems query the same underlying storage, users can observe the same metric values that drive alert decisions. This consistency is crucial for debugging alert behavior and understanding why alerts fire or resolve at specific times.</p>\n<p>The system maintains this consistency even during high-load periods by using the same data retrieval and aggregation logic for both use cases. However, alert queries typically focus on recent data and use simpler aggregations to minimize evaluation latency and resource usage.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical implementation advice for building the component interaction and data flow systems described above. The code examples and patterns shown here will help translate the design concepts into working software.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Server</td>\n<td>Go net/http with gorilla/mux</td>\n<td>Go fiber or gin frameworks</td>\n</tr>\n<tr>\n<td>Message Serialization</td>\n<td>JSON with encoding/json</td>\n<td>Protocol Buffers with protobuf</td>\n</tr>\n<tr>\n<td>Concurrency Control</td>\n<td>Go channels and sync package</td>\n<td>WorkerPool patterns with semaphores</td>\n</tr>\n<tr>\n<td>Logging</td>\n<td>Go slog package</td>\n<td>Structured logging with zerolog</td>\n</tr>\n<tr>\n<td>Metrics Instrumentation</td>\n<td>Prometheus client library</td>\n<td>Custom metrics with OpenTelemetry</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>YAML files with gopkg.in/yaml</td>\n<td>Consul/etcd with dynamic updates</td>\n</tr>\n</tbody></table>\n<h4 id=\"project-structure\">Project Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n├── cmd/\n│   ├── metrics-server/\n│   │   └── main.go                    ← Main server entry point\n│   └── metrics-cli/\n│       └── main.go                    ← CLI tools for testing\n├── internal/\n│   ├── coordinator/\n│   │   ├── coordinator.go             ← ComponentCoordinator implementation\n│   │   ├── component.go               ← Component interface and base types\n│   │   └── coordinator_test.go\n│   ├── ingestion/\n│   │   ├── ingester.go                ← MetricsIngester implementation\n│   │   ├── validator.go               ← Validation and normalization\n│   │   ├── cardinality.go             ← CardinalityManager implementation\n│   │   └── handlers.go                ← HTTP handlers for ingestion\n│   ├── storage/\n│   │   ├── engine.go                  ← StorageEngine implementation\n│   │   ├── series.go                  ← SeriesIndex and SeriesInfo\n│   │   ├── wal.go                     ← WriteAheadLog implementation\n│   │   └── blocks.go                  ← Block storage management\n│   ├── query/\n│   │   ├── processor.go               ← QueryProcessor implementation\n│   │   ├── parser.go                  ← Query parsing and AST\n│   │   ├── executor.go                ← Query execution engine\n│   │   └── cache.go                   ← QueryCache implementation\n│   ├── dashboard/\n│   │   ├── server.go                  ← DashboardServer implementation\n│   │   ├── websocket.go               ← WebSocket client management\n│   │   └── handlers.go                ← HTTP handlers for dashboard API\n│   ├── alerting/\n│   │   ├── evaluator.go               ← AlertEvaluator implementation\n│   │   ├── state.go                   ← StateManager implementation\n│   │   ├── notifications.go           ← NotificationManager implementation\n│   │   └── channels/                  ← Channel-specific implementations\n│   └── common/\n│       ├── types.go                   ← Shared type definitions\n│       ├── config.go                  ← Configuration structures\n│       └── health.go                  ← HealthManager implementation\n├── pkg/\n│   └── client/                        ← Client library for external use\n└── scripts/\n    ├── docker-compose.yml             ← Development environment\n    └── test-data/                     ← Sample metrics for testing</code></pre></div>\n\n<h4 id=\"core-component-coordination-infrastructure\">Core Component Coordination Infrastructure</h4>\n<p>The following code provides the foundation for component lifecycle management and coordination:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/coordinator/component.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Component represents a system component with lifecycle management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Component</span><span style=\"color:#F97583\"> interface</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start initializes and starts the component</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Stop gracefully shuts down the component</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Stop</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Name returns the component identifier</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    Name</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // HealthCheck returns the current component health status</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    HealthCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ComponentCoordinator manages the lifecycle of all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ComponentCoordinator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    health     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    components </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Component</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    started    </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh     </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wg         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewCoordinator creates a new component coordinator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewCoordinator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">config</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">Config</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        config:     config,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:     logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        health:     </span><span style=\"color:#B392F0\">NewHealthManager</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        components: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">Component</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stopCh:     </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterComponent adds a component to the coordinator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterComponent</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">component</span><span style=\"color:#B392F0\"> Component</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> c.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.started {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"cannot register component </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">: coordinator already started\"</span><span style=\"color:#E1E4E8\">, component.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _, exists </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> c.components[component.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">()]; exists {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"component </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> already registered\"</span><span style=\"color:#E1E4E8\">, component.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.components[component.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">()] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.logger.</span><span style=\"color:#B392F0\">Info</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"registered component\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">, component.</span><span style=\"color:#B392F0\">Name</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start initializes and starts all registered components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> c.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> c.started {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"coordinator already started\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start each component in dependency order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Register health checks for each component</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Start background health monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Set up graceful shutdown signal handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Mark coordinator as started</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Stop gracefully shuts down all components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">c </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ComponentCoordinator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    c.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> c.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">c.started {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Signal all components to stop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Wait for components to finish with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Force stop any components that don't respond</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Clean up resources and close channels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"ingestion-flow-infrastructure\">Ingestion Flow Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/ingestion/ingester.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> ingestion</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricsIngester handles incoming metric data with validation and storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricsIngester</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage      </span><span style=\"color:#B392F0\">TimeSeriesStorage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validator    </span><span style=\"color:#B392F0\">Validator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cardinality  </span><span style=\"color:#B392F0\">CardinalityManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stats        </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IngestionStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxBatchSize </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout      </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMetricsIngester creates a new metrics ingester</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMetricsIngester</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> TimeSeriesStorage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">validator</span><span style=\"color:#B392F0\"> Validator</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cardinality</span><span style=\"color:#B392F0\"> CardinalityManager</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsIngester</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MetricsIngester</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:      storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        validator:    validator,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cardinality:  cardinality,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:       logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stats:        </span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#B392F0\">IngestionStats</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxBatchSize: </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        timeout:      </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IngestMetrics processes a batch of metrics through the complete ingestion pipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsIngester</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IngestMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        m.logger.</span><span style=\"color:#B392F0\">Debug</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ingestion completed\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"duration\"</span><span style=\"color:#E1E4E8\">, time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(startTime),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"metric_count\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(metrics))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate batch size and basic request parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each metric in the batch:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Validate metric structure and values using m.validator.ValidateMetric()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Check cardinality limits using m.cardinality.CheckCardinality()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. Normalize metric format using m.validator.NormalizeMetric()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   d. Generate series ID and update series index</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Write all validated samples to storage using m.storage.WriteSamples()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update ingestion statistics (success/failure counts)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return any validation errors or storage failures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ScrapeEndpoint retrieves metrics from a Prometheus-compatible endpoint</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsIngester</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ScrapeEndpoint</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">url</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Make HTTP GET request to the endpoint with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse Prometheus exposition format from response body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Convert parsed metrics to internal Metric structures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call IngestMetrics() with the converted metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle scraping errors and update endpoint statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HTTP handler for push-based ingestion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricsIngester</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">HandlePushMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Validate HTTP method and content-type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Read and parse JSON request body into Metric slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Call IngestMetrics() with parsed metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return appropriate HTTP status and response body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log request details and performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"query-processing-infrastructure\">Query Processing Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/query/processor.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> query</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryProcessor executes metric queries and returns formatted results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryProcessor</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage   </span><span style=\"color:#B392F0\">TimeSeriesStorage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryParser</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache     </span><span style=\"color:#B392F0\">QueryCache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Execution limits</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSeries    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxSamples   </span><span style=\"color:#F97583\">int64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryTimeout </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewQueryProcessor creates a new query processor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewQueryProcessor</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">storage</span><span style=\"color:#B392F0\"> TimeSeriesStorage</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryProcessor</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">QueryProcessor</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        storage:      storage,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parser:       </span><span style=\"color:#B392F0\">NewQueryParser</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cache:        </span><span style=\"color:#B392F0\">NewQueryCache</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Minute),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:       logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxSeries:    </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        maxSamples:   </span><span style=\"color:#79B8FF\">1000000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryTimeout: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ExecuteQuery processes a query string and returns formatted results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">q </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryProcessor</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ExecuteQuery</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> TimeRange</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        q.logger.</span><span style=\"color:#B392F0\">Debug</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"query executed\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"duration\"</span><span style=\"color:#E1E4E8\">, time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(startTime),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"query\"</span><span style=\"color:#E1E4E8\">, queryString)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check query cache for existing results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse query string into AST using q.parser.Parse()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate query semantics and estimate resource requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Generate optimized execution plan using PlanQuery()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Execute plan against storage:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Retrieve matching series using storage.QuerySeries()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Fetch sample data using storage.QueryRange()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. Apply aggregation functions and transformations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Format results according to client requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Update query cache with results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Return QueryResult with data and execution statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"alert-evaluation-infrastructure\">Alert Evaluation Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/alerting/evaluator.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> alerting</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// AlertEvaluator continuously evaluates alert rules and manages alert states</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AlertEvaluator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryProcessor    </span><span style=\"color:#B392F0\">QueryProcessor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stateManager     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StateManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ruleManager      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">RuleManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    notificationMgr  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NotificationManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger           </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Evaluation control</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    evaluationInterval </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh            </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    wg                </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">WaitGroup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    running           </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu                </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewAlertEvaluator creates a new alert evaluator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewAlertEvaluator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">queryProcessor</span><span style=\"color:#B392F0\"> QueryProcessor</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">stateManager</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">StateManager</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">notificationMgr</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">NotificationManager</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        queryProcessor:     queryProcessor,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stateManager:      stateManager,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        notificationMgr:   notificationMgr,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger:            logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        evaluationInterval: </span><span style=\"color:#79B8FF\">15</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stopCh:            </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Start begins the alert evaluation loop</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Start</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> a.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> a.running {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"alert evaluator already running\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start the evaluation ticker</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Launch background goroutine for evaluation loop</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Set up context cancellation handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Mark evaluator as running</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    a.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> true</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// EvaluateRules processes all active alert rules against current metric data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">EvaluateRules</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get all active alert rules from rule manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each rule:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   a. Execute rule query using query processor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   b. Compare results against rule threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   c. Update alert instance state using state manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    //   d. Trigger notifications if state changed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Clean up resolved alerts older than retention period</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update evaluation statistics and health metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// UpdateAlertState manages alert state transitions and notifications</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AlertEvaluator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">UpdateAlertState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">ruleID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesLabels</span><span style=\"color:#B392F0\"> Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">currentValue</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">threshold</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">conditionMet</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">rule</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">AlertRule</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Get current alert instance from state manager</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Determine next state based on current state and condition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check duration requirements for state transitions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update alert instance with new state and value</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If state changed to firing or resolved, queue notifications</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Implementing Component Coordination (Foundation)</strong></p>\n<ul>\n<li>Run: <code>go test ./internal/coordinator/...</code></li>\n<li>Expected: All tests pass, components start/stop cleanly</li>\n<li>Manual test: Start server, check health endpoint returns 200 OK</li>\n<li>Debug: Check logs for component registration and startup sequence</li>\n</ul>\n<p><strong>After Implementing Ingestion Flow (Milestone 1)</strong></p>\n<ul>\n<li>Run: <code>curl -X POST http://localhost:8080/api/v1/metrics -d &#39;[{&quot;name&quot;:&quot;test_counter&quot;,&quot;type&quot;:0,&quot;labels&quot;:{&quot;job&quot;:&quot;test&quot;},&quot;samples&quot;:[{&quot;value&quot;:42,&quot;timestamp&quot;:&quot;2024-01-01T12:00:00Z&quot;}]}]&#39;</code></li>\n<li>Expected: 200 OK response with ingestion statistics</li>\n<li>Manual test: Send invalid metrics, verify proper error responses</li>\n<li>Debug: Check ingestion logs and storage files for metric data</li>\n</ul>\n<p><strong>After Implementing Query Processing (Milestone 2)</strong></p>\n<ul>\n<li>Run: <code>curl &quot;http://localhost:8080/api/v1/query?query=test_counter&amp;start=2024-01-01T11:00:00Z&amp;end=2024-01-01T13:00:00Z&quot;</code></li>\n<li>Expected: JSON response with time-series data</li>\n<li>Manual test: Query non-existent metrics, verify empty results</li>\n<li>Debug: Check query logs and execution time statistics</li>\n</ul>\n<p><strong>After Implementing Alert Evaluation (Milestone 4)</strong></p>\n<ul>\n<li>Configure alert rule with low threshold, send metrics that exceed it</li>\n<li>Expected: Alert transitions to firing state, notification sent</li>\n<li>Manual test: Verify alert resolves when condition clears</li>\n<li>Debug: Check alert evaluation logs and state persistence</li>\n</ul>\n<h4 id=\"common-debugging-patterns\">Common Debugging Patterns</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metrics not appearing in queries</td>\n<td>Ingestion validation failure</td>\n<td>Check ingestion endpoint logs, verify metric format</td>\n<td>Fix metric names/labels, check cardinality limits</td>\n</tr>\n<tr>\n<td>Slow query performance</td>\n<td>Inefficient series selection</td>\n<td>Enable query execution logging, check series count</td>\n<td>Add label indexes, optimize query selectors</td>\n</tr>\n<tr>\n<td>Alerts not firing</td>\n<td>Query execution errors</td>\n<td>Check alert evaluation logs, test queries manually</td>\n<td>Fix alert rule expressions, verify metric availability</td>\n</tr>\n<tr>\n<td>WebSocket disconnections</td>\n<td>Client timeout or server overload</td>\n<td>Monitor connection counts, check message queue sizes</td>\n<td>Tune timeout settings, implement backpressure</td>\n</tr>\n<tr>\n<td>Memory usage growth</td>\n<td>Query result caching or buffer leaks</td>\n<td>Profile memory usage, check cache sizes</td>\n<td>Implement cache eviction, fix buffer cleanup</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Blocking Operations in Component Startup</strong>\nComponents that perform blocking I/O during startup (like connecting to external services) can prevent the entire system from starting. Always use timeouts and context cancellation for startup operations. If a component fails to start, the coordinator should continue with other components rather than failing completely.</p>\n<p>⚠️ <strong>Pitfall: Missing Error Context in Data Flows</strong>\nWhen errors occur in the middle of data processing pipelines, insufficient context makes debugging nearly impossible. Always include relevant identifiers (series ID, query ID, alert rule ID) in error messages and logs. Use structured logging to capture processing context.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Time Handling</strong>\nDifferent components using different time zones or timestamp formats can cause data correlation issues. Always use UTC internally and only convert to local time zones for user display. Include timezone information in all external APIs.</p>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (comprehensive error handling is critical throughout the entire metrics and alerting system, from ingestion through visualization to notification delivery)</p>\n</blockquote>\n<h3 id=\"the-defense-in-depth-metaphor-building-resilient-systems\">The Defense-in-Depth Metaphor: Building Resilient Systems</h3>\n<p>Think of error handling in a metrics system like a medieval castle&#39;s defense system. A well-designed castle doesn&#39;t rely on a single wall to protect against attack—it employs multiple layers of defense working together. The outer moat catches obvious threats, the walls deflect serious attacks, the inner courtyard provides fallback positions, and the keep serves as the final refuge. Each layer has different responsibilities and failure modes, but together they create a robust defense against various types of attacks.</p>\n<p>Similarly, our metrics and alerting system employs defense-in-depth error handling. Input validation acts as the outer moat, catching malformed data before it enters the system. Rate limiting and backpressure serve as the castle walls, protecting against overwhelming traffic. Graceful degradation provides fallback positions when components fail, and persistent state recovery acts as the keep—ensuring the system can rebuild itself even after catastrophic failure.</p>\n<p>The key insight is that errors are not exceptional events to be avoided, but predictable phenomena to be managed systematically. Just as castle architects studied siege warfare to design appropriate defenses, we must study the failure modes of distributed systems to build appropriate error handling mechanisms.</p>\n<blockquote>\n<p><strong>Decision: Comprehensive Error Taxonomy</strong></p>\n<ul>\n<li><strong>Context</strong>: Metrics systems face numerous failure modes across ingestion, storage, querying, and alerting</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Handle errors locally in each component</li>\n<li>Central error handling service</li>\n<li>Layered error handling with component-specific recovery</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Layered error handling with component-specific recovery strategies</li>\n<li><strong>Rationale</strong>: Each component has unique failure modes and recovery requirements that demand specialized handling</li>\n<li><strong>Consequences</strong>: More complex implementation but significantly better system resilience and observability</li>\n</ul>\n</blockquote>\n<h3 id=\"error-classification-framework\">Error Classification Framework</h3>\n<p>Our error handling strategy categorizes failures into distinct classes, each requiring different detection, reporting, and recovery mechanisms. This classification helps us build appropriate responses rather than treating all errors the same way.</p>\n<table>\n<thead>\n<tr>\n<th>Error Class</th>\n<th>Characteristics</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>User Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transient Network</td>\n<td>Temporary connectivity issues, timeouts</td>\n<td>Request timeout, connection refused</td>\n<td>Retry with exponential backoff</td>\n<td>Brief delays in data availability</td>\n</tr>\n<tr>\n<td>Resource Exhaustion</td>\n<td>Memory, disk, file handles depleted</td>\n<td>System monitoring, allocation failures</td>\n<td>Backpressure, graceful degradation</td>\n<td>Reduced throughput, some data loss</td>\n</tr>\n<tr>\n<td>Data Corruption</td>\n<td>Invalid data format, checksum mismatches</td>\n<td>Validation failures, parsing errors</td>\n<td>Skip corrupt data, request resend</td>\n<td>Missing data points in time series</td>\n</tr>\n<tr>\n<td>Configuration Errors</td>\n<td>Invalid settings, missing required fields</td>\n<td>Startup validation, runtime checks</td>\n<td>Fail-fast with clear error messages</td>\n<td>Service unavailable until fixed</td>\n</tr>\n<tr>\n<td>Component Failures</td>\n<td>Process crashes, hardware failures</td>\n<td>Health checks, heartbeat monitoring</td>\n<td>Restart, failover to backup</td>\n<td>Service interruption, potential data loss</td>\n</tr>\n<tr>\n<td>Cascading Failures</td>\n<td>One failure triggers others</td>\n<td>Dependency monitoring, circuit breakers</td>\n<td>Isolate failures, prevent propagation</td>\n<td>System-wide degradation</td>\n</tr>\n</tbody></table>\n<p>The critical principle here is that different error classes require fundamentally different handling strategies. Retrying a configuration error will never succeed, while giving up immediately on a transient network error wastes an opportunity for automatic recovery.</p>\n<h3 id=\"ingestion-error-handling\">Ingestion Error Handling</h3>\n<p>The metrics ingestion engine faces several distinct failure modes that require specialized handling approaches. The ingestion pipeline must balance data durability with system availability, making intelligent decisions about when to accept, reject, or defer incoming metrics.</p>\n<h4 id=\"mental-model-the-quality-control-checkpoint\">Mental Model: The Quality Control Checkpoint</h4>\n<p>Think of metrics ingestion error handling like a quality control checkpoint at a manufacturing plant. Raw materials (metrics) arrive continuously from suppliers (client applications), and the checkpoint must quickly assess each batch for quality and safety. Defective materials are rejected with clear feedback to the supplier. When the production line becomes overwhelmed, the checkpoint implements controlled throttling rather than shutting down completely. Critical defects that could damage downstream equipment trigger immediate safety protocols.</p>\n<p>This quality control metaphor captures the key insight that ingestion error handling is about maintaining system integrity while providing useful feedback to metric producers.</p>\n<h4 id=\"invalid-metrics-processing\">Invalid Metrics Processing</h4>\n<p>The ingestion engine encounters various forms of invalid metric data that must be handled without disrupting overall system operation. Each validation failure type requires specific handling to provide meaningful feedback while protecting the system from malformed data.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Failure</th>\n<th>Detection Method</th>\n<th>Error Response</th>\n<th>Recovery Action</th>\n<th>Prevention Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Invalid Metric Name</td>\n<td>Regex pattern matching</td>\n<td>HTTP 400 with field details</td>\n<td>Reject metric, continue processing batch</td>\n<td>Client-side validation libraries</td>\n</tr>\n<tr>\n<td>Label Cardinality Explosion</td>\n<td>Label combination counting</td>\n<td>HTTP 429 with cardinality info</td>\n<td>Reject high-cardinality metrics</td>\n<td>Cardinality monitoring and alerts</td>\n</tr>\n<tr>\n<td>Timestamp Out of Range</td>\n<td>Time bounds checking</td>\n<td>HTTP 400 with acceptable range</td>\n<td>Reject samples, accept others</td>\n<td>Clock synchronization monitoring</td>\n</tr>\n<tr>\n<td>Invalid Sample Values</td>\n<td>Numeric validation</td>\n<td>HTTP 400 with value constraints</td>\n<td>Skip invalid samples</td>\n<td>Input sanitization at source</td>\n</tr>\n<tr>\n<td>Missing Required Fields</td>\n<td>Schema validation</td>\n<td>HTTP 422 with missing fields</td>\n<td>Reject entire metric</td>\n<td>Schema documentation and examples</td>\n</tr>\n<tr>\n<td>Malformed JSON/Protocol</td>\n<td>Parsing failure</td>\n<td>HTTP 400 with parse error</td>\n<td>Reject request, maintain connection</td>\n<td>Protocol version negotiation</td>\n</tr>\n</tbody></table>\n<p>The <code>MetricsIngester</code> component implements a multi-stage validation pipeline that processes metrics through increasingly sophisticated validation checks. Early-stage validation catches obvious format errors quickly, while later stages perform more expensive operations like cardinality analysis and duplicate detection.</p>\n<blockquote>\n<p><strong>Decision: Batch-Level vs Individual Metric Error Handling</strong></p>\n<ul>\n<li><strong>Context</strong>: When processing batches of metrics, individual metrics may fail validation while others succeed</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Fail entire batch if any metric is invalid</li>\n<li>Process all valid metrics, report errors for invalid ones</li>\n<li>Fail fast on first error</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Process valid metrics, report detailed errors for invalid ones</li>\n<li><strong>Rationale</strong>: Maximizes data availability while providing actionable error feedback</li>\n<li><strong>Consequences</strong>: More complex error tracking but better system resilience</li>\n</ul>\n</blockquote>\n<p>The validation pipeline maintains detailed error statistics that help identify systematic problems in metric production. These statistics are exposed through the health monitoring system and can trigger alerts when error rates exceed acceptable thresholds.</p>\n<h4 id=\"network-failures-and-connectivity-issues\">Network Failures and Connectivity Issues</h4>\n<p>Network-related failures in metrics ingestion require careful handling to balance data durability with system responsiveness. The ingestion engine must distinguish between temporary connectivity issues that warrant retry attempts and persistent failures that indicate configuration or infrastructure problems.</p>\n<p>The ingestion system implements a sophisticated backpressure mechanism that dynamically adjusts acceptance rates based on downstream capacity. When the storage engine becomes overwhelmed, the ingestion engine gradually increases response latencies and begins rejecting new requests with HTTP 503 status codes that include retry-after headers.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Symptom</th>\n<th>Detection</th>\n<th>Response</th>\n<th>Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Client Timeout</td>\n<td>Connection drops during request</td>\n<td>Request context cancellation</td>\n<td>Return partial success status</td>\n<td>Client implements retry logic</td>\n</tr>\n<tr>\n<td>DNS Resolution</td>\n<td>Cannot resolve ingestion endpoint</td>\n<td>DNS lookup failure</td>\n<td>Return service unavailable</td>\n<td>Verify DNS configuration</td>\n</tr>\n<tr>\n<td>TLS Handshake</td>\n<td>Certificate validation fails</td>\n<td>TLS negotiation error</td>\n<td>Return TLS error with details</td>\n<td>Check certificate validity</td>\n</tr>\n<tr>\n<td>TCP Connection</td>\n<td>Network unreachable</td>\n<td>Connection refused/timeout</td>\n<td>Return connection error</td>\n<td>Verify network connectivity</td>\n</tr>\n<tr>\n<td>HTTP Parsing</td>\n<td>Malformed HTTP request</td>\n<td>Parse error during request processing</td>\n<td>Return HTTP 400</td>\n<td>Validate client HTTP implementation</td>\n</tr>\n<tr>\n<td>Request Size</td>\n<td>Payload exceeds limits</td>\n<td>Content-length or memory checks</td>\n<td>Return HTTP 413</td>\n<td>Implement client-side batching</td>\n</tr>\n</tbody></table>\n<p>The ingestion engine maintains circuit breakers for downstream dependencies like the storage engine and validation services. When failure rates exceed configurable thresholds, the circuit breaker opens and begins failing requests immediately rather than waiting for timeouts. This prevents cascade failures and provides faster feedback to clients.</p>\n<h4 id=\"backpressure-and-flow-control\">Backpressure and Flow Control</h4>\n<p>When the metrics ingestion rate exceeds the system&#39;s processing capacity, the ingestion engine must implement intelligent backpressure mechanisms that maintain system stability while maximizing throughput. The backpressure system operates on multiple levels, from individual request handling to system-wide flow control.</p>\n<p>The ingestion engine monitors several key metrics to detect capacity constraints:</p>\n<ul>\n<li>Storage engine write latency and error rates</li>\n<li>Memory usage in validation and processing pipelines  </li>\n<li>CPU utilization in metric parsing and validation</li>\n<li>Network buffer utilization for incoming connections</li>\n<li>Queue depths in asynchronous processing stages</li>\n</ul>\n<p>When backpressure conditions are detected, the system implements graduated responses:</p>\n<ol>\n<li><strong>Initial Response</strong>: Increase response latencies slightly to encourage client-side rate limiting</li>\n<li><strong>Moderate Pressure</strong>: Begin rejecting lowest-priority metrics while accepting high-priority ones</li>\n<li><strong>High Pressure</strong>: Return HTTP 503 responses with retry-after headers to implement explicit backoff</li>\n<li><strong>Critical Pressure</strong>: Temporarily refuse new connections while processing existing request backlog</li>\n<li><strong>Emergency Mode</strong>: Activate emergency shedding of non-critical metrics to prevent system collapse</li>\n</ol>\n<p>The backpressure system maintains fairness by implementing per-client rate limiting that prevents any single metric producer from overwhelming the system. Clients that consistently produce high-quality, well-formatted metrics receive higher priority during backpressure conditions.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Cardinality Explosion During High Load</strong>\nMany implementations focus on request rate limiting but ignore cardinality validation during high load conditions. This can lead to memory exhaustion as the system accepts high-cardinality metrics that consume excessive indexing resources. Always validate cardinality even when implementing backpressure—reject high-cardinality metrics first during resource constraints.</p>\n<h3 id=\"storage-error-handling\">Storage Error Handling</h3>\n<p>The time-series storage engine faces unique challenges related to data persistence, consistency, and recovery from various failure modes. Storage errors can have long-lasting impact on system reliability and data availability, requiring sophisticated detection and recovery mechanisms.</p>\n<h4 id=\"mental-model-the-bank-vault-system\">Mental Model: The Bank Vault System</h4>\n<p>Think of storage error handling like a bank&#39;s vault security system. The vault has multiple layers of protection: tamper-evident seals detect unauthorized access, environmental sensors monitor temperature and humidity, backup power systems ensure continuous operation, and detailed audit logs track every access. When problems are detected, the system has specific protocols for different scenarios—some trigger immediate lockdown, others activate backup systems, and some simply log the event for later investigation.</p>\n<p>Similarly, our storage engine employs multiple protective mechanisms. Write-ahead logging serves as the tamper-evident seal, ensuring we can detect and recover from interrupted operations. Health monitoring acts as environmental sensors, detecting degraded performance before complete failure. Backup and replication provide continuity during hardware failures. And comprehensive error logging enables forensic analysis of storage problems.</p>\n<p>The key insight is that storage systems must be paranoid about data integrity while remaining operational during various failure conditions. Like a bank vault, the storage engine must balance security (data integrity) with accessibility (query performance).</p>\n<h4 id=\"disk-failures-and-hardware-issues\">Disk Failures and Hardware Issues</h4>\n<p>Hardware-related storage failures require immediate detection and graceful degradation to prevent data loss while maintaining system availability. The storage engine implements comprehensive monitoring of disk health, filesystem status, and I/O performance to detect impending failures before they cause data corruption.</p>\n<table>\n<thead>\n<tr>\n<th>Hardware Failure</th>\n<th>Early Warning Signs</th>\n<th>Detection Method</th>\n<th>Immediate Response</th>\n<th>Recovery Procedure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Disk Bad Sectors</td>\n<td>Increasing read/write latency</td>\n<td>SMART monitoring, I/O error tracking</td>\n<td>Mark affected blocks read-only</td>\n<td>Copy data to healthy storage, mark disk for replacement</td>\n</tr>\n<tr>\n<td>Filesystem Corruption</td>\n<td>Checksum validation failures</td>\n<td>File integrity verification</td>\n<td>Switch to read-only mode</td>\n<td>Run filesystem repair, restore from backup if needed</td>\n</tr>\n<tr>\n<td>Out of Disk Space</td>\n<td>Free space below threshold</td>\n<td>Disk usage monitoring</td>\n<td>Activate emergency compaction</td>\n<td>Delete oldest data per retention policy</td>\n</tr>\n<tr>\n<td>I/O Subsystem Failure</td>\n<td>All disk operations failing</td>\n<td>System call error detection</td>\n<td>Activate read-only mode</td>\n<td>Restart storage engine with backup configuration</td>\n</tr>\n<tr>\n<td>Memory Corruption</td>\n<td>Unexpected data patterns</td>\n<td>Data validation checks</td>\n<td>Isolate corrupt memory regions</td>\n<td>Restart process, reload data from persistent storage</td>\n</tr>\n<tr>\n<td>Network Partition</td>\n<td>Cannot reach replica nodes</td>\n<td>Network connectivity monitoring</td>\n<td>Continue serving local data</td>\n<td>Implement split-brain prevention protocols</td>\n</tr>\n</tbody></table>\n<p>The storage engine maintains multiple redundancy mechanisms to handle hardware failures gracefully. The write-ahead log provides immediate durability for recent writes, while periodic snapshots enable recovery of the complete dataset. Block-based storage with checksums detects corruption early, and automatic compaction can often recover from minor filesystem issues.</p>\n<p>When hardware failures are detected, the storage engine transitions through a series of degraded operational modes:</p>\n<ol>\n<li><strong>Normal Operation</strong>: All read and write operations functioning normally</li>\n<li><strong>Degraded Writes</strong>: Read operations continue, writes queued to WAL only</li>\n<li><strong>Read-Only Mode</strong>: Serve existing data, reject all write operations</li>\n<li><strong>Emergency Mode</strong>: Serve cached data only, prepare for shutdown</li>\n<li><strong>Offline Mode</strong>: Reject all operations, preserve data integrity for recovery</li>\n</ol>\n<h4 id=\"data-corruption-detection-and-recovery\">Data Corruption Detection and Recovery</h4>\n<p>Data corruption in time-series storage can occur at multiple levels, from individual sample values to entire storage blocks. The storage engine implements comprehensive corruption detection and recovery mechanisms that operate continuously during normal operations.</p>\n<p>The storage engine uses a multi-layered approach to corruption detection:</p>\n<p><strong>Block-Level Integrity</strong>: Each storage block maintains cryptographic checksums that are validated on every read operation. When checksum mismatches are detected, the system immediately marks the block as corrupt and attempts to recover data from replicas or backup copies.</p>\n<p><strong>Series-Level Validation</strong>: The storage engine periodically validates the consistency of time-series data, checking for impossible timestamp sequences, duplicate samples, and value ranges that violate metric type constraints. These validation passes run during low-traffic periods to minimize performance impact.</p>\n<p><strong>Cross-Reference Validation</strong>: The storage engine maintains multiple indexes for the same data and periodically cross-validates these indexes to detect inconsistencies that might indicate memory corruption or software bugs.</p>\n<table>\n<thead>\n<tr>\n<th>Corruption Type</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>Prevention Measure</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Block Checksum Mismatch</td>\n<td>Read-time validation</td>\n<td>Restore from replica or backup</td>\n<td>Regular checksum verification</td>\n</tr>\n<tr>\n<td>Invalid Timestamps</td>\n<td>Series consistency checks</td>\n<td>Remove invalid samples</td>\n<td>Timestamp validation on write</td>\n</tr>\n<tr>\n<td>Duplicate Samples</td>\n<td>Index consistency validation</td>\n<td>Deduplicate during compaction</td>\n<td>Write-time duplicate detection</td>\n</tr>\n<tr>\n<td>Missing Index Entries</td>\n<td>Cross-reference validation</td>\n<td>Rebuild index from raw data</td>\n<td>Atomic index updates</td>\n</tr>\n<tr>\n<td>Truncated Files</td>\n<td>File size validation</td>\n<td>Restore from backup or WAL</td>\n<td>Atomic file operations</td>\n</tr>\n<tr>\n<td>Invalid Sample Values</td>\n<td>Range validation checks</td>\n<td>Mark samples as invalid</td>\n<td>Input validation pipeline</td>\n</tr>\n</tbody></table>\n<p>The recovery process for detected corruption follows a systematic approach:</p>\n<ol>\n<li><strong>Isolation</strong>: Immediately mark corrupted data as unavailable for queries</li>\n<li><strong>Assessment</strong>: Determine the extent of corruption and available recovery options  </li>\n<li><strong>Recovery</strong>: Restore data from the most recent clean backup or replica</li>\n<li><strong>Validation</strong>: Verify the recovered data passes all integrity checks</li>\n<li><strong>Integration</strong>: Gradually reintroduce recovered data into normal operations</li>\n<li><strong>Analysis</strong>: Investigate root cause to prevent similar corruption</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Assuming Filesystem Reliability</strong>\nMany storage implementations assume the underlying filesystem provides perfect data integrity. However, filesystems can experience silent corruption, especially under high write loads or during power failures. Always implement application-level checksums and never rely solely on filesystem guarantees for data integrity.</p>\n<h4 id=\"storage-resource-management\">Storage Resource Management</h4>\n<p>The storage engine must carefully manage various system resources to prevent resource exhaustion while maintaining optimal performance. Resource management errors can cascade through the system, causing failures in other components that depend on storage availability.</p>\n<p>Memory management in the storage engine involves multiple pools with different lifecycle characteristics. The ingestion buffer pool handles short-lived allocations for incoming metric samples. The query result pool manages medium-lived allocations for query processing. The index cache pool maintains long-lived allocations for frequently accessed index data.</p>\n<p>File descriptor management becomes critical in systems handling thousands of time series, as each active series may require multiple file handles for data blocks, indexes, and WAL segments. The storage engine implements sophisticated file handle pooling and lazy loading to stay within system limits while maintaining performance.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Monitoring Method</th>\n<th>Warning Threshold</th>\n<th>Critical Response</th>\n<th>Prevention Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Memory Usage</td>\n<td>Process memory monitoring</td>\n<td>80% of available RAM</td>\n<td>Activate memory pressure relief</td>\n<td>Implement LRU caching with configurable limits</td>\n</tr>\n<tr>\n<td>File Descriptors</td>\n<td>Open file handle counting</td>\n<td>90% of ulimit</td>\n<td>Close least-recently-used files</td>\n<td>File handle pooling and sharing</td>\n</tr>\n<tr>\n<td>Disk I/O Bandwidth</td>\n<td>I/O utilization tracking</td>\n<td>90% of measured capacity</td>\n<td>Throttle non-critical operations</td>\n<td>Separate high/low priority queues</td>\n</tr>\n<tr>\n<td>Network Connections</td>\n<td>Active connection monitoring</td>\n<td>Connection pool 90% full</td>\n<td>Reject new connections</td>\n<td>Connection pooling and reuse</td>\n</tr>\n<tr>\n<td>CPU Usage</td>\n<td>Process CPU monitoring</td>\n<td>95% sustained usage</td>\n<td>Reduce background tasks</td>\n<td>Asynchronous processing pipelines</td>\n</tr>\n<tr>\n<td>Temporary Disk Space</td>\n<td>Temp directory monitoring</td>\n<td>95% of available space</td>\n<td>Purge temporary files</td>\n<td>Aggressive temporary file cleanup</td>\n</tr>\n</tbody></table>\n<p>The storage engine implements resource quotas that prevent any single operation from consuming excessive resources. Query operations have memory limits that prevent runaway aggregations from exhausting system memory. Compaction operations have I/O bandwidth limits that prevent them from interfering with real-time ingestion and queries.</p>\n<h3 id=\"alerting-error-handling\">Alerting Error Handling</h3>\n<p>The alerting system must maintain high reliability while handling various failure modes that could prevent critical notifications from reaching their intended recipients. Alerting failures can have severe operational consequences, making robust error handling essential for production deployments.</p>\n<h4 id=\"mental-model-the-emergency-communication-network\">Mental Model: The Emergency Communication Network</h4>\n<p>Think of alerting error handling like an emergency communication network during a natural disaster. The network has multiple communication channels (radio, satellite, cellular), backup power systems, redundant message routing, and detailed logging of all communication attempts. When the primary communication channel fails, the system automatically switches to backup channels. When messages can&#39;t be delivered immediately, they&#39;re queued for retry with escalating urgency. Critical messages are sent through multiple channels simultaneously to ensure delivery.</p>\n<p>This emergency network metaphor captures the essential characteristics of reliable alerting systems: redundancy, persistence, escalation, and comprehensive audit trails. Just as emergency responders need absolute confidence that their communications will reach recipients, operations teams need confidence that critical alerts will be delivered even during system failures.</p>\n<h4 id=\"notification-delivery-failures\">Notification Delivery Failures</h4>\n<p>Notification delivery failures represent some of the most critical errors in the metrics system, as they can prevent teams from responding to production incidents. The alerting system must implement sophisticated retry logic, escalation policies, and fallback mechanisms to ensure critical notifications reach their intended recipients.</p>\n<p>The notification delivery system categorizes failures into distinct classes that require different handling approaches:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Class</th>\n<th>Examples</th>\n<th>Retry Strategy</th>\n<th>Escalation Policy</th>\n<th>Fallback Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Transient Network</td>\n<td>DNS timeouts, connection refused</td>\n<td>Exponential backoff, max 5 retries</td>\n<td>Try alternate endpoints after 3 failures</td>\n<td>Switch to backup notification channel</td>\n</tr>\n<tr>\n<td>Authentication</td>\n<td>Invalid API keys, expired tokens</td>\n<td>No retry, immediate escalation</td>\n<td>Alert configuration team</td>\n<td>Use emergency webhook endpoint</td>\n</tr>\n<tr>\n<td>Rate Limiting</td>\n<td>API quota exceeded, throttling</td>\n<td>Respect rate limits, queue messages</td>\n<td>Spread notifications across time</td>\n<td>Batch multiple alerts into single message</td>\n</tr>\n<tr>\n<td>Recipient Invalid</td>\n<td>Email bounces, invalid Slack channel</td>\n<td>No retry, log error</td>\n<td>Remove invalid recipient from future notifications</td>\n<td>Send to backup recipient list</td>\n</tr>\n<tr>\n<td>Message Format</td>\n<td>Invalid JSON, template errors</td>\n<td>Fix format if possible, otherwise skip</td>\n<td>Alert template maintainers</td>\n<td>Use plain text fallback message</td>\n</tr>\n<tr>\n<td>Service Unavailable</td>\n<td>Slack/PagerDuty outage</td>\n<td>Extended retry with circuit breaker</td>\n<td>Try all configured channels</td>\n<td>Store notifications for later delivery</td>\n</tr>\n</tbody></table>\n<p>The notification delivery engine maintains detailed logs of every delivery attempt, including timestamps, recipient information, delivery status, error details, and retry attempts. These logs are essential for troubleshooting notification issues and proving compliance with SLA requirements.</p>\n<blockquote>\n<p><strong>Decision: Notification Persistence Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Notifications may fail delivery due to various transient issues, but critical alerts must eventually reach recipients</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Best-effort delivery with no persistence</li>\n<li>In-memory queues with limited retry</li>\n<li>Persistent notification queue with guaranteed delivery</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Persistent notification queue with configurable retry policies</li>\n<li><strong>Rationale</strong>: Critical alerts justify the complexity of persistent delivery guarantees</li>\n<li><strong>Consequences</strong>: Increased system complexity but much higher notification reliability</li>\n</ul>\n</blockquote>\n<p>The notification persistence system stores failed notifications in a durable queue that survives system restarts and network outages. Each notification includes metadata about retry attempts, delivery windows, and escalation policies. The system implements intelligent retry scheduling that backs off exponentially while respecting recipient preferences for notification frequency.</p>\n<h4 id=\"alert-flapping-prevention\">Alert Flapping Prevention</h4>\n<p>Alert flapping—rapid transitions between firing and resolved states—can overwhelm notification channels and desensitize operations teams to legitimate alerts. The alerting system implements sophisticated flapping detection and suppression mechanisms that maintain alert responsiveness while preventing notification storms.</p>\n<p>The flapping detection algorithm analyzes alert state transition patterns over configurable time windows. It considers factors like transition frequency, state duration, and the underlying metric volatility to distinguish between legitimate alerts and flapping conditions.</p>\n<table>\n<thead>\n<tr>\n<th>Flapping Pattern</th>\n<th>Detection Criteria</th>\n<th>Suppression Action</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rapid Fire/Resolve</td>\n<td>&gt;5 transitions in 10 minutes</td>\n<td>Extend evaluation duration</td>\n<td>Increase alert threshold hysteresis</td>\n</tr>\n<tr>\n<td>Threshold Bouncing</td>\n<td>Value oscillates around threshold</td>\n<td>Apply smoothing to evaluation</td>\n<td>Use moving averages instead of instant values</td>\n</tr>\n<tr>\n<td>Intermittent Failures</td>\n<td>Alternating success/failure states</td>\n<td>Require sustained failure condition</td>\n<td>Implement &quot;consecutive failures&quot; logic</td>\n</tr>\n<tr>\n<td>Metric Spikes</td>\n<td>Brief value spikes trigger alerts</td>\n<td>Ignore single-sample anomalies</td>\n<td>Require multiple samples above threshold</td>\n</tr>\n<tr>\n<td>Clock Skew Issues</td>\n<td>Timestamp inconsistencies cause false alerts</td>\n<td>Detect and correct timestamp drift</td>\n<td>Implement timestamp validation and adjustment</td>\n</tr>\n<tr>\n<td>Configuration Errors</td>\n<td>Overly sensitive thresholds</td>\n<td>Temporarily disable problematic rules</td>\n<td>Alert rule configuration validation</td>\n</tr>\n</tbody></table>\n<p>The alerting system implements configurable hysteresis for threshold-based alerts, requiring different thresholds for firing and resolving conditions. For example, a CPU usage alert might fire when usage exceeds 90% but only resolve when usage drops below 85%. This prevents flapping when metric values oscillate around the threshold.</p>\n<p>The flapping prevention system maintains state for each alert rule, tracking recent transition history and applying appropriate suppression policies. When flapping is detected, the system can take several actions:</p>\n<ol>\n<li><strong>Temporary Suppression</strong>: Silence notifications while allowing the alert to continue evaluation</li>\n<li><strong>Extended Evaluation</strong>: Increase the duration requirement before firing alerts</li>\n<li><strong>Threshold Adjustment</strong>: Temporarily modify thresholds to reduce sensitivity</li>\n<li><strong>Aggregation Mode</strong>: Switch to evaluating aggregated metrics instead of instant values</li>\n<li><strong>Maintenance Mode</strong>: Mark the alert rule for manual review and adjustment</li>\n</ol>\n<p>⚠️ <strong>Pitfall: Over-Aggressive Flapping Suppression</strong>\nImplementing flapping suppression that&#39;s too aggressive can prevent legitimate alerts from firing during rapidly changing conditions. Always maintain escape hatches that allow critical alerts to bypass flapping suppression, and regularly review suppressed alerts to ensure they represent genuine flapping rather than system issues.</p>\n<h4 id=\"silence-management-and-maintenance-windows\">Silence Management and Maintenance Windows</h4>\n<p>Operations teams need the ability to temporarily suppress notifications during planned maintenance windows or when investigating known issues. The alerting system implements sophisticated silence management that prevents unwanted notifications while maintaining audit trails and preventing accidentally permanent suppressions.</p>\n<p>The silence management system supports multiple types of suppressions:</p>\n<p><strong>Time-Based Silences</strong>: Suppress alerts for specific time periods, commonly used for maintenance windows. These silences have definite start and end times and automatically expire to prevent forgotten suppressions.</p>\n<p><strong>Label-Based Silences</strong>: Suppress alerts matching specific label combinations, useful for suppressing alerts from problematic hosts or services during investigation.</p>\n<p><strong>Alert-Specific Silences</strong>: Suppress specific alert rules entirely, typically used for rules that are being debugged or reconfigured.</p>\n<p><strong>Escalation Silences</strong>: Allow alerts to fire but suppress specific notification channels, useful for reducing noise during incident response.</p>\n<table>\n<thead>\n<tr>\n<th>Silence Type</th>\n<th>Configuration</th>\n<th>Audit Requirements</th>\n<th>Expiration Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Maintenance Window</td>\n<td>Start/end time, affected labels</td>\n<td>Who created, reason, affected systems</td>\n<td>Automatic expiration at end time</td>\n</tr>\n<tr>\n<td>Investigation Silence</td>\n<td>Label matchers, duration</td>\n<td>Incident ticket reference</td>\n<td>Manual renewal required after 24 hours</td>\n</tr>\n<tr>\n<td>Rule Debugging</td>\n<td>Specific rule ID, duration</td>\n<td>Configuration change justification</td>\n<td>Maximum 1 week duration</td>\n</tr>\n<tr>\n<td>Channel Suppression</td>\n<td>Channel and alert combinations</td>\n<td>Escalation acknowledgment</td>\n<td>Automatic expiration after incident resolution</td>\n</tr>\n<tr>\n<td>Emergency Silence</td>\n<td>Broad label matchers, short duration</td>\n<td>Incident commander authorization</td>\n<td>Maximum 4 hours, requires manual renewal</td>\n</tr>\n</tbody></table>\n<p>The silence management system maintains comprehensive audit logs that track silence creation, modification, and expiration. These logs include the identity of the user creating the silence, the justification for the suppression, and the expected impact on alerting coverage.</p>\n<p>All silences require explicit expiration times to prevent accidentally permanent suppressions. The system sends notifications to silence creators before silences expire, allowing them to extend or modify the suppression if needed. Critical silences that broadly suppress alerts require additional approval workflows and shorter maximum durations.</p>\n<p>The alerting system provides visibility into active silences through the dashboard interface, showing which alerts are currently suppressed and why. This visibility helps prevent situations where important alerts are inadvertently suppressed during critical incidents.</p>\n<h3 id=\"common-pitfalls-in-error-handling\">Common Pitfalls in Error Handling</h3>\n<p>Understanding common error handling mistakes helps avoid subtle bugs that can compromise system reliability. These pitfalls represent patterns frequently encountered when building production metrics systems.</p>\n<p>⚠️ <strong>Pitfall: Cascading Failure Amplification</strong>\nWhen one component fails, poorly designed error handling can amplify the failure across the entire system. For example, if the storage engine becomes slow, aggressive retry logic in the ingestion engine can make the problem worse by increasing load. Always implement circuit breakers and exponential backoff to prevent failure amplification.</p>\n<p>⚠️ <strong>Pitfall: Silent Data Loss During Errors</strong>\nSystems sometimes silently drop data during error conditions without proper logging or alerting. This is particularly dangerous in metrics systems where missing data points can hide real problems. Always log data loss events and implement monitoring for data ingestion rates.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Error Response Formats</strong>\nDifferent components returning errors in different formats makes client error handling difficult and unreliable. Standardize error response formats across all API endpoints and include sufficient detail for programmatic handling.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Error Context</strong>\nError messages that lack context make troubleshooting difficult and time-consuming. Always include relevant identifiers (metric names, timestamps, client IDs) in error messages to enable efficient debugging.</p>\n<p>⚠️ <strong>Pitfall: Blocking Operations During Error Handling</strong>\nError handling code that performs blocking operations can cause the entire system to become unresponsive during failure conditions. Use timeouts, asynchronous processing, and non-blocking I/O in all error handling paths.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical code structure and patterns for implementing comprehensive error handling throughout the metrics and alerting system.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Error Logging</td>\n<td>Standard library logging with structured fields</td>\n<td>Structured logging with correlation IDs (slog)</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>HTTP endpoint with JSON status</td>\n<td>Prometheus metrics with detailed health checks</td>\n</tr>\n<tr>\n<td>Circuit Breakers</td>\n<td>Simple failure counting with timeouts</td>\n<td>Hystrix-style circuit breakers with metrics</td>\n</tr>\n<tr>\n<td>Retry Logic</td>\n<td>Exponential backoff with jitter</td>\n<td>Sophisticated retry policies with circuit breakers</td>\n</tr>\n<tr>\n<td>Error Persistence</td>\n<td>In-memory error queues</td>\n<td>Persistent error storage with WAL</td>\n</tr>\n<tr>\n<td>Monitoring Integration</td>\n<td>Basic metrics collection</td>\n<td>Full observability with tracing and metrics</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure-for-error-handling\">File Structure for Error Handling</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/\n├── errors/\n│   ├── types.go              ← error type definitions and interfaces\n│   ├── validation.go         ← input validation errors and handling\n│   ├── storage.go           ← storage-specific error types\n│   ├── network.go           ← network and transport errors\n│   └── recovery.go          ← error recovery and retry logic\n├── health/\n│   ├── manager.go           ← health check coordination\n│   ├── checks.go            ← individual health check implementations\n│   └── status.go            ← health status aggregation\n├── circuit/\n│   ├── breaker.go           ← circuit breaker implementation\n│   └── metrics.go           ← circuit breaker metrics and monitoring\n└── retry/\n    ├── policy.go            ← retry policy definitions\n    ├── backoff.go          ← backoff algorithms\n    └── queue.go            ← persistent retry queue</code></pre></div>\n\n<h4 id=\"core-error-type-definitions\">Core Error Type Definitions</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> errors</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidationErrors represents a collection of validation failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ValidationErrors</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Errors []</span><span style=\"color:#B392F0\">ValidationError</span><span style=\"color:#9ECBFF\"> `json:\"errors\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidationError represents a single field validation failure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ValidationError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Field   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"field\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Value   </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"value\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\"> `json:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Add appends a new validation error to the collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ve </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ValidationErrors</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Add</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">field</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">message</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create new ValidationError with provided parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Append to the Errors slice</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Ensure thread safety if called concurrently</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ToJSON converts validation errors to JSON for API responses</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ve </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ValidationErrors</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ToJSON</span><span style=\"color:#E1E4E8\">() ([]</span><span style=\"color:#F97583\">byte</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Use encoding/json to marshal the struct</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Handle marshaling errors appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Return formatted JSON bytes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// StorageError represents errors from the storage engine</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"operation\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Cause     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">    `json:\"cause\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Retryable </span><span style=\"color:#F97583\">bool</span><span style=\"color:#9ECBFF\">      `json:\"retryable\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error implements the error interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">se </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"storage </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> failed: </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, se.Operation, se.Cause)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NetworkError represents network-related failures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> NetworkError</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Endpoint  </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"endpoint\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Operation </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">        `json:\"operation\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"timeout\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Attempt   </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">           `json:\"attempt\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Error implements the error interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">ne </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NetworkError</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"network </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> to </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> failed on attempt </span><span style=\"color:#79B8FF\">%d</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, ne.Operation, ne.Endpoint, ne.Attempt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"health-management-implementation\">Health Management Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> health</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthManager coordinates health checks across system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checks    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthCheck</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    interval  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh    </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck represents a single component health check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status      </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\"> `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastChecked </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">    `json:\"last_checked\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CheckFunc   </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewHealthManager creates a new health check coordinator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHealthManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize HealthManager struct with provided parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create empty checks map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create stop channel for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return configured manager instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RegisterCheck adds a health check function for a component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">checkFunc</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock for thread safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create HealthCheck struct with name and function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize status as unknown and set initial timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Add to checks map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log registration event</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RunChecks executes all registered health checks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RunChecks</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock to get copy of checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Iterate through all registered checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute each check function with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Update check status based on result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Log any status changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Handle context cancellation gracefully</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetOverallStatus returns the worst status among all checks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetOverallStatus</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire read lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Initialize status as Healthy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Iterate through all checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Find the worst status (Unhealthy > Degraded > Healthy)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return overall system status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"circuit-breaker-implementation\">Circuit Breaker Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> circuit</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">errors</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Breaker implements circuit breaker pattern for fault tolerance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Breaker</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name           </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    maxFailures    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resetTimeout   </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    state          </span><span style=\"color:#B392F0\">BreakerState</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failures       </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lastFailure    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu            </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    onStateChange </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">BreakerState</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">BreakerState</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// BreakerState represents circuit breaker states</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> BreakerState</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateClosed</span><span style=\"color:#B392F0\"> BreakerState</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateOpen</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    StateHalfOpen</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Execute runs the provided function through the circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Execute</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check current circuit breaker state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If open, check if reset timeout has elapsed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: If closed or half-open, attempt to execute operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Handle operation success (reset failure count)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle operation failure (increment count, possibly trip breaker)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Update state and notify state change listeners</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Return appropriate error based on state and operation result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// tripBreaker changes state to open and records the failure time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">tripBreaker</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Change state to StateOpen</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Record current time as lastFailure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call state change callback if registered</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// resetBreaker changes state to closed and clears failure count</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">cb </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Breaker</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">resetBreaker</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Acquire write lock</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Change state to StateClosed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Reset failures count to zero</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Call state change callback if registered</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"retry-policy-implementation\">Retry Policy Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> retry</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Policy defines retry behavior for failed operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> Policy</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxAttempts     </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    InitialDelay    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MaxDelay        </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Multiplier      </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Jitter          </span><span style=\"color:#F97583\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    RetryableErrors []</span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Execute attempts an operation with retry logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Policy</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Execute</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operation</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize attempt counter and delay</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Loop until max attempts reached or context cancelled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute operation and check result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: If successful, return nil</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: If error not retryable, return immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Calculate next delay with exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 7: Apply jitter if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 8: Wait for delay duration or context cancellation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 9: Continue to next attempt</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// calculateDelay computes the next retry delay with exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Policy</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">calculateDelay</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">attempt</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate exponential delay: initialDelay * (multiplier ^ attempt)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Apply maximum delay limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add jitter if enabled (±25% random variation)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return final delay duration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// isRetryable checks if an error should trigger a retry attempt</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Policy</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">isRetryable</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">bool</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if error is nil (should not retry success)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: If no retryable errors configured, retry all errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Iterate through configured retryable error types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Use errors.Is() to match error types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return true if error matches any retryable type</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After Implementing Error Handling Infrastructure:</strong></p>\n<ol>\n<li>Run <code>go test ./internal/errors/... -v</code> to verify error type handling</li>\n<li>Test health checks with <code>curl http://localhost:8080/health</code> - should return JSON with component statuses</li>\n<li>Verify circuit breaker functionality by simulating downstream failures</li>\n<li>Confirm retry policies work by testing with flaky network connections</li>\n<li>Check error logging appears in structured format with appropriate detail levels</li>\n</ol>\n<p><strong>Signs of Proper Error Handling:</strong></p>\n<ul>\n<li>Error responses include actionable information for clients</li>\n<li>System remains responsive during various failure conditions</li>\n<li>Health checks accurately reflect component status</li>\n<li>Circuit breakers prevent cascade failures during outages</li>\n<li>Retry logic recovers from transient failures automatically</li>\n</ul>\n<p><strong>Common Issues to Debug:</strong></p>\n<ul>\n<li>Errors returning without sufficient context for troubleshooting</li>\n<li>Circuit breakers tripping too frequently due to overly sensitive thresholds  </li>\n<li>Retry logic causing thundering herd problems during recovery</li>\n<li>Health checks reporting false positives or negatives</li>\n<li>Error handling blocking normal operations during failure conditions</li>\n</ul>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (comprehensive testing is essential throughout the entire metrics and alerting system, from initial metric ingestion through storage, querying, visualization, and alerting capabilities)</p>\n</blockquote>\n<h3 id=\"the-quality-assurance-laboratory-understanding-testing-approach\">The Quality Assurance Laboratory: Understanding Testing Approach</h3>\n<p>Think of our testing strategy as operating a quality assurance laboratory for a pharmaceutical company. Just as a pharmaceutical lab has multiple testing phases—from testing individual chemical compounds in isolation, to testing drug interactions, to full clinical trials with real patients—our metrics system requires layered testing that validates everything from individual component behavior to complete end-to-end workflows. Each testing layer serves a specific purpose and catches different categories of problems, much like how pharmaceutical testing catches different types of safety and efficacy issues at each stage.</p>\n<p>The fundamental challenge in testing a metrics and alerting system lies in the inherently time-dependent and stateful nature of the system. Unlike testing a simple web API where requests are largely independent, our system maintains complex state across time-series data, alert conditions, and dashboard subscriptions. We must validate not just that individual operations work correctly, but that the system maintains consistency and correctness over time, handles various failure scenarios gracefully, and performs adequately under realistic load conditions.</p>\n<p>Our testing strategy employs three complementary layers that work together to provide comprehensive validation coverage. Unit testing validates individual component behavior in isolation, integration testing verifies component interactions and data flows, and milestone validation checkpoints ensure that each major system capability works correctly in realistic scenarios. This layered approach allows us to catch bugs early in development while also validating that the complete system meets its functional and non-functional requirements.</p>\n<h3 id=\"unit-testing-strategy\">Unit Testing Strategy</h3>\n<p>Unit testing forms the foundation of our testing pyramid, focusing on validating individual components in complete isolation from their dependencies. The primary goal is to verify that each component correctly implements its specified behavior under all possible input conditions, including edge cases and error scenarios that might be difficult to reproduce in integration testing.</p>\n<h4 id=\"component-isolation-and-mocking-strategy\">Component Isolation and Mocking Strategy</h4>\n<p>Our unit testing approach treats each component as a black box with clearly defined inputs and outputs. We use dependency injection and interface-based design to isolate components from their dependencies, allowing us to substitute mock implementations that provide predictable behavior during testing.</p>\n<blockquote>\n<p><strong>Decision: Interface-Based Mocking</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to test components in isolation without external dependencies like storage, network, or file system</li>\n<li><strong>Options Considered</strong>: Direct mocking libraries, interface-based mocks, test doubles with recording capabilities</li>\n<li><strong>Decision</strong>: Interface-based mocks with behavior verification</li>\n<li><strong>Rationale</strong>: Provides type safety, clear contracts, and ability to verify interaction patterns</li>\n<li><strong>Consequences</strong>: Requires interfaces for all major dependencies but provides robust isolation and clear test intent</li>\n</ul>\n</blockquote>\n<p>The following table outlines our mocking strategy for major component dependencies:</p>\n<table>\n<thead>\n<tr>\n<th>Component Under Test</th>\n<th>Mock Dependencies</th>\n<th>Mock Behavior</th>\n<th>Verification Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MetricsIngester</code></td>\n<td><code>TimeSeriesStorage</code>, <code>Validator</code>, <code>CardinalityManager</code></td>\n<td>Return predictable responses for storage operations</td>\n<td>Verify correct validation sequence, storage calls, error propagation</td>\n</tr>\n<tr>\n<td><code>QueryProcessor</code></td>\n<td><code>TimeSeriesStorage</code>, <code>QueryCache</code></td>\n<td>Simulate various data availability scenarios</td>\n<td>Verify query optimization, cache utilization, result formatting</td>\n</tr>\n<tr>\n<td><code>AlertEvaluator</code></td>\n<td><code>QueryProcessor</code>, <code>StateManager</code>, <code>NotificationManager</code></td>\n<td>Control metric values and state transitions</td>\n<td>Verify rule evaluation logic, state changes, notification triggers</td>\n</tr>\n<tr>\n<td><code>StorageEngine</code></td>\n<td>File system, network resources</td>\n<td>Simulate disk operations and failures</td>\n<td>Verify data persistence, compaction logic, error recovery</td>\n</tr>\n<tr>\n<td><code>DashboardServer</code></td>\n<td><code>QueryProcessor</code>, WebSocket connections</td>\n<td>Control query results and client interactions</td>\n<td>Verify subscription management, real-time updates, error handling</td>\n</tr>\n</tbody></table>\n<h4 id=\"core-component-testing-coverage\">Core Component Testing Coverage</h4>\n<p>Each component requires comprehensive testing that covers normal operation, boundary conditions, error scenarios, and concurrent access patterns. The testing approach varies based on the component&#39;s responsibility and complexity.</p>\n<p><strong>Metrics Ingestion Testing</strong></p>\n<p>The <code>MetricsIngester</code> component requires extensive testing of its validation and processing pipeline. We test each metric type individually to ensure proper handling of counters, gauges, and histograms. Critical test scenarios include:</p>\n<ol>\n<li><p><strong>Metric Type Validation</strong>: Verify that each metric type is processed according to its semantics—counters must be monotonically increasing, gauges can have arbitrary values, and histograms must include proper bucket definitions.</p>\n</li>\n<li><p><strong>Label Validation and Cardinality Control</strong>: Test label parsing, normalization, and cardinality explosion prevention. This includes testing the rejection of metrics that would exceed cardinality limits and proper handling of malformed label specifications.</p>\n</li>\n<li><p><strong>Batch Processing</strong>: Validate that metric batches are processed atomically and that partial failures in a batch don&#39;t corrupt the storage state.</p>\n</li>\n<li><p><strong>Backpressure Handling</strong>: Test behavior when the storage layer cannot keep up with ingestion rates, ensuring proper flow control and graceful degradation.</p>\n</li>\n</ol>\n<p><strong>Time-Series Storage Testing</strong></p>\n<p>The <code>StorageEngine</code> component requires testing of its complex persistence and compaction logic. Key testing areas include:</p>\n<ol>\n<li><p><strong>Write Path Validation</strong>: Test sample appending through the write-ahead log, ensuring proper durability guarantees and crash recovery capabilities.</p>\n</li>\n<li><p><strong>Compaction Logic</strong>: Verify that compaction correctly merges blocks, maintains temporal ordering, and applies retention policies without data loss.</p>\n</li>\n<li><p><strong>Query Path Optimization</strong>: Test series lookup, time range queries, and label filtering to ensure correct results and acceptable performance characteristics.</p>\n</li>\n<li><p><strong>Concurrent Access</strong>: Validate thread safety during simultaneous read and write operations, ensuring no data corruption or race conditions.</p>\n</li>\n</ol>\n<p><strong>Query Engine Testing</strong></p>\n<p>The <code>QueryProcessor</code> component requires testing of its parsing, planning, and execution pipeline:</p>\n<ol>\n<li><p><strong>Query Parsing</strong>: Test the lexical analyzer and parser with various query syntaxes, including edge cases like empty queries, malformed expressions, and complex nested functions.</p>\n</li>\n<li><p><strong>Query Planning</strong>: Verify that the execution planner generates optimal plans for different query patterns and correctly estimates resource requirements.</p>\n</li>\n<li><p><strong>Aggregation Functions</strong>: Test each aggregation function individually with various input patterns, including sparse data, missing values, and extreme values.</p>\n</li>\n<li><p><strong>Cache Integration</strong>: Validate cache hit/miss behavior, cache invalidation, and memory management under different query patterns.</p>\n</li>\n</ol>\n<p><strong>Alert Evaluation Testing</strong></p>\n<p>The <code>AlertEvaluator</code> component requires testing of its state machine and notification logic:</p>\n<ol>\n<li><p><strong>State Transitions</strong>: Test all possible alert state transitions, ensuring proper timing constraints and notification triggers.</p>\n</li>\n<li><p><strong>Rule Evaluation</strong>: Verify that alert rules are evaluated correctly with various metric patterns, including missing data and counter resets.</p>\n</li>\n<li><p><strong>Notification Dispatch</strong>: Test the notification queue and channel integration, including retry logic and failure handling.</p>\n</li>\n<li><p><strong>Timing Accuracy</strong>: Validate evaluation intervals and duration-based conditions are handled with appropriate precision.</p>\n</li>\n</ol>\n<h4 id=\"error-condition-testing\">Error Condition Testing</h4>\n<p>Each component must handle various error conditions gracefully, maintaining system stability and providing useful diagnostic information. Our error testing strategy covers the following categories:</p>\n<table>\n<thead>\n<tr>\n<th>Error Category</th>\n<th>Test Scenarios</th>\n<th>Expected Behavior</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Validation</td>\n<td>Malformed metrics, invalid labels, type mismatches</td>\n<td>Reject invalid input with descriptive errors</td>\n<td>Assert specific error messages and codes</td>\n</tr>\n<tr>\n<td>Resource Exhaustion</td>\n<td>Memory limits, disk space, connection limits</td>\n<td>Graceful degradation with backpressure</td>\n<td>Monitor resource usage and response times</td>\n</tr>\n<tr>\n<td>External Dependencies</td>\n<td>Storage failures, network timeouts, cache misses</td>\n<td>Proper error propagation and retry logic</td>\n<td>Inject failures and verify recovery</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>Race conditions, deadlocks, inconsistent reads</td>\n<td>Thread-safe operation without data corruption</td>\n<td>Concurrent test execution with verification</td>\n</tr>\n<tr>\n<td>Configuration Errors</td>\n<td>Invalid settings, missing parameters, type errors</td>\n<td>Startup validation with clear error messages</td>\n<td>Configuration validation testing</td>\n</tr>\n</tbody></table>\n<h4 id=\"performance-and-resource-usage-testing\">Performance and Resource Usage Testing</h4>\n<p>Unit tests must also validate performance characteristics and resource usage to prevent regressions:</p>\n<ol>\n<li><p><strong>Memory Usage</strong>: Test that components maintain bounded memory usage even with large inputs or extended operation periods.</p>\n</li>\n<li><p><strong>Processing Latency</strong>: Verify that individual operations complete within acceptable time bounds under various load conditions.</p>\n</li>\n<li><p><strong>Resource Cleanup</strong>: Ensure that components properly release resources (file handles, connections, goroutines) when stopped or during error conditions.</p>\n</li>\n<li><p><strong>Scalability Characteristics</strong>: Test behavior with varying data sizes to identify algorithmic complexity issues early.</p>\n</li>\n</ol>\n<h4 id=\"common-unit-testing-pitfalls\">Common Unit Testing Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Testing Implementation Details Instead of Behavior</strong>\nMany developers write tests that verify internal implementation details rather than observable behavior. For example, testing that a specific internal method was called with particular parameters rather than testing that the component produces the correct output for given inputs. This makes tests brittle and couples them unnecessarily to implementation details.</p>\n<p><strong>Why it&#39;s wrong</strong>: Tests that depend on implementation details break when the implementation changes, even if the behavior remains correct. This reduces confidence in refactoring and makes maintenance more difficult.</p>\n<p><strong>How to fix</strong>: Focus tests on the public interface and observable behavior. Test inputs and outputs, not internal method calls. Use behavior verification sparingly and only when the interaction pattern is part of the contract.</p>\n<p>⚠️ <strong>Pitfall: Insufficient Error Scenario Coverage</strong>\nUnit tests often focus heavily on happy-path scenarios while neglecting error conditions. This leaves error handling code untested and can lead to poor error messages or improper resource cleanup during failures.</p>\n<p><strong>Why it&#39;s wrong</strong>: Error conditions are often the most critical code paths for system reliability. Untested error handling can lead to data corruption, resource leaks, or cascading failures.</p>\n<p><strong>How to fix</strong>: Systematically test every error condition that can be triggered. Use dependency injection to simulate failures in external dependencies. Test resource cleanup during both normal and error conditions.</p>\n<p>⚠️ <strong>Pitfall: Non-Deterministic Test Behavior</strong>\nTests that depend on timing, random values, or external state can produce inconsistent results, making them unreliable for continuous integration and debugging.</p>\n<p><strong>Why it&#39;s wrong</strong>: Flaky tests reduce confidence in the test suite and make it difficult to identify real regressions. Teams often ignore failing tests that are known to be unreliable.</p>\n<p><strong>How to fix</strong>: Use deterministic inputs and mock time-dependent operations. Control randomness with fixed seeds. Isolate tests from external state and shared resources.</p>\n<h3 id=\"integration-testing\">Integration Testing</h3>\n<p>Integration testing validates the interactions between components and verifies that data flows correctly through the complete system pipeline. Unlike unit tests that use mocks to isolate components, integration tests use real implementations of dependencies to validate that components work correctly together.</p>\n<h4 id=\"end-to-end-workflow-validation\">End-to-End Workflow Validation</h4>\n<p>Our integration testing strategy focuses on validating complete workflows that span multiple components. Each workflow represents a critical system capability that must function correctly for the system to provide value to users.</p>\n<p><strong>Metric Ingestion to Storage Workflow</strong></p>\n<p>This integration test validates the complete path from metric submission through validation, processing, and persistence. The test creates a realistic mix of metric types and verifies that data is correctly stored and retrievable:</p>\n<ol>\n<li><p><strong>Setup Phase</strong>: Initialize a complete storage engine with real file system persistence, configure the ingestion pipeline with actual validators and cardinality managers.</p>\n</li>\n<li><p><strong>Data Submission</strong>: Submit a variety of metrics including counters with increasing values, gauges with fluctuating values, and histograms with different bucket distributions.</p>\n</li>\n<li><p><strong>Processing Verification</strong>: Verify that metrics are processed according to their type semantics and that validation rules are properly applied.</p>\n</li>\n<li><p><strong>Storage Validation</strong>: Query the storage engine directly to verify that samples are persisted with correct timestamps, values, and label associations.</p>\n</li>\n<li><p><strong>Compaction Testing</strong>: Allow time for compaction to occur and verify that data remains accessible and correct after compaction operations.</p>\n</li>\n</ol>\n<p><strong>Query Processing to Dashboard Workflow</strong></p>\n<p>This integration test validates the complete query execution pipeline from parsing through result delivery to dashboard clients:</p>\n<ol>\n<li><p><strong>Data Preparation</strong>: Populate the storage engine with time-series data covering multiple metrics with various label combinations and time ranges.</p>\n</li>\n<li><p><strong>Query Execution</strong>: Execute queries of varying complexity, including simple metric selection, label filtering, time range specification, and aggregation functions.</p>\n</li>\n<li><p><strong>Result Validation</strong>: Verify that query results contain the expected data points with correct values and timestamps.</p>\n</li>\n<li><p><strong>Dashboard Integration</strong>: Test the dashboard server&#39;s ability to execute queries and deliver results to WebSocket clients with proper formatting.</p>\n</li>\n<li><p><strong>Real-Time Updates</strong>: Verify that new data points trigger appropriate updates to subscribed dashboard panels.</p>\n</li>\n</ol>\n<p><strong>Alert Evaluation to Notification Workflow</strong></p>\n<p>This integration test validates the complete alerting pipeline from rule evaluation through notification delivery:</p>\n<ol>\n<li><p><strong>Alert Rule Configuration</strong>: Configure alert rules with various threshold conditions and notification channels.</p>\n</li>\n<li><p><strong>Metric Data Injection</strong>: Submit metric data that should trigger alert conditions, including scenarios for alert firing and resolution.</p>\n</li>\n<li><p><strong>Evaluation Process</strong>: Verify that the alert evaluator correctly processes rules according to their evaluation intervals.</p>\n</li>\n<li><p><strong>State Management</strong>: Validate that alert states transition correctly through the state machine with proper timing constraints.</p>\n</li>\n<li><p><strong>Notification Delivery</strong>: Verify that notifications are generated and delivered through configured channels with correct message content.</p>\n</li>\n</ol>\n<h4 id=\"cross-component-data-flow-testing\">Cross-Component Data Flow Testing</h4>\n<p>Integration tests must validate that data maintains consistency and correctness as it flows between components. This includes testing serialization/deserialization, data transformations, and error propagation.</p>\n<table>\n<thead>\n<tr>\n<th>Data Flow Path</th>\n<th>Test Scenarios</th>\n<th>Validation Points</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion → Storage</td>\n<td>Metric batches, label normalization, type validation</td>\n<td>Data persistence, retrieval accuracy, error handling</td>\n<td>Metrics stored exactly as specified with proper indexing</td>\n</tr>\n<tr>\n<td>Storage → Query</td>\n<td>Time range queries, label filtering, aggregation</td>\n<td>Result correctness, performance, partial results</td>\n<td>Accurate query results within performance bounds</td>\n</tr>\n<tr>\n<td>Query → Dashboard</td>\n<td>Real-time updates, subscription management, formatting</td>\n<td>Data freshness, update frequency, client synchronization</td>\n<td>Timely delivery of formatted data to subscribed clients</td>\n</tr>\n<tr>\n<td>Query → Alerting</td>\n<td>Rule evaluation, state transitions, threshold checking</td>\n<td>Evaluation accuracy, timing precision, state consistency</td>\n<td>Correct alert firing and resolution based on metric values</td>\n</tr>\n<tr>\n<td>Alerting → Notification</td>\n<td>Message formatting, channel routing, delivery confirmation</td>\n<td>Template processing, channel selection, retry logic</td>\n<td>Reliable notification delivery with proper formatting</td>\n</tr>\n</tbody></table>\n<h4 id=\"database-and-file-system-integration\">Database and File System Integration</h4>\n<p>The storage engine requires extensive integration testing with the actual file system to validate persistence, durability, and recovery capabilities:</p>\n<ol>\n<li><p><strong>Crash Recovery Testing</strong>: Simulate system crashes at various points during write operations to verify that the write-ahead log correctly recovers uncommitted data.</p>\n</li>\n<li><p><strong>Compaction Integration</strong>: Test compaction operations with real file I/O to verify that data is correctly merged and old files are properly cleaned up.</p>\n</li>\n<li><p><strong>Concurrent Access</strong>: Run multiple ingestion and query processes simultaneously to validate file locking and concurrent access patterns.</p>\n</li>\n<li><p><strong>Disk Space Management</strong>: Test behavior when disk space is exhausted, including proper error handling and graceful degradation.</p>\n</li>\n</ol>\n<h4 id=\"network-and-protocol-integration\">Network and Protocol Integration</h4>\n<p>The system&#39;s network interfaces require testing with real HTTP clients and WebSocket connections:</p>\n<ol>\n<li><p><strong>HTTP API Testing</strong>: Test the metric ingestion API with various HTTP clients, including proper handling of request timeouts, connection limits, and error responses.</p>\n</li>\n<li><p><strong>WebSocket Integration</strong>: Test dashboard WebSocket connections with real clients, including connection lifecycle management, message delivery, and error handling.</p>\n</li>\n<li><p><strong>Prometheus Integration</strong>: Test compatibility with Prometheus exposition format and scraping protocols.</p>\n</li>\n<li><p><strong>Load Balancer Integration</strong>: Test behavior when deployed behind load balancers, including proper session affinity for WebSocket connections.</p>\n</li>\n</ol>\n<h4 id=\"configuration-and-environment-integration\">Configuration and Environment Integration</h4>\n<p>Integration tests must validate that the system works correctly across different configuration scenarios and deployment environments:</p>\n<table>\n<thead>\n<tr>\n<th>Configuration Area</th>\n<th>Test Scenarios</th>\n<th>Validation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Storage Configuration</td>\n<td>Different retention policies, compaction intervals, data directories</td>\n<td>Verify policy enforcement and resource usage</td>\n</tr>\n<tr>\n<td>Network Configuration</td>\n<td>Various port assignments, timeout settings, connection limits</td>\n<td>Test connectivity and performance characteristics</td>\n</tr>\n<tr>\n<td>Alert Configuration</td>\n<td>Different evaluation intervals, notification channels, message templates</td>\n<td>Validate alert behavior and message formatting</td>\n</tr>\n<tr>\n<td>Security Configuration</td>\n<td>Authentication settings, access controls, encryption options</td>\n<td>Test security enforcement and proper access control</td>\n</tr>\n</tbody></table>\n<h4 id=\"performance-integration-testing\">Performance Integration Testing</h4>\n<p>Integration tests must validate system performance under realistic conditions:</p>\n<ol>\n<li><p><strong>Throughput Testing</strong>: Measure ingestion rates with realistic metric loads and verify that storage can keep up with sustained ingestion.</p>\n</li>\n<li><p><strong>Query Performance</strong>: Test query response times with realistic data volumes and concurrent query loads.</p>\n</li>\n<li><p><strong>Memory Usage</strong>: Monitor memory consumption during extended operation to identify memory leaks or excessive usage patterns.</p>\n</li>\n<li><p><strong>Resource Scaling</strong>: Test system behavior as resource usage approaches configured limits.</p>\n</li>\n</ol>\n<blockquote>\n<p>The key insight for integration testing is that it validates the system&#39;s behavior in realistic operational conditions, catching issues that unit tests cannot detect because they occur only when components interact with real dependencies.</p>\n</blockquote>\n<h4 id=\"common-integration-testing-pitfalls\">Common Integration Testing Pitfalls</h4>\n<p>⚠️ <strong>Pitfall: Insufficient Test Data Diversity</strong>\nIntegration tests often use simple, uniform test data that doesn&#39;t reflect the complexity and diversity of real-world metric data. This can miss edge cases that only occur with specific data patterns or label combinations.</p>\n<p><strong>Why it&#39;s wrong</strong>: Real-world metrics data has complex patterns including sparse data, high cardinality labels, irregular timestamps, and extreme values. Simple test data doesn&#39;t exercise the system under realistic conditions.</p>\n<p><strong>How to fix</strong>: Use representative test data that includes various metric types, cardinality patterns, and temporal characteristics. Generate test data that reflects actual usage patterns and includes edge cases like missing data points and counter resets.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Resource Cleanup Between Tests</strong>\nIntegration tests that don&#39;t properly clean up resources between test runs can have interdependencies that make test results non-deterministic or cause resource exhaustion.</p>\n<p><strong>Why it&#39;s wrong</strong>: Resource leaks or shared state between tests makes it difficult to isolate test failures and can cause tests to pass or fail based on execution order rather than actual correctness.</p>\n<p><strong>How to fix</strong>: Implement thorough cleanup procedures that reset all persistent state, close network connections, and release file handles. Use isolated test environments and verify resource cleanup as part of the test framework.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Error Injection</strong>\nIntegration tests often focus on happy-path scenarios and don&#39;t adequately test error conditions that can occur when components interact with real dependencies.</p>\n<p><strong>Why it&#39;s wrong</strong>: Many errors only manifest when components interact with real dependencies under stress or failure conditions. These scenarios are critical for system reliability but are difficult to test without proper error injection.</p>\n<p><strong>How to fix</strong>: Use chaos engineering techniques to inject failures into dependencies. Test network partitions, disk failures, memory pressure, and other realistic failure scenarios. Verify that the system maintains consistency and provides meaningful error messages during failures.</p>\n<h3 id=\"milestone-validation-checkpoints\">Milestone Validation Checkpoints</h3>\n<p>Milestone validation checkpoints provide structured verification that each major system capability works correctly before moving to the next development phase. These checkpoints bridge the gap between technical testing and user-visible functionality, ensuring that the system meets its acceptance criteria at each milestone.</p>\n<h4 id=\"milestone-1-metrics-collection-validation\">Milestone 1: Metrics Collection Validation</h4>\n<p>The first milestone validation focuses on verifying that the system can correctly ingest and store different types of metrics with proper validation and error handling.</p>\n<p><strong>Functional Verification Checkpoints</strong></p>\n<table>\n<thead>\n<tr>\n<th>Validation Area</th>\n<th>Test Procedure</th>\n<th>Expected Behavior</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Counter Ingestion</td>\n<td>Submit counter metrics with increasing values</td>\n<td>Values stored with proper monotonic validation</td>\n<td>Counter resets detected and handled correctly</td>\n</tr>\n<tr>\n<td>Gauge Processing</td>\n<td>Submit gauge metrics with fluctuating values</td>\n<td>All values stored without validation constraints</td>\n<td>Gauge updates reflected immediately in storage</td>\n</tr>\n<tr>\n<td>Histogram Handling</td>\n<td>Submit histogram metrics with bucket distributions</td>\n<td>Bucket counts and sums calculated correctly</td>\n<td>Percentile calculations work with stored data</td>\n</tr>\n<tr>\n<td>Label Validation</td>\n<td>Submit metrics with various label combinations</td>\n<td>Labels parsed, normalized, and indexed correctly</td>\n<td>Label cardinality limits enforced properly</td>\n</tr>\n<tr>\n<td>Prometheus Compatibility</td>\n<td>Scrape metrics from Prometheus exposition format</td>\n<td>Prometheus metrics ingested correctly</td>\n<td>Standard Prometheus metrics work without modification</td>\n</tr>\n</tbody></table>\n<p><strong>Performance Validation Checkpoints</strong></p>\n<p>The system must demonstrate acceptable performance characteristics under realistic load conditions:</p>\n<ol>\n<li><p><strong>Ingestion Throughput</strong>: Submit 10,000 metrics per second for 60 seconds and verify that all metrics are processed and stored within acceptable latency bounds (95th percentile &lt; 100ms).</p>\n</li>\n<li><p><strong>Memory Usage</strong>: Monitor memory consumption during ingestion and verify that it remains bounded even with sustained high ingestion rates.</p>\n</li>\n<li><p><strong>Storage Efficiency</strong>: Verify that stored data uses reasonable disk space and that compression ratios are within expected ranges.</p>\n</li>\n<li><p><strong>Cardinality Performance</strong>: Test ingestion performance with varying cardinality levels and verify that high cardinality doesn&#39;t cause unacceptable performance degradation.</p>\n</li>\n</ol>\n<p><strong>Error Handling Validation</strong></p>\n<p>The system must handle various error conditions gracefully:</p>\n<ol>\n<li><p><strong>Invalid Metric Rejection</strong>: Submit malformed metrics and verify that they are rejected with appropriate error messages without affecting valid metrics.</p>\n</li>\n<li><p><strong>Cardinality Explosion Prevention</strong>: Submit metrics that would exceed cardinality limits and verify that they are rejected with clear error messages.</p>\n</li>\n<li><p><strong>Network Failure Recovery</strong>: Simulate network failures during metric ingestion and verify that the system recovers properly when connectivity is restored.</p>\n</li>\n<li><p><strong>Disk Space Exhaustion</strong>: Fill the disk space and verify that the system provides clear error messages and doesn&#39;t corrupt existing data.</p>\n</li>\n</ol>\n<p><strong>Manual Verification Procedures</strong></p>\n<ol>\n<li>Start the metrics ingestion service with debug logging enabled</li>\n<li>Submit a variety of metrics using curl commands targeting the ingestion API</li>\n<li>Verify that metrics appear in the storage engine using direct storage queries</li>\n<li>Check logs for any error messages or warnings</li>\n<li>Monitor system resource usage during sustained ingestion</li>\n<li>Verify that Prometheus metrics exposition format works by configuring a Prometheus server to scrape the metrics endpoint</li>\n</ol>\n<h4 id=\"milestone-2-storage-amp-querying-validation\">Milestone 2: Storage &amp; Querying Validation</h4>\n<p>The second milestone validation focuses on verifying that the storage engine correctly persists data and that the query engine can retrieve and aggregate data accurately.</p>\n<p><strong>Storage Validation Checkpoints</strong></p>\n<table>\n<thead>\n<tr>\n<th>Validation Area</th>\n<th>Test Procedure</th>\n<th>Expected Behavior</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time-Series Persistence</td>\n<td>Store millions of samples across multiple series</td>\n<td>Data persisted with compression and indexing</td>\n<td>Query performance remains acceptable with large datasets</td>\n</tr>\n<tr>\n<td>Retention Policy Enforcement</td>\n<td>Configure retention policies and wait for enforcement</td>\n<td>Old data deleted according to policy</td>\n<td>Disk space reclaimed and queries continue working</td>\n</tr>\n<tr>\n<td>Compaction Operation</td>\n<td>Allow compaction to run and verify data integrity</td>\n<td>Data merged correctly with improved performance</td>\n<td>No data loss and improved query response times</td>\n</tr>\n<tr>\n<td>Range Query Accuracy</td>\n<td>Execute queries with various time ranges</td>\n<td>Correct data returned within specified ranges</td>\n<td>Results match expected values with proper timestamps</td>\n</tr>\n<tr>\n<td>Aggregation Correctness</td>\n<td>Execute sum, average, and percentile queries</td>\n<td>Aggregation results mathematically correct</td>\n<td>Aggregated values match manual calculations</td>\n</tr>\n</tbody></table>\n<p><strong>Query Engine Validation Checkpoints</strong></p>\n<p>The query engine must demonstrate accurate query processing across various query patterns:</p>\n<ol>\n<li><p><strong>Simple Metric Selection</strong>: Execute queries that select individual metrics and verify that results contain the expected data points.</p>\n</li>\n<li><p><strong>Label Filtering</strong>: Execute queries with label filters and verify that only matching series are returned.</p>\n</li>\n<li><p><strong>Time Range Specification</strong>: Execute queries with various time ranges and verify that results are properly bounded.</p>\n</li>\n<li><p><strong>Aggregation Functions</strong>: Test each aggregation function with known data sets and verify that results match expected calculations.</p>\n</li>\n<li><p><strong>Complex Query Processing</strong>: Execute queries that combine multiple operations and verify that results are correct.</p>\n</li>\n</ol>\n<p><strong>Performance Validation Checkpoints</strong></p>\n<ol>\n<li><p><strong>Query Response Times</strong>: Execute queries against large datasets and verify that response times remain within acceptable bounds (95th percentile &lt; 500ms).</p>\n</li>\n<li><p><strong>Concurrent Query Handling</strong>: Execute multiple concurrent queries and verify that the system maintains good performance without resource contention.</p>\n</li>\n<li><p><strong>Memory Management</strong>: Monitor memory usage during query execution and verify that it remains bounded even with complex queries.</p>\n</li>\n<li><p><strong>Cache Effectiveness</strong>: Verify that query caching improves performance for repeated queries without affecting result accuracy.</p>\n</li>\n</ol>\n<p><strong>Manual Verification Procedures</strong></p>\n<ol>\n<li>Populate the storage engine with a realistic dataset spanning several days</li>\n<li>Execute queries using the query API and verify results manually</li>\n<li>Monitor query execution times and resource usage</li>\n<li>Verify that compaction operations complete successfully</li>\n<li>Test retention policy enforcement by configuring short retention periods</li>\n<li>Verify that aggregation functions produce mathematically correct results</li>\n</ol>\n<h4 id=\"milestone-3-visualization-dashboard-validation\">Milestone 3: Visualization Dashboard Validation</h4>\n<p>The third milestone validation focuses on verifying that the dashboard system correctly displays metric data with real-time updates and proper user interaction.</p>\n<p><strong>Dashboard Functionality Validation</strong></p>\n<table>\n<thead>\n<tr>\n<th>Validation Area</th>\n<th>Test Procedure</th>\n<th>Expected Behavior</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Chart Rendering</td>\n<td>Configure panels with various chart types</td>\n<td>Charts display data accurately with proper formatting</td>\n<td>Visual representations match underlying data</td>\n</tr>\n<tr>\n<td>Configuration Persistence</td>\n<td>Save and load dashboard configurations</td>\n<td>Configurations persist correctly across sessions</td>\n<td>Dashboard state maintained after restart</td>\n</tr>\n<tr>\n<td>Auto-Refresh Functionality</td>\n<td>Enable auto-refresh with various intervals</td>\n<td>Charts update automatically without manual intervention</td>\n<td>Data freshness maintained according to refresh intervals</td>\n</tr>\n<tr>\n<td>Time Range Selection</td>\n<td>Use time range controls to change displayed periods</td>\n<td>Chart data updates to reflect selected time ranges</td>\n<td>Historical and real-time data displayed correctly</td>\n</tr>\n</tbody></table>\n<p><strong>Real-Time Update Validation</strong></p>\n<p>The dashboard must demonstrate proper real-time behavior:</p>\n<ol>\n<li><p><strong>WebSocket Connection Management</strong>: Verify that WebSocket connections are established correctly and remain stable during extended sessions.</p>\n</li>\n<li><p><strong>Live Data Updates</strong>: Submit new metrics and verify that dashboard panels update automatically without requiring page refresh.</p>\n</li>\n<li><p><strong>Subscription Management</strong>: Verify that panels only receive updates for metrics they are configured to display.</p>\n</li>\n<li><p><strong>Update Frequency Control</strong>: Test different auto-refresh intervals and verify that updates occur at the specified frequency.</p>\n</li>\n<li><p><strong>Error Handling</strong>: Test dashboard behavior when the backend service is unavailable and verify proper error messages and recovery.</p>\n</li>\n</ol>\n<p><strong>User Interface Validation</strong></p>\n<ol>\n<li><p><strong>Panel Configuration</strong>: Test the ability to create, modify, and delete dashboard panels with various configurations.</p>\n</li>\n<li><p><strong>Layout Management</strong>: Verify that panels can be arranged in different grid layouts and that configurations persist correctly.</p>\n</li>\n<li><p><strong>Chart Interaction</strong>: Test chart zoom, pan, and tooltip functionality to ensure proper user experience.</p>\n</li>\n<li><p><strong>Responsive Design</strong>: Test dashboard functionality across different screen sizes and browser configurations.</p>\n</li>\n</ol>\n<p><strong>Manual Verification Procedures</strong></p>\n<ol>\n<li>Access the dashboard web interface and create a new dashboard</li>\n<li>Configure panels with different chart types and metric queries</li>\n<li>Verify that charts display data correctly and update in real-time</li>\n<li>Test time range controls and auto-refresh functionality</li>\n<li>Save and reload dashboard configurations</li>\n<li>Test WebSocket connectivity and error handling by restarting backend services</li>\n</ol>\n<h4 id=\"milestone-4-alerting-system-validation\">Milestone 4: Alerting System Validation</h4>\n<p>The fourth milestone validation focuses on verifying that the alerting system correctly evaluates alert rules, manages alert states, and delivers notifications reliably.</p>\n<p><strong>Alert Rule Validation Checkpoints</strong></p>\n<table>\n<thead>\n<tr>\n<th>Validation Area</th>\n<th>Test Procedure</th>\n<th>Expected Behavior</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Threshold Evaluation</td>\n<td>Configure alerts with various threshold conditions</td>\n<td>Alerts fire when thresholds are exceeded for specified duration</td>\n<td>Alert timing accuracy within evaluation interval tolerance</td>\n</tr>\n<tr>\n<td>State Transition Management</td>\n<td>Submit metric data that triggers state changes</td>\n<td>Alert states transition correctly through the state machine</td>\n<td>Proper notifications sent for each state transition</td>\n</tr>\n<tr>\n<td>Duration-Based Conditions</td>\n<td>Configure alerts with different duration requirements</td>\n<td>Alerts only fire after conditions persist for specified time</td>\n<td>Flapping alerts properly suppressed by duration requirements</td>\n</tr>\n<tr>\n<td>Multi-Channel Notification</td>\n<td>Configure alerts with multiple notification channels</td>\n<td>Notifications delivered to all configured channels</td>\n<td>Channel-specific message formatting applied correctly</td>\n</tr>\n</tbody></table>\n<p><strong>Notification System Validation</strong></p>\n<p>The notification system must demonstrate reliable delivery across different channels:</p>\n<ol>\n<li><p><strong>Email Integration</strong>: Configure email notifications and verify that alert messages are delivered with proper formatting and timing.</p>\n</li>\n<li><p><strong>Slack Integration</strong>: Configure Slack notifications and verify that messages appear in designated channels with appropriate urgency indicators.</p>\n</li>\n<li><p><strong>Webhook Delivery</strong>: Configure webhook notifications and verify that HTTP requests are sent with correct payloads and retry logic.</p>\n</li>\n<li><p><strong>Message Templating</strong>: Test custom message templates and verify that alert data is properly substituted into message content.</p>\n</li>\n<li><p><strong>Rate Limiting</strong>: Test notification rate limiting to verify that notification storms are properly controlled.</p>\n</li>\n</ol>\n<p><strong>Alert Lifecycle Validation</strong></p>\n<ol>\n<li><p><strong>Alert Firing</strong>: Submit metric data that exceeds thresholds and verify that alerts transition to firing state within the expected timeframe.</p>\n</li>\n<li><p><strong>Alert Resolution</strong>: Submit metric data that resolves conditions and verify that alerts transition to resolved state with appropriate notifications.</p>\n</li>\n<li><p><strong>Alert Persistence</strong>: Restart the alerting system and verify that alert states are properly restored from persistence storage.</p>\n</li>\n<li><p><strong>Silence Management</strong>: Test alert silencing functionality and verify that notifications are suppressed during maintenance windows.</p>\n</li>\n</ol>\n<p><strong>Manual Verification Procedures</strong></p>\n<ol>\n<li>Configure alert rules targeting test metrics with low threshold values</li>\n<li>Submit metric data that triggers alert conditions</li>\n<li>Verify that alerts fire and notifications are delivered to configured channels</li>\n<li>Submit metric data that resolves alert conditions</li>\n<li>Verify that resolution notifications are sent</li>\n<li>Test alert silencing and verify that notifications are suppressed</li>\n<li>Restart the alerting system and verify that alert states persist correctly</li>\n</ol>\n<blockquote>\n<p>The milestone validation checkpoints serve as quality gates that prevent progression to the next development phase until the current capabilities are fully functional and meet their acceptance criteria. This approach ensures that the system maintains quality throughout the development process rather than deferring quality concerns to the end.</p>\n</blockquote>\n<h4 id=\"comprehensive-system-validation\">Comprehensive System Validation</h4>\n<p>After completing all individual milestones, a comprehensive system validation verifies that all components work together correctly in realistic scenarios:</p>\n<ol>\n<li><p><strong>End-to-End Workflow</strong>: Execute a complete workflow from metric ingestion through dashboard visualization and alert notification.</p>\n</li>\n<li><p><strong>Load Testing</strong>: Subject the system to realistic load conditions and verify that all components maintain acceptable performance.</p>\n</li>\n<li><p><strong>Failure Recovery</strong>: Simulate various failure scenarios and verify that the system recovers properly with minimal data loss.</p>\n</li>\n<li><p><strong>Operational Readiness</strong>: Verify that the system provides adequate monitoring, logging, and diagnostic capabilities for operational deployment.</p>\n</li>\n</ol>\n<p><strong>System Integration Scenarios</strong></p>\n<table>\n<thead>\n<tr>\n<th>Scenario</th>\n<th>Test Procedure</th>\n<th>Expected Outcome</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High-Volume Ingestion</td>\n<td>Submit sustained high metric volume</td>\n<td>System maintains performance without data loss</td>\n<td>Monitor throughput and verify data completeness</td>\n</tr>\n<tr>\n<td>Complex Dashboard Usage</td>\n<td>Multiple users with different dashboards</td>\n<td>All dashboards update correctly without interference</td>\n<td>Verify independent panel updates and resource usage</td>\n</tr>\n<tr>\n<td>Alert Storm Handling</td>\n<td>Trigger multiple simultaneous alerts</td>\n<td>Notifications delivered reliably without system overload</td>\n<td>Verify notification delivery and system stability</td>\n</tr>\n<tr>\n<td>Extended Operation</td>\n<td>Run system continuously for extended period</td>\n<td>No memory leaks or performance degradation</td>\n<td>Monitor resource usage trends over time</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides practical guidance for implementing the comprehensive testing strategy outlined above, with specific focus on Go-based testing tools and patterns that support both unit and integration testing requirements.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Testing Layer</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Recommended Choice</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing Framework</td>\n<td><code>testing</code> package with table-driven tests</td>\n<td><code>testify</code> suite with assertions and mocks</td>\n<td><code>testing</code> package (built-in simplicity)</td>\n</tr>\n<tr>\n<td>Mocking Strategy</td>\n<td>Manual interface mocks</td>\n<td><code>testify/mock</code> with code generation</td>\n<td>Manual mocks (explicit dependencies)</td>\n</tr>\n<tr>\n<td>Integration Testing</td>\n<td>Docker Compose for dependencies</td>\n<td>Kubernetes test environments</td>\n<td>Docker Compose (simpler setup)</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Simple goroutine-based generators</td>\n<td><code>k6</code> or <code>vegeta</code> for sophisticated scenarios</td>\n<td><code>k6</code> (better reporting and scripting)</td>\n</tr>\n<tr>\n<td>Test Data Management</td>\n<td>Hardcoded test fixtures</td>\n<td><code>gofakeit</code> for generated test data</td>\n<td>Combination of both approaches</td>\n</tr>\n<tr>\n<td>Assertion Library</td>\n<td>Standard <code>if</code> statements with <code>t.Error</code></td>\n<td><code>testify/assert</code> with rich assertions</td>\n<td><code>testify/assert</code> (better error messages)</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<p>The testing infrastructure should be organized to support both unit and integration testing while maintaining clear separation of concerns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    server/main.go\n    server/main_test.go        ← integration tests for server startup\n  internal/\n    ingestion/\n      ingester.go\n      ingester_test.go         ← unit tests for ingestion logic\n      ingester_integration_test.go ← integration tests with real storage\n    storage/\n      engine.go\n      engine_test.go           ← unit tests for storage operations\n      engine_integration_test.go   ← integration tests with file system\n    query/\n      processor.go\n      processor_test.go        ← unit tests for query processing\n      processor_integration_test.go ← integration tests with storage\n    alerting/\n      evaluator.go\n      evaluator_test.go        ← unit tests for alert logic\n      evaluator_integration_test.go ← integration tests with notifications\n  test/\n    fixtures/                  ← shared test data and utilities\n      metrics.go               ← test metric generators\n      storage.go               ← test storage helpers\n      time.go                  ← time manipulation utilities\n    integration/               ← system-level integration tests\n      end_to_end_test.go       ← complete workflow tests\n      performance_test.go      ← load and performance tests\n    mocks/                     ← interface implementations for testing\n      storage_mock.go          ← storage interface mock\n      notifier_mock.go         ← notification interface mock\n  docker/\n    test-compose.yml           ← Docker composition for integration testing\n  Makefile                     ← test execution automation</code></pre></div>\n\n<h4 id=\"testing-infrastructure-components\">Testing Infrastructure Components</h4>\n<p><strong>Mock Storage Engine Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: test/mocks/storage_mock.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> mocks</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/metrics-system/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MockTimeSeriesStorage provides a controllable storage implementation for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MockTimeSeriesStorage</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    samples    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#6A737D\">  // seriesID -> samples</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    series     </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesInfo</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    writeErr   </span><span style=\"color:#F97583\">error</span><span style=\"color:#6A737D\">                        // error to return on writes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryErr   </span><span style=\"color:#F97583\">error</span><span style=\"color:#6A737D\">                        // error to return on queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    writeDelay </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\">                // simulated write latency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryDelay </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#6A737D\">                // simulated query latency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu         </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Operation tracking for verification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    WriteCalls []</span><span style=\"color:#B392F0\">WriteCall</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryCalls []</span><span style=\"color:#B392F0\">QueryCall</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> WriteCall</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Samples   []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Context   </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> QueryCall</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SeriesID   </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StartTime  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    EndTime    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Context    </span><span style=\"color:#B392F0\">context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMockTimeSeriesStorage creates a new mock storage instance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMockTimeSeriesStorage</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeSeriesStorage</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MockTimeSeriesStorage</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        samples: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        series:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesInfo</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteSamples implements the TimeSeriesStorage interface</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeSeriesStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Record the write call for verification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if write error should be returned</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Simulate write delay if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Store samples in the mock storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Update series information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Sleep for delay simulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Group samples by series ID for storage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryRange implements the TimeSeriesStorage interface  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeSeriesStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">QueryRange</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">start</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">end</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Record the query call for verification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check if query error should be returned</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Simulate query delay if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Filter samples by time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return matching samples in chronological order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sort.Slice for chronological ordering</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetWriteError configures the mock to return errors on write operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeSeriesStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetWriteError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">err</span><span style=\"color:#F97583\"> error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.writeErr </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> err</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// SetWriteDelay configures simulated write latency</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeSeriesStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">SetWriteDelay</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">delay</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.writeDelay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetWriteCalls returns all recorded write operations for verification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeSeriesStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetWriteCalls</span><span style=\"color:#E1E4E8\">() []</span><span style=\"color:#B392F0\">WriteCall</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    calls </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#B392F0\">WriteCall</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(m.WriteCalls))</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    copy</span><span style=\"color:#E1E4E8\">(calls, m.WriteCalls)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> calls</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Reset clears all stored data and operation history</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">m </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MockTimeSeriesStorage</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Reset</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> m.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.samples </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">][]</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.series </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">SeriesInfo</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.WriteCalls </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.QueryCalls </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.writeErr </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.queryErr </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.writeDelay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    m.queryDelay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Test Fixture Generation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: test/fixtures/metrics.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> fixtures</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">math/rand</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/metrics-system/internal/storage</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// MetricGenerator provides utilities for creating test metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> MetricGenerator</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    rand   </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">rand</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Rand</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    labels []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewMetricGenerator creates a generator with deterministic randomness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewMetricGenerator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">seed</span><span style=\"color:#F97583\"> int64</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricGenerator</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">MetricGenerator</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        rand: rand.</span><span style=\"color:#B392F0\">New</span><span style=\"color:#E1E4E8\">(rand.</span><span style=\"color:#B392F0\">NewSource</span><span style=\"color:#E1E4E8\">(seed)),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateCounterSamples creates realistic counter metric samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateCounterSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metricName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate number of samples based on duration and interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate monotonically increasing counter values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add realistic noise and occasional resets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create samples with proper timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return samples in chronological order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Counter values should generally increase over time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Occasional resets simulate process restarts</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateGaugeSamples creates realistic gauge metric samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateGaugeSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metricName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate number of samples based on duration and interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate gauge values with realistic fluctuation patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Add seasonal patterns and random walk behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Create samples with proper timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return samples in chronological order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Gauge values can increase or decrease freely</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use sine waves and random walk for realistic patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateHistogramSamples creates realistic histogram metric samples</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateHistogramSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">metricName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">labels</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">buckets</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate number of samples based on duration and interval</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate observation counts for each bucket</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Ensure bucket counts are monotonically increasing across buckets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Generate count and sum values consistent with buckets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Create samples for all histogram series (buckets, count, sum)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Bucket counts must be cumulative (each bucket includes previous buckets)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Generate separate samples for each bucket, plus _count and _sum series</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateHighCardinalityLabels creates label combinations for cardinality testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">g </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricGenerator</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateHighCardinalityLabels</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">baseLabels</span><span style=\"color:#B392F0\"> storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cardinalityLevel</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">) []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Labels</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Start with base labels as foundation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate additional label dimensions based on cardinality level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Use realistic label names and values (service, instance, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Ensure label combinations are unique</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return label sets sorted for deterministic testing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Common high-cardinality labels include instance_id, user_id, request_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use consistent naming patterns for realistic scenarios</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Integration Test Harness</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: test/integration/harness.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> integration</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">path/filepath</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/metrics-system/internal/config</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/metrics-system/internal/coordinator</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// TestHarness provides a complete system instance for integration testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> TestHarness</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Config      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">config</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Config</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Coordinator </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">coordinator</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ComponentCoordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    TempDir     </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    BasePort    </span><span style=\"color:#F97583\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Component references for direct testing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Storage   </span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TimeSeriesStorage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Query     </span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryProcessor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Dashboard </span><span style=\"color:#B392F0\">dashboard</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">DashboardServer</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Alerting  </span><span style=\"color:#B392F0\">alerting</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">AlertEvaluator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewTestHarness creates a complete system instance for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewTestHarness</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestHarness</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Create temporary directory for test data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Generate unique port numbers to avoid conflicts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Create test configuration with appropriate settings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Initialize component coordinator with test config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Start all system components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 6: Register cleanup function with testing.T</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use t.TempDir() for automatic cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use port 0 for automatic port assignment or find free ports</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use t.Cleanup() to ensure proper shutdown</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// IngestTestMetrics submits metrics through the ingestion API</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestHarness</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">IngestTestMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metrics</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Metric</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Format metrics according to ingestion API requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Submit metrics via HTTP POST to ingestion endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify successful response codes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Wait for metrics to be processed and stored</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return any errors encountered during ingestion</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use http.Client for API calls</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Add retry logic for transient failures</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// QueryMetrics executes queries against the system</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestHarness</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">QueryMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TimeRange</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">query</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">QueryResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Format query according to query API requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Submit query via HTTP GET to query endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Parse response into QueryResult structure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Validate response format and content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return parsed results or error</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use URL encoding for query parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Handle different result types (time series, scalar, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WaitForAlert waits for an alert to reach the specified state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestHarness</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WaitForAlert</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">alertID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">expectedState</span><span style=\"color:#B392F0\"> alerting</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">AlertState</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Set up polling loop with context timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Query alert status via alerting API</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check if alert has reached expected state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return success when state matches or timeout when exceeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Provide informative error messages for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use time.Ticker for regular polling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Include current state in timeout error messages</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GetSystemMetrics retrieves internal system metrics for verification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestHarness</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetSystemMetrics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">float64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Query system metrics endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Parse metrics in Prometheus exposition format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Extract key metrics for verification (ingestion rate, storage usage, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Return metrics as key-value map</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Handle parsing errors gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Look for metrics like metrics_ingested_total, storage_bytes_used</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use Prometheus client library for parsing if available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Shutdown gracefully stops all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">TestHarness</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">Shutdown</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Stop component coordinator gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Wait for all components to shutdown completely</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Clean up temporary files and directories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Close any open network connections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return any errors encountered during shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Use context timeout to prevent hanging shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Hint: Log shutdown progress for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-implementation\">Milestone Checkpoint Implementation</h4>\n<p><strong>Milestone 1 Validation Test</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// File: test/integration/milestone1_test.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> integration</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/stretchr/testify/assert</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/stretchr/testify/require</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">github.com/metrics-system/test/fixtures</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> TestMilestone1_MetricsCollection</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    harness </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> NewTestHarness</span><span style=\"color:#E1E4E8\">(t)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ctx </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Counter Metrics Ingestion\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Generate counter metrics with increasing values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Submit metrics through ingestion API</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Query storage directly to verify persistence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Validate that counter semantics are enforced</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Check that monotonic increases are preserved</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        generator </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> fixtures.</span><span style=\"color:#B392F0\">NewMetricGenerator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">12345</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Implementation continues with detailed test steps...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Gauge Metrics Processing\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Generate gauge metrics with fluctuating values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Submit metrics and verify immediate storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Validate that gauge values can increase and decrease</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Check that latest values are queryable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Verify proper label handling and indexing</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Implementation continues with detailed test steps...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Histogram Metrics Handling\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Generate histogram metrics with proper bucket distributions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Submit histograms and verify bucket processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Validate bucket count calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Check that sum and count values are correct</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Verify percentile calculation capability</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Implementation continues with detailed test steps...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Label Validation and Cardinality\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Test label parsing and normalization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Submit metrics with high cardinality labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Verify cardinality limits are enforced</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Check that valid labels are processed correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Validate error messages for invalid labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Implementation continues with detailed test steps...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    t.</span><span style=\"color:#B392F0\">Run</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Performance Validation\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 1: Generate high-volume metric load</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 2: Monitor ingestion rates and latency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 3: Verify memory usage remains bounded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 4: Check that system maintains responsiveness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // TODO 5: Validate that no metrics are lost during load</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        startTime </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Generate 10,000 metrics over 60 seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metricsCount </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 10000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        duration </span><span style=\"color:#F97583\">:=</span><span style=\"color:#79B8FF\"> 60</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Implementation continues with performance testing...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// validateCounterSemantics verifies counter-specific behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> validateCounterSemantics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">harness</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">TestHarness</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">storage</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify that counter values are monotonically increasing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check for proper counter reset detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Validate that rate calculations work correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Ensure proper handling of counter overflow</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Verify storage optimization for counter data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Implementation continues with validation logic...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// validateIngestionPerformance checks performance characteristics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> validateIngestionPerformance</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">t</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">testing</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">T</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">harness</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">TestHarness</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metricsSubmitted</span><span style=\"color:#F97583\"> int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">duration</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Calculate actual ingestion rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Check that 95th percentile latency is acceptable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Verify memory usage didn't spike excessively</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Ensure no metrics were dropped or lost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Validate system remained responsive during load</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    systemMetrics, err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> harness.</span><span style=\"color:#B392F0\">GetSystemMetrics</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    require.</span><span style=\"color:#B392F0\">NoError</span><span style=\"color:#E1E4E8\">(t, err)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actualRate </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> systemMetrics[</span><span style=\"color:#9ECBFF\">\"metrics_ingested_total\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> duration.</span><span style=\"color:#B392F0\">Seconds</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expectedMinRate </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> float64</span><span style=\"color:#E1E4E8\">(metricsSubmitted) </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> duration.</span><span style=\"color:#B392F0\">Seconds</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.95</span><span style=\"color:#6A737D\"> // Allow 5% tolerance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    assert.</span><span style=\"color:#B392F0\">GreaterOrEqual</span><span style=\"color:#E1E4E8\">(t, actualRate, expectedMinRate, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"Ingestion rate </span><span style=\"color:#79B8FF\">%f</span><span style=\"color:#9ECBFF\">/sec below expected minimum </span><span style=\"color:#79B8FF\">%f</span><span style=\"color:#9ECBFF\">/sec\"</span><span style=\"color:#E1E4E8\">, actualRate, expectedMinRate)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Continue with additional performance validations...</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p>This comprehensive testing strategy provides multiple layers of validation that ensure system correctness, performance, and reliability throughout development. The combination of unit tests, integration tests, and milestone checkpoints creates a robust quality assurance framework that catches issues early and validates that the system meets its requirements under realistic conditions.</p>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (debugging techniques are essential throughout the entire metrics and alerting system lifecycle, from initial development through production operations)</p>\n</blockquote>\n<h3 id=\"the-detective-work-metaphor-understanding-system-debugging\">The Detective Work Metaphor: Understanding System Debugging</h3>\n<p>Think of debugging a metrics system like being a detective investigating a complex case. You have multiple witnesses (log files), physical evidence (metric data), and crime scene locations (system components). The key is knowing which evidence to collect, how to interpret the clues, and following a systematic investigation process. Just as a detective starts with the most obvious suspects before diving into complex conspiracy theories, we debug metrics systems by checking the most common failure modes first—network connectivity, configuration errors, and resource constraints—before investigating more subtle issues like timing races or cardinality explosions.</p>\n<p>Unlike debugging a simple web application where problems are often isolated to a single request-response cycle, metrics systems involve continuous data flows with multiple asynchronous components. A single problem can manifest symptoms in multiple locations: metrics might disappear at ingestion, queries might return partial results, dashboards might show stale data, or alerts might fire unexpectedly. The challenge is connecting these distributed symptoms back to their root cause while the system continues operating.</p>\n<h3 id=\"common-problem-patterns-in-metrics-systems\">Common Problem Patterns in Metrics Systems</h3>\n<p>Before diving into specific debugging techniques, it&#39;s important to understand the common failure patterns that occur in metrics and alerting systems. These patterns help guide your investigation and prioritize which components to examine first.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Pattern</th>\n<th>Typical Symptoms</th>\n<th>Investigation Priority</th>\n<th>Common Root Causes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Pipeline Failures</td>\n<td>Missing metrics, partial ingestion, query timeouts</td>\n<td>High</td>\n<td>Network issues, storage full, validation errors</td>\n</tr>\n<tr>\n<td>Resource Exhaustion</td>\n<td>Slow queries, high memory usage, ingestion backlog</td>\n<td>High</td>\n<td>Cardinality explosion, insufficient capacity, memory leaks</td>\n</tr>\n<tr>\n<td>Configuration Errors</td>\n<td>Alerts not firing, incorrect dashboards, connection failures</td>\n<td>Medium</td>\n<td>Typos, wrong endpoints, invalid credentials</td>\n</tr>\n<tr>\n<td>Timing and Synchronization</td>\n<td>Intermittent failures, race conditions, inconsistent behavior</td>\n<td>Medium</td>\n<td>Clock skew, concurrent access, async processing</td>\n</tr>\n<tr>\n<td>State Corruption</td>\n<td>Inconsistent data, crashed components, recovery failures</td>\n<td>Low</td>\n<td>Hardware failures, bugs in storage layer, power loss</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: Most production issues in metrics systems stem from resource exhaustion or configuration problems rather than complex algorithmic bugs. Start your investigation with resource monitoring and configuration validation before diving into code-level debugging.</p>\n</blockquote>\n<p>The debugging approach for metrics systems follows a structured methodology:</p>\n<ol>\n<li><strong>Establish the Failure Scope</strong>: Determine whether the problem affects all metrics, specific metric types, certain time ranges, or particular dashboard panels</li>\n<li><strong>Trace the Data Path</strong>: Follow metric data from ingestion through storage, querying, and visualization to identify where the pipeline breaks</li>\n<li><strong>Correlate Symptoms Across Components</strong>: Check if problems in one component are causing cascading failures in dependent systems</li>\n<li><strong>Validate System Resources</strong>: Ensure adequate CPU, memory, disk space, and network capacity for current load</li>\n<li><strong>Reproduce with Controlled Input</strong>: Use synthetic metrics and queries to isolate variables and confirm fixes</li>\n</ol>\n<h3 id=\"ingestion-issues\">Ingestion Issues</h3>\n<p>The ingestion engine is the entry point for all metric data, making it a common source of problems when metrics fail to appear in dashboards or alerts don&#39;t fire as expected. Ingestion issues typically manifest as missing metrics, partial data ingestion, or performance degradation during high-volume periods.</p>\n<h4 id=\"metrics-not-appearing\">Metrics Not Appearing</h4>\n<p>The most common ingestion problem is metrics disappearing somewhere between client submission and storage persistence. This creates a frustrating debugging experience because the problem could exist at multiple layers of the ingestion pipeline.</p>\n<p><strong>Systematic Investigation Approach:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Investigation Step</th>\n<th>Commands/Actions</th>\n<th>What to Look For</th>\n<th>Next Step If Found</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Verify Network Connectivity</td>\n<td><code>curl -X POST /api/metrics -d @test-metric.json</code></td>\n<td>HTTP response code, connection timeouts</td>\n<td>Check firewall rules, DNS resolution</td>\n</tr>\n<tr>\n<td>Check Ingestion Logs</td>\n<td><code>grep &quot;metric ingestion&quot; /var/log/metrics.log</code></td>\n<td>Validation errors, parsing failures</td>\n<td>Review metric format against schema</td>\n</tr>\n<tr>\n<td>Validate Metric Format</td>\n<td>Compare against <code>Metric</code> struct requirements</td>\n<td>Missing fields, invalid types, malformed JSON</td>\n<td>Fix client-side metric serialization</td>\n</tr>\n<tr>\n<td>Examine Storage Writes</td>\n<td>Monitor <code>WriteSamples</code> method calls and errors</td>\n<td>Write failures, disk space errors</td>\n<td>Check storage configuration and capacity</td>\n</tr>\n<tr>\n<td>Verify Label Cardinality</td>\n<td>Check cardinality manager logs and metrics</td>\n<td>High-cardinality rejections, quota exceeded</td>\n<td>Reduce label diversity or increase limits</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Ingestion Pipeline Observability Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: When metrics don&#39;t appear, determining where they&#39;re lost requires visibility into each pipeline stage</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Per-component error logging only</li>\n<li>End-to-end tracing with correlation IDs</li>\n<li>Pipeline metrics with stage-specific counters</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach combining pipeline metrics with correlation IDs for complex failures</li>\n<li><strong>Rationale</strong>: Pipeline metrics provide immediate visibility into healthy vs. failing stages, while correlation IDs enable detailed investigation of specific metric flows when needed</li>\n<li><strong>Consequences</strong>: Enables rapid problem isolation but requires additional instrumentation overhead</li>\n</ul>\n</blockquote>\n<p><strong>⚠️ Pitfall: Assuming Network Problems Are External</strong></p>\n<p>Many developers assume that HTTP 200 responses mean successful metric ingestion, but the ingestion pipeline continues after the HTTP response. The <code>MetricsIngester</code> might accept the request but later reject metrics during validation or storage. Always check the complete pipeline, not just the HTTP layer.</p>\n<p><strong>Common Validation Failures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Validation Error</th>\n<th>Symptom</th>\n<th>Debug Command</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Invalid Metric Name</td>\n<td><code>ValidationErrors</code> in logs with &quot;invalid metric name&quot;</td>\n<td>Check metric name against regex pattern</td>\n<td>Use alphanumeric names with underscores</td>\n</tr>\n<tr>\n<td>Label Cardinality Exceeded</td>\n<td><code>CardinalityManager</code> rejection logs</td>\n<td>Review unique label combinations</td>\n<td>Reduce label diversity or increase limits</td>\n</tr>\n<tr>\n<td>Timestamp Out of Range</td>\n<td>&quot;timestamp too old/new&quot; errors</td>\n<td>Compare timestamps to retention policy</td>\n<td>Adjust timestamp or retention configuration</td>\n</tr>\n<tr>\n<td>Malformed Sample Values</td>\n<td>&quot;invalid float64&quot; parsing errors</td>\n<td>Validate sample values are numeric</td>\n<td>Fix client serialization of infinity/NaN</td>\n</tr>\n<tr>\n<td>Missing Required Labels</td>\n<td>&quot;required label missing&quot; validation errors</td>\n<td>Check against label requirements config</td>\n<td>Add missing labels to metric submissions</td>\n</tr>\n</tbody></table>\n<h4 id=\"cardinality-explosions\">Cardinality Explosions</h4>\n<p>Cardinality explosions occur when metrics contain too many unique label combinations, overwhelming storage and query performance. This is one of the most dangerous ingestion problems because it can quickly consume all available system resources.</p>\n<p><strong>Detection Strategies:</strong></p>\n<p>The <code>CardinalityManager</code> component tracks unique label combinations and provides early warning when cardinality approaches dangerous levels. Monitor these metrics continuously:</p>\n<table>\n<thead>\n<tr>\n<th>Cardinality Metric</th>\n<th>Healthy Range</th>\n<th>Warning Threshold</th>\n<th>Critical Threshold</th>\n<th>Response Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unique Series Count</td>\n<td>&lt; 100K per metric</td>\n<td>&gt; 500K per metric</td>\n<td>&gt; 1M per metric</td>\n<td>Enable label filtering</td>\n</tr>\n<tr>\n<td>New Series Rate</td>\n<td>&lt; 1K/minute</td>\n<td>&gt; 10K/minute</td>\n<td>&gt; 50K/minute</td>\n<td>Investigate metric source</td>\n</tr>\n<tr>\n<td>Label Combination Diversity</td>\n<td>&lt; 10 per label</td>\n<td>&gt; 100 per label</td>\n<td>&gt; 1000 per label</td>\n<td>Review label design</td>\n</tr>\n<tr>\n<td>Memory Usage per Series</td>\n<td>&lt; 1KB average</td>\n<td>&gt; 5KB average</td>\n<td>&gt; 10KB average</td>\n<td>Optimize storage format</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: Cardinality problems are exponential—each new label dimension multiplies the potential series count. A metric with 10 possible values for 3 labels creates 1,000 unique series (10³), but adding one more label with 10 values creates 10,000 series (10⁴).</p>\n</blockquote>\n<p><strong>Investigation Process:</strong></p>\n<ol>\n<li><strong>Identify High-Cardinality Metrics</strong>: Query the series index to find metrics with excessive unique combinations</li>\n<li><strong>Analyze Label Distribution</strong>: Examine which labels contribute most to cardinality growth</li>\n<li><strong>Review Metric Design</strong>: Determine if high-cardinality labels are necessary for observability goals</li>\n<li><strong>Implement Cardinality Controls</strong>: Configure limits, sampling, or label filtering to manage growth</li>\n</ol>\n<p><strong>Common High-Cardinality Label Patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Problematic Pattern</th>\n<th>Example</th>\n<th>Why It&#39;s Dangerous</th>\n<th>Better Alternative</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request IDs in Labels</td>\n<td><code>request_id=&quot;uuid-123&quot;</code></td>\n<td>Every request creates new series</td>\n<td>Use request ID in annotations or separate tracing</td>\n</tr>\n<tr>\n<td>Timestamp Labels</td>\n<td><code>hour=&quot;2024-01-15-14&quot;</code></td>\n<td>Creates series per time bucket</td>\n<td>Use native timestamp field instead</td>\n</tr>\n<tr>\n<td>User IDs in Labels</td>\n<td><code>user_id=&quot;user123&quot;</code></td>\n<td>Series grows with user base</td>\n<td>Aggregate by user groups or roles</td>\n</tr>\n<tr>\n<td>Full URLs as Labels</td>\n<td><code>url=&quot;/api/users/123/profile&quot;</code></td>\n<td>Creates series per URL variant</td>\n<td>Extract URL patterns or endpoints only</td>\n</tr>\n<tr>\n<td>Random Values</td>\n<td><code>session_id=&quot;random123&quot;</code></td>\n<td>Unbounded series growth</td>\n<td>Remove or use in annotations</td>\n</tr>\n</tbody></table>\n<h4 id=\"performance-problems-during-high-volume\">Performance Problems During High Volume</h4>\n<p>Ingestion performance problems typically manifest as increased latency, memory growth, or eventually request timeouts when the system cannot keep up with incoming metric volume.</p>\n<p><strong>Performance Monitoring Approach:</strong></p>\n<p>The <code>MetricsIngester</code> component should expose these performance metrics for continuous monitoring:</p>\n<table>\n<thead>\n<tr>\n<th>Performance Metric</th>\n<th>Description</th>\n<th>Healthy Baseline</th>\n<th>Investigation Trigger</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Ingestion Throughput</td>\n<td>Metrics processed per second</td>\n<td>&gt; 10K metrics/sec</td>\n<td>&lt; 1K metrics/sec sustained</td>\n</tr>\n<tr>\n<td>Ingestion Latency P99</td>\n<td>Time from request to storage write</td>\n<td>&lt; 100ms</td>\n<td>&gt; 500ms</td>\n</tr>\n<tr>\n<td>Validation Queue Depth</td>\n<td>Pending metrics awaiting validation</td>\n<td>&lt; 1000</td>\n<td>&gt; 10000</td>\n</tr>\n<tr>\n<td>Storage Write Latency</td>\n<td>Time for <code>WriteSamples</code> to complete</td>\n<td>&lt; 50ms</td>\n<td>&gt; 200ms</td>\n</tr>\n<tr>\n<td>Memory Usage per Batch</td>\n<td>Memory consumed processing metric batches</td>\n<td>&lt; 10MB per batch</td>\n<td>&gt; 100MB per batch</td>\n</tr>\n</tbody></table>\n<p><strong>Scalability Bottleneck Analysis:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Bottleneck Location</th>\n<th>Symptoms</th>\n<th>Root Cause Analysis</th>\n<th>Scaling Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Request Processing</td>\n<td>High request latency, connection timeouts</td>\n<td>Single-threaded request handlers</td>\n<td>Increase <code>GOMAXPROCS</code>, use connection pooling</td>\n</tr>\n<tr>\n<td>Metric Validation</td>\n<td>CPU spikes during validation, processing delays</td>\n<td>Complex validation rules, inefficient regex</td>\n<td>Optimize validation algorithms, cache compiled patterns</td>\n</tr>\n<tr>\n<td>Label Processing</td>\n<td>Memory growth with label-heavy metrics</td>\n<td>Inefficient label storage, no deduplication</td>\n<td>Implement label interning, optimize <code>Labels</code> type</td>\n</tr>\n<tr>\n<td>Storage Writes</td>\n<td>Write latency increases, disk I/O saturation</td>\n<td>Synchronous writes, no write batching</td>\n<td>Implement write batching, async persistence</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>GC pauses, out-of-memory errors</td>\n<td>Large object allocations, no object pooling</td>\n<td>Use sync.Pool for metric objects, optimize GC tuning</td>\n</tr>\n</tbody></table>\n<p><strong>⚠️ Pitfall: Optimizing the Wrong Component</strong></p>\n<p>Performance problems often appear in one component but originate in another. For example, slow query performance might be caused by inefficient ingestion creating poorly structured storage blocks, not the query engine itself. Always profile the entire pipeline before optimizing individual components.</p>\n<h3 id=\"storage-and-query-issues\">Storage and Query Issues</h3>\n<p>Storage and query problems are particularly challenging to debug because they often involve subtle interactions between the storage engine&#39;s complex components: the write-ahead log, block manager, compaction engine, and query processor.</p>\n<h4 id=\"slow-query-performance\">Slow Query Performance</h4>\n<p>Query performance problems typically stem from inefficient storage layouts, missing indexes, or queries that scan excessive amounts of data. The challenge is determining whether the problem is in the query itself, the underlying storage structure, or resource constraints.</p>\n<p><strong>Query Performance Investigation Framework:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Investigation Phase</th>\n<th>Diagnostic Actions</th>\n<th>Key Metrics to Examine</th>\n<th>Resolution Path</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Analysis</td>\n<td>Parse query complexity, time range scope</td>\n<td>Series count, sample count, aggregation types</td>\n<td>Optimize query structure or add filters</td>\n</tr>\n<tr>\n<td>Index Utilization</td>\n<td>Check series selection efficiency</td>\n<td>Index hit ratio, series scan count</td>\n<td>Add indexes or improve label filtering</td>\n</tr>\n<tr>\n<td>Storage Layout</td>\n<td>Examine block structure and compaction</td>\n<td>Block count, block time ranges, overlap ratio</td>\n<td>Trigger manual compaction or adjust policies</td>\n</tr>\n<tr>\n<td>Resource Constraints</td>\n<td>Monitor CPU, memory, I/O during queries</td>\n<td>CPU utilization, memory allocation, disk reads</td>\n<td>Scale resources or implement query limits</td>\n</tr>\n</tbody></table>\n<p><strong>Common Query Performance Anti-Patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Anti-Pattern</th>\n<th>Example Query Symptom</th>\n<th>Storage Impact</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unbounded Time Range</td>\n<td>Query spans months of data</td>\n<td>Scans thousands of storage blocks</td>\n<td>Add explicit time range limits</td>\n</tr>\n<tr>\n<td>High-Cardinality Grouping</td>\n<td>Group by high-diversity labels</td>\n<td>Processes millions of series</td>\n<td>Use sampling or pre-aggregated metrics</td>\n</tr>\n<tr>\n<td>Inefficient Label Matching</td>\n<td>Regular expressions on label values</td>\n<td>Full index scan required</td>\n<td>Use exact matches or optimize regex</td>\n</tr>\n<tr>\n<td>Cross-Series Math</td>\n<td>Complex arithmetic across many series</td>\n<td>Memory explosion during aggregation</td>\n<td>Break into smaller queries or use streaming</td>\n</tr>\n<tr>\n<td>Missing Filters</td>\n<td>No label selectors in metric queries</td>\n<td>Processes all series for metric</td>\n<td>Add specific label matchers</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Query Optimization Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Queries can become extremely expensive as data volume grows, potentially impacting system stability</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Query timeout limits only</li>\n<li>Resource-based query rejection (memory/CPU limits)</li>\n<li>Query complexity analysis with progressive limits</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach with complexity analysis, resource limits, and progressive timeouts</li>\n<li><strong>Rationale</strong>: Prevents runaway queries while allowing legitimate complex queries during low-load periods</li>\n<li><strong>Consequences</strong>: Requires query complexity estimation but provides better user experience than hard timeouts</li>\n</ul>\n</blockquote>\n<p><strong>Query Execution Pipeline Debugging:</strong></p>\n<p>The <code>QueryProcessor</code> breaks query execution into distinct phases, each of which can be monitored and optimized independently:</p>\n<ol>\n<li><p><strong>Query Parsing Phase</strong>: Convert query string to abstract syntax tree</p>\n<ul>\n<li>Monitor: Parse time, syntax errors, unsupported functions</li>\n<li>Debug: Enable query parser logging, validate AST structure</li>\n</ul>\n</li>\n<li><p><strong>Query Planning Phase</strong>: Generate execution plan with series selection strategy</p>\n<ul>\n<li>Monitor: Planning time, estimated series count, memory requirements</li>\n<li>Debug: Log execution plans, compare estimated vs. actual resource usage</li>\n</ul>\n</li>\n<li><p><strong>Series Selection Phase</strong>: Identify time series matching query selectors</p>\n<ul>\n<li>Monitor: Index lookup time, series count, label matching efficiency</li>\n<li>Debug: Log index utilization, profile label matching algorithms</li>\n</ul>\n</li>\n<li><p><strong>Data Retrieval Phase</strong>: Load sample data from storage blocks</p>\n<ul>\n<li>Monitor: Block read count, decompression time, I/O wait time</li>\n<li>Debug: Track block access patterns, identify hot/cold data distribution</li>\n</ul>\n</li>\n<li><p><strong>Aggregation Phase</strong>: Process samples according to query functions</p>\n<ul>\n<li>Monitor: Aggregation time, memory usage, output series count</li>\n<li>Debug: Profile aggregation functions, monitor streaming vs. batch processing</li>\n</ul>\n</li>\n</ol>\n<h4 id=\"missing-or-inconsistent-data\">Missing or Inconsistent Data</h4>\n<p>Data consistency problems in time-series systems often result from race conditions between ingestion, compaction, and querying processes. These problems are particularly insidious because they may only affect specific time ranges or metric combinations.</p>\n<p><strong>Data Consistency Investigation Process:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Investigation Step</th>\n<th>Diagnostic Approach</th>\n<th>Tools and Commands</th>\n<th>Expected Findings</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Verify Data Ingestion</td>\n<td>Check if samples reached storage</td>\n<td>Review ingestion logs and <code>WriteSamples</code> calls</td>\n<td>All submitted samples should appear in WAL</td>\n</tr>\n<tr>\n<td>Examine WAL State</td>\n<td>Inspect write-ahead log for sample presence</td>\n<td>WAL reader tools, log file analysis</td>\n<td>Samples should exist with correct timestamps</td>\n</tr>\n<tr>\n<td>Check Block Structure</td>\n<td>Verify samples exist in storage blocks</td>\n<td>Block inspection tools, storage debugging</td>\n<td>Samples should be in appropriate time-bucketed blocks</td>\n</tr>\n<tr>\n<td>Analyze Compaction Impact</td>\n<td>Determine if compaction corrupted data</td>\n<td>Compaction logs, before/after block comparison</td>\n<td>Sample count and values should be preserved</td>\n</tr>\n<tr>\n<td>Query Path Validation</td>\n<td>Trace query execution through storage layers</td>\n<td>Query debugging logs, execution plan analysis</td>\n<td>Queries should access all relevant blocks</td>\n</tr>\n</tbody></table>\n<p><strong>Common Data Consistency Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Consistency Problem</th>\n<th>Typical Manifestation</th>\n<th>Root Cause Analysis</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Samples Disappear After Compaction</td>\n<td>Queries return fewer points than expected</td>\n<td>Compaction algorithm bug or configuration error</td>\n<td>Validate compaction logic, check downsampling rules</td>\n</tr>\n<tr>\n<td>Duplicate Samples for Same Timestamp</td>\n<td>Aggregations produce incorrect results</td>\n<td>Multiple ingestion paths or replay issues</td>\n<td>Implement timestamp deduplication, fix replay logic</td>\n</tr>\n<tr>\n<td>Time Range Gaps</td>\n<td>Missing data for specific time periods</td>\n<td>WAL corruption, failed writes, or clock skew</td>\n<td>Recover from WAL backups, sync system clocks</td>\n</tr>\n<tr>\n<td>Label Inconsistency</td>\n<td>Same metric appears with different labels</td>\n<td>Concurrent label updates, caching issues</td>\n<td>Implement label validation, clear metadata caches</td>\n</tr>\n<tr>\n<td>Query Result Variations</td>\n<td>Same query returns different results</td>\n<td>Read-write race conditions, index staleness</td>\n<td>Add read barriers, refresh indexes before queries</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: Time-series data consistency problems often have delayed manifestation—samples might be ingested successfully but lost during later compaction or corrupted during concurrent reads. Always verify data through the complete storage lifecycle.</p>\n</blockquote>\n<p><strong>⚠️ Pitfall: Assuming WAL Guarantees Consistency</strong></p>\n<p>While the write-ahead log provides durability guarantees, it doesn&#39;t protect against logic errors in compaction or querying. A correctly written WAL can still result in missing data if the compaction process has bugs or if queries don&#39;t properly handle block boundaries.</p>\n<h4 id=\"storage-corruption-detection-and-recovery\">Storage Corruption Detection and Recovery</h4>\n<p>Storage corruption in time-series systems can occur at multiple levels: file system corruption, block-level data corruption, or index inconsistencies. Early detection and recovery procedures are critical for maintaining data integrity.</p>\n<p><strong>Corruption Detection Strategy:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Detection Method</th>\n<th>Implementation</th>\n<th>Trigger Conditions</th>\n<th>Recovery Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Block Checksums</td>\n<td>Compute checksums on write, verify on read</td>\n<td>Checksum mismatch during block access</td>\n<td>Mark block as corrupted, attempt recovery from WAL</td>\n</tr>\n<tr>\n<td>Index Validation</td>\n<td>Periodic consistency checks between index and blocks</td>\n<td>Scheduled validation, startup checks</td>\n<td>Rebuild index from storage blocks</td>\n</tr>\n<tr>\n<td>Sample Count Validation</td>\n<td>Track expected vs. actual sample counts</td>\n<td>Query result discrepancies</td>\n<td>Compare with ingestion metrics, identify missing blocks</td>\n</tr>\n<tr>\n<td>Temporal Consistency</td>\n<td>Verify timestamp ordering within blocks</td>\n<td>Out-of-order samples detected</td>\n<td>Re-sort block contents, check compaction logic</td>\n</tr>\n<tr>\n<td>Cross-Reference Validation</td>\n<td>Compare WAL entries with storage blocks</td>\n<td>Samples in WAL missing from blocks</td>\n<td>Replay missing WAL entries</td>\n</tr>\n</tbody></table>\n<p><strong>Storage Recovery Procedures:</strong></p>\n<p>The <code>StorageEngine</code> should implement comprehensive recovery mechanisms that can handle various corruption scenarios:</p>\n<ol>\n<li><strong>WAL-Based Recovery</strong>: Replay write-ahead log entries to reconstruct missing or corrupted blocks</li>\n<li><strong>Block Reconstruction</strong>: Rebuild corrupted blocks from available data sources</li>\n<li><strong>Index Rebuilding</strong>: Regenerate series indexes from existing storage blocks</li>\n<li><strong>Partial Recovery</strong>: Recover what data is possible and mark irretrievable data as missing</li>\n<li><strong>Backup Integration</strong>: Restore from external backups when local recovery fails</li>\n</ol>\n<h3 id=\"alerting-issues\">Alerting Issues</h3>\n<p>Alerting system problems are often the most critical because they directly impact incident response and system reliability. Alert failures can range from alerts not firing when they should (false negatives) to excessive alert noise (false positives).</p>\n<h4 id=\"alerts-not-firing\">Alerts Not Firing</h4>\n<p>The most dangerous alerting problem is alerts failing to fire when conditions are met, as this can lead to undetected outages or performance degradation.</p>\n<p><strong>Alert Evaluation Investigation Process:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Investigation Step</th>\n<th>Diagnostic Actions</th>\n<th>Key Information to Gather</th>\n<th>Resolution Path</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Verify Alert Rule Configuration</td>\n<td>Review <code>AlertRule</code> definition syntax</td>\n<td>Rule expression, threshold values, duration settings</td>\n<td>Fix syntax errors, validate threshold logic</td>\n</tr>\n<tr>\n<td>Check Alert Evaluation Loop</td>\n<td>Monitor <code>AlertEvaluator</code> execution</td>\n<td>Evaluation frequency, rule processing time, errors</td>\n<td>Restart evaluator, fix evaluation logic</td>\n</tr>\n<tr>\n<td>Validate Data Availability</td>\n<td>Confirm metrics exist for alert queries</td>\n<td>Query results, series existence, timestamp alignment</td>\n<td>Fix metric ingestion or query problems</td>\n</tr>\n<tr>\n<td>Examine Alert State Management</td>\n<td>Review state transitions and timing</td>\n<td>Current state, state change history, duration tracking</td>\n<td>Reset alert state, fix state machine logic</td>\n</tr>\n<tr>\n<td>Test Query Execution</td>\n<td>Run alert query manually</td>\n<td>Query results, execution errors, performance</td>\n<td>Optimize query or fix data source</td>\n</tr>\n</tbody></table>\n<p><strong>Common Alert Evaluation Failures:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Symptoms</th>\n<th>Root Cause</th>\n<th>Debug Approach</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Returns No Data</td>\n<td>Alert never transitions from Inactive</td>\n<td>Metric name mismatch, missing labels</td>\n<td>Execute alert query manually</td>\n<td>Correct metric selector syntax</td>\n</tr>\n<tr>\n<td>Threshold Logic Error</td>\n<td>Alert fires at wrong values</td>\n<td>Incorrect comparison operators, type mismatches</td>\n<td>Log threshold comparisons</td>\n<td>Fix operator logic (&gt;, &lt;, ==)</td>\n</tr>\n<tr>\n<td>Duration Requirements Not Met</td>\n<td>Alert stays in Pending state</td>\n<td>Insufficient duration for sustained condition</td>\n<td>Check state transition timing</td>\n<td>Adjust duration requirements</td>\n</tr>\n<tr>\n<td>Clock Skew Issues</td>\n<td>Timestamps don&#39;t align with evaluation</td>\n<td>System clock differences, timezone problems</td>\n<td>Compare timestamps across components</td>\n<td>Synchronize system clocks</td>\n</tr>\n<tr>\n<td>Resource Exhaustion</td>\n<td>Alert evaluator stops processing</td>\n<td>Memory/CPU limits exceeded during evaluation</td>\n<td>Monitor evaluator resource usage</td>\n<td>Scale alert evaluation capacity</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Alert Evaluation Reliability Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Alert failures can have severe operational consequences, requiring high reliability guarantees</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single-threaded evaluation with checkpointing</li>\n<li>Distributed evaluation with consensus</li>\n<li>Redundant evaluators with leader election</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Single-threaded evaluation with comprehensive checkpointing and fast failure detection</li>\n<li><strong>Rationale</strong>: Simpler architecture with fewer failure modes while maintaining reliability through rapid detection and recovery</li>\n<li><strong>Consequences</strong>: Single point of failure but faster recovery and easier debugging</li>\n</ul>\n</blockquote>\n<p><strong>Alert State Machine Debugging:</strong></p>\n<p>The alert state machine manages transitions between <code>AlertStateInactive</code>, <code>AlertStatePending</code>, <code>AlertStateFiring</code>, and <code>AlertStateResolved</code>. State transition problems often indicate timing issues or logic errors.</p>\n<table>\n<thead>\n<tr>\n<th>Current State</th>\n<th>Expected Trigger</th>\n<th>Expected Next State</th>\n<th>Common Failures</th>\n<th>Debug Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inactive</td>\n<td>Threshold exceeded</td>\n<td>Pending</td>\n<td>Query returns no data, threshold comparison error</td>\n<td>Verify query results, log comparison values</td>\n</tr>\n<tr>\n<td>Pending</td>\n<td>Duration requirement met</td>\n<td>Firing</td>\n<td>Timer not advancing, state not persisted</td>\n<td>Check duration tracking, verify state storage</td>\n</tr>\n<tr>\n<td>Firing</td>\n<td>Condition no longer met</td>\n<td>Resolved</td>\n<td>Notification delivery blocking state updates</td>\n<td>Separate notification from state management</td>\n</tr>\n<tr>\n<td>Resolved</td>\n<td>Condition met again</td>\n<td>Pending</td>\n<td>Rapid state flapping, insufficient hysteresis</td>\n<td>Add state change delays, implement hysteresis</td>\n</tr>\n</tbody></table>\n<h4 id=\"notification-delivery-problems\">Notification Delivery Problems</h4>\n<p>Even when alerts fire correctly, notification delivery failures can prevent teams from receiving critical alerts. Notification problems often involve external services and network connectivity issues.</p>\n<p><strong>Notification Pipeline Debugging:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pipeline Stage</th>\n<th>Common Failures</th>\n<th>Detection Methods</th>\n<th>Resolution Approaches</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Message Formatting</td>\n<td>Template errors, missing data</td>\n<td>Template compilation errors, malformed messages</td>\n<td>Fix template syntax, validate data availability</td>\n</tr>\n<tr>\n<td>Channel Selection</td>\n<td>Wrong channels chosen, disabled channels</td>\n<td>Routing logs, channel configuration</td>\n<td>Update routing rules, enable required channels</td>\n</tr>\n<tr>\n<td>External Service Integration</td>\n<td>API failures, authentication errors</td>\n<td>HTTP response codes, service error logs</td>\n<td>Fix credentials, handle API rate limits</td>\n</tr>\n<tr>\n<td>Network Connectivity</td>\n<td>DNS resolution, firewall blocks</td>\n<td>Network timeouts, connection errors</td>\n<td>Check network configuration, update DNS</td>\n</tr>\n<tr>\n<td>Rate Limiting</td>\n<td>Too many notifications sent</td>\n<td>Rate limit exceeded errors</td>\n<td>Implement backoff, batch notifications</td>\n</tr>\n</tbody></table>\n<p><strong>⚠️ Pitfall: Ignoring Notification Delivery Confirmations</strong></p>\n<p>Many alert systems send notifications without verifying delivery success. Network failures, API outages, or configuration errors can silently prevent notifications from reaching their destination. Always implement delivery confirmation and retry logic.</p>\n<p><strong>Notification Channel Health Monitoring:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Channel Type</th>\n<th>Health Check Method</th>\n<th>Failure Detection</th>\n<th>Recovery Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Email SMTP</td>\n<td>SMTP connection test, authentication validation</td>\n<td>SMTP errors, timeout responses</td>\n<td>Retry with exponential backoff, try alternate SMTP servers</td>\n</tr>\n<tr>\n<td>Slack Webhook</td>\n<td>HTTP POST test to webhook URL</td>\n<td>HTTP 4xx/5xx responses, network timeouts</td>\n<td>Refresh webhook URLs, check Slack app permissions</td>\n</tr>\n<tr>\n<td>PagerDuty API</td>\n<td>API key validation, service availability</td>\n<td>API authentication errors, service unavailable</td>\n<td>Rotate API keys, use backup notification channels</td>\n</tr>\n<tr>\n<td>Custom Webhook</td>\n<td>HTTP endpoint availability test</td>\n<td>Connection refused, DNS resolution failures</td>\n<td>Health check endpoints, implement circuit breakers</td>\n</tr>\n</tbody></table>\n<h4 id=\"alert-flapping-and-noise-reduction\">Alert Flapping and Noise Reduction</h4>\n<p>Alert flapping occurs when conditions rapidly oscillate around threshold values, causing alerts to fire and resolve repeatedly. This creates noise that can mask genuine problems and lead to alert fatigue.</p>\n<p><strong>Flapping Detection and Mitigation:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Flapping Pattern</th>\n<th>Detection Method</th>\n<th>Root Cause</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Rapid Fire/Resolve Cycles</td>\n<td>State changes within short time windows</td>\n<td>Threshold too close to normal values</td>\n<td>Increase threshold gap, add hysteresis</td>\n</tr>\n<tr>\n<td>Periodic Oscillation</td>\n<td>Regular pattern of state changes</td>\n<td>Scheduled processes, batch jobs</td>\n<td>Adjust evaluation timing, use trend analysis</td>\n</tr>\n<tr>\n<td>Network-Induced Flapping</td>\n<td>Correlates with network events</td>\n<td>Network partitions, intermittent connectivity</td>\n<td>Increase evaluation duration, use circuit breakers</td>\n</tr>\n<tr>\n<td>Load-Based Flapping</td>\n<td>Correlates with traffic patterns</td>\n<td>Resource saturation during peak load</td>\n<td>Use percentile thresholds, add capacity buffers</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: Alert flapping is often a symptom of poorly chosen thresholds rather than system instability. Effective alerting requires thresholds that account for normal system variation while still detecting genuine problems promptly.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This debugging section requires sophisticated tooling and instrumentation to make problems visible when they occur. The following implementation provides structured debugging capabilities across all system components.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Debugging Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logging Framework</td>\n<td>Go <code>slog</code> with JSON output</td>\n<td>Structured logging with correlation IDs</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Prometheus client library</td>\n<td>Custom metrics with detailed labels</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td>Go <code>net/http/pprof</code></td>\n<td>Continuous profiling with flame graphs</td>\n</tr>\n<tr>\n<td>Health Checking</td>\n<td>HTTP endpoints with JSON status</td>\n<td>Comprehensive health dashboard</td>\n</tr>\n<tr>\n<td>Error Tracking</td>\n<td>File-based error logs</td>\n<td>Centralized error aggregation</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    metrics-server/main.go          ← main entry point with debug flags\n    debug-tools/\n      cardinality-analyzer/main.go  ← tool for cardinality analysis\n      storage-inspector/main.go     ← tool for examining storage blocks\n      alert-tester/main.go         ← tool for testing alert rules\n  internal/\n    debugging/\n      health_manager.go            ← centralized health checking\n      performance_monitor.go       ← performance metrics collection\n      debug_handlers.go           ← HTTP handlers for debug endpoints\n    diagnostics/\n      ingestion_diagnostics.go     ← ingestion-specific debugging\n      storage_diagnostics.go       ← storage-specific debugging\n      alert_diagnostics.go         ← alerting-specific debugging\n  tools/\n    validate-config.go             ← configuration validation\n    generate-test-metrics.go       ← synthetic metric generation</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Health Management System:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/debugging/health_manager.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debugging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">encoding/json</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">net/http</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthStatus represents the overall health state of a component</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> int</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">const</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusHealthy</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> iota</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusDegraded</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HealthStatusUnhealthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">s </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> s {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusHealthy:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusDegraded:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusUnhealthy:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    default</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheck represents a single health check result</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Name        </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"name\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Status      </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#9ECBFF\"> `json:\"status\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    LastChecked </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\">    `json:\"last_checked\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Message     </span><span style=\"color:#F97583\">string</span><span style=\"color:#9ECBFF\">       `json:\"message\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthCheckFunc defines the signature for health check functions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthCheckFunc</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// HealthManager coordinates health checks across all system components</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HealthManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checks </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthCheck</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    funcs  </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">HealthCheckFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu     </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewHealthManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checks: </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        funcs:  </span><span style=\"color:#B392F0\">make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">HealthCheckFunc</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger: logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start periodic health check execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> hm.</span><span style=\"color:#B392F0\">runPeriodicChecks</span><span style=\"color:#E1E4E8\">(interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RegisterCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">checkFunc</span><span style=\"color:#B392F0\"> HealthCheckFunc</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hm.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.funcs[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> checkFunc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.checks[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Name:        name,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Status:      HealthStatusUnhealthy,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        LastChecked: </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Message:     </span><span style=\"color:#9ECBFF\">\"not yet checked\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RunChecks</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    funcs </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#B392F0\">HealthCheckFunc</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, fn </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hm.funcs {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        funcs[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fn</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> name, checkFunc </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> funcs {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hm.</span><span style=\"color:#B392F0\">runSingleCheck</span><span style=\"color:#E1E4E8\">(ctx, name, checkFunc)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">runSingleCheck</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">name</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">checkFunc</span><span style=\"color:#B392F0\"> HealthCheckFunc</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    err </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> checkFunc</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Since</span><span style=\"color:#E1E4E8\">(start)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    check </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hm.checks[name]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    check.LastChecked </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> err </span><span style=\"color:#F97583\">!=</span><span style=\"color:#79B8FF\"> nil</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        check.Status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatusUnhealthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        check.Message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"check failed: </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\"> (took </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">, err, duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hm.logger.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"health check failed\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            slog.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"check\"</span><span style=\"color:#E1E4E8\">, name),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            slog.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">, err.</span><span style=\"color:#B392F0\">Error</span><span style=\"color:#E1E4E8\">()),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            slog.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"duration\"</span><span style=\"color:#E1E4E8\">, duration))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Consider slow checks as degraded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> duration </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            check.Status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatusDegraded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            check.Message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"check slow but successful (took </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">, duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        } </span><span style=\"color:#F97583\">else</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            check.Status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatusHealthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            check.Message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Sprintf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"check successful (took </span><span style=\"color:#79B8FF\">%v</span><span style=\"color:#9ECBFF\">)\"</span><span style=\"color:#E1E4E8\">, duration)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetOverallStatus</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#B392F0\">HealthStatus</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> hm.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#B392F0\"> len</span><span style=\"color:#E1E4E8\">(hm.checks) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> HealthStatusUnhealthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    overallStatus </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> HealthStatusHealthy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, check </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hm.checks {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> check.Status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatusUnhealthy {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatusUnhealthy</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> check.Status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatusDegraded {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            overallStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatusDegraded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> overallStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ServeHTTP</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">w</span><span style=\"color:#B392F0\"> http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">ResponseWriter</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">r</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">http</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Request</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checks </span><span style=\"color:#F97583\">:=</span><span style=\"color:#B392F0\"> make</span><span style=\"color:#E1E4E8\">([]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthCheck</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#B392F0\">len</span><span style=\"color:#E1E4E8\">(hm.checks))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> _, check </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> hm.checks {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checks </span><span style=\"color:#F97583\">=</span><span style=\"color:#B392F0\"> append</span><span style=\"color:#E1E4E8\">(checks, check)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    hm.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    overallStatus </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> hm.</span><span style=\"color:#B392F0\">GetOverallStatus</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">interface</span><span style=\"color:#E1E4E8\">{}{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"status\"</span><span style=\"color:#E1E4E8\">: overallStatus.</span><span style=\"color:#B392F0\">String</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"checks\"</span><span style=\"color:#E1E4E8\">: checks,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"timestamp\"</span><span style=\"color:#E1E4E8\">: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    w.</span><span style=\"color:#B392F0\">Header</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#B392F0\">Set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Content-Type\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"application/json\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Set HTTP status based on health</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    switch</span><span style=\"color:#E1E4E8\"> overallStatus {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusHealthy:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusOK)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusDegraded:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusOK) </span><span style=\"color:#6A737D\">// 200 but degraded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    case</span><span style=\"color:#E1E4E8\"> HealthStatusUnhealthy:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        w.</span><span style=\"color:#B392F0\">WriteHeader</span><span style=\"color:#E1E4E8\">(http.StatusServiceUnavailable)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json.</span><span style=\"color:#B392F0\">NewEncoder</span><span style=\"color:#E1E4E8\">(w).</span><span style=\"color:#B392F0\">Encode</span><span style=\"color:#E1E4E8\">(response)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">hm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HealthManager</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">runPeriodicChecks</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">interval</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(interval)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ticker.C {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ctx, cancel </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> context.</span><span style=\"color:#B392F0\">WithTimeout</span><span style=\"color:#E1E4E8\">(context.</span><span style=\"color:#B392F0\">Background</span><span style=\"color:#E1E4E8\">(), </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hm.</span><span style=\"color:#B392F0\">RunChecks</span><span style=\"color:#E1E4E8\">(ctx)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">        cancel</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Complete Performance Monitoring System:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/debugging/performance_monitor.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> debugging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">sync</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PerformanceMetrics tracks system performance indicators</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PerformanceMetrics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Ingestion metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IngestRate          </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"ingest_rate_per_sec\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    IngestLatencyP99    </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"ingest_latency_p99\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ValidationErrors    </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"validation_errors\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Storage metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StorageWriteRate    </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"storage_write_rate_per_sec\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    StorageWriteLatency </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"storage_write_latency\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    CompactionDuration  </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"last_compaction_duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Query metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryRate           </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"query_rate_per_sec\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    QueryLatencyP99     </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"query_latency_p99\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    SlowQueryCount      </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"slow_query_count\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Alert metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AlertEvaluationRate </span><span style=\"color:#F97583\">float64</span><span style=\"color:#9ECBFF\">   `json:\"alert_eval_rate_per_sec\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    AlertsActive        </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">     `json:\"alerts_active\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    NotificationFailures </span><span style=\"color:#F97583\">int64</span><span style=\"color:#9ECBFF\">    `json:\"notification_failures\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // System metrics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    MemoryUsage         </span><span style=\"color:#F97583\">uint64</span><span style=\"color:#9ECBFF\">    `json:\"memory_usage_bytes\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GoroutineCount      </span><span style=\"color:#F97583\">int</span><span style=\"color:#9ECBFF\">       `json:\"goroutine_count\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    GCDuration          </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#9ECBFF\"> `json:\"gc_duration\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Timestamps</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    Timestamp           </span><span style=\"color:#B392F0\">time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Time</span><span style=\"color:#9ECBFF\"> `json:\"timestamp\"`</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu                  </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewPerformanceMonitor</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMetrics</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pm </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">PerformanceMetrics</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        Timestamp: time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Start periodic system metrics collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    go</span><span style=\"color:#E1E4E8\"> pm.</span><span style=\"color:#B392F0\">collectSystemMetrics</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> pm</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">collectSystemMetrics</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ticker </span><span style=\"color:#F97583\">:=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">NewTicker</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> time.Second)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> ticker.</span><span style=\"color:#B392F0\">Stop</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#F97583\"> range</span><span style=\"color:#E1E4E8\"> ticker.C {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pm.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Memory statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        var</span><span style=\"color:#E1E4E8\"> memStats </span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MemStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        runtime.</span><span style=\"color:#B392F0\">ReadMemStats</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">memStats)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pm.MemoryUsage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> memStats.Alloc</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pm.GCDuration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">(memStats.PauseTotalNs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        // Goroutine count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pm.GoroutineCount </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> runtime.</span><span style=\"color:#B392F0\">NumGoroutine</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pm.Timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.</span><span style=\"color:#B392F0\">Now</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pm.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RecordIngestMetric</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">latency</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">success</span><span style=\"color:#F97583\"> bool</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pm.mu.</span><span style=\"color:#B392F0\">Lock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> pm.mu.</span><span style=\"color:#B392F0\">Unlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Update ingestion rate (simplified - real implementation would use sliding window)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pm.IngestRate</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Update latency percentile (simplified - real implementation would use histogram)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> latency </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> pm.IngestLatencyP99 {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pm.IngestLatencyP99 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> latency</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#F97583\"> !</span><span style=\"color:#E1E4E8\">success {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pm.ValidationErrors</span><span style=\"color:#F97583\">++</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">pm </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMetrics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GetSnapshot</span><span style=\"color:#E1E4E8\">() </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PerformanceMetrics</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pm.mu.</span><span style=\"color:#B392F0\">RLock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    defer</span><span style=\"color:#E1E4E8\"> pm.mu.</span><span style=\"color:#B392F0\">RUnlock</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    snapshot </span><span style=\"color:#F97583\">:=</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\">pm</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#E1E4E8\">snapshot</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Ingestion Diagnostics Framework:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/diagnostics/ingestion_diagnostics.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> diagnostics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> IngestionDiagnostics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add references to ingestion components</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewIngestionDiagnostics</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IngestionDiagnostics</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> &#x26;</span><span style=\"color:#B392F0\">IngestionDiagnostics</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger: logger,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DiagnoseMetricIngestion performs comprehensive ingestion pipeline analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">id </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IngestionDiagnostics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DiagnoseMetricIngestion</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">metricName</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">timeRange</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check if metrics are reaching the ingestion endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Review HTTP access logs for metric submission requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Verify request payloads are valid JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check response codes (200 vs 4xx vs 5xx)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate metric format and content</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Parse submitted metrics against expected schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check metric names, types, labels, and values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify validation rule violations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Trace metric flow through validation pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check if metrics pass cardinality limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Verify label format and allowed values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify metrics rejected during validation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Verify storage writes are successful</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check WAL entries for submitted metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Verify samples appear in storage blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify storage write failures or delays</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate diagnostic report with findings</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Summarize ingestion health for the specified time range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - List specific failures and their root causes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Recommend remediation steps for identified issues</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"ingestion diagnostics not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// CheckCardinalityExplosion analyzes label combinations for cardinality issues</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">id </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">IngestionDiagnostics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">CheckCardinalityExplosion</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Query current series count per metric</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Group series by metric name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Count unique label combinations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify metrics with excessive cardinality</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Analyze label contribution to cardinality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - For high-cardinality metrics, examine individual labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Calculate unique value count per label</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify labels with excessive diversity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Project cardinality growth trends</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Calculate new series creation rate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Estimate storage and memory impact</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Predict when cardinality limits will be exceeded</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Generate cardinality optimization recommendations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Suggest label consolidation opportunities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Recommend cardinality limit adjustments</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify metrics suitable for sampling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"cardinality analysis not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<p><strong>Storage Diagnostics Framework:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// internal/diagnostics/storage_diagnostics.go</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">package</span><span style=\"color:#B392F0\"> diagnostics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">context</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">fmt</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">log/slog</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"</span><span style=\"color:#B392F0\">time</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> StorageDiagnostics</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // Add references to storage components</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DiagnoseQueryPerformance analyzes slow query execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageDiagnostics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DiagnoseQueryPerformance</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">queryString</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse and analyze query complexity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Break down query into component parts (selectors, functions, time range)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Estimate series count and sample count requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify potentially expensive operations (regex, aggregations)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Examine storage block access patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Determine which blocks need to be read for the query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check for block overlaps requiring merge operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify hot vs cold data access patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Profile query execution phases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Measure time spent in parsing, planning, execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Track memory allocation during query processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify bottleneck phases in the query pipeline</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Analyze index utilization efficiency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check if label indexes are being used effectively</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify full scans that could benefit from better indexes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Measure index hit ratio for the query</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate performance optimization recommendations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Suggest query restructuring for better performance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Recommend index improvements or additions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify opportunities for result caching</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"query performance diagnostics not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// ValidateStorageConsistency checks for data corruption or inconsistencies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">sd </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageDiagnostics</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">ValidateStorageConsistency</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Verify WAL and storage block consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Compare WAL entries with corresponding storage blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check that all WAL entries have been persisted to blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Identify any samples that exist in WAL but not in blocks</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Validate block internal consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Verify block checksums match stored data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check timestamp ordering within blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Validate sample count matches block metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Check index consistency with actual data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Verify series index entries match storage blocks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check that all series in blocks have index entries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Validate label indexes match series labels</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Analyze compaction impact on data integrity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Compare pre- and post-compaction sample counts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Verify compaction preserved data accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // - Check for any data loss during compaction operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"storage consistency validation not implemented\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"language-specific-debugging-hints\">Language-Specific Debugging Hints</h4>\n<p><strong>Go-Specific Debugging Techniques:</strong></p>\n<ul>\n<li>Use <code>go tool pprof</code> to analyze CPU and memory profiles during high load periods</li>\n<li>Enable <code>GODEBUG=gctrace=1</code> to monitor garbage collection impact on performance</li>\n<li>Use <code>go tool trace</code> to analyze goroutine scheduling and coordination issues</li>\n<li>Implement custom <code>slog.Handler</code> to add correlation IDs to log entries</li>\n<li>Use <code>sync/atomic</code> for lock-free performance counters in hot paths</li>\n</ul>\n<p><strong>Performance Profiling Integration:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#B392F0\">net/http/pprof</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Add to main HTTP server</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">go</span><span style=\"color:#F97583\"> func</span><span style=\"color:#E1E4E8\">() {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log.</span><span style=\"color:#B392F0\">Println</span><span style=\"color:#E1E4E8\">(http.</span><span style=\"color:#B392F0\">ListenAndServe</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"localhost:6060\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">nil</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}()</span></span></code></pre></div>\n\n<p><strong>Memory Leak Detection:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Add to health checks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> checkMemoryGrowth</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    var</span><span style=\"color:#E1E4E8\"> m1, m2 </span><span style=\"color:#B392F0\">runtime</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">MemStats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    runtime.</span><span style=\"color:#B392F0\">ReadMemStats</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m1)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    runtime.</span><span style=\"color:#B392F0\">GC</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    runtime.</span><span style=\"color:#B392F0\">ReadMemStats</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">&#x26;</span><span style=\"color:#E1E4E8\">m2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> m2.Alloc </span><span style=\"color:#F97583\">></span><span style=\"color:#E1E4E8\"> m1.Alloc</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> fmt.</span><span style=\"color:#B392F0\">Errorf</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"potential memory leak detected\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> nil</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p><strong>After implementing debugging infrastructure:</strong></p>\n<ol>\n<li><strong>Run Health Checks</strong>: <code>curl http://localhost:8080/health</code> should return comprehensive health status</li>\n<li><strong>Generate Test Load</strong>: Use synthetic metrics to trigger various failure modes</li>\n<li><strong>Verify Diagnostic Tools</strong>: Each diagnostic function should provide actionable insights</li>\n<li><strong>Test Error Recovery</strong>: Inject failures and verify system recovery behavior</li>\n</ol>\n<p><strong>Expected Debug Output Example:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"degraded\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"checks\"</span><span style=\"color:#E1E4E8\">: [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"ingestion_pipeline\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"healthy\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"last_checked\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"2024-01-15T10:30:00Z\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"check successful (took 45ms)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"storage_engine\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"status\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"degraded\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"last_checked\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"2024-01-15T10:30:00Z\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"message\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"compaction behind schedule (took 3.2s)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  ],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"timestamp\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"2024-01-15T10:30:00Z\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (future extensions build upon the complete metrics and alerting system to address scalability, reliability, and advanced functionality requirements)</p>\n</blockquote>\n<h3 id=\"the-evolution-principle-building-for-tomorrow39s-challenges\">The Evolution Principle: Building for Tomorrow&#39;s Challenges</h3>\n<p>Think of system extensions like urban planning for a growing city. When you design the initial road network, you don&#39;t just consider current traffic patterns—you plan for wider roads, additional lanes, and subway systems that might be needed as the population grows. Similarly, our metrics and alerting system needs to accommodate future growth in data volume, user requirements, and operational complexity without requiring a complete architectural overhaul.</p>\n<p>The key insight is that <strong>extensibility isn&#39;t about predicting the future perfectly</strong>—it&#39;s about creating flexible architectural joints and extension points that can accommodate unforeseen requirements. Just as a well-designed building has load-bearing walls that can support additional floors, our system has architectural foundations that can support distributed deployment, advanced analytics, and new data sources.</p>\n<p>This section explores two categories of extensions: <strong>scalability enhancements</strong> that address growing data volumes and user loads, and <strong>feature enhancements</strong> that add new capabilities while leveraging the existing architectural foundation.</p>\n<h3 id=\"current-architecture39s-extension-points\">Current Architecture&#39;s Extension Points</h3>\n<p>Before diving into specific enhancements, it&#39;s important to understand how our current design accommodates future growth. The architecture includes several deliberate extension points:</p>\n<p><strong>Component Interface Abstraction</strong>: All major components implement well-defined interfaces (<code>TimeSeriesStorage</code>, <code>QueryProcessor</code>, <code>NotificationManager</code>). This allows swapping implementations without changing dependent components. For example, replacing the single-node storage engine with a distributed storage cluster requires only implementing the same <code>TimeSeriesStorage</code> interface.</p>\n<p><strong>Configuration-Driven Behavior</strong>: The <code>Config</code> structure and its nested configurations (<code>StorageConfig</code>, <code>AlertingConfig</code>) allow runtime behavior modification without code changes. New storage backends, notification channels, and query execution strategies can be enabled through configuration updates.</p>\n<p><strong>Plugin Architecture Foundations</strong>: The <code>NotificationChannel</code> system demonstrates a plugin pattern that can be extended to other components. New channel types (like Microsoft Teams or custom webhooks) can be added by implementing the notification interface without modifying core alerting logic.</p>\n<p><strong>Modular Component Design</strong>: The <code>ComponentCoordinator</code> manages component lifecycles independently, making it straightforward to add new components (like a machine learning inference engine) that integrate with existing data flows through well-defined interfaces.</p>\n<h2 id=\"scalability-enhancements\">Scalability Enhancements</h2>\n<h3 id=\"mental-model-from-corner-store-to-global-supply-chain\">Mental Model: From Corner Store to Global Supply Chain</h3>\n<p>Think of scalability enhancements like evolving from a small corner store to a global supply chain. The corner store serves its neighborhood well with simple operations: one cashier, local suppliers, and customers who walk in. But as demand grows beyond the neighborhood, you need multiple locations, regional distribution centers, sophisticated inventory management, and coordination between stores. The fundamental business logic remains the same—buying and selling goods—but the operational complexity increases dramatically.</p>\n<p>Our current metrics system is like that efficient corner store: it handles significant load on a single machine with straightforward operations. Scalability enhancements transform it into a distributed system that can serve global monitoring needs while preserving the same core functionality and user experience.</p>\n<h3 id=\"distributed-storage-architecture\">Distributed Storage Architecture</h3>\n<p>The most critical scalability enhancement involves distributing the storage layer across multiple nodes to handle data volumes that exceed single-machine capacity. This transformation requires careful consideration of data partitioning, replication, and consistency guarantees.</p>\n<blockquote>\n<p><strong>Decision: Horizontal Partitioning Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Single-node storage limits both data volume and query throughput. Time-series data has natural partitioning characteristics by metric name and time ranges.</li>\n<li><strong>Options Considered</strong>: Hash-based partitioning by series ID, time-based partitioning by timestamp ranges, hybrid partitioning combining both approaches</li>\n<li><strong>Decision</strong>: Hybrid partitioning with time-based primary partitioning and hash-based secondary partitioning</li>\n<li><strong>Rationale</strong>: Time-based partitioning aligns with query patterns (most queries are time-range based) and enables efficient data lifecycle management. Hash-based secondary partitioning distributes load evenly across nodes within time windows.</li>\n<li><strong>Consequences</strong>: Enables horizontal scaling and efficient range queries, but requires coordination for cross-partition queries and adds complexity for global operations.</li>\n</ul>\n</blockquote>\n<p>The distributed storage architecture introduces several new components that extend our existing <code>StorageEngine</code>:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Interface Changes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PartitionManager</code></td>\n<td>Maps time ranges and series to storage nodes</td>\n<td>Extends <code>TimeSeriesStorage</code> with partition awareness</td>\n</tr>\n<tr>\n<td><code>ReplicationCoordinator</code></td>\n<td>Manages data replication across nodes for durability</td>\n<td>Adds replication configuration to <code>StorageConfig</code></td>\n</tr>\n<tr>\n<td><code>ConsistencyManager</code></td>\n<td>Coordinates read/write consistency across replicas</td>\n<td>Introduces consistency level parameters to storage operations</td>\n</tr>\n<tr>\n<td><code>ShardingRouter</code></td>\n<td>Routes queries to appropriate partition nodes</td>\n<td>Extends <code>QueryProcessor</code> with distributed query planning</td>\n</tr>\n</tbody></table>\n<p>The distributed storage maintains backward compatibility by implementing the same <code>TimeSeriesStorage</code> interface, but internally coordinates operations across multiple nodes. Write operations become more complex as they must handle replication and consistency requirements:</p>\n<ol>\n<li><strong>Write Coordination</strong>: When <code>WriteSamples</code> is called, the <code>PartitionManager</code> determines which nodes should store each sample based on the series ID and timestamp</li>\n<li><strong>Replication Protocol</strong>: The <code>ReplicationCoordinator</code> ensures each write is replicated to the configured number of nodes before acknowledging success</li>\n<li><strong>Consistency Management</strong>: The <code>ConsistencyManager</code> enforces consistency levels (immediate, eventual, or quorum-based) based on the operation requirements</li>\n</ol>\n<p>Query processing becomes more sophisticated as queries may span multiple partitions:</p>\n<ol>\n<li><strong>Query Planning</strong>: The <code>ShardingRouter</code> analyzes queries to determine which partitions contain relevant data</li>\n<li><strong>Parallel Execution</strong>: Subqueries are executed in parallel against relevant partition nodes</li>\n<li><strong>Result Merging</strong>: Results from multiple nodes are merged and deduplicated to produce the final response</li>\n<li><strong>Cross-Partition Aggregation</strong>: Aggregation functions are decomposed into node-local operations and global merge operations</li>\n</ol>\n<h3 id=\"high-availability-and-fault-tolerance\">High Availability and Fault Tolerance</h3>\n<p>Distributed deployment introduces new failure modes that require sophisticated fault tolerance mechanisms. The system must continue operating even when individual nodes fail, network partitions occur, or storage becomes temporarily unavailable.</p>\n<p><strong>Node Failure Detection and Recovery</strong>: The system implements a gossip-based failure detection protocol where nodes periodically exchange heartbeat information. When a node failure is detected, the <code>ReplicationCoordinator</code> triggers automatic failover to replica nodes and initiates replication repair to restore the desired replication factor.</p>\n<p><strong>Split-Brain Prevention</strong>: Network partitions can create scenarios where multiple node groups believe they&#39;re the authoritative cluster. The system implements a quorum-based approach where operations require acknowledgment from a majority of nodes to prevent conflicting writes during partitions.</p>\n<p><strong>Data Consistency During Failures</strong>: The <code>ConsistencyManager</code> implements configurable consistency levels that balance availability and consistency based on operational requirements. During partial failures, the system can operate in degraded mode with reduced consistency guarantees rather than becoming completely unavailable.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Scenario</th>\n<th>Detection Method</th>\n<th>Recovery Action</th>\n<th>Impact on Operations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single node failure</td>\n<td>Gossip heartbeat timeout</td>\n<td>Failover to replica, start replication repair</td>\n<td>Temporary query latency increase</td>\n</tr>\n<tr>\n<td>Network partition</td>\n<td>Quorum loss detection</td>\n<td>Minority partition becomes read-only</td>\n<td>Write operations blocked in minority</td>\n</tr>\n<tr>\n<td>Storage corruption</td>\n<td>Checksum validation</td>\n<td>Restore from replica, mark node for rebuild</td>\n<td>Automatic transparent recovery</td>\n</tr>\n<tr>\n<td>Coordinator failure</td>\n<td>Leader election timeout</td>\n<td>Promote new coordinator</td>\n<td>Brief coordination pause</td>\n</tr>\n</tbody></table>\n<h3 id=\"load-balancing-and-query-distribution\">Load Balancing and Query Distribution</h3>\n<p>As query load increases, the system needs intelligent load balancing to distribute query processing across available resources. This involves both request routing and resource management at multiple levels.</p>\n<p><strong>Query Router Enhancement</strong>: The existing <code>QueryProcessor</code> is extended with a <code>DistributedQueryRouter</code> that implements several load balancing strategies. Round-robin distribution works well for uniform queries, while least-connections balancing is better for queries with varying execution times. Resource-aware routing considers CPU and memory utilization on target nodes.</p>\n<p><strong>Query Result Caching Distribution</strong>: The <code>QueryCache</code> becomes a distributed cache shared across query processor instances. This prevents redundant query execution when the same queries are submitted to different nodes. Cache invalidation becomes more complex as it must coordinate across multiple nodes when underlying data changes.</p>\n<p><strong>Adaptive Resource Allocation</strong>: The system monitors query execution patterns and automatically adjusts resource allocation. Long-running aggregation queries are routed to dedicated high-memory nodes, while simple point queries are handled by fast-response nodes. This prevents resource contention between different query types.</p>\n<h3 id=\"auto-scaling-infrastructure\">Auto-Scaling Infrastructure</h3>\n<p>Modern deployment environments support automatic scaling based on resource utilization and demand patterns. The enhanced system integrates with container orchestration platforms to automatically adjust capacity.</p>\n<p><strong>Demand-Based Scaling</strong>: The system exports internal metrics about query queue lengths, ingestion rates, and resource utilization. These metrics drive auto-scaling policies that add storage or query processing capacity when thresholds are exceeded. Scaling decisions consider both current load and historical patterns to anticipate demand.</p>\n<p><strong>Graceful Node Addition and Removal</strong>: When new nodes join the cluster, the <code>PartitionManager</code> gradually rebalances data to include the new capacity. Similarly, when nodes are removed (either manually or due to scaling down), their data is redistributed to remaining nodes before the node shuts down. This ensures continuous availability during scaling operations.</p>\n<p><strong>Geographic Distribution</strong>: For global deployments, the system supports multi-region deployment with intelligent routing to minimize latency. Users are automatically routed to their nearest region, while cross-region replication ensures data availability for disaster recovery.</p>\n<h2 id=\"feature-enhancements\">Feature Enhancements</h2>\n<h3 id=\"mental-model-from-basic-calculator-to-scientific-workstation\">Mental Model: From Basic Calculator to Scientific Workstation</h3>\n<p>Think of feature enhancements like evolving from a basic calculator to a scientific workstation. The basic calculator handles arithmetic operations perfectly for simple tasks, but scientists need advanced functions like statistical analysis, graphing capabilities, and programmable operations. However, the fundamental mathematical principles remain the same—it&#39;s the sophistication of available operations that increases.</p>\n<p>Our current metrics system provides solid foundational capabilities: collecting data, storing it efficiently, and providing basic visualization and alerting. Feature enhancements add advanced analytical capabilities, intelligent automation, and richer user experiences while building on the same core data model and architectural patterns.</p>\n<h3 id=\"advanced-query-functions-and-analytics\">Advanced Query Functions and Analytics</h3>\n<p>The current query engine provides basic aggregation functions like sum, average, and percentiles. Advanced analytics requires more sophisticated mathematical operations, statistical analysis, and time-series specific functions.</p>\n<p><strong>Statistical Functions Enhancement</strong>: The <code>AggregationFunction</code> interface is extended with advanced statistical operations that provide deeper insights into metric behavior patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Function Category</th>\n<th>New Functions</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Trend Analysis</td>\n<td><code>trend()</code>, <code>linear_regression()</code>, <code>correlation()</code></td>\n<td>Identifying performance degradation patterns</td>\n</tr>\n<tr>\n<td>Anomaly Detection</td>\n<td><code>zscore()</code>, <code>mad()</code>, <code>seasonal_decompose()</code></td>\n<td>Detecting unusual metric behavior</td>\n</tr>\n<tr>\n<td>Forecasting</td>\n<td><code>holt_winters()</code>, <code>arima()</code>, <code>exponential_smoothing()</code></td>\n<td>Capacity planning and predictive alerting</td>\n</tr>\n<tr>\n<td>Signal Processing</td>\n<td><code>moving_average()</code>, <code>exponential_decay()</code>, <code>fourier_transform()</code></td>\n<td>Noise reduction and frequency analysis</td>\n</tr>\n</tbody></table>\n<p><strong>Advanced Query Language Features</strong>: The query parser is enhanced to support more complex expressions including mathematical operations, conditional logic, and subqueries. This enables sophisticated metric analysis without requiring external data processing tools.</p>\n<p>For example, advanced queries can compute complex business metrics:</p>\n<ul>\n<li>Service level calculations: <code>(successful_requests / total_requests) * 100</code></li>\n<li>Capacity utilization trends: <code>trend(cpu_usage[7d]) &gt; 0.1</code> (detecting increasing utilization)</li>\n<li>Comparative analysis: <code>current_latency / baseline_latency[1w:offset 1w] &gt; 1.5</code> (comparing to previous week)</li>\n</ul>\n<p><strong>Query Optimization for Complex Analytics</strong>: Advanced analytical functions often require processing large amounts of historical data. The query execution engine implements several optimizations specifically for analytical workloads:</p>\n<ol>\n<li><strong>Incremental Computation</strong>: Many statistical functions can be computed incrementally, updating results as new data arrives rather than recomputing from scratch</li>\n<li><strong>Parallel Processing</strong>: Complex queries are decomposed into parallel operations that can execute simultaneously across multiple CPU cores</li>\n<li><strong>Intermediate Result Caching</strong>: Expensive intermediate computations (like moving averages) are cached and reused across multiple queries</li>\n<li><strong>Data Skipping</strong>: Query planning identifies time ranges and series that don&#39;t affect query results, avoiding unnecessary data access</li>\n</ol>\n<h3 id=\"machine-learning-integration\">Machine Learning Integration</h3>\n<p>Machine learning capabilities transform reactive monitoring into proactive intelligence. Instead of just reporting what happened, the system can predict what will happen and automatically adapt to changing conditions.</p>\n<blockquote>\n<p><strong>Decision: Embedded vs. External ML Integration</strong></p>\n<ul>\n<li><strong>Context</strong>: Machine learning models can be embedded within the metrics system or integrated with external ML platforms. Each approach has different performance, complexity, and flexibility characteristics.</li>\n<li><strong>Options Considered</strong>: Fully embedded ML engine, external ML service integration, hybrid approach with embedded inference and external training</li>\n<li><strong>Decision</strong>: Hybrid approach with lightweight embedded inference and external model training</li>\n<li><strong>Rationale</strong>: Embedded inference provides low-latency predictions for real-time alerting, while external training leverages specialized ML platforms for model development and heavy computation</li>\n<li><strong>Consequences</strong>: Enables real-time ML-powered features with acceptable complexity, but requires model deployment and synchronization infrastructure</li>\n</ul>\n</blockquote>\n<p><strong>Anomaly Detection Engine</strong>: The system includes an embedded anomaly detection engine that continuously learns normal behavior patterns for each metric and identifies deviations. This goes beyond simple threshold-based alerting to detect subtle behavioral changes that might indicate underlying issues.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Responsibility</th>\n<th>Integration Point</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>AnomalyDetector</code></td>\n<td>Identifies unusual metric patterns</td>\n<td>Integrates with <code>AlertEvaluator</code> for ML-powered alerts</td>\n</tr>\n<tr>\n<td><code>ModelRegistry</code></td>\n<td>Manages trained models and versioning</td>\n<td>Extends <code>Config</code> with model configuration</td>\n</tr>\n<tr>\n<td><code>FeatureExtractor</code></td>\n<td>Prepares metric data for ML inference</td>\n<td>Integrates with <code>QueryProcessor</code> for feature computation</td>\n</tr>\n<tr>\n<td><code>PredictionEngine</code></td>\n<td>Generates forecasts for capacity planning</td>\n<td>Provides new query functions for predictive analytics</td>\n</tr>\n</tbody></table>\n<p><strong>Predictive Alerting</strong>: Traditional alerts fire after problems occur. Predictive alerting uses machine learning models to identify conditions that historically lead to incidents, enabling proactive intervention. For example, the system might learn that specific combinations of CPU usage, memory pressure, and request rate patterns typically precede service outages.</p>\n<p><strong>Adaptive Thresholds</strong>: Static alert thresholds often generate false alarms because they don&#39;t account for natural variations in system behavior (daily cycles, seasonal patterns, gradual growth trends). Machine learning models automatically adjust alert thresholds based on historical patterns, reducing alert fatigue while maintaining sensitivity to genuine issues.</p>\n<p><strong>Automated Root Cause Analysis</strong>: When incidents occur, machine learning models analyze correlation patterns across multiple metrics to suggest potential root causes. This accelerates incident response by helping engineers focus their investigation on the most likely culprits.</p>\n<h3 id=\"enhanced-visualization-and-dashboard-features\">Enhanced Visualization and Dashboard Features</h3>\n<p>The current dashboard system provides basic line charts and panel layouts. Enhanced visualization adds sophisticated chart types, interactive analysis capabilities, and intelligent dashboard automation.</p>\n<p><strong>Advanced Chart Types</strong>: The chart renderer is extended with additional visualization options that better suit different types of metric analysis:</p>\n<table>\n<thead>\n<tr>\n<th>Chart Type</th>\n<th>Best For</th>\n<th>Key Features</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Heatmaps</td>\n<td>Distribution visualization</td>\n<td>Shows metric value distributions over time</td>\n</tr>\n<tr>\n<td>Scatter Plots</td>\n<td>Correlation analysis</td>\n<td>Reveals relationships between different metrics</td>\n</tr>\n<tr>\n<td>Network Diagrams</td>\n<td>Service dependency mapping</td>\n<td>Visualizes service interactions and bottlenecks</td>\n</tr>\n<tr>\n<td>Geographic Maps</td>\n<td>Location-based metrics</td>\n<td>Shows metric values across geographic regions</td>\n</tr>\n<tr>\n<td>Sankey Diagrams</td>\n<td>Flow analysis</td>\n<td>Tracks resource flows through system components</td>\n</tr>\n</tbody></table>\n<p><strong>Interactive Analysis Tools</strong>: Enhanced dashboards include interactive tools that enable ad-hoc analysis without requiring manual query construction:</p>\n<ul>\n<li><strong>Drill-down Navigation</strong>: Click on chart elements to automatically generate detailed views of the selected time range or metric dimensions</li>\n<li><strong>Comparative Analysis</strong>: Select multiple time ranges or metric variants for side-by-side comparison</li>\n<li><strong>Annotation Support</strong>: Add contextual annotations to charts (deployment markers, incident notes, configuration changes)</li>\n<li><strong>Real-time Collaboration</strong>: Multiple users can collaborate on dashboard analysis with shared cursors and comment threads</li>\n</ul>\n<p><strong>Intelligent Dashboard Automation</strong>: Machine learning analyzes dashboard usage patterns to provide intelligent recommendations:</p>\n<ol>\n<li><strong>Automated Panel Suggestions</strong>: Based on the metrics being viewed, the system suggests additional related metrics that might provide useful context</li>\n<li><strong>Dynamic Layout Optimization</strong>: Dashboard layouts automatically adapt based on the importance and correlation of displayed metrics</li>\n<li><strong>Smart Alerting Integration</strong>: Dashboards automatically highlight panels related to active alerts and suggest relevant diagnostic queries</li>\n<li><strong>Usage-Based Optimization</strong>: Frequently accessed dashboard elements are optimized for faster loading, while rarely used panels are deprioritized</li>\n</ol>\n<p><strong>Advanced Time Navigation</strong>: Enhanced time navigation tools support complex analysis scenarios:</p>\n<ul>\n<li><strong>Multi-timeline Comparison</strong>: View the same metrics across different time periods simultaneously</li>\n<li><strong>Event Correlation</strong>: Automatically align timelines with significant events (deployments, alerts, configuration changes)</li>\n<li><strong>Intelligent Time Selection</strong>: Machine learning suggests optimal time ranges based on the type of analysis and historical patterns</li>\n</ul>\n<h3 id=\"integration-ecosystem-expansion\">Integration Ecosystem Expansion</h3>\n<p>Modern observability requires integration with a broad ecosystem of tools and services. Enhanced integration capabilities make the metrics system a central hub in the observability infrastructure.</p>\n<p><strong>Extended Data Source Support</strong>: The <code>MetricsIngester</code> is enhanced to collect data from additional sources beyond traditional application metrics:</p>\n<table>\n<thead>\n<tr>\n<th>Data Source Category</th>\n<th>Examples</th>\n<th>Integration Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Infrastructure Logs</td>\n<td>Application logs, system logs, audit logs</td>\n<td>Log parsing and metric extraction</td>\n</tr>\n<tr>\n<td>Application Tracing</td>\n<td>Distributed trace data, span metrics</td>\n<td>Trace analysis and latency metrics</td>\n</tr>\n<tr>\n<td>Business Events</td>\n<td>User actions, transaction data, revenue metrics</td>\n<td>Event stream processing</td>\n</tr>\n<tr>\n<td>External APIs</td>\n<td>Cloud service metrics, third-party SaaS data</td>\n<td>Scheduled polling and API integration</td>\n</tr>\n<tr>\n<td>IoT and Sensor Data</td>\n<td>Temperature, pressure, location data</td>\n<td>MQTT and sensor protocol support</td>\n</tr>\n</tbody></table>\n<p><strong>Bi-directional Integration</strong>: The system not only collects data but also exports insights to other tools in the observability stack. This includes pushing alert information to incident management systems, sending capacity forecasts to auto-scaling controllers, and providing metric data to business intelligence platforms.</p>\n<p><strong>Workflow Integration</strong>: Enhanced notification channels integrate with workflow automation tools to trigger automated responses to alerts. For example, alerts about disk space can automatically trigger cleanup scripts, or performance degradation alerts can initiate auto-scaling actions.</p>\n<h3 id=\"security-and-compliance-enhancements\">Security and Compliance Enhancements</h3>\n<p>Enterprise deployment requires sophisticated security and compliance capabilities that go beyond basic authentication and authorization.</p>\n<p><strong>Audit Trail and Compliance Reporting</strong>: All system interactions are logged with comprehensive audit trails that support compliance with regulations like SOX, GDPR, and HIPAA. The audit system tracks who accessed what data, when changes were made, and what decisions were taken based on metric data.</p>\n<p><strong>Data Privacy and Anonymization</strong>: The system includes data privacy features that can automatically anonymize or pseudonymize sensitive metric data based on configurable policies. This enables analytics on sensitive data while protecting individual privacy.</p>\n<p><strong>Multi-tenancy and Isolation</strong>: Enhanced multi-tenancy support provides strict data isolation between different teams or customers sharing the same infrastructure. Each tenant has separate namespaces for metrics, dashboards, and alert rules with no possibility of cross-tenant data access.</p>\n<p><strong>Advanced Authentication Integration</strong>: The system integrates with enterprise identity providers (LDAP, Active Directory, SAML, OAuth) and supports advanced authentication features like multi-factor authentication, certificate-based authentication, and just-in-time access provisioning.</p>\n<h3 id=\"performance-and-scale-optimizations\">Performance and Scale Optimizations</h3>\n<p>Beyond horizontal scaling, numerous performance optimizations can significantly improve system efficiency and user experience.</p>\n<p><strong>Intelligent Data Compression</strong>: Advanced compression algorithms are tailored specifically for time-series data patterns, achieving better compression ratios than generic algorithms. The system automatically selects optimal compression strategies based on data characteristics and access patterns.</p>\n<p><strong>Query Result Streaming</strong>: Large query results are streamed to clients rather than being buffered entirely in memory. This enables analysis of massive datasets without overwhelming system resources or client applications.</p>\n<p><strong>Adaptive Sampling and Resolution</strong>: The system automatically adjusts metric collection frequency and storage resolution based on data importance and access patterns. Critical metrics maintain high resolution, while less important metrics are sampled at lower frequencies to save resources.</p>\n<p><strong>Edge Caching and CDN Integration</strong>: Dashboard assets and frequently accessed query results are cached at edge locations to minimize latency for global users. Intelligent cache invalidation ensures data freshness while maximizing cache hit rates.</p>\n<h2 id=\"implementation-guidance\">Implementation Guidance</h2>\n<h3 id=\"technology-recommendations\">Technology Recommendations</h3>\n<p>The scalability and feature enhancements require careful technology choices that balance functionality, performance, and operational complexity:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Distributed Storage</td>\n<td>etcd + file-based sharding</td>\n<td>Apache Cassandra / ScyllaDB</td>\n<td>etcd for coordination, dedicated TSDB for heavy workloads</td>\n</tr>\n<tr>\n<td>Message Queue</td>\n<td>Go channels + persistence</td>\n<td>Apache Kafka / NATS Streaming</td>\n<td>Channels for simple cases, external queue for reliability</td>\n</tr>\n<tr>\n<td>Machine Learning</td>\n<td>Basic statistical functions</td>\n<td>TensorFlow Serving / MLflow</td>\n<td>Start simple, integrate ML platforms as needed</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static configuration</td>\n<td>Consul / etcd</td>\n<td>Static config for small deployments, service discovery for dynamic environments</td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td>HTTP reverse proxy</td>\n<td>HAProxy / Envoy</td>\n<td>Standard reverse proxy sufficient for most cases</td>\n</tr>\n</tbody></table>\n<h3 id=\"enhanced-architecture-structure\">Enhanced Architecture Structure</h3>\n<p>The extended system maintains the same core organization while adding new components for advanced features:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  cmd/\n    server/main.go                    ← enhanced with feature flags\n    migrate/main.go                   ← database migration tool\n  internal/\n    coordinator/                      ← enhanced with distributed coordination\n      coordinator.go\n      cluster_manager.go              ← NEW: cluster membership and coordination\n      health_checker.go               ← enhanced with distributed health checking\n    storage/\n      engine.go                       ← enhanced with partitioning awareness\n      distributed/                    ← NEW: distributed storage components\n        partition_manager.go\n        replication_coordinator.go\n        consistency_manager.go\n      compaction/\n        distributed_compaction.go     ← NEW: cross-node compaction coordination\n    query/\n      processor.go                    ← enhanced with advanced functions\n      distributed_router.go           ← NEW: distributed query routing\n      ml/                            ← NEW: machine learning integration\n        anomaly_detector.go\n        prediction_engine.go\n        model_registry.go\n    dashboard/\n      server.go                       ← enhanced with new chart types\n      charts/                         ← NEW: advanced visualization components\n        heatmap.go\n        network_diagram.go\n        geographic_map.go\n    alerting/\n      evaluator.go                    ← enhanced with ML-powered alerting\n      adaptive_thresholds.go          ← NEW: ML-based threshold adjustment\n    integrations/                     ← NEW: external system integrations\n      log_parser.go\n      trace_collector.go\n      workflow_triggers.go\n  pkg/\n    ml/                              ← NEW: machine learning utilities\n      features.go                     ← feature extraction for ML models\n      inference.go                    ← lightweight inference engine\n  deployments/\n    kubernetes/                       ← NEW: Kubernetes deployment manifests\n      storage-cluster.yaml\n      query-processors.yaml\n      dashboard-service.yaml\n    docker-compose/\n      distributed-stack.yml           ← multi-node development environment\n  migrations/                         ← NEW: schema migration scripts\n    001_initial_schema.sql\n    002_add_partitioning.sql</code></pre></div>\n\n<h3 id=\"distributed-storage-infrastructure\">Distributed Storage Infrastructure</h3>\n<p>The distributed storage enhancement requires several new components that coordinate data placement and access across multiple nodes:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// ClusterManager coordinates distributed operations across storage nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> ClusterManager</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodeID       </span><span style=\"color:#F97583\">string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    nodes        </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">NodeInfo</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    coordinator  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PartitionManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    replication  </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ReplicationCoordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gossip       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">GossipProtocol</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger       </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stopCh       </span><span style=\"color:#F97583\">chan</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu           </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// NewClusterManager creates a distributed coordination manager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#B392F0\"> NewClusterManager</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">nodeID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seedNodes</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">logger</span><span style=\"color:#F97583\"> *</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterManager</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Initialize gossip protocol for node discovery and failure detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Create partition manager for data placement decisions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Initialize replication coordinator for data durability</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Set up periodic maintenance tasks (failure detection, rebalancing)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Start gossip protocol and join cluster using seed nodes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DistributedStorageEngine extends StorageEngine with cluster awareness</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> DistributedStorageEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    localEngine    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">StorageEngine</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clusterManager </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ClusterManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    router         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ShardingRouter</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    config         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedStorageConfig</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// WriteSamples distributes writes across appropriate nodes with replication</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">d </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributedStorageEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">WriteSamples</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Group samples by target partition using consistent hashing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: For each partition, determine primary and replica nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Execute writes to primary nodes in parallel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Await replication confirmation based on consistency level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return error if minimum replicas not achieved within timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"machine-learning-integration-components\">Machine Learning Integration Components</h3>\n<p>The ML integration provides intelligent analysis capabilities while maintaining simple deployment for basic use cases:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// AnomalyDetector identifies unusual patterns in metric data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> AnomalyDetector</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    models      </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AnomalyModel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    features    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FeatureExtractor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    threshold   </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sensitivity </span><span style=\"color:#F97583\">float64</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu          </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// DetectAnomalies analyzes metric samples for unusual behavior</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">a </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AnomalyDetector</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">DetectAnomalies</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">samples</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">Sample</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">Anomaly</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Extract features from sample data (trend, seasonality, variance)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Load or create model for this series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Compute anomaly score based on deviation from expected pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply threshold to determine which samples are anomalous</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return anomaly objects with confidence scores and explanations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// PredictionEngine generates forecasts for capacity planning</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> PredictionEngine</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    models    </span><span style=\"color:#F97583\">map</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#F97583\">string</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ForecastModel</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    extractor </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">FeatureExtractor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cache     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PredictionCache</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger    </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mu        </span><span style=\"color:#B392F0\">sync</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">RWMutex</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateForecast predicts future values for a metric series</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">p </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">PredictionEngine</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateForecast</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">seriesID</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">horizon</span><span style=\"color:#B392F0\"> time</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Duration</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">Forecast</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Check cache for recent forecast for this series</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Retrieve historical data for model training/inference</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Extract time-series features (seasonality, trend, level)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Apply appropriate forecasting model (Holt-Winters, ARIMA, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Generate forecast with confidence intervals and cache result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"advanced-visualization-components\">Advanced Visualization Components</h3>\n<p>Enhanced visualization requires sophisticated chart rendering and interactive analysis tools:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">go</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">// HeatmapRenderer creates heatmap visualizations for distribution analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> HeatmapRenderer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    canvas     </span><span style=\"color:#B392F0\">Canvas</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    colorScale </span><span style=\"color:#B392F0\">ColorScale</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    aggregator </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">DistributionAggregator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// RenderHeatmap generates heatmap visualization from metric data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">h </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">HeatmapRenderer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">RenderHeatmap</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#E1E4E8\"> []</span><span style=\"color:#B392F0\">TimeSeries</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">config</span><span style=\"color:#B392F0\"> HeatmapConfig</span><span style=\"color:#E1E4E8\">) (</span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">ChartResult</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Aggregate data into time/value buckets based on config</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Calculate bucket densities and statistical measures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Apply color mapping based on density distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Render chart with proper axes, legends, and interactivity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return chart data with metadata for client rendering</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// InteractiveAnalyzer provides drill-down and exploration capabilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">type</span><span style=\"color:#B392F0\"> InteractiveAnalyzer</span><span style=\"color:#F97583\"> struct</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    queryProcessor </span><span style=\"color:#B392F0\">QueryProcessor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlator     </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">MetricCorrelator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    annotator      </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">AnnotationManager</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger         </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">slog</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Logger</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// GenerateRelatedQueries suggests additional metrics for analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">func</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#FFAB70\">i </span><span style=\"color:#F97583\">*</span><span style=\"color:#B392F0\">InteractiveAnalyzer</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#B392F0\">GenerateRelatedQueries</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">ctx</span><span style=\"color:#B392F0\"> context</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Context</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">baseQuery</span><span style=\"color:#F97583\"> string</span><span style=\"color:#E1E4E8\">) ([]</span><span style=\"color:#B392F0\">SuggestedQuery</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">error</span><span style=\"color:#E1E4E8\">) {</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 1: Parse base query to extract metric names and labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 2: Find correlated metrics based on historical patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 3: Generate contextually relevant query variations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 4: Rank suggestions by relevance and usefulness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    // TODO 5: Return formatted queries with explanations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"milestone-checkpoints-for-extensions\">Milestone Checkpoints for Extensions</h3>\n<p>After implementing scalability enhancements, verify the following capabilities:</p>\n<p><strong>Distributed Storage Checkpoint</strong>:</p>\n<ul>\n<li>Start 3-node cluster: <code>go run cmd/server/main.go --cluster-mode --node-id=node1 --seed-nodes=node2:8080,node3:8080</code></li>\n<li>Verify node discovery: <code>curl http://localhost:8080/api/cluster/nodes</code> should show all 3 nodes</li>\n<li>Test data distribution: ingest metrics and verify they&#39;re stored across multiple nodes</li>\n<li>Test failover: stop one node and verify queries still work with slight latency increase</li>\n<li>Expected behavior: System continues operating with one node failure, automatically rebalances data</li>\n</ul>\n<p><strong>Machine Learning Checkpoint</strong>:</p>\n<ul>\n<li>Enable anomaly detection: add <code>ml.anomaly_detection.enabled=true</code> to config</li>\n<li>Ingest normal baseline data for 24 hours with consistent patterns</li>\n<li>Inject anomalous data points (values 3x higher than normal)</li>\n<li>Verify anomaly alerts: <code>curl http://localhost:8080/api/alerts</code> should show ML-generated alerts</li>\n<li>Expected behavior: System learns normal patterns and alerts on deviations without manual thresholds</li>\n</ul>\n<p><strong>Advanced Visualization Checkpoint</strong>:</p>\n<ul>\n<li>Create heatmap dashboard: add heatmap panel to existing dashboard configuration</li>\n<li>Verify interactive features: click on heatmap cells to drill down to detailed time series</li>\n<li>Test correlation analysis: select multiple metrics and generate correlation matrix</li>\n<li>Expected behavior: Rich visualizations render correctly with interactive navigation</li>\n</ul>\n<h3 id=\"common-enhancement-pitfalls\">Common Enhancement Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Premature Distributed Deployment</strong>\nMany teams attempt distributed deployment before understanding their actual scale requirements. Distributed systems add significant operational complexity—multiple failure modes, complex debugging, consistency challenges. <strong>Why it&#39;s wrong</strong>: The coordination overhead can actually reduce performance for small-scale deployments. <strong>How to fix</strong>: Implement and validate single-node optimizations first. Only distribute when single-node capacity is genuinely exceeded (&gt;100GB/day ingestion or &gt;1000 queries/second).</p>\n<p>⚠️ <strong>Pitfall: Over-Complex ML Integration</strong>\nTeams often integrate sophisticated ML platforms for simple use cases that could be solved with basic statistical methods. <strong>Why it&#39;s wrong</strong>: Complex ML infrastructure requires specialized expertise, adds deployment dependencies, and introduces model training/versioning overhead. <strong>How to fix</strong>: Start with simple statistical functions (moving averages, standard deviation thresholds) and only add ML complexity when clear value is demonstrated.</p>\n<p>⚠️ <strong>Pitfall: Feature Creep in Visualization</strong>\nEnhanced dashboards can become overwhelmingly complex with too many chart types, interactive features, and configuration options. <strong>Why it&#39;s wrong</strong>: Complex interfaces reduce usability and increase maintenance burden. <strong>How to fix</strong>: Follow progressive disclosure principles—start with simple, common visualizations and add advanced features only when specific use cases are identified.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Backward Compatibility</strong>\nEnhancements often break existing configurations, APIs, or data formats in pursuit of new features. <strong>Why it&#39;s wrong</strong>: Forces users to rewrite configurations and dashboards during upgrades, creating adoption friction. <strong>How to fix</strong>: Design extensions that enhance existing interfaces rather than replacing them. Provide migration tools and maintain compatibility layers for deprecated features.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Performance Testing</strong>\nEnhanced features often have different performance characteristics that aren&#39;t validated under realistic load. <strong>Why it&#39;s wrong</strong>: New features may work well in development but degrade system performance in production. <strong>How to fix</strong>: Implement performance benchmarks for each enhancement and validate them with realistic data volumes and query patterns before deployment.</p>\n<h3 id=\"integration-testing-strategy\">Integration Testing Strategy</h3>\n<p>Comprehensive integration testing ensures enhancements work correctly with existing functionality:</p>\n<p><strong>Distributed System Testing</strong>: Use containerized environments to simulate multi-node deployments with realistic network latency and failure scenarios. Test network partitions, node failures, and data consistency under various failure modes.</p>\n<p><strong>ML Model Validation</strong>: Create synthetic datasets with known anomaly patterns to validate ML model accuracy. Test model performance degradation over time and automatic retraining capabilities.</p>\n<p><strong>Cross-Feature Integration</strong>: Test how advanced visualizations perform with ML-generated data, how distributed queries work with enhanced analytics functions, and how notification integrations handle increased alert volume from ML-powered alerting.</p>\n<p><strong>Performance Regression Testing</strong>: Establish performance baselines before implementing enhancements and validate that new features don&#39;t degrade core system performance. Pay particular attention to query latency impact from advanced analytics functions.</p>\n<p>The extensibility design ensures these enhancements integrate seamlessly with the existing architecture while providing clear upgrade paths for organizations with growing monitoring requirements.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All milestones (this comprehensive terminology reference supports understanding across the entire metrics and alerting system)</p>\n</blockquote>\n<h3 id=\"the-dictionary-metaphor-building-a-shared-language\">The Dictionary Metaphor: Building a Shared Language</h3>\n<p>Think of this glossary as a specialized dictionary for our metrics and alerting system. Just as doctors, lawyers, and engineers each have their own professional vocabulary that enables precise communication within their field, our system has specific terminology that carries exact technical meanings. When we say &quot;cardinality explosion,&quot; every team member should immediately understand we&#39;re talking about a performance problem caused by too many unique label combinations, not just &quot;lots of data.&quot;</p>\n<p>This shared vocabulary serves three critical purposes: it eliminates ambiguity in technical discussions, it helps new team members quickly understand system concepts, and it ensures consistent naming across code, documentation, and monitoring. Just as a medical dictionary defines &quot;tachycardia&quot; as &quot;heart rate above 100 beats per minute&quot; rather than just &quot;fast heartbeat,&quot; our glossary provides precise definitions that translate directly to measurable system behaviors.</p>\n<p>The terms in this glossary fall into several categories: data model concepts that define what we store and how, architectural patterns that describe how components interact, operational procedures that define how the system behaves, and troubleshooting vocabulary that helps identify and resolve issues. Each definition includes not just what the term means, but why it matters and how it impacts system design decisions.</p>\n<h3 id=\"core-data-model-terms\">Core Data Model Terms</h3>\n<p><strong>Abstract Syntax Tree (AST)</strong>: A hierarchical representation of a parsed query structure that captures the logical organization of operations, functions, and data references without concern for the specific textual syntax used to express them. The AST enables query optimization, validation, and execution planning by providing a standardized internal representation that separates query semantics from parsing details.</p>\n<p><strong>Alert Instance</strong>: A specific occurrence of an alert rule evaluation that represents the current state, value, and notification history for a particular combination of alert rule and time series labels. Each instance tracks its lifecycle through states like pending, firing, and resolved, maintaining timestamps for state changes and a log of notification attempts to enable proper alert management and debugging.</p>\n<p><strong>Alert Rule</strong>: A configuration that defines the conditions under which alerts should be triggered, specifying a metric query expression, comparison operator, threshold value, evaluation interval, and notification settings. Alert rules serve as templates that generate alert instances when their conditions are met, enabling automated monitoring of system health and business metrics.</p>\n<p><strong>Alert State</strong>: An enumerated value representing the current lifecycle phase of an alert instance, including inactive (condition not met), pending (condition met but duration requirement not satisfied), firing (condition met for required duration), resolved (condition no longer met), and silenced (notifications suppressed). State transitions follow a defined state machine that ensures proper notification behavior and prevents alert flapping.</p>\n<p><strong>Alert State Machine</strong>: A finite state automaton that governs transitions between alert lifecycle phases, defining valid state changes based on evaluation results, duration requirements, and administrative actions. The state machine ensures consistent alert behavior, prevents invalid transitions like jumping directly from inactive to firing, and manages notification timing to avoid spam while ensuring critical alerts are communicated promptly.</p>\n<p><strong>Cardinality</strong>: The number of unique time-series combinations in a dataset, calculated as the product of all possible label value combinations for each metric name. High cardinality can severely impact query performance and memory usage, as each unique combination requires separate storage and indexing, making cardinality management crucial for system scalability.</p>\n<p><strong>Cardinality Explosion</strong>: A performance-degrading condition where the number of unique time-series combinations grows exponentially due to high-variability labels like user IDs, request IDs, or timestamps being used as label values. This explosion can overwhelm storage systems, consume excessive memory during queries, and make the system unresponsive, requiring careful label design and cardinality monitoring.</p>\n<p><strong>Dashboard</strong>: A structured collection of visualization panels that displays metric data in charts, graphs, and tables, organized according to a saved configuration that specifies layout, queries, time ranges, and refresh intervals. Dashboards serve as the primary interface for observing system behavior and investigating issues, providing both overview perspectives and detailed drill-down capabilities.</p>\n<p><strong>Histogram Buckets</strong>: Predefined ranges that group metric observations into discrete intervals for distribution analysis, enabling calculation of percentiles, averages, and counts across different value ranges. Each bucket maintains a count of observations falling within its range, allowing statistical analysis of latency distributions, request sizes, and other measured quantities without storing individual sample values.</p>\n<p><strong>Labels</strong>: Key-value pairs attached to metrics that provide dimensional metadata enabling filtering, grouping, and aggregation across different facets of the data. Labels transform flat metric streams into multi-dimensional datasets, allowing queries like &quot;CPU usage for service=web, environment=production&quot; while requiring careful management to prevent cardinality explosion.</p>\n<p><strong>Metric</strong>: A named measurement that captures quantitative information about system behavior, application performance, or business processes over time. Metrics include metadata like type (counter, gauge, histogram), labels for dimensional filtering, and a stream of timestamped sample values that enable trend analysis and alerting.</p>\n<p><strong>Panel</strong>: An individual visualization component within a dashboard that displays metric data as charts, graphs, tables, or other visual formats according to configured queries, time ranges, and display options. Panels can be resized, repositioned, and configured independently, allowing flexible dashboard layouts that match specific monitoring needs.</p>\n<p><strong>Sample</strong>: A single timestamped measurement value representing the state of a metric at a specific point in time, consisting of a numeric value and a timestamp that enables time-series analysis. Samples are the atomic units of metric data, and their efficient storage and retrieval determines system performance and query capabilities.</p>\n<p><strong>Series ID</strong>: A unique identifier for a specific metric-labels combination computed as a hash of the metric name and sorted label pairs, enabling efficient storage indexing and query optimization. Series IDs allow the system to quickly locate related samples without expensive string comparisons and provide a stable reference for time-series data across compaction and retention operations.</p>\n<p><strong>Time-Series Data</strong>: Sequences of timestamped measurements that capture how values change over time, enabling trend analysis, forecasting, and anomaly detection. Time-series data exhibits temporal locality patterns that can be exploited for efficient storage compression and query optimization.</p>\n<h3 id=\"storage-and-query-terms\">Storage and Query Terms</h3>\n<p><strong>Block-Based Storage</strong>: An organizational strategy that groups time-series samples into immutable, time-windowed storage blocks that can be efficiently compressed, indexed, and queried. This approach enables effective compaction strategies, optimizes disk I/O patterns, and supports retention policies by allowing entire blocks to be deleted when they exceed configured age limits.</p>\n<p><strong>Compaction</strong>: The process of merging and downsampling stored data blocks to improve query performance, reduce storage overhead, and maintain system efficiency over time. Compaction combines multiple small blocks into larger ones, applies compression algorithms, and can downsample high-resolution data to lower resolutions for long-term storage while preserving statistical properties.</p>\n<p><strong>Counter Reset</strong>: A situation where a counter metric&#39;s value decreases, typically indicating a process restart, metric redefinition, or data collection issue. Counter resets require special handling in rate calculations to avoid negative spikes and ensure accurate trend analysis, often involving reset detection algorithms and rate calculation adjustments.</p>\n<p><strong>Downsampling</strong>: The process of reducing data resolution by aggregating high-frequency samples into lower-frequency summaries while preserving important statistical properties like averages, maximums, and percentiles. Downsampling enables long-term data retention by reducing storage requirements for older data while maintaining sufficient detail for historical analysis.</p>\n<p><strong>Query Execution Pipeline</strong>: A multi-phase process that transforms query strings into results through parsing, query planning, storage access, data aggregation, and result formatting. The pipeline enables optimization opportunities at each stage, supports caching strategies, and provides clear error isolation for debugging query performance issues.</p>\n<p><strong>Retention Policy</strong>: Automated rules that manage data lifecycle by deleting or downsampling data that exceeds configured age limits, preventing unlimited storage growth while preserving important historical information. Retention policies must balance storage costs with analytical needs, often implementing tiered strategies that keep high-resolution recent data and low-resolution historical data.</p>\n<p><strong>Streaming Aggregation</strong>: A processing technique that computes aggregation functions incrementally as data flows through the system, without requiring the entire dataset to be buffered in memory. This approach enables real-time analytics on large datasets and reduces memory pressure during query execution, particularly important for high-cardinality queries.</p>\n<p><strong>Timestamp Alignment</strong>: The process of synchronizing samples from multiple time series to common time intervals to enable mathematical operations and comparisons between series. Alignment handles cases where different metrics are sampled at different frequencies or times, using interpolation or alignment functions to create consistent time grids.</p>\n<p><strong>Write-Ahead Log (WAL)</strong>: A durability mechanism that persists metric samples to disk before acknowledging writes, ensuring data survival across system crashes and enabling recovery by replaying logged operations. The WAL provides atomicity guarantees for batch writes and supports consistent recovery semantics essential for reliable metric storage.</p>\n<h3 id=\"query-and-analysis-terms\">Query and Analysis Terms</h3>\n<p><strong>Aggregation Function</strong>: A mathematical operation that combines multiple time-series values into summary statistics like sum, average, maximum, minimum, or percentiles across specified dimensions or time ranges. Aggregation functions enable dashboard visualizations and alert evaluations that operate on grouped data rather than individual samples.</p>\n<p><strong>Cache Entry</strong>: A stored query result with associated metadata including the original query, result data, creation timestamp, access statistics, and expiration information. Cache entries enable query result reuse to improve dashboard performance and reduce storage system load for frequently accessed data.</p>\n<p><strong>Counter Semantics</strong>: The behavioral rules governing counter-type metrics, which represent monotonically increasing cumulative values that can only increase or reset to zero. Counter semantics require special handling for rate calculations, reset detection, and ensuring that derived metrics like queries per second correctly handle counter resets.</p>\n<p><strong>Gauge Semantics</strong>: The behavioral rules for gauge-type metrics, which represent point-in-time values that can increase or decrease freely, such as memory usage, queue lengths, or temperature readings. Gauge semantics allow direct value comparisons and mathematical operations without the reset-handling complexity required for counters.</p>\n<p><strong>Label Explosion</strong>: A condition where the number of unique label combinations grows uncontrollably, typically caused by using high-cardinality values like user IDs, request IDs, or IP addresses as label values. Label explosion can severely degrade query performance and exhaust system memory, requiring careful label design and monitoring.</p>\n<p><strong>Lexical Analysis</strong>: The process of breaking query strings into meaningful tokens like metric names, operators, functions, and literal values, serving as the first phase of query parsing. Lexical analysis handles syntax elements like whitespace, quotes, and escape sequences while identifying token types that guide subsequent parsing phases.</p>\n<p><strong>Query Result Caching</strong>: A performance optimization technique that stores expensive query results in memory with associated cache keys, expiration times, and invalidation rules to avoid recomputing identical queries. Caching reduces storage system load and improves dashboard responsiveness, particularly for complex aggregation queries over large time ranges.</p>\n<h3 id=\"operational-and-monitoring-terms\">Operational and Monitoring Terms</h3>\n<p><strong>Adaptive refresh rates</strong>: Dynamically adjusted update frequencies for dashboard panels based on system performance, data velocity, and user interaction patterns. Adaptive refresh rates prevent system overload during high-traffic periods while ensuring timely updates for critical monitoring scenarios.</p>\n<p><strong>Alert Flapping</strong>: Rapid oscillation between firing and resolved states caused by threshold values that are too close to normal fluctuations in metric values. Flapping creates notification storms that desensitize users to real problems and can be prevented through proper threshold selection, hysteresis, and duration requirements.</p>\n<p><strong>Backpressure</strong>: A flow control mechanism that prevents system overload by slowing down or rejecting input when downstream components cannot keep up with the incoming data rate. Backpressure protects system stability during traffic spikes but must be carefully implemented to avoid cascading delays.</p>\n<p><strong>Circuit Breaker</strong>: A fault tolerance pattern that prevents cascade failures by automatically stopping requests to failing services after a configured failure threshold is exceeded. Circuit breakers provide fast-fail behavior during outages and can automatically recover when the downstream service becomes healthy again.</p>\n<p><strong>Dashboard Templating</strong>: A variable substitution system that enables creation of reusable parameterized dashboards where users can select different services, environments, or time ranges without creating separate dashboard configurations. Templating reduces configuration maintenance and enables consistent monitoring across similar components.</p>\n<p><strong>Duration-Based Evaluation</strong>: An alert evaluation strategy that requires conditions to persist for a specified time period before transitioning to firing state, preventing transient spikes from triggering false alerts. Duration-based evaluation filters noise but introduces delay in alert notification, requiring balanced configuration.</p>\n<p><strong>Exponential Backoff</strong>: A retry delay strategy where the wait time increases exponentially with each failed attempt, helping to reduce system load during outages while still attempting recovery. Exponential backoff prevents retry storms that could worsen system problems during failures.</p>\n<p><strong>Graceful Degradation</strong>: System design that maintains partial functionality during failures rather than complete unavailability, such as displaying cached dashboard data when query systems are unavailable. Graceful degradation improves user experience during outages and maintains basic monitoring capabilities.</p>\n<p><strong>Grid Layout System</strong>: A responsive positioning framework for organizing dashboard panels that automatically adjusts to different screen sizes and supports drag-and-drop rearrangement. Grid layouts provide consistent visual organization while enabling flexible dashboard customization.</p>\n<p><strong>Health Checking</strong>: Periodic verification of component availability and functionality through automated tests that validate both internal state and external dependencies. Health checks enable automated failure detection, load balancer routing decisions, and system health dashboards.</p>\n<p><strong>Horizontal Partitioning</strong>: A data distribution strategy that splits datasets across multiple storage nodes based on partitioning keys like metric name hash or time ranges. Horizontal partitioning enables system scaling by distributing load and storage requirements across multiple machines.</p>\n<p><strong>Incremental Updates</strong>: A data transmission optimization that sends only new or changed information rather than complete datasets, reducing network bandwidth and improving dashboard responsiveness. Incremental updates require careful state management to ensure data consistency and handle missed updates.</p>\n<p><strong>Message Templating</strong>: A customizable formatting system for notification content that allows channel-specific message generation using variables from alert context like metric values, labels, and timestamps. Templates enable consistent notification formatting while supporting different communication channels.</p>\n<p><strong>Notification Routing</strong>: A rule-based system that determines which communication channels receive specific alert notifications based on factors like alert severity, time of day, escalation policies, and team assignments. Proper routing ensures the right people receive timely notifications without overwhelming any individual or channel.</p>\n<p><strong>Panel Subscription</strong>: A client-side registration system that tracks which dashboard panels a user is viewing and their required refresh rates, enabling efficient server-side update delivery through WebSocket connections. Subscriptions prevent unnecessary data transmission for off-screen panels.</p>\n<p><strong>Pull-Based Collection</strong>: A metrics gathering approach where the monitoring system actively scrapes metric data from application endpoints at regular intervals, providing centralized control over collection timing and enabling service discovery integration. Pull-based collection simplifies application configuration but requires network connectivity and service registration.</p>\n<p><strong>Push-Based Collection</strong>: A metrics transmission method where applications actively send metric data to the monitoring system, enabling immediate data delivery and supporting scenarios where pull-based access is not feasible. Push-based collection provides lower latency but requires applications to handle delivery failures and retries.</p>\n<p><strong>WebSocket Connections</strong>: Persistent bidirectional communication channels between dashboard clients and servers that enable real-time data updates without polling overhead. WebSocket connections support efficient live dashboard updates but require connection management and reconnection handling for reliability.</p>\n<h3 id=\"error-handling-and-reliability-terms\">Error Handling and Reliability Terms</h3>\n<p><strong>Cascade Failure</strong>: A failure propagation pattern where problems in one system component trigger failures in dependent components, potentially leading to complete system unavailability. Cascade failures can be prevented through circuit breakers, timeouts, bulkheads, and other fault isolation techniques.</p>\n<p><strong>Correlation IDs</strong>: Unique identifiers attached to requests that enable tracing of operations across multiple system components and log aggregation for debugging distributed system interactions. Correlation IDs are essential for troubleshooting complex failures that span multiple services.</p>\n<p><strong>Delivery Confirmation</strong>: Verification mechanisms that ensure alert notifications were successfully transmitted to their intended destinations, including retry logic for failed deliveries and escalation procedures for persistent failures. Delivery confirmation is critical for ensuring that important alerts reach their intended recipients.</p>\n<p><strong>Error Injection</strong>: A testing technique that deliberately introduces failures into system components to validate error handling code paths and system resilience. Error injection helps identify weak points in fault tolerance and ensures that error handling code actually works as intended.</p>\n<p><strong>Notification Queue</strong>: A background processing system that manages alert notification delivery with features like retry logic, rate limiting, and priority handling to ensure reliable message delivery while preventing notification storms. Queues provide delivery guarantees and help manage notification volume during incident situations.</p>\n<p><strong>Rate Limiting</strong>: A throttling mechanism that prevents notification storms by limiting the number of alerts that can be sent through specific channels within defined time windows. Rate limiting protects communication channels from overload while ensuring critical alerts still get delivered.</p>\n<p><strong>State Persistence</strong>: Durability mechanisms that ensure alert states, dashboard configurations, and other critical system data survive system restarts and failures. State persistence enables proper alert lifecycle management and prevents data loss during system maintenance or unexpected outages.</p>\n<h3 id=\"testing-and-development-terms\">Testing and Development Terms</h3>\n<p><strong>Integration Testing</strong>: Testing approach that validates component interactions using real dependencies and network communication to ensure the complete system works correctly end-to-end. Integration testing catches issues that unit tests miss, such as serialization problems, network timeouts, and protocol mismatches.</p>\n<p><strong>Milestone Validation Checkpoints</strong>: Structured verification procedures that confirm major system capabilities are working correctly after each development phase, including specific test cases, expected behaviors, and success criteria. Checkpoints help ensure progress is solid before moving to more complex features.</p>\n<p><strong>Mock Dependencies</strong>: Test doubles that simulate the behavior of external components like databases, network services, and file systems, enabling isolated unit testing and controlled failure scenario testing. Mocks allow testing error conditions that would be difficult or dangerous to reproduce with real systems.</p>\n<p><strong>Performance Validation</strong>: Testing procedures that verify system behavior under realistic load conditions, including throughput measurement, latency analysis, and resource utilization monitoring. Performance validation ensures the system can handle production workloads and identifies scalability bottlenecks.</p>\n<p><strong>Test Fixtures</strong>: Reusable test data sets, configuration files, and utility functions that provide consistent testing environments and reduce test setup complexity. Fixtures enable reliable test reproduction and make it easier to create comprehensive test coverage.</p>\n<p><strong>Test Harness</strong>: Infrastructure for setting up complete system instances in test environments, including component coordination, configuration management, and cleanup procedures. Test harnesses enable integration testing and provide controlled environments for manual testing and debugging.</p>\n<p><strong>Unit Testing</strong>: Testing individual components in isolation with mocked dependencies to validate specific functionality, error conditions, and edge cases. Unit testing provides fast feedback during development and ensures individual components work correctly before integration.</p>\n<h3 id=\"advanced-and-extension-terms\">Advanced and Extension Terms</h3>\n<p><strong>Anomaly Detection</strong>: Machine learning techniques that identify unusual patterns in metric data that deviate significantly from historical behavior, enabling proactive alerting for problems that don&#39;t have known thresholds. Anomaly detection can identify subtle issues that traditional threshold-based alerting would miss.</p>\n<p><strong>Auto-Scaling</strong>: Automatic adjustment of system capacity based on demand metrics like CPU usage, memory pressure, or queue lengths, enabling cost optimization while maintaining performance. Auto-scaling requires careful configuration to avoid oscillation and must integrate with deployment systems.</p>\n<p><strong>Consistency Level</strong>: Guarantees about data consistency across replicas in distributed systems, ranging from eventual consistency (data will be consistent eventually) to strong consistency (all reads return the most recent write). Consistency levels involve trade-offs between performance and data accuracy.</p>\n<p><strong>Feature Extraction</strong>: The process of deriving meaningful variables from raw metric data for machine learning analysis, such as trend calculations, seasonal decomposition, and statistical summaries. Feature extraction transforms time-series data into formats suitable for classification and prediction algorithms.</p>\n<p><strong>Gossip Protocol</strong>: A decentralized communication method for sharing cluster membership information and failure detection where each node periodically exchanges state information with a subset of other nodes. Gossip protocols provide eventual consistency and fault tolerance without requiring central coordination.</p>\n<p><strong>Heatmap Visualization</strong>: Color-coded matrix displays that show data density, value distributions, or correlation patterns across two-dimensional parameter spaces. Heatmaps are particularly useful for showing request latency distributions over time or error rates across different services.</p>\n<p><strong>Horizontal Scaling</strong>: System architecture that handles increased load by adding more servers rather than upgrading existing hardware, enabling theoretically unlimited capacity growth. Horizontal scaling requires careful design of data partitioning, load balancing, and coordination protocols.</p>\n<p><strong>Model Registry</strong>: Centralized management system for trained machine learning models, including version control, deployment tracking, and performance monitoring. Model registries enable systematic management of ML-powered features like anomaly detection and forecasting.</p>\n<p><strong>Multi-Tenancy</strong>: System design that supports multiple isolated user groups on shared infrastructure while maintaining data isolation, performance guarantees, and security boundaries. Multi-tenancy reduces operational overhead while requiring careful resource management and access control.</p>\n<p><strong>Predictive Alerting</strong>: Alert systems that use forecasting models to identify potential future problems before they occur, enabling proactive intervention rather than reactive response. Predictive alerting can prevent outages but requires sophisticated modeling and careful false positive management.</p>\n<p><strong>Quorum-Based Approach</strong>: Decision-making systems that require agreement from a majority of participants to proceed with operations, preventing split-brain scenarios in distributed systems. Quorum systems provide strong consistency guarantees but require careful configuration to handle network partitions.</p>\n<p><strong>Replication Factor</strong>: The number of copies of each data item stored across different nodes in a distributed system, providing durability and availability at the cost of storage overhead and consistency complexity. Higher replication factors improve fault tolerance but increase coordination requirements.</p>\n<p><strong>Split-Brain Prevention</strong>: Architectural patterns and protocols that avoid scenarios where network partitions cause different parts of a distributed system to make conflicting decisions. Prevention techniques include quorum systems, witness nodes, and careful leader election protocols.</p>\n<h3 id=\"performance-and-optimization-terms\">Performance and Optimization Terms</h3>\n<p><strong>Compression Optimization</strong>: Tailoring data compression algorithms to specific data patterns found in time-series metrics, such as delta encoding for counter values and dictionary encoding for repetitive label values. Optimized compression significantly reduces storage requirements and improves query performance.</p>\n<p><strong>Edge Caching</strong>: Performance optimization that stores frequently accessed data closer to users or applications to reduce latency and backend load. Edge caching is particularly effective for dashboard data and metric metadata that are accessed repeatedly.</p>\n<p><strong>Load Balancing</strong>: Distribution of incoming requests across multiple servers to optimize resource utilization, minimize response time, and prevent individual server overload. Load balancing requires careful selection of algorithms and health checking to ensure optimal distribution.</p>\n<p><strong>Query Optimization</strong>: Techniques for improving query performance through better execution planning, index utilization, result caching, and algorithm selection. Query optimization is crucial for maintaining dashboard responsiveness and alert evaluation performance as data volumes grow.</p>\n<h3 id=\"acronyms-and-abbreviations\">Acronyms and Abbreviations</h3>\n<p><strong>ADR</strong>: Architecture Decision Record - a structured format for documenting important design decisions with context, options, rationales, and consequences.</p>\n<p><strong>API</strong>: Application Programming Interface - the defined methods and data formats that components use to communicate with each other.</p>\n<p><strong>AST</strong>: Abstract Syntax Tree - the hierarchical representation of parsed queries used for optimization and execution.</p>\n<p><strong>HTTP</strong>: HyperText Transfer Protocol - the communication protocol used for web-based interfaces and REST APIs.</p>\n<p><strong>JSON</strong>: JavaScript Object Notation - the data serialization format used for configuration files, API responses, and data exchange.</p>\n<p><strong>REST</strong>: Representational State Transfer - an architectural style for designing web APIs using standard HTTP methods.</p>\n<p><strong>SLA</strong>: Service Level Agreement - formal commitments about system availability, performance, and reliability.</p>\n<p><strong>SLI</strong>: Service Level Indicator - specific metrics that measure system performance against defined objectives.</p>\n<p><strong>SLO</strong>: Service Level Objective - target values or ranges for service level indicators that define acceptable performance.</p>\n<p><strong>WAL</strong>: Write-Ahead Log - durability mechanism that records intended operations before executing them to enable recovery.</p>\n<p><strong>YAML</strong>: YAML Ain&#39;t Markup Language - human-readable data serialization format commonly used for configuration files.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The terminology in this glossary directly maps to specific implementation patterns and naming conventions used throughout the codebase. Each term has been carefully chosen to reflect industry standards while maintaining consistency with our specific architectural decisions.</p>\n<p><strong>A. Naming Convention Mapping</strong></p>\n<p>The glossary terms serve as the foundation for consistent naming across all system components. When implementing any feature, developers should use these exact terms in variable names, function names, and documentation to maintain clarity and searchability.</p>\n<table>\n<thead>\n<tr>\n<th>Concept Category</th>\n<th>Implementation Pattern</th>\n<th>Example Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Structures</td>\n<td>Direct mapping to struct/type names</td>\n<td><code>AlertState</code>, <code>MetricType</code>, <code>Labels</code></td>\n</tr>\n<tr>\n<td>Operations</td>\n<td>Verb-noun pattern using glossary terms</td>\n<td><code>ValidateCardinality()</code>, <code>CompactBlocks()</code></td>\n</tr>\n<tr>\n<td>Constants</td>\n<td>ALL_CAPS with underscore separation</td>\n<td><code>ALERT_STATE_FIRING</code>, <code>METRIC_TYPE_COUNTER</code></td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Nested structures following glossary hierarchy</td>\n<td><code>StorageConfig.RetentionPeriod</code></td>\n</tr>\n</tbody></table>\n<p><strong>B. Documentation Standards</strong></p>\n<p>Every public function, method, and type should include documentation that references appropriate glossary terms to ensure consistent understanding across the codebase. This creates a self-reinforcing system where code documentation strengthens terminology understanding.</p>\n<p><strong>C. Error Message Consistency</strong></p>\n<p>Error messages throughout the system should use glossary terminology to help users quickly understand problems and find relevant documentation. For example, &quot;cardinality explosion detected&quot; is more helpful than &quot;too many time series&quot; because it points to specific glossary entries and troubleshooting procedures.</p>\n<p><strong>D. Monitoring and Alerting Vocabulary</strong></p>\n<p>The operational procedures and alert definitions should use glossary terms in their names and descriptions, creating consistency between system behavior and documentation. This makes it easier to correlate alerts with architectural documentation and debugging procedures.</p>\n<p><strong>E. Team Communication Guidelines</strong></p>\n<p>During code reviews, design discussions, and incident response, team members should use glossary terminology to ensure precise communication. When someone uses a term differently than the glossary definition, it signals a need to either update the glossary or clarify the usage to maintain consistency.</p>\n<p>The glossary serves as both a reference document and a communication contract that enables precise technical discussions and consistent system implementation across the entire metrics and alerting platform.</p>\n","toc":[{"level":1,"text":"Metrics &amp; Alerting Dashboard: Design Document","id":"metrics-amp-alerting-dashboard-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"The Hospital Monitoring Analogy","id":"the-hospital-monitoring-analogy"},{"level":3,"text":"Technical Challenges","id":"technical-challenges"},{"level":4,"text":"High-Cardinality Data Management","id":"high-cardinality-data-management"},{"level":4,"text":"Storage Efficiency and Compression","id":"storage-efficiency-and-compression"},{"level":4,"text":"Query Performance at Scale","id":"query-performance-at-scale"},{"level":4,"text":"Alert Reliability and Accuracy","id":"alert-reliability-and-accuracy"},{"level":3,"text":"Existing Solutions Comparison","id":"existing-solutions-comparison"},{"level":4,"text":"Prometheus: Pull-Based Open Source","id":"prometheus-pull-based-open-source"},{"level":4,"text":"InfluxDB: Purpose-Built Time-Series Database","id":"influxdb-purpose-built-time-series-database"},{"level":4,"text":"DataDog: Cloud-Native SaaS","id":"datadog-cloud-native-saas"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Stack Recommendations","id":"technology-stack-recommendations"},{"level":4,"text":"Project Structure Foundation","id":"project-structure-foundation"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Common Debugging Issues","id":"common-debugging-issues"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"The Goldilocks Principle for System Design","id":"the-goldilocks-principle-for-system-design"},{"level":3,"text":"Functional Goals","id":"functional-goals"},{"level":4,"text":"Metric Collection and Storage","id":"metric-collection-and-storage"},{"level":4,"text":"Time-Series Data Management","id":"time-series-data-management"},{"level":4,"text":"Query and Visualization Capabilities","id":"query-and-visualization-capabilities"},{"level":4,"text":"Alerting and Notification System","id":"alerting-and-notification-system"},{"level":3,"text":"Non-Functional Goals","id":"non-functional-goals"},{"level":4,"text":"Performance Requirements","id":"performance-requirements"},{"level":4,"text":"Reliability Requirements","id":"reliability-requirements"},{"level":4,"text":"Scalability Requirements","id":"scalability-requirements"},{"level":4,"text":"Operational Requirements","id":"operational-requirements"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":4,"text":"Advanced Analytics and Machine Learning","id":"advanced-analytics-and-machine-learning"},{"level":4,"text":"Multi-Tenancy and Access Control","id":"multi-tenancy-and-access-control"},{"level":4,"text":"Distributed Deployment and High Availability","id":"distributed-deployment-and-high-availability"},{"level":4,"text":"Log Aggregation and Tracing Integration","id":"log-aggregation-and-tracing-integration"},{"level":4,"text":"Advanced Visualization and Dashboarding","id":"advanced-visualization-and-dashboarding"},{"level":4,"text":"Performance Beyond Single-Node Limits","id":"performance-beyond-single-node-limits"},{"level":4,"text":"Complex Query Languages and Analytics","id":"complex-query-languages-and-analytics"},{"level":3,"text":"Success Criteria and Acceptance","id":"success-criteria-and-acceptance"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Foundation","id":"technology-foundation"},{"level":4,"text":"Project Structure","id":"project-structure"},{"level":4,"text":"Core Type Definitions","id":"core-type-definitions"},{"level":4,"text":"HTTP Server Foundation","id":"http-server-foundation"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"The Orchestra Metaphor: Understanding System Architecture","id":"the-orchestra-metaphor-understanding-system-architecture"},{"level":3,"text":"Component Responsibilities","id":"component-responsibilities"},{"level":3,"text":"Data Flow Overview","id":"data-flow-overview"},{"level":3,"text":"Deployment Architecture","id":"deployment-architecture"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Architecture Skeleton","id":"core-architecture-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"The Blueprint Metaphor: Understanding System Data Models","id":"the-blueprint-metaphor-understanding-system-data-models"},{"level":3,"text":"Metric Types and Structure","id":"metric-types-and-structure"},{"level":3,"text":"Time-Series Schema","id":"time-series-schema"},{"level":3,"text":"Alert and Dashboard Schema","id":"alert-and-dashboard-schema"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Metrics Ingestion Engine","id":"metrics-ingestion-engine"},{"level":3,"text":"Mental Model: The Data Processing Factory","id":"mental-model-the-data-processing-factory"},{"level":3,"text":"Push vs Pull Ingestion Models","id":"push-vs-pull-ingestion-models"},{"level":4,"text":"Push-Based Ingestion Architecture","id":"push-based-ingestion-architecture"},{"level":4,"text":"Pull-Based Ingestion Architecture","id":"pull-based-ingestion-architecture"},{"level":4,"text":"Implementation Strategy Comparison","id":"implementation-strategy-comparison"},{"level":4,"text":"Hybrid Implementation Design","id":"hybrid-implementation-design"},{"level":3,"text":"Validation and Preprocessing","id":"validation-and-preprocessing"},{"level":4,"text":"Label Cardinality Control","id":"label-cardinality-control"},{"level":4,"text":"Metric Type Validation","id":"metric-type-validation"},{"level":4,"text":"Data Normalization","id":"data-normalization"},{"level":4,"text":"Error Handling and Feedback","id":"error-handling-and-feedback"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Hints","id":"language-specific-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Time-Series Storage Engine","id":"time-series-storage-engine"},{"level":3,"text":"Mental Model: The Library Archive System","id":"mental-model-the-library-archive-system"},{"level":3,"text":"Storage Format and Indexing","id":"storage-format-and-indexing"},{"level":3,"text":"Compaction and Retention Policies","id":"compaction-and-retention-policies"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Query Engine","id":"query-engine"},{"level":3,"text":"Mental Model: The Research Assistant","id":"mental-model-the-research-assistant"},{"level":3,"text":"Query Language Design","id":"query-language-design"},{"level":3,"text":"Query Execution Pipeline","id":"query-execution-pipeline"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure","id":"file-structure"},{"level":4,"text":"Query Parser Infrastructure","id":"query-parser-infrastructure"},{"level":4,"text":"Query Execution Engine Core","id":"query-execution-engine-core"},{"level":4,"text":"Aggregation Function Library","id":"aggregation-function-library"},{"level":4,"text":"Query Result Caching System","id":"query-result-caching-system"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Visualization Dashboard","id":"visualization-dashboard"},{"level":3,"text":"Mental Model: The Mission Control Center","id":"mental-model-the-mission-control-center"},{"level":3,"text":"Dashboard Configuration System","id":"dashboard-configuration-system"},{"level":3,"text":"Real-Time Data Updates","id":"real-time-data-updates"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Alerting System","id":"alerting-system"},{"level":3,"text":"Mental Model: The Security Guard System","id":"mental-model-the-security-guard-system"},{"level":3,"text":"Alert Rule Definition and Evaluation","id":"alert-rule-definition-and-evaluation"},{"level":3,"text":"Alert State Management","id":"alert-state-management"},{"level":3,"text":"Notification Channel Integration","id":"notification-channel-integration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure","id":"file-structure"},{"level":4,"text":"Alert State Management Infrastructure","id":"alert-state-management-infrastructure"},{"level":4,"text":"Alert Evaluation Engine Core","id":"alert-evaluation-engine-core"},{"level":4,"text":"Notification Delivery System","id":"notification-delivery-system"},{"level":4,"text":"Email Notification Channel Implementation","id":"email-notification-channel-implementation"},{"level":4,"text":"Slack Notification Channel Implementation","id":"slack-notification-channel-implementation"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Component Interactions and Data Flow","id":"component-interactions-and-data-flow"},{"level":3,"text":"The City Infrastructure Metaphor: Understanding System Communication","id":"the-city-infrastructure-metaphor-understanding-system-communication"},{"level":3,"text":"Metric Ingestion Flow","id":"metric-ingestion-flow"},{"level":4,"text":"Push-Based Ingestion Process","id":"push-based-ingestion-process"},{"level":4,"text":"Pull-Based Scraping Process","id":"pull-based-scraping-process"},{"level":4,"text":"Common Ingestion Flow Patterns","id":"common-ingestion-flow-patterns"},{"level":3,"text":"Query Processing Flow","id":"query-processing-flow"},{"level":4,"text":"Query Request Initiation","id":"query-request-initiation"},{"level":4,"text":"Query Parsing and AST Generation","id":"query-parsing-and-ast-generation"},{"level":4,"text":"Query Execution Planning","id":"query-execution-planning"},{"level":4,"text":"Storage Interaction and Data Retrieval","id":"storage-interaction-and-data-retrieval"},{"level":4,"text":"Aggregation and Result Processing","id":"aggregation-and-result-processing"},{"level":4,"text":"Query Processing Flow Patterns","id":"query-processing-flow-patterns"},{"level":3,"text":"Alert Evaluation Flow","id":"alert-evaluation-flow"},{"level":4,"text":"Alert Rule Evaluation Cycle","id":"alert-rule-evaluation-cycle"},{"level":4,"text":"Alert State Management","id":"alert-state-management"},{"level":4,"text":"Notification Processing and Delivery","id":"notification-processing-and-delivery"},{"level":4,"text":"Alert Evaluation Flow Patterns","id":"alert-evaluation-flow-patterns"},{"level":4,"text":"Integration with Dashboard and Query Systems","id":"integration-with-dashboard-and-query-systems"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Project Structure","id":"project-structure"},{"level":4,"text":"Core Component Coordination Infrastructure","id":"core-component-coordination-infrastructure"},{"level":4,"text":"Ingestion Flow Infrastructure","id":"ingestion-flow-infrastructure"},{"level":4,"text":"Query Processing Infrastructure","id":"query-processing-infrastructure"},{"level":4,"text":"Alert Evaluation Infrastructure","id":"alert-evaluation-infrastructure"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Common Debugging Patterns","id":"common-debugging-patterns"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"The Defense-in-Depth Metaphor: Building Resilient Systems","id":"the-defense-in-depth-metaphor-building-resilient-systems"},{"level":3,"text":"Error Classification Framework","id":"error-classification-framework"},{"level":3,"text":"Ingestion Error Handling","id":"ingestion-error-handling"},{"level":4,"text":"Mental Model: The Quality Control Checkpoint","id":"mental-model-the-quality-control-checkpoint"},{"level":4,"text":"Invalid Metrics Processing","id":"invalid-metrics-processing"},{"level":4,"text":"Network Failures and Connectivity Issues","id":"network-failures-and-connectivity-issues"},{"level":4,"text":"Backpressure and Flow Control","id":"backpressure-and-flow-control"},{"level":3,"text":"Storage Error Handling","id":"storage-error-handling"},{"level":4,"text":"Mental Model: The Bank Vault System","id":"mental-model-the-bank-vault-system"},{"level":4,"text":"Disk Failures and Hardware Issues","id":"disk-failures-and-hardware-issues"},{"level":4,"text":"Data Corruption Detection and Recovery","id":"data-corruption-detection-and-recovery"},{"level":4,"text":"Storage Resource Management","id":"storage-resource-management"},{"level":3,"text":"Alerting Error Handling","id":"alerting-error-handling"},{"level":4,"text":"Mental Model: The Emergency Communication Network","id":"mental-model-the-emergency-communication-network"},{"level":4,"text":"Notification Delivery Failures","id":"notification-delivery-failures"},{"level":4,"text":"Alert Flapping Prevention","id":"alert-flapping-prevention"},{"level":4,"text":"Silence Management and Maintenance Windows","id":"silence-management-and-maintenance-windows"},{"level":3,"text":"Common Pitfalls in Error Handling","id":"common-pitfalls-in-error-handling"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure for Error Handling","id":"file-structure-for-error-handling"},{"level":4,"text":"Core Error Type Definitions","id":"core-error-type-definitions"},{"level":4,"text":"Health Management Implementation","id":"health-management-implementation"},{"level":4,"text":"Circuit Breaker Implementation","id":"circuit-breaker-implementation"},{"level":4,"text":"Retry Policy Implementation","id":"retry-policy-implementation"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"The Quality Assurance Laboratory: Understanding Testing Approach","id":"the-quality-assurance-laboratory-understanding-testing-approach"},{"level":3,"text":"Unit Testing Strategy","id":"unit-testing-strategy"},{"level":4,"text":"Component Isolation and Mocking Strategy","id":"component-isolation-and-mocking-strategy"},{"level":4,"text":"Core Component Testing Coverage","id":"core-component-testing-coverage"},{"level":4,"text":"Error Condition Testing","id":"error-condition-testing"},{"level":4,"text":"Performance and Resource Usage Testing","id":"performance-and-resource-usage-testing"},{"level":4,"text":"Common Unit Testing Pitfalls","id":"common-unit-testing-pitfalls"},{"level":3,"text":"Integration Testing","id":"integration-testing"},{"level":4,"text":"End-to-End Workflow Validation","id":"end-to-end-workflow-validation"},{"level":4,"text":"Cross-Component Data Flow Testing","id":"cross-component-data-flow-testing"},{"level":4,"text":"Database and File System Integration","id":"database-and-file-system-integration"},{"level":4,"text":"Network and Protocol Integration","id":"network-and-protocol-integration"},{"level":4,"text":"Configuration and Environment Integration","id":"configuration-and-environment-integration"},{"level":4,"text":"Performance Integration Testing","id":"performance-integration-testing"},{"level":4,"text":"Common Integration Testing Pitfalls","id":"common-integration-testing-pitfalls"},{"level":3,"text":"Milestone Validation Checkpoints","id":"milestone-validation-checkpoints"},{"level":4,"text":"Milestone 1: Metrics Collection Validation","id":"milestone-1-metrics-collection-validation"},{"level":4,"text":"Milestone 2: Storage &amp; Querying Validation","id":"milestone-2-storage-amp-querying-validation"},{"level":4,"text":"Milestone 3: Visualization Dashboard Validation","id":"milestone-3-visualization-dashboard-validation"},{"level":4,"text":"Milestone 4: Alerting System Validation","id":"milestone-4-alerting-system-validation"},{"level":4,"text":"Comprehensive System Validation","id":"comprehensive-system-validation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Testing Infrastructure Components","id":"testing-infrastructure-components"},{"level":4,"text":"Milestone Checkpoint Implementation","id":"milestone-checkpoint-implementation"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"The Detective Work Metaphor: Understanding System Debugging","id":"the-detective-work-metaphor-understanding-system-debugging"},{"level":3,"text":"Common Problem Patterns in Metrics Systems","id":"common-problem-patterns-in-metrics-systems"},{"level":3,"text":"Ingestion Issues","id":"ingestion-issues"},{"level":4,"text":"Metrics Not Appearing","id":"metrics-not-appearing"},{"level":4,"text":"Cardinality Explosions","id":"cardinality-explosions"},{"level":4,"text":"Performance Problems During High Volume","id":"performance-problems-during-high-volume"},{"level":3,"text":"Storage and Query Issues","id":"storage-and-query-issues"},{"level":4,"text":"Slow Query Performance","id":"slow-query-performance"},{"level":4,"text":"Missing or Inconsistent Data","id":"missing-or-inconsistent-data"},{"level":4,"text":"Storage Corruption Detection and Recovery","id":"storage-corruption-detection-and-recovery"},{"level":3,"text":"Alerting Issues","id":"alerting-issues"},{"level":4,"text":"Alerts Not Firing","id":"alerts-not-firing"},{"level":4,"text":"Notification Delivery Problems","id":"notification-delivery-problems"},{"level":4,"text":"Alert Flapping and Noise Reduction","id":"alert-flapping-and-noise-reduction"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Language-Specific Debugging Hints","id":"language-specific-debugging-hints"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"The Evolution Principle: Building for Tomorrow&#39;s Challenges","id":"the-evolution-principle-building-for-tomorrow39s-challenges"},{"level":3,"text":"Current Architecture&#39;s Extension Points","id":"current-architecture39s-extension-points"},{"level":2,"text":"Scalability Enhancements","id":"scalability-enhancements"},{"level":3,"text":"Mental Model: From Corner Store to Global Supply Chain","id":"mental-model-from-corner-store-to-global-supply-chain"},{"level":3,"text":"Distributed Storage Architecture","id":"distributed-storage-architecture"},{"level":3,"text":"High Availability and Fault Tolerance","id":"high-availability-and-fault-tolerance"},{"level":3,"text":"Load Balancing and Query Distribution","id":"load-balancing-and-query-distribution"},{"level":3,"text":"Auto-Scaling Infrastructure","id":"auto-scaling-infrastructure"},{"level":2,"text":"Feature Enhancements","id":"feature-enhancements"},{"level":3,"text":"Mental Model: From Basic Calculator to Scientific Workstation","id":"mental-model-from-basic-calculator-to-scientific-workstation"},{"level":3,"text":"Advanced Query Functions and Analytics","id":"advanced-query-functions-and-analytics"},{"level":3,"text":"Machine Learning Integration","id":"machine-learning-integration"},{"level":3,"text":"Enhanced Visualization and Dashboard Features","id":"enhanced-visualization-and-dashboard-features"},{"level":3,"text":"Integration Ecosystem Expansion","id":"integration-ecosystem-expansion"},{"level":3,"text":"Security and Compliance Enhancements","id":"security-and-compliance-enhancements"},{"level":3,"text":"Performance and Scale Optimizations","id":"performance-and-scale-optimizations"},{"level":2,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":3,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":3,"text":"Enhanced Architecture Structure","id":"enhanced-architecture-structure"},{"level":3,"text":"Distributed Storage Infrastructure","id":"distributed-storage-infrastructure"},{"level":3,"text":"Machine Learning Integration Components","id":"machine-learning-integration-components"},{"level":3,"text":"Advanced Visualization Components","id":"advanced-visualization-components"},{"level":3,"text":"Milestone Checkpoints for Extensions","id":"milestone-checkpoints-for-extensions"},{"level":3,"text":"Common Enhancement Pitfalls","id":"common-enhancement-pitfalls"},{"level":3,"text":"Integration Testing Strategy","id":"integration-testing-strategy"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"The Dictionary Metaphor: Building a Shared Language","id":"the-dictionary-metaphor-building-a-shared-language"},{"level":3,"text":"Core Data Model Terms","id":"core-data-model-terms"},{"level":3,"text":"Storage and Query Terms","id":"storage-and-query-terms"},{"level":3,"text":"Query and Analysis Terms","id":"query-and-analysis-terms"},{"level":3,"text":"Operational and Monitoring Terms","id":"operational-and-monitoring-terms"},{"level":3,"text":"Error Handling and Reliability Terms","id":"error-handling-and-reliability-terms"},{"level":3,"text":"Testing and Development Terms","id":"testing-and-development-terms"},{"level":3,"text":"Advanced and Extension Terms","id":"advanced-and-extension-terms"},{"level":3,"text":"Performance and Optimization Terms","id":"performance-and-optimization-terms"},{"level":3,"text":"Acronyms and Abbreviations","id":"acronyms-and-abbreviations"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"}],"title":"Metrics & Alerting Dashboard: Design Document","markdown":"# Metrics & Alerting Dashboard: Design Document\n\n\n## Overview\n\nThis system collects, stores, and visualizes time-series metrics data while providing intelligent alerting capabilities. The key architectural challenge is building a scalable time-series database that can handle high-throughput metric ingestion, efficient storage with compaction, and real-time querying for both dashboards and alert evaluation.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** All milestones (this foundational understanding applies throughout the project)\n\nBuilding a metrics and alerting system is fundamentally about solving the **observability problem**: how do you understand what's happening inside complex software systems that you can't directly see or touch? Modern applications are like vast, interconnected organisms with thousands of moving parts, and without proper instrumentation, diagnosing problems becomes like trying to perform surgery in complete darkness.\n\nThis section establishes why metrics systems are essential, what makes them technically challenging to build correctly, and how existing solutions approach these challenges. Understanding these fundamentals will guide every architectural decision we make throughout this project.\n\n### The Hospital Monitoring Analogy\n\nThink of a modern metrics and alerting system as the **comprehensive monitoring infrastructure** in a large hospital's intensive care unit. This analogy helps build intuition for the core concepts and challenges we'll encounter.\n\nIn an ICU, every patient has multiple sensors continuously measuring vital signs: heart rate, blood pressure, oxygen saturation, temperature, respiratory rate, and dozens of other physiological indicators. These sensors generate **time-series data** — measurements that only make sense when paired with the exact timestamp they were recorded. A heart rate of 85 BPM is meaningless without knowing when it was measured, just like a CPU usage reading of 73% is useless without its timestamp.\n\nThe hospital's monitoring system faces the same fundamental challenges our metrics system must solve:\n\n**High-Volume Data Ingestion**: Across hundreds of patients, thousands of sensors generate measurements every few seconds. The monitoring system must ingest this constant stream of data without dropping measurements or falling behind. Similarly, our metrics system must handle thousands of application instances each reporting dozens of metrics every 10-30 seconds — potentially millions of data points per minute.\n\n**Efficient Storage with Retention Policies**: The hospital can't store every heartbeat measurement forever — they need different retention strategies. Critical measurements from the last 24 hours are kept at full resolution. Data from the past week might be downsampled to one measurement per minute. Older data gets compressed into hourly or daily averages before eventually being archived or deleted. Our metrics system needs identical **compaction and retention** strategies to manage storage costs while preserving the right level of detail for different time ranges.\n\n**Real-Time Visualization**: Nurses and doctors need live dashboards showing current patient status across multiple time scales — the last few minutes for immediate assessment, the last few hours to understand trends, and longer periods to track recovery progress. These dashboards must update automatically as new measurements arrive. Our system's visualization layer serves the same role for application operators and developers.\n\n**Intelligent Alerting**: The most critical function is automated alerting when vital signs indicate danger. But this is surprisingly complex — a temporary spike in heart rate during physical therapy is normal, but the same reading at 3 AM while sleeping could indicate cardiac distress. Alert rules must consider **context, duration, and patterns**, not just simple thresholds. They must also avoid \"alert fatigue\" — if every minor fluctuation triggers an alarm, staff will start ignoring them, potentially missing real emergencies.\n\n**Multi-Dimensional Context**: Each measurement needs rich context to be meaningful. A blood pressure reading must be tagged with the patient ID, measurement location (left arm vs right arm), patient position (sitting vs lying down), and the specific device used. This **labeling and dimensionality** allows medical staff to filter and aggregate data meaningfully. Our metrics system uses the same concept — a CPU usage metric needs labels for the server hostname, application name, data center region, and deployment version.\n\n**Cardinality Management**: The hospital must be careful about creating too many unique combinations of patient + measurement type + device + location. With 500 patients, 20 vital signs, 50 different monitoring devices, and 10 possible measurement locations, you could theoretically have 5 million unique time series. But most combinations never exist in practice, and creating storage allocation for all possibilities would be wasteful and slow. This is the **cardinality explosion** problem our metrics system must solve — preventing users from accidentally creating millions of sparsely-populated time series through poorly designed label schemes.\n\nThe hospital analogy illuminates why building an effective metrics system is challenging: it's not just about storing numbers, but about creating a reliable, efficient, and intelligent monitoring infrastructure that helps operators understand complex systems and respond to problems before they become catastrophic.\n\n### Technical Challenges\n\nBuilding a production-ready metrics system involves solving several interconnected technical challenges that don't have obvious solutions. Each challenge influences the others, creating a web of architectural dependencies that must be carefully balanced.\n\n#### High-Cardinality Data Management\n\n**Cardinality** refers to the number of unique time series in your system, determined by every unique combination of metric name and label values. This becomes a scalability nightmare faster than most developers anticipate.\n\nConsider a simple web application that tracks HTTP request duration with labels for `method`, `endpoint`, `status_code`, `datacenter`, and `version`. With just 5 HTTP methods, 20 endpoints, 10 possible status codes, 3 datacenters, and 5 active versions, you have 5 × 20 × 10 × 3 × 5 = 15,000 unique time series for a single metric. Add user-specific labels like `user_id` or `session_id`, and you suddenly have millions of time series, most containing only a few data points.\n\nHigh cardinality creates cascading problems:\n\n- **Memory exhaustion**: Each time series requires index entries and metadata structures in memory. With millions of series, even lightweight metadata can consume gigabytes of RAM.\n- **Query performance degradation**: Finding relevant time series requires scanning larger indexes. A query that should return results in milliseconds starts taking seconds.\n- **Storage inefficiency**: Most high-cardinality series contain sparse data, leading to poor compression ratios and wasted disk space.\n- **Compaction overhead**: Background processes that merge and downsample data must handle exponentially more files and indexes.\n\nThe challenge is designing validation and limiting mechanisms that prevent cardinality explosions without restricting legitimate use cases. Users need enough dimensional flexibility to slice and dice their data meaningfully, but the system must protect itself from abuse or misconfiguration.\n\n> **Design Insight**: The cardinality problem is fundamentally about the tension between expressiveness and scalability. Every additional label dimension multiplies potential cardinality, but removing dimensions reduces the system's analytical power.\n\n#### Storage Efficiency and Compression\n\nTime-series data has unique storage characteristics that make traditional database approaches inefficient. The data is **immutable** (historical measurements never change), **temporally ordered** (queries almost always involve time ranges), and **highly compressible** (consecutive values often have small deltas).\n\nHowever, the data also arrives out of order — different application instances have clock skew, network delays vary, and some metrics are batch-uploaded from offline processing. The storage engine must efficiently handle both real-time ingestion and historical backfill while maintaining query performance.\n\nCompression becomes critical at scale. Raw time-series data might consume terabytes, but with proper compression (delta encoding, variable-length integers, and block compression), storage requirements can shrink by 10-20x. However, compression complicates queries — you can't seek to arbitrary time points within compressed blocks without partial decompression.\n\nThe storage engine must also implement **compaction** — the process of merging small files into larger ones, downsampling high-resolution data to lower resolutions, and eventually deleting old data according to retention policies. Compaction runs continuously in the background but must not interfere with real-time ingestion or queries.\n\n#### Query Performance at Scale\n\nUsers expect sub-second query responses even when requesting data across long time ranges or high-cardinality dimensions. A dashboard loading 20 charts simultaneously might generate 20 concurrent queries, each potentially scanning millions of data points across hundreds of time series.\n\nQuery performance depends on several factors:\n\n**Index Efficiency**: Finding the right time series for a query requires efficient indexing by metric name and labels. Traditional B-tree indexes work poorly for high-cardinality label combinations. The system needs specialized indexing structures optimized for multi-dimensional filtering.\n\n**Data Layout**: Physical storage layout dramatically affects query performance. Storing all metrics together requires extensive filtering. Storing each time series separately creates too many small files. The optimal layout depends on query patterns, which vary by application.\n\n**Aggregation Complexity**: Most queries involve aggregation — summing values across multiple time series, computing percentiles, or calculating rates. These operations must be pushed down to the storage layer to avoid transferring massive amounts of raw data over the network.\n\n**Caching Strategy**: Repeated queries for the same data should be cached, but time-series data is continuously growing. The caching layer must invalidate cached results appropriately while maximizing hit rates for relatively static historical data.\n\n#### Alert Reliability and Accuracy\n\nAlerting is perhaps the most critical component because alert failures can mean missing production outages. However, reliable alerting involves subtle challenges that are easy to get wrong.\n\n**Evaluation Timing**: Alert rules must be evaluated at precise intervals, but evaluation itself takes time. If evaluating all alert rules takes 45 seconds, but the evaluation interval is 30 seconds, the system falls behind and might miss brief anomalies. The evaluation scheduler must handle timing constraints gracefully.\n\n**State Management**: Alerts transition through states (inactive → pending → firing → resolved), and these transitions must be tracked reliably across system restarts. Alert state must be persisted durably, but state updates are frequent and must not become a performance bottleneck.\n\n**Flapping Prevention**: Metrics that oscillate around alert thresholds can cause alerts to rapidly transition between firing and resolved states, generating notification spam. The system needs hysteresis mechanisms and minimum duration requirements to prevent flapping.\n\n**Notification Delivery**: Once an alert fires, delivering notifications reliably is surprisingly complex. External services (email, Slack, PagerDuty) can be temporarily unavailable, rate-limited, or return ambiguous error responses. The system must implement retry logic, exponential backoff, and failure detection without losing notifications or creating duplicates.\n\n### Existing Solutions Comparison\n\nUnderstanding how established metrics systems approach these challenges provides valuable context for our architectural decisions. Three representative solutions illustrate different trade-offs and design philosophies.\n\n#### Prometheus: Pull-Based Open Source\n\nPrometheus pioneered the modern metrics monitoring approach and remains the most influential open-source solution. Its design decisions reflect a philosophy prioritizing simplicity, reliability, and operational transparency.\n\n**Architecture Philosophy**: Prometheus uses a **pull-based** model where the central server actively scrapes metrics from application endpoints. This inverts the traditional push model and provides several advantages: the monitoring system controls scraping frequency, can detect when applications become unreachable, and avoids overwhelming the central server with concurrent metric submissions.\n\n**Data Model**: Prometheus uses a dimensional data model where each time series is identified by a metric name plus a set of key-value labels. This model is both powerful and dangerous — it enables flexible querying but makes cardinality explosions easy to create accidentally.\n\n**Storage Engine**: The Prometheus storage engine (TSDB) uses a custom file format optimized for time-series data. Data is organized into time-based blocks, each containing compressed chunks of time series data plus indexes for efficient querying. The block-based approach enables efficient compaction and retention policy enforcement.\n\n**Query Language**: PromQL (Prometheus Query Language) is a functional query language designed specifically for time-series data. It supports complex aggregations, mathematical operations, and time-based functions. However, PromQL has a steep learning curve and can produce queries that accidentally consume excessive resources.\n\n| Aspect | Advantages | Disadvantages |\n|--------|------------|---------------|\n| **Pull Model** | Service discovery integration, failure detection, backpressure control | Requires network connectivity to all targets, complex NAT/firewall traversal |\n| **Local Storage** | Simple deployment, no external dependencies, predictable performance | Limited scalability, no built-in high availability, single point of failure |\n| **PromQL** | Powerful aggregations, mathematical flexibility, time-series specific operations | Complex syntax, easy to write inefficient queries, limited statistical functions |\n| **Alertmanager** | Sophisticated routing and grouping, silence management, template system | Complex configuration, separate component to manage, limited notification channels |\n\n**Scalability Limitations**: Prometheus is designed for single-node deployment with federation for multi-datacenter scenarios. A single Prometheus server can handle millions of time series, but scaling beyond that requires complex federation topologies or third-party solutions like Thanos or Cortex.\n\n#### InfluxDB: Purpose-Built Time-Series Database\n\nInfluxDB represents a different approach: building a complete database system optimized specifically for time-series workloads. This allows more sophisticated features but introduces additional complexity.\n\n**Architecture Philosophy**: InfluxDB uses a **push-based** model where applications send metrics directly to the database. This approach scales more naturally for high-throughput scenarios but requires careful backpressure and rate limiting design.\n\n**Data Model**: InfluxDB's data model includes measurements (metric names), tags (indexed labels), fields (actual values), and timestamps. The distinction between tags and fields is important — tags are indexed and support efficient filtering, while fields store the actual metric values and support mathematical operations.\n\n**Storage Engine**: InfluxDB uses the TSM (Time-Structured Merge tree) storage engine, which combines concepts from LSM-trees with time-series-specific optimizations. Data is initially written to a WAL (Write-Ahead Log), then organized into TSM files with sophisticated compression and indexing.\n\n**Query Language**: InfluxQL is SQL-like, making it more familiar to developers than PromQL. However, SQL wasn't designed for time-series operations, so some queries feel awkward or require complex syntax.\n\n| Aspect | Advantages | Disadvantages |\n|--------|------------|---------------|\n| **Push Model** | Better for high-throughput scenarios, works behind firewalls/NAT, simpler network topology | Requires backpressure handling, harder to detect application failures, potential overload scenarios |\n| **SQL-like Query Language** | Familiar syntax for most developers, rich statistical functions, flexible data manipulation | Some time-series operations feel unnatural, complex joins for multi-metric queries |\n| **Clustered Architecture** | Built-in horizontal scaling, high availability, automatic data distribution | Complex cluster management, eventual consistency trade-offs, higher operational overhead |\n| **Schema Flexibility** | Dynamic schema creation, multiple field types per measurement, flexible tag structures | Easy to create inefficient schemas, tag/field distinction confusion, index bloat potential |\n\n**Commercial Focus**: InfluxData's business model centers on InfluxDB Enterprise and InfluxDB Cloud, with the open-source version having limitations in clustering and advanced features. This affects the development priority of community-requested features.\n\n#### DataDog: Cloud-Native SaaS\n\nDataDog represents the fully managed SaaS approach, where the metrics system complexity is hidden behind a service API. This illustrates how cloud providers solve scalability challenges that are difficult for individual organizations.\n\n**Architecture Philosophy**: DataDog operates a **multi-tenant** metrics platform serving thousands of customers from shared infrastructure. This requires sophisticated resource isolation, security controls, and cost allocation mechanisms that single-tenant systems don't need.\n\n**Data Ingestion**: DataDog supports multiple ingestion methods (StatsD, HTTP API, agent-based collection) and handles the complexity of routing, validation, and processing at massive scale. They can absorb traffic spikes and provide backpressure without customers needing to design these systems.\n\n**Storage and Retention**: DataDog automatically manages retention policies, downsampling, and storage optimization. Customers specify retention requirements through pricing tiers rather than configuring technical parameters.\n\n**Alerting and Dashboards**: The platform provides sophisticated alerting with machine learning-based anomaly detection, automatic correlation analysis, and integrated incident management workflows.\n\n| Aspect | Advantages | Disadvantages |\n|--------|------------|---------------|\n| **Managed Infrastructure** | No operational overhead, automatic scaling, built-in high availability | Vendor lock-in, limited customization, ongoing subscription costs |\n| **Advanced Analytics** | Machine learning features, anomaly detection, correlation analysis | Less control over algorithms, potential false positives, black-box analysis |\n| **Integrated Ecosystem** | Unified logging/metrics/tracing, pre-built integrations, collaborative features | Expensive for high-volume scenarios, feature coupling, data export limitations |\n| **Enterprise Features** | SSO integration, audit trails, role-based access, compliance certifications | Overkill for simple use cases, complex pricing models, feature overwhelming |\n\n**Cost Considerations**: SaaS metrics platforms typically charge based on the number of custom metrics, data retention periods, and advanced feature usage. For high-cardinality applications, monthly costs can quickly reach thousands of dollars, making self-hosted solutions economically attractive.\n\n> **Decision: Architecture Philosophy**\n> - **Context**: We must choose between pull-based (Prometheus-style), push-based (InfluxDB-style), or hybrid approaches for metric collection\n> - **Options Considered**: \n>   1. Pure pull-based with service discovery\n>   2. Pure push-based with client libraries\n>   3. Hybrid supporting both models\n> - **Decision**: Hybrid approach supporting both push and pull models\n> - **Rationale**: Different applications have different constraints — containerized services work well with pull models, while batch jobs and edge devices need push capabilities. Supporting both provides maximum flexibility for adoption.\n> - **Consequences**: Increases implementation complexity but provides better real-world usability. Requires designing consistent APIs for both ingestion methods.\n\nThe comparison reveals that each approach optimizes for different priorities: operational simplicity (Prometheus), database sophistication (InfluxDB), or comprehensive managed services (DataDog). Our system will need to make similar trade-offs based on the specific requirements and constraints we're targeting.\n\nUnderstanding these existing solutions helps us identify proven patterns worth adopting and pitfalls worth avoiding. However, building our own system allows us to make different trade-offs and potentially improve on areas where existing solutions have limitations.\n\n### Implementation Guidance\n\nThe context and challenges described above inform several critical technology choices for our implementation. Understanding the problem space deeply allows us to select appropriate tools and avoid common architectural mistakes.\n\n#### Technology Stack Recommendations\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|-----------------|-----------|\n| **HTTP Server** | `net/http` with `gorilla/mux` | `gin-gonic/gin` or `echo` | Standard library provides everything needed; frameworks add convenience but dependency overhead |\n| **Storage Format** | JSON files with `encoding/json` | Protocol Buffers with `google.golang.org/protobuf` | JSON is human-readable for debugging; protobuf provides better performance and schema evolution |\n| **Time Series Storage** | File-based with `os` package | Embedded `badger` or `bbolt` database | Files are simple and debuggable; embedded DBs provide better concurrency and consistency |\n| **Background Processing** | `time.Ticker` with goroutines | `robfig/cron` for scheduling | Simple ticker adequate for basic needs; cron expressions provide more flexible scheduling |\n| **Configuration** | `flag` package | `spf13/viper` with YAML/TOML | Command-line flags are simple; structured config files scale better for complex systems |\n\n#### Project Structure Foundation\n\nOrganize the codebase to support the four major milestones while maintaining clear separation of concerns:\n\n```\nmetrics-system/\n├── cmd/\n│   ├── server/main.go              ← HTTP server entry point\n│   └── query/main.go               ← Query CLI tool for testing\n├── internal/\n│   ├── ingestion/                  ← Milestone 1: Metrics Collection\n│   │   ├── handler.go              ← HTTP handlers for push API\n│   │   ├── scraper.go              ← Pull-based metric scraping\n│   │   ├── validator.go            ← Label validation and cardinality control\n│   │   └── processor.go            ← Metric type processing pipeline\n│   ├── storage/                    ← Milestone 2: Storage & Querying\n│   │   ├── engine.go               ← Time-series storage engine\n│   │   ├── index.go                ← Label indexing and lookup\n│   │   ├── compaction.go           ← Background compaction process\n│   │   └── retention.go            ← Data retention policy enforcement\n│   ├── query/                      ← Milestone 2: Query processing\n│   │   ├── parser.go               ← Query language parsing\n│   │   ├── executor.go             ← Query execution engine\n│   │   └── aggregation.go          ← Aggregation function library\n│   ├── dashboard/                  ← Milestone 3: Visualization\n│   │   ├── server.go               ← Web dashboard HTTP handlers\n│   │   ├── websocket.go            ← Real-time data updates\n│   │   └── renderer.go             ← Chart generation logic\n│   └── alerting/                   ← Milestone 4: Alerting System\n│       ├── evaluator.go            ← Alert rule evaluation engine\n│       ├── state.go                ← Alert state management\n│       └── notifier.go             ← Notification delivery system\n├── pkg/                            ← Public APIs and shared types\n│   ├── types/                      ← Core data structures\n│   │   ├── metric.go               ← Metric, Sample, Label definitions\n│   │   ├── query.go                ← Query request/response types\n│   │   └── alert.go                ← Alert rule and state types\n│   └── client/                     ← Client library for metric submission\n│       └── client.go               ← HTTP client for push API\n├── web/                            ← Dashboard static assets\n│   ├── static/                     ← CSS, JavaScript, images\n│   └── templates/                  ← HTML templates\n└── testdata/                       ← Sample metrics and test fixtures\n    ├── metrics/                    ← Example metric data files\n    └── configs/                    ← Sample configuration files\n```\n\n#### Infrastructure Starter Code\n\n**HTTP Server Foundation** (`internal/server/server.go`):\n```go\npackage server\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"time\"\n    \n    \"github.com/gorilla/mux\"\n)\n\n// Server wraps the HTTP server with graceful shutdown support\ntype Server struct {\n    httpServer *http.Server\n    router     *mux.Router\n}\n\n// NewServer creates a new HTTP server with default middleware\nfunc NewServer(addr string) *Server {\n    router := mux.NewRouter()\n    \n    // Add basic middleware\n    router.Use(loggingMiddleware)\n    router.Use(corsMiddleware)\n    \n    return &Server{\n        httpServer: &http.Server{\n            Addr:         addr,\n            Handler:      router,\n            ReadTimeout:  30 * time.Second,\n            WriteTimeout: 30 * time.Second,\n            IdleTimeout:  120 * time.Second,\n        },\n        router: router,\n    }\n}\n\n// RegisterRoutes adds all API endpoints to the router\nfunc (s *Server) RegisterRoutes(ingestionHandler, queryHandler, dashboardHandler http.Handler) {\n    // Metrics ingestion endpoints\n    s.router.PathPrefix(\"/api/v1/metrics\").Handler(ingestionHandler)\n    \n    // Query API endpoints  \n    s.router.PathPrefix(\"/api/v1/query\").Handler(queryHandler)\n    \n    // Dashboard and static assets\n    s.router.PathPrefix(\"/\").Handler(dashboardHandler)\n}\n\n// Start begins serving HTTP requests\nfunc (s *Server) Start() error {\n    log.Printf(\"Starting HTTP server on %s\", s.httpServer.Addr)\n    return s.httpServer.ListenAndServe()\n}\n\n// Shutdown gracefully stops the server\nfunc (s *Server) Shutdown(ctx context.Context) error {\n    log.Println(\"Shutting down HTTP server...\")\n    return s.httpServer.Shutdown(ctx)\n}\n\nfunc loggingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n        next.ServeHTTP(w, r)\n        log.Printf(\"%s %s %v\", r.Method, r.URL.Path, time.Since(start))\n    })\n}\n\nfunc corsMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        w.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\n        w.Header().Set(\"Access-Control-Allow-Methods\", \"GET, POST, PUT, DELETE, OPTIONS\")\n        w.Header().Set(\"Access-Control-Allow-Headers\", \"Content-Type\")\n        \n        if r.Method == \"OPTIONS\" {\n            w.WriteHeader(http.StatusOK)\n            return\n        }\n        \n        next.ServeHTTP(w, r)\n    })\n}\n```\n\n**Core Data Types** (`pkg/types/metric.go`):\n```go\npackage types\n\nimport (\n    \"fmt\"\n    \"sort\"\n    \"strings\"\n    \"time\"\n)\n\n// MetricType represents the type of metric being stored\ntype MetricType int\n\nconst (\n    MetricTypeCounter MetricType = iota\n    MetricTypeGauge\n    MetricTypeHistogram\n)\n\n// String returns the string representation of the metric type\nfunc (mt MetricType) String() string {\n    switch mt {\n    case MetricTypeCounter:\n        return \"counter\"\n    case MetricTypeGauge:\n        return \"gauge\" \n    case MetricTypeHistogram:\n        return \"histogram\"\n    default:\n        return \"unknown\"\n    }\n}\n\n// Labels represents a set of key-value pairs attached to a metric\ntype Labels map[string]string\n\n// String returns a canonical string representation of labels\nfunc (l Labels) String() string {\n    if len(l) == 0 {\n        return \"{}\"\n    }\n    \n    // Sort keys for consistent output\n    keys := make([]string, 0, len(l))\n    for k := range l {\n        keys = append(keys, k)\n    }\n    sort.Strings(keys)\n    \n    parts := make([]string, 0, len(l))\n    for _, k := range keys {\n        parts = append(parts, fmt.Sprintf(`%s=\"%s\"`, k, l[k]))\n    }\n    \n    return \"{\" + strings.Join(parts, \",\") + \"}\"\n}\n\n// Copy creates a deep copy of the labels\nfunc (l Labels) Copy() Labels {\n    result := make(Labels, len(l))\n    for k, v := range l {\n        result[k] = v\n    }\n    return result\n}\n\n// Sample represents a single data point in a time series\ntype Sample struct {\n    Value     float64   `json:\"value\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\n// Metric represents a complete metric with its metadata and samples\ntype Metric struct {\n    Name    string     `json:\"name\"`\n    Type    MetricType `json:\"type\"`\n    Labels  Labels     `json:\"labels\"`\n    Samples []Sample   `json:\"samples\"`\n    Help    string     `json:\"help,omitempty\"`\n}\n\n// SeriesID generates a unique identifier for this metric's time series\nfunc (m *Metric) SeriesID() string {\n    return m.Name + m.Labels.String()\n}\n\n// Validate checks if the metric has valid values\nfunc (m *Metric) Validate() error {\n    if m.Name == \"\" {\n        return fmt.Errorf(\"metric name cannot be empty\")\n    }\n    \n    if len(m.Samples) == 0 {\n        return fmt.Errorf(\"metric must have at least one sample\")\n    }\n    \n    // Validate label keys and values\n    for key, value := range m.Labels {\n        if key == \"\" {\n            return fmt.Errorf(\"label key cannot be empty\")\n        }\n        if strings.HasPrefix(key, \"__\") {\n            return fmt.Errorf(\"label key %s: keys starting with __ are reserved\", key)\n        }\n        if len(value) > 1024 {\n            return fmt.Errorf(\"label value for key %s exceeds maximum length\", key)\n        }\n    }\n    \n    return nil\n}\n```\n\n#### Milestone Checkpoints\n\n**After Milestone 1 (Metrics Collection):**\nRun the following to verify ingestion is working:\n```bash\n# Start the server\ngo run cmd/server/main.go\n\n# Send a test metric\ncurl -X POST http://localhost:8080/api/v1/metrics \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"http_requests_total\",\n    \"type\": \"counter\", \n    \"labels\": {\"method\": \"GET\", \"endpoint\": \"/api\"},\n    \"samples\": [{\"value\": 42, \"timestamp\": \"2023-01-01T12:00:00Z\"}]\n  }'\n\n# Should receive 200 OK response\n# Check server logs for ingestion confirmation\n```\n\n**After Milestone 2 (Storage & Querying):**\n```bash\n# Query the stored metric\ncurl \"http://localhost:8080/api/v1/query?metric=http_requests_total&start=2023-01-01T11:00:00Z&end=2023-01-01T13:00:00Z\"\n\n# Should return JSON with the stored sample\n# Verify data persists across server restarts\n```\n\n#### Common Debugging Issues\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| **HTTP 500 on metric ingestion** | JSON parsing error or validation failure | Check request body format and server logs | Validate JSON structure and required fields |\n| **Metrics disappear after restart** | In-memory storage without persistence | Check if data is being written to disk | Implement file-based storage in storage engine |\n| **Query returns empty results** | Label matching issues or wrong time range | Verify metric name and labels match exactly | Add debug logging to query execution |\n| **High memory usage** | Unbounded metric storage or label cardinality explosion | Monitor number of unique time series | Implement retention policies and label validation |\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** All milestones (this section establishes the foundational scope and boundaries for the entire project)\n\n### The Goldilocks Principle for System Design\n\nBuilding a metrics and alerting system is like designing a security system for a large office building. You need to balance comprehensive coverage with practical constraints. Install too few sensors and you miss critical events. Install too many and you're overwhelmed with false alarms, maintenance costs, and complexity. The key is finding the \"just right\" balance that meets your actual needs without overengineering.\n\nThis section establishes clear boundaries for what our system will and will not do. These boundaries are crucial because metrics systems can easily become sprawling, complex beasts that try to solve every observability problem under the sun. By explicitly defining our scope, we prevent feature creep and ensure we build a focused, reliable system that excels at its core mission.\n\nThe goals we set here directly drive every architectural decision throughout the project. When we face trade-offs between simplicity and features, performance and functionality, or immediate needs versus future flexibility, we'll return to these goals as our north star.\n\n### Functional Goals\n\nThese are the core capabilities our system must provide to be considered successful. Each goal maps directly to one or more project milestones and represents functionality that users will interact with directly.\n\n#### Metric Collection and Storage\n\nOur system must reliably collect, validate, and store three fundamental metric types that form the foundation of observability. **Counters** track cumulative values that only increase over time, such as total HTTP requests or bytes processed. **Gauges** capture point-in-time values that can fluctuate up or down, like current memory usage or active connections. **Histograms** bucket observations into configurable ranges to analyze distributions, enabling percentile calculations for latency or request size analysis.\n\nThe system must support both push-based and pull-based metric collection models. In the push model, applications actively send metrics to our collection endpoints, providing immediate data delivery and working well with dynamic environments like containers. In the pull model, our system scrapes metrics from application endpoints, offering better control over collection timing and reducing load on applications.\n\n| Metric Type | Purpose | Example Use Cases | Key Characteristics |\n|-------------|---------|-------------------|-------------------|\n| Counter | Track cumulative events | HTTP requests, errors, bytes sent | Monotonically increasing, reset on restart |\n| Gauge | Capture current state | Memory usage, queue depth, temperature | Can increase or decrease, point-in-time value |\n| Histogram | Analyze distributions | Request latency, response size | Buckets observations, tracks count and sum |\n\n> **Decision: Support Multi-Dimensional Metrics**\n> - **Context**: Modern applications need to slice metrics by various dimensions (service, region, user type)\n> - **Options Considered**: Flat metrics with no labels, limited predefined dimensions, flexible label system\n> - **Decision**: Implement flexible label system similar to Prometheus\n> - **Rationale**: Labels enable powerful querying and aggregation while maintaining storage efficiency through shared time series\n> - **Consequences**: Adds complexity to storage and indexing but provides essential functionality for real-world use\n\n#### Time-Series Data Management\n\nThe system must efficiently store millions of time-series data points with high write throughput while maintaining fast query performance. Time-series data has unique characteristics: it's append-only, timestamp-ordered, and often exhibits predictable patterns that enable compression.\n\nData retention policies automatically manage the lifecycle of stored metrics. Recent data (last 24 hours) remains at full resolution for detailed analysis. Older data gets progressively downsampled to reduce storage costs while preserving long-term trends. Data older than the configured retention period gets automatically deleted.\n\n| Data Age | Resolution | Purpose | Storage Impact |\n|----------|------------|---------|---------------|\n| 0-24 hours | Original (15s intervals) | Real-time monitoring | High storage, fast queries |\n| 1-7 days | 1 minute averages | Recent troubleshooting | Medium storage |\n| 1-4 weeks | 5 minute averages | Trend analysis | Low storage |\n| > 1 month | Deleted or archived | Compliance only | Minimal storage |\n\nCompaction processes run automatically in the background, merging small data files into larger, more efficient structures. This reduces the number of files that queries must read and improves compression ratios by processing larger data blocks together.\n\n#### Query and Visualization Capabilities\n\nThe system must provide a query language that allows users to select metrics by name and labels, specify time ranges, and apply aggregation functions. Queries should feel natural to operations teams familiar with tools like Prometheus or SQL.\n\nBasic aggregation functions include sum, average, maximum, minimum, and rate calculations over time windows. Rate functions are particularly important for converting counter metrics into meaningful rates (requests per second, error rate percentages).\n\n| Function | Purpose | Input | Output |\n|----------|---------|-------|--------|\n| `sum()` | Total across series | Multiple time series | Single aggregated series |\n| `avg()` | Average across series | Multiple time series | Single averaged series |\n| `rate()` | Change rate over time | Counter series | Rate per second |\n| `quantile()` | Percentile calculation | Histogram series | Percentile value series |\n\nThe web dashboard provides visual representations of metric data through line charts, bar charts, and heatmaps. Dashboard configurations persist as JSON documents, enabling sharing and version control. Real-time updates refresh charts automatically without requiring manual page reloads.\n\n#### Alerting and Notification System\n\nAlert rules define conditions that trigger notifications when metric values cross thresholds or exhibit anomalous behavior. Each rule specifies a metric query, comparison operator, threshold value, and evaluation interval.\n\nAlert state management handles transitions between inactive, pending, firing, and resolved states. Alerts enter pending state when conditions are first met, then escalate to firing after persisting for the configured duration. This prevents noisy alerts from temporary spikes.\n\n| Alert State | Trigger Condition | Actions Taken | Next States |\n|-------------|------------------|---------------|-------------|\n| Inactive | Normal metric values | None | Pending |\n| Pending | Threshold crossed | Start timer | Inactive, Firing |\n| Firing | Duration exceeded | Send notifications | Inactive, Resolved |\n| Resolved | Condition cleared | Send resolution notice | Inactive |\n\nNotification channels deliver alert messages through multiple integrations: email, Slack, PagerDuty, and generic webhooks. Message templates allow customization of alert content with metric values, labels, and contextual information.\n\n### Non-Functional Goals\n\nThese requirements define how well the system performs its functional capabilities. They establish the operational characteristics needed for production deployment and long-term success.\n\n#### Performance Requirements\n\nThe metrics ingestion system must handle sustained write loads of 10,000 samples per second on modest hardware (4 CPU cores, 8GB RAM). This throughput supports monitoring for hundreds of applications or services in a medium-sized organization.\n\nQuery response times should remain under 5 seconds for dashboard refreshes and under 30 seconds for complex analytical queries. These targets ensure dashboards feel responsive while accommodating the inherent costs of aggregating large time-series datasets.\n\n| Workload Type | Target Performance | Resource Constraints |\n|---------------|-------------------|---------------------|\n| Metric ingestion | 10,000 samples/sec | 4 CPU cores, 8GB RAM |\n| Dashboard queries | < 5 seconds | Standard SSD storage |\n| Analytical queries | < 30 seconds | Network bandwidth < 1Gbps |\n| Alert evaluation | < 60 seconds | Configurable intervals |\n\nStorage efficiency targets aim for 80% compression ratios compared to raw metric data. Time-series data compresses well due to temporal locality and predictable patterns, making this target achievable with proper encoding techniques.\n\n#### Reliability Requirements\n\nThe system must maintain 99.9% uptime, allowing for planned maintenance windows and occasional failures. This translates to less than 9 hours of downtime per year, appropriate for internal monitoring systems that aren't customer-facing.\n\nData durability guarantees ensure that metric data, once acknowledged as received, survives single-node failures. This requires synchronous persistence to disk before sending acknowledgments, with optional replication for higher availability environments.\n\n| Component | Availability Target | Recovery Time | Data Loss Tolerance |\n|-----------|-------------------|---------------|-------------------|\n| Ingestion API | 99.9% | < 5 minutes | Zero (acknowledged data) |\n| Query API | 99.5% | < 10 minutes | Read-only degradation acceptable |\n| Dashboard | 99.5% | < 10 minutes | Cached data acceptable |\n| Alerting | 99.9% | < 2 minutes | Critical for incident response |\n\nAlert delivery reliability requires redundant notification channels and retry mechanisms. Critical alerts should reach on-call personnel even if primary notification channels fail.\n\n#### Scalability Requirements\n\nThe system must scale vertically on single nodes before requiring distributed deployment. This simplifies operations and reduces complexity for organizations that don't need web-scale infrastructure.\n\nStorage scalability targets support 1TB of metric data on standard hardware, sufficient for monitoring medium-sized infrastructures. Beyond this threshold, organizations typically invest in dedicated time-series databases or cloud services.\n\n| Resource | Scaling Target | Growth Strategy |\n|----------|----------------|-----------------|\n| CPU utilization | < 70% average | Vertical scaling first |\n| Memory usage | < 80% available | Efficient data structures |\n| Storage capacity | 1TB total | Automatic compaction/retention |\n| Network bandwidth | < 50% link capacity | Compression and batching |\n\nHorizontal scaling preparation means avoiding architectural choices that prevent future distribution. While the initial implementation runs on single nodes, the design should accommodate future sharding or federation.\n\n#### Operational Requirements\n\nThe system must provide comprehensive observability into its own operation through self-monitoring metrics, structured logging, and health check endpoints. Operations teams need visibility into ingestion rates, storage usage, query performance, and alert evaluation timing.\n\nConfiguration management uses file-based configuration with hot-reload capabilities for non-disruptive updates. This enables GitOps workflows where configuration changes flow through version control and automated deployment pipelines.\n\n| Operational Aspect | Requirement | Implementation |\n|-------------------|-------------|----------------|\n| Self-monitoring | All components instrumented | Internal metrics exposure |\n| Configuration | Hot-reload support | File watching mechanisms |\n| Logging | Structured JSON logs | Configurable log levels |\n| Health checks | HTTP endpoints | Dependency verification |\n\n### Explicit Non-Goals\n\nThese are capabilities we explicitly choose not to build, either due to complexity constraints, timeline limitations, or because better solutions exist elsewhere. Defining non-goals is as important as defining goals because it prevents scope creep and helps resist the temptation to build unnecessary features.\n\n#### Advanced Analytics and Machine Learning\n\nOur system will not include anomaly detection algorithms, predictive analytics, or machine learning-based alerting. These capabilities require significant expertise in data science, large amounts of historical data for training, and ongoing model maintenance.\n\n> The complexity of implementing reliable machine learning features far exceeds the value for most monitoring use cases. Traditional threshold-based alerting handles 90% of operational needs effectively.\n\nOrganizations needing advanced analytics should integrate dedicated ML platforms or cloud services that specialize in these capabilities. Our system can export data to these platforms through standard APIs.\n\n#### Multi-Tenancy and Access Control\n\nThe system will not implement user authentication, role-based access control, or tenant isolation. These features add significant complexity to every component and require expertise in security architecture.\n\nFor production deployments requiring access control, the system should be deployed behind reverse proxies or API gateways that handle authentication and authorization. This separation of concerns keeps our system focused on metrics while leveraging specialized security tools.\n\n#### Distributed Deployment and High Availability\n\nThe initial implementation will not support clustering, replication, or distributed deployment. Building reliable distributed systems requires expertise in consensus algorithms, failure detection, and data consistency that would dominate the project scope.\n\nOrganizations requiring high availability should deploy multiple independent instances or use cloud-managed time-series databases for critical production workloads. Our system focuses on being an excellent single-node solution.\n\n#### Log Aggregation and Tracing Integration\n\nWhile metrics are one pillar of observability, we will not build log aggregation or distributed tracing capabilities. These domains have different data models, storage requirements, and query patterns that warrant dedicated systems.\n\nThe system should provide integration points (webhook notifications, metric export APIs) that allow correlation with external logging and tracing systems, but will not attempt to replace specialized tools like ELK stack or Jaeger.\n\n#### Advanced Visualization and Dashboarding\n\nWe will not build advanced chart types (3D visualizations, geographic maps, complex statistical plots) or dashboard features like annotation, collaboration, or advanced templating. These features require significant frontend development effort and compete with established visualization platforms.\n\nThe dashboard will focus on core time-series visualizations (line charts, bar charts, simple heatmaps) that handle 80% of monitoring use cases. Users needing advanced visualizations should export data to specialized tools like Grafana or custom applications.\n\n| Non-Goal Category | Rationale | Alternative Approach |\n|-------------------|-----------|---------------------|\n| Machine Learning | Complexity exceeds project scope | Integration with ML platforms |\n| Multi-tenancy | Security complexity too high | Deploy behind auth proxy |\n| Distributed Systems | Requires distributed systems expertise | Multiple independent instances |\n| Log Aggregation | Different data model and requirements | Use dedicated logging platforms |\n| Advanced Visualization | Frontend complexity | Export to Grafana or similar |\n\n#### Performance Beyond Single-Node Limits\n\nWe will not optimize for ingestion rates exceeding 100,000 samples per second or storage requirements beyond 10TB. These requirements indicate need for specialized time-series databases or distributed systems.\n\nThe design should avoid architectural choices that prevent future scaling, but will not implement complex optimization techniques like custom storage engines, advanced indexing structures, or distributed query processing.\n\n#### Complex Query Languages and Analytics\n\nThe query language will not support joins between different metric series, complex mathematical functions, or SQL-like analytical queries. These features require query optimization engines and add significant implementation complexity.\n\nUsers needing complex analytics should export metric data to dedicated analytics platforms or data warehouses that provide full SQL support and analytical processing capabilities.\n\n> **Key Insight**: By explicitly limiting scope, we can build a robust, reliable system that excels at core metrics functionality rather than a complex system that attempts everything but does nothing well.\n\n### Success Criteria and Acceptance\n\nSuccess will be measured by the system's ability to handle realistic monitoring workloads for development and staging environments. A successful implementation should enable a small operations team to monitor a few dozen applications and services effectively.\n\nThe system succeeds if it can replace simple monitoring solutions like basic Prometheus setups while providing better usability and integration capabilities. It should feel natural to operations engineers familiar with modern monitoring tools.\n\n| Success Metric | Target Value | Measurement Method |\n|----------------|--------------|-------------------|\n| Metric ingestion rate | 5,000+ samples/sec sustained | Load testing with realistic data |\n| Query response time | 95th percentile < 10 seconds | Dashboard usage simulation |\n| Storage efficiency | > 70% compression ratio | Compare raw vs compressed sizes |\n| Alert reliability | 99%+ delivery success | Notification tracking logs |\n\nThe ultimate test is whether the system provides value in real monitoring scenarios: detecting actual incidents, enabling root cause analysis through dashboard exploration, and reducing time to resolution for operational issues.\n\n### Implementation Guidance\n\n#### Technology Foundation\n\nThe system builds on proven technologies that balance simplicity with capability. These choices prioritize reliability and maintainability over cutting-edge features.\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | `net/http` with standard middleware | Fiber or Echo framework |\n| Storage Backend | File-based with custom indexing | Embedded database (BadgerDB) |\n| Data Serialization | JSON for config, binary for metrics | Protocol Buffers with compression |\n| Frontend Framework | Vanilla JavaScript with Chart.js | React or Vue.js SPA |\n\n#### Project Structure\n\nThe codebase organization separates concerns clearly while maintaining simplicity for learning purposes:\n\n```\nmetrics-dashboard/\n├── cmd/\n│   ├── server/main.go              ← HTTP server entry point\n│   └── tools/                      ← Data import/export utilities\n├── internal/\n│   ├── metrics/                    ← Core metric types and validation\n│   │   ├── types.go               ← MetricType, Labels, Sample structs\n│   │   └── validation.go          ← Validate() methods\n│   ├── ingestion/                 ← Metric collection engine\n│   │   ├── handlers.go            ← HTTP endpoint handlers\n│   │   ├── prometheus.go          ← Prometheus format support\n│   │   └── validation.go          ← Label cardinality control\n│   ├── storage/                   ← Time-series storage engine\n│   │   ├── engine.go              ← Main storage interface\n│   │   ├── compaction.go          ← Background compaction\n│   │   └── retention.go           ← Data lifecycle management\n│   ├── query/                     ← Query processing engine\n│   │   ├── parser.go              ← Query language parsing\n│   │   ├── executor.go            ← Query execution\n│   │   └── functions.go           ← Aggregation functions\n│   ├── dashboard/                 ← Web dashboard\n│   │   ├── server.go              ← Static file serving\n│   │   ├── api.go                 ← Dashboard API endpoints\n│   │   └── websocket.go           ← Real-time updates\n│   └── alerting/                  ← Alert evaluation and notification\n│       ├── evaluator.go           ← Alert rule evaluation\n│       ├── notifier.go            ← Notification delivery\n│       └── state.go               ← Alert state management\n├── web/                           ← Frontend assets\n│   ├── static/                    ← CSS, JavaScript, images\n│   └── templates/                 ← HTML templates\n├── config/                        ← Configuration files\n│   ├── server.yaml                ← Main server configuration\n│   └── alerts.yaml                ← Alert rule definitions\n└── docs/                          ← Documentation and examples\n```\n\n#### Core Type Definitions\n\nThese fundamental types establish the data model foundation that all components build upon:\n\n```go\n// Package metrics defines core data structures for the metrics system.\npackage metrics\n\nimport (\n    \"fmt\"\n    \"sort\"\n    \"strings\"\n    \"time\"\n)\n\n// MetricType represents the type of metric being stored.\ntype MetricType int\n\nconst (\n    MetricTypeCounter MetricType = iota\n    MetricTypeGauge\n    MetricTypeHistogram\n)\n\n// Labels represents key-value pairs attached to metrics for multi-dimensional querying.\ntype Labels map[string]string\n\n// String returns the canonical string representation of labels for storage keys.\nfunc (l Labels) String() string {\n    // TODO 1: Extract keys into slice and sort alphabetically\n    // TODO 2: Build string in format \"key1=value1,key2=value2\"\n    // TODO 3: Handle special characters in keys/values\n}\n\n// Copy creates a deep copy of the labels map.\nfunc (l Labels) Copy() Labels {\n    // TODO 1: Create new map with same capacity\n    // TODO 2: Copy each key-value pair\n    // TODO 3: Return new map\n}\n\n// Sample represents a single metric measurement at a point in time.\ntype Sample struct {\n    Value     float64   `json:\"value\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\n// Metric represents a complete metric with metadata and sample data.\ntype Metric struct {\n    Name    string     `json:\"name\"`\n    Type    MetricType `json:\"type\"`\n    Labels  Labels     `json:\"labels\"`\n    Samples []Sample   `json:\"samples\"`\n}\n\n// SeriesID returns a unique identifier for this metric's time series.\nfunc (m *Metric) SeriesID() string {\n    // TODO 1: Combine metric name with sorted labels\n    // TODO 2: Create deterministic string representation\n    // TODO 3: Consider using hash for very long IDs\n}\n\n// Validate checks if the metric has valid fields and structure.\nfunc (m *Metric) Validate() error {\n    // TODO 1: Check metric name is non-empty and valid format\n    // TODO 2: Verify metric type is valid enum value\n    // TODO 3: Validate labels don't exceed cardinality limits\n    // TODO 4: Check samples are sorted by timestamp\n    // TODO 5: Validate sample values for metric type (counters non-negative, etc.)\n}\n```\n\n#### HTTP Server Foundation\n\nThe server infrastructure provides the foundation for all API endpoints with proper middleware and graceful shutdown:\n\n```go\n// Package server provides HTTP server infrastructure for the metrics system.\npackage server\n\nimport (\n    \"context\"\n    \"net/http\"\n    \"time\"\n)\n\n// Server wraps an HTTP server with middleware and graceful shutdown capabilities.\ntype Server struct {\n    httpServer *http.Server\n    mux        *http.ServeMux\n}\n\n// NewServer creates a new HTTP server listening on the specified address.\nfunc NewServer(addr string) *Server {\n    mux := http.NewServeMux()\n    \n    server := &http.Server{\n        Addr:         addr,\n        Handler:      mux,\n        ReadTimeout:  15 * time.Second,\n        WriteTimeout: 15 * time.Second,\n        IdleTimeout:  60 * time.Second,\n    }\n    \n    return &Server{\n        httpServer: server,\n        mux:        mux,\n    }\n}\n\n// RegisterRoutes adds API endpoint handlers to the server.\nfunc (s *Server) RegisterRoutes(handlers map[string]http.HandlerFunc) {\n    // TODO 1: Iterate through handler map\n    // TODO 2: Register each route with middleware\n    // TODO 3: Add logging and metrics middleware\n    // TODO 4: Add CORS headers for browser requests\n}\n\n// Start begins serving HTTP requests.\nfunc (s *Server) Start() error {\n    // TODO 1: Log server startup information\n    // TODO 2: Start HTTP server\n    // TODO 3: Handle startup errors\n}\n\n// Shutdown performs graceful shutdown with timeout.\nfunc (s *Server) Shutdown(ctx context.Context) error {\n    // TODO 1: Log shutdown initiation\n    // TODO 2: Stop accepting new connections\n    // TODO 3: Wait for existing requests to complete\n    // TODO 4: Force close after timeout\n}\n```\n\n#### Milestone Checkpoints\n\nAfter implementing each milestone, verify the system works correctly with these concrete tests:\n\n**Milestone 1 - Metrics Collection:**\n- Start the server: `go run cmd/server/main.go`\n- Send a counter metric: `curl -X POST localhost:8080/api/v1/metrics -d '{\"name\":\"http_requests\",\"type\":0,\"labels\":{\"service\":\"api\"},\"samples\":[{\"value\":1,\"timestamp\":\"2024-01-01T12:00:00Z\"}]}'`\n- Verify response: Should return 200 OK with no error message\n- Check logs: Should show metric ingestion and validation messages\n\n**Milestone 2 - Storage & Querying:**\n- Query the stored metric: `curl \"localhost:8080/api/v1/query?metric=http_requests&start=2024-01-01T11:00:00Z&end=2024-01-01T13:00:00Z\"`\n- Verify response: Should return JSON with the stored sample\n- Check storage files: Should see new files created in data directory\n- Test aggregation: `curl \"localhost:8080/api/v1/query?metric=http_requests&start=2024-01-01T11:00:00Z&end=2024-01-01T13:00:00Z&func=sum\"`\n\n**Milestone 3 - Visualization Dashboard:**\n- Open browser: Navigate to `http://localhost:8080/dashboard`\n- Create chart: Should see interface for adding metric queries\n- View data: Should see line chart with the stored metrics\n- Test auto-refresh: Chart should update automatically every 30 seconds\n\n**Milestone 4 - Alerting System:**\n- Create alert rule: POST to `/api/v1/alerts` with threshold condition\n- Trigger alert: Send metric values that exceed the threshold\n- Verify notification: Check configured notification channel for alert message\n- Test resolution: Send normal values and verify resolution notification\n\n\n## High-Level Architecture\n\n> **Milestone(s):** All milestones (this architectural foundation supports the entire metrics and alerting system)\n\n### The Orchestra Metaphor: Understanding System Architecture\n\nThink of a metrics and alerting system like a symphony orchestra preparing for and performing a concert. The **Ingestion Engine** is like the stage crew collecting sheet music from various composers (applications) and organizing it by instrument and section. The **Storage Engine** acts as the music library, carefully cataloging and preserving every piece of music with efficient indexing so any composition can be quickly retrieved. The **Query Engine** functions as the conductor's assistant, who can instantly find specific musical passages, transpose them to different keys, or create medleys by combining multiple pieces. The **Dashboard** serves as the concert hall's display system, showing the audience (operators) what's currently being performed with beautiful visualizations of the musical data. Finally, the **Alerting System** operates like the concert hall's fire safety system, continuously monitoring for problems and immediately notifying the right people when something goes wrong.\n\nJust as each orchestra section has distinct responsibilities but must coordinate seamlessly to create beautiful music, our metrics system components have clear ownership boundaries while collaborating through well-defined interfaces to provide comprehensive observability.\n\n### Component Responsibilities\n\nOur metrics and alerting system divides functionality across five major components, each with distinct responsibilities and clear interfaces. This separation of concerns enables independent development, testing, and scaling of each component while maintaining clean integration points.\n\n![System Component Architecture](./diagrams/system-architecture.svg)\n\nThe **Ingestion Engine** owns all aspects of receiving and preprocessing metrics data from external sources. It handles both push-based metrics (where applications actively send data to our system) and pull-based metrics (where our system scrapes metrics from application endpoints). The ingestion engine is responsible for validating incoming metric data, enforcing label cardinality limits to prevent explosion, normalizing metric names and label formats, and performing any necessary data transformations before storage. It also implements rate limiting and backpressure mechanisms to protect downstream components from being overwhelmed by metric data floods.\n\n| Component Responsibility | Description | Key Interfaces |\n|-------------------------|-------------|----------------|\n| Protocol Handling | Accept HTTP POST requests with metric data, scrape Prometheus endpoints | `IngestMetrics(metrics []Metric) error`, `ScrapeEndpoint(url string) error` |\n| Data Validation | Verify metric types, validate label formats, enforce naming conventions | `ValidateMetric(metric Metric) error`, `CheckCardinality(labels Labels) error` |\n| Preprocessing | Normalize metric names, sanitize label values, apply transformations | `NormalizeMetric(metric Metric) Metric`, `SanitizeLabels(labels Labels) Labels` |\n| Backpressure Control | Implement rate limiting, queue management, graceful degradation | `CheckRateLimit(source string) bool`, `QueueMetric(metric Metric) error` |\n\nThe **Storage Engine** takes complete ownership of persisting time-series data efficiently and providing fast access patterns for both recent and historical data. It implements a sophisticated storage format optimized for time-series workloads, with separate handling for metadata (metric names and labels) and sample data (timestamps and values). The storage engine automatically manages data lifecycle through compaction processes that merge small files into larger ones and downsample older data to reduce storage requirements. It also enforces retention policies by automatically deleting data that exceeds configured age limits.\n\n| Storage Responsibility | Description | Key Interfaces |\n|------------------------|-------------|----------------|\n| Sample Storage | Persist timestamped metric values with efficient compression | `WriteSamples(samples []Sample) error`, `ReadSamples(query TimeRange) []Sample` |\n| Metadata Management | Index metric names and labels for fast lookup | `IndexMetric(name string, labels Labels) SeriesID`, `LookupSeries(query LabelQuery) []SeriesID` |\n| Data Lifecycle | Automatic compaction, downsampling, and retention enforcement | `CompactBlocks(blocks []BlockID) error`, `ApplyRetentionPolicy() error` |\n| Query Optimization | Maintain indexes and statistics for efficient query execution | `GetSeriesStats(series SeriesID) SeriesStats`, `OptimizeQuery(query Query) QueryPlan` |\n\nThe **Query Engine** serves as the intelligent data access layer, translating user queries into efficient storage operations and applying mathematical transformations to raw time-series data. It parses query expressions written in our custom query language, develops optimal execution plans that minimize storage I/O, and executes aggregation functions across multiple time series. The query engine also handles time alignment, ensuring that data points from different series are properly synchronized when performing mathematical operations.\n\n| Query Responsibility | Description | Key Interfaces |\n|---------------------|-------------|----------------|\n| Query Parsing | Parse query strings into structured query objects | `ParseQuery(queryString string) (Query, error)` |\n| Execution Planning | Optimize query execution order and storage access patterns | `PlanQuery(query Query) QueryPlan`, `EstimateCost(plan QueryPlan) int` |\n| Data Aggregation | Apply mathematical functions across time series | `Aggregate(function AggFunc, series []TimeSeries) TimeSeries` |\n| Result Formatting | Transform query results into client-requested formats | `FormatResults(results []TimeSeries, format OutputFormat) []byte` |\n\nThe **Dashboard** component provides the web-based user interface for visualizing metrics data and managing system configuration. It maintains a persistent store of dashboard configurations, handles user authentication and authorization, and manages real-time data updates through WebSocket connections. The dashboard component also generates shareable URLs and embeddable widgets for distributing metrics visibility across different teams and systems.\n\n| Dashboard Responsibility | Description | Key Interfaces |\n|--------------------------|-------------|----------------|\n| Configuration Management | Save/load dashboard layouts and panel configurations | `SaveDashboard(config DashboardConfig) error`, `LoadDashboard(id string) DashboardConfig` |\n| Real-Time Updates | Push live metric data to browser clients via WebSockets | `SubscribeToUpdates(client ClientID, queries []Query)`, `BroadcastUpdate(data MetricData)` |\n| Visualization Rendering | Generate charts from time-series data | `RenderChart(data []TimeSeries, chartType ChartType) ChartData` |\n| Access Control | Manage user permissions for viewing and editing dashboards | `CheckPermission(user User, dashboard DashboardID, action Action) bool` |\n\nThe **Alerting System** continuously monitors metrics data against user-defined rules and manages the complete alert lifecycle from detection through resolution notification. It evaluates alert conditions at regular intervals, maintains alert state across restarts, and delivers notifications through multiple channels with proper rate limiting and escalation policies. The alerting system also provides alert silencing capabilities for maintenance windows and implements intelligent grouping to reduce notification noise.\n\n| Alerting Responsibility | Description | Key Interfaces |\n|-------------------------|-------------|----------------|\n| Rule Evaluation | Periodically check metric values against alert conditions | `EvaluateRule(rule AlertRule) AlertState`, `ScheduleEvaluation(rule AlertRule)` |\n| State Management | Track alert states and manage transitions between states | `UpdateAlertState(alert AlertID, state AlertState)`, `GetAlertHistory(alert AlertID) []StateTransition` |\n| Notification Delivery | Send alert messages through configured channels | `SendNotification(alert Alert, channel NotificationChannel) error` |\n| Alert Grouping | Combine related alerts to reduce notification volume | `GroupAlerts(alerts []Alert) []AlertGroup`, `ShouldGroup(alert1, alert2 Alert) bool` |\n\n> **Design Insight: Clear Boundaries Enable Independent Evolution**\n> \n> By defining precise component responsibilities and stable interfaces, each component can evolve independently without breaking the entire system. The ingestion engine can add new protocol support, the storage engine can optimize its internal format, and the query engine can implement new aggregation functions, all without requiring changes to other components.\n\n### Data Flow Overview\n\nUnderstanding how data flows through our metrics system is crucial for debugging issues, optimizing performance, and ensuring data consistency. The complete data journey follows several distinct paths depending on whether we're handling metric ingestion, dashboard queries, or alert evaluations.\n\n**Metric Ingestion Flow**: The primary data flow begins when applications send metrics to our ingestion endpoints or when our system scrapes metrics from application endpoints. Raw metric data enters through HTTP requests containing JSON payloads or Prometheus exposition format text. The ingestion engine immediately performs validation, checking that metric names follow naming conventions, label values don't exceed cardinality limits, and timestamps fall within acceptable ranges. Valid metrics undergo preprocessing where label values are sanitized, metric names are normalized to lowercase, and any configured transformations are applied.\n\nProcessed metrics then flow to the storage engine through an internal queue that provides buffering and backpressure protection. The storage engine writes sample data to time-series blocks while simultaneously updating metadata indexes with new metric names and label combinations. Each metric sample generates a `SeriesID` that uniquely identifies the combination of metric name and labels, enabling efficient storage and retrieval of related data points.\n\n| Ingestion Stage | Input Format | Processing | Output Format |\n|-----------------|--------------|------------|---------------|\n| Protocol Reception | HTTP JSON/Prometheus text | Parse request body, extract metrics | `[]Metric` structs |\n| Validation | `[]Metric` | Check names, labels, timestamps, cardinality | Valid `[]Metric` |\n| Preprocessing | Valid `[]Metric` | Normalize names, sanitize labels | Processed `[]Metric` |\n| Storage Writing | Processed `[]Metric` | Generate SeriesID, write samples and metadata | Persistent storage blocks |\n\n**Dashboard Query Flow**: When users view dashboards or create new visualizations, the dashboard component sends queries to the query engine requesting specific time-series data. These queries specify metric names, label filters, time ranges, and aggregation functions needed to generate chart data. The query engine parses each query string into a structured query object, then develops an execution plan that minimizes storage I/O by identifying which storage blocks contain relevant data.\n\nQuery execution retrieves raw samples from storage blocks, applies any necessary filtering based on label conditions, and performs mathematical aggregations like sum, average, or rate calculations across multiple time series. Results are formatted according to the client's requirements and sent back to the dashboard component, which renders them into interactive charts and updates browser clients through WebSocket connections.\n\n| Query Stage | Input | Processing | Output |\n|-------------|--------|------------|--------|\n| Query Reception | Query string from dashboard | Parse into structured Query object | `Query` struct |\n| Execution Planning | `Query` struct | Identify relevant storage blocks, optimize access order | `QueryPlan` |\n| Data Retrieval | `QueryPlan` | Read samples from storage, apply label filters | Raw `[]Sample` |\n| Aggregation | Raw `[]Sample` | Apply mathematical functions, align timestamps | `[]TimeSeries` |\n| Result Formatting | `[]TimeSeries` | Convert to client format (JSON/CSV) | Formatted response |\n\n**Alert Evaluation Flow**: The alerting system runs on independent schedules, continuously evaluating alert rules against live metrics data. Each alert rule defines a query expression, comparison operator, threshold value, and evaluation frequency. The alert evaluator sends these queries to the query engine using the same interfaces as dashboard queries, but typically focuses on recent data within the last few minutes or hours.\n\nQuery results undergo threshold comparison to determine if alert conditions are met. The alerting system maintains state machines for each alert rule, tracking transitions between inactive, pending, firing, and resolved states. State changes trigger notification delivery through configured channels like email, Slack, or PagerDuty, with message content generated from customizable templates that include metric values and trend information.\n\n| Alert Stage | Input | Processing | Output |\n|-------------|--------|------------|--------|\n| Rule Scheduling | Timer trigger | Load active alert rules from configuration | `[]AlertRule` |\n| Query Generation | `AlertRule` | Convert rule conditions into query expressions | `Query` |\n| Condition Evaluation | Query results | Compare metric values against thresholds | `AlertState` |\n| State Management | Current and new `AlertState` | Determine state transitions, update persistence | State change events |\n| Notification Delivery | State change events | Generate messages, send through channels | External notifications |\n\n> **Architecture Decision: Event-Driven Component Communication**\n> - **Context**: Components need to coordinate without tight coupling, enabling independent scaling and deployment\n> - **Options Considered**: \n>   1. Direct method calls between components\n>   2. Event-driven communication with message queues\n>   3. Shared database with polling\n> - **Decision**: Event-driven communication with internal message queues\n> - **Rationale**: Provides loose coupling, natural backpressure, and enables async processing without external dependencies\n> - **Consequences**: Adds complexity but enables independent component scaling and improves system resilience\n\n### Deployment Architecture\n\nThe deployment architecture determines how our components are packaged, configured, and operated in production environments. Our design supports both single-node deployments for development and testing, as well as distributed deployments for production scale and high availability.\n\n**Single-Node Deployment**: For development, testing, and small production environments, all components run within a single process as separate goroutines communicating through Go channels. This deployment model minimizes operational complexity while providing the full functionality of our metrics system. The single binary includes HTTP servers for ingestion and dashboard endpoints, background goroutines for alert evaluation, and embedded storage using local filesystem.\n\nConfiguration is provided through a single YAML file that defines ingestion endpoints, storage paths, dashboard settings, and alert rules. The process listens on multiple ports: 8080 for metric ingestion, 3000 for dashboard web interface, and 9090 for health checks and administrative operations. Internal communication between components uses Go channels for metric data flow and shared memory for configuration and state information.\n\n| Deployment Component | Process | Ports | Storage |\n|---------------------|---------|-------|---------|\n| Ingestion Engine | Main process, goroutine pool | 8080 (HTTP) | In-memory queues |\n| Storage Engine | Main process, background workers | N/A (internal) | Local filesystem blocks |\n| Query Engine | Main process, shared service | N/A (internal) | Shared memory caches |\n| Dashboard | Main process, HTTP server | 3000 (HTTP/WebSocket) | SQLite for configs |\n| Alerting System | Main process, cron scheduler | N/A (internal) | Local state files |\n\n**Multi-Node Deployment**: Production environments requiring higher throughput and availability can deploy components as separate services communicating over HTTP APIs. The ingestion engine runs as a stateless service that can be horizontally scaled behind a load balancer. Multiple ingestion instances write to a shared storage layer that coordinates writes through file locking and atomic operations.\n\nThe storage engine operates as a separate service with dedicated high-performance storage volumes and background processes for compaction and retention enforcement. Query engines run as stateless services that connect to storage over internal network interfaces, enabling horizontal scaling of query processing. Dashboard instances share configuration through an external database, while alerting systems use distributed coordination to prevent duplicate alert evaluations.\n\nService discovery enables components to find each other dynamically as instances are added or removed. Health checks monitor each service independently, and failures trigger automatic restarts or traffic redirection to healthy instances.\n\n| Service | Scaling Model | Dependencies | Communication |\n|---------|---------------|--------------|---------------|\n| Ingestion Service | Horizontal (stateless) | Storage service | HTTP POST to storage |\n| Storage Service | Vertical (shared state) | Persistent volumes | HTTP API for queries |\n| Query Service | Horizontal (stateless) | Storage service | HTTP API calls |\n| Dashboard Service | Horizontal (shared DB) | External database | HTTP + WebSocket |\n| Alerting Service | Active/standby | Query service | HTTP queries, webhook notifications |\n\n**Container Deployment**: Both single-node and multi-node deployments support containerization using Docker images. The single-node deployment produces one image containing all components, while multi-node deployment creates separate images for each service. Container images include all necessary dependencies and use multi-stage builds to minimize image size.\n\nKubernetes deployment manifests define resource requirements, health checks, and scaling policies for each component. Persistent volumes ensure storage durability across container restarts, while ConfigMaps and Secrets manage configuration and sensitive credentials. Service meshes like Istio can provide additional features like traffic encryption, request tracing, and advanced load balancing.\n\n| Container Type | Image Size | Resource Requirements | Persistent Storage |\n|----------------|------------|----------------------|-------------------|\n| Single-node | ~50MB | 512MB RAM, 1 CPU | 10GB volume |\n| Ingestion service | ~20MB | 256MB RAM, 0.5 CPU | None (stateless) |\n| Storage service | ~30MB | 2GB RAM, 2 CPU | 100GB+ volume |\n| Query service | ~25MB | 1GB RAM, 1 CPU | None (stateless) |\n| Dashboard service | ~35MB | 512MB RAM, 0.5 CPU | External database |\n| Alerting service | ~20MB | 256MB RAM, 0.5 CPU | Small state volume |\n\n> **Design Principle: Deployment Flexibility Without Architecture Compromise**\n> \n> Our component architecture naturally supports multiple deployment models without requiring architectural changes. The same codebase can run as a single process for development or as distributed services for production, with deployment-specific configuration determining communication mechanisms.\n\n**Configuration Management**: Deployment architecture includes comprehensive configuration management that adapts to different environments while maintaining consistency. Configuration is organized hierarchically with global defaults, environment-specific overrides, and runtime parameters. The single-node deployment uses local configuration files, while distributed deployments can integrate with configuration management systems like Consul or etcd.\n\nEnvironment variables provide deployment-specific parameters like network addresses, credentials, and resource limits. Configuration validation occurs at startup to catch errors before services begin processing data. Hot configuration reloading enables operational changes without service restarts for non-critical settings like dashboard themes or alert message templates.\n\n| Configuration Layer | Source | Scope | Reload Support |\n|--------------------|--------|-------|----------------|\n| Global Defaults | Compiled into binary | All deployments | Requires restart |\n| Environment Config | YAML files or config service | Per environment | Hot reload |\n| Runtime Parameters | Environment variables | Per instance | Hot reload |\n| User Preferences | Database or local storage | Per user | Immediate |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Tight Component Coupling Through Direct Database Access**\nMultiple components directly accessing the same database tables creates hidden dependencies and makes it difficult to evolve components independently. For example, if both the query engine and alerting system directly read from the same storage tables, changes to the storage schema require coordinating updates across multiple components. Instead, all data access should flow through well-defined service interfaces that can evolve their internal implementations without breaking clients.\n\n⚠️ **Pitfall: Ignoring Backpressure in Data Flow Design**\nDesigning data flows without considering what happens when downstream components can't keep up leads to memory exhaustion and data loss. A common mistake is having the ingestion engine accept unlimited metrics without checking if the storage engine can handle the write load. This causes metric data to accumulate in memory until the process crashes. Always implement queue limits, circuit breakers, and graceful degradation when downstream services are overloaded.\n\n⚠️ **Pitfall: Mixing Configuration and State in Component Design**\nStoring configuration data (like dashboard definitions) in the same storage systems as operational data (like metric samples) creates operational complexity and scaling problems. Configuration data has different access patterns, consistency requirements, and backup needs than time-series data. Keep them separated with configuration using databases optimized for transactional consistency and operational data using storage optimized for time-series workloads.\n\n⚠️ **Pitfall: Single Points of Failure in Distributed Architecture**\nWhile distributed deployment provides scalability benefits, introducing single points of failure eliminates availability advantages. Common mistakes include having only one storage service instance, sharing state through a single external database, or requiring all components to register with a single service discovery instance. Design redundancy into every component that stores state, and ensure the system can operate with reduced functionality when individual components fail.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| HTTP Server | Go `net/http` with `gorilla/mux` router | Go `gin-gonic/gin` or `echo` framework |\n| Inter-Component Communication | Go channels for single-node, HTTP for multi-node | gRPC with Protocol Buffers |\n| Configuration Management | YAML files with `gopkg.in/yaml.v3` | Consul or etcd integration |\n| Service Discovery | Static configuration files | Kubernetes DNS or Consul |\n| Health Checks | Simple HTTP endpoints returning status | Structured health checks with dependency status |\n| Logging | Go `log/slog` with structured output | External logging systems like ELK or Prometheus logs |\n\n#### Recommended File Structure\n\n```\nmetrics-system/\n├── cmd/\n│   ├── single-node/\n│   │   └── main.go                 ← Single-process deployment\n│   └── distributed/\n│       ├── ingestion/main.go       ← Ingestion service\n│       ├── storage/main.go         ← Storage service\n│       ├── query/main.go           ← Query service\n│       ├── dashboard/main.go       ← Dashboard service\n│       └── alerting/main.go        ← Alerting service\n├── internal/\n│   ├── ingestion/\n│   │   ├── server.go               ← HTTP handlers and validation\n│   │   ├── processor.go            ← Metric preprocessing logic\n│   │   └── ingestion_test.go\n│   ├── storage/\n│   │   ├── engine.go               ← Storage interface implementation\n│   │   ├── blocks.go               ← Time-series block management\n│   │   └── storage_test.go\n│   ├── query/\n│   │   ├── engine.go               ← Query processing logic\n│   │   ├── parser.go               ← Query language parsing\n│   │   └── query_test.go\n│   ├── dashboard/\n│   │   ├── server.go               ← Web server and WebSocket handling\n│   │   ├── config.go               ← Dashboard configuration management\n│   │   └── dashboard_test.go\n│   ├── alerting/\n│   │   ├── evaluator.go            ← Alert rule evaluation\n│   │   ├── notifier.go             ← Notification delivery\n│   │   └── alerting_test.go\n│   └── shared/\n│       ├── types.go                ← Common data structures\n│       ├── config.go               ← Configuration loading\n│       └── health.go               ← Health check utilities\n├── web/\n│   ├── static/                     ← Dashboard HTML, CSS, JavaScript\n│   └── templates/                  ← Go templates for dashboard rendering\n├── configs/\n│   ├── single-node.yaml           ← Single-node configuration\n│   └── distributed/                ← Per-service configurations\n└── docker/\n    ├── Dockerfile.single           ← Single-node container\n    └── Dockerfile.service          ← Multi-stage service containers\n```\n\n#### Infrastructure Starter Code\n\nThe following code provides complete, working infrastructure that handles cross-cutting concerns so you can focus on the core metrics system logic:\n\n**Configuration Management (`internal/shared/config.go`):**\n\n```go\npackage shared\n\nimport (\n    \"fmt\"\n    \"os\"\n    \"time\"\n    \n    \"gopkg.in/yaml.v3\"\n)\n\n// Config represents the complete system configuration with all component settings.\n// Load this once at startup and pass relevant sections to each component.\ntype Config struct {\n    Server   ServerConfig   `yaml:\"server\"`\n    Storage  StorageConfig  `yaml:\"storage\"`\n    Alerting AlertingConfig `yaml:\"alerting\"`\n}\n\ntype ServerConfig struct {\n    IngestionPort int           `yaml:\"ingestion_port\"`\n    DashboardPort int           `yaml:\"dashboard_port\"`\n    ReadTimeout   time.Duration `yaml:\"read_timeout\"`\n    WriteTimeout  time.Duration `yaml:\"write_timeout\"`\n}\n\ntype StorageConfig struct {\n    DataDir         string        `yaml:\"data_dir\"`\n    RetentionPeriod time.Duration `yaml:\"retention_period\"`\n    CompactionInterval time.Duration `yaml:\"compaction_interval\"`\n}\n\ntype AlertingConfig struct {\n    EvaluationInterval time.Duration `yaml:\"evaluation_interval\"`\n    NotificationChannels []NotificationChannel `yaml:\"notification_channels\"`\n}\n\ntype NotificationChannel struct {\n    Type string            `yaml:\"type\"` // \"email\", \"slack\", \"webhook\"\n    Name string            `yaml:\"name\"`\n    Config map[string]string `yaml:\"config\"`\n}\n\n// LoadConfig reads configuration from the specified file path with environment variable overrides.\n// Returns a complete Config struct ready for use by all components.\nfunc LoadConfig(configPath string) (*Config, error) {\n    data, err := os.ReadFile(configPath)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to read config file: %w\", err)\n    }\n    \n    var config Config\n    if err := yaml.Unmarshal(data, &config); err != nil {\n        return nil, fmt.Errorf(\"failed to parse config: %w\", err)\n    }\n    \n    // Apply environment variable overrides\n    if port := os.Getenv(\"INGESTION_PORT\"); port != \"\" {\n        fmt.Sscanf(port, \"%d\", &config.Server.IngestionPort)\n    }\n    if dataDir := os.Getenv(\"STORAGE_DATA_DIR\"); dataDir != \"\" {\n        config.Storage.DataDir = dataDir\n    }\n    \n    return &config, nil\n}\n\n// Validate checks that all required configuration values are present and valid.\n// Call this after loading configuration to catch errors before starting services.\nfunc (c *Config) Validate() error {\n    if c.Server.IngestionPort <= 0 || c.Server.IngestionPort > 65535 {\n        return fmt.Errorf(\"invalid ingestion port: %d\", c.Server.IngestionPort)\n    }\n    if c.Storage.DataDir == \"\" {\n        return fmt.Errorf(\"storage data directory must be specified\")\n    }\n    if c.Storage.RetentionPeriod <= 0 {\n        return fmt.Errorf(\"retention period must be positive\")\n    }\n    return nil\n}\n```\n\n**Health Check System (`internal/shared/health.go`):**\n\n```go\npackage shared\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n)\n\n// HealthStatus represents the health state of a component or dependency.\ntype HealthStatus int\n\nconst (\n    HealthStatusHealthy HealthStatus = iota\n    HealthStatusDegraded\n    HealthStatusUnhealthy\n)\n\nfunc (h HealthStatus) String() string {\n    switch h {\n    case HealthStatusHealthy:\n        return \"healthy\"\n    case HealthStatusDegraded:\n        return \"degraded\"\n    case HealthStatusUnhealthy:\n        return \"unhealthy\"\n    default:\n        return \"unknown\"\n    }\n}\n\n// HealthCheck represents a single health check with its current status.\ntype HealthCheck struct {\n    Name        string                                `json:\"name\"`\n    Status      HealthStatus                         `json:\"status\"`\n    LastChecked time.Time                           `json:\"last_checked\"`\n    Message     string                              `json:\"message,omitempty\"`\n    CheckFunc   func(ctx context.Context) (HealthStatus, string) `json:\"-\"`\n}\n\n// HealthManager coordinates health checks across all system components.\n// Each component registers its health checks, and the manager periodically executes them.\ntype HealthManager struct {\n    checks map[string]*HealthCheck\n    mu     sync.RWMutex\n}\n\n// NewHealthManager creates a new health manager ready for check registration.\nfunc NewHealthManager() *HealthManager {\n    return &HealthManager{\n        checks: make(map[string]*HealthCheck),\n    }\n}\n\n// RegisterCheck adds a health check that will be executed periodically.\n// The checkFunc should return the current health status and an optional message.\nfunc (hm *HealthManager) RegisterCheck(name string, checkFunc func(ctx context.Context) (HealthStatus, string)) {\n    hm.mu.Lock()\n    defer hm.mu.Unlock()\n    \n    hm.checks[name] = &HealthCheck{\n        Name:      name,\n        Status:    HealthStatusUnhealthy, // Start unhealthy until first check\n        CheckFunc: checkFunc,\n    }\n}\n\n// RunChecks executes all registered health checks and updates their status.\n// Call this periodically (e.g., every 30 seconds) to keep health status current.\nfunc (hm *HealthManager) RunChecks(ctx context.Context) {\n    hm.mu.Lock()\n    defer hm.mu.Unlock()\n    \n    for _, check := range hm.checks {\n        status, message := check.CheckFunc(ctx)\n        check.Status = status\n        check.Message = message\n        check.LastChecked = time.Now()\n    }\n}\n\n// GetOverallStatus returns the worst status among all checks.\n// System is healthy only if all checks are healthy.\nfunc (hm *HealthManager) GetOverallStatus() HealthStatus {\n    hm.mu.RLock()\n    defer hm.mu.RUnlock()\n    \n    worst := HealthStatusHealthy\n    for _, check := range hm.checks {\n        if check.Status > worst {\n            worst = check.Status\n        }\n    }\n    return worst\n}\n\n// ServeHTTP implements http.Handler to provide health check endpoint.\n// Returns 200 for healthy, 503 for degraded/unhealthy with JSON details.\nfunc (hm *HealthManager) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    hm.mu.RLock()\n    checks := make(map[string]*HealthCheck, len(hm.checks))\n    for name, check := range hm.checks {\n        // Copy struct to avoid exposing internal pointers\n        checkCopy := *check\n        checkCopy.CheckFunc = nil // Don't serialize function\n        checks[name] = &checkCopy\n    }\n    hm.mu.RUnlock()\n    \n    overall := hm.GetOverallStatus()\n    \n    response := struct {\n        Status string                     `json:\"status\"`\n        Checks map[string]*HealthCheck   `json:\"checks\"`\n    }{\n        Status: overall.String(),\n        Checks: checks,\n    }\n    \n    w.Header().Set(\"Content-Type\", \"application/json\")\n    if overall != HealthStatusHealthy {\n        w.WriteHeader(http.StatusServiceUnavailable)\n    }\n    \n    json.NewEncoder(w).Encode(response)\n}\n```\n\n#### Core Architecture Skeleton\n\nThe following code provides the main architectural structure with detailed TODOs mapping to the design concepts above:\n\n**Component Interface Definitions (`internal/shared/interfaces.go`):**\n\n```go\npackage shared\n\nimport (\n    \"context\"\n    \"time\"\n)\n\n// MetricsIngester defines the interface for receiving and processing metrics data.\n// Implement this interface in your ingestion engine component.\ntype MetricsIngester interface {\n    // IngestMetrics processes a batch of metrics from push-based sources.\n    // TODO 1: Validate each metric using Metric.Validate()\n    // TODO 2: Check label cardinality to prevent explosion\n    // TODO 3: Apply any configured preprocessing transformations\n    // TODO 4: Send processed metrics to storage engine via queue\n    // TODO 5: Return error if any metrics were rejected\n    IngestMetrics(ctx context.Context, metrics []Metric) error\n    \n    // ScrapeEndpoint retrieves metrics from a pull-based Prometheus endpoint.\n    // TODO 1: Make HTTP GET request to the provided URL\n    // TODO 2: Parse Prometheus exposition format response\n    // TODO 3: Convert parsed data to internal Metric structs\n    // TODO 4: Call IngestMetrics() with converted metrics\n    ScrapeEndpoint(ctx context.Context, url string) error\n}\n\n// TimeSeriesStorage defines the interface for persisting and retrieving time-series data.\n// Implement this interface in your storage engine component.\ntype TimeSeriesStorage interface {\n    // WriteSamples persists metric samples to storage blocks.\n    // TODO 1: Generate SeriesID for each unique metric name + labels combination\n    // TODO 2: Append samples to appropriate time-series blocks\n    // TODO 3: Update metadata indexes with new series information\n    // TODO 4: Trigger compaction if block size thresholds exceeded\n    // TODO 5: Return error if write operation fails\n    WriteSamples(ctx context.Context, samples []Sample) error\n    \n    // QueryRange retrieves samples within the specified time range.\n    // TODO 1: Parse label selectors to identify matching series\n    // TODO 2: Find storage blocks that overlap with time range\n    // TODO 3: Read samples from blocks, applying label filters\n    // TODO 4: Sort samples by timestamp and return\n    QueryRange(ctx context.Context, query RangeQuery) ([]TimeSeries, error)\n    \n    // GetSeriesMetadata returns metadata for series matching label selectors.\n    // TODO 1: Consult metadata indexes for matching series\n    // TODO 2: Return series names and label sets without sample data\n    GetSeriesMetadata(ctx context.Context, selectors []LabelSelector) ([]SeriesMetadata, error)\n}\n\n// QueryProcessor defines the interface for executing queries against time-series data.\n// Implement this interface in your query engine component.\ntype QueryProcessor interface {\n    // ExecuteQuery parses and executes a query string, returning formatted results.\n    // TODO 1: Parse query string into Query struct\n    // TODO 2: Validate query syntax and semantic correctness\n    // TODO 3: Create execution plan optimized for storage access\n    // TODO 4: Execute plan against storage engine\n    // TODO 5: Apply aggregation functions and mathematical operations\n    // TODO 6: Format results according to requested output format\n    ExecuteQuery(ctx context.Context, queryString string) (QueryResult, error)\n}\n\n// AlertEvaluator defines the interface for processing alert rules and managing alert state.\n// Implement this interface in your alerting system component.\ntype AlertEvaluator interface {\n    // EvaluateRules checks all active alert rules against current metric data.\n    // TODO 1: Load active alert rules from configuration\n    // TODO 2: Convert each rule condition to query expression\n    // TODO 3: Execute query using QueryProcessor interface\n    // TODO 4: Compare results against rule thresholds\n    // TODO 5: Update alert states based on comparison results\n    // TODO 6: Trigger notifications for state changes\n    EvaluateRules(ctx context.Context) error\n    \n    // UpdateAlertState changes the state of an alert and triggers notifications.\n    // TODO 1: Load current alert state from persistence\n    // TODO 2: Validate state transition is allowed\n    // TODO 3: Update persistent state storage\n    // TODO 4: Generate notification message from template\n    // TODO 5: Send notification through configured channels\n    UpdateAlertState(ctx context.Context, alertID string, newState AlertState) error\n}\n```\n\n**Component Coordinator (`internal/coordinator/coordinator.go`):**\n\n```go\npackage coordinator\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log/slog\"\n    \"sync\"\n    \"time\"\n    \n    \"metrics-system/internal/shared\"\n)\n\n// ComponentCoordinator manages the lifecycle of all system components and coordinates\n// their interactions through well-defined interfaces.\ntype ComponentCoordinator struct {\n    config     *shared.Config\n    logger     *slog.Logger\n    health     *shared.HealthManager\n    components map[string]Component\n    wg         sync.WaitGroup\n    ctx        context.Context\n    cancel     context.CancelFunc\n}\n\n// Component represents a system component that can be started and stopped independently.\ntype Component interface {\n    Start(ctx context.Context) error\n    Stop(ctx context.Context) error\n    Name() string\n    HealthCheck(ctx context.Context) (shared.HealthStatus, string)\n}\n\n// NewCoordinator creates a new component coordinator with the provided configuration.\n// Use this as the main orchestrator for both single-node and distributed deployments.\nfunc NewCoordinator(config *shared.Config, logger *slog.Logger) *ComponentCoordinator {\n    ctx, cancel := context.WithCancel(context.Background())\n    \n    return &ComponentCoordinator{\n        config:     config,\n        logger:     logger,\n        health:     shared.NewHealthManager(),\n        components: make(map[string]Component),\n        ctx:        ctx,\n        cancel:     cancel,\n    }\n}\n\n// RegisterComponent adds a component to the coordinator for lifecycle management.\n// TODO 1: Validate component name is unique\n// TODO 2: Store component in components map\n// TODO 3: Register component's health check with health manager\n// TODO 4: Log component registration for debugging\nfunc (cc *ComponentCoordinator) RegisterComponent(component Component) error {\n    // TODO: Implement component registration logic here\n    // This should store the component and set up its health monitoring\n    panic(\"implement me\")\n}\n\n// Start begins all registered components in dependency order.\n// TODO 1: Start components in order: Storage -> Query -> Ingestion -> Dashboard -> Alerting\n// TODO 2: Wait for each component to report healthy before starting next\n// TODO 3: Start health check monitoring goroutine\n// TODO 4: Set up signal handlers for graceful shutdown\nfunc (cc *ComponentCoordinator) Start() error {\n    // TODO: Implement startup sequence here\n    // Consider component dependencies and startup ordering\n    panic(\"implement me\")\n}\n\n// Stop gracefully shuts down all components in reverse dependency order.\n// TODO 1: Cancel context to signal all components to stop\n// TODO 2: Stop components in reverse order with timeout\n// TODO 3: Wait for all component goroutines to exit\n// TODO 4: Close any shared resources\nfunc (cc *ComponentCoordinator) Stop() error {\n    // TODO: Implement shutdown sequence here\n    // Ensure graceful cleanup of all resources\n    panic(\"implement me\")\n}\n\n// HealthHandler returns an HTTP handler for system health checks.\nfunc (cc *ComponentCoordinator) HealthHandler() http.Handler {\n    return cc.health\n}\n```\n\n#### Milestone Checkpoints\n\n**After implementing component interfaces:**\n- Run `go build ./cmd/single-node/` - should compile without errors\n- All interface methods should be defined with proper signatures\n- Component registration should work without panics\n\n**After implementing basic coordinator:**\n- Start the system with `./single-node -config configs/single-node.yaml`\n- Health check endpoint at `http://localhost:9090/health` should return component status\n- System should shut down gracefully with CTRL+C\n\n**After connecting components:**\n- Send a test metric: `curl -X POST localhost:8080/metrics -d '{\"name\":\"test_metric\",\"value\":42}'`\n- Query the metric: `curl \"localhost:8080/query?query=test_metric\"`\n- Check dashboard at `http://localhost:3000` shows the metric\n\n\n## Data Model\n\n> **Milestone(s):** 1 (Metrics Collection), 2 (Storage & Querying), 3 (Visualization Dashboard), 4 (Alerting System) - The data model provides the foundational structures used across all system components\n\n### The Blueprint Metaphor: Understanding System Data Models\n\nThink of a data model as the blueprint for a complex building project. Just as architectural blueprints define how rooms connect, what materials to use, and how utilities flow through the structure, our data model defines how metrics, time-series data, alerts, and dashboards are structured and relate to each other. A well-designed blueprint ensures that electricians, plumbers, and carpenters can work independently while their components integrate seamlessly. Similarly, our data model ensures that the ingestion engine, storage layer, query processor, and alerting system can operate independently while sharing a common understanding of data structures.\n\nThe data model serves as the contract between all system components. When the ingestion engine receives a metric, it must understand the difference between a counter that only increases and a gauge that can fluctuate. When the storage engine persists this data, it must organize time-series efficiently for later retrieval. When the alerting system evaluates rules, it must understand how to extract values from different metric types. This shared understanding prevents integration headaches and ensures data consistency across the entire system.\n\n### Metric Types and Structure\n\nUnderstanding metric types requires thinking about what each type measures in the real world. A **counter** behaves like an odometer in a car - it only moves forward, tracking cumulative totals like total HTTP requests served or total bytes transmitted. A **gauge** behaves like a speedometer or thermometer - it shows the current value of something that can increase or decrease, like current CPU usage or active database connections. A **histogram** behaves like a survey response collector - it groups observations into ranges (buckets) to show distribution patterns, like response time distributions or request size distributions.\n\nEach metric type has fundamentally different mathematical properties that affect how they are stored, queried, and aggregated. Counters require rate calculations to be meaningful (requests per second rather than total requests since boot). Gauges can be averaged, but their historical values may not be meaningful for trend analysis. Histograms require special aggregation logic to merge buckets across multiple instances or time windows.\n\n![Data Model Relationships](./diagrams/data-model.svg)\n\nThe core metric structure must capture not just the value and timestamp, but also the rich metadata that makes metrics searchable and aggregatable. **Labels** provide this multi-dimensional metadata, allowing metrics to be sliced and diced across different attributes like service name, environment, region, or instance ID. However, labels create a fundamental tension: more labels provide more flexibility but exponentially increase **cardinality** (the number of unique time-series combinations), which directly impacts storage requirements and query performance.\n\n> **Decision: Metric Structure Design**\n> - **Context**: Need to represent different types of metrics with their metadata while supporting efficient storage and querying\n> - **Options Considered**: \n>   1. Single unified metric structure with type discrimination\n>   2. Separate data structures for each metric type  \n>   3. Inheritance-based hierarchy with base metric class\n> - **Decision**: Single unified structure with `MetricType` enum discrimination\n> - **Rationale**: Simplifies ingestion pipeline and storage layer by having consistent interfaces, while type-specific behavior is handled through switch statements rather than complex inheritance\n> - **Consequences**: Enables polymorphic handling in storage and query layers, but requires careful validation to ensure type-specific constraints are enforced\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| Name | string | Metric identifier following hierarchical naming convention (e.g., \"http_requests_total\", \"cpu_usage_percent\") |\n| Type | MetricType | Enum indicating Counter, Gauge, or Histogram behavior and aggregation rules |\n| Labels | Labels | Key-value metadata map for multi-dimensional filtering and grouping |\n| Samples | []Sample | Ordered array of timestamped values representing the metric's evolution over time |\n\nThe `MetricType` enumeration defines the fundamental behavior categories that determine how values should be interpreted and aggregated:\n\n| Metric Type | Numeric Value | Aggregation Behavior | Typical Use Cases |\n|-------------|---------------|---------------------|-------------------|\n| Counter | 0 | Rate calculations, cumulative sums | Request counts, error counts, bytes transmitted |\n| Gauge | 1 | Point-in-time snapshots, averages | CPU usage, memory consumption, queue depth |\n| Histogram | 2 | Bucket merging, percentile calculations | Response times, request sizes, batch processing durations |\n\nThe `Labels` type provides powerful multi-dimensional indexing but requires careful cardinality management. Each unique combination of label values creates a distinct time-series, so labels with high cardinality values (like user IDs or request IDs) can create storage explosions. The system must provide both flexibility for legitimate use cases and protection against accidental cardinality bombs.\n\n| Label Constraint | Validation Rule | Purpose |\n|------------------|-----------------|---------|\n| Key Format | Must match `^[a-zA-Z_][a-zA-Z0-9_]*$` | Ensures consistent naming and prevents injection attacks |\n| Key Length | Maximum 128 characters | Prevents excessive memory usage in indexes |\n| Value Length | Maximum 1024 characters | Balances expressiveness with storage efficiency |\n| Total Labels | Maximum 32 per metric | Limits combinatorial explosion while supporting rich metadata |\n| Reserved Keys | Cannot start with `__` (double underscore) | Reserves namespace for system-generated labels |\n\nThe `Sample` structure captures the atomic unit of time-series data - a value paired with its observation timestamp. This pairing is fundamental to time-series analysis, enabling trend detection, rate calculations, and temporal aggregations.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| Value | float64 | Numeric measurement value with IEEE 754 double precision for wide range support |\n| Timestamp | time.Time | UTC timestamp with nanosecond precision for high-resolution time-series |\n\n⚠️ **Pitfall: Timestamp Precision Inconsistency**\nMany beginners mix timestamp precisions (seconds vs milliseconds vs nanoseconds) across different components, leading to data alignment issues during queries. Always normalize to nanosecond precision at ingestion time and use UTC timezone to avoid daylight savings time complications. The storage engine can downsample precision during compaction, but ingestion should preserve maximum precision.\n\n### Time-Series Schema\n\nTime-series data organization follows the principle that **storage locality drives query performance**. Think of time-series storage like organizing a vast library where books (data points) must be found quickly based on multiple criteria: author (metric name), topic (labels), and publication date (timestamp). The organization scheme determines whether finding all books by a specific author from a specific year requires scanning the entire library or jumping directly to the right shelf.\n\nThe fundamental insight is that time-series data exhibits strong **temporal locality** - queries typically request ranges of data points for the same metric and label combination. This access pattern drives the storage schema design toward grouping samples by time-series identity rather than by timestamp globally.\n\n> **Decision: Time-Series Identification Strategy**\n> - **Context**: Need to uniquely identify each time-series for efficient storage and retrieval while supporting label-based filtering\n> - **Options Considered**:\n>   1. Composite primary key of (metric_name, sorted_labels, timestamp)\n>   2. Hash-based series ID with separate label index\n>   3. Hierarchical key structure with metric name prefix\n> - **Decision**: Hash-based series ID with deterministic label sorting\n> - **Rationale**: Provides O(1) lookup performance while maintaining deterministic behavior across system restarts, and allows label indexing to be optimized separately from time-series storage\n> - **Consequences**: Enables efficient range scans within a time-series while supporting complex label queries through secondary indexes, but requires careful hash collision handling\n\n| Schema Component | Structure | Purpose |\n|------------------|-----------|---------|\n| Series ID | SHA-256 hash of metric name + sorted labels | Provides unique, deterministic identifier for each time-series |\n| Label Index | Inverted index mapping label keys/values to series IDs | Enables efficient label-based filtering and selection |\n| Sample Blocks | Compressed chunks of samples grouped by series ID and time range | Optimizes storage efficiency and range query performance |\n| Metadata Index | Series ID → metric metadata mapping | Provides series discovery and label enumeration |\n\nThe **Series ID generation** algorithm ensures consistent identification across system components and restarts:\n\n1. Concatenate metric name with newline separator\n2. Sort label pairs by key name lexicographically  \n3. Append each sorted label as \"key=value\\n\"\n4. Compute SHA-256 hash of resulting string\n5. Encode hash as hexadecimal string for human readability\n\nThis deterministic approach ensures that the same metric with identical labels always produces the same series ID, regardless of label order in the original submission or which system component processes it.\n\nThe **Label Index** provides efficient multi-dimensional filtering by maintaining inverted indexes for each label key-value pair. This allows queries like \"find all HTTP request metrics for the production environment\" to be resolved through index lookups rather than full table scans.\n\n| Index Type | Key Format | Value Format | Use Case |\n|------------|------------|--------------|----------|\n| Label Key Index | `__key__:{label_key}` | Set of series IDs | Find all series with specific label keys |\n| Label Value Index | `__kv__:{label_key}={label_value}` | Set of series IDs | Find all series with specific label key-value pairs |\n| Metric Name Index | `__name__:{metric_name}` | Set of series IDs | Find all series for a specific metric |\n| Series Metadata | `__meta__:{series_id}` | Metric name + labels JSON | Resolve series ID back to human-readable metadata |\n\n**Sample Blocks** organize the actual time-series data for optimal storage and retrieval. Each block contains samples for a single series within a specific time range, enabling efficient range queries and compression.\n\n| Block Component | Format | Purpose |\n|-----------------|--------|---------|\n| Block Header | Series ID + time range + sample count | Enables range filtering without decompressing block data |\n| Timestamp Array | Delta-compressed timestamps | Stores sample timestamps with compression for temporal locality |\n| Value Array | Float64 values with optional compression | Stores metric values with optional bit-packing for integer-like values |\n| Block Footer | Checksum + compression metadata | Ensures data integrity and provides decompression parameters |\n\nThe storage layout prioritizes **write efficiency** for recent data while maintaining **read efficiency** for historical queries. Recent samples are accumulated in memory-based blocks that are periodically flushed to disk. Historical data is organized in immutable, compressed blocks that can be efficiently scanned for range queries.\n\n> The key insight is that time-series workloads exhibit a \"hot-warm-cold\" access pattern. Recent data (last few hours) receives frequent writes and reads. Medium-aged data (days to weeks) receives occasional reads but no writes. Ancient data (months to years) receives rare reads but consumes the most storage space. The schema optimizes for each access pattern differently.\n\n⚠️ **Pitfall: Timestamp Alignment Assumptions**\nBeginning developers often assume that samples from different metrics arrive with perfectly aligned timestamps, leading to complex join logic that fails in practice. Real-world metrics arrive with jittered timestamps due to network delays, processing variations, and clock skew between systems. Design aggregation and comparison logic to handle timestamp misalignment gracefully, typically by bucketing samples into time windows rather than expecting exact timestamp matches.\n\n### Alert and Dashboard Schema\n\nThe alerting and dashboard schemas must capture not just the static configuration but also the dynamic state transitions and user interactions that occur during system operation. Think of alert rules like smoke detectors - they must define what constitutes danger (threshold conditions), how to confirm it's not a false alarm (evaluation duration), and who to notify when action is required. Dashboard configurations are like mission control displays - they must define what data to show, how to visualize it, and how to keep the information current.\n\nAlert rule design balances expressiveness with reliability. Rules must be simple enough for operators to understand and debug, yet powerful enough to detect complex failure conditions. The schema captures both the declarative rule definition and the runtime state management required for proper alert lifecycle handling.\n\n> **Decision: Alert Rule Expression Language**\n> - **Context**: Need to define alert conditions that can reference metrics, apply mathematical operations, and specify thresholds while remaining debuggable\n> - **Options Considered**:\n>   1. Simple threshold-based rules with metric name + comparison operator\n>   2. SQL-like query language with aggregation functions\n>   3. PromQL-inspired expression language with time-series functions\n> - **Decision**: PromQL-inspired expressions with simplified function set\n> - **Rationale**: Provides sufficient expressiveness for complex conditions while maintaining compatibility with monitoring industry standards, and allows rule expressions to be tested independently of alert evaluation\n> - **Consequences**: Enables powerful alert conditions like rate calculations and multi-metric comparisons, but requires implementing a query parser and expression evaluator\n\n| Alert Rule Field | Type | Description |\n|------------------|------|-------------|\n| ID | string | Unique identifier for the alert rule, used for state tracking and notifications |\n| Name | string | Human-readable alert name displayed in notifications and dashboard |\n| Expression | string | Query expression that returns numeric values for threshold comparison |\n| Threshold | float64 | Numeric value that triggers alert when expression result crosses this boundary |\n| Operator | string | Comparison operator: \"gt\" (greater than), \"lt\" (less than), \"eq\" (equal), \"ne\" (not equal) |\n| Duration | time.Duration | How long condition must persist before transitioning from pending to firing state |\n| EvaluationInterval | time.Duration | How frequently to evaluate the alert expression against current data |\n| Labels | Labels | Static labels attached to all alert instances for routing and grouping |\n| Annotations | map[string]string | Additional metadata like description, runbook URL, and dashboard links |\n\nAlert **state management** requires tracking the lifecycle of each alert instance as conditions change over time. The state machine ensures proper notification delivery and prevents alert flapping.\n\n| Alert State | Description | Entry Conditions | Exit Conditions |\n|-------------|-------------|-----------------|------------------|\n| Inactive | Alert condition is not met | Expression result does not cross threshold | Expression result crosses threshold |\n| Pending | Condition met but duration requirement not satisfied | Threshold crossed but duration timer not expired | Duration timer expires OR condition no longer met |\n| Firing | Alert is actively triggering notifications | Duration requirement satisfied while condition remains true | Condition no longer met for resolution duration |\n| Resolved | Alert has recovered from firing state | Firing alert condition returns to normal | Alert rule deleted OR condition becomes true again |\n\n| Alert Instance Field | Type | Description |\n|---------------------|------|-------------|\n| RuleID | string | References the parent alert rule configuration |\n| InstanceLabels | Labels | Label values extracted from the metric data that triggered this instance |\n| State | AlertState | Current state in the alert lifecycle (Inactive, Pending, Firing, Resolved) |\n| StateChanged | time.Time | Timestamp when alert last transitioned between states |\n| Value | float64 | Most recent expression result value for debugging and display |\n| FiredAt | *time.Time | Timestamp when alert first entered firing state (nil if never fired) |\n| ResolvedAt | *time.Time | Timestamp when alert was resolved (nil if still active) |\n\nThe **notification system** schema defines how alerts are delivered to external systems while handling delivery failures and rate limiting.\n\n| Notification Channel Field | Type | Description |\n|----------------------------|------|-------------|\n| Type | string | Channel type identifier: \"email\", \"slack\", \"webhook\", \"pagerduty\" |\n| Name | string | Human-readable channel name for management and debugging |\n| Config | map[string]string | Channel-specific configuration like URLs, tokens, and recipient lists |\n| Enabled | bool | Whether this channel should receive notifications |\n| RouteFilter | Labels | Label selector determining which alerts use this channel |\n\n**Dashboard configuration** captures the declarative specification of data visualizations while supporting real-time updates and user customization. The schema separates presentation logic from data queries to enable flexible visualization while maintaining performance.\n\n| Dashboard Field | Type | Description |\n|------------------|------|-------------|\n| ID | string | Unique dashboard identifier for URL routing and sharing |\n| Title | string | Human-readable dashboard name displayed in browser title and navigation |\n| Description | string | Optional dashboard description for documentation and search |\n| Tags | []string | Categorization tags for dashboard organization and discovery |\n| Panels | []Panel | Array of visualization panels arranged in the dashboard layout |\n| TimeRange | TimeRange | Default time window for all panels unless overridden |\n| RefreshInterval | time.Duration | How frequently panels should update their data automatically |\n| Editable | bool | Whether dashboard configuration can be modified by viewers |\n\n| Panel Field | Type | Description |\n|-------------|------|-------------|\n| ID | string | Unique panel identifier within the dashboard for layout and updates |\n| Title | string | Panel title displayed above the visualization |\n| Type | string | Visualization type: \"line\", \"bar\", \"stat\", \"heatmap\", \"table\" |\n| GridPos | GridPosition | Panel position and size within the dashboard grid layout |\n| Query | PanelQuery | Data source query configuration for this panel |\n| Options | map[string]interface{} | Panel-specific configuration like axis labels, colors, and thresholds |\n| AlertRule | *PanelAlertRule | Optional alert rule configuration tied to this panel's query |\n\n| Panel Query Field | Type | Description |\n|-------------------|------|-------------|\n| Expression | string | Time-series query expression to retrieve data for visualization |\n| TimeRange | *TimeRange | Panel-specific time range override if different from dashboard default |\n| RefreshInterval | *time.Duration | Panel-specific refresh rate override for high-frequency data |\n| MaxDataPoints | int | Maximum number of data points to retrieve for performance and readability |\n| MinInterval | time.Duration | Minimum step size for data aggregation to prevent over-resolution |\n\n⚠️ **Pitfall: Dashboard State Synchronization**\nDevelopers often implement dashboard updates by polling for new data on a fixed interval, leading to unnecessary load and inconsistent user experience. Instead, implement event-driven updates where the query engine notifies dashboards when relevant metric data changes. This provides instant updates for rapidly changing metrics while avoiding unnecessary queries for stable metrics. Implement exponential backoff for panels that haven't changed recently to optimize resource usage.\n\n⚠️ **Pitfall: Alert Flapping**\nAlert rules that toggle rapidly between firing and resolved states create notification spam and erode operator confidence. This typically occurs when thresholds are set exactly at normal operating values or when evaluation intervals are too short relative to metric volatility. Implement hysteresis by requiring different thresholds for firing versus resolving (e.g., fire at 90% CPU but don't resolve until below 80%), and require minimum durations for both firing and resolution states.\n\n### Implementation Guidance\n\nThe data model implementation requires careful attention to serialization performance, validation consistency, and type safety across all system components. The following code structure provides a foundation for building robust data handling while maintaining flexibility for future enhancements.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Serialization | JSON with `encoding/json` | Protocol Buffers with `protobuf/proto` |\n| Validation | Manual validation functions | Struct tags with `validator` library |\n| Label Storage | Map with string keys/values | Interned strings with reference counting |\n| Time Handling | `time.Time` with UTC normalization | Custom timestamp type with nanosecond precision |\n| Hash Function | `crypto/sha256` for series IDs | `xxhash` for performance-critical paths |\n\n**Recommended File Structure:**\n```\ninternal/model/\n  types.go              ← Core data types (Metric, Sample, Labels)\n  metric_types.go       ← MetricType enum and behavior\n  series.go             ← Time-series identification and indexing\n  alerts.go             ← Alert rule and state management types  \n  dashboard.go          ← Dashboard and panel configuration types\n  validation.go         ← Input validation and constraint checking\n  serialization.go      ← JSON/binary serialization helpers\n  types_test.go         ← Comprehensive unit tests for all types\n```\n\n**Core Types Infrastructure (Complete Implementation):**\n\n```go\npackage model\n\nimport (\n    \"crypto/sha256\"\n    \"fmt\"\n    \"sort\"\n    \"strings\"\n    \"time\"\n)\n\n// MetricType defines the fundamental behavior of a metric\ntype MetricType int\n\nconst (\n    MetricTypeCounter   MetricType = 0\n    MetricTypeGauge     MetricType = 1\n    MetricTypeHistogram MetricType = 2\n)\n\nfunc (mt MetricType) String() string {\n    switch mt {\n    case MetricTypeCounter:\n        return \"counter\"\n    case MetricTypeGauge:\n        return \"gauge\"\n    case MetricTypeHistogram:\n        return \"histogram\"\n    default:\n        return \"unknown\"\n    }\n}\n\n// Labels represents metric metadata with validation and indexing support\ntype Labels map[string]string\n\n// Copy creates a deep copy of the labels map\nfunc (l Labels) Copy() Labels {\n    if l == nil {\n        return nil\n    }\n    result := make(Labels, len(l))\n    for k, v := range l {\n        result[k] = v\n    }\n    return result\n}\n\n// String returns the canonical string representation for hashing and display\nfunc (l Labels) String() string {\n    if len(l) == 0 {\n        return \"{}\"\n    }\n    \n    pairs := make([]string, 0, len(l))\n    for k, v := range l {\n        pairs = append(pairs, fmt.Sprintf(`%s=\"%s\"`, k, v))\n    }\n    sort.Strings(pairs)\n    return \"{\" + strings.Join(pairs, \",\") + \"}\"\n}\n\n// Sample represents a single timestamped measurement\ntype Sample struct {\n    Value     float64   `json:\"value\"`\n    Timestamp time.Time `json:\"timestamp\"`\n}\n\n// Metric represents a named time-series with type and metadata\ntype Metric struct {\n    Name    string     `json:\"name\"`\n    Type    MetricType `json:\"type\"`\n    Labels  Labels     `json:\"labels\"`\n    Samples []Sample   `json:\"samples\"`\n}\n\n// SeriesID generates a unique, deterministic identifier for this time-series\nfunc (m *Metric) SeriesID() string {\n    // TODO 1: Create hash input string starting with metric name\n    // TODO 2: Sort labels by key to ensure deterministic ordering\n    // TODO 3: Append each label as \"key=value\\n\" to hash input\n    // TODO 4: Compute SHA-256 hash of the complete string\n    // TODO 5: Return hexadecimal encoding of hash for readability\n    // Hint: Use crypto/sha256 and fmt.Sprintf for hex encoding\n}\n\n// Validate checks metric data integrity and constraint compliance\nfunc (m *Metric) Validate() error {\n    // TODO 1: Validate metric name format (alphanumeric + underscore, no leading numbers)\n    // TODO 2: Check metric type is one of the defined enum values\n    // TODO 3: Validate each label key/value against length and format constraints  \n    // TODO 4: Ensure samples are sorted by timestamp (required for efficient storage)\n    // TODO 5: Check for reasonable value ranges (no NaN/Inf for counters)\n    // Hint: Use regular expressions for name/label validation\n}\n```\n\n**Alert and Dashboard Types (Skeleton Implementation):**\n\n```go\n// AlertRule defines conditions for triggering notifications\ntype AlertRule struct {\n    ID                 string            `json:\"id\"`\n    Name               string            `json:\"name\"`\n    Expression         string            `json:\"expression\"`\n    Threshold          float64           `json:\"threshold\"`\n    Operator           string            `json:\"operator\"`\n    Duration           time.Duration     `json:\"duration\"`\n    EvaluationInterval time.Duration     `json:\"evaluation_interval\"`\n    Labels             Labels            `json:\"labels\"`\n    Annotations        map[string]string `json:\"annotations\"`\n}\n\n// Validate ensures alert rule configuration is valid and safe\nfunc (ar *AlertRule) Validate() error {\n    // TODO 1: Validate ID is non-empty and URL-safe\n    // TODO 2: Check operator is one of: \"gt\", \"lt\", \"eq\", \"ne\"  \n    // TODO 3: Ensure duration and evaluation interval are positive\n    // TODO 4: Validate expression syntax (implement basic PromQL parser)\n    // TODO 5: Check threshold is finite (no NaN/Inf values)\n}\n\n// Dashboard represents a collection of metric visualization panels\ntype Dashboard struct {\n    ID              string        `json:\"id\"`\n    Title           string        `json:\"title\"`\n    Description     string        `json:\"description\"`\n    Tags            []string      `json:\"tags\"`\n    Panels          []Panel       `json:\"panels\"`\n    TimeRange       TimeRange     `json:\"time_range\"`\n    RefreshInterval time.Duration `json:\"refresh_interval\"`\n    Editable        bool          `json:\"editable\"`\n}\n\n// Panel represents a single visualization within a dashboard\ntype Panel struct {\n    ID        string                 `json:\"id\"`\n    Title     string                 `json:\"title\"`\n    Type      string                 `json:\"type\"`\n    GridPos   GridPosition          `json:\"grid_pos\"`\n    Query     PanelQuery            `json:\"query\"`\n    Options   map[string]interface{} `json:\"options\"`\n    AlertRule *PanelAlertRule       `json:\"alert_rule,omitempty\"`\n}\n\n// TimeRange specifies absolute or relative time bounds for queries\ntype TimeRange struct {\n    From time.Time `json:\"from\"`\n    To   time.Time `json:\"to\"`\n}\n\n// GridPosition defines panel layout within dashboard grid\ntype GridPosition struct {\n    X      int `json:\"x\"`      // Horizontal position (0-24)\n    Y      int `json:\"y\"`      // Vertical position  \n    Width  int `json:\"width\"`  // Panel width (1-24)\n    Height int `json:\"height\"` // Panel height in grid units\n}\n```\n\n**Validation and Serialization Helpers:**\n\n```go\n// ValidationError represents constraint violations during data validation\ntype ValidationError struct {\n    Field   string `json:\"field\"`\n    Value   string `json:\"value\"`\n    Message string `json:\"message\"`\n}\n\nfunc (ve *ValidationError) Error() string {\n    return fmt.Sprintf(\"validation failed for field '%s': %s\", ve.Field, ve.Message)\n}\n\n// ValidateLabels checks label constraints for cardinality and format compliance\nfunc ValidateLabels(labels Labels) error {\n    // TODO 1: Check total label count against maximum limit (32)\n    // TODO 2: Validate each key matches allowed pattern ^[a-zA-Z_][a-zA-Z0-9_]*$\n    // TODO 3: Ensure keys don't start with __ (reserved for system labels)\n    // TODO 4: Check key/value length limits (128/1024 characters)\n    // TODO 5: Return ValidationError with specific constraint violations\n    // Hint: Use regexp.MustCompile for pattern validation\n}\n\n// SerializeMetric converts metric to JSON with proper timestamp formatting\nfunc SerializeMetric(m *Metric) ([]byte, error) {\n    // TODO 1: Create a serialization wrapper with RFC3339 timestamp format\n    // TODO 2: Convert time.Time values to standardized string representation\n    // TODO 3: Use json.Marshal with proper error handling\n    // TODO 4: Consider compression for large sample arrays\n    // Hint: Create intermediate struct with string timestamps for JSON compatibility\n}\n```\n\n**Language-Specific Hints:**\n- Use `time.Time.UTC()` to normalize all timestamps and avoid timezone issues\n- Implement `json.Marshaler` and `json.Unmarshaler` interfaces for custom timestamp formatting\n- Use `sync.Pool` for label validation to avoid allocation overhead during high-throughput ingestion\n- Consider using `unsafe.Pointer` for zero-copy string operations in label processing (advanced optimization)\n- Use `sort.Slice` with stable sorting for deterministic label ordering in series ID generation\n\n**Milestone Checkpoint:**\nAfter implementing the data model types, verify correctness with:\n```bash\ngo test ./internal/model/...\n```\n\nExpected test coverage should validate:\n- Series ID generation produces identical results for same metric+labels regardless of label order\n- Label validation rejects malformed keys/values and enforces cardinality limits  \n- Metric validation catches type mismatches and constraint violations\n- JSON serialization round-trips preserve all data with proper timestamp formatting\n- Copy operations create true deep copies that don't share memory references\n\nSigns of implementation issues:\n- **Series ID inconsistency**: Same metric produces different IDs → check label sorting in SeriesID()\n- **Validation bypass**: Invalid data passes validation → tighten constraint checking in Validate() methods\n- **Memory leaks**: Label copying shares references → ensure Copy() creates independent map instances\n- **Timestamp precision loss**: Nanosecond precision not preserved → use RFC3339Nano format in JSON serialization\n\n\n## Metrics Ingestion Engine\n\n> **Milestone(s):** 1 (Metrics Collection) - This section details the first major component that handles incoming metrics data with both push and pull collection models\n\n### Mental Model: The Data Processing Factory\n\nThink of the metrics ingestion engine as a sophisticated **data processing factory** with multiple assembly lines working in parallel. Just like a modern manufacturing plant, our factory has several key characteristics:\n\n**Receiving Docks**: The factory has two types of receiving areas. The **push dock** is like a loading bay where delivery trucks (applications) arrive at any time to drop off shipments of raw materials (metrics data). The **pull dock** operates more like a scheduled pickup service where factory workers (scrapers) drive out on regular routes to collect materials from various suppliers (application endpoints) according to a predetermined schedule.\n\n**Quality Control Stations**: Before any raw materials enter the main production line, they pass through rigorous **quality control checkpoints**. Inspectors (validators) examine each shipment to ensure it meets specifications - checking that labels are properly formatted, metric types are valid, and that we're not receiving so many unique product variations that our storage systems would overflow. Materials that don't pass inspection are either rejected with detailed error reports or sent to a preprocessing station for standardization.\n\n**Assembly Line Processing**: Once materials pass quality control, they enter the main **assembly line** where workers (processors) perform standardization operations. Counter materials get special handling to ensure they're always increasing, gauge materials are processed to capture point-in-time snapshots, and histogram materials are sorted into predefined buckets. Each processed item gets a unique identifier (series ID) based on its name and characteristics.\n\n**Dispatch Center**: Finally, processed materials are packaged and sent to the **warehouse** (storage engine) with proper routing information. The dispatch center maintains strict ordering and batching to ensure the warehouse can efficiently store everything without becoming overwhelmed.\n\nThis factory operates 24/7 with **built-in resilience mechanisms**. If one assembly line gets backed up, work can be redistributed. If quality control detects a problematic supplier, it can temporarily reject their materials while alerting operations staff. The entire system is designed to handle sudden spikes in incoming materials while maintaining consistent quality and throughput.\n\n### Push vs Pull Ingestion Models\n\nThe choice between push and pull ingestion models represents one of the most fundamental architectural decisions in metrics collection systems. Each model has distinct advantages and addresses different operational patterns, making it essential to support both approaches in a comprehensive metrics platform.\n\n![Metric Ingestion Data Flow](./diagrams/ingestion-flow.svg)\n\n> **Decision: Support Both Push and Pull Ingestion Models**\n> - **Context**: Applications have different operational patterns - some generate metrics continuously and prefer to send data immediately, while others benefit from having a central system collect data on a schedule. Additionally, different deployment environments favor different approaches.\n> - **Options Considered**: Push-only (applications send data), Pull-only (server scrapes data), Hybrid approach (support both)\n> - **Decision**: Implement hybrid approach supporting both push and pull models\n> - **Rationale**: Push provides immediate data delivery and works well with ephemeral workloads, while pull provides centralized control and works better with service discovery. Supporting both maximizes compatibility and deployment flexibility.\n> - **Consequences**: Increased implementation complexity but significantly broader applicability across different environments and use cases.\n\n#### Push-Based Ingestion Architecture\n\nIn the **push model**, applications take responsibility for actively sending their metrics data to the ingestion engine. This approach treats the metrics system as a service that applications can call whenever they have data to report. The ingestion engine operates as a passive receiver, exposing HTTP endpoints that accept metric payloads and respond with success or error indicators.\n\nThe push model excels in environments with **dynamic or ephemeral workloads**. Consider a serverless function that runs for only a few seconds - it can immediately push its execution metrics before terminating, ensuring no data is lost. Similarly, applications running in containers that scale up and down rapidly can send metrics as soon as they're generated, without requiring the metrics system to maintain awareness of their existence.\n\nPush-based ingestion also provides **immediate data availability**. The moment an application generates a critical metric - such as an error rate spike or a performance degradation - it can send that data directly to the ingestion engine. This immediacy is crucial for time-sensitive alerting scenarios where even a few seconds of delay could impact response times.\n\nHowever, the push model places **operational burden on applications**. Each application must implement metrics client libraries, handle network failures, implement retry logic, and manage authentication credentials for the metrics service. Applications must also be configured with the metrics service endpoint, creating operational dependencies that can complicate deployments.\n\n#### Pull-Based Ingestion Architecture\n\nThe **pull model** inverts the responsibility relationship, making the ingestion engine an active collector that periodically scrapes metrics from application endpoints. Applications expose their current metrics via HTTP endpoints (typically `/metrics`) in a standardized format, and the ingestion engine maintains a schedule of which endpoints to scrape and how frequently.\n\nThis approach provides **centralized operational control**. The metrics system maintains the master list of what to monitor, how frequently to collect data, and how to authenticate with each endpoint. Operations teams can add new applications to monitoring, adjust collection frequencies, or temporarily disable problematic endpoints without requiring application changes or redeployments.\n\nPull-based collection also enables **sophisticated service discovery integration**. The ingestion engine can automatically discover new application instances through service registries, cloud provider APIs, or orchestration platforms like Kubernetes. As new instances start up, they automatically become part of the monitoring system without manual configuration.\n\nThe pull model provides **built-in health checking capabilities**. If an application becomes unresponsive, the scraping process will detect this condition through failed HTTP requests, enabling the metrics system to generate alerts about application availability. This creates a natural integration between metrics collection and basic health monitoring.\n\nHowever, pull-based collection has **timing limitations**. Metrics are only collected at predetermined intervals, meaning short-lived events or rapid changes might be missed between scrapes. Additionally, applications behind firewalls or NAT may not be reachable by external scrapers, limiting deployment options.\n\n#### Implementation Strategy Comparison\n\n| Aspect | Push Model | Pull Model | Hybrid Approach |\n|--------|------------|------------|-----------------|\n| **Data Freshness** | Immediate (sub-second) | Delayed (scrape interval) | Best of both worlds |\n| **Operational Complexity** | Distributed (each app) | Centralized (metrics system) | Higher initial complexity |\n| **Service Discovery** | Manual configuration | Automatic discovery | Both manual and automatic |\n| **Network Requirements** | Outbound from apps | Inbound to apps | Both directions |\n| **Ephemeral Workloads** | Excellent support | Poor support | Excellent support |\n| **Debugging Visibility** | Limited (app-side logs) | Excellent (central logs) | Comprehensive visibility |\n| **Authentication** | Per-app credentials | Central credential management | Flexible auth strategies |\n\n#### Hybrid Implementation Design\n\nOur metrics ingestion engine implements both models through a **unified processing pipeline** that normalizes data from both sources before validation and storage. This design ensures consistent behavior regardless of how metrics arrive in the system.\n\nThe **HTTP API layer** provides RESTful endpoints for push-based submissions while also hosting the scraping orchestrator that manages pull-based collection. Both pathways converge into the same validation and preprocessing pipeline, ensuring data consistency and uniform error handling.\n\n**Configuration management** allows operators to define both push endpoints (for applications to submit data) and pull targets (for the system to scrape) in a single configuration file. This unified approach simplifies operational procedures and provides a single place to manage all metric collection policies.\n\nThe system maintains **separate health monitoring** for push and pull pathways. Push endpoints are monitored for request rates, error rates, and processing latency. Pull operations are monitored for scrape success rates, target reachability, and scrape duration. This comprehensive monitoring ensures operators can quickly identify issues with either collection method.\n\n### Validation and Preprocessing\n\nThe validation and preprocessing stage serves as the **quality gateway** between raw metric submissions and the storage engine. This component must handle the inherent messiness of real-world data while protecting the system from malicious or malformed inputs that could compromise performance or reliability.\n\n#### Label Cardinality Control\n\n**Label cardinality explosion** represents one of the most dangerous failure modes in metrics systems. Cardinality refers to the number of unique combinations of label key-value pairs across all metrics. When applications generate labels with unbounded or high-variability values - such as user IDs, request IDs, or timestamps - the number of unique time series grows exponentially.\n\nConsider an application that creates a metric `http_requests_total` with labels for `method`, `status_code`, and `user_id`. If the application serves 1 million users, this single metric type generates 1 million unique time series (assuming one method and status code combination per user). With multiple methods and status codes, the cardinality explodes to millions or tens of millions of series.\n\n> **Critical Insight**: Each unique time series consumes memory for indexing and storage space for samples. Cardinality explosions can quickly exhaust system resources, causing the entire metrics platform to become unresponsive or crash.\n\nThe ingestion engine implements **multi-layered cardinality protection**:\n\n**Static Label Validation** examines each metric submission to ensure labels conform to naming conventions and value constraints. Label names must follow DNS-like naming rules (alphanumeric characters and underscores only), and label values are checked against length limits and character restrictions.\n\n**Dynamic Cardinality Tracking** maintains counters for unique time series being created in real-time. The system tracks cardinality per metric name, per application source, and globally. When any of these counters approaches configured thresholds, the system begins rejecting new series creation while continuing to accept samples for existing series.\n\n**High-Cardinality Detection** uses statistical analysis to identify potentially problematic label patterns. If a metric name suddenly generates many new series in a short time period, or if specific label keys show high variability, the system flags these patterns for operator review and can automatically implement temporary restrictions.\n\n#### Metric Type Validation\n\nEach metric type has specific **semantic requirements** that must be enforced during ingestion to ensure data consistency and query correctness. The validation logic differs significantly across the three primary metric types.\n\n**Counter Validation** ensures that counter metrics behave as monotonically increasing cumulative values. The ingestion engine tracks the last seen value for each counter series and rejects samples with values lower than the previous sample (which would indicate incorrect counter usage). When counter resets occur (such as application restarts), the system detects these by identifying large decreases in value and handles them appropriately by recording reset events.\n\n**Gauge Validation** is more permissive since gauges represent point-in-time measurements that can increase or decrease freely. However, the system still validates that gauge values fall within reasonable numeric ranges and aren't special values like NaN or infinity unless explicitly configured to allow them.\n\n**Histogram Validation** requires the most complex logic since histograms consist of multiple related series (buckets, count, and sum). The ingestion engine validates that bucket values are monotonically increasing across bucket boundaries, that the count matches the sum of all bucket increments, and that the sum value is mathematically consistent with the observed count and bucket distributions.\n\n| Metric Type | Key Validations | Failure Actions |\n|-------------|----------------|-----------------|\n| **Counter** | Monotonic increase, numeric range, reset detection | Reject decreasing values, log reset events |\n| **Gauge** | Numeric range, finite values | Reject invalid numbers, allow all valid changes |\n| **Histogram** | Bucket consistency, count/sum coherence, bucket ordering | Reject inconsistent histograms, validate all components |\n\n#### Data Normalization\n\nRaw metric data arrives in various formats and with inconsistent naming patterns. The **data normalization process** standardizes this data into canonical formats that the storage engine can efficiently process.\n\n**Metric Name Normalization** converts metric names to standard formats, typically lowercase with underscores separating words. The system may also apply prefix rules (such as adding application or environment prefixes) and suffix standardization (ensuring counter metrics end with `_total` or similar conventions).\n\n**Label Standardization** includes sorting label keys alphabetically, normalizing label values (such as converting HTTP status codes to strings or normalizing case), and applying label transformation rules. The system may also inject standard labels such as application name, environment, or collection timestamp.\n\n**Timestamp Processing** handles the complexities of time-based data. Samples may arrive with timestamps in various formats (Unix seconds, milliseconds, nanoseconds, or ISO 8601 strings), in different time zones, or without timestamps at all. The normalization process converts all timestamps to a canonical format (typically Unix nanoseconds in UTC) and may apply clock skew detection to identify submissions with timestamps significantly different from the ingestion time.\n\n**Value Normalization** ensures numeric consistency across different input formats. This includes handling scientific notation, different decimal precision levels, and unit conversions where configured (such as converting milliseconds to seconds for duration metrics).\n\n#### Error Handling and Feedback\n\nThe validation and preprocessing pipeline must provide **detailed error feedback** while maintaining high throughput performance. This requires careful balance between comprehensive validation and processing speed.\n\n**Validation Error Classification** categorizes errors into different severity levels:\n- **Fatal errors** (malformed JSON, missing required fields) that cause entire batch rejection\n- **Warning errors** (unusual cardinality patterns, timestamp skew) that allow processing but generate alerts  \n- **Individual metric errors** (single invalid metric in a batch) that reject specific metrics while processing others\n\n**Error Response Design** provides structured feedback that applications can use to fix submission problems:\n\n| Error Type | HTTP Status | Response Format | Retry Recommendation |\n|------------|-------------|-----------------|----------------------|\n| **Malformed Request** | 400 Bad Request | JSON with field-specific errors | Fix format, retry immediately |\n| **Cardinality Limit** | 429 Too Many Requests | JSON with cardinality details | Reduce cardinality, retry with backoff |\n| **Authentication** | 401 Unauthorized | JSON with auth requirements | Fix credentials, retry immediately |\n| **Rate Limiting** | 429 Too Many Requests | JSON with retry-after header | Respect rate limits, retry after delay |\n| **Server Error** | 500 Internal Server Error | JSON with error tracking ID | Retry with exponential backoff |\n\n**Batch Processing Logic** handles mixed-success scenarios where some metrics in a batch are valid while others fail validation. The system processes all valid metrics and returns detailed error information for failed metrics, allowing applications to resubmit only the corrected data.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Ignoring Label Cardinality Limits**\nMany developers create metrics with labels containing user IDs, request IDs, or other high-cardinality values without understanding the exponential impact on system resources. A single metric with a user ID label in a system with millions of users creates millions of unique time series.\n**Why it's wrong**: Each unique time series consumes memory for indexing and storage space for data points. High cardinality can quickly exhaust system memory and make queries extremely slow.\n**How to fix**: Implement strict cardinality limits per metric name (typically 1000-10000 series). Use high-cardinality values in logs, not metrics. Create aggregate metrics instead of per-entity metrics.\n\n⚠️ **Pitfall: Not Handling Clock Skew in Distributed Systems**\nWhen multiple applications submit metrics with their local timestamps, clock differences between systems can cause samples to arrive with timestamps in the future or significantly in the past, breaking time-series ordering assumptions.\n**Why it's wrong**: Storage engines often assume samples arrive in roughly chronological order. Out-of-order samples can cause index corruption or query inconsistencies.\n**How to fix**: Detect timestamp skew during ingestion (samples more than 5 minutes in future or 1 hour in past). Either reject skewed samples or use server-side timestamps for all ingestion.\n\n⚠️ **Pitfall: Insufficient Validation of Counter Decreases**\nAllowing counter metrics to decrease without proper handling breaks the fundamental counter semantic and makes rate calculations incorrect.\n**Why it's wrong**: Counters represent cumulative values that should only increase. Decreases typically indicate counter resets (like application restarts) that need special handling for accurate rate calculations.\n**How to fix**: Track previous counter values per series. Reject samples with decreased values unless they represent valid resets (detected by large decreases). Log reset events for debugging.\n\n⚠️ **Pitfall: Blocking Ingestion During Validation**\nPerforming expensive validation operations (like cardinality lookups or histogram consistency checks) in the main ingestion request path creates latency spikes and reduces throughput.\n**Why it's wrong**: Applications submitting metrics expect fast response times. Slow validation creates backpressure that can affect application performance.\n**How to fix**: Use asynchronous validation where possible. Perform expensive checks in background workers. Cache validation results to avoid repeated expensive operations.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **HTTP Server** | Go net/http with middleware | Fiber or Echo framework |\n| **Metric Parsing** | encoding/json for JSON format | Prometheus text format parser |\n| **Cardinality Tracking** | In-memory maps with sync.RWMutex | Redis or embedded key-value store |\n| **Validation Rules** | Hard-coded validation functions | Rule engine with YAML configuration |\n| **Background Processing** | Goroutines with channels | Worker pool with job queue |\n| **Health Checking** | Simple HTTP endpoint | Comprehensive health check framework |\n\n#### Recommended File Structure\n\n```\ninternal/ingestion/\n  ingester.go              ← Main MetricsIngester implementation\n  ingester_test.go         ← Unit tests for ingestion logic\n  validator.go             ← Validation and preprocessing logic\n  validator_test.go        ← Validation tests\n  scraper.go               ← Pull-based metric collection\n  scraper_test.go          ← Scraper tests\n  handlers/\n    push_handler.go        ← HTTP handlers for push ingestion\n    health_handler.go      ← Health check endpoints\n    scrape_handler.go      ← Scrape target management\n  models/\n    metric.go              ← Core metric data structures\n    validation.go          ← Validation error types\n  config/\n    ingestion_config.go    ← Configuration structures\n```\n\n#### Infrastructure Starter Code\n\n**HTTP Server Infrastructure** (complete implementation):\n\n```go\npackage ingestion\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"log/slog\"\n    \"net/http\"\n    \"time\"\n)\n\n// Server wraps HTTP server with graceful shutdown and middleware\ntype Server struct {\n    httpServer *http.Server\n    logger     *slog.Logger\n    ingester   MetricsIngester\n}\n\n// NewServer creates HTTP server with configured routes and middleware\nfunc NewServer(addr string, ingester MetricsIngester, logger *slog.Logger) *Server {\n    mux := http.NewServeMux()\n    \n    server := &Server{\n        httpServer: &http.Server{\n            Addr:         addr,\n            Handler:      mux,\n            ReadTimeout:  30 * time.Second,\n            WriteTimeout: 30 * time.Second,\n        },\n        logger:   logger,\n        ingester: ingester,\n    }\n    \n    server.registerRoutes(mux)\n    return server\n}\n\nfunc (s *Server) registerRoutes(mux *http.ServeMux) {\n    // Push ingestion endpoint\n    mux.HandleFunc(\"/api/v1/metrics\", s.handlePushMetrics)\n    // Health check\n    mux.HandleFunc(\"/health\", s.handleHealth)\n    // Scrape target management\n    mux.HandleFunc(\"/api/v1/scrape\", s.handleScrapeConfig)\n}\n\nfunc (s *Server) Start() error {\n    s.logger.Info(\"starting ingestion server\", \"addr\", s.httpServer.Addr)\n    return s.httpServer.ListenAndServe()\n}\n\nfunc (s *Server) Shutdown(ctx context.Context) error {\n    s.logger.Info(\"shutting down ingestion server\")\n    return s.httpServer.Shutdown(ctx)\n}\n```\n\n**Validation Error Infrastructure** (complete implementation):\n\n```go\npackage models\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"strings\"\n)\n\n// ValidationError represents field-specific validation failures\ntype ValidationError struct {\n    Field   string `json:\"field\"`\n    Value   string `json:\"value\"`\n    Message string `json:\"message\"`\n}\n\nfunc (v ValidationError) Error() string {\n    return fmt.Sprintf(\"validation failed for field '%s' with value '%s': %s\",\n        v.Field, v.Value, v.Message)\n}\n\n// ValidationErrors represents multiple validation failures\ntype ValidationErrors struct {\n    Errors []ValidationError `json:\"errors\"`\n}\n\nfunc (v ValidationErrors) Error() string {\n    var messages []string\n    for _, err := range v.Errors {\n        messages = append(messages, err.Error())\n    }\n    return strings.Join(messages, \"; \")\n}\n\nfunc (v ValidationErrors) HasErrors() bool {\n    return len(v.Errors) > 0\n}\n\nfunc (v *ValidationErrors) Add(field, value, message string) {\n    v.Errors = append(v.Errors, ValidationError{\n        Field:   field,\n        Value:   value,\n        Message: message,\n    })\n}\n\n// ToJSON serializes validation errors for HTTP responses\nfunc (v ValidationErrors) ToJSON() ([]byte, error) {\n    return json.Marshal(v)\n}\n```\n\n#### Core Logic Skeleton Code\n\n**Main Ingestion Interface** (signatures + TODOs):\n\n```go\npackage ingestion\n\nimport (\n    \"context\"\n    \"time\"\n    \n    \"internal/models\"\n)\n\n// MetricsIngester handles both push and pull metric collection\ntype MetricsIngester interface {\n    IngestMetrics(ctx context.Context, metrics []models.Metric) error\n    ScrapeEndpoint(ctx context.Context, url string) error\n    GetStats() IngestionStats\n}\n\n// IngestionStats provides operational metrics about the ingestion process\ntype IngestionStats struct {\n    MetricsReceived    int64\n    MetricsRejected    int64\n    ValidationErrors   int64\n    CardinalityViolations int64\n    ScrapeTargets     int\n    LastScrapeTime    time.Time\n}\n\n// Implementation skeleton\ntype ingesterImpl struct {\n    validator      Validator\n    storage        TimeSeriesStorage\n    cardinalityMgr CardinalityManager\n    logger         *slog.Logger\n    stats          IngestionStats\n}\n\n// IngestMetrics processes a batch of metrics through validation and storage\nfunc (i *ingesterImpl) IngestMetrics(ctx context.Context, metrics []models.Metric) error {\n    // TODO 1: Initialize validation errors collector\n    // TODO 2: For each metric in the batch:\n    //   - Validate metric name and type using i.validator.ValidateMetric()\n    //   - Check cardinality limits using i.cardinalityMgr.CheckCardinality()\n    //   - Normalize labels and values using i.validator.NormalizeMetric()\n    //   - Add to validated batch if all checks pass\n    // TODO 3: If any metrics passed validation:\n    //   - Convert to storage format using models.MetricToSamples()\n    //   - Call i.storage.WriteSamples() with validated metrics\n    //   - Update ingestion statistics\n    // TODO 4: Return validation errors if any metrics failed\n    // Hint: Use sync.Mutex to protect stats updates in concurrent scenarios\n}\n```\n\n**Validation Logic** (signatures + TODOs):\n\n```go\npackage ingestion\n\nimport (\n    \"internal/models\"\n    \"regexp\"\n    \"strings\"\n)\n\n// Validator handles metric validation and normalization\ntype Validator interface {\n    ValidateMetric(metric *models.Metric) *models.ValidationErrors\n    NormalizeMetric(metric *models.Metric) error\n}\n\ntype validatorImpl struct {\n    metricNameRegex *regexp.Regexp\n    labelNameRegex  *regexp.Regexp\n    maxLabelLength  int\n    maxValueLength  int\n}\n\n// ValidateMetric checks metric name, type, labels, and values\nfunc (v *validatorImpl) ValidateMetric(metric *models.Metric) *models.ValidationErrors {\n    errors := &models.ValidationErrors{}\n    \n    // TODO 1: Validate metric name:\n    //   - Check if name matches regex pattern (alphanumeric + underscore)\n    //   - Verify name length is within limits (1-200 characters)\n    //   - Ensure name doesn't start with double underscore (reserved)\n    // TODO 2: Validate metric type:\n    //   - Check if Type is one of Counter, Gauge, Histogram\n    //   - For Counter: verify all sample values are >= 0\n    //   - For Histogram: validate bucket structure and consistency\n    // TODO 3: Validate labels:\n    //   - Check each label name matches regex pattern\n    //   - Verify label names don't start with double underscore\n    //   - Validate label values are within length limits\n    //   - Check for reserved label names (__name__, job, instance)\n    // TODO 4: Validate samples:\n    //   - Ensure timestamps are within acceptable range (not too far in past/future)\n    //   - Check that sample values are finite numbers\n    //   - For counters, verify values don't decrease (unless reset detected)\n    // Hint: Use errors.Add() to collect multiple validation failures\n    \n    return errors\n}\n```\n\n**Cardinality Management** (signatures + TODOs):\n\n```go\npackage ingestion\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \n    \"internal/models\"\n)\n\n// CardinalityManager tracks and limits time-series cardinality\ntype CardinalityManager interface {\n    CheckCardinality(metric *models.Metric) error\n    GetCardinalityStats() CardinalityStats\n    RegisterSeries(seriesID string) error\n}\n\ntype CardinalityStats struct {\n    TotalSeries     int\n    SeriesPerMetric map[string]int\n    MaxCardinality  int\n}\n\ntype cardinalityManagerImpl struct {\n    mu               sync.RWMutex\n    seriesIndex      map[string]bool  // seriesID -> exists\n    metricCardinality map[string]int   // metricName -> count\n    maxSeriesTotal   int\n    maxSeriesPerMetric int\n}\n\n// CheckCardinality verifies adding this metric won't exceed cardinality limits\nfunc (c *cardinalityManagerImpl) CheckCardinality(metric *models.Metric) error {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    // TODO 1: Generate series ID using metric.SeriesID() method\n    // TODO 2: Check if this series already exists in c.seriesIndex\n    //   - If exists, return nil (no new cardinality impact)\n    // TODO 3: Check global cardinality limit:\n    //   - If len(c.seriesIndex) >= c.maxSeriesTotal, return cardinality error\n    // TODO 4: Check per-metric cardinality limit:\n    //   - If c.metricCardinality[metric.Name] >= c.maxSeriesPerMetric, return error\n    // TODO 5: Temporarily register the series (actual registration happens after successful ingestion)\n    // Hint: Use metric name + sorted labels to create consistent series ID\n    \n    return nil\n}\n```\n\n#### Language-Specific Hints\n\n**Go-Specific Implementation Details**:\n\n- Use `sync.RWMutex` for cardinality tracking to allow concurrent read access while protecting writes\n- Implement `http.HandlerFunc` for push endpoints with proper JSON parsing using `json.NewDecoder(r.Body)`\n- Use `time.NewTicker` for scrape scheduling with proper cleanup in goroutines\n- Handle context cancellation in long-running operations using `select` with `ctx.Done()`\n- Use `slog.Logger` for structured logging with contextual fields\n- Implement graceful shutdown by listening for OS signals with `signal.Notify`\n\n**Error Handling Patterns**:\n```go\n// Wrap validation errors with context\nif err := validator.ValidateMetric(metric); err != nil {\n    return fmt.Errorf(\"validation failed for metric %s: %w\", metric.Name, err)\n}\n\n// Use typed errors for different failure modes\nvar ErrCardinalityExceeded = errors.New(\"cardinality limit exceeded\")\n```\n\n**Concurrency Patterns**:\n```go\n// Worker pool for batch processing\nfor i := 0; i < numWorkers; i++ {\n    go func() {\n        for batch := range workChan {\n            if err := processBatch(batch); err != nil {\n                errorChan <- err\n            }\n        }\n    }()\n}\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the ingestion engine, verify the following behaviors:\n\n**Push Ingestion Test**:\n```bash\n# Start the server\ngo run cmd/server/main.go\n\n# Submit test metrics via HTTP\ncurl -X POST http://localhost:8080/api/v1/metrics \\\n  -H \"Content-Type: application/json\" \\\n  -d '[{\n    \"name\": \"test_counter_total\",\n    \"type\": 0,\n    \"labels\": {\"job\": \"test\", \"instance\": \"localhost:8080\"},\n    \"samples\": [{\"value\": 42.0, \"timestamp\": \"'$(date -u +%s)'\"}]\n  }]'\n\n# Expected: HTTP 200 response with empty JSON body {}\n# Expected logs: \"ingested 1 metrics successfully\"\n```\n\n**Validation Error Test**:\n```bash\n# Submit invalid metric\ncurl -X POST http://localhost:8080/api/v1/metrics \\\n  -H \"Content-Type: application/json\" \\\n  -d '[{\n    \"name\": \"\",\n    \"type\": 999,\n    \"labels\": {},\n    \"samples\": []\n  }]'\n\n# Expected: HTTP 400 response with validation errors JSON\n# Expected response body contains field-specific error details\n```\n\n**Cardinality Limit Test**:\n```bash\n# Configure low cardinality limit in config\n# Submit metrics with many unique label combinations\n# Expected: HTTP 429 after hitting cardinality limit\n# Expected logs: \"cardinality limit exceeded for metric X\"\n```\n\n**Health Check Verification**:\n```bash\ncurl http://localhost:8080/health\n# Expected: HTTP 200 with {\"status\": \"healthy\", \"timestamp\": \"...\"}\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| **Metrics not appearing in storage** | Validation failures silently dropping data | Check ingestion logs for validation errors | Add comprehensive logging to validation pipeline |\n| **High memory usage** | Cardinality explosion from unbounded labels | Monitor cardinality statistics endpoint | Implement stricter label validation and limits |\n| **Slow ingestion performance** | Synchronous validation blocking request processing | Profile CPU usage during ingestion | Move expensive validation to background workers |\n| **Inconsistent timestamps** | Clock skew between client and server | Compare ingestion timestamps with sample timestamps | Implement timestamp normalization and skew detection |\n| **Counter values decreasing** | Application restarts not handled properly | Track counter resets in logs | Implement reset detection and handle counter resets gracefully |\n\n\n## Time-Series Storage Engine\n\n> **Milestone(s):** 2 (Storage & Querying) - This section details the storage layer that persists metrics data and enables efficient querying for both dashboards and alerting systems\n\n### Mental Model: The Library Archive System\n\nThink of our time-series storage engine as a vast, highly organized library archive system that specializes in chronological documents. Just as a library must efficiently store millions of books, organize them for quick retrieval, and manage space by moving older materials to different storage areas, our storage engine must handle millions of metric samples with similar organizational challenges.\n\nIn this library analogy, each **time-series** is like a specific periodical or journal - say \"The Daily Weather Reports for New York City\" or \"Monthly Server CPU Usage for Database-01\". Each **sample** within a time-series is like an individual issue of that periodical, with a specific date (timestamp) and content (the metric value). The **labels** on a metric are like the catalog classification system - they help us group related periodicals together and find exactly what we're looking for among millions of possibilities.\n\nThe library's **card catalog system** represents our indexing structure. Just as you can look up books by author, title, subject, or publication date, we need indexes that let us quickly find time-series by metric name, label combinations, and time ranges. Without proper indexing, finding specific data would require scanning through every stored sample - like searching for a specific magazine issue by checking every shelf in the building.\n\nThe library's **archival policies** mirror our compaction and retention strategies. Recent magazines are kept in the main reading room for immediate access (high-resolution recent data), older issues are moved to compact storage with lower accessibility (downsampled historical data), and very old materials are eventually discarded according to institutional policies (retention limits). This tiered storage approach balances accessibility, storage costs, and retrieval performance.\n\nJust as librarians periodically reorganize sections, merge duplicate materials, and transfer items between storage areas, our storage engine continuously runs background processes to compact data files, merge small storage blocks into larger ones, and apply retention policies. This maintenance work happens transparently while the library remains open for readers - our storage engine must serve queries and accept new data even while reorganizing itself.\n\n### Storage Format and Indexing\n\nThe foundation of our time-series storage system rests on a **block-based architecture** where metric samples are organized into immutable storage blocks on disk. This design choice provides several critical advantages: it enables efficient compression within each block, simplifies concurrent access patterns, and makes background compaction operations straightforward to implement safely.\n\n![Storage Engine Architecture](./diagrams/storage-architecture.svg)\n\n**Storage Block Structure**\n\nEach storage block contains samples for multiple time-series within a specific time window, typically spanning 2-4 hours of data. Within a block, samples are stored in a columnar format that maximizes compression efficiency. The timestamp column uses delta encoding (storing differences between consecutive timestamps rather than absolute values), while the value column applies appropriate compression based on the data patterns detected during ingestion.\n\n| Block Component | Data Structure | Purpose |\n|-----------------|----------------|---------|\n| Block Header | BlockID, TimeRange, SeriesCount, CompressionType | Metadata for quick block filtering and loading |\n| Series Index | SeriesID → ValueOffset mapping | Enables direct jump to specific time-series data within block |\n| Timestamp Column | Delta-encoded timestamp array | Compressed temporal data shared across all series in block |\n| Value Columns | Series-specific compressed value arrays | Actual metric values with optimal compression per series type |\n| Block Footer | CRC checksum, Block size | Data integrity verification and corruption detection |\n\nThe **series ID** serves as the primary key that uniquely identifies each time-series. It's computed as a hash of the metric name combined with the lexicographically sorted label pairs. For example, the series ID for `http_requests_total{method=\"GET\", status=\"200\"}` would be `hash(\"http_requests_total\" + \"method=GET,status=200\")`. This deterministic approach ensures that samples belonging to the same logical time-series always map to the same series ID, regardless of the order in which labels appear in incoming metrics.\n\n> **Decision: Block-Based vs. Row-Based Storage**\n> - **Context**: Need to choose fundamental storage layout for time-series samples\n> - **Options Considered**: \n>   - Row-based: Each sample stored as complete record with all fields\n>   - Block-based: Samples grouped into columnar blocks by time window\n>   - Hybrid: Combination approach with recent data row-based, historical data block-based\n> - **Decision**: Block-based columnar storage with 2-4 hour time windows\n> - **Rationale**: Time-series data exhibits excellent compression characteristics when organized columnar, query patterns typically involve time ranges rather than individual samples, and block immutability simplifies concurrent access and compaction\n> - **Consequences**: Enables 5-10x compression ratios, optimizes for range queries, but requires more complex indexing for point lookups\n\n**Indexing Architecture**\n\nThe storage engine maintains multiple index structures to support different query patterns efficiently. The **primary index** maps series IDs to the list of blocks containing data for that time-series. This index is kept entirely in memory for fast lookups and is persisted to disk for crash recovery. Each index entry contains the series metadata (metric name and labels) plus a sorted list of block references with their time ranges.\n\n| Index Type | Structure | Query Pattern Supported |\n|------------|-----------|------------------------|\n| Primary Index | SeriesID → BlockList mapping | Exact series lookup by metric name + labels |\n| Label Index | LabelName → LabelValue → SeriesID list | Label-based filtering and series discovery |\n| Time Index | TimeRange → BlockID list | Time-range queries and block pruning |\n| Inverted Index | LabelValue → SeriesID list | Cross-metric label searches |\n\nThe **label index** enables efficient filtering by label values without scanning all series. For each unique label name that appears in the dataset, we maintain a secondary index mapping each possible value to the list of series IDs that contain that label-value pair. This structure allows queries like \"find all series with `status='500'`\" to execute in logarithmic time rather than requiring a full series scan.\n\n**Compression Strategies**\n\nDifferent metric types exhibit distinct value patterns that benefit from specialized compression approaches. **Counter metrics** typically show monotonically increasing values, making them excellent candidates for delta-of-delta encoding where we store the difference between consecutive delta values. **Gauge metrics** often have values clustered around certain ranges, benefiting from dictionary compression that maps frequent values to short codes.\n\n| Metric Type | Compression Algorithm | Typical Compression Ratio | Rationale |\n|-------------|----------------------|--------------------------|-----------|\n| Counter | Delta-of-delta encoding | 8:1 to 12:1 | Values increase monotonically with predictable rates |\n| Gauge | Dictionary + RLE compression | 4:1 to 8:1 | Values cluster around common ranges with repetition |\n| Histogram | Mixed: delta encoding for timestamps, dictionary for bucket counts | 6:1 to 10:1 | Bucket structure provides natural compression opportunities |\n\n**File Organization**\n\nStorage blocks are organized into a hierarchical directory structure that facilitates efficient querying and maintenance operations. The root storage directory contains subdirectories organized by day, with each day containing hour-based subdirectories that hold the actual block files. This organization allows time-based query optimizations and simplifies retention policy implementation.\n\n```\ndata/\n├── 2024-01-15/\n│   ├── 00/  (midnight hour)\n│   │   ├── block_001.db\n│   │   ├── block_002.db\n│   │   └── index.db\n│   ├── 01/  (1 AM hour)\n│   │   ├── block_003.db\n│   │   └── index.db\n│   └── ...\n├── 2024-01-16/\n│   └── ...\n└── meta/\n    ├── series_index.db\n    └── label_index.db\n```\n\nThis hierarchical approach provides several operational benefits. Time-range queries can immediately eliminate entire directories from consideration, reducing I/O overhead. Retention policies can delete entire day directories atomically. Background maintenance tasks can process data in convenient time-based chunks without interfering with current ingestion.\n\n### Compaction and Retention Policies\n\nThe storage engine's longevity and performance depend critically on automated data lifecycle management. Without compaction, the system would accumulate thousands of small storage blocks that degrade query performance and waste disk space. Without retention policies, historical data would consume unbounded storage and eventually exhaust available capacity.\n\n**Compaction Strategy**\n\nOur compaction system operates on multiple levels, similar to LSM-tree approaches but adapted for time-series workloads. **Level 0 compaction** runs continuously, merging small blocks created during active ingestion into larger, more efficiently compressed blocks. **Level 1 compaction** runs hourly, reorganizing data across longer time windows to optimize for common query patterns. **Level 2 compaction** runs daily, performing deep reorganization and applying downsampling policies to historical data.\n\n| Compaction Level | Trigger | Input Blocks | Output Blocks | Primary Goal |\n|------------------|---------|--------------|---------------|--------------|\n| Level 0 | Every 15 minutes | 5-10 small ingestion blocks | 1 medium block | Reduce file count, improve compression |\n| Level 1 | Hourly | All blocks in hour window | 1-2 optimized blocks | Optimize query performance, remove duplicates |\n| Level 2 | Daily | Day's worth of historical blocks | Downsampled summary blocks | Apply retention policies, reduce storage footprint |\n\nThe compaction process must handle several complex scenarios. **Overlapping time ranges** occur when blocks contain data from slightly different time windows - the compaction algorithm merges samples in timestamp order while detecting and removing duplicates. **Schema evolution** happens when new labels appear in the dataset - the compaction process updates index structures to accommodate new label combinations without breaking existing queries.\n\n**Downsampling Algorithms**\n\nAs data ages, maintaining full-resolution samples becomes increasingly expensive while providing diminishing value for most use cases. Our downsampling system applies configurable policies that reduce data resolution while preserving the statistical properties most important for analysis and alerting.\n\n> **Decision: Fixed vs. Adaptive Downsampling Windows**\n> - **Context**: Need to balance storage efficiency with data fidelity for historical metrics\n> - **Options Considered**:\n>   - Fixed windows: Predetermined downsampling intervals (1h → 5m, 1d → 1h, 1w → 6h)\n>   - Adaptive windows: Dynamic intervals based on data volatility and query patterns  \n>   - Query-driven: Downsample only when storage pressure or query performance degrades\n> - **Decision**: Fixed windows with configurable thresholds\n> - **Rationale**: Predictable storage growth, simplified implementation, deterministic behavior for alerting rules that depend on historical data\n> - **Consequences**: May oversample stable metrics or undersample volatile ones, but provides consistent query performance and storage characteristics\n\nThe downsampling process preserves different statistical properties depending on the metric type. For **counter metrics**, we maintain the rate-of-change information by storing periodic snapshots along with minimum and maximum observed rates within each downsampling window. For **gauge metrics**, we preserve the average, minimum, maximum, and last observed value within each window. For **histogram metrics**, we merge bucket counts and recalculate percentiles at the reduced resolution.\n\n| Downsampling Window | Original Resolution | Target Resolution | Statistical Preservation |\n|--------------------|-------------------|------------------|------------------------|\n| 1 hour - 24 hours | 15 second intervals | 1 minute intervals | Full statistical detail maintained |\n| 1 day - 7 days | 1 minute intervals | 5 minute intervals | Min/max/avg/last preserved per window |\n| 1 week - 30 days | 5 minute intervals | 30 minute intervals | Statistical summary with confidence intervals |\n| 30+ days | 30 minute intervals | 2 hour intervals | Trend data only, detailed statistics discarded |\n\n**Retention Policy Implementation**\n\nRetention policies operate at multiple granularities to provide fine-grained control over data lifecycle management. **Global policies** apply default retention periods to all metrics, **metric-specific policies** override defaults for particular metric names, and **label-based policies** allow retention decisions based on label values such as environment or criticality levels.\n\nThe retention engine runs as a background process that evaluates policies every hour and marks eligible data for deletion. Rather than immediately removing data, the system uses a **two-phase deletion** approach: first marking blocks as expired, then physically removing them after a grace period. This approach prevents accidental data loss and allows recovery from misconfigured retention policies.\n\n| Policy Type | Evaluation Frequency | Scope | Example Use Case |\n|-------------|---------------------|-------|-----------------|\n| Global | Every hour | All metrics | \"Delete all data older than 90 days\" |\n| Metric-specific | Every hour | Named metrics | \"Keep http_request logs for 180 days\" |\n| Label-based | Every hour | Metrics matching label patterns | \"Keep production env data for 1 year, dev data for 30 days\" |\n| Storage-based | Every 30 minutes | Entire system | \"When disk usage > 80%, delete oldest non-critical data\" |\n\nThe policy evaluation engine maintains a priority queue of deletion candidates, allowing the system to free storage space intelligently during capacity pressure situations. Critical metrics (those referenced by active alert rules or displayed on important dashboards) receive higher retention priority than metrics that haven't been queried recently.\n\n**Consistency and Durability Guarantees**\n\nThe storage engine provides **eventual consistency** for compaction operations while maintaining **strong consistency** for retention policy application. During compaction, readers may temporarily observe both the original blocks and the newly compacted blocks, but query results remain correct because the block selection logic handles overlapping time ranges properly.\n\nWrite operations use a **write-ahead log (WAL)** to ensure durability. Incoming samples are first written to the WAL with an fsync operation, then asynchronously flushed to storage blocks. This approach provides immediate durability guarantees while allowing batched writes to optimize storage block creation. The WAL is automatically truncated as samples are successfully persisted to storage blocks.\n\n⚠️ **Pitfall: Ignoring Compaction During Development**\nMany developers focus on implementing the basic read/write functionality and defer compaction implementation until later. This approach leads to severe performance degradation as the system accumulates thousands of small storage blocks. Query performance degrades exponentially with block count, and storage space usage can be 5-10x higher than necessary. Implement basic compaction early, even if it's a simple periodic merger of small blocks.\n\n⚠️ **Pitfall: Retention Policies That Don't Account for Alert Dependencies**\nImplementing retention policies that delete historical data without considering active alert rules can break alerting functionality. Alert rules often need historical data for baseline calculations or trend analysis. The retention system must maintain metadata about which metrics are referenced by active alerts and apply appropriate retention extensions automatically.\n\n### Implementation Guidance\n\nThe time-series storage engine implementation requires careful attention to concurrent access patterns, efficient file I/O, and robust error handling. The following guidance provides the foundation for building a production-ready storage system.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|----------------|\n| File Format | JSON lines with gzip compression | Custom binary format with advanced compression |\n| Indexing | In-memory maps with periodic persistence | B+ tree or LSM-tree based indexes |\n| Compression | Go's gzip/lz4 for general compression | Specialized time-series compression (Gorilla, etc.) |\n| Concurrency | sync.RWMutex for index protection | Lock-free data structures with atomic operations |\n| Background Tasks | time.Ticker with simple scheduling | Priority queue with adaptive scheduling |\n\n**File Structure**\n\n```\ninternal/storage/\n├── engine.go              ← Main storage engine implementation\n├── block.go               ← Storage block read/write operations  \n├── index.go               ← In-memory and persistent indexing\n├── compaction.go          ← Background compaction processes\n├── retention.go           ← Data lifecycle management\n├── wal.go                 ← Write-ahead log for durability\n├── compression.go         ← Compression algorithm implementations\n├── query.go               ← Query execution against storage blocks\n└── storage_test.go        ← Comprehensive integration tests\n```\n\n**Core Data Structures**\n\n```go\npackage storage\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// StorageEngine manages time-series data with compaction and retention\ntype StorageEngine struct {\n    config        *StorageConfig\n    dataDir       string\n    seriesIndex   *SeriesIndex\n    blockManager  *BlockManager\n    compactor     *Compactor\n    retentionMgr  *RetentionManager\n    wal           *WriteAheadLog\n    mu            sync.RWMutex\n    closed        bool\n}\n\n// SeriesIndex maintains the mapping from series IDs to storage blocks\ntype SeriesIndex struct {\n    series    map[string]*SeriesInfo\n    labels    map[string]map[string][]string  // label_name -> label_value -> series_ids\n    mu        sync.RWMutex\n    dirty     bool\n}\n\n// SeriesInfo contains metadata and block references for a time-series\ntype SeriesInfo struct {\n    SeriesID    string\n    MetricName  string\n    Labels      Labels\n    Blocks      []*BlockReference\n    LastSample  time.Time\n}\n\n// BlockReference points to a storage block containing series data\ntype BlockReference struct {\n    BlockID    string\n    FilePath   string\n    TimeRange  TimeRange\n    SeriesCount int\n    Compressed bool\n}\n\n// StorageBlock represents an immutable collection of time-series samples\ntype StorageBlock struct {\n    ID          string\n    TimeRange   TimeRange\n    Series      map[string][]Sample\n    Metadata    map[string]interface{}\n    compressed  bool\n}\n```\n\n**Core Storage Engine Interface**\n\n```go\n// TimeSeriesStorage defines the primary storage interface\ntype TimeSeriesStorage interface {\n    // WriteSamples persists metric samples to storage with durability guarantees\n    WriteSamples(ctx context.Context, samples []Sample) error\n    \n    // QueryRange retrieves samples for specified series within time range\n    QueryRange(ctx context.Context, seriesID string, start, end time.Time) ([]Sample, error)\n    \n    // QuerySeries finds all series matching label selectors\n    QuerySeries(ctx context.Context, matchers []LabelMatcher) ([]SeriesInfo, error)\n    \n    // GetSeriesInfo retrieves metadata for a specific series\n    GetSeriesInfo(ctx context.Context, seriesID string) (*SeriesInfo, error)\n    \n    // Close gracefully shuts down storage engine and background processes\n    Close() error\n}\n\n// LabelMatcher defines criteria for series selection\ntype LabelMatcher struct {\n    Name     string\n    Value    string\n    Operator MatchOperator  // Equal, NotEqual, RegexMatch, etc.\n}\n```\n\n**Write-Ahead Log Implementation**\n\nThe WAL ensures durability by persisting samples before acknowledging writes. This complete implementation handles the critical path for data integrity:\n\n```go\npackage storage\n\nimport (\n    \"bufio\"\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n    \"path/filepath\"\n    \"sync\"\n    \"time\"\n)\n\n// WriteAheadLog provides durability guarantees for metric samples\ntype WriteAheadLog struct {\n    filePath    string\n    file        *os.File\n    writer      *bufio.Writer\n    mu          sync.Mutex\n    lastFlush   time.Time\n    flushInterval time.Duration\n}\n\n// WALRecord represents a single entry in the write-ahead log\ntype WALRecord struct {\n    Timestamp time.Time `json:\"timestamp\"`\n    Type      string    `json:\"type\"`\n    Data      []Sample  `json:\"data\"`\n    Checksum  uint32    `json:\"checksum\"`\n}\n\n// NewWriteAheadLog creates and initializes a new WAL instance\nfunc NewWriteAheadLog(dataDir string, flushInterval time.Duration) (*WriteAheadLog, error) {\n    walPath := filepath.Join(dataDir, \"wal.log\")\n    \n    file, err := os.OpenFile(walPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to open WAL file: %w\", err)\n    }\n    \n    wal := &WriteAheadLog{\n        filePath:      walPath,\n        file:          file,\n        writer:        bufio.NewWriter(file),\n        flushInterval: flushInterval,\n        lastFlush:     time.Now(),\n    }\n    \n    // Start background flush routine\n    go wal.backgroundFlush()\n    \n    return wal, nil\n}\n\n// AppendSamples writes samples to WAL with immediate durability\nfunc (w *WriteAheadLog) AppendSamples(samples []Sample) error {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    \n    record := WALRecord{\n        Timestamp: time.Now(),\n        Type:      \"samples\",\n        Data:      samples,\n        Checksum:  calculateChecksum(samples),\n    }\n    \n    // Serialize record to JSON\n    data, err := json.Marshal(record)\n    if err != nil {\n        return fmt.Errorf(\"failed to marshal WAL record: %w\", err)\n    }\n    \n    // Write to buffer\n    if _, err := w.writer.Write(data); err != nil {\n        return fmt.Errorf(\"failed to write to WAL: %w\", err)\n    }\n    \n    if _, err := w.writer.WriteString(\"\\n\"); err != nil {\n        return fmt.Errorf(\"failed to write newline: %w\", err)\n    }\n    \n    // Force immediate flush for durability\n    if err := w.writer.Flush(); err != nil {\n        return fmt.Errorf(\"failed to flush WAL buffer: %w\", err)\n    }\n    \n    if err := w.file.Sync(); err != nil {\n        return fmt.Errorf(\"failed to sync WAL to disk: %w\", err)\n    }\n    \n    return nil\n}\n\n// backgroundFlush periodically ensures data is written to disk\nfunc (w *WriteAheadLog) backgroundFlush() {\n    ticker := time.NewTicker(w.flushInterval)\n    defer ticker.Stop()\n    \n    for range ticker.C {\n        w.mu.Lock()\n        w.writer.Flush()\n        w.file.Sync()\n        w.lastFlush = time.Now()\n        w.mu.Unlock()\n    }\n}\n```\n\n**Storage Engine Core Implementation Template**\n\nThe following template provides the structure for the main storage engine. Key methods are stubbed with detailed TODOs that correspond to the design algorithms described above:\n\n```go\n// NewStorageEngine creates a new time-series storage engine\nfunc NewStorageEngine(config *StorageConfig) (*StorageEngine, error) {\n    engine := &StorageEngine{\n        config:       config,\n        dataDir:      config.DataDir,\n        seriesIndex:  NewSeriesIndex(),\n        blockManager: NewBlockManager(config.DataDir),\n    }\n    \n    // TODO 1: Create data directory if it doesn't exist\n    // TODO 2: Initialize write-ahead log with configured flush interval\n    // TODO 3: Load existing series index from disk (if present)\n    // TODO 4: Start background compaction goroutine\n    // TODO 5: Start background retention policy goroutine\n    // TODO 6: Register shutdown cleanup handlers\n    \n    return engine, nil\n}\n\n// WriteSamples persists metric samples with durability guarantees\nfunc (e *StorageEngine) WriteSamples(ctx context.Context, samples []Sample) error {\n    e.mu.RLock()\n    defer e.mu.RUnlock()\n    \n    if e.closed {\n        return fmt.Errorf(\"storage engine is closed\")\n    }\n    \n    // TODO 1: Write samples to WAL for immediate durability\n    // TODO 2: Group samples by series ID for efficient batching\n    // TODO 3: Update series index with new samples and time ranges\n    // TODO 4: Add samples to current active storage block\n    // TODO 5: If block reaches size threshold, trigger compaction\n    // TODO 6: Update cardinality tracking for label explosion detection\n    \n    return nil\n}\n\n// QueryRange retrieves samples for specified series within time range\nfunc (e *StorageEngine) QueryRange(ctx context.Context, seriesID string, start, end time.Time) ([]Sample, error) {\n    e.mu.RLock()\n    defer e.mu.RUnlock()\n    \n    // TODO 1: Look up series info in index to get block references\n    // TODO 2: Filter blocks by time range to minimize I/O\n    // TODO 3: Load samples from each relevant block\n    // TODO 4: Merge samples in chronological order\n    // TODO 5: Apply any necessary decompression\n    // TODO 6: Filter samples to exact time range requested\n    \n    return nil, nil\n}\n```\n\n**Background Compaction Process**\n\n```go\n// Compactor handles background compaction of storage blocks\ntype Compactor struct {\n    engine      *StorageEngine\n    running     bool\n    stopCh      chan struct{}\n    mu          sync.Mutex\n}\n\n// runCompaction executes the main compaction logic\nfunc (c *Compactor) runCompaction(ctx context.Context) error {\n    // TODO 1: Scan data directory for blocks eligible for compaction\n    // TODO 2: Group blocks by time range and size for optimal merging\n    // TODO 3: For each group, read all samples into memory\n    // TODO 4: Sort samples by timestamp and remove duplicates\n    // TODO 5: Apply compression and write to new compacted block\n    // TODO 6: Update series index to reference new block\n    // TODO 7: Mark old blocks for deletion after grace period\n    // TODO 8: Update compaction statistics and metrics\n    \n    return nil\n}\n```\n\n**Language-Specific Optimization Hints**\n\n- **File I/O**: Use `os.File.ReadAt()` and `os.File.WriteAt()` for concurrent access to different file regions without seeking\n- **Memory Management**: Pre-allocate slices with known capacity using `make([]Sample, 0, expectedCount)` to avoid repeated allocations\n- **Concurrency**: Use `sync.RWMutex` for the series index since reads vastly outnumber writes in typical workloads\n- **Error Handling**: Wrap file system errors with context using `fmt.Errorf(\"operation failed: %w\", err)` for better debugging\n- **JSON Performance**: Consider using `encoding/json.Encoder` with a pooled buffer for better performance than `json.Marshal`\n\n**Milestone Checkpoint**\n\nAfter implementing the storage engine, verify functionality with these concrete tests:\n\n1. **Basic Write/Read Test**: Write 1000 samples across 10 different series, then query each series individually. Verify all samples are returned in correct chronological order.\n\n2. **Compaction Test**: Write enough data to trigger compaction (based on your block size threshold), wait for compaction to complete, then verify queries still return correct results and storage directory contains fewer files.\n\n3. **WAL Recovery Test**: Write samples, kill the process before graceful shutdown, restart and verify all samples are recovered from the WAL.\n\n4. **Concurrent Access Test**: Run multiple goroutines writing different series while others query existing data. Verify no race conditions using `go test -race`.\n\nExpected behavior: Write operations should complete in under 10ms for batches of 100 samples, queries should return results in under 50ms for 24-hour ranges, and compaction should reduce file count by 5-10x while maintaining query correctness.\n\n**Common Implementation Pitfalls**\n\n⚠️ **Pitfall: Not Handling Partial Writes**\nFile system operations can be interrupted, leaving storage blocks in an inconsistent state. Always write to temporary files first, then atomically rename them to the final location using `os.Rename()`. This ensures readers never see partially written data.\n\n⚠️ **Pitfall: Index Corruption During Crashes**\nThe series index is critical for query performance but vulnerable to corruption if the process crashes during updates. Implement index versioning with atomic updates: write new index to `index.new`, fsync, then rename to `index.db`.\n\n⚠️ **Pitfall: Memory Leaks in Long-Running Compaction**\nCompaction processes can accumulate large amounts of data in memory when processing multiple blocks. Implement streaming compaction that processes blocks in chunks rather than loading entire datasets into memory simultaneously.\n\n\n## Query Engine\n\n> **Milestone(s):** 2 (Storage & Querying) - This section details the query processing system that executes metric queries for both dashboard visualization and alert evaluation\n\n### Mental Model: The Research Assistant\n\nThink of the Query Engine as an experienced research assistant working in a vast scientific library. When you need specific information, you don't hand the assistant a raw storage location - instead, you make a request in natural language: \"Find me all the temperature readings from Building A sensors between 2 PM and 4 PM yesterday, and calculate the average for each floor.\"\n\nThe research assistant then performs a sophisticated multi-step process. First, they parse your request to understand exactly what you're looking for - which metrics, what time period, which filtering criteria, and what calculations to perform. Next, they create a research plan - determining which sections of the library to visit, in what order, and how to efficiently gather the information. Then they execute this plan, navigating the library's indexing system to locate the relevant data quickly. Finally, they process and analyze the raw information, performing the requested calculations and presenting the results in the format you need.\n\nThis analogy captures the essence of query processing: translating high-level information requests into efficient data retrieval and computation operations. Just as a good research assistant knows shortcuts through the library and can optimize their research strategy based on the type of question, our Query Engine must understand how to leverage the time-series storage structure to answer metric queries efficiently.\n\nThe key insight is that query processing is not just about finding data - it's about finding data *efficiently* and performing meaningful computations on it. A research assistant who checks every single book in the library for your temperature readings would eventually find the answer, but would take far too long. Similarly, a query engine that scans every stored sample to answer a simple aggregation query would be functionally useless at scale.\n\n### Query Language Design\n\nOur query language provides a simple but powerful syntax for expressing metric selection, time range filtering, and aggregation operations. The design balances expressiveness with simplicity, targeting the common use cases that appear in both dashboard visualizations and alert rule definitions.\n\n> **Decision: Simplified Query Syntax Over Full Programming Language**\n> - **Context**: We need a way for users to specify what metric data they want and how to process it, with options ranging from simple metric selection to complex mathematical expressions\n> - **Options Considered**: \n>   1. Simple metric selection with basic aggregation functions\n>   2. SQL-like query language with time-series extensions\n>   3. Full expression language with custom functions and variables\n> - **Decision**: Simple metric selection syntax with built-in aggregation functions and basic arithmetic\n> - **Rationale**: Dashboard users and alert definitions need to be readable by operations teams, not just developers. Complex query languages create a barrier to adoption and increase the chance of errors in critical alerting rules\n> - **Consequences**: Enables quick learning curve and reduces syntax errors, but limits advanced analytical capabilities and may require multiple queries for complex scenarios\n\n| Query Language Option | Pros | Cons | Chosen? |\n|----------------------|------|------|---------|\n| Simple Selection + Aggregation | Easy to learn, hard to misuse, fast parsing | Limited analytical power, may need multiple queries | ✓ Yes |\n| SQL-like Syntax | Familiar to many users, powerful joins and subqueries | Complex for time-series concepts, verbose syntax | No |\n| Full Expression Language | Maximum flexibility, supports complex math | High learning curve, error-prone for alerts | No |\n\nThe query language consists of four main components that work together to specify data selection and processing:\n\n**Metric Selection** forms the foundation of every query, specifying which time-series to include in the result set. The basic syntax uses the metric name followed by optional label matchers enclosed in curly braces. Label matchers support exact matching, regular expressions, and negative matching to provide flexible filtering capabilities.\n\n| Selector Type | Syntax | Example | Description |\n|--------------|--------|---------|-------------|\n| Basic Metric | `metric_name` | `cpu_usage` | Selects all series for the named metric |\n| Label Exact Match | `metric_name{label=\"value\"}` | `cpu_usage{host=\"web01\"}` | Matches series with exact label value |\n| Label Regex Match | `metric_name{label=~\"pattern\"}` | `cpu_usage{host=~\"web.*\"}` | Matches series with label matching regex |\n| Label Not Equal | `metric_name{label!=\"value\"}` | `cpu_usage{host!=\"localhost\"}` | Excludes series with specific label value |\n| Label Not Regex | `metric_name{label!~\"pattern\"}` | `cpu_usage{host!~\"test.*\"}` | Excludes series matching regex pattern |\n| Multiple Labels | `metric_name{label1=\"value1\",label2=\"value2\"}` | `cpu_usage{host=\"web01\",cpu=\"0\"}` | All label conditions must match |\n\n**Time Range Specification** determines the temporal scope of the query, supporting both absolute timestamps and relative time expressions. The time range can be specified as part of the query string or provided separately by the query execution context (such as dashboard time controls).\n\n| Time Range Type | Syntax | Example | Description |\n|-----------------|--------|---------|-------------|\n| Relative Range | `[5m]` | `cpu_usage[5m]` | Last 5 minutes of data |\n| Absolute Range | `[2024-01-15T10:00:00Z:2024-01-15T11:00:00Z]` | `cpu_usage[2024-01-15T10:00:00Z:2024-01-15T11:00:00Z]` | Specific time window |\n| Duration Units | `s`, `m`, `h`, `d` | `[30s]`, `[2h]`, `[7d]` | Seconds, minutes, hours, days |\n\n**Aggregation Functions** process the selected time-series data to produce meaningful results for visualization and alerting. These functions operate either across time (reducing a time-series to a single value) or across series (combining multiple time-series into fewer series).\n\n| Function Category | Function Name | Syntax | Description | Use Case |\n|-------------------|---------------|--------|-------------|----------|\n| Statistical | `avg` | `avg(cpu_usage)` | Average across all selected series | Overall system load |\n| Statistical | `sum` | `sum(http_requests_total)` | Sum across all selected series | Total request rate |\n| Statistical | `min` | `min(disk_free_bytes)` | Minimum value across series | Worst-case capacity |\n| Statistical | `max` | `max(response_time)` | Maximum value across series | Worst-case latency |\n| Statistical | `count` | `count(up)` | Number of series with data | Service availability |\n| Temporal | `rate` | `rate(http_requests_total[5m])` | Per-second rate of increase | Request rate calculation |\n| Temporal | `increase` | `increase(counter_metric[1h])` | Total increase over time range | Growth measurement |\n| Percentile | `quantile` | `quantile(0.95, response_time)` | Percentile across series | SLA measurements |\n\n**Arithmetic Operations** allow combining metrics and constants using basic mathematical operators, enabling the calculation of derived metrics and ratios that provide business context to raw measurements.\n\n| Operator | Syntax | Example | Description |\n|----------|--------|---------|-------------|\n| Addition | `+` | `cpu_usage + memory_usage` | Add metrics together |\n| Subtraction | `-` | `disk_total - disk_used` | Calculate differences |\n| Multiplication | `*` | `cpu_usage * 100` | Scale by constant |\n| Division | `/` | `http_errors / http_total` | Calculate ratios |\n| Parentheses | `()` | `(cpu_usage + memory_usage) / 2` | Control operation order |\n\nThe query language supports function composition and chaining to build complex expressions from simple building blocks. For example, calculating an error rate percentage requires combining metric selection, rate calculation, and arithmetic: `rate(http_errors_total[5m]) / rate(http_requests_total[5m]) * 100`.\n\n> The critical design insight is that query complexity should grow gradually - simple queries should have simple syntax, while complex analytics remain possible through composition rather than requiring entirely different syntax patterns.\n\n### Query Execution Pipeline\n\nThe query execution pipeline transforms user query strings into efficient operations against the time-series storage engine through a carefully orchestrated sequence of parsing, planning, and execution phases. This pipeline design separates concerns cleanly, enabling optimization opportunities while maintaining correctness guarantees.\n\n![Query Processing Flow](./diagrams/query-execution.svg)\n\n**Phase 1: Query Parsing and Validation** converts the query string into a structured internal representation that can be processed programmatically. The parser employs a recursive descent approach that handles the nested nature of function calls and arithmetic expressions while providing clear error messages for syntax errors.\n\nThe parsing process follows these steps:\n\n1. **Lexical Analysis**: The query string is tokenized into meaningful symbols - metric names, label selectors, function names, operators, and literals. This phase handles quoted strings, regular expressions, and numeric constants while detecting malformed tokens.\n\n2. **Syntax Analysis**: Tokens are organized into an Abstract Syntax Tree (AST) that represents the query structure. The parser validates that function calls have the correct number of arguments, operators are used in valid contexts, and parentheses are balanced.\n\n3. **Semantic Validation**: The AST is analyzed for logical correctness - metric names are checked against known metrics, label names are validated, and function signatures are verified. This phase catches errors like attempting to calculate rates on gauge metrics or using undefined aggregation functions.\n\n4. **Type Analysis**: Each node in the AST is annotated with its expected result type (scalar, vector, or matrix) to ensure operations are compatible. For example, arithmetic operations between vectors require matching label sets.\n\n| Parsing Component | Input | Output | Error Types Detected |\n|-------------------|-------|--------|---------------------|\n| Lexer | Raw query string | Token stream | Invalid characters, unterminated strings |\n| Parser | Token stream | Abstract Syntax Tree | Syntax errors, malformed expressions |\n| Semantic Analyzer | AST + Metric Metadata | Validated AST | Unknown metrics, invalid functions |\n| Type Checker | Validated AST | Typed AST | Type mismatches, incompatible operations |\n\n**Phase 2: Query Planning and Optimization** analyzes the validated query to determine the most efficient execution strategy. The planner considers factors like storage layout, available indexes, and data locality to minimize the amount of data that must be read and processed.\n\nThe planning process generates an execution plan that specifies:\n\n1. **Series Selection Strategy**: Determines which series indexes to consult and what filtering can be pushed down to the storage layer. For queries with highly selective label matchers, the planner can avoid reading irrelevant series entirely.\n\n2. **Time Range Optimization**: Analyzes the query's time requirements to determine which storage blocks need to be accessed. Queries covering short recent time periods can skip older compacted blocks entirely.\n\n3. **Aggregation Scheduling**: Decides whether aggregations can be performed incrementally as data is read (reducing memory usage) or need to buffer all data first (required for percentile calculations).\n\n4. **Parallelization Opportunities**: Identifies independent operations that can be executed concurrently, such as processing different series or time ranges in parallel.\n\n| Planning Decision | Factors Considered | Optimization Applied |\n|-------------------|-------------------|---------------------|\n| Index Usage | Label selectivity, cardinality | Choose most selective indexes first |\n| Block Selection | Query time range, block boundaries | Skip blocks outside time range |\n| Memory Management | Result set size, aggregation type | Stream processing vs. buffering |\n| Parallelization | CPU cores, I/O patterns | Concurrent series processing |\n\n**Phase 3: Storage Access and Data Retrieval** executes the optimized query plan against the time-series storage engine, retrieving the minimum necessary data while maintaining consistency guarantees.\n\nThe execution engine coordinates several storage operations:\n\n1. **Series Discovery**: Uses the series index to identify all time-series that match the query's label selectors. This phase leverages inverted indexes to quickly find candidate series without scanning all stored data.\n\n2. **Block Access**: Reads the storage blocks that contain samples within the query's time range. The engine uses block metadata to skip blocks that don't contain relevant data and can read multiple blocks concurrently.\n\n3. **Sample Filtering**: Applies additional filtering criteria that couldn't be pushed down to the storage layer, such as complex regular expression matches or range conditions.\n\n4. **Data Streaming**: Organizes the retrieved samples into time-ordered streams that can be processed efficiently by the aggregation functions.\n\n**Phase 4: Aggregation and Result Computation** applies the query's mathematical operations to the retrieved data, producing the final result set that will be returned to the caller.\n\nDifferent aggregation types require different processing strategies:\n\n1. **Series Aggregation**: Functions like `sum()` and `avg()` combine multiple time-series into fewer series. These operations must align timestamps across series and handle cases where series have different sampling intervals.\n\n2. **Temporal Aggregation**: Functions like `rate()` and `increase()` analyze how individual time-series change over time. These operations must handle counter resets and missing data points appropriately.\n\n3. **Statistical Functions**: Operations like `quantile()` require collecting all relevant data points before computation, using streaming algorithms when possible to manage memory usage.\n\n4. **Arithmetic Operations**: Binary operations between metrics must align series by labels and handle cases where operand series don't have matching labels.\n\n| Aggregation Type | Processing Strategy | Memory Requirements | Special Considerations |\n|------------------|-------------------|-------------------|----------------------|\n| Series Sum/Average | Streaming with timestamp alignment | O(time_points) | Handle missing series data |\n| Rate Calculation | Windowed streaming | O(window_size) | Detect counter resets |\n| Percentiles | Collect-then-compute | O(all_samples) | May require approximate algorithms |\n| Binary Arithmetic | Label-based matching | O(series_count) | Handle label mismatches |\n\n> **Decision: Streaming vs. Buffering Execution**\n> - **Context**: Query results can range from a few data points to millions of samples, and available memory may be limited\n> - **Options Considered**:\n>   1. Always buffer all data in memory before processing\n>   2. Always use streaming processing with fixed memory limits\n>   3. Choose strategy based on query characteristics and system resources\n> - **Decision**: Hybrid approach that uses streaming for compatible operations and buffering only when required\n> - **Rationale**: Streaming enables processing datasets larger than available memory and provides consistent latency, while some operations (like percentiles) fundamentally require seeing all data\n> - **Consequences**: Enables scaling to large datasets with predictable resource usage, but requires more complex execution engine implementation\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Timestamp Alignment Errors**\nWhen aggregating multiple time-series, failing to properly align timestamps leads to incorrect results. This happens when developers assume all time-series have samples at identical timestamps, but in reality, metrics are collected at slightly different times. For example, averaging CPU usage across multiple hosts might miss samples or double-count values if timestamps aren't aligned to common intervals. The fix is to implement timestamp bucketing that groups nearby samples into common time slots before aggregation.\n\n⚠️ **Pitfall: Counter Reset Handling**\nRate calculations become incorrect when counter metrics reset to zero (usually due to process restarts). A naive rate calculation sees a large negative change and produces meaningless results. The correct approach detects when the current value is less than the previous value and treats this as a reset, calculating the rate from the reset point rather than the previous sample.\n\n⚠️ **Pitfall: Memory Explosion with High Cardinality**\nQueries that select high-cardinality metrics without sufficient filtering can consume enormous amounts of memory. For example, querying all HTTP request metrics without filtering by endpoint or status code might return millions of time-series. The query engine must implement memory limits and provide clear error messages when queries would exceed available resources, guiding users to add more selective label filters.\n\n⚠️ **Pitfall: Inefficient Range Query Patterns**\nRepeatedly executing queries with small time ranges (like dashboard panels refreshing every 10 seconds) creates unnecessary load on the storage layer. Each query incurs parsing and planning overhead, and may read overlapping data. The solution is implementing query result caching and encouraging dashboard designs that use longer refresh intervals with appropriate time range selections.\n\n⚠️ **Pitfall: Missing Data Point Handling**\nTime-series data often has gaps due to collection failures or network issues. Aggregation functions that don't handle missing data appropriately can produce misleading results - for example, averaging over incomplete data sets or assuming zero values where no data exists. The query engine must distinguish between explicit zero values and missing data, providing options for interpolation, forwarding previous values, or excluding incomplete time periods from calculations.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Query Parser | Hand-written recursive descent parser | ANTLR or similar parser generator |\n| Expression Evaluation | Direct AST traversal | Bytecode compilation with virtual machine |\n| Result Serialization | JSON with standard library | MessagePack or Protocol Buffers |\n| Query Caching | In-memory LRU cache | Redis with TTL-based invalidation |\n| Concurrency Control | Worker pool with channels | Actor model with Akka-style messaging |\n\n#### File Structure\n\n```\ninternal/query/\n  engine.go              ← main QueryProcessor implementation\n  parser.go              ← query string parsing logic\n  planner.go             ← query optimization and planning\n  executor.go            ← execution engine coordination\n  aggregation.go         ← aggregation function implementations\n  ast.go                 ← abstract syntax tree definitions\n  cache.go               ← query result caching\n  engine_test.go         ← comprehensive query engine tests\n  testdata/              ← test query files and expected results\n    queries.json         ← test cases with expected outputs\n```\n\n#### Query Parser Infrastructure\n\n```go\npackage query\n\nimport (\n    \"fmt\"\n    \"regexp\"\n    \"strconv\"\n    \"strings\"\n    \"time\"\n)\n\n// TokenType represents different types of tokens in query strings\ntype TokenType int\n\nconst (\n    TokenMetricName TokenType = iota\n    TokenLabelName\n    TokenString\n    TokenRegex\n    TokenNumber\n    TokenFunction\n    TokenOperator\n    TokenLeftParen\n    TokenRightParen\n    TokenLeftBrace\n    TokenRightBrace\n    TokenLeftBracket\n    TokenRightBracket\n    TokenComma\n    TokenEqual\n    TokenNotEqual\n    TokenRegexMatch\n    TokenRegexNotMatch\n    TokenEOF\n    TokenError\n)\n\n// Token represents a single token from query lexical analysis\ntype Token struct {\n    Type     TokenType\n    Value    string\n    Position int\n}\n\n// Lexer tokenizes query strings into structured tokens\ntype Lexer struct {\n    input    string\n    position int\n    current  rune\n}\n\n// NewLexer creates a lexer for the given query string\nfunc NewLexer(input string) *Lexer {\n    l := &Lexer{\n        input:    input,\n        position: 0,\n    }\n    if len(input) > 0 {\n        l.current = rune(input[0])\n    }\n    return l\n}\n\n// NextToken returns the next token from the input stream\nfunc (l *Lexer) NextToken() Token {\n    for l.current != 0 {\n        if l.isWhitespace(l.current) {\n            l.skipWhitespace()\n            continue\n        }\n        \n        switch l.current {\n        case '(':\n            return l.singleCharToken(TokenLeftParen)\n        case ')':\n            return l.singleCharToken(TokenRightParen)\n        case '{':\n            return l.singleCharToken(TokenLeftBrace)\n        case '}':\n            return l.singleCharToken(TokenRightBrace)\n        case '[':\n            return l.singleCharToken(TokenLeftBracket)\n        case ']':\n            return l.singleCharToken(TokenRightBracket)\n        case ',':\n            return l.singleCharToken(TokenComma)\n        case '=':\n            return l.handleEquals()\n        case '!':\n            return l.handleNotEquals()\n        case '\"':\n            return l.readString()\n        case '/':\n            return l.readRegex()\n        default:\n            if l.isLetter(l.current) {\n                return l.readIdentifier()\n            }\n            if l.isDigit(l.current) {\n                return l.readNumber()\n            }\n            return Token{Type: TokenError, Value: string(l.current), Position: l.position}\n        }\n    }\n    return Token{Type: TokenEOF, Position: l.position}\n}\n\n// QueryAST represents the parsed structure of a query\ntype QueryAST struct {\n    Root ASTNode\n}\n\n// ASTNode represents a node in the query abstract syntax tree\ntype ASTNode interface {\n    NodeType() string\n    String() string\n}\n\n// MetricSelectorNode represents metric selection with label matchers\ntype MetricSelectorNode struct {\n    MetricName   string\n    LabelMatchers []LabelMatcher\n    TimeRange    *TimeRangeNode\n}\n\nfunc (n *MetricSelectorNode) NodeType() string { return \"MetricSelector\" }\nfunc (n *MetricSelectorNode) String() string {\n    // Implementation for string representation\n    return fmt.Sprintf(\"%s{...}\", n.MetricName)\n}\n\n// FunctionCallNode represents aggregation function calls\ntype FunctionCallNode struct {\n    FunctionName string\n    Arguments    []ASTNode\n    Parameters   map[string]interface{}\n}\n\nfunc (n *FunctionCallNode) NodeType() string { return \"FunctionCall\" }\nfunc (n *FunctionCallNode) String() string {\n    return fmt.Sprintf(\"%s(...)\", n.FunctionName)\n}\n\n// BinaryOpNode represents arithmetic operations between expressions\ntype BinaryOpNode struct {\n    Left     ASTNode\n    Operator string\n    Right    ASTNode\n}\n\nfunc (n *BinaryOpNode) NodeType() string { return \"BinaryOp\" }\nfunc (n *BinaryOpNode) String() string {\n    return fmt.Sprintf(\"(%s %s %s)\", n.Left, n.Operator, n.Right)\n}\n\n// TimeRangeNode represents time range specifications\ntype TimeRangeNode struct {\n    Duration time.Duration\n    Start    *time.Time\n    End      *time.Time\n}\n\nfunc (n *TimeRangeNode) NodeType() string { return \"TimeRange\" }\nfunc (n *TimeRangeNode) String() string {\n    if n.Duration > 0 {\n        return fmt.Sprintf(\"[%s]\", n.Duration)\n    }\n    return fmt.Sprintf(\"[%s:%s]\", n.Start, n.End)\n}\n```\n\n#### Query Execution Engine Core\n\n```go\n// QueryResult contains the result of query execution\ntype QueryResult struct {\n    ResultType string                 `json:\"resultType\"` // \"vector\", \"matrix\", \"scalar\"\n    Data       interface{}           `json:\"data\"`\n    Warnings   []string              `json:\"warnings,omitempty\"`\n    Stats      *QueryExecutionStats  `json:\"stats,omitempty\"`\n}\n\n// QueryExecutionStats provides metrics about query performance\ntype QueryExecutionStats struct {\n    SeriesCount      int           `json:\"seriesCount\"`\n    SamplesProcessed int           `json:\"samplesProcessed\"`\n    ExecutionTime    time.Duration `json:\"executionTime\"`\n    BlocksRead       int           `json:\"blocksRead\"`\n}\n\n// ExecuteQuery processes a query string and returns results\n// This is the main entry point that coordinates parsing, planning, and execution\nfunc (qp *QueryProcessor) ExecuteQuery(ctx context.Context, queryString string) (QueryResult, error) {\n    // TODO 1: Parse the query string into an AST\n    // Hint: Use NewParser(queryString).Parse() and handle syntax errors\n    \n    // TODO 2: Validate the AST against available metrics and labels\n    // Hint: Check metric names exist in storage, validate function signatures\n    \n    // TODO 3: Create an optimized execution plan\n    // Hint: Analyze label selectivity, determine block access patterns\n    \n    // TODO 4: Execute the plan against storage engine\n    // Hint: Retrieve samples, apply aggregations, handle errors\n    \n    // TODO 5: Format results according to query type\n    // Hint: Convert internal format to JSON-serializable result\n    \n    // TODO 6: Collect execution statistics for monitoring\n    // Hint: Track series count, samples processed, execution time\n}\n\n// PlanQuery analyzes a parsed query to create an optimized execution strategy\nfunc (qp *QueryProcessor) PlanQuery(ctx context.Context, ast *QueryAST) (*ExecutionPlan, error) {\n    // TODO 1: Analyze label matchers to estimate series selectivity\n    // Hint: Query series index to count matching series before data retrieval\n    \n    // TODO 2: Determine optimal block access strategy\n    // Hint: Check time range against block boundaries, skip unnecessary blocks\n    \n    // TODO 3: Plan aggregation execution order\n    // Hint: Some aggregations can stream, others need all data collected first\n    \n    // TODO 4: Identify parallelization opportunities\n    // Hint: Independent series can be processed concurrently\n    \n    // TODO 5: Set memory limits based on estimated result size\n    // Hint: Calculate expected series count × time range to estimate memory usage\n}\n\n// ExecutionPlan contains optimized strategy for query execution\ntype ExecutionPlan struct {\n    SeriesSelectors  []SeriesSelector\n    TimeRange       TimeRange\n    AggregationPlan []AggregationStep\n    MemoryLimit     int64\n    Parallelism     int\n}\n\n// SeriesSelector defines how to efficiently find matching time-series\ntype SeriesSelector struct {\n    MetricName    string\n    LabelMatchers []LabelMatcher\n    EstimatedSeries int\n}\n\n// AggregationStep defines one step in the aggregation pipeline\ntype AggregationStep struct {\n    Function    string\n    Parameters  map[string]interface{}\n    InputType   string  // \"vector\", \"matrix\", \"scalar\"\n    OutputType  string\n    CanStream   bool    // whether this step can process data incrementally\n}\n```\n\n#### Aggregation Function Library\n\n```go\n// AggregationFunction defines the interface for all aggregation operations\ntype AggregationFunction interface {\n    Name() string\n    Aggregate(ctx context.Context, series []TimeSeries, params map[string]interface{}) ([]TimeSeries, error)\n    CanStream() bool\n    RequiredParameters() []string\n}\n\n// SumAggregation implements sum() function across multiple time-series\ntype SumAggregation struct{}\n\nfunc (a *SumAggregation) Name() string { return \"sum\" }\nfunc (a *SumAggregation) CanStream() bool { return true }\nfunc (a *SumAggregation) RequiredParameters() []string { return []string{} }\n\nfunc (a *SumAggregation) Aggregate(ctx context.Context, series []TimeSeries, params map[string]interface{}) ([]TimeSeries, error) {\n    // TODO 1: Validate input series are compatible for summation\n    // Hint: Check that series have overlapping time ranges\n    \n    // TODO 2: Create timestamp alignment buckets\n    // Hint: Find common timestamp intervals across all input series\n    \n    // TODO 3: For each timestamp bucket, sum values from all series\n    // Hint: Handle missing values appropriately (skip or interpolate)\n    \n    // TODO 4: Generate result time-series with combined labels\n    // Hint: Remove labels that differ across input series\n    \n    // TODO 5: Handle streaming execution for large datasets\n    // Hint: Process samples in time order without buffering entire series\n}\n\n// RateAggregation implements rate() function for counter metrics\ntype RateAggregation struct{}\n\nfunc (a *RateAggregation) Name() string { return \"rate\" }\nfunc (a *RateAggregation) CanStream() bool { return true }\nfunc (a *RateAggregation) RequiredParameters() []string { return []string{\"range\"} }\n\nfunc (a *RateAggregation) Aggregate(ctx context.Context, series []TimeSeries, params map[string]interface{}) ([]TimeSeries, error) {\n    // TODO 1: Validate this is being applied to counter metrics only\n    // Hint: Check metric type metadata or detect always-increasing values\n    \n    // TODO 2: For each series, calculate rate over the specified time range\n    // Hint: rate = (current_value - previous_value) / time_difference\n    \n    // TODO 3: Handle counter resets (when current < previous)\n    // Hint: When counter resets, use current_value / time_since_reset for rate\n    \n    // TODO 4: Apply rate calculation across the specified time window\n    // Hint: Use the time range parameter to determine window size\n    \n    // TODO 5: Return new time-series with per-second rate values\n    // Hint: Preserve original labels but update metric name to indicate rate\n}\n\n// QuantileAggregation implements quantile() function for percentile calculations\ntype QuantileAggregation struct{}\n\nfunc (a *QuantileAggregation) Name() string { return \"quantile\" }\nfunc (a *QuantileAggregation) CanStream() bool { return false } // needs all data for sorting\nfunc (a *QuantileAggregation) RequiredParameters() []string { return []string{\"q\"} }\n\nfunc (a *QuantileAggregation) Aggregate(ctx context.Context, series []TimeSeries, params map[string]interface{}) ([]TimeSeries, error) {\n    // TODO 1: Extract quantile parameter (0.0 to 1.0)\n    // Hint: Validate q parameter is valid percentile value\n    \n    // TODO 2: For each timestamp, collect all series values\n    // Hint: Align timestamps and gather values across all series\n    \n    // TODO 3: Sort values at each timestamp\n    // Hint: Use sort.Float64s or implement streaming quantile algorithm for large datasets\n    \n    // TODO 4: Calculate quantile value using interpolation\n    // Hint: For q=0.95, find value at position (count-1)*0.95 in sorted array\n    \n    // TODO 5: Generate result series with quantile values over time\n    // Hint: Create single output series with quantile labels added\n}\n```\n\n#### Query Result Caching System\n\n```go\n// QueryCache provides caching for expensive query results\ntype QueryCache struct {\n    cache    map[string]*CacheEntry\n    mu       sync.RWMutex\n    maxSize  int64\n    maxAge   time.Duration\n    hitCount int64\n    missCount int64\n}\n\n// CacheEntry represents a cached query result with metadata\ntype CacheEntry struct {\n    Result    QueryResult\n    QueryHash string\n    CreatedAt time.Time\n    AccessCount int64\n    Size      int64\n}\n\n// NewQueryCache creates a query result cache with specified limits\nfunc NewQueryCache(maxSize int64, maxAge time.Duration) *QueryCache {\n    return &QueryCache{\n        cache:   make(map[string]*CacheEntry),\n        maxSize: maxSize,\n        maxAge:  maxAge,\n    }\n}\n\n// Get retrieves cached query result if available and fresh\nfunc (c *QueryCache) Get(ctx context.Context, queryString string, timeRange TimeRange) (QueryResult, bool) {\n    // TODO 1: Generate cache key from query string and time range\n    // Hint: Use hash of query + normalized time range for consistent keys\n    \n    // TODO 2: Check if cached entry exists and is still valid\n    // Hint: Verify entry age is less than maxAge and data covers requested time range\n    \n    // TODO 3: Update access statistics for cache hit\n    // Hint: Increment hit count and entry access count atomically\n    \n    // TODO 4: Return cached result if valid\n    // Hint: Deep copy result to prevent caller modifications\n}\n\n// Put stores query result in cache with size limits\nfunc (c *QueryCache) Put(ctx context.Context, queryString string, timeRange TimeRange, result QueryResult) {\n    // TODO 1: Calculate result size for memory management\n    // Hint: Estimate memory usage of result data structures\n    \n    // TODO 2: Check if adding this entry would exceed cache limits\n    // Hint: Implement LRU eviction if cache is full\n    \n    // TODO 3: Store entry with metadata\n    // Hint: Include creation time, access count, and size information\n    \n    // TODO 4: Update cache statistics\n    // Hint: Track cache size, entry count, and miss count\n}\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the Query Engine, verify the following behaviors:\n\n**Basic Query Execution**: Start your metrics system and ingest sample data with multiple time-series. Execute a simple query like `cpu_usage{host=\"web01\"}` and verify it returns the correct samples within the requested time range.\n\n**Aggregation Functions**: Test aggregation functions by executing `sum(cpu_usage)` and `avg(cpu_usage)` queries. Verify that the results correctly combine multiple time-series and handle timestamp alignment appropriately.\n\n**Rate Calculations**: For counter metrics, test `rate(http_requests_total[5m])` and verify it correctly calculates per-second rates while handling counter resets appropriately.\n\n**Error Handling**: Submit invalid queries with syntax errors, unknown metrics, and incompatible operations. Verify that the query engine returns helpful error messages rather than crashing.\n\n**Performance Testing**: Execute queries against datasets with varying cardinalities (100 series, 1000 series, 10000 series) and measure response times. Query execution should remain responsive even with high-cardinality metrics.\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Queries return empty results | Metric names or labels don't match stored data | Check series index for available metrics and labels | Verify query selectors match actual stored data |\n| Aggregation results are incorrect | Timestamp alignment issues or missing data handling | Log timestamp distribution across input series | Implement proper timestamp bucketing and interpolation |\n| Rate calculations show negative values | Counter resets not detected properly | Check for decreasing values in counter series | Add counter reset detection in rate function |\n| Query execution is extremely slow | Reading too much data or inefficient aggregation | Profile query execution and check block access patterns | Add more selective label filters or optimize storage access |\n| Memory usage grows without bounds | High cardinality queries without limits | Monitor memory allocation during query processing | Implement memory limits and streaming aggregation |\n| Cached results are stale or incorrect | Cache invalidation logic has bugs | Check cache hit/miss ratios and entry timestamps | Fix cache key generation and expiration logic |\n\n\n## Visualization Dashboard\n\n> **Milestone(s):** 3 (Visualization Dashboard) - This section details the web-based dashboard system that provides real-time metric visualization with configurable panels and sharing capabilities\n\n### Mental Model: The Mission Control Center\n\nThink of a metrics dashboard as NASA's mission control center during a space launch. The mission control center has multiple large displays, each showing different aspects of the mission - rocket telemetry, weather conditions, trajectory calculations, and system health indicators. Each display is carefully positioned and sized based on its importance and the information it conveys. Mission controllers can quickly glance across all displays to understand the complete system state, and critical alerts are highlighted prominently to demand immediate attention.\n\nYour metrics dashboard operates on the same principle. Each panel acts like one of those mission control displays, showing a specific aspect of your system's health through charts and graphs. Just as mission control displays update in real-time as new telemetry data arrives from the spacecraft, your dashboard panels refresh continuously as new metric samples flow into the storage engine. The dashboard's layout and panel configuration serve as the \"mission plan\" - defining what information to display, where to position it, and how frequently to update it.\n\nThe mission control analogy extends to user interaction patterns. Mission controllers don't constantly reconfigure their displays during critical operations - they rely on pre-configured views that show the right information at the right time. Similarly, effective dashboard design involves creating persistent configurations that teams can rely on during incident response. The ability to share dashboard views mirrors how mission control can broadcast their displays to other teams who need situational awareness.\n\nThis mental model helps us understand why dashboard performance and reliability are critical. Just as mission control cannot afford display failures during launch sequences, your metrics dashboard must maintain consistent performance even when visualizing thousands of time-series or handling rapid data updates. The real-time nature of the system means that stale or missing data can lead to incorrect operational decisions.\n\n### Dashboard Configuration System\n\nThe dashboard configuration system serves as the blueprint that transforms raw time-series data into meaningful visual representations. This system must balance flexibility with simplicity, allowing users to create sophisticated visualizations without requiring deep technical knowledge of the underlying query language or storage internals.\n\n**Dashboard Entity Structure**\n\nThe core dashboard entity encapsulates all information needed to render and maintain a complete dashboard view. The relationship between dashboards and panels follows a hierarchical ownership model where each dashboard owns its panel configurations completely.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| ID | string | Unique identifier for the dashboard, used for persistence and sharing |\n| Title | string | Human-readable dashboard name displayed in the interface |\n| Description | string | Optional detailed description explaining the dashboard's purpose |\n| Tags | []string | Searchable keywords for dashboard categorization and discovery |\n| Panels | []Panel | Complete list of visualization panels with their configurations |\n| TimeRange | TimeRange | Global time window applied to all panels unless overridden |\n| RefreshInterval | time.Duration | Automatic update frequency for all panels in the dashboard |\n| Editable | bool | Whether users can modify this dashboard configuration |\n\n**Panel Configuration Architecture**\n\nEach panel represents an independent visualization component with its own query, display options, and positioning information. The panel system uses a plugin-like architecture where different panel types (line charts, bar charts, single-stat displays) share common configuration patterns while allowing type-specific customization.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| ID | string | Unique panel identifier within the dashboard scope |\n| Title | string | Panel header text displayed above the visualization |\n| Type | string | Panel renderer type (linechart, barchart, singlestat, table) |\n| GridPos | GridPosition | Panel size and position within the dashboard grid layout |\n| Query | PanelQuery | Metric query definition and time range specifications |\n| Options | map[string]interface{} | Type-specific rendering options and styling preferences |\n| AlertRule | *PanelAlertRule | Optional alert threshold configuration tied to this panel |\n\nThe `GridPosition` structure implements a responsive grid system that ensures consistent layout across different screen sizes while allowing precise positioning control.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| X | int | Horizontal grid position (0-based, left to right) |\n| Y | int | Vertical grid position (0-based, top to bottom) |\n| Width | int | Panel width in grid units (typically 1-12 columns) |\n| Height | int | Panel height in grid units (affects chart detail level) |\n\n**Query Integration Architecture**\n\nThe panel query system bridges the dashboard layer with the underlying query engine, translating user-friendly configuration into executable query plans. This abstraction allows dashboard users to work with intuitive concepts like \"metric name\" and \"time range\" without understanding the complexities of query optimization or storage access patterns.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| Expression | string | Query expression using the system's query language syntax |\n| TimeRange | *TimeRange | Panel-specific time range override (inherits from dashboard if nil) |\n| RefreshInterval | *time.Duration | Panel-specific refresh rate override |\n| MaxDataPoints | int | Maximum number of data points to retrieve (controls resolution) |\n| FormatAs | string | Result format preference (timeseries, table, scalar) |\n\n> **Decision: JSON-Based Configuration Storage**\n> - **Context**: Dashboard configurations need persistent storage with human-readable format for version control and manual editing\n> - **Options Considered**: Binary protocol buffers, YAML files, JSON documents, SQL schema\n> - **Decision**: JSON documents stored as files with optional database backend\n> - **Rationale**: JSON provides excellent tooling support, version control compatibility, and direct JavaScript integration for the web interface\n> - **Consequences**: Enables easy dashboard sharing and backup, but requires careful schema versioning for backward compatibility\n\n**Configuration Validation and Normalization**\n\nThe dashboard configuration system implements comprehensive validation to prevent runtime errors and ensure consistent behavior across different deployment environments. Validation occurs at multiple layers: structural validation during JSON parsing, semantic validation during dashboard loading, and runtime validation during query execution.\n\nDashboard validation encompasses several critical areas:\n\n1. **Structural Validation**: Verifies that required fields are present, data types match expectations, and nested objects conform to their schemas\n2. **Query Validation**: Ensures that panel queries use valid syntax and reference existing metrics\n3. **Layout Validation**: Confirms that panel grid positions don't create overlapping layouts or exceed reasonable size constraints\n4. **Reference Validation**: Checks that alert rule references point to valid notification channels and use appropriate threshold operators\n\nThe validation system produces detailed error reports that help users identify and fix configuration problems quickly. Rather than failing silently or with cryptic error messages, the system provides specific field-level feedback with suggested corrections.\n\n**Template and Variable System**\n\nAdvanced dashboard configurations benefit from templating capabilities that allow dynamic metric selection and parameterized queries. This system enables creation of reusable dashboard templates that work across different environments or services without requiring manual reconfiguration.\n\nDashboard variables support several interpolation patterns:\n\n1. **Metric Name Variables**: Allow selection from available metrics matching a pattern\n2. **Label Value Variables**: Provide dropdown selection of label values for filtering\n3. **Time Range Variables**: Enable quick switching between common time windows\n4. **Custom Variables**: Support arbitrary string substitution for advanced use cases\n\nThe template engine processes variable substitution during query generation, ensuring that the underlying query engine receives fully-resolved expressions while maintaining the user-friendly parameterized interface.\n\n### Real-Time Data Updates\n\nReal-time dashboard updates represent one of the most technically challenging aspects of the visualization system. The architecture must efficiently push fresh data to multiple concurrent dashboard viewers while managing resource consumption and maintaining consistent user experience across varying network conditions.\n\n![Real-Time Dashboard Updates](./diagrams/dashboard-update-flow.svg)\n\n**WebSocket Communication Architecture**\n\nThe real-time update system uses WebSocket connections to establish persistent, bidirectional communication channels between the dashboard server and client browsers. This approach provides significantly lower latency compared to HTTP polling while reducing server resource consumption through connection reuse.\n\nThe WebSocket protocol implementation handles several critical responsibilities:\n\n1. **Connection Lifecycle Management**: Establishes connections during dashboard load, maintains heartbeat monitoring, and gracefully handles disconnections with automatic reconnection logic\n2. **Message Routing**: Distributes metric updates only to clients displaying relevant dashboards, avoiding unnecessary data transmission\n3. **Update Scheduling**: Coordinates refresh cycles across panels to prevent overwhelming clients with excessive update frequencies\n4. **Backpressure Handling**: Implements client-side buffering and server-side rate limiting when update rates exceed client processing capacity\n\n**Update Subscription Model**\n\nEach dashboard client establishes subscriptions for the specific metrics and time ranges displayed in its panels. The subscription system tracks active queries and their refresh requirements, enabling the server to batch related updates and minimize redundant query execution.\n\n| Subscription Component | Purpose | Key Attributes |\n|------------------------|---------|----------------|\n| Query Fingerprint | Identifies unique query/time-range combinations | Hash of expression, labels, time window |\n| Refresh Schedule | Determines update timing for each subscription | Interval, next execution time, jitter offset |\n| Client Set | Tracks which clients need specific updates | Connection IDs, panel mappings, active status |\n| Result Cache | Stores recent query results for immediate delivery | Timestamped data, expiration time, memory limit |\n\nThe subscription model optimizes resource usage by sharing query results across multiple clients viewing identical data. When multiple users access the same dashboard, the system executes each unique query only once per refresh cycle, then distributes results to all interested clients.\n\n**Incremental Update Strategy**\n\nRather than retransmitting complete datasets on every refresh, the system implements incremental updates that send only new or changed data points. This approach dramatically reduces bandwidth consumption and improves client-side rendering performance, especially for dashboards displaying long time ranges with high-frequency updates.\n\nThe incremental update algorithm works through a multi-step process:\n\n1. **Baseline Establishment**: New clients receive complete datasets for initial rendering, establishing a known synchronization point\n2. **Delta Computation**: Subsequent updates calculate differences between current and previously transmitted data\n3. **Efficient Encoding**: Delta messages use compact representation focusing on timestamp ranges and value changes\n4. **Client-Side Merging**: Dashboard clients merge incremental updates with existing datasets, maintaining complete time-series views\n5. **Periodic Resynchronization**: Full dataset refresh occurs periodically to prevent drift from accumulated deltas\n\n**Adaptive Update Frequency**\n\nThe real-time update system implements adaptive refresh rates that balance data freshness with system performance. Rather than using fixed update intervals, the system monitors several factors to determine optimal refresh timing for each dashboard and panel combination.\n\nAdaptive frequency calculation considers:\n\n- **Data Velocity**: Metrics with higher ingestion rates benefit from more frequent updates\n- **Panel Visibility**: Off-screen or inactive panels receive lower-priority update scheduling\n- **Client Performance**: Slow client connections or processing delays trigger automatic rate reduction\n- **System Load**: High query engine utilization results in reduced refresh frequencies across all clients\n- **User Preferences**: Explicit refresh rate settings override automatic calculations when specified\n\n> **Decision: WebSocket-Based Real-Time Updates**\n> - **Context**: Dashboard users need immediate visibility into metric changes without manual refresh actions\n> - **Options Considered**: HTTP polling, Server-Sent Events (SSE), WebSocket connections, UDP broadcast\n> - **Decision**: WebSocket connections with intelligent subscription management\n> - **Rationale**: WebSockets provide bidirectional communication needed for subscription management while maintaining low latency and efficient resource usage\n> - **Consequences**: Enables true real-time experience but requires connection state management and fallback handling for network disruptions\n\n**Error Handling and Resilience**\n\nReal-time update systems must gracefully handle various failure modes without disrupting the overall dashboard experience. The architecture implements comprehensive error handling that maintains service availability even when individual components experience problems.\n\nConnection-level error handling addresses network disruptions, client disconnections, and server restart scenarios. The client-side implementation includes exponential backoff reconnection logic that prevents overwhelming the server during widespread connectivity issues. Server-side connection tracking removes stale subscriptions and reclaims associated resources when clients disconnect unexpectedly.\n\nQuery-level error handling ensures that problems with individual panel queries don't affect other dashboard components. Failed queries trigger client-side error displays while continuing to update successful panels normally. The system maintains error state information to provide meaningful feedback about persistent query problems.\n\n### Implementation Guidance\n\nThis section provides concrete implementation details for building the dashboard visualization system, focusing on the web server architecture, real-time communication, and client-side rendering components.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Web Server | net/http with gorilla/mux router | Gin or Echo framework with middleware |\n| WebSocket | gorilla/websocket library | Custom WebSocket with compression |\n| Frontend | Vanilla JavaScript + Chart.js | React/Vue with D3.js or Plotly |\n| Static Assets | Embedded with go:embed | CDN with build pipeline |\n| Configuration | JSON files with validation | YAML with schema validation |\n\n**File Structure**\n\n```\nproject-root/\n  cmd/\n    dashboard-server/\n      main.go                     ← Dashboard server entry point\n  internal/\n    dashboard/\n      server.go                   ← HTTP server and WebSocket handling\n      config.go                   ← Dashboard configuration management\n      subscription.go             ← Real-time update subscriptions\n      panel.go                    ← Panel rendering and query execution\n      validation.go               ← Configuration validation logic\n      templates.go                ← Variable substitution and templating\n    dashboard/handlers/\n      dashboard_handlers.go       ← REST API for dashboard CRUD operations\n      websocket_handlers.go       ← WebSocket connection management\n      static_handlers.go          ← Static asset serving\n    dashboard/models/\n      dashboard.go                ← Dashboard and Panel data structures\n      subscription.go             ← Subscription and update models\n  web/\n    static/\n      js/\n        dashboard.js              ← Dashboard client-side logic\n        chart-renderer.js         ← Chart rendering with Chart.js\n        websocket-client.js       ← WebSocket communication handling\n      css/\n        dashboard.css             ← Dashboard styling\n      index.html                  ← Main dashboard interface\n  configs/\n    dashboard-server.yaml         ← Dashboard server configuration\n```\n\n**Dashboard Server Infrastructure**\n\n```go\npackage dashboard\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"log/slog\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n\n    \"github.com/gorilla/mux\"\n    \"github.com/gorilla/websocket\"\n)\n\n// DashboardServer manages the web interface and real-time updates for metrics visualization\ntype DashboardServer struct {\n    config         *ServerConfig\n    logger         *slog.Logger\n    queryProcessor QueryProcessor\n    \n    // WebSocket management\n    upgrader       websocket.Upgrader\n    clients        map[string]*WebSocketClient\n    subscriptions  *SubscriptionManager\n    clientsMu      sync.RWMutex\n    \n    // Dashboard storage\n    dashboards     map[string]*Dashboard\n    dashboardsMu   sync.RWMutex\n    \n    server         *http.Server\n    shutdown       chan struct{}\n}\n\n// WebSocketClient represents a connected dashboard client\ntype WebSocketClient struct {\n    ID           string\n    conn         *websocket.Conn\n    send         chan []byte\n    subscriptions map[string]*PanelSubscription\n    mu           sync.RWMutex\n}\n\n// PanelSubscription tracks a client's interest in specific panel data\ntype PanelSubscription struct {\n    PanelID         string\n    Query           string\n    RefreshInterval time.Duration\n    LastUpdate      time.Time\n    TimeRange       TimeRange\n}\n\n// NewDashboardServer creates a new dashboard server instance\nfunc NewDashboardServer(config *ServerConfig, queryProcessor QueryProcessor, logger *slog.Logger) *DashboardServer {\n    return &DashboardServer{\n        config:        config,\n        logger:        logger,\n        queryProcessor: queryProcessor,\n        upgrader: websocket.Upgrader{\n            CheckOrigin: func(r *http.Request) bool { return true }, // Configure appropriately for production\n        },\n        clients:       make(map[string]*WebSocketClient),\n        subscriptions: NewSubscriptionManager(),\n        dashboards:    make(map[string]*Dashboard),\n        shutdown:      make(chan struct{}),\n    }\n}\n\n// Start initializes the dashboard server and begins serving requests\nfunc (ds *DashboardServer) Start() error {\n    // TODO 1: Load existing dashboard configurations from storage\n    // TODO 2: Set up HTTP routes for dashboard CRUD operations and static assets\n    // TODO 3: Initialize WebSocket upgrade handler for real-time connections\n    // TODO 4: Start subscription manager for coordinating panel updates\n    // TODO 5: Begin periodic cleanup of stale connections and subscriptions\n    // TODO 6: Start HTTP server on configured port with timeout settings\n    \n    // Hint: Use gorilla/mux for routing and implement graceful shutdown\n}\n\n// Stop gracefully shuts down the dashboard server\nfunc (ds *DashboardServer) Stop(ctx context.Context) error {\n    // TODO 1: Signal shutdown to all background goroutines\n    // TODO 2: Close all active WebSocket connections with proper close frames\n    // TODO 3: Save any modified dashboard configurations to persistent storage\n    // TODO 4: Shutdown HTTP server with context timeout for graceful connection draining\n    // TODO 5: Wait for all goroutines to complete or context timeout\n}\n```\n\n**WebSocket Connection Management**\n\n```go\n// HandleWebSocketUpgrade processes WebSocket connection requests from dashboard clients\nfunc (ds *DashboardServer) HandleWebSocketUpgrade(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Upgrade HTTP connection to WebSocket using ds.upgrader\n    // TODO 2: Generate unique client ID and create WebSocketClient instance\n    // TODO 3: Register client in ds.clients map with proper locking\n    // TODO 4: Start goroutines for reading client messages and writing updates\n    // TODO 5: Handle client disconnection cleanup in defer statement\n    \n    // Hint: Use separate goroutines for reading and writing to avoid deadlocks\n}\n\n// HandleClientMessage processes incoming messages from WebSocket clients\nfunc (ds *DashboardServer) HandleClientMessage(client *WebSocketClient, messageType int, data []byte) error {\n    // TODO 1: Parse message JSON to determine message type (subscribe, unsubscribe, ping)\n    // TODO 2: For subscribe messages, create PanelSubscription and register with SubscriptionManager\n    // TODO 3: For unsubscribe messages, remove subscription and clean up resources\n    // TODO 4: For ping messages, respond with pong to maintain connection health\n    // TODO 5: Log any parsing errors and send error response to client\n    \n    // Hint: Define message types as constants and use type switches for handling\n}\n\n// BroadcastPanelUpdate sends updated data to all clients subscribed to a specific panel\nfunc (ds *DashboardServer) BroadcastPanelUpdate(panelID string, data *PanelUpdateMessage) error {\n    // TODO 1: Find all clients subscribed to this panel ID\n    // TODO 2: Serialize update message to JSON format\n    // TODO 3: Send message to each client's send channel (non-blocking)\n    // TODO 4: Remove clients with full send channels (indicates slow/disconnected clients)\n    // TODO 5: Log broadcast statistics for monitoring purposes\n    \n    // Hint: Use select with default case for non-blocking channel sends\n}\n```\n\n**Dashboard Configuration Management**\n\n```go\n// LoadDashboard retrieves a dashboard configuration by ID\nfunc (ds *DashboardServer) LoadDashboard(dashboardID string) (*Dashboard, error) {\n    // TODO 1: Check in-memory cache first with read lock\n    // TODO 2: If not cached, load from persistent storage (file or database)\n    // TODO 3: Validate dashboard configuration using validation.go functions\n    // TODO 4: Cache validated dashboard in memory with write lock\n    // TODO 5: Return dashboard or appropriate error if not found/invalid\n    \n    // Hint: Use sync.RWMutex for efficient concurrent access to dashboard cache\n}\n\n// SaveDashboard persists a dashboard configuration\nfunc (ds *DashboardServer) SaveDashboard(dashboard *Dashboard) error {\n    // TODO 1: Validate complete dashboard configuration including all panels\n    // TODO 2: Generate unique ID if this is a new dashboard\n    // TODO 3: Write dashboard JSON to persistent storage with atomic operations\n    // TODO 4: Update in-memory cache with write lock\n    // TODO 5: Notify any clients viewing this dashboard about configuration changes\n    \n    // Hint: Use temporary files and atomic rename for safe persistence\n}\n\n// ValidateDashboard checks dashboard configuration for correctness\nfunc (ds *DashboardServer) ValidateDashboard(dashboard *Dashboard) error {\n    // TODO 1: Validate required fields (ID, Title) are non-empty\n    // TODO 2: Check TimeRange values are reasonable (From before To, not too far in past/future)\n    // TODO 3: Validate each panel configuration including GridPosition bounds\n    // TODO 4: Parse and validate all panel queries using query engine\n    // TODO 5: Check for panel ID uniqueness within the dashboard\n    \n    // Hint: Return ValidationErrors with specific field-level error messages\n}\n```\n\n**Client-Side Dashboard Implementation**\n\n```javascript\n// web/static/js/dashboard.js\n\nclass DashboardClient {\n    constructor(dashboardId) {\n        this.dashboardId = dashboardId;\n        this.panels = new Map();\n        this.websocket = null;\n        this.reconnectAttempts = 0;\n        this.maxReconnectAttempts = 5;\n        this.reconnectDelay = 1000; // Start with 1 second delay\n    }\n\n    // Initialize dashboard and establish WebSocket connection\n    async initialize() {\n        // TODO 1: Load dashboard configuration from REST API\n        // TODO 2: Create and render all panels based on configuration\n        // TODO 3: Establish WebSocket connection for real-time updates\n        // TODO 4: Subscribe to updates for all visible panels\n        // TODO 5: Set up periodic connection health checks\n        \n        // Hint: Use fetch() for REST calls and native WebSocket API\n    }\n\n    // Handle incoming WebSocket messages with panel updates\n    handleWebSocketMessage(event) {\n        // TODO 1: Parse JSON message and determine message type\n        // TODO 2: For panel updates, find corresponding panel and update data\n        // TODO 3: For error messages, display user-friendly error in affected panel\n        // TODO 4: For connection status changes, update UI connection indicator\n        // TODO 5: Log any parsing errors to browser console for debugging\n        \n        // Hint: Use try-catch around JSON.parse() for robust error handling\n    }\n\n    // Reconnect WebSocket with exponential backoff\n    reconnectWebSocket() {\n        // TODO 1: Check if reconnection attempts exceed maximum limit\n        // TODO 2: Calculate delay using exponential backoff with jitter\n        // TODO 3: Schedule reconnection attempt using setTimeout\n        // TODO 4: Reset reconnect counter on successful connection\n        // TODO 5: Update UI to show connection status during reconnection\n        \n        // Hint: Use Math.random() for jitter to avoid thundering herd\n    }\n}\n\nclass DashboardPanel {\n    constructor(panelConfig, container) {\n        this.config = panelConfig;\n        this.container = container;\n        this.chart = null;\n        this.lastUpdate = null;\n    }\n\n    // Render panel with Chart.js or similar library\n    render() {\n        // TODO 1: Create canvas element for chart rendering\n        // TODO 2: Initialize Chart.js with panel-specific configuration\n        // TODO 3: Configure chart options based on panel type (line, bar, etc.)\n        // TODO 4: Set up chart update methods for real-time data\n        // TODO 5: Handle panel resize events for responsive layout\n        \n        // Hint: Use Chart.js responsive configuration for automatic resizing\n    }\n\n    // Update panel with new data from WebSocket\n    updateData(newData) {\n        // TODO 1: Validate incoming data format matches panel expectations\n        // TODO 2: Merge new data with existing chart dataset\n        // TODO 3: Update chart using Chart.js update() method\n        // TODO 4: Update last update timestamp for debugging\n        // TODO 5: Handle any rendering errors gracefully with fallback display\n        \n        // Hint: Use chart.update('none') for performance when data changes frequently\n    }\n}\n```\n\n**Milestone Checkpoint**\n\nAfter implementing the dashboard visualization system, verify the following behaviors:\n\n1. **Dashboard Configuration**: Create a simple dashboard JSON file with 2-3 panels. Load it through the REST API and verify all panels render correctly with proper positioning.\n\n2. **Real-Time Updates**: Start the dashboard server, open a dashboard in a browser, and inject new metrics through the ingestion API. Verify that charts update automatically without page refresh within the configured refresh interval.\n\n3. **WebSocket Connectivity**: Use browser developer tools to monitor WebSocket traffic. Verify that subscription messages are sent when dashboards load and that panel updates arrive as JSON messages.\n\n4. **Multiple Clients**: Open the same dashboard in multiple browser tabs or different browsers. Verify that all clients receive updates simultaneously and that server resource usage remains reasonable.\n\n5. **Error Handling**: Stop the metrics ingestion engine while keeping the dashboard open. Verify that panels display appropriate error states and recover automatically when ingestion resumes.\n\nExpected command output for successful implementation:\n```bash\n$ go run cmd/dashboard-server/main.go\n2024/01/15 10:30:00 INFO Dashboard server starting port=3000\n2024/01/15 10:30:00 INFO WebSocket handler registered endpoint=/ws\n2024/01/15 10:30:00 INFO Static assets served path=/static/\n2024/01/15 10:30:01 INFO Dashboard server ready for connections\n\n# In another terminal, test dashboard loading\n$ curl http://localhost:3000/api/dashboards/test-dashboard\n{\"id\":\"test-dashboard\",\"title\":\"System Overview\",\"panels\":[...]}\n```\n\n**Debugging Tips**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Charts not updating in real-time | WebSocket connection not established | Check browser console for WebSocket errors | Verify WebSocket endpoint URL and CORS configuration |\n| High CPU usage during updates | Too frequent refresh intervals | Monitor WebSocket message frequency | Implement adaptive refresh rates based on client performance |\n| Panels showing \"No Data\" | Query execution errors | Check server logs for query parsing failures | Validate panel queries against available metrics |\n| Dashboard layout broken | Grid position conflicts | Check panel GridPosition values for overlaps | Implement layout validation during dashboard save |\n| Memory leaks in browser | Chart.js instances not destroyed | Monitor browser memory usage over time | Call chart.destroy() when panels are removed |\n| WebSocket connections dropping | Network issues or server overload | Check WebSocket close codes and server metrics | Implement exponential backoff reconnection logic |\n\n\n## Alerting System\n\n> **Milestone(s):** 4 (Alerting System) - This section details the alerting system that monitors metric data in real-time, evaluates threshold conditions, manages alert states, and delivers notifications through multiple channels\n\n### Mental Model: The Security Guard System\n\nThink of the alerting system as a sophisticated security monitoring service that guards a large corporate campus. Just like a security system, our alerting infrastructure operates on multiple layers of vigilance and response.\n\nThe **security guards** are our `AlertEvaluator` instances - they patrol their assigned zones (metric queries) on regular schedules, constantly checking for anomalies. Each guard has a **patrol route** (alert rule expression) that defines which areas to monitor and what constitutes suspicious activity (threshold conditions). When a guard notices something unusual - say, a door that should be locked is open (CPU usage above 90%) - they don't immediately sound the alarm. Instead, they watch carefully for a specified duration to ensure it's not a false alarm (evaluation period).\n\nThe **dispatch center** is our alert state management system. When guards report suspicious activity, the dispatch center tracks the evolving situation through distinct phases: **investigating** (pending state), **confirmed incident** (firing state), and **all clear** (resolved state). The dispatch center maintains detailed incident logs, ensuring that every status change is properly documented and that appropriate personnel are notified at each stage.\n\nThe **communication network** represents our notification channels - some incidents require immediate phone calls to on-duty personnel (PagerDuty for critical alerts), while others might warrant email reports to the facilities team (email for warnings), and some situations need real-time updates in the security chat room (Slack integration). Each communication channel has its own protocols, message formats, and delivery confirmation requirements.\n\nMost importantly, the system includes **intelligent filtering** to prevent alert fatigue - just like how a security system shouldn't wake the entire building every time a cat triggers a motion sensor. This involves proper alert prioritization, silence periods during maintenance, and sophisticated grouping to avoid overwhelming responders with duplicate notifications.\n\nThis mental model helps us understand that alerting isn't just about detecting problems - it's about managing the entire lifecycle of incident awareness, from initial detection through resolution, while maintaining the trust and attention of the people who respond to these alerts.\n\n![Alert State Machine](./diagrams/alert-state-machine.svg)\n\n### Alert Rule Definition and Evaluation\n\nAlert rules form the foundation of our monitoring system, defining the specific conditions that warrant human attention. Each rule represents a formal specification of what constitutes an abnormal system state and how the system should respond when that condition occurs.\n\n> **Decision: Alert Rule Data Model**\n> - **Context**: We need a standardized way to define monitoring conditions that can be evaluated consistently across the system while providing sufficient flexibility for diverse monitoring scenarios.\n> - **Options Considered**: Simple threshold-only rules, complex multi-condition rules with Boolean logic, time-series anomaly detection rules\n> - **Decision**: Structured threshold-based rules with duration conditions and metadata annotations\n> - **Rationale**: Threshold-based rules are intuitive for operators, predictable in behavior, and can handle 80% of common monitoring scenarios. Duration conditions prevent alert flapping from brief spikes.\n> - **Consequences**: Simple to implement and understand, but may require multiple rules for complex scenarios. Future extensions can add advanced detection methods.\n\nThe `AlertRule` structure captures all essential information needed to evaluate and manage alerts:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `ID` | `string` | Unique identifier for the alert rule, used for tracking and referencing |\n| `Name` | `string` | Human-readable name displayed in notifications and dashboards |\n| `Expression` | `string` | Metric query expression that returns the value to evaluate against threshold |\n| `Threshold` | `float64` | Numeric value that triggers the alert when comparison condition is met |\n| `Operator` | `string` | Comparison operator: \"gt\" (greater than), \"lt\" (less than), \"eq\" (equal), \"ne\" (not equal) |\n| `Duration` | `time.Duration` | Minimum time the condition must persist before transitioning to firing state |\n| `EvaluationInterval` | `time.Duration` | How frequently this rule should be evaluated against current metric data |\n| `Labels` | `Labels` | Key-value pairs attached to alerts generated by this rule for routing and grouping |\n| `Annotations` | `map[string]string` | Additional metadata for alert descriptions, runbook links, and context information |\n\nAlert rule expressions leverage our query engine to retrieve metric data for evaluation. The expression syntax follows the same patterns established in our query processor, enabling complex aggregations and transformations. For example, an expression might be `rate(http_requests_total[5m])` to monitor request rate over a 5-minute window, or `avg(cpu_usage) by (instance)` to track average CPU usage per server instance.\n\n> **Decision: Duration-Based Alert Evaluation**\n> - **Context**: Raw metric values can be noisy, leading to false alerts from brief spikes or temporary network issues.\n> - **Options Considered**: Immediate alerting on threshold breach, duration-based evaluation, statistical outlier detection\n> - **Decision**: Require conditions to persist for a specified duration before firing alerts\n> - **Rationale**: Duration thresholds dramatically reduce false positives while adding minimal delay for genuine issues. This matches operational intuition that brief spikes are usually not actionable.\n> - **Consequences**: Alerts may take longer to fire, but operator confidence in alert validity increases significantly.\n\nThe alert evaluation process follows a carefully orchestrated sequence:\n\n1. **Rule Scheduling**: The `AlertEvaluator` maintains an internal scheduler that tracks the next evaluation time for each active rule. Rules with shorter evaluation intervals receive priority scheduling to ensure timely detection of rapidly changing conditions.\n\n2. **Query Execution**: When a rule's evaluation time arrives, the evaluator constructs a query request using the rule's expression and submits it to our `QueryProcessor`. The query includes the current timestamp and any necessary lookback window for rate calculations or aggregations.\n\n3. **Threshold Comparison**: The evaluator extracts the numeric result from the query response and applies the specified comparison operator against the configured threshold. For queries returning multiple time series (e.g., per-instance metrics), each series is evaluated independently.\n\n4. **Duration Tracking**: The system maintains state for each rule-series combination, recording when threshold conditions first became true. Only after the condition persists for the full duration period does the alert transition to firing state.\n\n5. **State Transition**: Based on current conditions and historical state, the evaluator determines appropriate state transitions and triggers any necessary notifications or state persistence operations.\n\nThe `AlertEvaluator` interface defines the core evaluation contract:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `EvaluateRules` | `ctx context.Context` | `error` | Processes all active alert rules, evaluating conditions and updating states |\n| `UpdateAlertState` | `ruleID string, seriesLabels Labels, newState AlertState` | `error` | Persists alert state changes and triggers appropriate notifications |\n| `GetActiveAlerts` | `ctx context.Context` | `[]ActiveAlert, error` | Retrieves currently firing or pending alerts for dashboard display |\n| `SilenceAlert` | `ctx context.Context, ruleID string, duration time.Duration` | `error` | Temporarily suppresses notifications for specified alert during maintenance |\n\n⚠️ **Pitfall: Evaluation Interval vs Duration Confusion**\nA common mistake is setting the evaluation interval longer than the alert duration. If a rule has a 30-second duration but evaluates every 60 seconds, the alert can never fire because the condition never persists long enough between evaluations. Always ensure evaluation interval ≤ duration/2 for reliable detection.\n\n⚠️ **Pitfall: Query Timeout Handling**\nAlert evaluation queries must complete within the evaluation interval to maintain scheduling accuracy. Implement proper timeout handling and degraded operation modes when the query engine is overloaded. Consider using cached or approximate results rather than skipping evaluations entirely.\n\n![Alert Notification Flow](./diagrams/notification-flow.svg)\n\n### Alert State Management\n\nAlert state management represents one of the most critical aspects of reliable alerting systems. Unlike simple binary on/off switches, production alerting requires sophisticated state tracking that accounts for the temporal nature of system problems and the human workflow of incident response.\n\n> **Decision: Four-State Alert Model**\n> - **Context**: Alerts need to represent the lifecycle of problems from initial detection through resolution, while avoiding false positives from brief anomalies.\n> - **Options Considered**: Binary firing/resolved model, three-state pending/firing/resolved model, four-state model with explicit inactive state\n> - **Decision**: Four distinct states: Inactive, Pending, Firing, and Resolved with explicit transition rules\n> - **Rationale**: Four states provide clear semantics for each phase of alert lifecycle while supporting duration-based evaluation and proper resolution tracking.\n> - **Consequences**: More complex state management logic, but significantly better operator experience and reduced alert fatigue.\n\nThe alert state machine governs all transitions between states with strict rules that ensure consistency and predictable behavior:\n\n| Current State | Condition | Next State | Actions Taken |\n|---------------|-----------|------------|---------------|\n| `Inactive` | Threshold breached | `Pending` | Start duration timer, no notifications sent |\n| `Pending` | Duration timer expires while condition persists | `Firing` | Send firing notification, update alert database |\n| `Pending` | Condition returns to normal before duration expires | `Inactive` | Reset duration timer, no notifications sent |\n| `Firing` | Condition returns to normal | `Resolved` | Send resolution notification, log resolution timestamp |\n| `Resolved` | Condition breaches threshold again | `Pending` | Start new duration timer for potential re-firing |\n| `Firing` | Manual silence activated | `Silenced` | Suppress notifications while maintaining state tracking |\n| `Silenced` | Silence period expires | `Firing` or `Resolved` | Resume normal notification behavior based on current condition |\n\nThis state model addresses several critical operational concerns. The **Pending** state prevents false alarms from brief spikes while still tracking that something unusual occurred. The **Resolved** state provides explicit confirmation that problems have cleared, which is essential for incident tracking and post-mortem analysis. The distinction between **Resolved** and **Inactive** ensures that resolution notifications are sent appropriately.\n\nEach alert instance maintains detailed state information:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `AlertID` | `string` | Unique identifier combining rule ID and series labels hash |\n| `RuleID` | `string` | Reference to the alert rule that generated this alert instance |\n| `SeriesLabels` | `Labels` | Specific label combination for this alert instance (e.g., instance=web-01) |\n| `State` | `AlertState` | Current state in the alert lifecycle state machine |\n| `LastEvaluation` | `time.Time` | Timestamp of the most recent rule evaluation for this alert |\n| `StateChanged` | `time.Time` | When the alert last transitioned between states |\n| `FiredAt` | `*time.Time` | When alert transitioned to firing state, nil if never fired |\n| `ResolvedAt` | `*time.Time` | When alert transitioned to resolved state, nil if not resolved |\n| `Value` | `float64` | Current metric value that was compared against threshold |\n| `ActiveSince` | `*time.Time` | When condition first became true (pending state start) |\n| `Annotations` | `map[string]string` | Resolved template variables and additional context |\n\nState transitions are not immediate but follow a careful evaluation process that considers timing constraints and current conditions. When evaluating a rule, the system must account for:\n\n**Timing Considerations**: Each state transition has specific timing requirements. The pending-to-firing transition requires the condition to persist for the full duration period. Resolution detection should happen quickly to minimize notification delay, but must be confirmed across multiple evaluation cycles to avoid flapping.\n\n**Multi-Series Handling**: Rules that return multiple time series (such as per-instance metrics) require independent state tracking for each series. An alert rule monitoring CPU usage across 10 servers creates 10 separate alert instances, each with its own state machine and notification lifecycle.\n\n**Persistence Requirements**: Alert state must survive system restarts and component failures. The state management system implements write-ahead logging for all state transitions, ensuring that no alert state is lost even if the alerting system crashes during evaluation.\n\n> **Decision: Alert State Persistence Strategy**\n> - **Context**: Alert state must survive system restarts while maintaining high write throughput during normal operations.\n> - **Options Considered**: In-memory only with periodic snapshots, database with transactions, append-only log with periodic compaction\n> - **Decision**: Hybrid approach with in-memory state and write-ahead log for durability\n> - **Rationale**: In-memory operations provide fast evaluation performance, while WAL ensures durability without requiring complex database transactions.\n> - **Consequences**: Fast evaluation performance with strong durability guarantees, but requires careful recovery logic during startup.\n\nThe state management system implements several sophisticated features to handle edge cases:\n\n**Flapping Detection**: When alerts rapidly transition between firing and resolved states, the system implements exponential backoff for notifications and marks the alert as flapping. This prevents notification storms while maintaining visibility into unstable conditions.\n\n**Staleness Handling**: If metric data becomes unavailable (e.g., due to application crashes), the system implements configurable staleness policies. Alerts can be automatically resolved after a timeout period, or marked as \"insufficient data\" to distinguish from genuine resolution.\n\n**Group Notifications**: Related alerts can be grouped together to reduce notification volume. For example, if 50 servers all lose network connectivity simultaneously, operators receive one grouped notification rather than 50 individual alerts.\n\n⚠️ **Pitfall: State Transition Concurrency**\nAlert state updates must be atomic to prevent race conditions when multiple evaluation goroutines update the same alert. Always use proper locking or atomic operations when modifying alert state, and ensure state transitions are logged before notifications are sent.\n\n⚠️ **Pitfall: Resolution Detection Delay**\nSetting evaluation intervals too long can delay resolution detection, leaving alerts in firing state long after problems clear. For critical alerts, consider using shorter evaluation intervals or implementing separate resolution checking with faster cycles.\n\n### Notification Channel Integration\n\nThe notification delivery system serves as the crucial bridge between alert detection and human response. A sophisticated notification system must handle multiple delivery channels, message formatting, delivery confirmation, and failure recovery while maintaining operator trust through reliable and well-formatted communications.\n\n> **Decision: Multi-Channel Notification Architecture**\n> - **Context**: Different alerts require different notification urgency and routing, and organizations use diverse communication tools for incident response.\n> - **Options Considered**: Single notification channel, plug-in architecture for channels, hardcoded integrations for common services\n> - **Decision**: Unified notification interface with pluggable channel implementations and intelligent routing\n> - **Rationale**: Pluggable architecture allows easy extension while unified interface ensures consistent behavior. Intelligent routing enables escalation policies and channel selection based on alert severity.\n> - **Consequences**: More complex initial implementation, but highly flexible and maintainable for diverse organizational needs.\n\nThe `NotificationChannel` structure provides a standardized interface for all delivery methods:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Type` | `string` | Channel type identifier: \"email\", \"slack\", \"pagerduty\", \"webhook\" |\n| `Name` | `string` | Human-readable channel name for configuration and debugging |\n| `Config` | `map[string]string` | Channel-specific configuration parameters (API keys, URLs, etc.) |\n| `Enabled` | `bool` | Whether this channel is currently active for notification delivery |\n| `RateLimitBurst` | `int` | Maximum notifications that can be sent in rapid succession |\n| `RateLimitPeriod` | `time.Duration` | Time window for rate limiting calculations |\n| `RetryConfig` | `RetryConfig` | Backoff and retry policies for failed delivery attempts |\n| `MessageTemplate` | `string` | Go template string for formatting notification messages |\n| `Filters` | `[]NotificationFilter` | Rules determining which alerts should use this channel |\n\nEach notification channel type implements a common interface that abstracts delivery details:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `SendNotification` | `ctx context.Context, alert Alert, message NotificationMessage` | `error` | Delivers notification through this channel with delivery confirmation |\n| `ValidateConfig` | `config map[string]string` | `error` | Checks channel configuration for required parameters and connectivity |\n| `HealthCheck` | `ctx context.Context` | `error` | Verifies channel availability and authentication status |\n| `FormatMessage` | `alert Alert, template string` | `NotificationMessage, error` | Applies message templating to generate channel-specific content |\n\n**Email Notification Implementation**: Email channels support both plain text and HTML message formats with embedded metric charts and alert context. The implementation handles SMTP authentication, TLS encryption, and delivery status tracking. Email notifications include alert details, direct links to relevant dashboards, and suggested runbook procedures.\n\n**Slack Integration**: Slack notifications leverage webhook integrations or bot API tokens to deliver rich formatted messages with interactive elements. Messages include color-coded severity indicators, metric snapshots, and action buttons for alert acknowledgment or silencing. The system supports both channel posting and direct message delivery based on alert routing rules.\n\n**PagerDuty Integration**: For critical alerts requiring immediate response, PagerDuty integration creates incidents with proper escalation policies. The system maps alert severity levels to PagerDuty incident priorities and includes detailed context in incident descriptions. Auto-resolution is supported when alerts return to normal state.\n\n**Webhook Channels**: Generic webhook support enables integration with custom tooling and services not directly supported. Webhook notifications include configurable payload formatting, authentication headers, and retry logic with exponential backoff.\n\nMessage templating provides flexible content customization while ensuring consistent formatting across channels:\n\n```\nAlert: {{ .Alert.RuleName }}\nStatus: {{ .Alert.State }}\nInstance: {{ .Alert.Labels.instance }}\nValue: {{ .Alert.Value }} (threshold: {{ .Alert.Threshold }})\nStarted: {{ .Alert.ActiveSince.Format \"2006-01-02 15:04:05\" }}\nRunbook: {{ .Alert.Annotations.runbook_url }}\nDashboard: https://dashboard.example.com/d/{{ .Alert.Annotations.dashboard_id }}\n```\n\nThe notification routing system determines which channels receive each alert based on configurable rules:\n\n| Filter Type | Parameters | Description |\n|-------------|------------|-------------|\n| `LabelFilter` | `label string, values []string` | Route alerts with specific label values to designated channels |\n| `SeverityFilter` | `min_severity string, max_severity string` | Channel receives alerts within specified severity range |\n| `TimeFilter` | `start_time string, end_time string, timezone string` | Time-based routing for business hours vs. on-call coverage |\n| `TeamFilter` | `teams []string` | Route alerts to channels based on team ownership labels |\n| `EscalationFilter` | `delay time.Duration` | Secondary channels activated after specified delay if alert persists |\n\n**Delivery Reliability**: The notification system implements sophisticated retry logic to handle transient failures. Each channel maintains a delivery queue with exponential backoff for failed attempts. Critical notifications use multiple delivery attempts across different time intervals, and persistent failures trigger fallback notification channels.\n\n**Rate Limiting**: To prevent notification storms during widespread outages, each channel implements configurable rate limiting. The system uses token bucket algorithms to allow burst notifications while preventing sustained high-volume delivery that could overwhelm recipients or trigger service quotas.\n\n**Delivery Confirmation**: Where supported by the underlying service, the system tracks delivery confirmation and maintains metrics on notification success rates. Failed deliveries are logged for troubleshooting, and persistent delivery failures can trigger alerts about the alerting system itself.\n\n⚠️ **Pitfall: Template Rendering Errors**\nNotification templates can fail to render if alert data is missing expected fields or contains invalid characters. Always validate templates during configuration and implement fallback rendering with basic alert information when template processing fails.\n\n⚠️ **Pitfall: Notification Loop Prevention**\nBe careful not to create notification loops where alerts about the alerting system itself trigger recursive notifications. Implement special handling for infrastructure alerts and avoid routing alerting system failures through the same notification channels.\n\n⚠️ **Pitfall: Credential Management**\nNotification channels require API keys and credentials that must be securely stored and rotated. Never log sensitive configuration values, implement proper secret management, and provide clear error messages when authentication fails without exposing credentials.\n\n### Implementation Guidance\n\nThis section provides concrete implementation details for building the alerting system in Go, bridging the gap between the design concepts and working code.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Alert Storage | In-memory with JSON file persistence | BadgerDB embedded database |\n| Notification Queue | Go channels with worker pools | Redis with persistent queues |\n| HTTP Client | net/http with custom retry logic | go-retryablehttp library |\n| Message Templating | text/template standard library | Advanced templating with sprig functions |\n| State Persistence | JSON files with atomic writes | Write-ahead log with periodic snapshots |\n| Scheduling | time.Ticker with goroutines | Cron-like scheduler with distributed coordination |\n\n#### File Structure\n\n```\nproject-root/\n  cmd/server/main.go                    ← server entry point\n  internal/alerting/                    ← alerting system implementation\n    evaluator.go                        ← core alert evaluation logic\n    evaluator_test.go                   ← evaluation engine tests\n    state_manager.go                    ← alert state tracking\n    notification_manager.go             ← notification delivery coordination\n    channels/                           ← notification channel implementations\n      email.go                          ← email notification channel\n      slack.go                          ← Slack integration\n      webhook.go                        ← generic webhook channel\n      pagerduty.go                      ← PagerDuty integration\n    rules/                              ← alert rule management\n      parser.go                         ← rule configuration parsing\n      validator.go                      ← rule validation logic\n    templates/                          ← notification templates\n      default.tmpl                      ← default message template\n      slack.tmpl                        ← Slack-specific template\n  internal/storage/                     ← shared storage interfaces\n    interfaces.go                       ← storage contracts\n  pkg/config/                           ← configuration management\n    alerting.go                         ← alerting configuration structures\n```\n\n#### Alert State Management Infrastructure\n\n```go\npackage alerting\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\n// AlertState represents the current state of an alert in the lifecycle\ntype AlertState int\n\nconst (\n    AlertStateInactive AlertState = iota\n    AlertStatePending\n    AlertStateFiring\n    AlertStateResolved\n    AlertStateSilenced\n)\n\n// AlertInstance tracks the state and history of a specific alert\ntype AlertInstance struct {\n    AlertID         string                 `json:\"alert_id\"`\n    RuleID          string                 `json:\"rule_id\"`\n    SeriesLabels    Labels                 `json:\"series_labels\"`\n    State           AlertState             `json:\"state\"`\n    LastEvaluation  time.Time              `json:\"last_evaluation\"`\n    StateChanged    time.Time              `json:\"state_changed\"`\n    FiredAt         *time.Time             `json:\"fired_at,omitempty\"`\n    ResolvedAt      *time.Time             `json:\"resolved_at,omitempty\"`\n    Value           float64                `json:\"value\"`\n    ActiveSince     *time.Time             `json:\"active_since,omitempty\"`\n    Annotations     map[string]string      `json:\"annotations\"`\n    NotificationLog []NotificationRecord   `json:\"notification_log\"`\n}\n\n// NotificationRecord tracks when and how notifications were delivered\ntype NotificationRecord struct {\n    Timestamp   time.Time `json:\"timestamp\"`\n    Channel     string    `json:\"channel\"`\n    State       AlertState `json:\"state\"`\n    Success     bool      `json:\"success\"`\n    ErrorMsg    string    `json:\"error_msg,omitempty\"`\n    RetryCount  int       `json:\"retry_count\"`\n}\n\n// StateManager handles alert state transitions and persistence\ntype StateManager struct {\n    alerts      map[string]*AlertInstance\n    mu          sync.RWMutex\n    persistence StatePersistence\n    logger      *slog.Logger\n}\n\n// StatePersistence interface for alert state durability\ntype StatePersistence interface {\n    SaveState(alertID string, state *AlertInstance) error\n    LoadState(alertID string) (*AlertInstance, error)\n    LoadAllStates() (map[string]*AlertInstance, error)\n    DeleteState(alertID string) error\n}\n\n// NewStateManager creates a new alert state manager with persistence\nfunc NewStateManager(persistence StatePersistence, logger *slog.Logger) *StateManager {\n    return &StateManager{\n        alerts:      make(map[string]*AlertInstance),\n        persistence: persistence,\n        logger:      logger,\n    }\n}\n\n// UpdateAlertState processes alert state transitions with proper validation\nfunc (sm *StateManager) UpdateAlertState(ctx context.Context, ruleID string, seriesLabels Labels, currentValue float64, threshold float64, conditionMet bool, rule *AlertRule) error {\n    // TODO 1: Generate unique alert ID from rule ID and series labels\n    // TODO 2: Acquire write lock and get current alert state\n    // TODO 3: Determine new state based on current state and condition\n    // TODO 4: Validate state transition is legal according to state machine\n    // TODO 5: Update alert instance with new state and metadata\n    // TODO 6: Persist state change to storage\n    // TODO 7: Release lock and trigger notifications if state changed\n    // Hint: Use SeriesID() method to generate consistent alert identifiers\n    return nil\n}\n\n// GetActiveAlerts returns all alerts in firing or pending states\nfunc (sm *StateManager) GetActiveAlerts(ctx context.Context) ([]*AlertInstance, error) {\n    // TODO 1: Acquire read lock for safe concurrent access\n    // TODO 2: Iterate through all alerts and filter by active states\n    // TODO 3: Create slice of active alerts for return\n    // TODO 4: Release lock and return filtered results\n    return nil, nil\n}\n\n// CleanupResolvedAlerts removes old resolved alerts from memory and storage\nfunc (sm *StateManager) CleanupResolvedAlerts(ctx context.Context, maxAge time.Duration) error {\n    // TODO 1: Calculate cutoff time based on maxAge parameter\n    // TODO 2: Acquire write lock for alert map modifications\n    // TODO 3: Identify resolved alerts older than cutoff time\n    // TODO 4: Remove alerts from memory and delete from persistence\n    // TODO 5: Log cleanup statistics and release lock\n    return nil\n}\n```\n\n#### Alert Evaluation Engine Core\n\n```go\npackage alerting\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n)\n\n// AlertEvaluator handles the core alert evaluation logic and scheduling\ntype AlertEvaluator struct {\n    queryProcessor   QueryProcessor\n    stateManager     *StateManager\n    ruleManager      *RuleManager\n    notificationMgr  *NotificationManager\n    logger          *slog.Logger\n    stopCh          chan struct{}\n    evaluationStats *EvaluationStats\n}\n\n// EvaluationStats tracks alert evaluation performance metrics\ntype EvaluationStats struct {\n    TotalEvaluations    int64\n    SuccessfulEvaluations int64\n    FailedEvaluations   int64\n    AverageEvaluationTime time.Duration\n    LastEvaluation      time.Time\n}\n\n// NewAlertEvaluator creates a new alert evaluation engine\nfunc NewAlertEvaluator(queryProcessor QueryProcessor, stateManager *StateManager, ruleManager *RuleManager, notificationMgr *NotificationManager, logger *slog.Logger) *AlertEvaluator {\n    return &AlertEvaluator{\n        queryProcessor:  queryProcessor,\n        stateManager:   stateManager,\n        ruleManager:    ruleManager,\n        notificationMgr: notificationMgr,\n        logger:         logger,\n        stopCh:         make(chan struct{}),\n        evaluationStats: &EvaluationStats{},\n    }\n}\n\n// Start begins the alert evaluation loop with proper scheduling\nfunc (ae *AlertEvaluator) Start(ctx context.Context) error {\n    // TODO 1: Initialize evaluation scheduler with configurable intervals\n    // TODO 2: Start background goroutine for continuous evaluation loop\n    // TODO 3: Implement graceful shutdown handling with context cancellation\n    // TODO 4: Set up periodic statistics reporting and health checks\n    // TODO 5: Return any initialization errors\n    // Hint: Use time.NewTicker for regular evaluation scheduling\n    return nil\n}\n\n// EvaluateRules processes all active alert rules against current metric data\nfunc (ae *AlertEvaluator) EvaluateRules(ctx context.Context) error {\n    // TODO 1: Get list of active alert rules from rule manager\n    // TODO 2: Iterate through each rule and check if evaluation is due\n    // TODO 3: Execute rule query expression against query processor\n    // TODO 4: Extract numeric results and compare against thresholds\n    // TODO 5: Update alert state through state manager\n    // TODO 6: Trigger notifications for state changes\n    // TODO 7: Update evaluation statistics and handle any errors\n    // Hint: Handle multi-series query results by evaluating each series independently\n    return nil\n}\n\n// evaluateRule processes a single alert rule against current metric data\nfunc (ae *AlertEvaluator) evaluateRule(ctx context.Context, rule *AlertRule) error {\n    // TODO 1: Build query request with rule expression and current time\n    // TODO 2: Execute query and handle timeout/error conditions\n    // TODO 3: Parse query results into individual time series\n    // TODO 4: For each series, extract current value and labels\n    // TODO 5: Apply threshold comparison using rule operator\n    // TODO 6: Update alert state with new condition and value\n    // TODO 7: Log evaluation results and any errors encountered\n    return nil\n}\n\n// Stop gracefully shuts down the alert evaluator\nfunc (ae *AlertEvaluator) Stop(ctx context.Context) error {\n    // TODO 1: Signal stop to evaluation loop via channel\n    // TODO 2: Wait for current evaluations to complete\n    // TODO 3: Clean up any pending notifications\n    // TODO 4: Log final evaluation statistics\n    close(ae.stopCh)\n    return nil\n}\n```\n\n#### Notification Delivery System\n\n```go\npackage alerting\n\nimport (\n    \"bytes\"\n    \"context\"\n    \"text/template\"\n    \"time\"\n)\n\n// NotificationManager coordinates notification delivery across multiple channels\ntype NotificationManager struct {\n    channels    map[string]NotificationChannel\n    templates   map[string]*template.Template\n    deliveryQueue chan *NotificationRequest\n    workers     int\n    logger      *slog.Logger\n}\n\n// NotificationRequest represents a pending notification delivery\ntype NotificationRequest struct {\n    Alert        *AlertInstance\n    Channels     []string\n    State        AlertState\n    Timestamp    time.Time\n    RetryCount   int\n    MaxRetries   int\n}\n\n// NotificationChannel interface for all notification delivery methods\ntype NotificationChannel interface {\n    SendNotification(ctx context.Context, alert *AlertInstance, message string) error\n    ValidateConfig(config map[string]string) error\n    HealthCheck(ctx context.Context) error\n    FormatMessage(alert *AlertInstance, template string) (string, error)\n    Name() string\n    Type() string\n}\n\n// NewNotificationManager creates a notification manager with configured channels\nfunc NewNotificationManager(channels []NotificationChannel, workers int, logger *slog.Logger) *NotificationManager {\n    mgr := &NotificationManager{\n        channels:      make(map[string]NotificationChannel),\n        templates:     make(map[string]*template.Template),\n        deliveryQueue: make(chan *NotificationRequest, 1000),\n        workers:       workers,\n        logger:        logger,\n    }\n    \n    // Register notification channels\n    for _, channel := range channels {\n        mgr.channels[channel.Name()] = channel\n    }\n    \n    return mgr\n}\n\n// SendNotification queues alert notification for delivery through specified channels\nfunc (nm *NotificationManager) SendNotification(ctx context.Context, alert *AlertInstance, channels []string) error {\n    // TODO 1: Validate that requested channels are configured and healthy\n    // TODO 2: Create notification request with alert details and channel list\n    // TODO 3: Apply any routing rules or filters for channel selection\n    // TODO 4: Queue notification request for background delivery\n    // TODO 5: Return immediate acknowledgment or queueing errors\n    // Hint: Non-blocking queue insertion to avoid deadlocks during high load\n    return nil\n}\n\n// Start begins notification delivery workers\nfunc (nm *NotificationManager) Start(ctx context.Context) error {\n    // TODO 1: Start configured number of worker goroutines\n    // TODO 2: Each worker processes delivery queue continuously\n    // TODO 3: Implement graceful shutdown with context cancellation\n    // TODO 4: Set up delivery retry logic with exponential backoff\n    // TODO 5: Track delivery statistics and health metrics\n    return nil\n}\n\n// deliverNotification handles actual delivery to a specific channel\nfunc (nm *NotificationManager) deliverNotification(ctx context.Context, req *NotificationRequest, channelName string) error {\n    // TODO 1: Get notification channel by name from registered channels\n    // TODO 2: Select appropriate message template based on channel type\n    // TODO 3: Render notification message using template and alert data\n    // TODO 4: Attempt delivery through channel with timeout handling\n    // TODO 5: Record delivery attempt result and implement retry logic\n    // TODO 6: Update notification log in alert instance\n    return nil\n}\n\n// LoadTemplates reads notification templates from configuration\nfunc (nm *NotificationManager) LoadTemplates(templateDir string) error {\n    // TODO 1: Scan template directory for .tmpl files\n    // TODO 2: Parse each template file using text/template\n    // TODO 3: Validate template syntax and required variables\n    // TODO 4: Store parsed templates in manager template map\n    // TODO 5: Set up template reloading on configuration changes\n    return nil\n}\n\n// GetDeliveryStats returns notification delivery statistics\nfunc (nm *NotificationManager) GetDeliveryStats() *DeliveryStats {\n    // TODO 1: Aggregate delivery statistics from all channels\n    // TODO 2: Calculate success rates and average delivery times\n    // TODO 3: Return structured statistics for monitoring\n    return nil\n}\n```\n\n#### Email Notification Channel Implementation\n\n```go\npackage channels\n\nimport (\n    \"context\"\n    \"crypto/tls\"\n    \"fmt\"\n    \"net/smtp\"\n    \"strings\"\n    \"time\"\n)\n\n// EmailChannel implements notification delivery via SMTP email\ntype EmailChannel struct {\n    name     string\n    config   EmailConfig\n    auth     smtp.Auth\n    client   *smtp.Client\n    logger   *slog.Logger\n}\n\n// EmailConfig holds SMTP configuration parameters\ntype EmailConfig struct {\n    SMTPHost     string `yaml:\"smtp_host\"`\n    SMTPPort     int    `yaml:\"smtp_port\"`\n    Username     string `yaml:\"username\"`\n    Password     string `yaml:\"password\"`\n    FromAddress  string `yaml:\"from_address\"`\n    ToAddresses  []string `yaml:\"to_addresses\"`\n    UseTLS       bool   `yaml:\"use_tls\"`\n    Subject      string `yaml:\"subject\"`\n}\n\n// NewEmailChannel creates a new email notification channel\nfunc NewEmailChannel(name string, config EmailConfig, logger *slog.Logger) (*EmailChannel, error) {\n    // TODO 1: Validate SMTP configuration parameters\n    // TODO 2: Set up SMTP authentication with provided credentials\n    // TODO 3: Test SMTP connection to validate configuration\n    // TODO 4: Initialize email channel with working SMTP client\n    // TODO 5: Return configured channel or connection errors\n    return nil, nil\n}\n\n// SendNotification delivers alert notification via email\nfunc (ec *EmailChannel) SendNotification(ctx context.Context, alert *AlertInstance, message string) error {\n    // TODO 1: Format email subject using alert data and configuration\n    // TODO 2: Build complete email message with headers and body\n    // TODO 3: Connect to SMTP server with TLS if configured\n    // TODO 4: Authenticate using stored credentials\n    // TODO 5: Send email to all configured recipients\n    // TODO 6: Handle SMTP errors and implement retry logic\n    // Hint: Use net/smtp package with proper TLS configuration\n    return nil\n}\n\n// ValidateConfig checks email channel configuration for required fields\nfunc (ec *EmailChannel) ValidateConfig(config map[string]string) error {\n    // TODO 1: Check that SMTP host and port are provided\n    // TODO 2: Validate authentication credentials\n    // TODO 3: Verify from address and recipient list\n    // TODO 4: Test SMTP connectivity if possible\n    // TODO 5: Return detailed validation errors\n    return nil\n}\n\n// FormatMessage renders alert data into email-formatted message\nfunc (ec *EmailChannel) FormatMessage(alert *AlertInstance, tmplStr string) (string, error) {\n    // TODO 1: Parse message template string\n    // TODO 2: Create template data context with alert information\n    // TODO 3: Execute template rendering with alert data\n    // TODO 4: Handle template errors gracefully\n    // TODO 5: Return formatted message or rendering errors\n    return \"\", nil\n}\n\n// HealthCheck verifies SMTP connectivity and authentication\nfunc (ec *EmailChannel) HealthCheck(ctx context.Context) error {\n    // TODO 1: Establish SMTP connection with timeout\n    // TODO 2: Verify authentication credentials\n    // TODO 3: Test basic SMTP commands without sending email\n    // TODO 4: Return health status or connectivity errors\n    return nil\n}\n\nfunc (ec *EmailChannel) Name() string { return ec.name }\nfunc (ec *EmailChannel) Type() string { return \"email\" }\n```\n\n#### Slack Notification Channel Implementation\n\n```go\npackage channels\n\nimport (\n    \"bytes\"\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\n// SlackChannel implements notification delivery via Slack webhooks\ntype SlackChannel struct {\n    name      string\n    webhookURL string\n    channel   string\n    username  string\n    iconEmoji string\n    httpClient *http.Client\n    logger    *slog.Logger\n}\n\n// SlackMessage represents a Slack webhook payload\ntype SlackMessage struct {\n    Channel   string            `json:\"channel,omitempty\"`\n    Username  string            `json:\"username,omitempty\"`\n    IconEmoji string            `json:\"icon_emoji,omitempty\"`\n    Text      string            `json:\"text\"`\n    Attachments []SlackAttachment `json:\"attachments,omitempty\"`\n}\n\n// SlackAttachment provides rich formatting for Slack messages\ntype SlackAttachment struct {\n    Color     string              `json:\"color\"`\n    Title     string              `json:\"title\"`\n    Text      string              `json:\"text\"`\n    Fields    []SlackField        `json:\"fields\"`\n    Timestamp int64               `json:\"ts\"`\n    Footer    string              `json:\"footer\"`\n}\n\n// SlackField represents a field in a Slack attachment\ntype SlackField struct {\n    Title string `json:\"title\"`\n    Value string `json:\"value\"`\n    Short bool   `json:\"short\"`\n}\n\n// NewSlackChannel creates a new Slack notification channel\nfunc NewSlackChannel(name, webhookURL, channel, username, iconEmoji string, logger *slog.Logger) *SlackChannel {\n    return &SlackChannel{\n        name:       name,\n        webhookURL: webhookURL,\n        channel:    channel,\n        username:   username,\n        iconEmoji:  iconEmoji,\n        httpClient: &http.Client{Timeout: 10 * time.Second},\n        logger:     logger,\n    }\n}\n\n// SendNotification delivers alert notification to Slack channel\nfunc (sc *SlackChannel) SendNotification(ctx context.Context, alert *AlertInstance, message string) error {\n    // TODO 1: Build Slack message with proper formatting and attachments\n    // TODO 2: Set color coding based on alert state (red=firing, green=resolved)\n    // TODO 3: Include alert metadata as attachment fields\n    // TODO 4: Serialize message to JSON payload\n    // TODO 5: Send HTTP POST request to Slack webhook URL\n    // TODO 6: Handle HTTP errors and implement retry logic\n    // Hint: Use different colors for different alert states to provide visual cues\n    return nil\n}\n\n// FormatMessage creates Slack-formatted message with rich attachments\nfunc (sc *SlackChannel) FormatMessage(alert *AlertInstance, tmplStr string) (string, error) {\n    // TODO 1: Create Slack attachment with alert details\n    // TODO 2: Add fields for alert name, state, value, and duration\n    // TODO 3: Include links to dashboards and runbooks if available\n    // TODO 4: Format timestamp and alert metadata\n    // TODO 5: Return formatted Slack message structure\n    return \"\", nil\n}\n\n// ValidateConfig verifies Slack channel configuration\nfunc (sc *SlackChannel) ValidateConfig(config map[string]string) error {\n    // TODO 1: Check that webhook URL is provided and valid\n    // TODO 2: Validate channel name format if specified\n    // TODO 3: Test webhook connectivity with a simple message\n    // TODO 4: Return validation errors with helpful messages\n    return nil\n}\n\n// HealthCheck tests Slack webhook connectivity\nfunc (sc *SlackChannel) HealthCheck(ctx context.Context) error {\n    // TODO 1: Send test message to Slack webhook\n    // TODO 2: Verify HTTP response indicates successful delivery\n    // TODO 3: Handle rate limiting and authentication errors\n    // TODO 4: Return health status\n    return nil\n}\n\nfunc (sc *SlackChannel) Name() string { return sc.name }\nfunc (sc *SlackChannel) Type() string { return \"slack\" }\n```\n\n#### Milestone Checkpoints\n\n**Checkpoint 1: Alert Rule Evaluation**\nAfter implementing the basic alert evaluation logic:\n```bash\ngo test ./internal/alerting/...\n```\nExpected behavior:\n- Rules evaluate against mock metric data\n- State transitions work correctly (inactive → pending → firing → resolved)\n- Duration-based firing prevents false alarms\n- Multiple series from single rule create separate alert instances\n\n**Checkpoint 2: Notification Delivery**\nAfter implementing notification channels:\n```bash\ngo run cmd/server/main.go --config=test-config.yaml\n```\nTest manual notification delivery:\n- Send test alert via API: `curl -X POST localhost:8080/api/v1/alerts/test`\n- Verify email delivery to configured SMTP server\n- Check Slack webhook receives properly formatted messages\n- Confirm notification logs are recorded in alert instances\n\n**Checkpoint 3: End-to-End Alerting**\nAfter complete implementation:\n- Configure alert rule monitoring CPU usage > 80%\n- Generate load to trigger threshold breach\n- Verify alert transitions: inactive → pending → firing\n- Confirm notifications sent during state transitions\n- Test alert resolution when condition clears\n- Validate silence functionality during maintenance windows\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|-------------|-----------|-----|\n| Alerts never fire | Duration longer than evaluation interval | Check rule timing configuration | Set evaluation interval ≤ duration/2 |\n| Alert flapping | Threshold too close to normal values | Analyze metric variance over time | Adjust threshold or use averaging |\n| Missing notifications | Notification channel configuration error | Check channel health endpoints | Validate SMTP/webhook credentials |\n| Duplicate notifications | State transition logic error | Review alert state logs | Fix state persistence or locking |\n| Slow evaluation | Expensive query expressions | Profile query execution times | Optimize queries or reduce evaluation frequency |\n| Memory growth | Alert instances not cleaned up | Monitor resolved alert counts | Implement periodic cleanup of old alerts |\n| Notification delays | Delivery queue backlog | Check notification worker metrics | Increase worker pool size or optimize delivery |\n\n\n## Component Interactions and Data Flow\n\n> **Milestone(s):** All milestones (component interactions span the entire system architecture from metrics ingestion through storage, querying, visualization, and alerting)\n\n### The City Infrastructure Metaphor: Understanding System Communication\n\nThink of our metrics system as a modern city's infrastructure. The **metrics ingestion engine** acts like the city's logistics network, receiving shipments from various sources and routing them to appropriate destinations. The **storage engine** functions as the city's warehouses and archives, organizing and preserving goods for future retrieval. The **query engine** operates like the city's information services, helping residents find and access what they need. The **dashboard** serves as the city's command center, providing real-time visibility into city operations. Finally, the **alerting system** acts as the emergency response network, monitoring conditions and coordinating responses when problems arise.\n\nJust as a city's infrastructure components must communicate seamlessly through well-defined protocols and channels, our metrics system components interact through carefully designed APIs, message formats, and data flows. Each component has specific responsibilities but must coordinate with others to achieve the overall system goals. Understanding these interactions is crucial because the system's reliability and performance depend not just on individual component quality, but on how effectively they work together.\n\nThe key insight is that **data flows through the system in predictable patterns**, and each component transformation adds value while maintaining data integrity. When a metric enters the system, it triggers a cascade of interactions: validation, storage, indexing, and potentially alerting. When a user queries data, it initiates a different interaction pattern: query parsing, execution planning, storage retrieval, and result formatting. Understanding these flows helps developers debug issues, optimize performance, and extend functionality.\n\n> **Decision: Event-Driven vs Request-Response Communication**\n> - **Context**: Components need to communicate both for real-time data flow and on-demand operations\n> - **Options Considered**: Pure event-driven messaging, pure request-response APIs, hybrid approach\n> - **Decision**: Hybrid approach with request-response for queries and event-driven for data flow\n> - **Rationale**: Request-response provides immediate feedback for user interactions and error handling, while event-driven enables efficient real-time data processing and decoupling\n> - **Consequences**: Enables both interactive user experience and high-throughput data processing, but requires managing two communication patterns\n\n| Communication Pattern | Use Cases | Implementation | Benefits | Trade-offs |\n|---|---|---|---|---|\n| Request-Response | Dashboard queries, API calls, health checks | HTTP REST APIs with JSON | Immediate feedback, error handling, debugging | Higher latency, tighter coupling |\n| Event-Driven | Metric ingestion, alert notifications, real-time updates | WebSocket connections, internal channels | Low latency, loose coupling, scalability | Complex error handling, eventual consistency |\n| Hybrid | Complete system operations | Both patterns appropriately applied | Best of both worlds | Increased complexity, multiple failure modes |\n\n### Metric Ingestion Flow\n\nThe metric ingestion flow represents the system's primary data entry pipeline, transforming raw metric data from external sources into properly validated, normalized, and stored time-series data. This flow must handle high throughput, validate data integrity, manage cardinality constraints, and ensure durability guarantees.\n\n![Metric Ingestion Data Flow](./diagrams/ingestion-flow.svg)\n\n#### Push-Based Ingestion Process\n\nThe push-based ingestion process begins when external applications send metric data to the system's ingestion endpoints. This follows a multi-stage pipeline designed to validate, process, and store metrics while maintaining high throughput and reliability.\n\n**Stage 1: HTTP Request Reception and Initial Processing**\n\nThe ingestion process starts when the `MetricsIngester` receives an HTTP POST request containing metric data. The HTTP server performs initial request validation, checking content-type headers, request size limits, and authentication tokens if configured. The request body, typically containing a batch of metrics in JSON format, gets parsed into the internal `Metric` data structures.\n\nDuring this stage, the system performs basic request hygiene checks: ensuring the payload size doesn't exceed configured limits (typically 1MB to prevent memory exhaustion), validating the content-type is application/json, and verifying any required authentication headers. If these basic checks fail, the system immediately returns an HTTP 400 or 401 response without further processing.\n\n**Stage 2: Metric Validation and Normalization**\n\nEach metric in the batch undergoes comprehensive validation through the `Validator` component. This validation ensures data quality and prevents common issues that could corrupt the storage engine or cause query failures.\n\nThe validation process examines multiple aspects of each metric:\n\n1. **Metric Name Validation**: Confirms the metric name follows the required naming conventions (alphanumeric characters, underscores, and dots only), doesn't exceed maximum length limits (typically 256 characters), and doesn't use reserved prefixes that could conflict with system metrics.\n\n2. **Metric Type Consistency**: Verifies that the `MetricType` field contains a valid enum value (Counter, Gauge, or Histogram) and that the associated sample data is appropriate for the metric type. For example, counter values must be non-negative and monotonically increasing.\n\n3. **Label Validation**: Ensures all label keys and values in the `Labels` map follow naming conventions, don't exceed length limits, and don't contain prohibited characters that could interfere with query parsing. The system also validates that reserved label names (like `__name__` and `__timestamp__`) are not used.\n\n4. **Sample Data Validation**: Checks that timestamp values are reasonable (not too far in the past or future), numeric values are valid (not NaN or infinite), and the sample count is within acceptable limits.\n\n5. **Cardinality Validation**: The `CardinalityManager` evaluates whether accepting this metric would create excessive cardinality by generating too many unique time-series combinations. This prevents \"cardinality explosions\" that could overwhelm the storage system.\n\nAfter validation, the `NormalizeMetric` function standardizes the metric format: converting timestamps to UTC, sorting labels alphabetically for consistent series ID generation, and applying any configured label transformations or defaults.\n\n**Stage 3: Series ID Generation and Indexing**\n\nFor each validated metric, the system generates a unique series ID by computing a hash of the metric name and sorted labels. This series ID serves as the primary key for time-series data storage and enables efficient retrieval during queries.\n\nThe series ID generation process combines the metric name and all label key-value pairs into a canonical string representation, then computes a cryptographic hash (typically SHA-256) to create a unique identifier. This approach ensures that metrics with identical names and labels always generate the same series ID, enabling proper time-series continuity.\n\nThe `SeriesIndex` maintains mapping between series IDs and their metadata, including the metric name, full label set, and references to storage blocks containing the series data. When a new series ID is encountered, the index creates a new `SeriesInfo` entry; for existing series, it updates the last sample timestamp to track data freshness.\n\n**Stage 4: Write-Ahead Log Persistence**\n\nBefore acknowledging the ingestion request, the system writes all validated samples to the `WriteAheadLog` for durability. This ensures that even if the process crashes before samples reach permanent storage, the data can be recovered and reprocessed during system restart.\n\nThe WAL write process creates a `WALRecord` containing the timestamp, metric samples, and a checksum for integrity verification. The record gets appended to the current WAL file, and the system calls `fsync()` to ensure the data reaches persistent storage before proceeding. This durability guarantee is essential for preventing data loss in production environments.\n\n**Stage 5: Storage Engine Integration**\n\nAfter successful WAL persistence, the samples get forwarded to the `StorageEngine` through the `WriteSamples` method. The storage engine handles the complex process of organizing samples into time-based blocks, managing in-memory buffers, and scheduling background compaction operations.\n\nThe storage engine may buffer samples in memory before writing them to disk blocks, depending on the configured flush intervals and memory pressure. However, since samples are already persisted in the WAL, this buffering doesn't compromise durability - it only affects query latency for the most recent data points.\n\n**Stage 6: Response Generation and Statistics Updates**\n\nOnce all samples in the batch have been successfully processed and stored, the ingestion endpoint returns an HTTP 200 response to the client. The response includes statistics about the processing operation: number of metrics accepted, number rejected due to validation failures, and any warnings about potential issues.\n\nThe system updates internal metrics about ingestion performance, including throughput rates, error counts, and processing latencies. These operational metrics help monitor system health and identify performance bottlenecks or data quality issues.\n\n#### Pull-Based Scraping Process\n\nThe pull-based scraping process involves the metrics system actively retrieving data from external endpoints that expose metrics in standard formats like Prometheus exposition format. This process runs on configurable intervals and handles endpoint discovery, metric parsing, and error recovery.\n\n**Endpoint Discovery and Scheduling**\n\nThe `ScrapeEndpoint` method manages a registry of target endpoints that should be scraped for metrics. Each target includes the endpoint URL, scraping interval, timeout settings, and any required authentication credentials. The system maintains a scheduler that triggers scrape operations at the appropriate intervals for each target.\n\nThe scraping scheduler uses a priority queue to manage scrape timing, ensuring that endpoints are scraped at their configured intervals while distributing the load evenly over time. If a scrape operation takes longer than expected, the scheduler can detect this and adjust future scheduling to prevent overlapping scrapes that could overwhelm either the metrics system or the target endpoint.\n\n**HTTP Client Operations and Format Parsing**\n\nWhen scraping an endpoint, the system makes an HTTP GET request with appropriate timeout settings and authentication headers. The response body typically contains metrics in Prometheus exposition format, which uses a text-based representation with metric names, labels, values, and timestamps.\n\nThe parsing process converts the text format into internal `Metric` data structures, handling the various metric types and label formats supported by the exposition format. This includes parsing counter metrics (with `_total` suffixes), gauge metrics, histogram metrics (with `_bucket`, `_count`, and `_sum` suffixes), and extracting label values from the metric names and label sets.\n\n**Error Handling and Retry Logic**\n\nScraping operations can fail due to network issues, endpoint unavailability, or invalid data formats. The system implements exponential backoff retry logic to handle transient failures without overwhelming failing endpoints. For persistent failures, the system logs errors and continues attempting scrapes at reduced frequencies while alerting operators to the issue.\n\n#### Common Ingestion Flow Patterns\n\n| Flow Type | Trigger | Processing Steps | Success Outcome | Failure Handling |\n|---|---|---|---|---|\n| Batch Push | HTTP POST with metric array | Validate→Normalize→Store→Respond | 200 OK with statistics | 400/500 with error details |\n| Single Push | HTTP POST with single metric | Same as batch but optimized | 200 OK with confirmation | Detailed validation errors |\n| Prometheus Scrape | Timer-triggered pull | Fetch→Parse→Validate→Store | Updated series data | Retry with backoff |\n| Health Check | Monitor request | Quick validation only | System status | Degraded/unhealthy status |\n\n### Query Processing Flow\n\nThe query processing flow transforms user requests for metric data into efficient storage operations and properly formatted results. This flow must handle query parsing, execution planning, storage retrieval, data aggregation, and result formatting while maintaining good performance and accurate results.\n\n![Query Processing Flow](./diagrams/query-execution.svg)\n\n#### Query Request Initiation\n\nQuery processing begins when a client submits a query request through either the dashboard API or alerting system. The request contains a query expression string, time range specification, and optional parameters like maximum data points or output format preferences.\n\nThe `QueryProcessor` receives these requests through its `ExecuteQuery` method, which serves as the main entry point for all query operations. The processor immediately assigns a unique query ID for tracking and logging purposes, starts timing measurement for performance monitoring, and validates basic request parameters like time range sanity (end time after start time, reasonable date ranges).\n\n**Query Request Validation**\n\nBefore parsing the query string, the system performs preliminary validation to catch obvious errors early and prevent resource waste on malformed requests. This validation checks that the time range is reasonable (not requesting data from the year 2050 or from before the system was deployed), the query string is not empty, and any optional parameters are within acceptable ranges.\n\nFor queries originating from the dashboard, additional validation ensures that the requesting user has appropriate permissions to access the requested metrics, particularly if the system implements any access control policies based on metric labels or namespaces.\n\n#### Query Parsing and AST Generation\n\nThe query parser transforms the query string into a structured `QueryAST` that represents the query's logical structure. This parsing process uses a lexical analyzer (`Lexer`) to break the query string into tokens, followed by a recursive descent parser that builds the abstract syntax tree.\n\n**Lexical Analysis Phase**\n\nThe lexer scans the query string character by character, identifying tokens like metric names, label selectors, function calls, operators, and literal values. Each token gets classified with a `TokenType` (such as `TokenMetricName`, `TokenLabelName`, `TokenFunction`) and stored with its string value and position in the original query.\n\nThe lexical analysis handles various query syntax elements: quoted strings for label values containing special characters, numeric literals for threshold values and time durations, and reserved keywords for functions and operators. Error handling during lexing provides detailed feedback about syntax issues, including the character position where parsing failed.\n\n**Syntax Tree Construction**\n\nThe parser consumes tokens from the lexer and constructs a hierarchical `QueryAST` representing the query's logical structure. The tree contains different node types: `MetricSelectorNode` for basic metric selection with label filters, `FunctionCallNode` for aggregation and transformation operations, and `BinaryOpNode` for arithmetic operations between query results.\n\nEach AST node implements the `ASTNode` interface with methods for type identification and string representation. This design enables the system to traverse the tree, validate query semantics, and generate execution plans while maintaining clean separation between parsing and execution concerns.\n\n**Semantic Validation**\n\nAfter syntax parsing, the system performs semantic validation to ensure the query makes logical sense. This validation checks that referenced metric names exist in the system, label names follow proper conventions, function calls use correct parameter types and counts, and time range specifications are consistent with the query structure.\n\nSemantic validation also detects potential performance issues like queries that would need to scan excessive amounts of data or generate extremely high-cardinality result sets. These checks help prevent queries that could overwhelm the system or consume excessive memory during execution.\n\n#### Query Execution Planning\n\nThe query planner analyzes the parsed AST and generates an optimized `ExecutionPlan` that specifies how the query should be executed against the storage engine. This planning phase is crucial for query performance because it determines the order of operations, identifies optimization opportunities, and estimates resource requirements.\n\n**Series Selection Optimization**\n\nThe planner examines `MetricSelectorNode` elements in the AST to identify which time series need to be retrieved from storage. It analyzes label matchers to estimate the number of matching series and determines the most efficient retrieval strategy based on available indexes and label cardinality.\n\nFor queries with multiple label filters, the planner determines the optimal order for applying filters: starting with the most selective filters to minimize the amount of data that needs to be processed by subsequent filters. This optimization can dramatically improve query performance when dealing with high-cardinality metrics.\n\n**Aggregation Strategy Selection**\n\nWhen the query includes aggregation functions (like sum, average, or percentile calculations), the planner determines whether aggregations can be performed incrementally as data is retrieved (streaming aggregation) or require buffering entire result sets in memory. This decision depends on the specific function requirements and available system memory.\n\nThe planner also identifies opportunities for query result caching, particularly for expensive aggregations over static time ranges that are unlikely to change. Cached results can be reused for identical queries or combined with incremental calculations for queries that extend previous time ranges.\n\n**Parallel Execution Planning**\n\nFor queries that need to process large amounts of data, the planner can divide the work into parallel execution phases. This might involve processing different time ranges concurrently, distributing aggregation calculations across multiple worker threads, or parallelizing series retrieval operations.\n\nThe parallelization strategy considers system resources, storage architecture, and query characteristics to balance performance improvements against coordination overhead. Simple queries may execute faster with single-threaded processing, while complex aggregations over large datasets benefit significantly from parallel execution.\n\n#### Storage Interaction and Data Retrieval\n\nThe execution engine interacts with the `StorageEngine` through well-defined interfaces to retrieve the time-series data needed for query execution. This interaction must efficiently handle different query patterns while minimizing storage system load and memory usage.\n\n**Series Discovery and Selection**\n\nUsing the label matchers from the execution plan, the query engine calls `QuerySeries` to identify all time series that match the query criteria. This operation returns `SeriesInfo` objects containing series metadata but not the actual sample data, allowing the system to understand the scope of data retrieval before loading samples into memory.\n\nThe series selection process leverages the `SeriesIndex` to quickly identify matching series without scanning raw storage blocks. For queries with multiple label filters, the selection process can use index intersection operations to efficiently find series that match all criteria simultaneously.\n\n**Sample Data Retrieval**\n\nFor each selected series, the query engine calls `QueryRange` to retrieve sample data within the specified time range. The storage engine returns arrays of `Sample` objects containing timestamps and values, which the query engine then processes according to the execution plan.\n\nTo manage memory usage and processing efficiency, the query engine may retrieve sample data in batches rather than loading entire time ranges into memory simultaneously. This streaming approach enables processing of very large result sets without exhausting system memory.\n\n**Data Alignment and Interpolation**\n\nWhen processing multiple time series, the query engine often needs to align samples from different series to common timestamps for aggregation or arithmetic operations. This alignment process may involve interpolating missing values or selecting the most recent value before each timestamp.\n\nThe alignment strategy depends on the metric types and query requirements: counter metrics typically use rate calculations that account for counter resets, gauge metrics may use linear interpolation for missing values, and histogram metrics require special handling for bucket boundaries and statistical calculations.\n\n#### Aggregation and Result Processing\n\nAfter retrieving raw sample data from storage, the query engine applies aggregation functions and transformations specified in the query to compute final results. This processing phase must handle various aggregation types while maintaining numerical accuracy and handling edge cases like missing data.\n\n**Statistical Aggregation Functions**\n\nThe system implements various `AggregationFunction` types for different statistical operations: sum and average for combining multiple series, min and max for finding extreme values, and percentile calculations for understanding data distributions. Each function handles streaming aggregation where possible to minimize memory usage.\n\nFor histogram metrics, the system provides specialized aggregation functions that properly combine histogram buckets from multiple series, calculate quantile estimates across merged histograms, and maintain statistical accuracy even when dealing with different bucket boundaries across series.\n\n**Time-Based Processing**\n\nMany queries require processing data over time windows, such as calculating rates of change for counter metrics or computing moving averages for gauge metrics. The query engine implements these time-based operations by maintaining sliding windows of sample data and updating calculations as new samples are processed.\n\nRate calculations for counter metrics require special handling for counter resets, where the counter value decreases (typically due to process restarts). The system detects these resets and adjusts rate calculations to maintain accuracy across reset boundaries.\n\n**Result Formatting and Output**\n\nThe final query processing step formats the computed results according to the client's requirements. Dashboard queries typically receive results as time-series arrays with timestamp-value pairs, while alerting queries may only need scalar values or boolean results indicating threshold violations.\n\nThe result formatting process includes unit conversion (if requested), timestamp formatting for the client's time zone preferences, and data point sampling or aggregation if the client requested a maximum number of data points. This final processing ensures that results are immediately usable by the requesting component without additional transformation.\n\n#### Query Processing Flow Patterns\n\n| Query Type | Parsing Complexity | Storage Operations | Aggregation Needs | Result Format |\n|---|---|---|---|---|\n| Simple Series | Low (single selector) | Single series lookup | None | Raw time-series |\n| Multi-Series Sum | Medium (aggregation function) | Multiple series retrieval | Sum across series | Aggregated time-series |\n| Rate Calculation | Medium (rate function) | Series with counter handling | Rate computation | Derived time-series |\n| Complex Dashboard | High (multiple subqueries) | Optimized batch retrieval | Mixed aggregations | Multiple result sets |\n| Alert Evaluation | Low (threshold comparison) | Recent data only | Threshold comparison | Boolean or scalar |\n\n### Alert Evaluation Flow\n\nThe alert evaluation flow continuously monitors metric data against defined alert rules, manages alert state transitions, and triggers appropriate notifications when conditions change. This flow must provide reliable monitoring, minimize false positives, and ensure timely notification delivery.\n\n![Alert Notification Flow](./diagrams/notification-flow.svg)\n\n#### Alert Rule Evaluation Cycle\n\nThe alert evaluation process runs on a regular schedule, typically every 15-60 seconds, to check all active alert rules against current metric data. The `AlertEvaluator` coordinates this process, managing the evaluation schedule, tracking alert states, and triggering notifications when state changes occur.\n\n**Evaluation Scheduling and Coordination**\n\nThe `AlertEvaluator` maintains a scheduler that triggers evaluation cycles at the configured `EvaluationInterval`. Each evaluation cycle processes all active `AlertRule` definitions, executing their query expressions against the current metric data and comparing results to defined thresholds.\n\nThe scheduling system uses a ticker mechanism to ensure consistent evaluation intervals, but includes logic to handle cases where evaluations take longer than the scheduled interval. If an evaluation cycle is still running when the next cycle should start, the system can either wait for completion or run evaluations in parallel, depending on system resources and configuration.\n\n**Rule Query Execution**\n\nFor each `AlertRule`, the evaluator executes the rule's query expression using the same `QueryProcessor` used by the dashboard system. This ensures consistency between what users see in dashboards and what the alerting system monitors. The query typically retrieves recent data (last few minutes) to determine current metric values.\n\nAlert rule queries often include aggregation functions to reduce multiple time series to scalar values that can be compared against thresholds. For example, a rule monitoring average CPU usage might aggregate samples from multiple servers, while a rule monitoring error rates might calculate the ratio of error count to total request count.\n\n**Threshold Comparison and Condition Evaluation**\n\nAfter executing the rule query, the evaluator compares the resulting values against the rule's threshold using the specified comparison operator (greater than, less than, equals, etc.). This comparison determines whether the alert condition is currently met for each time series that matches the rule's selector.\n\nFor rules that monitor multiple time series (like per-server metrics), the evaluator creates separate `AlertInstance` objects for each series that violates the threshold. Each instance tracks its own state and history independently, allowing fine-grained alerting on individual series while maintaining overall rule coherence.\n\n**Duration-Based Alert Logic**\n\nMany alert rules include a duration requirement: the condition must persist for a specified time before the alert fires. This prevents spurious alerts from brief metric spikes or temporary network issues. The evaluator tracks how long each condition has been true and only transitions alerts to firing state after the duration threshold is met.\n\nThe duration tracking uses the alert instance's state history to determine when the condition first became true. If the condition stops being met before the duration expires, the instance returns to inactive state and the duration timer resets. This ensures that only sustained threshold violations trigger alerts.\n\n#### Alert State Management\n\nThe `StateManager` component maintains the lifecycle of all alert instances, tracking state transitions, managing persistence, and ensuring proper notification behavior. Alert states follow a well-defined state machine that prevents inconsistent behavior and ensures reliable alerting.\n\n**Alert State Machine Implementation**\n\nEach `AlertInstance` progresses through defined states based on evaluation results and time constraints:\n\n| Current State | Condition Met | Duration Exceeded | Next State | Actions Taken |\n|---|---|---|---|---|\n| `AlertStateInactive` | No | N/A | `AlertStateInactive` | No action |\n| `AlertStateInactive` | Yes | No | `AlertStatePending` | Start duration timer |\n| `AlertStatePending` | Yes | Yes | `AlertStateFiring` | Send firing notification |\n| `AlertStatePending` | No | N/A | `AlertStateInactive` | Cancel duration timer |\n| `AlertStateFiring` | Yes | N/A | `AlertStateFiring` | No notification (already firing) |\n| `AlertStateFiring` | No | N/A | `AlertStateResolved` | Send resolved notification |\n| `AlertStateResolved` | No | N/A | `AlertStateInactive` | Cleanup after grace period |\n| Any | Silenced | N/A | `AlertStateSilenced` | Suppress notifications |\n\n**State Persistence and Recovery**\n\nThe `StateManager` persists alert state information to survive system restarts and ensure continuity of alert monitoring. This persistence includes current state, state change timestamps, condition values, and notification history for each alert instance.\n\nDuring system startup, the state manager loads persisted alert states and reconciles them with current rule definitions. If rules have been modified or deleted, the system handles orphaned alert instances appropriately, either updating them to match new rule definitions or marking them for cleanup.\n\n**State Change Notification Triggering**\n\nWhen an alert instance transitions to a new state that requires notification (`AlertStateFiring` or `AlertStateResolved`), the state manager creates `NotificationRequest` objects and queues them for delivery through the `NotificationManager`. These requests include all necessary information for generating and sending appropriate notifications.\n\nThe state manager also maintains statistics about alert behavior, tracking metrics like time spent in each state, frequency of state changes, and notification success rates. This information helps operators understand alert behavior patterns and identify rules that may need tuning.\n\n#### Notification Processing and Delivery\n\nThe notification system handles the complex process of generating appropriate messages for different channels, managing delivery queues, handling failures, and ensuring reliable notification delivery even under adverse conditions.\n\n**Notification Channel Selection and Routing**\n\nWhen an alert state change triggers a notification, the system determines which notification channels should receive the alert based on rule configuration, channel filters, and current system state. Different alert rules may route to different channels based on severity, affected services, or time of day.\n\nThe routing logic evaluates each configured `NotificationChannel` against the alert instance to determine if the channel should receive the notification. This evaluation considers channel filters (which may match alert labels), channel availability status, and any active silencing rules that might suppress notifications for specific channels.\n\n**Message Templating and Formatting**\n\nFor each selected notification channel, the system generates appropriate message content using configured message templates. The templating system has access to all alert instance data, including current values, threshold settings, alert history, and custom annotations from the alert rule definition.\n\nDifferent channels require different message formats: email notifications might include detailed formatting with tables and charts, Slack notifications use rich message formatting with action buttons, while SMS notifications need concise text that fits within message length limits. The templating system handles these variations while maintaining consistent information content.\n\n**Delivery Queue Management and Retry Logic**\n\nNotification delivery uses a queue-based system that can handle high notification volumes and temporary delivery failures without blocking alert evaluation or losing notifications. The `NotificationManager` maintains separate delivery queues for different channel types, allowing independent processing and retry logic.\n\nWhen notification delivery fails (due to network issues, service outages, or configuration problems), the system implements exponential backoff retry logic with maximum retry limits. Failed notifications are logged with detailed error information to help operators diagnose and resolve delivery issues.\n\n**Rate Limiting and Notification Throttling**\n\nTo prevent notification storms that could overwhelm receiving systems or alert recipients, the notification system implements rate limiting based on channel configuration and alert frequency. This includes burst limiting (maximum notifications per minute) and sustained rate limiting (maximum notifications per hour).\n\nThe rate limiting system can also implement intelligent throttling that reduces notification frequency for frequently-flapping alerts while maintaining normal delivery for stable alerts. This helps reduce alert fatigue while ensuring that genuine issues receive appropriate attention.\n\n#### Alert Evaluation Flow Patterns\n\n| Evaluation Type | Trigger | Query Complexity | State Changes | Notification Volume |\n|---|---|---|---|---|\n| Simple Threshold | Scheduled timer | Single metric comparison | Infrequent (stable conditions) | Low (binary firing/resolved) |\n| Multi-Series Rule | Scheduled timer | Aggregated query | Variable (per-series instances) | Medium (multiple instances) |\n| Flapping Alert | Scheduled timer | Same query repeatedly | Frequent (unstable conditions) | High (requires throttling) |\n| Composite Condition | Scheduled timer | Multiple subqueries | Complex (dependent conditions) | Variable (conditional logic) |\n\n#### Integration with Dashboard and Query Systems\n\nThe alert evaluation system leverages the same query processing infrastructure used by the dashboard system, ensuring consistency between what users observe in visualizations and what triggers alerts. This shared infrastructure reduces system complexity and maintains behavioral consistency.\n\n**Shared Query Processing Pipeline**\n\nAlert rule evaluation uses the same `QueryProcessor`, query parsing logic, and storage interfaces as dashboard queries. This means that alert expressions can use the same syntax and functions available in dashboard queries, and operators can test alert conditions by running equivalent queries in the dashboard interface.\n\nThe shared pipeline also ensures that optimizations and bug fixes in the query processing system automatically benefit both dashboard and alerting functionality. This reduces maintenance burden and improves overall system reliability.\n\n**Real-Time Data Consistency**\n\nSince both dashboard and alerting systems query the same underlying storage, users can observe the same metric values that drive alert decisions. This consistency is crucial for debugging alert behavior and understanding why alerts fire or resolve at specific times.\n\nThe system maintains this consistency even during high-load periods by using the same data retrieval and aggregation logic for both use cases. However, alert queries typically focus on recent data and use simpler aggregations to minimize evaluation latency and resource usage.\n\n### Implementation Guidance\n\nThis section provides practical implementation advice for building the component interaction and data flow systems described above. The code examples and patterns shown here will help translate the design concepts into working software.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option | \n|---|---|---|\n| HTTP Server | Go net/http with gorilla/mux | Go fiber or gin frameworks |\n| Message Serialization | JSON with encoding/json | Protocol Buffers with protobuf |\n| Concurrency Control | Go channels and sync package | WorkerPool patterns with semaphores |\n| Logging | Go slog package | Structured logging with zerolog |\n| Metrics Instrumentation | Prometheus client library | Custom metrics with OpenTelemetry |\n| Configuration | YAML files with gopkg.in/yaml | Consul/etcd with dynamic updates |\n\n#### Project Structure\n\n```\nproject-root/\n├── cmd/\n│   ├── metrics-server/\n│   │   └── main.go                    ← Main server entry point\n│   └── metrics-cli/\n│       └── main.go                    ← CLI tools for testing\n├── internal/\n│   ├── coordinator/\n│   │   ├── coordinator.go             ← ComponentCoordinator implementation\n│   │   ├── component.go               ← Component interface and base types\n│   │   └── coordinator_test.go\n│   ├── ingestion/\n│   │   ├── ingester.go                ← MetricsIngester implementation\n│   │   ├── validator.go               ← Validation and normalization\n│   │   ├── cardinality.go             ← CardinalityManager implementation\n│   │   └── handlers.go                ← HTTP handlers for ingestion\n│   ├── storage/\n│   │   ├── engine.go                  ← StorageEngine implementation\n│   │   ├── series.go                  ← SeriesIndex and SeriesInfo\n│   │   ├── wal.go                     ← WriteAheadLog implementation\n│   │   └── blocks.go                  ← Block storage management\n│   ├── query/\n│   │   ├── processor.go               ← QueryProcessor implementation\n│   │   ├── parser.go                  ← Query parsing and AST\n│   │   ├── executor.go                ← Query execution engine\n│   │   └── cache.go                   ← QueryCache implementation\n│   ├── dashboard/\n│   │   ├── server.go                  ← DashboardServer implementation\n│   │   ├── websocket.go               ← WebSocket client management\n│   │   └── handlers.go                ← HTTP handlers for dashboard API\n│   ├── alerting/\n│   │   ├── evaluator.go               ← AlertEvaluator implementation\n│   │   ├── state.go                   ← StateManager implementation\n│   │   ├── notifications.go           ← NotificationManager implementation\n│   │   └── channels/                  ← Channel-specific implementations\n│   └── common/\n│       ├── types.go                   ← Shared type definitions\n│       ├── config.go                  ← Configuration structures\n│       └── health.go                  ← HealthManager implementation\n├── pkg/\n│   └── client/                        ← Client library for external use\n└── scripts/\n    ├── docker-compose.yml             ← Development environment\n    └── test-data/                     ← Sample metrics for testing\n```\n\n#### Core Component Coordination Infrastructure\n\nThe following code provides the foundation for component lifecycle management and coordination:\n\n```go\n// internal/coordinator/component.go\npackage coordinator\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log/slog\"\n    \"sync\"\n    \"time\"\n)\n\n// Component represents a system component with lifecycle management\ntype Component interface {\n    // Start initializes and starts the component\n    Start(ctx context.Context) error\n    \n    // Stop gracefully shuts down the component\n    Stop(ctx context.Context) error\n    \n    // Name returns the component identifier\n    Name() string\n    \n    // HealthCheck returns the current component health status\n    HealthCheck(ctx context.Context) error\n}\n\n// ComponentCoordinator manages the lifecycle of all system components\ntype ComponentCoordinator struct {\n    config     *Config\n    logger     *slog.Logger\n    health     *HealthManager\n    components map[string]Component\n    mu         sync.RWMutex\n    started    bool\n    stopCh     chan struct{}\n    wg         sync.WaitGroup\n}\n\n// NewCoordinator creates a new component coordinator\nfunc NewCoordinator(config *Config, logger *slog.Logger) *ComponentCoordinator {\n    return &ComponentCoordinator{\n        config:     config,\n        logger:     logger,\n        health:     NewHealthManager(),\n        components: make(map[string]Component),\n        stopCh:     make(chan struct{}),\n    }\n}\n\n// RegisterComponent adds a component to the coordinator\nfunc (c *ComponentCoordinator) RegisterComponent(component Component) error {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    if c.started {\n        return fmt.Errorf(\"cannot register component %s: coordinator already started\", component.Name())\n    }\n    \n    if _, exists := c.components[component.Name()]; exists {\n        return fmt.Errorf(\"component %s already registered\", component.Name())\n    }\n    \n    c.components[component.Name()] = component\n    c.logger.Info(\"registered component\", \"name\", component.Name())\n    return nil\n}\n\n// Start initializes and starts all registered components\nfunc (c *ComponentCoordinator) Start(ctx context.Context) error {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    if c.started {\n        return fmt.Errorf(\"coordinator already started\")\n    }\n    \n    // TODO 1: Start each component in dependency order\n    // TODO 2: Register health checks for each component\n    // TODO 3: Start background health monitoring\n    // TODO 4: Set up graceful shutdown signal handling\n    // TODO 5: Mark coordinator as started\n    \n    return nil\n}\n\n// Stop gracefully shuts down all components\nfunc (c *ComponentCoordinator) Stop(ctx context.Context) error {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    if !c.started {\n        return nil\n    }\n    \n    // TODO 1: Signal all components to stop\n    // TODO 2: Wait for components to finish with timeout\n    // TODO 3: Force stop any components that don't respond\n    // TODO 4: Clean up resources and close channels\n    \n    return nil\n}\n```\n\n#### Ingestion Flow Infrastructure\n\n```go\n// internal/ingestion/ingester.go\npackage ingestion\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\n// MetricsIngester handles incoming metric data with validation and storage\ntype MetricsIngester struct {\n    storage      TimeSeriesStorage\n    validator    Validator\n    cardinality  CardinalityManager\n    logger       *slog.Logger\n    stats        *IngestionStats\n    \n    // Configuration\n    maxBatchSize int\n    timeout      time.Duration\n}\n\n// NewMetricsIngester creates a new metrics ingester\nfunc NewMetricsIngester(storage TimeSeriesStorage, validator Validator, cardinality CardinalityManager, logger *slog.Logger) *MetricsIngester {\n    return &MetricsIngester{\n        storage:      storage,\n        validator:    validator,\n        cardinality:  cardinality,\n        logger:       logger,\n        stats:        &IngestionStats{},\n        maxBatchSize: 1000,\n        timeout:      30 * time.Second,\n    }\n}\n\n// IngestMetrics processes a batch of metrics through the complete ingestion pipeline\nfunc (m *MetricsIngester) IngestMetrics(ctx context.Context, metrics []Metric) error {\n    startTime := time.Now()\n    defer func() {\n        m.logger.Debug(\"ingestion completed\", \n            \"duration\", time.Since(startTime),\n            \"metric_count\", len(metrics))\n    }()\n    \n    // TODO 1: Validate batch size and basic request parameters\n    // TODO 2: For each metric in the batch:\n    //   a. Validate metric structure and values using m.validator.ValidateMetric()\n    //   b. Check cardinality limits using m.cardinality.CheckCardinality()\n    //   c. Normalize metric format using m.validator.NormalizeMetric()\n    //   d. Generate series ID and update series index\n    // TODO 3: Write all validated samples to storage using m.storage.WriteSamples()\n    // TODO 4: Update ingestion statistics (success/failure counts)\n    // TODO 5: Return any validation errors or storage failures\n    \n    return nil\n}\n\n// ScrapeEndpoint retrieves metrics from a Prometheus-compatible endpoint\nfunc (m *MetricsIngester) ScrapeEndpoint(ctx context.Context, url string) error {\n    // TODO 1: Make HTTP GET request to the endpoint with timeout\n    // TODO 2: Parse Prometheus exposition format from response body\n    // TODO 3: Convert parsed metrics to internal Metric structures\n    // TODO 4: Call IngestMetrics() with the converted metrics\n    // TODO 5: Handle scraping errors and update endpoint statistics\n    \n    return nil\n}\n\n// HTTP handler for push-based ingestion\nfunc (m *MetricsIngester) HandlePushMetrics(w http.ResponseWriter, r *http.Request) {\n    // TODO 1: Validate HTTP method and content-type\n    // TODO 2: Read and parse JSON request body into Metric slice\n    // TODO 3: Call IngestMetrics() with parsed metrics\n    // TODO 4: Return appropriate HTTP status and response body\n    // TODO 5: Log request details and performance metrics\n}\n```\n\n#### Query Processing Infrastructure\n\n```go\n// internal/query/processor.go\npackage query\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n)\n\n// QueryProcessor executes metric queries and returns formatted results\ntype QueryProcessor struct {\n    storage   TimeSeriesStorage\n    parser    *QueryParser\n    cache     QueryCache\n    logger    *slog.Logger\n    \n    // Execution limits\n    maxSeries    int\n    maxSamples   int64\n    queryTimeout time.Duration\n}\n\n// NewQueryProcessor creates a new query processor\nfunc NewQueryProcessor(storage TimeSeriesStorage, logger *slog.Logger) *QueryProcessor {\n    return &QueryProcessor{\n        storage:      storage,\n        parser:       NewQueryParser(),\n        cache:        NewQueryCache(1000, 5*time.Minute),\n        logger:       logger,\n        maxSeries:    10000,\n        maxSamples:   1000000,\n        queryTimeout: 30 * time.Second,\n    }\n}\n\n// ExecuteQuery processes a query string and returns formatted results\nfunc (q *QueryProcessor) ExecuteQuery(ctx context.Context, queryString string, timeRange TimeRange) (*QueryResult, error) {\n    startTime := time.Now()\n    defer func() {\n        q.logger.Debug(\"query executed\",\n            \"duration\", time.Since(startTime),\n            \"query\", queryString)\n    }()\n    \n    // TODO 1: Check query cache for existing results\n    // TODO 2: Parse query string into AST using q.parser.Parse()\n    // TODO 3: Validate query semantics and estimate resource requirements\n    // TODO 4: Generate optimized execution plan using PlanQuery()\n    // TODO 5: Execute plan against storage:\n    //   a. Retrieve matching series using storage.QuerySeries()\n    //   b. Fetch sample data using storage.QueryRange()\n    //   c. Apply aggregation functions and transformations\n    // TODO 6: Format results according to client requirements\n    // TODO 7: Update query cache with results\n    // TODO 8: Return QueryResult with data and execution statistics\n    \n    return nil, nil\n}\n```\n\n#### Alert Evaluation Infrastructure\n\n```go\n// internal/alerting/evaluator.go\npackage alerting\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n)\n\n// AlertEvaluator continuously evaluates alert rules and manages alert states\ntype AlertEvaluator struct {\n    queryProcessor    QueryProcessor\n    stateManager     *StateManager\n    ruleManager      *RuleManager\n    notificationMgr  *NotificationManager\n    logger           *slog.Logger\n    \n    // Evaluation control\n    evaluationInterval time.Duration\n    stopCh            chan struct{}\n    wg                sync.WaitGroup\n    running           bool\n    mu                sync.RWMutex\n}\n\n// NewAlertEvaluator creates a new alert evaluator\nfunc NewAlertEvaluator(queryProcessor QueryProcessor, stateManager *StateManager, notificationMgr *NotificationManager, logger *slog.Logger) *AlertEvaluator {\n    return &AlertEvaluator{\n        queryProcessor:     queryProcessor,\n        stateManager:      stateManager,\n        notificationMgr:   notificationMgr,\n        logger:            logger,\n        evaluationInterval: 15 * time.Second,\n        stopCh:            make(chan struct{}),\n    }\n}\n\n// Start begins the alert evaluation loop\nfunc (a *AlertEvaluator) Start(ctx context.Context) error {\n    a.mu.Lock()\n    defer a.mu.Unlock()\n    \n    if a.running {\n        return fmt.Errorf(\"alert evaluator already running\")\n    }\n    \n    // TODO 1: Start the evaluation ticker\n    // TODO 2: Launch background goroutine for evaluation loop\n    // TODO 3: Set up context cancellation handling\n    // TODO 4: Mark evaluator as running\n    \n    a.running = true\n    return nil\n}\n\n// EvaluateRules processes all active alert rules against current metric data\nfunc (a *AlertEvaluator) EvaluateRules(ctx context.Context) error {\n    // TODO 1: Get all active alert rules from rule manager\n    // TODO 2: For each rule:\n    //   a. Execute rule query using query processor\n    //   b. Compare results against rule threshold\n    //   c. Update alert instance state using state manager\n    //   d. Trigger notifications if state changed\n    // TODO 3: Clean up resolved alerts older than retention period\n    // TODO 4: Update evaluation statistics and health metrics\n    \n    return nil\n}\n\n// UpdateAlertState manages alert state transitions and notifications\nfunc (a *AlertEvaluator) UpdateAlertState(ctx context.Context, ruleID string, seriesLabels Labels, currentValue float64, threshold float64, conditionMet bool, rule *AlertRule) error {\n    // TODO 1: Get current alert instance from state manager\n    // TODO 2: Determine next state based on current state and condition\n    // TODO 3: Check duration requirements for state transitions\n    // TODO 4: Update alert instance with new state and value\n    // TODO 5: If state changed to firing or resolved, queue notifications\n    \n    return nil\n}\n```\n\n#### Milestone Checkpoints\n\n**After Implementing Component Coordination (Foundation)**\n- Run: `go test ./internal/coordinator/...`\n- Expected: All tests pass, components start/stop cleanly\n- Manual test: Start server, check health endpoint returns 200 OK\n- Debug: Check logs for component registration and startup sequence\n\n**After Implementing Ingestion Flow (Milestone 1)**\n- Run: `curl -X POST http://localhost:8080/api/v1/metrics -d '[{\"name\":\"test_counter\",\"type\":0,\"labels\":{\"job\":\"test\"},\"samples\":[{\"value\":42,\"timestamp\":\"2024-01-01T12:00:00Z\"}]}]'`\n- Expected: 200 OK response with ingestion statistics\n- Manual test: Send invalid metrics, verify proper error responses\n- Debug: Check ingestion logs and storage files for metric data\n\n**After Implementing Query Processing (Milestone 2)**\n- Run: `curl \"http://localhost:8080/api/v1/query?query=test_counter&start=2024-01-01T11:00:00Z&end=2024-01-01T13:00:00Z\"`\n- Expected: JSON response with time-series data\n- Manual test: Query non-existent metrics, verify empty results\n- Debug: Check query logs and execution time statistics\n\n**After Implementing Alert Evaluation (Milestone 4)**\n- Configure alert rule with low threshold, send metrics that exceed it\n- Expected: Alert transitions to firing state, notification sent\n- Manual test: Verify alert resolves when condition clears\n- Debug: Check alert evaluation logs and state persistence\n\n#### Common Debugging Patterns\n\n| Symptom | Likely Cause | Diagnostic Steps | Resolution |\n|---|---|---|---|\n| Metrics not appearing in queries | Ingestion validation failure | Check ingestion endpoint logs, verify metric format | Fix metric names/labels, check cardinality limits |\n| Slow query performance | Inefficient series selection | Enable query execution logging, check series count | Add label indexes, optimize query selectors |\n| Alerts not firing | Query execution errors | Check alert evaluation logs, test queries manually | Fix alert rule expressions, verify metric availability |\n| WebSocket disconnections | Client timeout or server overload | Monitor connection counts, check message queue sizes | Tune timeout settings, implement backpressure |\n| Memory usage growth | Query result caching or buffer leaks | Profile memory usage, check cache sizes | Implement cache eviction, fix buffer cleanup |\n\n⚠️ **Pitfall: Blocking Operations in Component Startup**\nComponents that perform blocking I/O during startup (like connecting to external services) can prevent the entire system from starting. Always use timeouts and context cancellation for startup operations. If a component fails to start, the coordinator should continue with other components rather than failing completely.\n\n⚠️ **Pitfall: Missing Error Context in Data Flows**\nWhen errors occur in the middle of data processing pipelines, insufficient context makes debugging nearly impossible. Always include relevant identifiers (series ID, query ID, alert rule ID) in error messages and logs. Use structured logging to capture processing context.\n\n⚠️ **Pitfall: Inconsistent Time Handling**\nDifferent components using different time zones or timestamp formats can cause data correlation issues. Always use UTC internally and only convert to local time zones for user display. Include timezone information in all external APIs.\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All milestones (comprehensive error handling is critical throughout the entire metrics and alerting system, from ingestion through visualization to notification delivery)\n\n### The Defense-in-Depth Metaphor: Building Resilient Systems\n\nThink of error handling in a metrics system like a medieval castle's defense system. A well-designed castle doesn't rely on a single wall to protect against attack—it employs multiple layers of defense working together. The outer moat catches obvious threats, the walls deflect serious attacks, the inner courtyard provides fallback positions, and the keep serves as the final refuge. Each layer has different responsibilities and failure modes, but together they create a robust defense against various types of attacks.\n\nSimilarly, our metrics and alerting system employs defense-in-depth error handling. Input validation acts as the outer moat, catching malformed data before it enters the system. Rate limiting and backpressure serve as the castle walls, protecting against overwhelming traffic. Graceful degradation provides fallback positions when components fail, and persistent state recovery acts as the keep—ensuring the system can rebuild itself even after catastrophic failure.\n\nThe key insight is that errors are not exceptional events to be avoided, but predictable phenomena to be managed systematically. Just as castle architects studied siege warfare to design appropriate defenses, we must study the failure modes of distributed systems to build appropriate error handling mechanisms.\n\n> **Decision: Comprehensive Error Taxonomy**\n> - **Context**: Metrics systems face numerous failure modes across ingestion, storage, querying, and alerting\n> - **Options Considered**: \n>   1. Handle errors locally in each component\n>   2. Central error handling service\n>   3. Layered error handling with component-specific recovery\n> - **Decision**: Layered error handling with component-specific recovery strategies\n> - **Rationale**: Each component has unique failure modes and recovery requirements that demand specialized handling\n> - **Consequences**: More complex implementation but significantly better system resilience and observability\n\n### Error Classification Framework\n\nOur error handling strategy categorizes failures into distinct classes, each requiring different detection, reporting, and recovery mechanisms. This classification helps us build appropriate responses rather than treating all errors the same way.\n\n| Error Class | Characteristics | Detection Method | Recovery Strategy | User Impact |\n|-------------|----------------|------------------|------------------|-------------|\n| Transient Network | Temporary connectivity issues, timeouts | Request timeout, connection refused | Retry with exponential backoff | Brief delays in data availability |\n| Resource Exhaustion | Memory, disk, file handles depleted | System monitoring, allocation failures | Backpressure, graceful degradation | Reduced throughput, some data loss |\n| Data Corruption | Invalid data format, checksum mismatches | Validation failures, parsing errors | Skip corrupt data, request resend | Missing data points in time series |\n| Configuration Errors | Invalid settings, missing required fields | Startup validation, runtime checks | Fail-fast with clear error messages | Service unavailable until fixed |\n| Component Failures | Process crashes, hardware failures | Health checks, heartbeat monitoring | Restart, failover to backup | Service interruption, potential data loss |\n| Cascading Failures | One failure triggers others | Dependency monitoring, circuit breakers | Isolate failures, prevent propagation | System-wide degradation |\n\nThe critical principle here is that different error classes require fundamentally different handling strategies. Retrying a configuration error will never succeed, while giving up immediately on a transient network error wastes an opportunity for automatic recovery.\n\n### Ingestion Error Handling\n\nThe metrics ingestion engine faces several distinct failure modes that require specialized handling approaches. The ingestion pipeline must balance data durability with system availability, making intelligent decisions about when to accept, reject, or defer incoming metrics.\n\n#### Mental Model: The Quality Control Checkpoint\n\nThink of metrics ingestion error handling like a quality control checkpoint at a manufacturing plant. Raw materials (metrics) arrive continuously from suppliers (client applications), and the checkpoint must quickly assess each batch for quality and safety. Defective materials are rejected with clear feedback to the supplier. When the production line becomes overwhelmed, the checkpoint implements controlled throttling rather than shutting down completely. Critical defects that could damage downstream equipment trigger immediate safety protocols.\n\nThis quality control metaphor captures the key insight that ingestion error handling is about maintaining system integrity while providing useful feedback to metric producers.\n\n#### Invalid Metrics Processing\n\nThe ingestion engine encounters various forms of invalid metric data that must be handled without disrupting overall system operation. Each validation failure type requires specific handling to provide meaningful feedback while protecting the system from malformed data.\n\n| Validation Failure | Detection Method | Error Response | Recovery Action | Prevention Strategy |\n|-------------------|------------------|----------------|------------------|-------------------|\n| Invalid Metric Name | Regex pattern matching | HTTP 400 with field details | Reject metric, continue processing batch | Client-side validation libraries |\n| Label Cardinality Explosion | Label combination counting | HTTP 429 with cardinality info | Reject high-cardinality metrics | Cardinality monitoring and alerts |\n| Timestamp Out of Range | Time bounds checking | HTTP 400 with acceptable range | Reject samples, accept others | Clock synchronization monitoring |\n| Invalid Sample Values | Numeric validation | HTTP 400 with value constraints | Skip invalid samples | Input sanitization at source |\n| Missing Required Fields | Schema validation | HTTP 422 with missing fields | Reject entire metric | Schema documentation and examples |\n| Malformed JSON/Protocol | Parsing failure | HTTP 400 with parse error | Reject request, maintain connection | Protocol version negotiation |\n\nThe `MetricsIngester` component implements a multi-stage validation pipeline that processes metrics through increasingly sophisticated validation checks. Early-stage validation catches obvious format errors quickly, while later stages perform more expensive operations like cardinality analysis and duplicate detection.\n\n> **Decision: Batch-Level vs Individual Metric Error Handling**\n> - **Context**: When processing batches of metrics, individual metrics may fail validation while others succeed\n> - **Options Considered**:\n>   1. Fail entire batch if any metric is invalid\n>   2. Process all valid metrics, report errors for invalid ones\n>   3. Fail fast on first error\n> - **Decision**: Process valid metrics, report detailed errors for invalid ones\n> - **Rationale**: Maximizes data availability while providing actionable error feedback\n> - **Consequences**: More complex error tracking but better system resilience\n\nThe validation pipeline maintains detailed error statistics that help identify systematic problems in metric production. These statistics are exposed through the health monitoring system and can trigger alerts when error rates exceed acceptable thresholds.\n\n#### Network Failures and Connectivity Issues\n\nNetwork-related failures in metrics ingestion require careful handling to balance data durability with system responsiveness. The ingestion engine must distinguish between temporary connectivity issues that warrant retry attempts and persistent failures that indicate configuration or infrastructure problems.\n\nThe ingestion system implements a sophisticated backpressure mechanism that dynamically adjusts acceptance rates based on downstream capacity. When the storage engine becomes overwhelmed, the ingestion engine gradually increases response latencies and begins rejecting new requests with HTTP 503 status codes that include retry-after headers.\n\n| Failure Type | Symptom | Detection | Response | Recovery |\n|--------------|---------|-----------|----------|----------|\n| Client Timeout | Connection drops during request | Request context cancellation | Return partial success status | Client implements retry logic |\n| DNS Resolution | Cannot resolve ingestion endpoint | DNS lookup failure | Return service unavailable | Verify DNS configuration |\n| TLS Handshake | Certificate validation fails | TLS negotiation error | Return TLS error with details | Check certificate validity |\n| TCP Connection | Network unreachable | Connection refused/timeout | Return connection error | Verify network connectivity |\n| HTTP Parsing | Malformed HTTP request | Parse error during request processing | Return HTTP 400 | Validate client HTTP implementation |\n| Request Size | Payload exceeds limits | Content-length or memory checks | Return HTTP 413 | Implement client-side batching |\n\nThe ingestion engine maintains circuit breakers for downstream dependencies like the storage engine and validation services. When failure rates exceed configurable thresholds, the circuit breaker opens and begins failing requests immediately rather than waiting for timeouts. This prevents cascade failures and provides faster feedback to clients.\n\n#### Backpressure and Flow Control\n\nWhen the metrics ingestion rate exceeds the system's processing capacity, the ingestion engine must implement intelligent backpressure mechanisms that maintain system stability while maximizing throughput. The backpressure system operates on multiple levels, from individual request handling to system-wide flow control.\n\nThe ingestion engine monitors several key metrics to detect capacity constraints:\n\n- Storage engine write latency and error rates\n- Memory usage in validation and processing pipelines  \n- CPU utilization in metric parsing and validation\n- Network buffer utilization for incoming connections\n- Queue depths in asynchronous processing stages\n\nWhen backpressure conditions are detected, the system implements graduated responses:\n\n1. **Initial Response**: Increase response latencies slightly to encourage client-side rate limiting\n2. **Moderate Pressure**: Begin rejecting lowest-priority metrics while accepting high-priority ones\n3. **High Pressure**: Return HTTP 503 responses with retry-after headers to implement explicit backoff\n4. **Critical Pressure**: Temporarily refuse new connections while processing existing request backlog\n5. **Emergency Mode**: Activate emergency shedding of non-critical metrics to prevent system collapse\n\nThe backpressure system maintains fairness by implementing per-client rate limiting that prevents any single metric producer from overwhelming the system. Clients that consistently produce high-quality, well-formatted metrics receive higher priority during backpressure conditions.\n\n⚠️ **Pitfall: Ignoring Cardinality Explosion During High Load**\nMany implementations focus on request rate limiting but ignore cardinality validation during high load conditions. This can lead to memory exhaustion as the system accepts high-cardinality metrics that consume excessive indexing resources. Always validate cardinality even when implementing backpressure—reject high-cardinality metrics first during resource constraints.\n\n### Storage Error Handling\n\nThe time-series storage engine faces unique challenges related to data persistence, consistency, and recovery from various failure modes. Storage errors can have long-lasting impact on system reliability and data availability, requiring sophisticated detection and recovery mechanisms.\n\n#### Mental Model: The Bank Vault System\n\nThink of storage error handling like a bank's vault security system. The vault has multiple layers of protection: tamper-evident seals detect unauthorized access, environmental sensors monitor temperature and humidity, backup power systems ensure continuous operation, and detailed audit logs track every access. When problems are detected, the system has specific protocols for different scenarios—some trigger immediate lockdown, others activate backup systems, and some simply log the event for later investigation.\n\nSimilarly, our storage engine employs multiple protective mechanisms. Write-ahead logging serves as the tamper-evident seal, ensuring we can detect and recover from interrupted operations. Health monitoring acts as environmental sensors, detecting degraded performance before complete failure. Backup and replication provide continuity during hardware failures. And comprehensive error logging enables forensic analysis of storage problems.\n\nThe key insight is that storage systems must be paranoid about data integrity while remaining operational during various failure conditions. Like a bank vault, the storage engine must balance security (data integrity) with accessibility (query performance).\n\n#### Disk Failures and Hardware Issues\n\nHardware-related storage failures require immediate detection and graceful degradation to prevent data loss while maintaining system availability. The storage engine implements comprehensive monitoring of disk health, filesystem status, and I/O performance to detect impending failures before they cause data corruption.\n\n| Hardware Failure | Early Warning Signs | Detection Method | Immediate Response | Recovery Procedure |\n|------------------|-------------------|------------------|-------------------|-------------------|\n| Disk Bad Sectors | Increasing read/write latency | SMART monitoring, I/O error tracking | Mark affected blocks read-only | Copy data to healthy storage, mark disk for replacement |\n| Filesystem Corruption | Checksum validation failures | File integrity verification | Switch to read-only mode | Run filesystem repair, restore from backup if needed |\n| Out of Disk Space | Free space below threshold | Disk usage monitoring | Activate emergency compaction | Delete oldest data per retention policy |\n| I/O Subsystem Failure | All disk operations failing | System call error detection | Activate read-only mode | Restart storage engine with backup configuration |\n| Memory Corruption | Unexpected data patterns | Data validation checks | Isolate corrupt memory regions | Restart process, reload data from persistent storage |\n| Network Partition | Cannot reach replica nodes | Network connectivity monitoring | Continue serving local data | Implement split-brain prevention protocols |\n\nThe storage engine maintains multiple redundancy mechanisms to handle hardware failures gracefully. The write-ahead log provides immediate durability for recent writes, while periodic snapshots enable recovery of the complete dataset. Block-based storage with checksums detects corruption early, and automatic compaction can often recover from minor filesystem issues.\n\nWhen hardware failures are detected, the storage engine transitions through a series of degraded operational modes:\n\n1. **Normal Operation**: All read and write operations functioning normally\n2. **Degraded Writes**: Read operations continue, writes queued to WAL only\n3. **Read-Only Mode**: Serve existing data, reject all write operations\n4. **Emergency Mode**: Serve cached data only, prepare for shutdown\n5. **Offline Mode**: Reject all operations, preserve data integrity for recovery\n\n#### Data Corruption Detection and Recovery\n\nData corruption in time-series storage can occur at multiple levels, from individual sample values to entire storage blocks. The storage engine implements comprehensive corruption detection and recovery mechanisms that operate continuously during normal operations.\n\nThe storage engine uses a multi-layered approach to corruption detection:\n\n**Block-Level Integrity**: Each storage block maintains cryptographic checksums that are validated on every read operation. When checksum mismatches are detected, the system immediately marks the block as corrupt and attempts to recover data from replicas or backup copies.\n\n**Series-Level Validation**: The storage engine periodically validates the consistency of time-series data, checking for impossible timestamp sequences, duplicate samples, and value ranges that violate metric type constraints. These validation passes run during low-traffic periods to minimize performance impact.\n\n**Cross-Reference Validation**: The storage engine maintains multiple indexes for the same data and periodically cross-validates these indexes to detect inconsistencies that might indicate memory corruption or software bugs.\n\n| Corruption Type | Detection Method | Recovery Strategy | Prevention Measure |\n|----------------|------------------|------------------|-------------------|\n| Block Checksum Mismatch | Read-time validation | Restore from replica or backup | Regular checksum verification |\n| Invalid Timestamps | Series consistency checks | Remove invalid samples | Timestamp validation on write |\n| Duplicate Samples | Index consistency validation | Deduplicate during compaction | Write-time duplicate detection |\n| Missing Index Entries | Cross-reference validation | Rebuild index from raw data | Atomic index updates |\n| Truncated Files | File size validation | Restore from backup or WAL | Atomic file operations |\n| Invalid Sample Values | Range validation checks | Mark samples as invalid | Input validation pipeline |\n\nThe recovery process for detected corruption follows a systematic approach:\n\n1. **Isolation**: Immediately mark corrupted data as unavailable for queries\n2. **Assessment**: Determine the extent of corruption and available recovery options  \n3. **Recovery**: Restore data from the most recent clean backup or replica\n4. **Validation**: Verify the recovered data passes all integrity checks\n5. **Integration**: Gradually reintroduce recovered data into normal operations\n6. **Analysis**: Investigate root cause to prevent similar corruption\n\n⚠️ **Pitfall: Assuming Filesystem Reliability**\nMany storage implementations assume the underlying filesystem provides perfect data integrity. However, filesystems can experience silent corruption, especially under high write loads or during power failures. Always implement application-level checksums and never rely solely on filesystem guarantees for data integrity.\n\n#### Storage Resource Management\n\nThe storage engine must carefully manage various system resources to prevent resource exhaustion while maintaining optimal performance. Resource management errors can cascade through the system, causing failures in other components that depend on storage availability.\n\nMemory management in the storage engine involves multiple pools with different lifecycle characteristics. The ingestion buffer pool handles short-lived allocations for incoming metric samples. The query result pool manages medium-lived allocations for query processing. The index cache pool maintains long-lived allocations for frequently accessed index data.\n\nFile descriptor management becomes critical in systems handling thousands of time series, as each active series may require multiple file handles for data blocks, indexes, and WAL segments. The storage engine implements sophisticated file handle pooling and lazy loading to stay within system limits while maintaining performance.\n\n| Resource Type | Monitoring Method | Warning Threshold | Critical Response | Prevention Strategy |\n|---------------|------------------|-------------------|------------------|-------------------|\n| Memory Usage | Process memory monitoring | 80% of available RAM | Activate memory pressure relief | Implement LRU caching with configurable limits |\n| File Descriptors | Open file handle counting | 90% of ulimit | Close least-recently-used files | File handle pooling and sharing |\n| Disk I/O Bandwidth | I/O utilization tracking | 90% of measured capacity | Throttle non-critical operations | Separate high/low priority queues |\n| Network Connections | Active connection monitoring | Connection pool 90% full | Reject new connections | Connection pooling and reuse |\n| CPU Usage | Process CPU monitoring | 95% sustained usage | Reduce background tasks | Asynchronous processing pipelines |\n| Temporary Disk Space | Temp directory monitoring | 95% of available space | Purge temporary files | Aggressive temporary file cleanup |\n\nThe storage engine implements resource quotas that prevent any single operation from consuming excessive resources. Query operations have memory limits that prevent runaway aggregations from exhausting system memory. Compaction operations have I/O bandwidth limits that prevent them from interfering with real-time ingestion and queries.\n\n### Alerting Error Handling\n\nThe alerting system must maintain high reliability while handling various failure modes that could prevent critical notifications from reaching their intended recipients. Alerting failures can have severe operational consequences, making robust error handling essential for production deployments.\n\n#### Mental Model: The Emergency Communication Network\n\nThink of alerting error handling like an emergency communication network during a natural disaster. The network has multiple communication channels (radio, satellite, cellular), backup power systems, redundant message routing, and detailed logging of all communication attempts. When the primary communication channel fails, the system automatically switches to backup channels. When messages can't be delivered immediately, they're queued for retry with escalating urgency. Critical messages are sent through multiple channels simultaneously to ensure delivery.\n\nThis emergency network metaphor captures the essential characteristics of reliable alerting systems: redundancy, persistence, escalation, and comprehensive audit trails. Just as emergency responders need absolute confidence that their communications will reach recipients, operations teams need confidence that critical alerts will be delivered even during system failures.\n\n#### Notification Delivery Failures\n\nNotification delivery failures represent some of the most critical errors in the metrics system, as they can prevent teams from responding to production incidents. The alerting system must implement sophisticated retry logic, escalation policies, and fallback mechanisms to ensure critical notifications reach their intended recipients.\n\nThe notification delivery system categorizes failures into distinct classes that require different handling approaches:\n\n| Failure Class | Examples | Retry Strategy | Escalation Policy | Fallback Action |\n|---------------|----------|---------------|------------------|-----------------|\n| Transient Network | DNS timeouts, connection refused | Exponential backoff, max 5 retries | Try alternate endpoints after 3 failures | Switch to backup notification channel |\n| Authentication | Invalid API keys, expired tokens | No retry, immediate escalation | Alert configuration team | Use emergency webhook endpoint |\n| Rate Limiting | API quota exceeded, throttling | Respect rate limits, queue messages | Spread notifications across time | Batch multiple alerts into single message |\n| Recipient Invalid | Email bounces, invalid Slack channel | No retry, log error | Remove invalid recipient from future notifications | Send to backup recipient list |\n| Message Format | Invalid JSON, template errors | Fix format if possible, otherwise skip | Alert template maintainers | Use plain text fallback message |\n| Service Unavailable | Slack/PagerDuty outage | Extended retry with circuit breaker | Try all configured channels | Store notifications for later delivery |\n\nThe notification delivery engine maintains detailed logs of every delivery attempt, including timestamps, recipient information, delivery status, error details, and retry attempts. These logs are essential for troubleshooting notification issues and proving compliance with SLA requirements.\n\n> **Decision: Notification Persistence Strategy**\n> - **Context**: Notifications may fail delivery due to various transient issues, but critical alerts must eventually reach recipients\n> - **Options Considered**:\n>   1. Best-effort delivery with no persistence\n>   2. In-memory queues with limited retry\n>   3. Persistent notification queue with guaranteed delivery\n> - **Decision**: Persistent notification queue with configurable retry policies\n> - **Rationale**: Critical alerts justify the complexity of persistent delivery guarantees\n> - **Consequences**: Increased system complexity but much higher notification reliability\n\nThe notification persistence system stores failed notifications in a durable queue that survives system restarts and network outages. Each notification includes metadata about retry attempts, delivery windows, and escalation policies. The system implements intelligent retry scheduling that backs off exponentially while respecting recipient preferences for notification frequency.\n\n#### Alert Flapping Prevention\n\nAlert flapping—rapid transitions between firing and resolved states—can overwhelm notification channels and desensitize operations teams to legitimate alerts. The alerting system implements sophisticated flapping detection and suppression mechanisms that maintain alert responsiveness while preventing notification storms.\n\nThe flapping detection algorithm analyzes alert state transition patterns over configurable time windows. It considers factors like transition frequency, state duration, and the underlying metric volatility to distinguish between legitimate alerts and flapping conditions.\n\n| Flapping Pattern | Detection Criteria | Suppression Action | Resolution Strategy |\n|------------------|-------------------|-------------------|-------------------|\n| Rapid Fire/Resolve | >5 transitions in 10 minutes | Extend evaluation duration | Increase alert threshold hysteresis |\n| Threshold Bouncing | Value oscillates around threshold | Apply smoothing to evaluation | Use moving averages instead of instant values |\n| Intermittent Failures | Alternating success/failure states | Require sustained failure condition | Implement \"consecutive failures\" logic |\n| Metric Spikes | Brief value spikes trigger alerts | Ignore single-sample anomalies | Require multiple samples above threshold |\n| Clock Skew Issues | Timestamp inconsistencies cause false alerts | Detect and correct timestamp drift | Implement timestamp validation and adjustment |\n| Configuration Errors | Overly sensitive thresholds | Temporarily disable problematic rules | Alert rule configuration validation |\n\nThe alerting system implements configurable hysteresis for threshold-based alerts, requiring different thresholds for firing and resolving conditions. For example, a CPU usage alert might fire when usage exceeds 90% but only resolve when usage drops below 85%. This prevents flapping when metric values oscillate around the threshold.\n\nThe flapping prevention system maintains state for each alert rule, tracking recent transition history and applying appropriate suppression policies. When flapping is detected, the system can take several actions:\n\n1. **Temporary Suppression**: Silence notifications while allowing the alert to continue evaluation\n2. **Extended Evaluation**: Increase the duration requirement before firing alerts\n3. **Threshold Adjustment**: Temporarily modify thresholds to reduce sensitivity\n4. **Aggregation Mode**: Switch to evaluating aggregated metrics instead of instant values\n5. **Maintenance Mode**: Mark the alert rule for manual review and adjustment\n\n⚠️ **Pitfall: Over-Aggressive Flapping Suppression**\nImplementing flapping suppression that's too aggressive can prevent legitimate alerts from firing during rapidly changing conditions. Always maintain escape hatches that allow critical alerts to bypass flapping suppression, and regularly review suppressed alerts to ensure they represent genuine flapping rather than system issues.\n\n#### Silence Management and Maintenance Windows\n\nOperations teams need the ability to temporarily suppress notifications during planned maintenance windows or when investigating known issues. The alerting system implements sophisticated silence management that prevents unwanted notifications while maintaining audit trails and preventing accidentally permanent suppressions.\n\nThe silence management system supports multiple types of suppressions:\n\n**Time-Based Silences**: Suppress alerts for specific time periods, commonly used for maintenance windows. These silences have definite start and end times and automatically expire to prevent forgotten suppressions.\n\n**Label-Based Silences**: Suppress alerts matching specific label combinations, useful for suppressing alerts from problematic hosts or services during investigation.\n\n**Alert-Specific Silences**: Suppress specific alert rules entirely, typically used for rules that are being debugged or reconfigured.\n\n**Escalation Silences**: Allow alerts to fire but suppress specific notification channels, useful for reducing noise during incident response.\n\n| Silence Type | Configuration | Audit Requirements | Expiration Policy |\n|--------------|---------------|-------------------|------------------|\n| Maintenance Window | Start/end time, affected labels | Who created, reason, affected systems | Automatic expiration at end time |\n| Investigation Silence | Label matchers, duration | Incident ticket reference | Manual renewal required after 24 hours |\n| Rule Debugging | Specific rule ID, duration | Configuration change justification | Maximum 1 week duration |\n| Channel Suppression | Channel and alert combinations | Escalation acknowledgment | Automatic expiration after incident resolution |\n| Emergency Silence | Broad label matchers, short duration | Incident commander authorization | Maximum 4 hours, requires manual renewal |\n\nThe silence management system maintains comprehensive audit logs that track silence creation, modification, and expiration. These logs include the identity of the user creating the silence, the justification for the suppression, and the expected impact on alerting coverage.\n\nAll silences require explicit expiration times to prevent accidentally permanent suppressions. The system sends notifications to silence creators before silences expire, allowing them to extend or modify the suppression if needed. Critical silences that broadly suppress alerts require additional approval workflows and shorter maximum durations.\n\nThe alerting system provides visibility into active silences through the dashboard interface, showing which alerts are currently suppressed and why. This visibility helps prevent situations where important alerts are inadvertently suppressed during critical incidents.\n\n### Common Pitfalls in Error Handling\n\nUnderstanding common error handling mistakes helps avoid subtle bugs that can compromise system reliability. These pitfalls represent patterns frequently encountered when building production metrics systems.\n\n⚠️ **Pitfall: Cascading Failure Amplification**\nWhen one component fails, poorly designed error handling can amplify the failure across the entire system. For example, if the storage engine becomes slow, aggressive retry logic in the ingestion engine can make the problem worse by increasing load. Always implement circuit breakers and exponential backoff to prevent failure amplification.\n\n⚠️ **Pitfall: Silent Data Loss During Errors**\nSystems sometimes silently drop data during error conditions without proper logging or alerting. This is particularly dangerous in metrics systems where missing data points can hide real problems. Always log data loss events and implement monitoring for data ingestion rates.\n\n⚠️ **Pitfall: Inconsistent Error Response Formats**\nDifferent components returning errors in different formats makes client error handling difficult and unreliable. Standardize error response formats across all API endpoints and include sufficient detail for programmatic handling.\n\n⚠️ **Pitfall: Inadequate Error Context**\nError messages that lack context make troubleshooting difficult and time-consuming. Always include relevant identifiers (metric names, timestamps, client IDs) in error messages to enable efficient debugging.\n\n⚠️ **Pitfall: Blocking Operations During Error Handling**\nError handling code that performs blocking operations can cause the entire system to become unresponsive during failure conditions. Use timeouts, asynchronous processing, and non-blocking I/O in all error handling paths.\n\n### Implementation Guidance\n\nThis implementation guidance provides practical code structure and patterns for implementing comprehensive error handling throughout the metrics and alerting system.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Error Logging | Standard library logging with structured fields | Structured logging with correlation IDs (slog) |\n| Health Monitoring | HTTP endpoint with JSON status | Prometheus metrics with detailed health checks |\n| Circuit Breakers | Simple failure counting with timeouts | Hystrix-style circuit breakers with metrics |\n| Retry Logic | Exponential backoff with jitter | Sophisticated retry policies with circuit breakers |\n| Error Persistence | In-memory error queues | Persistent error storage with WAL |\n| Monitoring Integration | Basic metrics collection | Full observability with tracing and metrics |\n\n#### File Structure for Error Handling\n\n```\ninternal/\n├── errors/\n│   ├── types.go              ← error type definitions and interfaces\n│   ├── validation.go         ← input validation errors and handling\n│   ├── storage.go           ← storage-specific error types\n│   ├── network.go           ← network and transport errors\n│   └── recovery.go          ← error recovery and retry logic\n├── health/\n│   ├── manager.go           ← health check coordination\n│   ├── checks.go            ← individual health check implementations\n│   └── status.go            ← health status aggregation\n├── circuit/\n│   ├── breaker.go           ← circuit breaker implementation\n│   └── metrics.go           ← circuit breaker metrics and monitoring\n└── retry/\n    ├── policy.go            ← retry policy definitions\n    ├── backoff.go          ← backoff algorithms\n    └── queue.go            ← persistent retry queue\n```\n\n#### Core Error Type Definitions\n\n```go\npackage errors\n\nimport (\n    \"context\"\n    \"time\"\n    \"fmt\"\n)\n\n// ValidationErrors represents a collection of validation failures\ntype ValidationErrors struct {\n    Errors []ValidationError `json:\"errors\"`\n}\n\n// ValidationError represents a single field validation failure\ntype ValidationError struct {\n    Field   string `json:\"field\"`\n    Value   string `json:\"value\"`\n    Message string `json:\"message\"`\n}\n\n// Add appends a new validation error to the collection\nfunc (ve *ValidationErrors) Add(field, value, message string) {\n    // TODO 1: Create new ValidationError with provided parameters\n    // TODO 2: Append to the Errors slice\n    // TODO 3: Ensure thread safety if called concurrently\n}\n\n// ToJSON converts validation errors to JSON for API responses\nfunc (ve *ValidationErrors) ToJSON() ([]byte, error) {\n    // TODO 1: Use encoding/json to marshal the struct\n    // TODO 2: Handle marshaling errors appropriately\n    // TODO 3: Return formatted JSON bytes\n}\n\n// StorageError represents errors from the storage engine\ntype StorageError struct {\n    Operation string    `json:\"operation\"`\n    Cause     string    `json:\"cause\"`\n    Timestamp time.Time `json:\"timestamp\"`\n    Retryable bool      `json:\"retryable\"`\n}\n\n// Error implements the error interface\nfunc (se *StorageError) Error() string {\n    return fmt.Sprintf(\"storage %s failed: %s\", se.Operation, se.Cause)\n}\n\n// NetworkError represents network-related failures\ntype NetworkError struct {\n    Endpoint  string        `json:\"endpoint\"`\n    Operation string        `json:\"operation\"`\n    Timeout   time.Duration `json:\"timeout\"`\n    Attempt   int           `json:\"attempt\"`\n}\n\n// Error implements the error interface\nfunc (ne *NetworkError) Error() string {\n    return fmt.Sprintf(\"network %s to %s failed on attempt %d\", ne.Operation, ne.Endpoint, ne.Attempt)\n}\n```\n\n#### Health Management Implementation\n\n```go\npackage health\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \"log/slog\"\n)\n\n// HealthManager coordinates health checks across system components\ntype HealthManager struct {\n    checks    map[string]*HealthCheck\n    mu        sync.RWMutex\n    logger    *slog.Logger\n    interval  time.Duration\n    stopCh    chan struct{}\n}\n\n// HealthCheck represents a single component health check\ntype HealthCheck struct {\n    Name        string       `json:\"name\"`\n    Status      HealthStatus `json:\"status\"`\n    LastChecked time.Time    `json:\"last_checked\"`\n    Message     string       `json:\"message\"`\n    CheckFunc   func(context.Context) error\n}\n\n// NewHealthManager creates a new health check coordinator\nfunc NewHealthManager(logger *slog.Logger, interval time.Duration) *HealthManager {\n    // TODO 1: Initialize HealthManager struct with provided parameters\n    // TODO 2: Create empty checks map\n    // TODO 3: Create stop channel for graceful shutdown\n    // TODO 4: Return configured manager instance\n}\n\n// RegisterCheck adds a health check function for a component\nfunc (hm *HealthManager) RegisterCheck(name string, checkFunc func(context.Context) error) {\n    // TODO 1: Acquire write lock for thread safety\n    // TODO 2: Create HealthCheck struct with name and function\n    // TODO 3: Initialize status as unknown and set initial timestamp\n    // TODO 4: Add to checks map\n    // TODO 5: Log registration event\n}\n\n// RunChecks executes all registered health checks\nfunc (hm *HealthManager) RunChecks(ctx context.Context) {\n    // TODO 1: Acquire read lock to get copy of checks\n    // TODO 2: Iterate through all registered checks\n    // TODO 3: Execute each check function with timeout\n    // TODO 4: Update check status based on result\n    // TODO 5: Log any status changes\n    // TODO 6: Handle context cancellation gracefully\n}\n\n// GetOverallStatus returns the worst status among all checks\nfunc (hm *HealthManager) GetOverallStatus() HealthStatus {\n    // TODO 1: Acquire read lock\n    // TODO 2: Initialize status as Healthy\n    // TODO 3: Iterate through all checks\n    // TODO 4: Find the worst status (Unhealthy > Degraded > Healthy)\n    // TODO 5: Return overall system status\n}\n```\n\n#### Circuit Breaker Implementation\n\n```go\npackage circuit\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \"errors\"\n)\n\n// Breaker implements circuit breaker pattern for fault tolerance\ntype Breaker struct {\n    name           string\n    maxFailures    int\n    resetTimeout   time.Duration\n    state          BreakerState\n    failures       int\n    lastFailure    time.Time\n    mu            sync.RWMutex\n    onStateChange func(string, BreakerState, BreakerState)\n}\n\n// BreakerState represents circuit breaker states\ntype BreakerState int\n\nconst (\n    StateClosed BreakerState = iota\n    StateOpen\n    StateHalfOpen\n)\n\n// Execute runs the provided function through the circuit breaker\nfunc (cb *Breaker) Execute(ctx context.Context, operation func(context.Context) error) error {\n    // TODO 1: Check current circuit breaker state\n    // TODO 2: If open, check if reset timeout has elapsed\n    // TODO 3: If closed or half-open, attempt to execute operation\n    // TODO 4: Handle operation success (reset failure count)\n    // TODO 5: Handle operation failure (increment count, possibly trip breaker)\n    // TODO 6: Update state and notify state change listeners\n    // TODO 7: Return appropriate error based on state and operation result\n}\n\n// tripBreaker changes state to open and records the failure time\nfunc (cb *Breaker) tripBreaker() {\n    // TODO 1: Acquire write lock\n    // TODO 2: Change state to StateOpen\n    // TODO 3: Record current time as lastFailure\n    // TODO 4: Call state change callback if registered\n}\n\n// resetBreaker changes state to closed and clears failure count\nfunc (cb *Breaker) resetBreaker() {\n    // TODO 1: Acquire write lock\n    // TODO 2: Change state to StateClosed\n    // TODO 3: Reset failures count to zero\n    // TODO 4: Call state change callback if registered\n}\n```\n\n#### Retry Policy Implementation\n\n```go\npackage retry\n\nimport (\n    \"context\"\n    \"time\"\n    \"math\"\n    \"math/rand\"\n)\n\n// Policy defines retry behavior for failed operations\ntype Policy struct {\n    MaxAttempts     int\n    InitialDelay    time.Duration\n    MaxDelay        time.Duration\n    Multiplier      float64\n    Jitter          bool\n    RetryableErrors []error\n}\n\n// Execute attempts an operation with retry logic\nfunc (p *Policy) Execute(ctx context.Context, operation func(context.Context) error) error {\n    // TODO 1: Initialize attempt counter and delay\n    // TODO 2: Loop until max attempts reached or context cancelled\n    // TODO 3: Execute operation and check result\n    // TODO 4: If successful, return nil\n    // TODO 5: If error not retryable, return immediately\n    // TODO 6: Calculate next delay with exponential backoff\n    // TODO 7: Apply jitter if configured\n    // TODO 8: Wait for delay duration or context cancellation\n    // TODO 9: Continue to next attempt\n}\n\n// calculateDelay computes the next retry delay with exponential backoff\nfunc (p *Policy) calculateDelay(attempt int) time.Duration {\n    // TODO 1: Calculate exponential delay: initialDelay * (multiplier ^ attempt)\n    // TODO 2: Apply maximum delay limit\n    // TODO 3: Add jitter if enabled (±25% random variation)\n    // TODO 4: Return final delay duration\n}\n\n// isRetryable checks if an error should trigger a retry attempt\nfunc (p *Policy) isRetryable(err error) bool {\n    // TODO 1: Check if error is nil (should not retry success)\n    // TODO 2: If no retryable errors configured, retry all errors\n    // TODO 3: Iterate through configured retryable error types\n    // TODO 4: Use errors.Is() to match error types\n    // TODO 5: Return true if error matches any retryable type\n}\n```\n\n#### Milestone Checkpoints\n\n**After Implementing Error Handling Infrastructure:**\n1. Run `go test ./internal/errors/... -v` to verify error type handling\n2. Test health checks with `curl http://localhost:8080/health` - should return JSON with component statuses\n3. Verify circuit breaker functionality by simulating downstream failures\n4. Confirm retry policies work by testing with flaky network connections\n5. Check error logging appears in structured format with appropriate detail levels\n\n**Signs of Proper Error Handling:**\n- Error responses include actionable information for clients\n- System remains responsive during various failure conditions\n- Health checks accurately reflect component status\n- Circuit breakers prevent cascade failures during outages\n- Retry logic recovers from transient failures automatically\n\n**Common Issues to Debug:**\n- Errors returning without sufficient context for troubleshooting\n- Circuit breakers tripping too frequently due to overly sensitive thresholds  \n- Retry logic causing thundering herd problems during recovery\n- Health checks reporting false positives or negatives\n- Error handling blocking normal operations during failure conditions\n\n\n## Testing Strategy\n\n> **Milestone(s):** All milestones (comprehensive testing is essential throughout the entire metrics and alerting system, from initial metric ingestion through storage, querying, visualization, and alerting capabilities)\n\n### The Quality Assurance Laboratory: Understanding Testing Approach\n\nThink of our testing strategy as operating a quality assurance laboratory for a pharmaceutical company. Just as a pharmaceutical lab has multiple testing phases—from testing individual chemical compounds in isolation, to testing drug interactions, to full clinical trials with real patients—our metrics system requires layered testing that validates everything from individual component behavior to complete end-to-end workflows. Each testing layer serves a specific purpose and catches different categories of problems, much like how pharmaceutical testing catches different types of safety and efficacy issues at each stage.\n\nThe fundamental challenge in testing a metrics and alerting system lies in the inherently time-dependent and stateful nature of the system. Unlike testing a simple web API where requests are largely independent, our system maintains complex state across time-series data, alert conditions, and dashboard subscriptions. We must validate not just that individual operations work correctly, but that the system maintains consistency and correctness over time, handles various failure scenarios gracefully, and performs adequately under realistic load conditions.\n\nOur testing strategy employs three complementary layers that work together to provide comprehensive validation coverage. Unit testing validates individual component behavior in isolation, integration testing verifies component interactions and data flows, and milestone validation checkpoints ensure that each major system capability works correctly in realistic scenarios. This layered approach allows us to catch bugs early in development while also validating that the complete system meets its functional and non-functional requirements.\n\n### Unit Testing Strategy\n\nUnit testing forms the foundation of our testing pyramid, focusing on validating individual components in complete isolation from their dependencies. The primary goal is to verify that each component correctly implements its specified behavior under all possible input conditions, including edge cases and error scenarios that might be difficult to reproduce in integration testing.\n\n#### Component Isolation and Mocking Strategy\n\nOur unit testing approach treats each component as a black box with clearly defined inputs and outputs. We use dependency injection and interface-based design to isolate components from their dependencies, allowing us to substitute mock implementations that provide predictable behavior during testing.\n\n> **Decision: Interface-Based Mocking**\n> - **Context**: Need to test components in isolation without external dependencies like storage, network, or file system\n> - **Options Considered**: Direct mocking libraries, interface-based mocks, test doubles with recording capabilities\n> - **Decision**: Interface-based mocks with behavior verification\n> - **Rationale**: Provides type safety, clear contracts, and ability to verify interaction patterns\n> - **Consequences**: Requires interfaces for all major dependencies but provides robust isolation and clear test intent\n\nThe following table outlines our mocking strategy for major component dependencies:\n\n| Component Under Test | Mock Dependencies | Mock Behavior | Verification Points |\n|---------------------|-------------------|---------------|-------------------|\n| `MetricsIngester` | `TimeSeriesStorage`, `Validator`, `CardinalityManager` | Return predictable responses for storage operations | Verify correct validation sequence, storage calls, error propagation |\n| `QueryProcessor` | `TimeSeriesStorage`, `QueryCache` | Simulate various data availability scenarios | Verify query optimization, cache utilization, result formatting |\n| `AlertEvaluator` | `QueryProcessor`, `StateManager`, `NotificationManager` | Control metric values and state transitions | Verify rule evaluation logic, state changes, notification triggers |\n| `StorageEngine` | File system, network resources | Simulate disk operations and failures | Verify data persistence, compaction logic, error recovery |\n| `DashboardServer` | `QueryProcessor`, WebSocket connections | Control query results and client interactions | Verify subscription management, real-time updates, error handling |\n\n#### Core Component Testing Coverage\n\nEach component requires comprehensive testing that covers normal operation, boundary conditions, error scenarios, and concurrent access patterns. The testing approach varies based on the component's responsibility and complexity.\n\n**Metrics Ingestion Testing**\n\nThe `MetricsIngester` component requires extensive testing of its validation and processing pipeline. We test each metric type individually to ensure proper handling of counters, gauges, and histograms. Critical test scenarios include:\n\n1. **Metric Type Validation**: Verify that each metric type is processed according to its semantics—counters must be monotonically increasing, gauges can have arbitrary values, and histograms must include proper bucket definitions.\n\n2. **Label Validation and Cardinality Control**: Test label parsing, normalization, and cardinality explosion prevention. This includes testing the rejection of metrics that would exceed cardinality limits and proper handling of malformed label specifications.\n\n3. **Batch Processing**: Validate that metric batches are processed atomically and that partial failures in a batch don't corrupt the storage state.\n\n4. **Backpressure Handling**: Test behavior when the storage layer cannot keep up with ingestion rates, ensuring proper flow control and graceful degradation.\n\n**Time-Series Storage Testing**\n\nThe `StorageEngine` component requires testing of its complex persistence and compaction logic. Key testing areas include:\n\n1. **Write Path Validation**: Test sample appending through the write-ahead log, ensuring proper durability guarantees and crash recovery capabilities.\n\n2. **Compaction Logic**: Verify that compaction correctly merges blocks, maintains temporal ordering, and applies retention policies without data loss.\n\n3. **Query Path Optimization**: Test series lookup, time range queries, and label filtering to ensure correct results and acceptable performance characteristics.\n\n4. **Concurrent Access**: Validate thread safety during simultaneous read and write operations, ensuring no data corruption or race conditions.\n\n**Query Engine Testing**\n\nThe `QueryProcessor` component requires testing of its parsing, planning, and execution pipeline:\n\n1. **Query Parsing**: Test the lexical analyzer and parser with various query syntaxes, including edge cases like empty queries, malformed expressions, and complex nested functions.\n\n2. **Query Planning**: Verify that the execution planner generates optimal plans for different query patterns and correctly estimates resource requirements.\n\n3. **Aggregation Functions**: Test each aggregation function individually with various input patterns, including sparse data, missing values, and extreme values.\n\n4. **Cache Integration**: Validate cache hit/miss behavior, cache invalidation, and memory management under different query patterns.\n\n**Alert Evaluation Testing**\n\nThe `AlertEvaluator` component requires testing of its state machine and notification logic:\n\n1. **State Transitions**: Test all possible alert state transitions, ensuring proper timing constraints and notification triggers.\n\n2. **Rule Evaluation**: Verify that alert rules are evaluated correctly with various metric patterns, including missing data and counter resets.\n\n3. **Notification Dispatch**: Test the notification queue and channel integration, including retry logic and failure handling.\n\n4. **Timing Accuracy**: Validate evaluation intervals and duration-based conditions are handled with appropriate precision.\n\n#### Error Condition Testing\n\nEach component must handle various error conditions gracefully, maintaining system stability and providing useful diagnostic information. Our error testing strategy covers the following categories:\n\n| Error Category | Test Scenarios | Expected Behavior | Validation Method |\n|---------------|----------------|-------------------|-------------------|\n| Input Validation | Malformed metrics, invalid labels, type mismatches | Reject invalid input with descriptive errors | Assert specific error messages and codes |\n| Resource Exhaustion | Memory limits, disk space, connection limits | Graceful degradation with backpressure | Monitor resource usage and response times |\n| External Dependencies | Storage failures, network timeouts, cache misses | Proper error propagation and retry logic | Inject failures and verify recovery |\n| Concurrent Access | Race conditions, deadlocks, inconsistent reads | Thread-safe operation without data corruption | Concurrent test execution with verification |\n| Configuration Errors | Invalid settings, missing parameters, type errors | Startup validation with clear error messages | Configuration validation testing |\n\n#### Performance and Resource Usage Testing\n\nUnit tests must also validate performance characteristics and resource usage to prevent regressions:\n\n1. **Memory Usage**: Test that components maintain bounded memory usage even with large inputs or extended operation periods.\n\n2. **Processing Latency**: Verify that individual operations complete within acceptable time bounds under various load conditions.\n\n3. **Resource Cleanup**: Ensure that components properly release resources (file handles, connections, goroutines) when stopped or during error conditions.\n\n4. **Scalability Characteristics**: Test behavior with varying data sizes to identify algorithmic complexity issues early.\n\n#### Common Unit Testing Pitfalls\n\n⚠️ **Pitfall: Testing Implementation Details Instead of Behavior**\nMany developers write tests that verify internal implementation details rather than observable behavior. For example, testing that a specific internal method was called with particular parameters rather than testing that the component produces the correct output for given inputs. This makes tests brittle and couples them unnecessarily to implementation details.\n\n**Why it's wrong**: Tests that depend on implementation details break when the implementation changes, even if the behavior remains correct. This reduces confidence in refactoring and makes maintenance more difficult.\n\n**How to fix**: Focus tests on the public interface and observable behavior. Test inputs and outputs, not internal method calls. Use behavior verification sparingly and only when the interaction pattern is part of the contract.\n\n⚠️ **Pitfall: Insufficient Error Scenario Coverage**\nUnit tests often focus heavily on happy-path scenarios while neglecting error conditions. This leaves error handling code untested and can lead to poor error messages or improper resource cleanup during failures.\n\n**Why it's wrong**: Error conditions are often the most critical code paths for system reliability. Untested error handling can lead to data corruption, resource leaks, or cascading failures.\n\n**How to fix**: Systematically test every error condition that can be triggered. Use dependency injection to simulate failures in external dependencies. Test resource cleanup during both normal and error conditions.\n\n⚠️ **Pitfall: Non-Deterministic Test Behavior**\nTests that depend on timing, random values, or external state can produce inconsistent results, making them unreliable for continuous integration and debugging.\n\n**Why it's wrong**: Flaky tests reduce confidence in the test suite and make it difficult to identify real regressions. Teams often ignore failing tests that are known to be unreliable.\n\n**How to fix**: Use deterministic inputs and mock time-dependent operations. Control randomness with fixed seeds. Isolate tests from external state and shared resources.\n\n### Integration Testing\n\nIntegration testing validates the interactions between components and verifies that data flows correctly through the complete system pipeline. Unlike unit tests that use mocks to isolate components, integration tests use real implementations of dependencies to validate that components work correctly together.\n\n#### End-to-End Workflow Validation\n\nOur integration testing strategy focuses on validating complete workflows that span multiple components. Each workflow represents a critical system capability that must function correctly for the system to provide value to users.\n\n**Metric Ingestion to Storage Workflow**\n\nThis integration test validates the complete path from metric submission through validation, processing, and persistence. The test creates a realistic mix of metric types and verifies that data is correctly stored and retrievable:\n\n1. **Setup Phase**: Initialize a complete storage engine with real file system persistence, configure the ingestion pipeline with actual validators and cardinality managers.\n\n2. **Data Submission**: Submit a variety of metrics including counters with increasing values, gauges with fluctuating values, and histograms with different bucket distributions.\n\n3. **Processing Verification**: Verify that metrics are processed according to their type semantics and that validation rules are properly applied.\n\n4. **Storage Validation**: Query the storage engine directly to verify that samples are persisted with correct timestamps, values, and label associations.\n\n5. **Compaction Testing**: Allow time for compaction to occur and verify that data remains accessible and correct after compaction operations.\n\n**Query Processing to Dashboard Workflow**\n\nThis integration test validates the complete query execution pipeline from parsing through result delivery to dashboard clients:\n\n1. **Data Preparation**: Populate the storage engine with time-series data covering multiple metrics with various label combinations and time ranges.\n\n2. **Query Execution**: Execute queries of varying complexity, including simple metric selection, label filtering, time range specification, and aggregation functions.\n\n3. **Result Validation**: Verify that query results contain the expected data points with correct values and timestamps.\n\n4. **Dashboard Integration**: Test the dashboard server's ability to execute queries and deliver results to WebSocket clients with proper formatting.\n\n5. **Real-Time Updates**: Verify that new data points trigger appropriate updates to subscribed dashboard panels.\n\n**Alert Evaluation to Notification Workflow**\n\nThis integration test validates the complete alerting pipeline from rule evaluation through notification delivery:\n\n1. **Alert Rule Configuration**: Configure alert rules with various threshold conditions and notification channels.\n\n2. **Metric Data Injection**: Submit metric data that should trigger alert conditions, including scenarios for alert firing and resolution.\n\n3. **Evaluation Process**: Verify that the alert evaluator correctly processes rules according to their evaluation intervals.\n\n4. **State Management**: Validate that alert states transition correctly through the state machine with proper timing constraints.\n\n5. **Notification Delivery**: Verify that notifications are generated and delivered through configured channels with correct message content.\n\n#### Cross-Component Data Flow Testing\n\nIntegration tests must validate that data maintains consistency and correctness as it flows between components. This includes testing serialization/deserialization, data transformations, and error propagation.\n\n| Data Flow Path | Test Scenarios | Validation Points | Expected Behavior |\n|---------------|----------------|-------------------|-------------------|\n| Ingestion → Storage | Metric batches, label normalization, type validation | Data persistence, retrieval accuracy, error handling | Metrics stored exactly as specified with proper indexing |\n| Storage → Query | Time range queries, label filtering, aggregation | Result correctness, performance, partial results | Accurate query results within performance bounds |\n| Query → Dashboard | Real-time updates, subscription management, formatting | Data freshness, update frequency, client synchronization | Timely delivery of formatted data to subscribed clients |\n| Query → Alerting | Rule evaluation, state transitions, threshold checking | Evaluation accuracy, timing precision, state consistency | Correct alert firing and resolution based on metric values |\n| Alerting → Notification | Message formatting, channel routing, delivery confirmation | Template processing, channel selection, retry logic | Reliable notification delivery with proper formatting |\n\n#### Database and File System Integration\n\nThe storage engine requires extensive integration testing with the actual file system to validate persistence, durability, and recovery capabilities:\n\n1. **Crash Recovery Testing**: Simulate system crashes at various points during write operations to verify that the write-ahead log correctly recovers uncommitted data.\n\n2. **Compaction Integration**: Test compaction operations with real file I/O to verify that data is correctly merged and old files are properly cleaned up.\n\n3. **Concurrent Access**: Run multiple ingestion and query processes simultaneously to validate file locking and concurrent access patterns.\n\n4. **Disk Space Management**: Test behavior when disk space is exhausted, including proper error handling and graceful degradation.\n\n#### Network and Protocol Integration\n\nThe system's network interfaces require testing with real HTTP clients and WebSocket connections:\n\n1. **HTTP API Testing**: Test the metric ingestion API with various HTTP clients, including proper handling of request timeouts, connection limits, and error responses.\n\n2. **WebSocket Integration**: Test dashboard WebSocket connections with real clients, including connection lifecycle management, message delivery, and error handling.\n\n3. **Prometheus Integration**: Test compatibility with Prometheus exposition format and scraping protocols.\n\n4. **Load Balancer Integration**: Test behavior when deployed behind load balancers, including proper session affinity for WebSocket connections.\n\n#### Configuration and Environment Integration\n\nIntegration tests must validate that the system works correctly across different configuration scenarios and deployment environments:\n\n| Configuration Area | Test Scenarios | Validation Approach |\n|-------------------|----------------|-------------------|\n| Storage Configuration | Different retention policies, compaction intervals, data directories | Verify policy enforcement and resource usage |\n| Network Configuration | Various port assignments, timeout settings, connection limits | Test connectivity and performance characteristics |\n| Alert Configuration | Different evaluation intervals, notification channels, message templates | Validate alert behavior and message formatting |\n| Security Configuration | Authentication settings, access controls, encryption options | Test security enforcement and proper access control |\n\n#### Performance Integration Testing\n\nIntegration tests must validate system performance under realistic conditions:\n\n1. **Throughput Testing**: Measure ingestion rates with realistic metric loads and verify that storage can keep up with sustained ingestion.\n\n2. **Query Performance**: Test query response times with realistic data volumes and concurrent query loads.\n\n3. **Memory Usage**: Monitor memory consumption during extended operation to identify memory leaks or excessive usage patterns.\n\n4. **Resource Scaling**: Test system behavior as resource usage approaches configured limits.\n\n> The key insight for integration testing is that it validates the system's behavior in realistic operational conditions, catching issues that unit tests cannot detect because they occur only when components interact with real dependencies.\n\n#### Common Integration Testing Pitfalls\n\n⚠️ **Pitfall: Insufficient Test Data Diversity**\nIntegration tests often use simple, uniform test data that doesn't reflect the complexity and diversity of real-world metric data. This can miss edge cases that only occur with specific data patterns or label combinations.\n\n**Why it's wrong**: Real-world metrics data has complex patterns including sparse data, high cardinality labels, irregular timestamps, and extreme values. Simple test data doesn't exercise the system under realistic conditions.\n\n**How to fix**: Use representative test data that includes various metric types, cardinality patterns, and temporal characteristics. Generate test data that reflects actual usage patterns and includes edge cases like missing data points and counter resets.\n\n⚠️ **Pitfall: Ignoring Resource Cleanup Between Tests**\nIntegration tests that don't properly clean up resources between test runs can have interdependencies that make test results non-deterministic or cause resource exhaustion.\n\n**Why it's wrong**: Resource leaks or shared state between tests makes it difficult to isolate test failures and can cause tests to pass or fail based on execution order rather than actual correctness.\n\n**How to fix**: Implement thorough cleanup procedures that reset all persistent state, close network connections, and release file handles. Use isolated test environments and verify resource cleanup as part of the test framework.\n\n⚠️ **Pitfall: Inadequate Error Injection**\nIntegration tests often focus on happy-path scenarios and don't adequately test error conditions that can occur when components interact with real dependencies.\n\n**Why it's wrong**: Many errors only manifest when components interact with real dependencies under stress or failure conditions. These scenarios are critical for system reliability but are difficult to test without proper error injection.\n\n**How to fix**: Use chaos engineering techniques to inject failures into dependencies. Test network partitions, disk failures, memory pressure, and other realistic failure scenarios. Verify that the system maintains consistency and provides meaningful error messages during failures.\n\n### Milestone Validation Checkpoints\n\nMilestone validation checkpoints provide structured verification that each major system capability works correctly before moving to the next development phase. These checkpoints bridge the gap between technical testing and user-visible functionality, ensuring that the system meets its acceptance criteria at each milestone.\n\n#### Milestone 1: Metrics Collection Validation\n\nThe first milestone validation focuses on verifying that the system can correctly ingest and store different types of metrics with proper validation and error handling.\n\n**Functional Verification Checkpoints**\n\n| Validation Area | Test Procedure | Expected Behavior | Success Criteria |\n|-----------------|----------------|-------------------|------------------|\n| Counter Ingestion | Submit counter metrics with increasing values | Values stored with proper monotonic validation | Counter resets detected and handled correctly |\n| Gauge Processing | Submit gauge metrics with fluctuating values | All values stored without validation constraints | Gauge updates reflected immediately in storage |\n| Histogram Handling | Submit histogram metrics with bucket distributions | Bucket counts and sums calculated correctly | Percentile calculations work with stored data |\n| Label Validation | Submit metrics with various label combinations | Labels parsed, normalized, and indexed correctly | Label cardinality limits enforced properly |\n| Prometheus Compatibility | Scrape metrics from Prometheus exposition format | Prometheus metrics ingested correctly | Standard Prometheus metrics work without modification |\n\n**Performance Validation Checkpoints**\n\nThe system must demonstrate acceptable performance characteristics under realistic load conditions:\n\n1. **Ingestion Throughput**: Submit 10,000 metrics per second for 60 seconds and verify that all metrics are processed and stored within acceptable latency bounds (95th percentile < 100ms).\n\n2. **Memory Usage**: Monitor memory consumption during ingestion and verify that it remains bounded even with sustained high ingestion rates.\n\n3. **Storage Efficiency**: Verify that stored data uses reasonable disk space and that compression ratios are within expected ranges.\n\n4. **Cardinality Performance**: Test ingestion performance with varying cardinality levels and verify that high cardinality doesn't cause unacceptable performance degradation.\n\n**Error Handling Validation**\n\nThe system must handle various error conditions gracefully:\n\n1. **Invalid Metric Rejection**: Submit malformed metrics and verify that they are rejected with appropriate error messages without affecting valid metrics.\n\n2. **Cardinality Explosion Prevention**: Submit metrics that would exceed cardinality limits and verify that they are rejected with clear error messages.\n\n3. **Network Failure Recovery**: Simulate network failures during metric ingestion and verify that the system recovers properly when connectivity is restored.\n\n4. **Disk Space Exhaustion**: Fill the disk space and verify that the system provides clear error messages and doesn't corrupt existing data.\n\n**Manual Verification Procedures**\n\n1. Start the metrics ingestion service with debug logging enabled\n2. Submit a variety of metrics using curl commands targeting the ingestion API\n3. Verify that metrics appear in the storage engine using direct storage queries\n4. Check logs for any error messages or warnings\n5. Monitor system resource usage during sustained ingestion\n6. Verify that Prometheus metrics exposition format works by configuring a Prometheus server to scrape the metrics endpoint\n\n#### Milestone 2: Storage & Querying Validation\n\nThe second milestone validation focuses on verifying that the storage engine correctly persists data and that the query engine can retrieve and aggregate data accurately.\n\n**Storage Validation Checkpoints**\n\n| Validation Area | Test Procedure | Expected Behavior | Success Criteria |\n|-----------------|----------------|-------------------|------------------|\n| Time-Series Persistence | Store millions of samples across multiple series | Data persisted with compression and indexing | Query performance remains acceptable with large datasets |\n| Retention Policy Enforcement | Configure retention policies and wait for enforcement | Old data deleted according to policy | Disk space reclaimed and queries continue working |\n| Compaction Operation | Allow compaction to run and verify data integrity | Data merged correctly with improved performance | No data loss and improved query response times |\n| Range Query Accuracy | Execute queries with various time ranges | Correct data returned within specified ranges | Results match expected values with proper timestamps |\n| Aggregation Correctness | Execute sum, average, and percentile queries | Aggregation results mathematically correct | Aggregated values match manual calculations |\n\n**Query Engine Validation Checkpoints**\n\nThe query engine must demonstrate accurate query processing across various query patterns:\n\n1. **Simple Metric Selection**: Execute queries that select individual metrics and verify that results contain the expected data points.\n\n2. **Label Filtering**: Execute queries with label filters and verify that only matching series are returned.\n\n3. **Time Range Specification**: Execute queries with various time ranges and verify that results are properly bounded.\n\n4. **Aggregation Functions**: Test each aggregation function with known data sets and verify that results match expected calculations.\n\n5. **Complex Query Processing**: Execute queries that combine multiple operations and verify that results are correct.\n\n**Performance Validation Checkpoints**\n\n1. **Query Response Times**: Execute queries against large datasets and verify that response times remain within acceptable bounds (95th percentile < 500ms).\n\n2. **Concurrent Query Handling**: Execute multiple concurrent queries and verify that the system maintains good performance without resource contention.\n\n3. **Memory Management**: Monitor memory usage during query execution and verify that it remains bounded even with complex queries.\n\n4. **Cache Effectiveness**: Verify that query caching improves performance for repeated queries without affecting result accuracy.\n\n**Manual Verification Procedures**\n\n1. Populate the storage engine with a realistic dataset spanning several days\n2. Execute queries using the query API and verify results manually\n3. Monitor query execution times and resource usage\n4. Verify that compaction operations complete successfully\n5. Test retention policy enforcement by configuring short retention periods\n6. Verify that aggregation functions produce mathematically correct results\n\n#### Milestone 3: Visualization Dashboard Validation\n\nThe third milestone validation focuses on verifying that the dashboard system correctly displays metric data with real-time updates and proper user interaction.\n\n**Dashboard Functionality Validation**\n\n| Validation Area | Test Procedure | Expected Behavior | Success Criteria |\n|-----------------|----------------|-------------------|------------------|\n| Chart Rendering | Configure panels with various chart types | Charts display data accurately with proper formatting | Visual representations match underlying data |\n| Configuration Persistence | Save and load dashboard configurations | Configurations persist correctly across sessions | Dashboard state maintained after restart |\n| Auto-Refresh Functionality | Enable auto-refresh with various intervals | Charts update automatically without manual intervention | Data freshness maintained according to refresh intervals |\n| Time Range Selection | Use time range controls to change displayed periods | Chart data updates to reflect selected time ranges | Historical and real-time data displayed correctly |\n\n**Real-Time Update Validation**\n\nThe dashboard must demonstrate proper real-time behavior:\n\n1. **WebSocket Connection Management**: Verify that WebSocket connections are established correctly and remain stable during extended sessions.\n\n2. **Live Data Updates**: Submit new metrics and verify that dashboard panels update automatically without requiring page refresh.\n\n3. **Subscription Management**: Verify that panels only receive updates for metrics they are configured to display.\n\n4. **Update Frequency Control**: Test different auto-refresh intervals and verify that updates occur at the specified frequency.\n\n5. **Error Handling**: Test dashboard behavior when the backend service is unavailable and verify proper error messages and recovery.\n\n**User Interface Validation**\n\n1. **Panel Configuration**: Test the ability to create, modify, and delete dashboard panels with various configurations.\n\n2. **Layout Management**: Verify that panels can be arranged in different grid layouts and that configurations persist correctly.\n\n3. **Chart Interaction**: Test chart zoom, pan, and tooltip functionality to ensure proper user experience.\n\n4. **Responsive Design**: Test dashboard functionality across different screen sizes and browser configurations.\n\n**Manual Verification Procedures**\n\n1. Access the dashboard web interface and create a new dashboard\n2. Configure panels with different chart types and metric queries\n3. Verify that charts display data correctly and update in real-time\n4. Test time range controls and auto-refresh functionality\n5. Save and reload dashboard configurations\n6. Test WebSocket connectivity and error handling by restarting backend services\n\n#### Milestone 4: Alerting System Validation\n\nThe fourth milestone validation focuses on verifying that the alerting system correctly evaluates alert rules, manages alert states, and delivers notifications reliably.\n\n**Alert Rule Validation Checkpoints**\n\n| Validation Area | Test Procedure | Expected Behavior | Success Criteria |\n|-----------------|----------------|-------------------|------------------|\n| Threshold Evaluation | Configure alerts with various threshold conditions | Alerts fire when thresholds are exceeded for specified duration | Alert timing accuracy within evaluation interval tolerance |\n| State Transition Management | Submit metric data that triggers state changes | Alert states transition correctly through the state machine | Proper notifications sent for each state transition |\n| Duration-Based Conditions | Configure alerts with different duration requirements | Alerts only fire after conditions persist for specified time | Flapping alerts properly suppressed by duration requirements |\n| Multi-Channel Notification | Configure alerts with multiple notification channels | Notifications delivered to all configured channels | Channel-specific message formatting applied correctly |\n\n**Notification System Validation**\n\nThe notification system must demonstrate reliable delivery across different channels:\n\n1. **Email Integration**: Configure email notifications and verify that alert messages are delivered with proper formatting and timing.\n\n2. **Slack Integration**: Configure Slack notifications and verify that messages appear in designated channels with appropriate urgency indicators.\n\n3. **Webhook Delivery**: Configure webhook notifications and verify that HTTP requests are sent with correct payloads and retry logic.\n\n4. **Message Templating**: Test custom message templates and verify that alert data is properly substituted into message content.\n\n5. **Rate Limiting**: Test notification rate limiting to verify that notification storms are properly controlled.\n\n**Alert Lifecycle Validation**\n\n1. **Alert Firing**: Submit metric data that exceeds thresholds and verify that alerts transition to firing state within the expected timeframe.\n\n2. **Alert Resolution**: Submit metric data that resolves conditions and verify that alerts transition to resolved state with appropriate notifications.\n\n3. **Alert Persistence**: Restart the alerting system and verify that alert states are properly restored from persistence storage.\n\n4. **Silence Management**: Test alert silencing functionality and verify that notifications are suppressed during maintenance windows.\n\n**Manual Verification Procedures**\n\n1. Configure alert rules targeting test metrics with low threshold values\n2. Submit metric data that triggers alert conditions\n3. Verify that alerts fire and notifications are delivered to configured channels\n4. Submit metric data that resolves alert conditions\n5. Verify that resolution notifications are sent\n6. Test alert silencing and verify that notifications are suppressed\n7. Restart the alerting system and verify that alert states persist correctly\n\n> The milestone validation checkpoints serve as quality gates that prevent progression to the next development phase until the current capabilities are fully functional and meet their acceptance criteria. This approach ensures that the system maintains quality throughout the development process rather than deferring quality concerns to the end.\n\n#### Comprehensive System Validation\n\nAfter completing all individual milestones, a comprehensive system validation verifies that all components work together correctly in realistic scenarios:\n\n1. **End-to-End Workflow**: Execute a complete workflow from metric ingestion through dashboard visualization and alert notification.\n\n2. **Load Testing**: Subject the system to realistic load conditions and verify that all components maintain acceptable performance.\n\n3. **Failure Recovery**: Simulate various failure scenarios and verify that the system recovers properly with minimal data loss.\n\n4. **Operational Readiness**: Verify that the system provides adequate monitoring, logging, and diagnostic capabilities for operational deployment.\n\n**System Integration Scenarios**\n\n| Scenario | Test Procedure | Expected Outcome | Validation Method |\n|----------|----------------|------------------|-------------------|\n| High-Volume Ingestion | Submit sustained high metric volume | System maintains performance without data loss | Monitor throughput and verify data completeness |\n| Complex Dashboard Usage | Multiple users with different dashboards | All dashboards update correctly without interference | Verify independent panel updates and resource usage |\n| Alert Storm Handling | Trigger multiple simultaneous alerts | Notifications delivered reliably without system overload | Verify notification delivery and system stability |\n| Extended Operation | Run system continuously for extended period | No memory leaks or performance degradation | Monitor resource usage trends over time |\n\n### Implementation Guidance\n\nThis section provides practical guidance for implementing the comprehensive testing strategy outlined above, with specific focus on Go-based testing tools and patterns that support both unit and integration testing requirements.\n\n#### Technology Recommendations\n\n| Testing Layer | Simple Option | Advanced Option | Recommended Choice |\n|---------------|---------------|-----------------|-------------------|\n| Unit Testing Framework | `testing` package with table-driven tests | `testify` suite with assertions and mocks | `testing` package (built-in simplicity) |\n| Mocking Strategy | Manual interface mocks | `testify/mock` with code generation | Manual mocks (explicit dependencies) |\n| Integration Testing | Docker Compose for dependencies | Kubernetes test environments | Docker Compose (simpler setup) |\n| Load Testing | Simple goroutine-based generators | `k6` or `vegeta` for sophisticated scenarios | `k6` (better reporting and scripting) |\n| Test Data Management | Hardcoded test fixtures | `gofakeit` for generated test data | Combination of both approaches |\n| Assertion Library | Standard `if` statements with `t.Error` | `testify/assert` with rich assertions | `testify/assert` (better error messages) |\n\n#### Recommended Project Structure\n\nThe testing infrastructure should be organized to support both unit and integration testing while maintaining clear separation of concerns:\n\n```\nproject-root/\n  cmd/\n    server/main.go\n    server/main_test.go        ← integration tests for server startup\n  internal/\n    ingestion/\n      ingester.go\n      ingester_test.go         ← unit tests for ingestion logic\n      ingester_integration_test.go ← integration tests with real storage\n    storage/\n      engine.go\n      engine_test.go           ← unit tests for storage operations\n      engine_integration_test.go   ← integration tests with file system\n    query/\n      processor.go\n      processor_test.go        ← unit tests for query processing\n      processor_integration_test.go ← integration tests with storage\n    alerting/\n      evaluator.go\n      evaluator_test.go        ← unit tests for alert logic\n      evaluator_integration_test.go ← integration tests with notifications\n  test/\n    fixtures/                  ← shared test data and utilities\n      metrics.go               ← test metric generators\n      storage.go               ← test storage helpers\n      time.go                  ← time manipulation utilities\n    integration/               ← system-level integration tests\n      end_to_end_test.go       ← complete workflow tests\n      performance_test.go      ← load and performance tests\n    mocks/                     ← interface implementations for testing\n      storage_mock.go          ← storage interface mock\n      notifier_mock.go         ← notification interface mock\n  docker/\n    test-compose.yml           ← Docker composition for integration testing\n  Makefile                     ← test execution automation\n```\n\n#### Testing Infrastructure Components\n\n**Mock Storage Engine Implementation**\n\n```go\n// File: test/mocks/storage_mock.go\npackage mocks\n\nimport (\n    \"context\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/metrics-system/internal/storage\"\n)\n\n// MockTimeSeriesStorage provides a controllable storage implementation for testing\ntype MockTimeSeriesStorage struct {\n    samples    map[string][]storage.Sample  // seriesID -> samples\n    series     map[string]*storage.SeriesInfo\n    writeErr   error                        // error to return on writes\n    queryErr   error                        // error to return on queries\n    writeDelay time.Duration                // simulated write latency\n    queryDelay time.Duration                // simulated query latency\n    mu         sync.RWMutex\n    \n    // Operation tracking for verification\n    WriteCalls []WriteCall\n    QueryCalls []QueryCall\n}\n\ntype WriteCall struct {\n    Timestamp time.Time\n    Samples   []storage.Sample\n    Context   context.Context\n}\n\ntype QueryCall struct {\n    Timestamp  time.Time\n    SeriesID   string\n    StartTime  time.Time\n    EndTime    time.Time\n    Context    context.Context\n}\n\n// NewMockTimeSeriesStorage creates a new mock storage instance\nfunc NewMockTimeSeriesStorage() *MockTimeSeriesStorage {\n    return &MockTimeSeriesStorage{\n        samples: make(map[string][]storage.Sample),\n        series:  make(map[string]*storage.SeriesInfo),\n    }\n}\n\n// WriteSamples implements the TimeSeriesStorage interface\nfunc (m *MockTimeSeriesStorage) WriteSamples(ctx context.Context, samples []storage.Sample) error {\n    // TODO 1: Record the write call for verification\n    // TODO 2: Check if write error should be returned\n    // TODO 3: Simulate write delay if configured\n    // TODO 4: Store samples in the mock storage\n    // TODO 5: Update series information\n    // Hint: Use time.Sleep for delay simulation\n    // Hint: Group samples by series ID for storage\n}\n\n// QueryRange implements the TimeSeriesStorage interface  \nfunc (m *MockTimeSeriesStorage) QueryRange(ctx context.Context, seriesID string, start, end time.Time) ([]storage.Sample, error) {\n    // TODO 1: Record the query call for verification\n    // TODO 2: Check if query error should be returned\n    // TODO 3: Simulate query delay if configured\n    // TODO 4: Filter samples by time range\n    // TODO 5: Return matching samples in chronological order\n    // Hint: Use sort.Slice for chronological ordering\n}\n\n// SetWriteError configures the mock to return errors on write operations\nfunc (m *MockTimeSeriesStorage) SetWriteError(err error) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.writeErr = err\n}\n\n// SetWriteDelay configures simulated write latency\nfunc (m *MockTimeSeriesStorage) SetWriteDelay(delay time.Duration) {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.writeDelay = delay\n}\n\n// GetWriteCalls returns all recorded write operations for verification\nfunc (m *MockTimeSeriesStorage) GetWriteCalls() []WriteCall {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n    calls := make([]WriteCall, len(m.WriteCalls))\n    copy(calls, m.WriteCalls)\n    return calls\n}\n\n// Reset clears all stored data and operation history\nfunc (m *MockTimeSeriesStorage) Reset() {\n    m.mu.Lock()\n    defer m.mu.Unlock()\n    m.samples = make(map[string][]storage.Sample)\n    m.series = make(map[string]*storage.SeriesInfo)\n    m.WriteCalls = nil\n    m.QueryCalls = nil\n    m.writeErr = nil\n    m.queryErr = nil\n    m.writeDelay = 0\n    m.queryDelay = 0\n}\n```\n\n**Test Fixture Generation**\n\n```go\n// File: test/fixtures/metrics.go\npackage fixtures\n\nimport (\n    \"fmt\"\n    \"math/rand\"\n    \"time\"\n    \n    \"github.com/metrics-system/internal/storage\"\n)\n\n// MetricGenerator provides utilities for creating test metrics\ntype MetricGenerator struct {\n    rand   *rand.Rand\n    labels []storage.Labels\n}\n\n// NewMetricGenerator creates a generator with deterministic randomness\nfunc NewMetricGenerator(seed int64) *MetricGenerator {\n    return &MetricGenerator{\n        rand: rand.New(rand.NewSource(seed)),\n    }\n}\n\n// GenerateCounterSamples creates realistic counter metric samples\nfunc (g *MetricGenerator) GenerateCounterSamples(metricName string, labels storage.Labels, duration time.Duration, interval time.Duration) []storage.Sample {\n    // TODO 1: Calculate number of samples based on duration and interval\n    // TODO 2: Generate monotonically increasing counter values\n    // TODO 3: Add realistic noise and occasional resets\n    // TODO 4: Create samples with proper timestamps\n    // TODO 5: Return samples in chronological order\n    // Hint: Counter values should generally increase over time\n    // Hint: Occasional resets simulate process restarts\n}\n\n// GenerateGaugeSamples creates realistic gauge metric samples\nfunc (g *MetricGenerator) GenerateGaugeSamples(metricName string, labels storage.Labels, duration time.Duration, interval time.Duration) []storage.Sample {\n    // TODO 1: Calculate number of samples based on duration and interval\n    // TODO 2: Generate gauge values with realistic fluctuation patterns\n    // TODO 3: Add seasonal patterns and random walk behavior\n    // TODO 4: Create samples with proper timestamps\n    // TODO 5: Return samples in chronological order\n    // Hint: Gauge values can increase or decrease freely\n    // Hint: Use sine waves and random walk for realistic patterns\n}\n\n// GenerateHistogramSamples creates realistic histogram metric samples\nfunc (g *MetricGenerator) GenerateHistogramSamples(metricName string, labels storage.Labels, buckets []float64, duration time.Duration, interval time.Duration) []storage.Sample {\n    // TODO 1: Calculate number of samples based on duration and interval\n    // TODO 2: Generate observation counts for each bucket\n    // TODO 3: Ensure bucket counts are monotonically increasing across buckets\n    // TODO 4: Generate count and sum values consistent with buckets\n    // TODO 5: Create samples for all histogram series (buckets, count, sum)\n    // Hint: Bucket counts must be cumulative (each bucket includes previous buckets)\n    // Hint: Generate separate samples for each bucket, plus _count and _sum series\n}\n\n// GenerateHighCardinalityLabels creates label combinations for cardinality testing\nfunc (g *MetricGenerator) GenerateHighCardinalityLabels(baseLabels storage.Labels, cardinalityLevel int) []storage.Labels {\n    // TODO 1: Start with base labels as foundation\n    // TODO 2: Generate additional label dimensions based on cardinality level\n    // TODO 3: Use realistic label names and values (service, instance, etc.)\n    // TODO 4: Ensure label combinations are unique\n    // TODO 5: Return label sets sorted for deterministic testing\n    // Hint: Common high-cardinality labels include instance_id, user_id, request_id\n    // Hint: Use consistent naming patterns for realistic scenarios\n}\n```\n\n**Integration Test Harness**\n\n```go\n// File: test/integration/harness.go\npackage integration\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"path/filepath\"\n    \"testing\"\n    \"time\"\n    \n    \"github.com/metrics-system/internal/config\"\n    \"github.com/metrics-system/internal/coordinator\"\n)\n\n// TestHarness provides a complete system instance for integration testing\ntype TestHarness struct {\n    Config      *config.Config\n    Coordinator *coordinator.ComponentCoordinator\n    TempDir     string\n    BasePort    int\n    \n    // Component references for direct testing\n    Storage   storage.TimeSeriesStorage\n    Query     query.QueryProcessor\n    Dashboard dashboard.DashboardServer\n    Alerting  alerting.AlertEvaluator\n}\n\n// NewTestHarness creates a complete system instance for testing\nfunc NewTestHarness(t *testing.T) *TestHarness {\n    // TODO 1: Create temporary directory for test data\n    // TODO 2: Generate unique port numbers to avoid conflicts\n    // TODO 3: Create test configuration with appropriate settings\n    // TODO 4: Initialize component coordinator with test config\n    // TODO 5: Start all system components\n    // TODO 6: Register cleanup function with testing.T\n    // Hint: Use t.TempDir() for automatic cleanup\n    // Hint: Use port 0 for automatic port assignment or find free ports\n    // Hint: Use t.Cleanup() to ensure proper shutdown\n}\n\n// IngestTestMetrics submits metrics through the ingestion API\nfunc (h *TestHarness) IngestTestMetrics(ctx context.Context, metrics []storage.Metric) error {\n    // TODO 1: Format metrics according to ingestion API requirements\n    // TODO 2: Submit metrics via HTTP POST to ingestion endpoint\n    // TODO 3: Verify successful response codes\n    // TODO 4: Wait for metrics to be processed and stored\n    // TODO 5: Return any errors encountered during ingestion\n    // Hint: Use http.Client for API calls\n    // Hint: Add retry logic for transient failures\n}\n\n// QueryMetrics executes queries against the system\nfunc (h *TestHarness) QueryMetrics(ctx context.Context, queryString string, timeRange query.TimeRange) (*query.QueryResult, error) {\n    // TODO 1: Format query according to query API requirements\n    // TODO 2: Submit query via HTTP GET to query endpoint\n    // TODO 3: Parse response into QueryResult structure\n    // TODO 4: Validate response format and content\n    // TODO 5: Return parsed results or error\n    // Hint: Use URL encoding for query parameters\n    // Hint: Handle different result types (time series, scalar, etc.)\n}\n\n// WaitForAlert waits for an alert to reach the specified state\nfunc (h *TestHarness) WaitForAlert(ctx context.Context, alertID string, expectedState alerting.AlertState, timeout time.Duration) error {\n    // TODO 1: Set up polling loop with context timeout\n    // TODO 2: Query alert status via alerting API\n    // TODO 3: Check if alert has reached expected state\n    // TODO 4: Return success when state matches or timeout when exceeded\n    // TODO 5: Provide informative error messages for debugging\n    // Hint: Use time.Ticker for regular polling\n    // Hint: Include current state in timeout error messages\n}\n\n// GetSystemMetrics retrieves internal system metrics for verification\nfunc (h *TestHarness) GetSystemMetrics(ctx context.Context) (map[string]float64, error) {\n    // TODO 1: Query system metrics endpoint\n    // TODO 2: Parse metrics in Prometheus exposition format\n    // TODO 3: Extract key metrics for verification (ingestion rate, storage usage, etc.)\n    // TODO 4: Return metrics as key-value map\n    // TODO 5: Handle parsing errors gracefully\n    // Hint: Look for metrics like metrics_ingested_total, storage_bytes_used\n    // Hint: Use Prometheus client library for parsing if available\n}\n\n// Shutdown gracefully stops all system components\nfunc (h *TestHarness) Shutdown(ctx context.Context) error {\n    // TODO 1: Stop component coordinator gracefully\n    // TODO 2: Wait for all components to shutdown completely\n    // TODO 3: Clean up temporary files and directories\n    // TODO 4: Close any open network connections\n    // TODO 5: Return any errors encountered during shutdown\n    // Hint: Use context timeout to prevent hanging shutdown\n    // Hint: Log shutdown progress for debugging\n}\n```\n\n#### Milestone Checkpoint Implementation\n\n**Milestone 1 Validation Test**\n\n```go\n// File: test/integration/milestone1_test.go\npackage integration\n\nimport (\n    \"context\"\n    \"testing\"\n    \"time\"\n    \n    \"github.com/stretchr/testify/assert\"\n    \"github.com/stretchr/testify/require\"\n    \n    \"github.com/metrics-system/test/fixtures\"\n)\n\nfunc TestMilestone1_MetricsCollection(t *testing.T) {\n    harness := NewTestHarness(t)\n    ctx := context.Background()\n    \n    t.Run(\"Counter Metrics Ingestion\", func(t *testing.T) {\n        // TODO 1: Generate counter metrics with increasing values\n        // TODO 2: Submit metrics through ingestion API\n        // TODO 3: Query storage directly to verify persistence\n        // TODO 4: Validate that counter semantics are enforced\n        // TODO 5: Check that monotonic increases are preserved\n        \n        generator := fixtures.NewMetricGenerator(12345)\n        // Implementation continues with detailed test steps...\n    })\n    \n    t.Run(\"Gauge Metrics Processing\", func(t *testing.T) {\n        // TODO 1: Generate gauge metrics with fluctuating values\n        // TODO 2: Submit metrics and verify immediate storage\n        // TODO 3: Validate that gauge values can increase and decrease\n        // TODO 4: Check that latest values are queryable\n        // TODO 5: Verify proper label handling and indexing\n        \n        // Implementation continues with detailed test steps...\n    })\n    \n    t.Run(\"Histogram Metrics Handling\", func(t *testing.T) {\n        // TODO 1: Generate histogram metrics with proper bucket distributions\n        // TODO 2: Submit histograms and verify bucket processing\n        // TODO 3: Validate bucket count calculations\n        // TODO 4: Check that sum and count values are correct\n        // TODO 5: Verify percentile calculation capability\n        \n        // Implementation continues with detailed test steps...\n    })\n    \n    t.Run(\"Label Validation and Cardinality\", func(t *testing.T) {\n        // TODO 1: Test label parsing and normalization\n        // TODO 2: Submit metrics with high cardinality labels\n        // TODO 3: Verify cardinality limits are enforced\n        // TODO 4: Check that valid labels are processed correctly\n        // TODO 5: Validate error messages for invalid labels\n        \n        // Implementation continues with detailed test steps...\n    })\n    \n    t.Run(\"Performance Validation\", func(t *testing.T) {\n        // TODO 1: Generate high-volume metric load\n        // TODO 2: Monitor ingestion rates and latency\n        // TODO 3: Verify memory usage remains bounded\n        // TODO 4: Check that system maintains responsiveness\n        // TODO 5: Validate that no metrics are lost during load\n        \n        startTime := time.Now()\n        // Generate 10,000 metrics over 60 seconds\n        metricsCount := 10000\n        duration := 60 * time.Second\n        \n        // Implementation continues with performance testing...\n    })\n}\n\n// validateCounterSemantics verifies counter-specific behavior\nfunc validateCounterSemantics(t *testing.T, harness *TestHarness, samples []storage.Sample) {\n    // TODO 1: Verify that counter values are monotonically increasing\n    // TODO 2: Check for proper counter reset detection\n    // TODO 3: Validate that rate calculations work correctly\n    // TODO 4: Ensure proper handling of counter overflow\n    // TODO 5: Verify storage optimization for counter data\n    \n    // Implementation continues with validation logic...\n}\n\n// validateIngestionPerformance checks performance characteristics\nfunc validateIngestionPerformance(t *testing.T, harness *TestHarness, metricsSubmitted int, duration time.Duration) {\n    // TODO 1: Calculate actual ingestion rate\n    // TODO 2: Check that 95th percentile latency is acceptable\n    // TODO 3: Verify memory usage didn't spike excessively\n    // TODO 4: Ensure no metrics were dropped or lost\n    // TODO 5: Validate system remained responsive during load\n    \n    systemMetrics, err := harness.GetSystemMetrics(context.Background())\n    require.NoError(t, err)\n    \n    actualRate := systemMetrics[\"metrics_ingested_total\"] / duration.Seconds()\n    expectedMinRate := float64(metricsSubmitted) / duration.Seconds() * 0.95 // Allow 5% tolerance\n    \n    assert.GreaterOrEqual(t, actualRate, expectedMinRate, \n        \"Ingestion rate %f/sec below expected minimum %f/sec\", actualRate, expectedMinRate)\n    \n    // Continue with additional performance validations...\n}\n```\n\nThis comprehensive testing strategy provides multiple layers of validation that ensure system correctness, performance, and reliability throughout development. The combination of unit tests, integration tests, and milestone checkpoints creates a robust quality assurance framework that catches issues early and validates that the system meets its requirements under realistic conditions.\n\n\n## Debugging Guide\n\n> **Milestone(s):** All milestones (debugging techniques are essential throughout the entire metrics and alerting system lifecycle, from initial development through production operations)\n\n### The Detective Work Metaphor: Understanding System Debugging\n\nThink of debugging a metrics system like being a detective investigating a complex case. You have multiple witnesses (log files), physical evidence (metric data), and crime scene locations (system components). The key is knowing which evidence to collect, how to interpret the clues, and following a systematic investigation process. Just as a detective starts with the most obvious suspects before diving into complex conspiracy theories, we debug metrics systems by checking the most common failure modes first—network connectivity, configuration errors, and resource constraints—before investigating more subtle issues like timing races or cardinality explosions.\n\nUnlike debugging a simple web application where problems are often isolated to a single request-response cycle, metrics systems involve continuous data flows with multiple asynchronous components. A single problem can manifest symptoms in multiple locations: metrics might disappear at ingestion, queries might return partial results, dashboards might show stale data, or alerts might fire unexpectedly. The challenge is connecting these distributed symptoms back to their root cause while the system continues operating.\n\n### Common Problem Patterns in Metrics Systems\n\nBefore diving into specific debugging techniques, it's important to understand the common failure patterns that occur in metrics and alerting systems. These patterns help guide your investigation and prioritize which components to examine first.\n\n| Failure Pattern | Typical Symptoms | Investigation Priority | Common Root Causes |\n|---|---|---|---|\n| Data Pipeline Failures | Missing metrics, partial ingestion, query timeouts | High | Network issues, storage full, validation errors |\n| Resource Exhaustion | Slow queries, high memory usage, ingestion backlog | High | Cardinality explosion, insufficient capacity, memory leaks |\n| Configuration Errors | Alerts not firing, incorrect dashboards, connection failures | Medium | Typos, wrong endpoints, invalid credentials |\n| Timing and Synchronization | Intermittent failures, race conditions, inconsistent behavior | Medium | Clock skew, concurrent access, async processing |\n| State Corruption | Inconsistent data, crashed components, recovery failures | Low | Hardware failures, bugs in storage layer, power loss |\n\n> **Key Insight**: Most production issues in metrics systems stem from resource exhaustion or configuration problems rather than complex algorithmic bugs. Start your investigation with resource monitoring and configuration validation before diving into code-level debugging.\n\nThe debugging approach for metrics systems follows a structured methodology:\n\n1. **Establish the Failure Scope**: Determine whether the problem affects all metrics, specific metric types, certain time ranges, or particular dashboard panels\n2. **Trace the Data Path**: Follow metric data from ingestion through storage, querying, and visualization to identify where the pipeline breaks\n3. **Correlate Symptoms Across Components**: Check if problems in one component are causing cascading failures in dependent systems\n4. **Validate System Resources**: Ensure adequate CPU, memory, disk space, and network capacity for current load\n5. **Reproduce with Controlled Input**: Use synthetic metrics and queries to isolate variables and confirm fixes\n\n### Ingestion Issues\n\nThe ingestion engine is the entry point for all metric data, making it a common source of problems when metrics fail to appear in dashboards or alerts don't fire as expected. Ingestion issues typically manifest as missing metrics, partial data ingestion, or performance degradation during high-volume periods.\n\n#### Metrics Not Appearing\n\nThe most common ingestion problem is metrics disappearing somewhere between client submission and storage persistence. This creates a frustrating debugging experience because the problem could exist at multiple layers of the ingestion pipeline.\n\n**Systematic Investigation Approach:**\n\n| Investigation Step | Commands/Actions | What to Look For | Next Step If Found |\n|---|---|---|---|\n| Verify Network Connectivity | `curl -X POST /api/metrics -d @test-metric.json` | HTTP response code, connection timeouts | Check firewall rules, DNS resolution |\n| Check Ingestion Logs | `grep \"metric ingestion\" /var/log/metrics.log` | Validation errors, parsing failures | Review metric format against schema |\n| Validate Metric Format | Compare against `Metric` struct requirements | Missing fields, invalid types, malformed JSON | Fix client-side metric serialization |\n| Examine Storage Writes | Monitor `WriteSamples` method calls and errors | Write failures, disk space errors | Check storage configuration and capacity |\n| Verify Label Cardinality | Check cardinality manager logs and metrics | High-cardinality rejections, quota exceeded | Reduce label diversity or increase limits |\n\n> **Decision: Ingestion Pipeline Observability Strategy**\n> - **Context**: When metrics don't appear, determining where they're lost requires visibility into each pipeline stage\n> - **Options Considered**: \n>   1. Per-component error logging only\n>   2. End-to-end tracing with correlation IDs\n>   3. Pipeline metrics with stage-specific counters\n> - **Decision**: Hybrid approach combining pipeline metrics with correlation IDs for complex failures\n> - **Rationale**: Pipeline metrics provide immediate visibility into healthy vs. failing stages, while correlation IDs enable detailed investigation of specific metric flows when needed\n> - **Consequences**: Enables rapid problem isolation but requires additional instrumentation overhead\n\n**⚠️ Pitfall: Assuming Network Problems Are External**\n\nMany developers assume that HTTP 200 responses mean successful metric ingestion, but the ingestion pipeline continues after the HTTP response. The `MetricsIngester` might accept the request but later reject metrics during validation or storage. Always check the complete pipeline, not just the HTTP layer.\n\n**Common Validation Failures:**\n\n| Validation Error | Symptom | Debug Command | Fix |\n|---|---|---|---|\n| Invalid Metric Name | `ValidationErrors` in logs with \"invalid metric name\" | Check metric name against regex pattern | Use alphanumeric names with underscores |\n| Label Cardinality Exceeded | `CardinalityManager` rejection logs | Review unique label combinations | Reduce label diversity or increase limits |\n| Timestamp Out of Range | \"timestamp too old/new\" errors | Compare timestamps to retention policy | Adjust timestamp or retention configuration |\n| Malformed Sample Values | \"invalid float64\" parsing errors | Validate sample values are numeric | Fix client serialization of infinity/NaN |\n| Missing Required Labels | \"required label missing\" validation errors | Check against label requirements config | Add missing labels to metric submissions |\n\n#### Cardinality Explosions\n\nCardinality explosions occur when metrics contain too many unique label combinations, overwhelming storage and query performance. This is one of the most dangerous ingestion problems because it can quickly consume all available system resources.\n\n**Detection Strategies:**\n\nThe `CardinalityManager` component tracks unique label combinations and provides early warning when cardinality approaches dangerous levels. Monitor these metrics continuously:\n\n| Cardinality Metric | Healthy Range | Warning Threshold | Critical Threshold | Response Action |\n|---|---|---|---|---|\n| Unique Series Count | < 100K per metric | > 500K per metric | > 1M per metric | Enable label filtering |\n| New Series Rate | < 1K/minute | > 10K/minute | > 50K/minute | Investigate metric source |\n| Label Combination Diversity | < 10 per label | > 100 per label | > 1000 per label | Review label design |\n| Memory Usage per Series | < 1KB average | > 5KB average | > 10KB average | Optimize storage format |\n\n> **Key Insight**: Cardinality problems are exponential—each new label dimension multiplies the potential series count. A metric with 10 possible values for 3 labels creates 1,000 unique series (10³), but adding one more label with 10 values creates 10,000 series (10⁴).\n\n**Investigation Process:**\n\n1. **Identify High-Cardinality Metrics**: Query the series index to find metrics with excessive unique combinations\n2. **Analyze Label Distribution**: Examine which labels contribute most to cardinality growth\n3. **Review Metric Design**: Determine if high-cardinality labels are necessary for observability goals\n4. **Implement Cardinality Controls**: Configure limits, sampling, or label filtering to manage growth\n\n**Common High-Cardinality Label Patterns:**\n\n| Problematic Pattern | Example | Why It's Dangerous | Better Alternative |\n|---|---|---|---|\n| Request IDs in Labels | `request_id=\"uuid-123\"` | Every request creates new series | Use request ID in annotations or separate tracing |\n| Timestamp Labels | `hour=\"2024-01-15-14\"` | Creates series per time bucket | Use native timestamp field instead |\n| User IDs in Labels | `user_id=\"user123\"` | Series grows with user base | Aggregate by user groups or roles |\n| Full URLs as Labels | `url=\"/api/users/123/profile\"` | Creates series per URL variant | Extract URL patterns or endpoints only |\n| Random Values | `session_id=\"random123\"` | Unbounded series growth | Remove or use in annotations |\n\n#### Performance Problems During High Volume\n\nIngestion performance problems typically manifest as increased latency, memory growth, or eventually request timeouts when the system cannot keep up with incoming metric volume.\n\n**Performance Monitoring Approach:**\n\nThe `MetricsIngester` component should expose these performance metrics for continuous monitoring:\n\n| Performance Metric | Description | Healthy Baseline | Investigation Trigger |\n|---|---|---|---|\n| Ingestion Throughput | Metrics processed per second | > 10K metrics/sec | < 1K metrics/sec sustained |\n| Ingestion Latency P99 | Time from request to storage write | < 100ms | > 500ms |\n| Validation Queue Depth | Pending metrics awaiting validation | < 1000 | > 10000 |\n| Storage Write Latency | Time for `WriteSamples` to complete | < 50ms | > 200ms |\n| Memory Usage per Batch | Memory consumed processing metric batches | < 10MB per batch | > 100MB per batch |\n\n**Scalability Bottleneck Analysis:**\n\n| Bottleneck Location | Symptoms | Root Cause Analysis | Scaling Strategy |\n|---|---|---|---|\n| HTTP Request Processing | High request latency, connection timeouts | Single-threaded request handlers | Increase `GOMAXPROCS`, use connection pooling |\n| Metric Validation | CPU spikes during validation, processing delays | Complex validation rules, inefficient regex | Optimize validation algorithms, cache compiled patterns |\n| Label Processing | Memory growth with label-heavy metrics | Inefficient label storage, no deduplication | Implement label interning, optimize `Labels` type |\n| Storage Writes | Write latency increases, disk I/O saturation | Synchronous writes, no write batching | Implement write batching, async persistence |\n| Memory Management | GC pauses, out-of-memory errors | Large object allocations, no object pooling | Use sync.Pool for metric objects, optimize GC tuning |\n\n**⚠️ Pitfall: Optimizing the Wrong Component**\n\nPerformance problems often appear in one component but originate in another. For example, slow query performance might be caused by inefficient ingestion creating poorly structured storage blocks, not the query engine itself. Always profile the entire pipeline before optimizing individual components.\n\n### Storage and Query Issues\n\nStorage and query problems are particularly challenging to debug because they often involve subtle interactions between the storage engine's complex components: the write-ahead log, block manager, compaction engine, and query processor.\n\n#### Slow Query Performance\n\nQuery performance problems typically stem from inefficient storage layouts, missing indexes, or queries that scan excessive amounts of data. The challenge is determining whether the problem is in the query itself, the underlying storage structure, or resource constraints.\n\n**Query Performance Investigation Framework:**\n\n| Investigation Phase | Diagnostic Actions | Key Metrics to Examine | Resolution Path |\n|---|---|---|---|\n| Query Analysis | Parse query complexity, time range scope | Series count, sample count, aggregation types | Optimize query structure or add filters |\n| Index Utilization | Check series selection efficiency | Index hit ratio, series scan count | Add indexes or improve label filtering |\n| Storage Layout | Examine block structure and compaction | Block count, block time ranges, overlap ratio | Trigger manual compaction or adjust policies |\n| Resource Constraints | Monitor CPU, memory, I/O during queries | CPU utilization, memory allocation, disk reads | Scale resources or implement query limits |\n\n**Common Query Performance Anti-Patterns:**\n\n| Anti-Pattern | Example Query Symptom | Storage Impact | Fix Strategy |\n|---|---|---|---|\n| Unbounded Time Range | Query spans months of data | Scans thousands of storage blocks | Add explicit time range limits |\n| High-Cardinality Grouping | Group by high-diversity labels | Processes millions of series | Use sampling or pre-aggregated metrics |\n| Inefficient Label Matching | Regular expressions on label values | Full index scan required | Use exact matches or optimize regex |\n| Cross-Series Math | Complex arithmetic across many series | Memory explosion during aggregation | Break into smaller queries or use streaming |\n| Missing Filters | No label selectors in metric queries | Processes all series for metric | Add specific label matchers |\n\n> **Decision: Query Optimization Strategy**\n> - **Context**: Queries can become extremely expensive as data volume grows, potentially impacting system stability\n> - **Options Considered**:\n>   1. Query timeout limits only\n>   2. Resource-based query rejection (memory/CPU limits)\n>   3. Query complexity analysis with progressive limits\n> - **Decision**: Hybrid approach with complexity analysis, resource limits, and progressive timeouts\n> - **Rationale**: Prevents runaway queries while allowing legitimate complex queries during low-load periods\n> - **Consequences**: Requires query complexity estimation but provides better user experience than hard timeouts\n\n**Query Execution Pipeline Debugging:**\n\nThe `QueryProcessor` breaks query execution into distinct phases, each of which can be monitored and optimized independently:\n\n1. **Query Parsing Phase**: Convert query string to abstract syntax tree\n   - Monitor: Parse time, syntax errors, unsupported functions\n   - Debug: Enable query parser logging, validate AST structure\n\n2. **Query Planning Phase**: Generate execution plan with series selection strategy\n   - Monitor: Planning time, estimated series count, memory requirements\n   - Debug: Log execution plans, compare estimated vs. actual resource usage\n\n3. **Series Selection Phase**: Identify time series matching query selectors\n   - Monitor: Index lookup time, series count, label matching efficiency\n   - Debug: Log index utilization, profile label matching algorithms\n\n4. **Data Retrieval Phase**: Load sample data from storage blocks\n   - Monitor: Block read count, decompression time, I/O wait time\n   - Debug: Track block access patterns, identify hot/cold data distribution\n\n5. **Aggregation Phase**: Process samples according to query functions\n   - Monitor: Aggregation time, memory usage, output series count\n   - Debug: Profile aggregation functions, monitor streaming vs. batch processing\n\n#### Missing or Inconsistent Data\n\nData consistency problems in time-series systems often result from race conditions between ingestion, compaction, and querying processes. These problems are particularly insidious because they may only affect specific time ranges or metric combinations.\n\n**Data Consistency Investigation Process:**\n\n| Investigation Step | Diagnostic Approach | Tools and Commands | Expected Findings |\n|---|---|---|---|\n| Verify Data Ingestion | Check if samples reached storage | Review ingestion logs and `WriteSamples` calls | All submitted samples should appear in WAL |\n| Examine WAL State | Inspect write-ahead log for sample presence | WAL reader tools, log file analysis | Samples should exist with correct timestamps |\n| Check Block Structure | Verify samples exist in storage blocks | Block inspection tools, storage debugging | Samples should be in appropriate time-bucketed blocks |\n| Analyze Compaction Impact | Determine if compaction corrupted data | Compaction logs, before/after block comparison | Sample count and values should be preserved |\n| Query Path Validation | Trace query execution through storage layers | Query debugging logs, execution plan analysis | Queries should access all relevant blocks |\n\n**Common Data Consistency Issues:**\n\n| Consistency Problem | Typical Manifestation | Root Cause Analysis | Resolution Strategy |\n|---|---|---|---|\n| Samples Disappear After Compaction | Queries return fewer points than expected | Compaction algorithm bug or configuration error | Validate compaction logic, check downsampling rules |\n| Duplicate Samples for Same Timestamp | Aggregations produce incorrect results | Multiple ingestion paths or replay issues | Implement timestamp deduplication, fix replay logic |\n| Time Range Gaps | Missing data for specific time periods | WAL corruption, failed writes, or clock skew | Recover from WAL backups, sync system clocks |\n| Label Inconsistency | Same metric appears with different labels | Concurrent label updates, caching issues | Implement label validation, clear metadata caches |\n| Query Result Variations | Same query returns different results | Read-write race conditions, index staleness | Add read barriers, refresh indexes before queries |\n\n> **Key Insight**: Time-series data consistency problems often have delayed manifestation—samples might be ingested successfully but lost during later compaction or corrupted during concurrent reads. Always verify data through the complete storage lifecycle.\n\n**⚠️ Pitfall: Assuming WAL Guarantees Consistency**\n\nWhile the write-ahead log provides durability guarantees, it doesn't protect against logic errors in compaction or querying. A correctly written WAL can still result in missing data if the compaction process has bugs or if queries don't properly handle block boundaries.\n\n#### Storage Corruption Detection and Recovery\n\nStorage corruption in time-series systems can occur at multiple levels: file system corruption, block-level data corruption, or index inconsistencies. Early detection and recovery procedures are critical for maintaining data integrity.\n\n**Corruption Detection Strategy:**\n\n| Detection Method | Implementation | Trigger Conditions | Recovery Actions |\n|---|---|---|---|\n| Block Checksums | Compute checksums on write, verify on read | Checksum mismatch during block access | Mark block as corrupted, attempt recovery from WAL |\n| Index Validation | Periodic consistency checks between index and blocks | Scheduled validation, startup checks | Rebuild index from storage blocks |\n| Sample Count Validation | Track expected vs. actual sample counts | Query result discrepancies | Compare with ingestion metrics, identify missing blocks |\n| Temporal Consistency | Verify timestamp ordering within blocks | Out-of-order samples detected | Re-sort block contents, check compaction logic |\n| Cross-Reference Validation | Compare WAL entries with storage blocks | Samples in WAL missing from blocks | Replay missing WAL entries |\n\n**Storage Recovery Procedures:**\n\nThe `StorageEngine` should implement comprehensive recovery mechanisms that can handle various corruption scenarios:\n\n1. **WAL-Based Recovery**: Replay write-ahead log entries to reconstruct missing or corrupted blocks\n2. **Block Reconstruction**: Rebuild corrupted blocks from available data sources\n3. **Index Rebuilding**: Regenerate series indexes from existing storage blocks\n4. **Partial Recovery**: Recover what data is possible and mark irretrievable data as missing\n5. **Backup Integration**: Restore from external backups when local recovery fails\n\n### Alerting Issues\n\nAlerting system problems are often the most critical because they directly impact incident response and system reliability. Alert failures can range from alerts not firing when they should (false negatives) to excessive alert noise (false positives).\n\n#### Alerts Not Firing\n\nThe most dangerous alerting problem is alerts failing to fire when conditions are met, as this can lead to undetected outages or performance degradation.\n\n**Alert Evaluation Investigation Process:**\n\n| Investigation Step | Diagnostic Actions | Key Information to Gather | Resolution Path |\n|---|---|---|---|\n| Verify Alert Rule Configuration | Review `AlertRule` definition syntax | Rule expression, threshold values, duration settings | Fix syntax errors, validate threshold logic |\n| Check Alert Evaluation Loop | Monitor `AlertEvaluator` execution | Evaluation frequency, rule processing time, errors | Restart evaluator, fix evaluation logic |\n| Validate Data Availability | Confirm metrics exist for alert queries | Query results, series existence, timestamp alignment | Fix metric ingestion or query problems |\n| Examine Alert State Management | Review state transitions and timing | Current state, state change history, duration tracking | Reset alert state, fix state machine logic |\n| Test Query Execution | Run alert query manually | Query results, execution errors, performance | Optimize query or fix data source |\n\n**Common Alert Evaluation Failures:**\n\n| Failure Mode | Symptoms | Root Cause | Debug Approach | Fix Strategy |\n|---|---|---|---|---|\n| Query Returns No Data | Alert never transitions from Inactive | Metric name mismatch, missing labels | Execute alert query manually | Correct metric selector syntax |\n| Threshold Logic Error | Alert fires at wrong values | Incorrect comparison operators, type mismatches | Log threshold comparisons | Fix operator logic (>, <, ==) |\n| Duration Requirements Not Met | Alert stays in Pending state | Insufficient duration for sustained condition | Check state transition timing | Adjust duration requirements |\n| Clock Skew Issues | Timestamps don't align with evaluation | System clock differences, timezone problems | Compare timestamps across components | Synchronize system clocks |\n| Resource Exhaustion | Alert evaluator stops processing | Memory/CPU limits exceeded during evaluation | Monitor evaluator resource usage | Scale alert evaluation capacity |\n\n> **Decision: Alert Evaluation Reliability Strategy**\n> - **Context**: Alert failures can have severe operational consequences, requiring high reliability guarantees\n> - **Options Considered**:\n>   1. Single-threaded evaluation with checkpointing\n>   2. Distributed evaluation with consensus\n>   3. Redundant evaluators with leader election\n> - **Decision**: Single-threaded evaluation with comprehensive checkpointing and fast failure detection\n> - **Rationale**: Simpler architecture with fewer failure modes while maintaining reliability through rapid detection and recovery\n> - **Consequences**: Single point of failure but faster recovery and easier debugging\n\n**Alert State Machine Debugging:**\n\nThe alert state machine manages transitions between `AlertStateInactive`, `AlertStatePending`, `AlertStateFiring`, and `AlertStateResolved`. State transition problems often indicate timing issues or logic errors.\n\n| Current State | Expected Trigger | Expected Next State | Common Failures | Debug Actions |\n|---|---|---|---|---|\n| Inactive | Threshold exceeded | Pending | Query returns no data, threshold comparison error | Verify query results, log comparison values |\n| Pending | Duration requirement met | Firing | Timer not advancing, state not persisted | Check duration tracking, verify state storage |\n| Firing | Condition no longer met | Resolved | Notification delivery blocking state updates | Separate notification from state management |\n| Resolved | Condition met again | Pending | Rapid state flapping, insufficient hysteresis | Add state change delays, implement hysteresis |\n\n#### Notification Delivery Problems\n\nEven when alerts fire correctly, notification delivery failures can prevent teams from receiving critical alerts. Notification problems often involve external services and network connectivity issues.\n\n**Notification Pipeline Debugging:**\n\n| Pipeline Stage | Common Failures | Detection Methods | Resolution Approaches |\n|---|---|---|---|\n| Message Formatting | Template errors, missing data | Template compilation errors, malformed messages | Fix template syntax, validate data availability |\n| Channel Selection | Wrong channels chosen, disabled channels | Routing logs, channel configuration | Update routing rules, enable required channels |\n| External Service Integration | API failures, authentication errors | HTTP response codes, service error logs | Fix credentials, handle API rate limits |\n| Network Connectivity | DNS resolution, firewall blocks | Network timeouts, connection errors | Check network configuration, update DNS |\n| Rate Limiting | Too many notifications sent | Rate limit exceeded errors | Implement backoff, batch notifications |\n\n**⚠️ Pitfall: Ignoring Notification Delivery Confirmations**\n\nMany alert systems send notifications without verifying delivery success. Network failures, API outages, or configuration errors can silently prevent notifications from reaching their destination. Always implement delivery confirmation and retry logic.\n\n**Notification Channel Health Monitoring:**\n\n| Channel Type | Health Check Method | Failure Detection | Recovery Actions |\n|---|---|---|---|\n| Email SMTP | SMTP connection test, authentication validation | SMTP errors, timeout responses | Retry with exponential backoff, try alternate SMTP servers |\n| Slack Webhook | HTTP POST test to webhook URL | HTTP 4xx/5xx responses, network timeouts | Refresh webhook URLs, check Slack app permissions |\n| PagerDuty API | API key validation, service availability | API authentication errors, service unavailable | Rotate API keys, use backup notification channels |\n| Custom Webhook | HTTP endpoint availability test | Connection refused, DNS resolution failures | Health check endpoints, implement circuit breakers |\n\n#### Alert Flapping and Noise Reduction\n\nAlert flapping occurs when conditions rapidly oscillate around threshold values, causing alerts to fire and resolve repeatedly. This creates noise that can mask genuine problems and lead to alert fatigue.\n\n**Flapping Detection and Mitigation:**\n\n| Flapping Pattern | Detection Method | Root Cause | Mitigation Strategy |\n|---|---|---|---|\n| Rapid Fire/Resolve Cycles | State changes within short time windows | Threshold too close to normal values | Increase threshold gap, add hysteresis |\n| Periodic Oscillation | Regular pattern of state changes | Scheduled processes, batch jobs | Adjust evaluation timing, use trend analysis |\n| Network-Induced Flapping | Correlates with network events | Network partitions, intermittent connectivity | Increase evaluation duration, use circuit breakers |\n| Load-Based Flapping | Correlates with traffic patterns | Resource saturation during peak load | Use percentile thresholds, add capacity buffers |\n\n> **Key Insight**: Alert flapping is often a symptom of poorly chosen thresholds rather than system instability. Effective alerting requires thresholds that account for normal system variation while still detecting genuine problems promptly.\n\n### Implementation Guidance\n\nThis debugging section requires sophisticated tooling and instrumentation to make problems visible when they occur. The following implementation provides structured debugging capabilities across all system components.\n\n#### Technology Recommendations\n\n| Debugging Component | Simple Option | Advanced Option |\n|---|---|---|\n| Logging Framework | Go `slog` with JSON output | Structured logging with correlation IDs |\n| Metrics Collection | Prometheus client library | Custom metrics with detailed labels |\n| Performance Profiling | Go `net/http/pprof` | Continuous profiling with flame graphs |\n| Health Checking | HTTP endpoints with JSON status | Comprehensive health dashboard |\n| Error Tracking | File-based error logs | Centralized error aggregation |\n\n#### Recommended File Structure\n\n```\nproject-root/\n  cmd/\n    metrics-server/main.go          ← main entry point with debug flags\n    debug-tools/\n      cardinality-analyzer/main.go  ← tool for cardinality analysis\n      storage-inspector/main.go     ← tool for examining storage blocks\n      alert-tester/main.go         ← tool for testing alert rules\n  internal/\n    debugging/\n      health_manager.go            ← centralized health checking\n      performance_monitor.go       ← performance metrics collection\n      debug_handlers.go           ← HTTP handlers for debug endpoints\n    diagnostics/\n      ingestion_diagnostics.go     ← ingestion-specific debugging\n      storage_diagnostics.go       ← storage-specific debugging\n      alert_diagnostics.go         ← alerting-specific debugging\n  tools/\n    validate-config.go             ← configuration validation\n    generate-test-metrics.go       ← synthetic metric generation\n```\n\n#### Infrastructure Starter Code\n\n**Complete Health Management System:**\n\n```go\n// internal/debugging/health_manager.go\npackage debugging\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"log/slog\"\n    \"net/http\"\n    \"sync\"\n    \"time\"\n)\n\n// HealthStatus represents the overall health state of a component\ntype HealthStatus int\n\nconst (\n    HealthStatusHealthy HealthStatus = iota\n    HealthStatusDegraded\n    HealthStatusUnhealthy\n)\n\nfunc (s HealthStatus) String() string {\n    switch s {\n    case HealthStatusHealthy:\n        return \"healthy\"\n    case HealthStatusDegraded:\n        return \"degraded\"\n    case HealthStatusUnhealthy:\n        return \"unhealthy\"\n    default:\n        return \"unknown\"\n    }\n}\n\n// HealthCheck represents a single health check result\ntype HealthCheck struct {\n    Name        string       `json:\"name\"`\n    Status      HealthStatus `json:\"status\"`\n    LastChecked time.Time    `json:\"last_checked\"`\n    Message     string       `json:\"message\"`\n}\n\n// HealthCheckFunc defines the signature for health check functions\ntype HealthCheckFunc func(ctx context.Context) error\n\n// HealthManager coordinates health checks across all system components\ntype HealthManager struct {\n    checks map[string]*HealthCheck\n    funcs  map[string]HealthCheckFunc\n    mu     sync.RWMutex\n    logger *slog.Logger\n}\n\nfunc NewHealthManager(logger *slog.Logger, interval time.Duration) *HealthManager {\n    hm := &HealthManager{\n        checks: make(map[string]*HealthCheck),\n        funcs:  make(map[string]HealthCheckFunc),\n        logger: logger,\n    }\n    \n    // Start periodic health check execution\n    go hm.runPeriodicChecks(interval)\n    \n    return hm\n}\n\nfunc (hm *HealthManager) RegisterCheck(name string, checkFunc HealthCheckFunc) {\n    hm.mu.Lock()\n    defer hm.mu.Unlock()\n    \n    hm.funcs[name] = checkFunc\n    hm.checks[name] = &HealthCheck{\n        Name:        name,\n        Status:      HealthStatusUnhealthy,\n        LastChecked: time.Time{},\n        Message:     \"not yet checked\",\n    }\n}\n\nfunc (hm *HealthManager) RunChecks(ctx context.Context) {\n    hm.mu.RLock()\n    funcs := make(map[string]HealthCheckFunc)\n    for name, fn := range hm.funcs {\n        funcs[name] = fn\n    }\n    hm.mu.RUnlock()\n    \n    for name, checkFunc := range funcs {\n        hm.runSingleCheck(ctx, name, checkFunc)\n    }\n}\n\nfunc (hm *HealthManager) runSingleCheck(ctx context.Context, name string, checkFunc HealthCheckFunc) {\n    start := time.Now()\n    err := checkFunc(ctx)\n    duration := time.Since(start)\n    \n    hm.mu.Lock()\n    check := hm.checks[name]\n    check.LastChecked = time.Now()\n    \n    if err != nil {\n        check.Status = HealthStatusUnhealthy\n        check.Message = fmt.Sprintf(\"check failed: %v (took %v)\", err, duration)\n        hm.logger.Error(\"health check failed\",\n            slog.String(\"check\", name),\n            slog.String(\"error\", err.Error()),\n            slog.Duration(\"duration\", duration))\n    } else {\n        // Consider slow checks as degraded\n        if duration > 5*time.Second {\n            check.Status = HealthStatusDegraded\n            check.Message = fmt.Sprintf(\"check slow but successful (took %v)\", duration)\n        } else {\n            check.Status = HealthStatusHealthy\n            check.Message = fmt.Sprintf(\"check successful (took %v)\", duration)\n        }\n    }\n    hm.mu.Unlock()\n}\n\nfunc (hm *HealthManager) GetOverallStatus() HealthStatus {\n    hm.mu.RLock()\n    defer hm.mu.RUnlock()\n    \n    if len(hm.checks) == 0 {\n        return HealthStatusUnhealthy\n    }\n    \n    overallStatus := HealthStatusHealthy\n    for _, check := range hm.checks {\n        if check.Status == HealthStatusUnhealthy {\n            return HealthStatusUnhealthy\n        }\n        if check.Status == HealthStatusDegraded {\n            overallStatus = HealthStatusDegraded\n        }\n    }\n    \n    return overallStatus\n}\n\nfunc (hm *HealthManager) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    hm.mu.RLock()\n    checks := make([]*HealthCheck, 0, len(hm.checks))\n    for _, check := range hm.checks {\n        checks = append(checks, check)\n    }\n    hm.mu.RUnlock()\n    \n    overallStatus := hm.GetOverallStatus()\n    \n    response := map[string]interface{}{\n        \"status\": overallStatus.String(),\n        \"checks\": checks,\n        \"timestamp\": time.Now(),\n    }\n    \n    w.Header().Set(\"Content-Type\", \"application/json\")\n    \n    // Set HTTP status based on health\n    switch overallStatus {\n    case HealthStatusHealthy:\n        w.WriteHeader(http.StatusOK)\n    case HealthStatusDegraded:\n        w.WriteHeader(http.StatusOK) // 200 but degraded\n    case HealthStatusUnhealthy:\n        w.WriteHeader(http.StatusServiceUnavailable)\n    }\n    \n    json.NewEncoder(w).Encode(response)\n}\n\nfunc (hm *HealthManager) runPeriodicChecks(interval time.Duration) {\n    ticker := time.NewTicker(interval)\n    defer ticker.Stop()\n    \n    for range ticker.C {\n        ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n        hm.RunChecks(ctx)\n        cancel()\n    }\n}\n```\n\n**Complete Performance Monitoring System:**\n\n```go\n// internal/debugging/performance_monitor.go\npackage debugging\n\nimport (\n    \"runtime\"\n    \"sync\"\n    \"time\"\n)\n\n// PerformanceMetrics tracks system performance indicators\ntype PerformanceMetrics struct {\n    // Ingestion metrics\n    IngestRate          float64   `json:\"ingest_rate_per_sec\"`\n    IngestLatencyP99    time.Duration `json:\"ingest_latency_p99\"`\n    ValidationErrors    int64     `json:\"validation_errors\"`\n    \n    // Storage metrics\n    StorageWriteRate    float64   `json:\"storage_write_rate_per_sec\"`\n    StorageWriteLatency time.Duration `json:\"storage_write_latency\"`\n    CompactionDuration  time.Duration `json:\"last_compaction_duration\"`\n    \n    // Query metrics\n    QueryRate           float64   `json:\"query_rate_per_sec\"`\n    QueryLatencyP99     time.Duration `json:\"query_latency_p99\"`\n    SlowQueryCount      int64     `json:\"slow_query_count\"`\n    \n    // Alert metrics\n    AlertEvaluationRate float64   `json:\"alert_eval_rate_per_sec\"`\n    AlertsActive        int64     `json:\"alerts_active\"`\n    NotificationFailures int64    `json:\"notification_failures\"`\n    \n    // System metrics\n    MemoryUsage         uint64    `json:\"memory_usage_bytes\"`\n    GoroutineCount      int       `json:\"goroutine_count\"`\n    GCDuration          time.Duration `json:\"gc_duration\"`\n    \n    // Timestamps\n    Timestamp           time.Time `json:\"timestamp\"`\n    mu                  sync.RWMutex\n}\n\nfunc NewPerformanceMonitor() *PerformanceMetrics {\n    pm := &PerformanceMetrics{\n        Timestamp: time.Now(),\n    }\n    \n    // Start periodic system metrics collection\n    go pm.collectSystemMetrics()\n    \n    return pm\n}\n\nfunc (pm *PerformanceMetrics) collectSystemMetrics() {\n    ticker := time.NewTicker(10 * time.Second)\n    defer ticker.Stop()\n    \n    for range ticker.C {\n        pm.mu.Lock()\n        \n        // Memory statistics\n        var memStats runtime.MemStats\n        runtime.ReadMemStats(&memStats)\n        pm.MemoryUsage = memStats.Alloc\n        pm.GCDuration = time.Duration(memStats.PauseTotalNs)\n        \n        // Goroutine count\n        pm.GoroutineCount = runtime.NumGoroutine()\n        \n        pm.Timestamp = time.Now()\n        pm.mu.Unlock()\n    }\n}\n\nfunc (pm *PerformanceMetrics) RecordIngestMetric(latency time.Duration, success bool) {\n    pm.mu.Lock()\n    defer pm.mu.Unlock()\n    \n    // Update ingestion rate (simplified - real implementation would use sliding window)\n    pm.IngestRate++\n    \n    // Update latency percentile (simplified - real implementation would use histogram)\n    if latency > pm.IngestLatencyP99 {\n        pm.IngestLatencyP99 = latency\n    }\n    \n    if !success {\n        pm.ValidationErrors++\n    }\n}\n\nfunc (pm *PerformanceMetrics) GetSnapshot() *PerformanceMetrics {\n    pm.mu.RLock()\n    defer pm.mu.RUnlock()\n    \n    snapshot := *pm\n    return &snapshot\n}\n```\n\n#### Core Logic Skeleton Code\n\n**Ingestion Diagnostics Framework:**\n\n```go\n// internal/diagnostics/ingestion_diagnostics.go\npackage diagnostics\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log/slog\"\n    \"time\"\n)\n\ntype IngestionDiagnostics struct {\n    logger *slog.Logger\n    // Add references to ingestion components\n}\n\nfunc NewIngestionDiagnostics(logger *slog.Logger) *IngestionDiagnostics {\n    return &IngestionDiagnostics{\n        logger: logger,\n    }\n}\n\n// DiagnoseMetricIngestion performs comprehensive ingestion pipeline analysis\nfunc (id *IngestionDiagnostics) DiagnoseMetricIngestion(ctx context.Context, metricName string, timeRange time.Duration) error {\n    // TODO 1: Check if metrics are reaching the ingestion endpoint\n    // - Review HTTP access logs for metric submission requests\n    // - Verify request payloads are valid JSON\n    // - Check response codes (200 vs 4xx vs 5xx)\n    \n    // TODO 2: Validate metric format and content\n    // - Parse submitted metrics against expected schema\n    // - Check metric names, types, labels, and values\n    // - Identify validation rule violations\n    \n    // TODO 3: Trace metric flow through validation pipeline\n    // - Check if metrics pass cardinality limits\n    // - Verify label format and allowed values\n    // - Identify metrics rejected during validation\n    \n    // TODO 4: Verify storage writes are successful\n    // - Check WAL entries for submitted metrics\n    // - Verify samples appear in storage blocks\n    // - Identify storage write failures or delays\n    \n    // TODO 5: Generate diagnostic report with findings\n    // - Summarize ingestion health for the specified time range\n    // - List specific failures and their root causes\n    // - Recommend remediation steps for identified issues\n    \n    return fmt.Errorf(\"ingestion diagnostics not implemented\")\n}\n\n// CheckCardinalityExplosion analyzes label combinations for cardinality issues\nfunc (id *IngestionDiagnostics) CheckCardinalityExplosion(ctx context.Context) error {\n    // TODO 1: Query current series count per metric\n    // - Group series by metric name\n    // - Count unique label combinations\n    // - Identify metrics with excessive cardinality\n    \n    // TODO 2: Analyze label contribution to cardinality\n    // - For high-cardinality metrics, examine individual labels\n    // - Calculate unique value count per label\n    // - Identify labels with excessive diversity\n    \n    // TODO 3: Project cardinality growth trends\n    // - Calculate new series creation rate\n    // - Estimate storage and memory impact\n    // - Predict when cardinality limits will be exceeded\n    \n    // TODO 4: Generate cardinality optimization recommendations\n    // - Suggest label consolidation opportunities\n    // - Recommend cardinality limit adjustments\n    // - Identify metrics suitable for sampling\n    \n    return fmt.Errorf(\"cardinality analysis not implemented\")\n}\n```\n\n**Storage Diagnostics Framework:**\n\n```go\n// internal/diagnostics/storage_diagnostics.go\npackage diagnostics\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log/slog\"\n    \"time\"\n)\n\ntype StorageDiagnostics struct {\n    logger *slog.Logger\n    // Add references to storage components\n}\n\n// DiagnoseQueryPerformance analyzes slow query execution\nfunc (sd *StorageDiagnostics) DiagnoseQueryPerformance(ctx context.Context, queryString string) error {\n    // TODO 1: Parse and analyze query complexity\n    // - Break down query into component parts (selectors, functions, time range)\n    // - Estimate series count and sample count requirements\n    // - Identify potentially expensive operations (regex, aggregations)\n    \n    // TODO 2: Examine storage block access patterns\n    // - Determine which blocks need to be read for the query\n    // - Check for block overlaps requiring merge operations\n    // - Identify hot vs cold data access patterns\n    \n    // TODO 3: Profile query execution phases\n    // - Measure time spent in parsing, planning, execution\n    // - Track memory allocation during query processing\n    // - Identify bottleneck phases in the query pipeline\n    \n    // TODO 4: Analyze index utilization efficiency\n    // - Check if label indexes are being used effectively\n    // - Identify full scans that could benefit from better indexes\n    // - Measure index hit ratio for the query\n    \n    // TODO 5: Generate performance optimization recommendations\n    // - Suggest query restructuring for better performance\n    // - Recommend index improvements or additions\n    // - Identify opportunities for result caching\n    \n    return fmt.Errorf(\"query performance diagnostics not implemented\")\n}\n\n// ValidateStorageConsistency checks for data corruption or inconsistencies\nfunc (sd *StorageDiagnostics) ValidateStorageConsistency(ctx context.Context) error {\n    // TODO 1: Verify WAL and storage block consistency\n    // - Compare WAL entries with corresponding storage blocks\n    // - Check that all WAL entries have been persisted to blocks\n    // - Identify any samples that exist in WAL but not in blocks\n    \n    // TODO 2: Validate block internal consistency\n    // - Verify block checksums match stored data\n    // - Check timestamp ordering within blocks\n    // - Validate sample count matches block metadata\n    \n    // TODO 3: Check index consistency with actual data\n    // - Verify series index entries match storage blocks\n    // - Check that all series in blocks have index entries\n    // - Validate label indexes match series labels\n    \n    // TODO 4: Analyze compaction impact on data integrity\n    // - Compare pre- and post-compaction sample counts\n    // - Verify compaction preserved data accuracy\n    // - Check for any data loss during compaction operations\n    \n    return fmt.Errorf(\"storage consistency validation not implemented\")\n}\n```\n\n#### Language-Specific Debugging Hints\n\n**Go-Specific Debugging Techniques:**\n\n- Use `go tool pprof` to analyze CPU and memory profiles during high load periods\n- Enable `GODEBUG=gctrace=1` to monitor garbage collection impact on performance\n- Use `go tool trace` to analyze goroutine scheduling and coordination issues\n- Implement custom `slog.Handler` to add correlation IDs to log entries\n- Use `sync/atomic` for lock-free performance counters in hot paths\n\n**Performance Profiling Integration:**\n\n```go\nimport _ \"net/http/pprof\"\n\n// Add to main HTTP server\ngo func() {\n    log.Println(http.ListenAndServe(\"localhost:6060\", nil))\n}()\n```\n\n**Memory Leak Detection:**\n\n```go\n// Add to health checks\nfunc checkMemoryGrowth(ctx context.Context) error {\n    var m1, m2 runtime.MemStats\n    runtime.ReadMemStats(&m1)\n    runtime.GC()\n    runtime.ReadMemStats(&m2)\n    \n    if m2.Alloc > m1.Alloc*2 {\n        return fmt.Errorf(\"potential memory leak detected\")\n    }\n    return nil\n}\n```\n\n#### Milestone Checkpoint\n\n**After implementing debugging infrastructure:**\n\n1. **Run Health Checks**: `curl http://localhost:8080/health` should return comprehensive health status\n2. **Generate Test Load**: Use synthetic metrics to trigger various failure modes\n3. **Verify Diagnostic Tools**: Each diagnostic function should provide actionable insights\n4. **Test Error Recovery**: Inject failures and verify system recovery behavior\n\n**Expected Debug Output Example:**\n\n```json\n{\n  \"status\": \"degraded\",\n  \"checks\": [\n    {\n      \"name\": \"ingestion_pipeline\",\n      \"status\": \"healthy\",\n      \"last_checked\": \"2024-01-15T10:30:00Z\",\n      \"message\": \"check successful (took 45ms)\"\n    },\n    {\n      \"name\": \"storage_engine\",\n      \"status\": \"degraded\",\n      \"last_checked\": \"2024-01-15T10:30:00Z\",\n      \"message\": \"compaction behind schedule (took 3.2s)\"\n    }\n  ],\n  \"timestamp\": \"2024-01-15T10:30:00Z\"\n}\n\n```\n\n\n## Future Extensions\n\n> **Milestone(s):** All milestones (future extensions build upon the complete metrics and alerting system to address scalability, reliability, and advanced functionality requirements)\n\n### The Evolution Principle: Building for Tomorrow's Challenges\n\nThink of system extensions like urban planning for a growing city. When you design the initial road network, you don't just consider current traffic patterns—you plan for wider roads, additional lanes, and subway systems that might be needed as the population grows. Similarly, our metrics and alerting system needs to accommodate future growth in data volume, user requirements, and operational complexity without requiring a complete architectural overhaul.\n\nThe key insight is that **extensibility isn't about predicting the future perfectly**—it's about creating flexible architectural joints and extension points that can accommodate unforeseen requirements. Just as a well-designed building has load-bearing walls that can support additional floors, our system has architectural foundations that can support distributed deployment, advanced analytics, and new data sources.\n\nThis section explores two categories of extensions: **scalability enhancements** that address growing data volumes and user loads, and **feature enhancements** that add new capabilities while leveraging the existing architectural foundation.\n\n### Current Architecture's Extension Points\n\nBefore diving into specific enhancements, it's important to understand how our current design accommodates future growth. The architecture includes several deliberate extension points:\n\n**Component Interface Abstraction**: All major components implement well-defined interfaces (`TimeSeriesStorage`, `QueryProcessor`, `NotificationManager`). This allows swapping implementations without changing dependent components. For example, replacing the single-node storage engine with a distributed storage cluster requires only implementing the same `TimeSeriesStorage` interface.\n\n**Configuration-Driven Behavior**: The `Config` structure and its nested configurations (`StorageConfig`, `AlertingConfig`) allow runtime behavior modification without code changes. New storage backends, notification channels, and query execution strategies can be enabled through configuration updates.\n\n**Plugin Architecture Foundations**: The `NotificationChannel` system demonstrates a plugin pattern that can be extended to other components. New channel types (like Microsoft Teams or custom webhooks) can be added by implementing the notification interface without modifying core alerting logic.\n\n**Modular Component Design**: The `ComponentCoordinator` manages component lifecycles independently, making it straightforward to add new components (like a machine learning inference engine) that integrate with existing data flows through well-defined interfaces.\n\n## Scalability Enhancements\n\n### Mental Model: From Corner Store to Global Supply Chain\n\nThink of scalability enhancements like evolving from a small corner store to a global supply chain. The corner store serves its neighborhood well with simple operations: one cashier, local suppliers, and customers who walk in. But as demand grows beyond the neighborhood, you need multiple locations, regional distribution centers, sophisticated inventory management, and coordination between stores. The fundamental business logic remains the same—buying and selling goods—but the operational complexity increases dramatically.\n\nOur current metrics system is like that efficient corner store: it handles significant load on a single machine with straightforward operations. Scalability enhancements transform it into a distributed system that can serve global monitoring needs while preserving the same core functionality and user experience.\n\n### Distributed Storage Architecture\n\nThe most critical scalability enhancement involves distributing the storage layer across multiple nodes to handle data volumes that exceed single-machine capacity. This transformation requires careful consideration of data partitioning, replication, and consistency guarantees.\n\n> **Decision: Horizontal Partitioning Strategy**\n> - **Context**: Single-node storage limits both data volume and query throughput. Time-series data has natural partitioning characteristics by metric name and time ranges.\n> - **Options Considered**: Hash-based partitioning by series ID, time-based partitioning by timestamp ranges, hybrid partitioning combining both approaches\n> - **Decision**: Hybrid partitioning with time-based primary partitioning and hash-based secondary partitioning\n> - **Rationale**: Time-based partitioning aligns with query patterns (most queries are time-range based) and enables efficient data lifecycle management. Hash-based secondary partitioning distributes load evenly across nodes within time windows.\n> - **Consequences**: Enables horizontal scaling and efficient range queries, but requires coordination for cross-partition queries and adds complexity for global operations.\n\nThe distributed storage architecture introduces several new components that extend our existing `StorageEngine`:\n\n| Component | Responsibility | Interface Changes |\n|-----------|---------------|------------------|\n| `PartitionManager` | Maps time ranges and series to storage nodes | Extends `TimeSeriesStorage` with partition awareness |\n| `ReplicationCoordinator` | Manages data replication across nodes for durability | Adds replication configuration to `StorageConfig` |\n| `ConsistencyManager` | Coordinates read/write consistency across replicas | Introduces consistency level parameters to storage operations |\n| `ShardingRouter` | Routes queries to appropriate partition nodes | Extends `QueryProcessor` with distributed query planning |\n\nThe distributed storage maintains backward compatibility by implementing the same `TimeSeriesStorage` interface, but internally coordinates operations across multiple nodes. Write operations become more complex as they must handle replication and consistency requirements:\n\n1. **Write Coordination**: When `WriteSamples` is called, the `PartitionManager` determines which nodes should store each sample based on the series ID and timestamp\n2. **Replication Protocol**: The `ReplicationCoordinator` ensures each write is replicated to the configured number of nodes before acknowledging success\n3. **Consistency Management**: The `ConsistencyManager` enforces consistency levels (immediate, eventual, or quorum-based) based on the operation requirements\n\nQuery processing becomes more sophisticated as queries may span multiple partitions:\n\n1. **Query Planning**: The `ShardingRouter` analyzes queries to determine which partitions contain relevant data\n2. **Parallel Execution**: Subqueries are executed in parallel against relevant partition nodes\n3. **Result Merging**: Results from multiple nodes are merged and deduplicated to produce the final response\n4. **Cross-Partition Aggregation**: Aggregation functions are decomposed into node-local operations and global merge operations\n\n### High Availability and Fault Tolerance\n\nDistributed deployment introduces new failure modes that require sophisticated fault tolerance mechanisms. The system must continue operating even when individual nodes fail, network partitions occur, or storage becomes temporarily unavailable.\n\n**Node Failure Detection and Recovery**: The system implements a gossip-based failure detection protocol where nodes periodically exchange heartbeat information. When a node failure is detected, the `ReplicationCoordinator` triggers automatic failover to replica nodes and initiates replication repair to restore the desired replication factor.\n\n**Split-Brain Prevention**: Network partitions can create scenarios where multiple node groups believe they're the authoritative cluster. The system implements a quorum-based approach where operations require acknowledgment from a majority of nodes to prevent conflicting writes during partitions.\n\n**Data Consistency During Failures**: The `ConsistencyManager` implements configurable consistency levels that balance availability and consistency based on operational requirements. During partial failures, the system can operate in degraded mode with reduced consistency guarantees rather than becoming completely unavailable.\n\n| Failure Scenario | Detection Method | Recovery Action | Impact on Operations |\n|------------------|------------------|-----------------|---------------------|\n| Single node failure | Gossip heartbeat timeout | Failover to replica, start replication repair | Temporary query latency increase |\n| Network partition | Quorum loss detection | Minority partition becomes read-only | Write operations blocked in minority |\n| Storage corruption | Checksum validation | Restore from replica, mark node for rebuild | Automatic transparent recovery |\n| Coordinator failure | Leader election timeout | Promote new coordinator | Brief coordination pause |\n\n### Load Balancing and Query Distribution\n\nAs query load increases, the system needs intelligent load balancing to distribute query processing across available resources. This involves both request routing and resource management at multiple levels.\n\n**Query Router Enhancement**: The existing `QueryProcessor` is extended with a `DistributedQueryRouter` that implements several load balancing strategies. Round-robin distribution works well for uniform queries, while least-connections balancing is better for queries with varying execution times. Resource-aware routing considers CPU and memory utilization on target nodes.\n\n**Query Result Caching Distribution**: The `QueryCache` becomes a distributed cache shared across query processor instances. This prevents redundant query execution when the same queries are submitted to different nodes. Cache invalidation becomes more complex as it must coordinate across multiple nodes when underlying data changes.\n\n**Adaptive Resource Allocation**: The system monitors query execution patterns and automatically adjusts resource allocation. Long-running aggregation queries are routed to dedicated high-memory nodes, while simple point queries are handled by fast-response nodes. This prevents resource contention between different query types.\n\n### Auto-Scaling Infrastructure\n\nModern deployment environments support automatic scaling based on resource utilization and demand patterns. The enhanced system integrates with container orchestration platforms to automatically adjust capacity.\n\n**Demand-Based Scaling**: The system exports internal metrics about query queue lengths, ingestion rates, and resource utilization. These metrics drive auto-scaling policies that add storage or query processing capacity when thresholds are exceeded. Scaling decisions consider both current load and historical patterns to anticipate demand.\n\n**Graceful Node Addition and Removal**: When new nodes join the cluster, the `PartitionManager` gradually rebalances data to include the new capacity. Similarly, when nodes are removed (either manually or due to scaling down), their data is redistributed to remaining nodes before the node shuts down. This ensures continuous availability during scaling operations.\n\n**Geographic Distribution**: For global deployments, the system supports multi-region deployment with intelligent routing to minimize latency. Users are automatically routed to their nearest region, while cross-region replication ensures data availability for disaster recovery.\n\n## Feature Enhancements\n\n### Mental Model: From Basic Calculator to Scientific Workstation\n\nThink of feature enhancements like evolving from a basic calculator to a scientific workstation. The basic calculator handles arithmetic operations perfectly for simple tasks, but scientists need advanced functions like statistical analysis, graphing capabilities, and programmable operations. However, the fundamental mathematical principles remain the same—it's the sophistication of available operations that increases.\n\nOur current metrics system provides solid foundational capabilities: collecting data, storing it efficiently, and providing basic visualization and alerting. Feature enhancements add advanced analytical capabilities, intelligent automation, and richer user experiences while building on the same core data model and architectural patterns.\n\n### Advanced Query Functions and Analytics\n\nThe current query engine provides basic aggregation functions like sum, average, and percentiles. Advanced analytics requires more sophisticated mathematical operations, statistical analysis, and time-series specific functions.\n\n**Statistical Functions Enhancement**: The `AggregationFunction` interface is extended with advanced statistical operations that provide deeper insights into metric behavior patterns.\n\n| Function Category | New Functions | Use Cases |\n|------------------|--------------|-----------|\n| Trend Analysis | `trend()`, `linear_regression()`, `correlation()` | Identifying performance degradation patterns |\n| Anomaly Detection | `zscore()`, `mad()`, `seasonal_decompose()` | Detecting unusual metric behavior |\n| Forecasting | `holt_winters()`, `arima()`, `exponential_smoothing()` | Capacity planning and predictive alerting |\n| Signal Processing | `moving_average()`, `exponential_decay()`, `fourier_transform()` | Noise reduction and frequency analysis |\n\n**Advanced Query Language Features**: The query parser is enhanced to support more complex expressions including mathematical operations, conditional logic, and subqueries. This enables sophisticated metric analysis without requiring external data processing tools.\n\nFor example, advanced queries can compute complex business metrics:\n- Service level calculations: `(successful_requests / total_requests) * 100`\n- Capacity utilization trends: `trend(cpu_usage[7d]) > 0.1` (detecting increasing utilization)\n- Comparative analysis: `current_latency / baseline_latency[1w:offset 1w] > 1.5` (comparing to previous week)\n\n**Query Optimization for Complex Analytics**: Advanced analytical functions often require processing large amounts of historical data. The query execution engine implements several optimizations specifically for analytical workloads:\n\n1. **Incremental Computation**: Many statistical functions can be computed incrementally, updating results as new data arrives rather than recomputing from scratch\n2. **Parallel Processing**: Complex queries are decomposed into parallel operations that can execute simultaneously across multiple CPU cores\n3. **Intermediate Result Caching**: Expensive intermediate computations (like moving averages) are cached and reused across multiple queries\n4. **Data Skipping**: Query planning identifies time ranges and series that don't affect query results, avoiding unnecessary data access\n\n### Machine Learning Integration\n\nMachine learning capabilities transform reactive monitoring into proactive intelligence. Instead of just reporting what happened, the system can predict what will happen and automatically adapt to changing conditions.\n\n> **Decision: Embedded vs. External ML Integration**\n> - **Context**: Machine learning models can be embedded within the metrics system or integrated with external ML platforms. Each approach has different performance, complexity, and flexibility characteristics.\n> - **Options Considered**: Fully embedded ML engine, external ML service integration, hybrid approach with embedded inference and external training\n> - **Decision**: Hybrid approach with lightweight embedded inference and external model training\n> - **Rationale**: Embedded inference provides low-latency predictions for real-time alerting, while external training leverages specialized ML platforms for model development and heavy computation\n> - **Consequences**: Enables real-time ML-powered features with acceptable complexity, but requires model deployment and synchronization infrastructure\n\n**Anomaly Detection Engine**: The system includes an embedded anomaly detection engine that continuously learns normal behavior patterns for each metric and identifies deviations. This goes beyond simple threshold-based alerting to detect subtle behavioral changes that might indicate underlying issues.\n\n| Component | Responsibility | Integration Point |\n|-----------|---------------|------------------|\n| `AnomalyDetector` | Identifies unusual metric patterns | Integrates with `AlertEvaluator` for ML-powered alerts |\n| `ModelRegistry` | Manages trained models and versioning | Extends `Config` with model configuration |\n| `FeatureExtractor` | Prepares metric data for ML inference | Integrates with `QueryProcessor` for feature computation |\n| `PredictionEngine` | Generates forecasts for capacity planning | Provides new query functions for predictive analytics |\n\n**Predictive Alerting**: Traditional alerts fire after problems occur. Predictive alerting uses machine learning models to identify conditions that historically lead to incidents, enabling proactive intervention. For example, the system might learn that specific combinations of CPU usage, memory pressure, and request rate patterns typically precede service outages.\n\n**Adaptive Thresholds**: Static alert thresholds often generate false alarms because they don't account for natural variations in system behavior (daily cycles, seasonal patterns, gradual growth trends). Machine learning models automatically adjust alert thresholds based on historical patterns, reducing alert fatigue while maintaining sensitivity to genuine issues.\n\n**Automated Root Cause Analysis**: When incidents occur, machine learning models analyze correlation patterns across multiple metrics to suggest potential root causes. This accelerates incident response by helping engineers focus their investigation on the most likely culprits.\n\n### Enhanced Visualization and Dashboard Features\n\nThe current dashboard system provides basic line charts and panel layouts. Enhanced visualization adds sophisticated chart types, interactive analysis capabilities, and intelligent dashboard automation.\n\n**Advanced Chart Types**: The chart renderer is extended with additional visualization options that better suit different types of metric analysis:\n\n| Chart Type | Best For | Key Features |\n|------------|----------|--------------|\n| Heatmaps | Distribution visualization | Shows metric value distributions over time |\n| Scatter Plots | Correlation analysis | Reveals relationships between different metrics |\n| Network Diagrams | Service dependency mapping | Visualizes service interactions and bottlenecks |\n| Geographic Maps | Location-based metrics | Shows metric values across geographic regions |\n| Sankey Diagrams | Flow analysis | Tracks resource flows through system components |\n\n**Interactive Analysis Tools**: Enhanced dashboards include interactive tools that enable ad-hoc analysis without requiring manual query construction:\n\n- **Drill-down Navigation**: Click on chart elements to automatically generate detailed views of the selected time range or metric dimensions\n- **Comparative Analysis**: Select multiple time ranges or metric variants for side-by-side comparison\n- **Annotation Support**: Add contextual annotations to charts (deployment markers, incident notes, configuration changes)\n- **Real-time Collaboration**: Multiple users can collaborate on dashboard analysis with shared cursors and comment threads\n\n**Intelligent Dashboard Automation**: Machine learning analyzes dashboard usage patterns to provide intelligent recommendations:\n\n1. **Automated Panel Suggestions**: Based on the metrics being viewed, the system suggests additional related metrics that might provide useful context\n2. **Dynamic Layout Optimization**: Dashboard layouts automatically adapt based on the importance and correlation of displayed metrics\n3. **Smart Alerting Integration**: Dashboards automatically highlight panels related to active alerts and suggest relevant diagnostic queries\n4. **Usage-Based Optimization**: Frequently accessed dashboard elements are optimized for faster loading, while rarely used panels are deprioritized\n\n**Advanced Time Navigation**: Enhanced time navigation tools support complex analysis scenarios:\n- **Multi-timeline Comparison**: View the same metrics across different time periods simultaneously\n- **Event Correlation**: Automatically align timelines with significant events (deployments, alerts, configuration changes)\n- **Intelligent Time Selection**: Machine learning suggests optimal time ranges based on the type of analysis and historical patterns\n\n### Integration Ecosystem Expansion\n\nModern observability requires integration with a broad ecosystem of tools and services. Enhanced integration capabilities make the metrics system a central hub in the observability infrastructure.\n\n**Extended Data Source Support**: The `MetricsIngester` is enhanced to collect data from additional sources beyond traditional application metrics:\n\n| Data Source Category | Examples | Integration Method |\n|---------------------|----------|------------------|\n| Infrastructure Logs | Application logs, system logs, audit logs | Log parsing and metric extraction |\n| Application Tracing | Distributed trace data, span metrics | Trace analysis and latency metrics |\n| Business Events | User actions, transaction data, revenue metrics | Event stream processing |\n| External APIs | Cloud service metrics, third-party SaaS data | Scheduled polling and API integration |\n| IoT and Sensor Data | Temperature, pressure, location data | MQTT and sensor protocol support |\n\n**Bi-directional Integration**: The system not only collects data but also exports insights to other tools in the observability stack. This includes pushing alert information to incident management systems, sending capacity forecasts to auto-scaling controllers, and providing metric data to business intelligence platforms.\n\n**Workflow Integration**: Enhanced notification channels integrate with workflow automation tools to trigger automated responses to alerts. For example, alerts about disk space can automatically trigger cleanup scripts, or performance degradation alerts can initiate auto-scaling actions.\n\n### Security and Compliance Enhancements\n\nEnterprise deployment requires sophisticated security and compliance capabilities that go beyond basic authentication and authorization.\n\n**Audit Trail and Compliance Reporting**: All system interactions are logged with comprehensive audit trails that support compliance with regulations like SOX, GDPR, and HIPAA. The audit system tracks who accessed what data, when changes were made, and what decisions were taken based on metric data.\n\n**Data Privacy and Anonymization**: The system includes data privacy features that can automatically anonymize or pseudonymize sensitive metric data based on configurable policies. This enables analytics on sensitive data while protecting individual privacy.\n\n**Multi-tenancy and Isolation**: Enhanced multi-tenancy support provides strict data isolation between different teams or customers sharing the same infrastructure. Each tenant has separate namespaces for metrics, dashboards, and alert rules with no possibility of cross-tenant data access.\n\n**Advanced Authentication Integration**: The system integrates with enterprise identity providers (LDAP, Active Directory, SAML, OAuth) and supports advanced authentication features like multi-factor authentication, certificate-based authentication, and just-in-time access provisioning.\n\n### Performance and Scale Optimizations\n\nBeyond horizontal scaling, numerous performance optimizations can significantly improve system efficiency and user experience.\n\n**Intelligent Data Compression**: Advanced compression algorithms are tailored specifically for time-series data patterns, achieving better compression ratios than generic algorithms. The system automatically selects optimal compression strategies based on data characteristics and access patterns.\n\n**Query Result Streaming**: Large query results are streamed to clients rather than being buffered entirely in memory. This enables analysis of massive datasets without overwhelming system resources or client applications.\n\n**Adaptive Sampling and Resolution**: The system automatically adjusts metric collection frequency and storage resolution based on data importance and access patterns. Critical metrics maintain high resolution, while less important metrics are sampled at lower frequencies to save resources.\n\n**Edge Caching and CDN Integration**: Dashboard assets and frequently accessed query results are cached at edge locations to minimize latency for global users. Intelligent cache invalidation ensures data freshness while maximizing cache hit rates.\n\n## Implementation Guidance\n\n### Technology Recommendations\n\nThe scalability and feature enhancements require careful technology choices that balance functionality, performance, and operational complexity:\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|--------------|------------------|-----------|\n| Distributed Storage | etcd + file-based sharding | Apache Cassandra / ScyllaDB | etcd for coordination, dedicated TSDB for heavy workloads |\n| Message Queue | Go channels + persistence | Apache Kafka / NATS Streaming | Channels for simple cases, external queue for reliability |\n| Machine Learning | Basic statistical functions | TensorFlow Serving / MLflow | Start simple, integrate ML platforms as needed |\n| Service Discovery | Static configuration | Consul / etcd | Static config for small deployments, service discovery for dynamic environments |\n| Load Balancing | HTTP reverse proxy | HAProxy / Envoy | Standard reverse proxy sufficient for most cases |\n\n### Enhanced Architecture Structure\n\nThe extended system maintains the same core organization while adding new components for advanced features:\n\n```\nproject-root/\n  cmd/\n    server/main.go                    ← enhanced with feature flags\n    migrate/main.go                   ← database migration tool\n  internal/\n    coordinator/                      ← enhanced with distributed coordination\n      coordinator.go\n      cluster_manager.go              ← NEW: cluster membership and coordination\n      health_checker.go               ← enhanced with distributed health checking\n    storage/\n      engine.go                       ← enhanced with partitioning awareness\n      distributed/                    ← NEW: distributed storage components\n        partition_manager.go\n        replication_coordinator.go\n        consistency_manager.go\n      compaction/\n        distributed_compaction.go     ← NEW: cross-node compaction coordination\n    query/\n      processor.go                    ← enhanced with advanced functions\n      distributed_router.go           ← NEW: distributed query routing\n      ml/                            ← NEW: machine learning integration\n        anomaly_detector.go\n        prediction_engine.go\n        model_registry.go\n    dashboard/\n      server.go                       ← enhanced with new chart types\n      charts/                         ← NEW: advanced visualization components\n        heatmap.go\n        network_diagram.go\n        geographic_map.go\n    alerting/\n      evaluator.go                    ← enhanced with ML-powered alerting\n      adaptive_thresholds.go          ← NEW: ML-based threshold adjustment\n    integrations/                     ← NEW: external system integrations\n      log_parser.go\n      trace_collector.go\n      workflow_triggers.go\n  pkg/\n    ml/                              ← NEW: machine learning utilities\n      features.go                     ← feature extraction for ML models\n      inference.go                    ← lightweight inference engine\n  deployments/\n    kubernetes/                       ← NEW: Kubernetes deployment manifests\n      storage-cluster.yaml\n      query-processors.yaml\n      dashboard-service.yaml\n    docker-compose/\n      distributed-stack.yml           ← multi-node development environment\n  migrations/                         ← NEW: schema migration scripts\n    001_initial_schema.sql\n    002_add_partitioning.sql\n```\n\n### Distributed Storage Infrastructure\n\nThe distributed storage enhancement requires several new components that coordinate data placement and access across multiple nodes:\n\n```go\n// ClusterManager coordinates distributed operations across storage nodes\ntype ClusterManager struct {\n    nodeID       string\n    nodes        map[string]*NodeInfo\n    coordinator  *PartitionManager\n    replication  *ReplicationCoordinator\n    gossip       *GossipProtocol\n    logger       *slog.Logger\n    stopCh       chan struct{}\n    mu           sync.RWMutex\n}\n\n// NewClusterManager creates a distributed coordination manager\nfunc NewClusterManager(nodeID string, seedNodes []string, logger *slog.Logger) *ClusterManager {\n    // TODO 1: Initialize gossip protocol for node discovery and failure detection\n    // TODO 2: Create partition manager for data placement decisions\n    // TODO 3: Initialize replication coordinator for data durability\n    // TODO 4: Set up periodic maintenance tasks (failure detection, rebalancing)\n    // TODO 5: Start gossip protocol and join cluster using seed nodes\n}\n\n// DistributedStorageEngine extends StorageEngine with cluster awareness\ntype DistributedStorageEngine struct {\n    localEngine    *StorageEngine\n    clusterManager *ClusterManager\n    router         *ShardingRouter\n    config         *DistributedStorageConfig\n    logger         *slog.Logger\n}\n\n// WriteSamples distributes writes across appropriate nodes with replication\nfunc (d *DistributedStorageEngine) WriteSamples(ctx context.Context, samples []Sample) error {\n    // TODO 1: Group samples by target partition using consistent hashing\n    // TODO 2: For each partition, determine primary and replica nodes\n    // TODO 3: Execute writes to primary nodes in parallel\n    // TODO 4: Await replication confirmation based on consistency level\n    // TODO 5: Return error if minimum replicas not achieved within timeout\n}\n```\n\n### Machine Learning Integration Components\n\nThe ML integration provides intelligent analysis capabilities while maintaining simple deployment for basic use cases:\n\n```go\n// AnomalyDetector identifies unusual patterns in metric data\ntype AnomalyDetector struct {\n    models      map[string]*AnomalyModel\n    features    *FeatureExtractor\n    threshold   float64\n    sensitivity float64\n    logger      *slog.Logger\n    mu          sync.RWMutex\n}\n\n// DetectAnomalies analyzes metric samples for unusual behavior\nfunc (a *AnomalyDetector) DetectAnomalies(ctx context.Context, seriesID string, samples []Sample) ([]Anomaly, error) {\n    // TODO 1: Extract features from sample data (trend, seasonality, variance)\n    // TODO 2: Load or create model for this series\n    // TODO 3: Compute anomaly score based on deviation from expected pattern\n    // TODO 4: Apply threshold to determine which samples are anomalous\n    // TODO 5: Return anomaly objects with confidence scores and explanations\n}\n\n// PredictionEngine generates forecasts for capacity planning\ntype PredictionEngine struct {\n    models    map[string]*ForecastModel\n    extractor *FeatureExtractor\n    cache     *PredictionCache\n    logger    *slog.Logger\n    mu        sync.RWMutex\n}\n\n// GenerateForecast predicts future values for a metric series\nfunc (p *PredictionEngine) GenerateForecast(ctx context.Context, seriesID string, horizon time.Duration) (*Forecast, error) {\n    // TODO 1: Check cache for recent forecast for this series\n    // TODO 2: Retrieve historical data for model training/inference\n    // TODO 3: Extract time-series features (seasonality, trend, level)\n    // TODO 4: Apply appropriate forecasting model (Holt-Winters, ARIMA, etc.)\n    // TODO 5: Generate forecast with confidence intervals and cache result\n}\n```\n\n### Advanced Visualization Components\n\nEnhanced visualization requires sophisticated chart rendering and interactive analysis tools:\n\n```go\n// HeatmapRenderer creates heatmap visualizations for distribution analysis\ntype HeatmapRenderer struct {\n    canvas     Canvas\n    colorScale ColorScale\n    aggregator *DistributionAggregator\n}\n\n// RenderHeatmap generates heatmap visualization from metric data\nfunc (h *HeatmapRenderer) RenderHeatmap(ctx context.Context, data []TimeSeries, config HeatmapConfig) (*ChartResult, error) {\n    // TODO 1: Aggregate data into time/value buckets based on config\n    // TODO 2: Calculate bucket densities and statistical measures\n    // TODO 3: Apply color mapping based on density distribution\n    // TODO 4: Render chart with proper axes, legends, and interactivity\n    // TODO 5: Return chart data with metadata for client rendering\n}\n\n// InteractiveAnalyzer provides drill-down and exploration capabilities\ntype InteractiveAnalyzer struct {\n    queryProcessor QueryProcessor\n    correlator     *MetricCorrelator\n    annotator      *AnnotationManager\n    logger         *slog.Logger\n}\n\n// GenerateRelatedQueries suggests additional metrics for analysis\nfunc (i *InteractiveAnalyzer) GenerateRelatedQueries(ctx context.Context, baseQuery string) ([]SuggestedQuery, error) {\n    // TODO 1: Parse base query to extract metric names and labels\n    // TODO 2: Find correlated metrics based on historical patterns\n    // TODO 3: Generate contextually relevant query variations\n    // TODO 4: Rank suggestions by relevance and usefulness\n    // TODO 5: Return formatted queries with explanations\n}\n```\n\n### Milestone Checkpoints for Extensions\n\nAfter implementing scalability enhancements, verify the following capabilities:\n\n**Distributed Storage Checkpoint**:\n- Start 3-node cluster: `go run cmd/server/main.go --cluster-mode --node-id=node1 --seed-nodes=node2:8080,node3:8080`\n- Verify node discovery: `curl http://localhost:8080/api/cluster/nodes` should show all 3 nodes\n- Test data distribution: ingest metrics and verify they're stored across multiple nodes\n- Test failover: stop one node and verify queries still work with slight latency increase\n- Expected behavior: System continues operating with one node failure, automatically rebalances data\n\n**Machine Learning Checkpoint**:\n- Enable anomaly detection: add `ml.anomaly_detection.enabled=true` to config\n- Ingest normal baseline data for 24 hours with consistent patterns\n- Inject anomalous data points (values 3x higher than normal)\n- Verify anomaly alerts: `curl http://localhost:8080/api/alerts` should show ML-generated alerts\n- Expected behavior: System learns normal patterns and alerts on deviations without manual thresholds\n\n**Advanced Visualization Checkpoint**:\n- Create heatmap dashboard: add heatmap panel to existing dashboard configuration\n- Verify interactive features: click on heatmap cells to drill down to detailed time series\n- Test correlation analysis: select multiple metrics and generate correlation matrix\n- Expected behavior: Rich visualizations render correctly with interactive navigation\n\n### Common Enhancement Pitfalls\n\n⚠️ **Pitfall: Premature Distributed Deployment**\nMany teams attempt distributed deployment before understanding their actual scale requirements. Distributed systems add significant operational complexity—multiple failure modes, complex debugging, consistency challenges. **Why it's wrong**: The coordination overhead can actually reduce performance for small-scale deployments. **How to fix**: Implement and validate single-node optimizations first. Only distribute when single-node capacity is genuinely exceeded (>100GB/day ingestion or >1000 queries/second).\n\n⚠️ **Pitfall: Over-Complex ML Integration**\nTeams often integrate sophisticated ML platforms for simple use cases that could be solved with basic statistical methods. **Why it's wrong**: Complex ML infrastructure requires specialized expertise, adds deployment dependencies, and introduces model training/versioning overhead. **How to fix**: Start with simple statistical functions (moving averages, standard deviation thresholds) and only add ML complexity when clear value is demonstrated.\n\n⚠️ **Pitfall: Feature Creep in Visualization**\nEnhanced dashboards can become overwhelmingly complex with too many chart types, interactive features, and configuration options. **Why it's wrong**: Complex interfaces reduce usability and increase maintenance burden. **How to fix**: Follow progressive disclosure principles—start with simple, common visualizations and add advanced features only when specific use cases are identified.\n\n⚠️ **Pitfall: Ignoring Backward Compatibility**\nEnhancements often break existing configurations, APIs, or data formats in pursuit of new features. **Why it's wrong**: Forces users to rewrite configurations and dashboards during upgrades, creating adoption friction. **How to fix**: Design extensions that enhance existing interfaces rather than replacing them. Provide migration tools and maintain compatibility layers for deprecated features.\n\n⚠️ **Pitfall: Inadequate Performance Testing**\nEnhanced features often have different performance characteristics that aren't validated under realistic load. **Why it's wrong**: New features may work well in development but degrade system performance in production. **How to fix**: Implement performance benchmarks for each enhancement and validate them with realistic data volumes and query patterns before deployment.\n\n### Integration Testing Strategy\n\nComprehensive integration testing ensures enhancements work correctly with existing functionality:\n\n**Distributed System Testing**: Use containerized environments to simulate multi-node deployments with realistic network latency and failure scenarios. Test network partitions, node failures, and data consistency under various failure modes.\n\n**ML Model Validation**: Create synthetic datasets with known anomaly patterns to validate ML model accuracy. Test model performance degradation over time and automatic retraining capabilities.\n\n**Cross-Feature Integration**: Test how advanced visualizations perform with ML-generated data, how distributed queries work with enhanced analytics functions, and how notification integrations handle increased alert volume from ML-powered alerting.\n\n**Performance Regression Testing**: Establish performance baselines before implementing enhancements and validate that new features don't degrade core system performance. Pay particular attention to query latency impact from advanced analytics functions.\n\nThe extensibility design ensures these enhancements integrate seamlessly with the existing architecture while providing clear upgrade paths for organizations with growing monitoring requirements.\n\n\n## Glossary\n\n> **Milestone(s):** All milestones (this comprehensive terminology reference supports understanding across the entire metrics and alerting system)\n\n### The Dictionary Metaphor: Building a Shared Language\n\nThink of this glossary as a specialized dictionary for our metrics and alerting system. Just as doctors, lawyers, and engineers each have their own professional vocabulary that enables precise communication within their field, our system has specific terminology that carries exact technical meanings. When we say \"cardinality explosion,\" every team member should immediately understand we're talking about a performance problem caused by too many unique label combinations, not just \"lots of data.\"\n\nThis shared vocabulary serves three critical purposes: it eliminates ambiguity in technical discussions, it helps new team members quickly understand system concepts, and it ensures consistent naming across code, documentation, and monitoring. Just as a medical dictionary defines \"tachycardia\" as \"heart rate above 100 beats per minute\" rather than just \"fast heartbeat,\" our glossary provides precise definitions that translate directly to measurable system behaviors.\n\nThe terms in this glossary fall into several categories: data model concepts that define what we store and how, architectural patterns that describe how components interact, operational procedures that define how the system behaves, and troubleshooting vocabulary that helps identify and resolve issues. Each definition includes not just what the term means, but why it matters and how it impacts system design decisions.\n\n### Core Data Model Terms\n\n**Abstract Syntax Tree (AST)**: A hierarchical representation of a parsed query structure that captures the logical organization of operations, functions, and data references without concern for the specific textual syntax used to express them. The AST enables query optimization, validation, and execution planning by providing a standardized internal representation that separates query semantics from parsing details.\n\n**Alert Instance**: A specific occurrence of an alert rule evaluation that represents the current state, value, and notification history for a particular combination of alert rule and time series labels. Each instance tracks its lifecycle through states like pending, firing, and resolved, maintaining timestamps for state changes and a log of notification attempts to enable proper alert management and debugging.\n\n**Alert Rule**: A configuration that defines the conditions under which alerts should be triggered, specifying a metric query expression, comparison operator, threshold value, evaluation interval, and notification settings. Alert rules serve as templates that generate alert instances when their conditions are met, enabling automated monitoring of system health and business metrics.\n\n**Alert State**: An enumerated value representing the current lifecycle phase of an alert instance, including inactive (condition not met), pending (condition met but duration requirement not satisfied), firing (condition met for required duration), resolved (condition no longer met), and silenced (notifications suppressed). State transitions follow a defined state machine that ensures proper notification behavior and prevents alert flapping.\n\n**Alert State Machine**: A finite state automaton that governs transitions between alert lifecycle phases, defining valid state changes based on evaluation results, duration requirements, and administrative actions. The state machine ensures consistent alert behavior, prevents invalid transitions like jumping directly from inactive to firing, and manages notification timing to avoid spam while ensuring critical alerts are communicated promptly.\n\n**Cardinality**: The number of unique time-series combinations in a dataset, calculated as the product of all possible label value combinations for each metric name. High cardinality can severely impact query performance and memory usage, as each unique combination requires separate storage and indexing, making cardinality management crucial for system scalability.\n\n**Cardinality Explosion**: A performance-degrading condition where the number of unique time-series combinations grows exponentially due to high-variability labels like user IDs, request IDs, or timestamps being used as label values. This explosion can overwhelm storage systems, consume excessive memory during queries, and make the system unresponsive, requiring careful label design and cardinality monitoring.\n\n**Dashboard**: A structured collection of visualization panels that displays metric data in charts, graphs, and tables, organized according to a saved configuration that specifies layout, queries, time ranges, and refresh intervals. Dashboards serve as the primary interface for observing system behavior and investigating issues, providing both overview perspectives and detailed drill-down capabilities.\n\n**Histogram Buckets**: Predefined ranges that group metric observations into discrete intervals for distribution analysis, enabling calculation of percentiles, averages, and counts across different value ranges. Each bucket maintains a count of observations falling within its range, allowing statistical analysis of latency distributions, request sizes, and other measured quantities without storing individual sample values.\n\n**Labels**: Key-value pairs attached to metrics that provide dimensional metadata enabling filtering, grouping, and aggregation across different facets of the data. Labels transform flat metric streams into multi-dimensional datasets, allowing queries like \"CPU usage for service=web, environment=production\" while requiring careful management to prevent cardinality explosion.\n\n**Metric**: A named measurement that captures quantitative information about system behavior, application performance, or business processes over time. Metrics include metadata like type (counter, gauge, histogram), labels for dimensional filtering, and a stream of timestamped sample values that enable trend analysis and alerting.\n\n**Panel**: An individual visualization component within a dashboard that displays metric data as charts, graphs, tables, or other visual formats according to configured queries, time ranges, and display options. Panels can be resized, repositioned, and configured independently, allowing flexible dashboard layouts that match specific monitoring needs.\n\n**Sample**: A single timestamped measurement value representing the state of a metric at a specific point in time, consisting of a numeric value and a timestamp that enables time-series analysis. Samples are the atomic units of metric data, and their efficient storage and retrieval determines system performance and query capabilities.\n\n**Series ID**: A unique identifier for a specific metric-labels combination computed as a hash of the metric name and sorted label pairs, enabling efficient storage indexing and query optimization. Series IDs allow the system to quickly locate related samples without expensive string comparisons and provide a stable reference for time-series data across compaction and retention operations.\n\n**Time-Series Data**: Sequences of timestamped measurements that capture how values change over time, enabling trend analysis, forecasting, and anomaly detection. Time-series data exhibits temporal locality patterns that can be exploited for efficient storage compression and query optimization.\n\n### Storage and Query Terms\n\n**Block-Based Storage**: An organizational strategy that groups time-series samples into immutable, time-windowed storage blocks that can be efficiently compressed, indexed, and queried. This approach enables effective compaction strategies, optimizes disk I/O patterns, and supports retention policies by allowing entire blocks to be deleted when they exceed configured age limits.\n\n**Compaction**: The process of merging and downsampling stored data blocks to improve query performance, reduce storage overhead, and maintain system efficiency over time. Compaction combines multiple small blocks into larger ones, applies compression algorithms, and can downsample high-resolution data to lower resolutions for long-term storage while preserving statistical properties.\n\n**Counter Reset**: A situation where a counter metric's value decreases, typically indicating a process restart, metric redefinition, or data collection issue. Counter resets require special handling in rate calculations to avoid negative spikes and ensure accurate trend analysis, often involving reset detection algorithms and rate calculation adjustments.\n\n**Downsampling**: The process of reducing data resolution by aggregating high-frequency samples into lower-frequency summaries while preserving important statistical properties like averages, maximums, and percentiles. Downsampling enables long-term data retention by reducing storage requirements for older data while maintaining sufficient detail for historical analysis.\n\n**Query Execution Pipeline**: A multi-phase process that transforms query strings into results through parsing, query planning, storage access, data aggregation, and result formatting. The pipeline enables optimization opportunities at each stage, supports caching strategies, and provides clear error isolation for debugging query performance issues.\n\n**Retention Policy**: Automated rules that manage data lifecycle by deleting or downsampling data that exceeds configured age limits, preventing unlimited storage growth while preserving important historical information. Retention policies must balance storage costs with analytical needs, often implementing tiered strategies that keep high-resolution recent data and low-resolution historical data.\n\n**Streaming Aggregation**: A processing technique that computes aggregation functions incrementally as data flows through the system, without requiring the entire dataset to be buffered in memory. This approach enables real-time analytics on large datasets and reduces memory pressure during query execution, particularly important for high-cardinality queries.\n\n**Timestamp Alignment**: The process of synchronizing samples from multiple time series to common time intervals to enable mathematical operations and comparisons between series. Alignment handles cases where different metrics are sampled at different frequencies or times, using interpolation or alignment functions to create consistent time grids.\n\n**Write-Ahead Log (WAL)**: A durability mechanism that persists metric samples to disk before acknowledging writes, ensuring data survival across system crashes and enabling recovery by replaying logged operations. The WAL provides atomicity guarantees for batch writes and supports consistent recovery semantics essential for reliable metric storage.\n\n### Query and Analysis Terms\n\n**Aggregation Function**: A mathematical operation that combines multiple time-series values into summary statistics like sum, average, maximum, minimum, or percentiles across specified dimensions or time ranges. Aggregation functions enable dashboard visualizations and alert evaluations that operate on grouped data rather than individual samples.\n\n**Cache Entry**: A stored query result with associated metadata including the original query, result data, creation timestamp, access statistics, and expiration information. Cache entries enable query result reuse to improve dashboard performance and reduce storage system load for frequently accessed data.\n\n**Counter Semantics**: The behavioral rules governing counter-type metrics, which represent monotonically increasing cumulative values that can only increase or reset to zero. Counter semantics require special handling for rate calculations, reset detection, and ensuring that derived metrics like queries per second correctly handle counter resets.\n\n**Gauge Semantics**: The behavioral rules for gauge-type metrics, which represent point-in-time values that can increase or decrease freely, such as memory usage, queue lengths, or temperature readings. Gauge semantics allow direct value comparisons and mathematical operations without the reset-handling complexity required for counters.\n\n**Label Explosion**: A condition where the number of unique label combinations grows uncontrollably, typically caused by using high-cardinality values like user IDs, request IDs, or IP addresses as label values. Label explosion can severely degrade query performance and exhaust system memory, requiring careful label design and monitoring.\n\n**Lexical Analysis**: The process of breaking query strings into meaningful tokens like metric names, operators, functions, and literal values, serving as the first phase of query parsing. Lexical analysis handles syntax elements like whitespace, quotes, and escape sequences while identifying token types that guide subsequent parsing phases.\n\n**Query Result Caching**: A performance optimization technique that stores expensive query results in memory with associated cache keys, expiration times, and invalidation rules to avoid recomputing identical queries. Caching reduces storage system load and improves dashboard responsiveness, particularly for complex aggregation queries over large time ranges.\n\n### Operational and Monitoring Terms\n\n**Adaptive refresh rates**: Dynamically adjusted update frequencies for dashboard panels based on system performance, data velocity, and user interaction patterns. Adaptive refresh rates prevent system overload during high-traffic periods while ensuring timely updates for critical monitoring scenarios.\n\n**Alert Flapping**: Rapid oscillation between firing and resolved states caused by threshold values that are too close to normal fluctuations in metric values. Flapping creates notification storms that desensitize users to real problems and can be prevented through proper threshold selection, hysteresis, and duration requirements.\n\n**Backpressure**: A flow control mechanism that prevents system overload by slowing down or rejecting input when downstream components cannot keep up with the incoming data rate. Backpressure protects system stability during traffic spikes but must be carefully implemented to avoid cascading delays.\n\n**Circuit Breaker**: A fault tolerance pattern that prevents cascade failures by automatically stopping requests to failing services after a configured failure threshold is exceeded. Circuit breakers provide fast-fail behavior during outages and can automatically recover when the downstream service becomes healthy again.\n\n**Dashboard Templating**: A variable substitution system that enables creation of reusable parameterized dashboards where users can select different services, environments, or time ranges without creating separate dashboard configurations. Templating reduces configuration maintenance and enables consistent monitoring across similar components.\n\n**Duration-Based Evaluation**: An alert evaluation strategy that requires conditions to persist for a specified time period before transitioning to firing state, preventing transient spikes from triggering false alerts. Duration-based evaluation filters noise but introduces delay in alert notification, requiring balanced configuration.\n\n**Exponential Backoff**: A retry delay strategy where the wait time increases exponentially with each failed attempt, helping to reduce system load during outages while still attempting recovery. Exponential backoff prevents retry storms that could worsen system problems during failures.\n\n**Graceful Degradation**: System design that maintains partial functionality during failures rather than complete unavailability, such as displaying cached dashboard data when query systems are unavailable. Graceful degradation improves user experience during outages and maintains basic monitoring capabilities.\n\n**Grid Layout System**: A responsive positioning framework for organizing dashboard panels that automatically adjusts to different screen sizes and supports drag-and-drop rearrangement. Grid layouts provide consistent visual organization while enabling flexible dashboard customization.\n\n**Health Checking**: Periodic verification of component availability and functionality through automated tests that validate both internal state and external dependencies. Health checks enable automated failure detection, load balancer routing decisions, and system health dashboards.\n\n**Horizontal Partitioning**: A data distribution strategy that splits datasets across multiple storage nodes based on partitioning keys like metric name hash or time ranges. Horizontal partitioning enables system scaling by distributing load and storage requirements across multiple machines.\n\n**Incremental Updates**: A data transmission optimization that sends only new or changed information rather than complete datasets, reducing network bandwidth and improving dashboard responsiveness. Incremental updates require careful state management to ensure data consistency and handle missed updates.\n\n**Message Templating**: A customizable formatting system for notification content that allows channel-specific message generation using variables from alert context like metric values, labels, and timestamps. Templates enable consistent notification formatting while supporting different communication channels.\n\n**Notification Routing**: A rule-based system that determines which communication channels receive specific alert notifications based on factors like alert severity, time of day, escalation policies, and team assignments. Proper routing ensures the right people receive timely notifications without overwhelming any individual or channel.\n\n**Panel Subscription**: A client-side registration system that tracks which dashboard panels a user is viewing and their required refresh rates, enabling efficient server-side update delivery through WebSocket connections. Subscriptions prevent unnecessary data transmission for off-screen panels.\n\n**Pull-Based Collection**: A metrics gathering approach where the monitoring system actively scrapes metric data from application endpoints at regular intervals, providing centralized control over collection timing and enabling service discovery integration. Pull-based collection simplifies application configuration but requires network connectivity and service registration.\n\n**Push-Based Collection**: A metrics transmission method where applications actively send metric data to the monitoring system, enabling immediate data delivery and supporting scenarios where pull-based access is not feasible. Push-based collection provides lower latency but requires applications to handle delivery failures and retries.\n\n**WebSocket Connections**: Persistent bidirectional communication channels between dashboard clients and servers that enable real-time data updates without polling overhead. WebSocket connections support efficient live dashboard updates but require connection management and reconnection handling for reliability.\n\n### Error Handling and Reliability Terms\n\n**Cascade Failure**: A failure propagation pattern where problems in one system component trigger failures in dependent components, potentially leading to complete system unavailability. Cascade failures can be prevented through circuit breakers, timeouts, bulkheads, and other fault isolation techniques.\n\n**Correlation IDs**: Unique identifiers attached to requests that enable tracing of operations across multiple system components and log aggregation for debugging distributed system interactions. Correlation IDs are essential for troubleshooting complex failures that span multiple services.\n\n**Delivery Confirmation**: Verification mechanisms that ensure alert notifications were successfully transmitted to their intended destinations, including retry logic for failed deliveries and escalation procedures for persistent failures. Delivery confirmation is critical for ensuring that important alerts reach their intended recipients.\n\n**Error Injection**: A testing technique that deliberately introduces failures into system components to validate error handling code paths and system resilience. Error injection helps identify weak points in fault tolerance and ensures that error handling code actually works as intended.\n\n**Notification Queue**: A background processing system that manages alert notification delivery with features like retry logic, rate limiting, and priority handling to ensure reliable message delivery while preventing notification storms. Queues provide delivery guarantees and help manage notification volume during incident situations.\n\n**Rate Limiting**: A throttling mechanism that prevents notification storms by limiting the number of alerts that can be sent through specific channels within defined time windows. Rate limiting protects communication channels from overload while ensuring critical alerts still get delivered.\n\n**State Persistence**: Durability mechanisms that ensure alert states, dashboard configurations, and other critical system data survive system restarts and failures. State persistence enables proper alert lifecycle management and prevents data loss during system maintenance or unexpected outages.\n\n### Testing and Development Terms\n\n**Integration Testing**: Testing approach that validates component interactions using real dependencies and network communication to ensure the complete system works correctly end-to-end. Integration testing catches issues that unit tests miss, such as serialization problems, network timeouts, and protocol mismatches.\n\n**Milestone Validation Checkpoints**: Structured verification procedures that confirm major system capabilities are working correctly after each development phase, including specific test cases, expected behaviors, and success criteria. Checkpoints help ensure progress is solid before moving to more complex features.\n\n**Mock Dependencies**: Test doubles that simulate the behavior of external components like databases, network services, and file systems, enabling isolated unit testing and controlled failure scenario testing. Mocks allow testing error conditions that would be difficult or dangerous to reproduce with real systems.\n\n**Performance Validation**: Testing procedures that verify system behavior under realistic load conditions, including throughput measurement, latency analysis, and resource utilization monitoring. Performance validation ensures the system can handle production workloads and identifies scalability bottlenecks.\n\n**Test Fixtures**: Reusable test data sets, configuration files, and utility functions that provide consistent testing environments and reduce test setup complexity. Fixtures enable reliable test reproduction and make it easier to create comprehensive test coverage.\n\n**Test Harness**: Infrastructure for setting up complete system instances in test environments, including component coordination, configuration management, and cleanup procedures. Test harnesses enable integration testing and provide controlled environments for manual testing and debugging.\n\n**Unit Testing**: Testing individual components in isolation with mocked dependencies to validate specific functionality, error conditions, and edge cases. Unit testing provides fast feedback during development and ensures individual components work correctly before integration.\n\n### Advanced and Extension Terms\n\n**Anomaly Detection**: Machine learning techniques that identify unusual patterns in metric data that deviate significantly from historical behavior, enabling proactive alerting for problems that don't have known thresholds. Anomaly detection can identify subtle issues that traditional threshold-based alerting would miss.\n\n**Auto-Scaling**: Automatic adjustment of system capacity based on demand metrics like CPU usage, memory pressure, or queue lengths, enabling cost optimization while maintaining performance. Auto-scaling requires careful configuration to avoid oscillation and must integrate with deployment systems.\n\n**Consistency Level**: Guarantees about data consistency across replicas in distributed systems, ranging from eventual consistency (data will be consistent eventually) to strong consistency (all reads return the most recent write). Consistency levels involve trade-offs between performance and data accuracy.\n\n**Feature Extraction**: The process of deriving meaningful variables from raw metric data for machine learning analysis, such as trend calculations, seasonal decomposition, and statistical summaries. Feature extraction transforms time-series data into formats suitable for classification and prediction algorithms.\n\n**Gossip Protocol**: A decentralized communication method for sharing cluster membership information and failure detection where each node periodically exchanges state information with a subset of other nodes. Gossip protocols provide eventual consistency and fault tolerance without requiring central coordination.\n\n**Heatmap Visualization**: Color-coded matrix displays that show data density, value distributions, or correlation patterns across two-dimensional parameter spaces. Heatmaps are particularly useful for showing request latency distributions over time or error rates across different services.\n\n**Horizontal Scaling**: System architecture that handles increased load by adding more servers rather than upgrading existing hardware, enabling theoretically unlimited capacity growth. Horizontal scaling requires careful design of data partitioning, load balancing, and coordination protocols.\n\n**Model Registry**: Centralized management system for trained machine learning models, including version control, deployment tracking, and performance monitoring. Model registries enable systematic management of ML-powered features like anomaly detection and forecasting.\n\n**Multi-Tenancy**: System design that supports multiple isolated user groups on shared infrastructure while maintaining data isolation, performance guarantees, and security boundaries. Multi-tenancy reduces operational overhead while requiring careful resource management and access control.\n\n**Predictive Alerting**: Alert systems that use forecasting models to identify potential future problems before they occur, enabling proactive intervention rather than reactive response. Predictive alerting can prevent outages but requires sophisticated modeling and careful false positive management.\n\n**Quorum-Based Approach**: Decision-making systems that require agreement from a majority of participants to proceed with operations, preventing split-brain scenarios in distributed systems. Quorum systems provide strong consistency guarantees but require careful configuration to handle network partitions.\n\n**Replication Factor**: The number of copies of each data item stored across different nodes in a distributed system, providing durability and availability at the cost of storage overhead and consistency complexity. Higher replication factors improve fault tolerance but increase coordination requirements.\n\n**Split-Brain Prevention**: Architectural patterns and protocols that avoid scenarios where network partitions cause different parts of a distributed system to make conflicting decisions. Prevention techniques include quorum systems, witness nodes, and careful leader election protocols.\n\n### Performance and Optimization Terms\n\n**Compression Optimization**: Tailoring data compression algorithms to specific data patterns found in time-series metrics, such as delta encoding for counter values and dictionary encoding for repetitive label values. Optimized compression significantly reduces storage requirements and improves query performance.\n\n**Edge Caching**: Performance optimization that stores frequently accessed data closer to users or applications to reduce latency and backend load. Edge caching is particularly effective for dashboard data and metric metadata that are accessed repeatedly.\n\n**Load Balancing**: Distribution of incoming requests across multiple servers to optimize resource utilization, minimize response time, and prevent individual server overload. Load balancing requires careful selection of algorithms and health checking to ensure optimal distribution.\n\n**Query Optimization**: Techniques for improving query performance through better execution planning, index utilization, result caching, and algorithm selection. Query optimization is crucial for maintaining dashboard responsiveness and alert evaluation performance as data volumes grow.\n\n### Acronyms and Abbreviations\n\n**ADR**: Architecture Decision Record - a structured format for documenting important design decisions with context, options, rationales, and consequences.\n\n**API**: Application Programming Interface - the defined methods and data formats that components use to communicate with each other.\n\n**AST**: Abstract Syntax Tree - the hierarchical representation of parsed queries used for optimization and execution.\n\n**HTTP**: HyperText Transfer Protocol - the communication protocol used for web-based interfaces and REST APIs.\n\n**JSON**: JavaScript Object Notation - the data serialization format used for configuration files, API responses, and data exchange.\n\n**REST**: Representational State Transfer - an architectural style for designing web APIs using standard HTTP methods.\n\n**SLA**: Service Level Agreement - formal commitments about system availability, performance, and reliability.\n\n**SLI**: Service Level Indicator - specific metrics that measure system performance against defined objectives.\n\n**SLO**: Service Level Objective - target values or ranges for service level indicators that define acceptable performance.\n\n**WAL**: Write-Ahead Log - durability mechanism that records intended operations before executing them to enable recovery.\n\n**YAML**: YAML Ain't Markup Language - human-readable data serialization format commonly used for configuration files.\n\n### Implementation Guidance\n\nThe terminology in this glossary directly maps to specific implementation patterns and naming conventions used throughout the codebase. Each term has been carefully chosen to reflect industry standards while maintaining consistency with our specific architectural decisions.\n\n**A. Naming Convention Mapping**\n\nThe glossary terms serve as the foundation for consistent naming across all system components. When implementing any feature, developers should use these exact terms in variable names, function names, and documentation to maintain clarity and searchability.\n\n| Concept Category | Implementation Pattern | Example Usage |\n|------------------|----------------------|---------------|\n| Data Structures | Direct mapping to struct/type names | `AlertState`, `MetricType`, `Labels` |\n| Operations | Verb-noun pattern using glossary terms | `ValidateCardinality()`, `CompactBlocks()` |\n| Constants | ALL_CAPS with underscore separation | `ALERT_STATE_FIRING`, `METRIC_TYPE_COUNTER` |\n| Configuration | Nested structures following glossary hierarchy | `StorageConfig.RetentionPeriod` |\n\n**B. Documentation Standards**\n\nEvery public function, method, and type should include documentation that references appropriate glossary terms to ensure consistent understanding across the codebase. This creates a self-reinforcing system where code documentation strengthens terminology understanding.\n\n**C. Error Message Consistency**\n\nError messages throughout the system should use glossary terminology to help users quickly understand problems and find relevant documentation. For example, \"cardinality explosion detected\" is more helpful than \"too many time series\" because it points to specific glossary entries and troubleshooting procedures.\n\n**D. Monitoring and Alerting Vocabulary**\n\nThe operational procedures and alert definitions should use glossary terms in their names and descriptions, creating consistency between system behavior and documentation. This makes it easier to correlate alerts with architectural documentation and debugging procedures.\n\n**E. Team Communication Guidelines**\n\nDuring code reviews, design discussions, and incident response, team members should use glossary terminology to ensure precise communication. When someone uses a term differently than the glossary definition, it signals a need to either update the glossary or clarify the usage to maintain consistency.\n\nThe glossary serves as both a reference document and a communication contract that enables precise technical discussions and consistent system implementation across the entire metrics and alerting platform.\n"}