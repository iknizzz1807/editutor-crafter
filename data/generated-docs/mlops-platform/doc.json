{"html":"<h1 id=\"mlops-platform-design-document\">MLOps Platform: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>An end-to-end MLOps platform that provides a unified workflow for machine learning teams to track experiments, version models, orchestrate training pipelines, deploy models to production, and monitor their performance. The key architectural challenge is building a scalable, fault-tolerant system that integrates diverse ML tools while maintaining data lineage and reproducibility across the entire ML lifecycle.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This foundational section provides context for all milestones (1-5) by establishing why integrated MLOps platforms are necessary.</p>\n</blockquote>\n<p>Machine learning development in most organizations resembles a chaotic research laboratory where brilliant scientists work in isolation, each maintaining their own experimental notebooks, storing samples in unmarked containers, and using incompatible equipment. While individual experiments might succeed, the lack of coordination creates a system where breakthroughs cannot be reliably reproduced, promising discoveries get lost in transition from research to production, and teams spend more time wrestling with infrastructure than advancing the science. The transformation from this ad-hoc approach to a coordinated MLOps platform mirrors the evolution from individual laboratory benches to modern pharmaceutical research facilities with standardized protocols, centralized sample management, and automated quality control systems.</p>\n<p>The core challenge lies not in the sophistication of any single ML algorithm, but in orchestrating the complex dependencies between data preparation, model training, validation, deployment, and monitoring across teams and time. Unlike traditional software development where code artifacts are largely deterministic, machine learning introduces probabilistic models trained on evolving datasets, creating a web of dependencies that traditional DevOps tools cannot adequately manage. The platform must handle the inherent uncertainty and experimentation nature of ML development while providing the reliability and reproducibility guarantees that production systems demand.</p>\n<h3 id=\"the-ml-development-chaos\">The ML Development Chaos</h3>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"MLOps Platform System Architecture\"></p>\n<p>Consider a typical machine learning team six months into a project. Data scientists have trained dozens of model variants, each stored on individual laptops with handwritten notes about hyperparameters. The most promising model achieved 87% accuracy, but nobody can remember which exact combination of data preprocessing, feature engineering, and training parameters produced that result. The engineering team has been waiting three weeks for a production-ready model, but every attempt to reproduce the data scientist&#39;s results yields slightly different accuracy scores. Meanwhile, the business stakeholders are asking why the model that worked perfectly in the demo now fails on real customer data, and the compliance team needs to audit the training process for a model that was deployed two months ago.</p>\n<p>This scenario illustrates the <strong>experiment tracking crisis</strong> that emerges when teams scale beyond individual contributors. Without structured logging of experiments, parameters become tribal knowledge that disappears when team members change roles or forget details. The lack of <strong>artifact management</strong> means that models, datasets, and preprocessing code exist in dozens of versions across different machines, with no authoritative source of truth. <strong>Reproducibility failures</strong> compound over time as subtle environment differences, library version mismatches, and undocumented manual steps make it impossible to recreate previous results.</p>\n<p>The <strong>deployment bottleneck</strong> represents another critical failure mode. In traditional software, deployment means copying stateless code to production servers. In machine learning, deployment requires coordinating model artifacts, inference serving infrastructure, data preprocessing pipelines, and monitoring systems. Teams often resort to manual deployment processes that take weeks to complete, during which time the model&#39;s performance may have already degraded due to data drift. The lack of <strong>version management</strong> means that rolling back a problematic model becomes a manual archaeology project to reconstruct the previous deployment state.</p>\n<table>\n<thead>\n<tr>\n<th>Problem Category</th>\n<th>Symptoms</th>\n<th>Root Causes</th>\n<th>Business Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Chaos</td>\n<td>Lost high-performing models, unreproducible results, duplicate work</td>\n<td>No structured logging, inconsistent environments, manual tracking</td>\n<td>Wasted research effort, delayed model delivery</td>\n</tr>\n<tr>\n<td>Deployment Friction</td>\n<td>Weeks-long deployment cycles, manual model updates, rollback failures</td>\n<td>Ad-hoc serving infrastructure, no version control, manual processes</td>\n<td>Delayed value realization, production incidents</td>\n</tr>\n<tr>\n<td>Monitoring Blindness</td>\n<td>Silent model degradation, undetected data drift, surprise accuracy drops</td>\n<td>No performance tracking, missing alerting, reactive debugging</td>\n<td>Revenue loss, customer satisfaction issues</td>\n</tr>\n<tr>\n<td>Team Coordination</td>\n<td>Duplicate experiments, incompatible toolchains, knowledge silos</td>\n<td>No shared infrastructure, inconsistent processes, individual workflows</td>\n<td>Reduced team velocity, knowledge loss</td>\n</tr>\n</tbody></table>\n<p>The <strong>collaboration breakdown</strong> occurs when team members use incompatible tools and workflows. Data scientists prefer Jupyter notebooks and Python libraries, while ML engineers favor containerized services and deployment automation. Data engineers work with batch processing systems, while platform engineers focus on real-time serving infrastructure. Without a unified platform that bridges these different working styles and tool preferences, teams develop parallel systems that cannot interoperate, leading to costly integration projects and duplicated effort.</p>\n<h3 id=\"existing-mlops-solutions\">Existing MLOps Solutions</h3>\n<p>The current landscape of MLOps tools reflects the evolution from individual solutions addressing specific pain points to comprehensive platforms attempting to cover the entire ML lifecycle. Understanding the strengths and limitations of existing approaches provides crucial context for architectural decisions in building a new platform.</p>\n<p><strong>MLflow</strong> represents the experiment tracking generation of tools, born from Databricks&#39; experience with large-scale ML projects. Its core strength lies in lightweight experiment logging that requires minimal changes to existing training code. Data scientists can add a few lines of <code>mlflow.log_param()</code> and <code>mlflow.log_metric()</code> calls to their existing scripts and immediately gain experiment tracking capabilities. The model registry provides basic versioning with manual stage transitions (None → Staging → Production → Archived), while the model serving component offers simple REST API deployment for common frameworks.</p>\n<p>However, MLflow&#39;s simplicity becomes a limitation at enterprise scale. The deployment capabilities are minimal compared to production requirements for auto-scaling, traffic management, and high availability. The pipeline orchestration is rudimentary, essentially a thin wrapper around existing workflow tools rather than a purpose-built ML orchestration engine. Security and multi-tenancy support lag behind enterprise requirements, and the monitoring capabilities focus primarily on system metrics rather than ML-specific concerns like data drift and model performance degradation.</p>\n<p><strong>Kubeflow</strong> takes the opposite approach, building a comprehensive ML platform on Kubernetes from the ground up. Its strength lies in scalability and integration with cloud-native infrastructure patterns. Kubeflow Pipelines provides sophisticated DAG-based orchestration with support for complex data dependencies and resource management. The multi-framework support through Kubeflow Training Operators enables distributed training across different ML libraries. KFServing (now KServe) offers production-grade model serving with advanced traffic management and auto-scaling capabilities.</p>\n<p>The complexity that enables Kubeflow&#39;s power also creates adoption barriers. Teams need deep Kubernetes expertise to operate Kubeflow effectively, making it inaccessible to data science teams focused on model development rather than infrastructure management. The component ecosystem is extensive but can be overwhelming, with multiple overlapping solutions for similar problems. Configuration complexity grows exponentially as teams customize components for their specific requirements.</p>\n<p><strong>Cloud-native solutions</strong> like SageMaker, Azure ML, and Vertex AI provide managed MLOps platforms that eliminate infrastructure management overhead. These platforms excel at integration with their respective cloud ecosystems, offering seamless data access, compute scaling, and billing integration. The managed nature means that teams can focus on ML development rather than platform operations. Built-in compliance features, security controls, and enterprise governance capabilities address requirements that open-source solutions often leave as integration challenges.</p>\n<p>The trade-off for convenience is vendor lock-in and reduced flexibility. Customization options are limited to what the cloud provider supports, which may not align with specialized ML workflows or existing tool preferences. Cost optimization becomes challenging when compute resources are bundled with platform features. Migration between cloud providers requires significant reworking of ML pipelines and training code.</p>\n<table>\n<thead>\n<tr>\n<th>Solution Category</th>\n<th>Strengths</th>\n<th>Limitations</th>\n<th>Best Fit Scenarios</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MLflow</td>\n<td>Lightweight adoption, broad framework support, strong experiment tracking</td>\n<td>Limited deployment capabilities, basic orchestration, minimal monitoring</td>\n<td>Small to medium teams, research-focused organizations, rapid prototyping</td>\n</tr>\n<tr>\n<td>Kubeflow</td>\n<td>Comprehensive ML capabilities, cloud-native architecture, scalable infrastructure</td>\n<td>High complexity, Kubernetes expertise required, steep learning curve</td>\n<td>Large engineering teams, cloud-native organizations, complex ML workflows</td>\n</tr>\n<tr>\n<td>Cloud Platforms</td>\n<td>Managed infrastructure, enterprise features, ecosystem integration</td>\n<td>Vendor lock-in, limited customization, cost optimization challenges</td>\n<td>Enterprise organizations, cloud-first strategies, compliance-heavy industries</td>\n</tr>\n<tr>\n<td>Custom Solutions</td>\n<td>Complete control, tailored to specific needs, no vendor dependencies</td>\n<td>High development cost, ongoing maintenance burden, expertise requirements</td>\n<td>Large tech companies, unique ML requirements, strong engineering teams</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>The Platform Integration Challenge</strong>: Most organizations end up with a combination of tools that address different aspects of the ML lifecycle, but lack integration between components. Data flows through manual handoffs between experiment tracking, model registry, deployment, and monitoring systems, creating opportunities for errors and inconsistencies. The challenge is not choosing the best individual tool for each function, but designing a cohesive system where components work together seamlessly.</p>\n</blockquote>\n<h3 id=\"core-technical-challenges\">Core Technical Challenges</h3>\n<p>Building an integrated MLOps platform requires solving three fundamental distributed systems challenges that emerge from the unique characteristics of machine learning workflows: <strong>state management</strong> across long-running experiments and training jobs, <strong>distributed coordination</strong> of pipeline steps with complex data dependencies, and <strong>data consistency</strong> guarantees in a system where artifacts, metadata, and model performance evolve continuously.</p>\n<h4 id=\"state-management-complexity\">State Management Complexity</h4>\n<p>Machine learning workflows differ fundamentally from traditional web services in their state management requirements. While web applications typically handle stateless requests that complete in milliseconds, ML training jobs run for hours or days while continuously generating state that must be preserved, queried, and correlated across multiple dimensions.</p>\n<p>Consider the state that accumulates during a single training experiment: hyperparameters selected at the beginning, training metrics logged at each epoch, model checkpoints saved periodically, validation scores computed on holdout data, and artifacts like feature importance plots generated at completion. This state has several challenging characteristics that traditional databases struggle to handle efficiently.</p>\n<p>The <strong>temporal dimension</strong> creates query complexity that standard relational models handle poorly. Analysts want to visualize how training loss decreased over time across multiple experiments, comparing learning curves between different hyperparameter configurations. This requires efficient time-series queries across potentially millions of metric points, with grouping and aggregation capabilities that span experiment boundaries. The naive approach of storing each metric point as a database row quickly becomes prohibitive at scale, while specialized time-series solutions often lack the metadata correlation capabilities needed for experiment analysis.</p>\n<p>The <strong>hierarchical experiment organization</strong> adds another layer of complexity. Teams organize experiments into projects, with runs grouped by model architecture or dataset version. Individual runs contain multiple training phases (preprocessing, training, evaluation), each generating its own metrics and artifacts. The metadata relationships form a complex graph rather than simple tree structures, as models may inherit preprocessed datasets from previous experiments or use transfer learning from models trained in different projects.</p>\n<p><strong>Artifact lifecycle management</strong> presents unique challenges because ML artifacts have different access patterns than typical file storage. Model checkpoints may be large (gigabytes) but accessed infrequently after training completion. Training datasets are read-heavy during active experimentation but rarely modified. Experiment notebooks and plots are small but require fast access for interactive analysis. The storage system must optimize for these mixed workloads while maintaining strong consistency guarantees for metadata queries.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The state management challenge requires treating experiments as long-lived, stateful entities rather than fire-and-forget jobs. The platform must provide transactional guarantees for experiment metadata updates while supporting high-throughput writes for metric logging and efficient reads for analysis queries.</p>\n</blockquote>\n<h4 id=\"distributed-coordination-challenges\">Distributed Coordination Challenges</h4>\n<p>ML pipeline orchestration introduces coordination complexity that traditional workflow engines struggle to handle effectively. Unlike business process workflows where task dependencies are primarily sequential, ML pipelines often involve complex data dependencies, resource contention, and conditional execution paths that must be coordinated across distributed compute resources.</p>\n<p><strong>Data dependency coordination</strong> represents the most complex aspect of ML pipeline orchestration. Consider a training pipeline that preprocesses data, trains multiple model variants in parallel, evaluates each variant on different test sets, and selects the best performer for deployment. The data dependencies form a complex graph where downstream steps may require outputs from multiple upstream steps, but those outputs may be generated at different times and on different compute nodes.</p>\n<p>The naive approach of copying data between steps quickly becomes prohibitive when working with large datasets. Instead, the orchestration system must coordinate <strong>data locality</strong> to minimize transfer overhead while ensuring that compute resources are available when data dependencies are satisfied. This requires sophisticated scheduling that considers both data placement and resource availability across the cluster.</p>\n<p><strong>Resource allocation complexity</strong> emerges from the heterogeneous compute requirements of ML workloads. Data preprocessing steps may require high-memory nodes, training steps need GPU clusters, and evaluation steps can run on standard CPU instances. The resource requirements may not be known until runtime, as they depend on data characteristics discovered during preprocessing. The orchestration system must support <strong>dynamic resource allocation</strong> while preventing resource starvation and ensuring fair sharing across concurrent pipelines.</p>\n<p><strong>Fault tolerance</strong> in ML pipelines requires more sophisticated recovery strategies than traditional workflows. Training jobs may run for days, making simple restart-from-beginning policies prohibitively expensive. Instead, the system must support <strong>checkpoint-based recovery</strong> where failed training steps can resume from the last saved checkpoint rather than restarting completely. However, checkpoint consistency across distributed training jobs introduces additional complexity, as the system must ensure that all workers reach checkpoints synchronously to maintain model consistency.</p>\n<p>The <strong>conditional execution</strong> patterns common in ML workflows challenge traditional DAG-based orchestration models. Hyperparameter search involves executing training steps with different parameters until convergence criteria are met. Early stopping may terminate training steps before completion based on validation metrics. Model selection may choose between alternative preprocessing or training approaches based on data characteristics discovered at runtime. These patterns require <strong>dynamic pipeline modification</strong> capabilities that go beyond static DAG execution.</p>\n<table>\n<thead>\n<tr>\n<th>Coordination Challenge</th>\n<th>Traditional Workflow Approach</th>\n<th>ML-Specific Requirements</th>\n<th>Technical Solutions Needed</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Dependencies</td>\n<td>File-based handoffs between steps</td>\n<td>Efficient large dataset transfer, data lineage tracking</td>\n<td>Distributed storage integration, metadata-driven coordination</td>\n</tr>\n<tr>\n<td>Resource Management</td>\n<td>Static resource allocation</td>\n<td>Dynamic GPU/CPU requirements, long-running jobs</td>\n<td>Resource scheduling with ML-aware policies</td>\n</tr>\n<tr>\n<td>Fault Tolerance</td>\n<td>Restart failed tasks</td>\n<td>Checkpoint recovery, distributed training consistency</td>\n<td>Stateful step recovery, coordinated checkpointing</td>\n</tr>\n<tr>\n<td>Conditional Execution</td>\n<td>Static DAG execution</td>\n<td>Hyperparameter search, early stopping, model selection</td>\n<td>Dynamic pipeline modification, event-driven orchestration</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-consistency-guarantees\">Data Consistency Guarantees</h4>\n<p>The distributed nature of MLOps platforms creates consistency challenges that span multiple storage systems and involve both structured metadata and unstructured artifacts. Unlike traditional applications where consistency requirements are localized to a single database, MLOps platforms must maintain consistency across experiment tracking databases, artifact storage systems, model registries, and deployment infrastructure.</p>\n<p><strong>Cross-system consistency</strong> emerges when an experiment run generates metadata stored in a relational database, artifacts stored in object storage, and model versions registered in a separate registry system. These updates must appear atomic from the perspective of platform users, even though they involve multiple storage systems with different consistency guarantees. Consider the failure scenario where experiment metadata is successfully written but artifact upload fails due to network issues. Without proper coordination, the experiment appears complete in queries but missing critical artifacts needed for reproducibility.</p>\n<p>The challenge intensifies with <strong>model promotion workflows</strong> that must coordinate updates across multiple systems. Promoting a model from staging to production involves updating the model registry metadata, deploying new serving infrastructure, updating traffic routing configuration, and initializing monitoring data collection. These updates must appear atomic to prevent inconsistent states where traffic routes to an undeployed model or monitoring systems track the wrong model version.</p>\n<p><strong>Eventual consistency trade-offs</strong> become critical in distributed MLOps systems that span multiple geographic regions or cloud providers. Experiment data logged in one region must be available for analysis in other regions, but strict consistency requirements would make the system unusable during network partitions. The platform must carefully choose which consistency guarantees are essential for correctness and which operations can tolerate eventual consistency.</p>\n<p><strong>Metadata vs. Artifact consistency</strong> requires different strategies due to the size and access pattern differences between structured metadata and large binary artifacts. Experiment metadata must be immediately consistent to support real-time analysis and comparison queries. Artifact storage can tolerate eventual consistency as long as metadata accurately reflects artifact availability. This suggests a <strong>two-tier consistency model</strong> where metadata operations require strong consistency while artifact operations can be eventually consistent.</p>\n<p>The <strong>lineage consistency</strong> challenge ensures that model lineage information remains accurate as experiments, models, and deployments evolve. Model lineage links deployed models back to their source experiments, training data versions, and code commits. As these entities are updated or archived, the lineage relationships must be maintained consistently to support compliance and debugging requirements. This requires <strong>referential integrity</strong> across multiple storage systems and careful handling of deletion operations.</p>\n<blockquote>\n<p><strong>Decision: Event-Driven Consistency Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: MLOps platforms must maintain consistency across multiple storage systems while supporting high-throughput operations and geographic distribution.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Two-phase commit across all storage systems</li>\n<li>Event-driven eventual consistency with compensation</li>\n<li>Single-system consistency with data duplication</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement event-driven consistency with compensation for non-critical operations and two-phase commit only for critical state transitions</li>\n<li><strong>Rationale</strong>: Two-phase commit across all systems would create availability and performance bottlenecks. Single-system approaches limit scalability and increase vendor lock-in. Event-driven consistency allows optimizing consistency vs. performance trade-offs per operation type.</li>\n<li><strong>Consequences</strong>: Enables high-throughput experiment logging with eventual consistency while maintaining strong consistency for critical operations like model promotion. Requires sophisticated event handling and compensation logic.</li>\n</ul>\n</blockquote>\n<p>⚠️ <strong>Pitfall: Ignoring Consistency Boundaries</strong></p>\n<p>A common mistake in MLOps platform design is treating all operations as requiring the same consistency guarantees. Teams often either apply strong consistency everywhere (creating performance bottlenecks) or eventual consistency everywhere (creating correctness issues). The correct approach is carefully identifying which operations require strong consistency (model promotion, experiment completion) and which can tolerate eventual consistency (metric logging, artifact availability). This requires explicit consistency boundary design rather than relying on default database settings.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Understanding the context and problems that MLOps platforms solve provides the foundation for making informed architectural decisions. The following technical recommendations help translate these insights into concrete implementation choices.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Metadata</td>\n<td>PostgreSQL with JSONB columns</td>\n<td>Time-series DB (InfluxDB) + metadata DB (PostgreSQL)</td>\n</tr>\n<tr>\n<td>Artifact Storage</td>\n<td>Local filesystem with backup</td>\n<td>Object storage (S3/GCS/Azure Blob) with CDN</td>\n</tr>\n<tr>\n<td>Pipeline Orchestration</td>\n<td>Simple job queue (Redis/RabbitMQ)</td>\n<td>Kubernetes-native workflows (Argo/Tekton)</td>\n</tr>\n<tr>\n<td>Model Serving</td>\n<td>HTTP REST with Python Flask/FastAPI</td>\n<td>Kubernetes + Istio with traffic splitting</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Application logs + basic metrics</td>\n<td>Dedicated observability platform (Prometheus/Grafana)</td>\n</tr>\n<tr>\n<td>Message Coordination</td>\n<td>HTTP APIs for synchronous coordination</td>\n<td>Event streaming (Kafka/Pulsar) for async coordination</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-project-structure\">Recommended Project Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>mlops-platform/\n├── cmd/                          # Entry points for different services\n│   ├── experiment-server/        # Experiment tracking HTTP API\n│   ├── model-registry/           # Model registry service\n│   ├── pipeline-orchestrator/    # Training pipeline scheduler\n│   ├── deployment-manager/       # Model deployment controller\n│   └── monitoring-collector/     # Model performance monitoring\n├── internal/                     # Private application code\n│   ├── storage/                  # Storage layer abstractions\n│   │   ├── metadata/             # Experiment and model metadata\n│   │   ├── artifacts/            # Model and artifact storage\n│   │   └── timeseries/           # Metrics and monitoring data\n│   ├── coordination/             # Cross-component communication\n│   │   ├── events/               # Event-driven messaging\n│   │   └── apis/                 # Internal API definitions\n│   └── common/                   # Shared utilities and types\n├── api/                          # Public API definitions\n│   ├── rest/                     # REST API specifications\n│   └── proto/                    # Protocol buffer definitions (if using gRPC)\n├── web/                          # Web dashboard and UI\n├── deploy/                       # Deployment configurations\n│   ├── docker/                   # Docker configurations\n│   ├── k8s/                      # Kubernetes manifests\n│   └── terraform/                # Infrastructure as code\n└── docs/                         # Documentation and design docs</code></pre></div>\n\n<h4 id=\"foundation-infrastructure-code\">Foundation Infrastructure Code</h4>\n<p>The following complete implementations provide the infrastructure foundation that subsequent components will build upon:</p>\n<p><strong>Event Coordination System</strong> (<code>internal/coordination/events/coordinator.go</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Event coordination system for cross-component communication.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides reliable event publishing and subscription for MLOps workflows.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Callable, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> queue </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Queue, Empty</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Event</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a system event with metadata and payload.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new event with auto-generated ID and timestamp.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">event_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            source</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">source,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            payload</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">payload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Coordinates events between MLOps platform components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports both synchronous and asynchronous event handling patterns.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._subscribers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Callable]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._event_queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Queue()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._worker_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start the event processing worker thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._running:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._worker_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._process_events)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._worker_thread.daemon </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._worker_thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop event processing and wait for worker thread completion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._worker_thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._worker_thread.join(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> publish</span><span style=\"color:#E1E4E8\">(self, event: Event, synchronous: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Publish an event to all registered subscribers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            event: Event to publish</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            synchronous: If True, process immediately. If False, queue for async processing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> synchronous:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._deliver_event(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._event_queue.put(event)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> subscribe</span><span style=\"color:#E1E4E8\">(self, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, handler: Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Subscribe to events of a specific type.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            event_type: Type of event to subscribe to (e.g., \"experiment.completed\")</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            handler: Function to call when event is received</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> event_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._subscribers:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._subscribers[event_type] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._subscribers[event_type].append(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _process_events</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Worker thread function that processes queued events.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._running:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                event </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._event_queue.get(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._deliver_event(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> Empty:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _deliver_event</span><span style=\"color:#E1E4E8\">(self, event: Event):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deliver event to all registered subscribers for the event type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            subscribers </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._subscribers.get(event.type, [])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subscribers:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    handler(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                    # Log error but continue processing other subscribers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Error in event handler for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global event coordinator instance</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">event_coordinator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> EventCoordinator()</span></span></code></pre></div>\n\n<p><strong>Storage Abstraction Layer</strong> (<code>internal/storage/base.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Storage abstraction layer providing unified interface for different storage backends.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Supports both metadata (structured) and artifact (binary) storage patterns.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional, BinaryIO</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sqlite3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetadataStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for structured metadata storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a table with the specified schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Insert data and return the generated ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update existing record by ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, filters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">             order_by: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Query records with optional filtering, ordering, and limiting.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_by_id</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get a single record by ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SQLiteMetadataStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MetadataStore</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"SQLite implementation of metadata storage for development/testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, db_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.db_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db_path</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        os.makedirs(os.path.dirname(db_path), </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._conn </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sqlite3.connect(db_path, </span><span style=\"color:#FFAB70\">check_same_thread</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._conn.row_factory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sqlite3.Row  </span><span style=\"color:#6A737D\"># Enable column access by name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create table with columns defined in schema dict.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        columns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#79B8FF\"> {</span><span style=\"color:#E1E4E8\">type_def</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> name, type_def </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> schema.items()]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"CREATE TABLE IF NOT EXISTS </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">', '</span><span style=\"color:#E1E4E8\">.join(columns)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._conn.execute(sql)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._conn.commit()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Insert data and return the row ID as string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        columns </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(data.keys())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        placeholders </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'?'</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> columns]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        values </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [data[col] </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> columns]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"INSERT INTO </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">', '</span><span style=\"color:#E1E4E8\">.join(columns)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">) VALUES (</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">', '</span><span style=\"color:#E1E4E8\">.join(placeholders)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._conn.execute(sql, values)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._conn.commit()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(cursor.lastrowid)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update record by ID, return True if successful.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        set_clause </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> ', '</span><span style=\"color:#E1E4E8\">.join([</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">col</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = ?\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> data.keys()])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        values </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(data.values()) </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">id</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"UPDATE </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> SET </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">set_clause</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> WHERE id = ?\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._conn.execute(sql, values)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._conn.commit()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> cursor.rowcount </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, filters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">             order_by: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Query with optional WHERE, ORDER BY, and LIMIT clauses.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"SELECT * FROM </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        values </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> filters:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            where_clause </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> ' AND '</span><span style=\"color:#E1E4E8\">.join([</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">col</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> = ?\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> filters.keys()])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sql </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\" WHERE </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">where_clause</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            values.extend(filters.values())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> order_by:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sql </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\" ORDER BY </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">order_by</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> limit:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sql </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\" LIMIT </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">limit</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._conn.execute(sql, values)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">(row) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> row </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> cursor.fetchall()]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_by_id</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get single record by ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._conn.execute(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"SELECT * FROM </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> WHERE id = ?\"</span><span style=\"color:#E1E4E8\">, [</span><span style=\"color:#79B8FF\">id</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        row </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cursor.fetchone()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> dict</span><span style=\"color:#E1E4E8\">(row) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> row </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ArtifactStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for binary artifact storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: BinaryIO, metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store binary data with optional metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[BinaryIO]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve binary data by key.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Delete artifact by key.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_keys</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"List artifact keys with optional prefix filter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FilesystemArtifactStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ArtifactStore</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Local filesystem implementation for artifact storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, base_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(base_path)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_path.mkdir(</span><span style=\"color:#FFAB70\">parents</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: BinaryIO, metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store data to filesystem with metadata as JSON sidecar file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            artifact_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.base_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            artifact_path.parent.mkdir(</span><span style=\"color:#FFAB70\">parents</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Write binary data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(artifact_path, </span><span style=\"color:#9ECBFF\">'wb'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                f.write(data.read())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Write metadata sidecar file</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> metadata:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                metadata_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> artifact_path.with_suffix(</span><span style=\"color:#9ECBFF\">'.metadata.json'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(metadata_path, </span><span style=\"color:#9ECBFF\">'w'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    json.dump(metadata, f)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[BinaryIO]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Read binary data from filesystem.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        artifact_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.base_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> key</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> artifact_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(artifact_path, </span><span style=\"color:#9ECBFF\">'rb'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Delete artifact and metadata files.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            artifact_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.base_path </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            metadata_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> artifact_path.with_suffix(</span><span style=\"color:#9ECBFF\">'.metadata.json'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> artifact_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                artifact_path.unlink()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> metadata_path.exists():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                metadata_path.unlink()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_keys</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"List all artifact keys with optional prefix filter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prefix</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">*\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> prefix </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"*\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        paths </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.base_path.glob(pattern)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Filter out metadata files and return relative paths</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(p.relative_to(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.base_path)) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> paths </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> p.is_file() </span><span style=\"color:#F97583\">and</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> p.name.endswith(</span><span style=\"color:#9ECBFF\">'.metadata.json'</span><span style=\"color:#E1E4E8\">)]</span></span></code></pre></div>\n\n<h4 id=\"core-component-integration-patterns\">Core Component Integration Patterns</h4>\n<p><strong>Component Health Monitoring</strong> (<code>internal/common/health.py</code>):</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Health monitoring utilities for component coordination and debugging.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides standardized health checks and status reporting across components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual health check result.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Manages health checks for a single component.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Used by each major component to report its health status.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.component_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._checks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, HealthCheck] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._last_update </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_check</span><span style=\"color:#E1E4E8\">(self, check_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: </span><span style=\"color:#79B8FF\">callable</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a health check function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store the check function for periodic execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add validation that check_func returns HealthCheck object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up periodic execution timer (every 30 seconds)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_checks</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, HealthCheck]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all registered health checks and return results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through all registered check functions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute each function and capture HealthCheck result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle exceptions by creating UNHEALTHY HealthCheck</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update self._checks dict with results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update self._last_update timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return the updated checks dictionary</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_overall_status</span><span style=\"color:#E1E4E8\">(self) -> HealthStatus:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine overall component health based on individual checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If no checks registered, return UNKNOWN</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If any check is UNHEALTHY, return UNHEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If any check is DEGRADED, return DEGRADED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If all checks are HEALTHY, return HEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Consider check staleness (> 5 minutes old = UNKNOWN)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"language-specific-implementation-hints\">Language-Specific Implementation Hints</h4>\n<p>For Python-based MLOps platform development:</p>\n<ul>\n<li><strong>Use SQLAlchemy for metadata storage</strong> to support multiple database backends while maintaining type safety with declarative models</li>\n<li><strong>Implement async/await patterns</strong> for I/O-heavy operations like artifact uploads and database queries to improve concurrent throughput</li>\n<li><strong>Use Pydantic models</strong> for API request/response validation and serialization, ensuring type safety across component boundaries  </li>\n<li><strong>Leverage pytest fixtures</strong> for creating test data factories that generate realistic experiment data for component testing</li>\n<li><strong>Use structlog for structured logging</strong> to enable correlation of log messages across distributed components using trace IDs</li>\n<li><strong>Implement circuit breaker patterns</strong> using libraries like <code>pybreaker</code> for external service calls to prevent cascade failures</li>\n<li><strong>Use celery or RQ for background tasks</strong> like artifact processing, model training orchestration, and monitoring data aggregation</li>\n</ul>\n<h4 id=\"platform-development-checkpoints\">Platform Development Checkpoints</h4>\n<p>After implementing the foundational infrastructure:</p>\n<p><strong>Checkpoint 1: Event System</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/test_event_coordinator.py</code></li>\n<li>Verify: Events published asynchronously are delivered to subscribers within 1 second</li>\n<li>Manual Test: Start coordinator, subscribe to &quot;test.event&quot;, publish event, confirm handler execution</li>\n<li>Debug: If events aren&#39;t delivered, check that <code>start()</code> was called and worker thread is running</li>\n</ul>\n<p><strong>Checkpoint 2: Storage Layer</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/test_storage.py</code></li>\n<li>Verify: Metadata operations support concurrent access without data corruption</li>\n<li>Manual Test: Store and retrieve a binary artifact, confirm metadata sidecar file creation</li>\n<li>Debug: If queries fail, check table schema matches expected column types in test data</li>\n</ul>\n<p><strong>Common Integration Issues</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Events not delivered</td>\n<td>Event coordinator not started</td>\n<td>Check <code>_running</code> flag and worker thread status</td>\n<td>Call <code>event_coordinator.start()</code> before publishing</td>\n</tr>\n<tr>\n<td>Storage queries timeout</td>\n<td>Database connection pool exhaustion</td>\n<td>Monitor open connection count</td>\n<td>Implement connection pooling with max limits</td>\n</tr>\n<tr>\n<td>Artifact uploads fail</td>\n<td>Insufficient filesystem permissions</td>\n<td>Check directory write permissions</td>\n<td>Ensure artifact storage directory has write access</td>\n</tr>\n<tr>\n<td>Component health unknown</td>\n<td>Health checks not registered</td>\n<td>Verify <code>add_check()</code> calls in component initialization</td>\n<td>Add health checks in component <code>__init__</code> method</td>\n</tr>\n</tbody></table>\n<p>This foundational infrastructure provides the building blocks for implementing the experiment tracking, model registry, pipeline orchestration, deployment, and monitoring components. Each component will extend these base patterns while adding domain-specific functionality for their particular aspect of the ML lifecycle.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This foundational section defines the scope and success criteria for all milestones (1-5) by establishing clear boundaries and requirements that guide implementation decisions across the entire platform.</p>\n</blockquote>\n<p>Building an MLOps platform is like designing a city&#39;s infrastructure - you need clear zoning laws, building codes, and service level agreements before breaking ground. Without explicit goals and boundaries, feature creep transforms a focused platform into an unwieldy monolith that serves no one well. This section establishes the platform&#39;s charter: what we commit to building, how we measure success, and equally important, what we explicitly won&#39;t attempt.</p>\n<p>The challenge with MLOps platforms lies in their inherent complexity - they sit at the intersection of data engineering, machine learning, software engineering, and DevOps. Each discipline brings its own requirements, tools, and expectations. A data scientist wants seamless experiment tracking that doesn&#39;t interrupt their research flow. An ML engineer needs reliable pipeline orchestration that handles distributed training workloads. A production engineer demands robust model serving with sub-100ms latency guarantees. Platform engineers require clear APIs, comprehensive monitoring, and straightforward operational procedures.</p>\n<p>Our goal setting process must balance these competing demands while maintaining architectural coherence. We&#39;ll define functional requirements that capture what the platform must accomplish, quality attributes that specify how well it must perform, and explicit non-goals that prevent scope expansion into adjacent problem domains.</p>\n<h3 id=\"functional-requirements\">Functional Requirements</h3>\n<p>Think of functional requirements as the platform&#39;s job description - the specific tasks it must complete successfully across the machine learning lifecycle. Each requirement maps to one or more platform components and defines measurable acceptance criteria.</p>\n<p><strong>Experiment Tracking Capabilities</strong></p>\n<p>The platform must provide comprehensive experiment tracking that captures the full context of machine learning experiments. This goes beyond simple logging - we&#39;re building a research laboratory information management system that maintains scientific rigor while supporting rapid experimentation.</p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Component</th>\n<th>Acceptance Criteria</th>\n<th>Business Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parameter Logging</td>\n<td>Experiment Tracking</td>\n<td>Record hyperparameter key-value pairs with automatic type inference and validation</td>\n<td>Enables systematic hyperparameter optimization and reproducible research</td>\n</tr>\n<tr>\n<td>Metric Tracking</td>\n<td>Experiment Tracking</td>\n<td>Store time-series metrics with step numbers, timestamps, and statistical aggregations</td>\n<td>Supports learning curve analysis and model performance comparison</td>\n</tr>\n<tr>\n<td>Artifact Management</td>\n<td>Experiment Tracking</td>\n<td>Version and store binary artifacts (models, plots, datasets) with content hashing</td>\n<td>Ensures experiment reproducibility and enables artifact reuse</td>\n</tr>\n<tr>\n<td>Run Comparison</td>\n<td>Experiment Tracking</td>\n<td>Side-by-side comparison of parameters, metrics, and artifacts across multiple runs</td>\n<td>Accelerates model development through systematic analysis</td>\n</tr>\n<tr>\n<td>Experiment Organization</td>\n<td>Experiment Tracking</td>\n<td>Hierarchical grouping of related runs with tagging and search capabilities</td>\n<td>Improves research organization and knowledge sharing</td>\n</tr>\n<tr>\n<td>Lineage Tracking</td>\n<td>Experiment Tracking</td>\n<td>Link experiments to source code commits, data versions, and environmental metadata</td>\n<td>Enables complete experiment reproduction and debugging</td>\n</tr>\n</tbody></table>\n<p>The experiment tracking system must handle the chaotic nature of ML research while maintaining data integrity. Data scientists often run dozens of experiments daily, each generating megabytes of artifacts and thousands of metric data points. The system must capture this information automatically without disrupting the research workflow.</p>\n<p><strong>Model Registry and Versioning</strong></p>\n<p>Model registry requirements center on treating trained models as first-class software artifacts with proper versioning, lifecycle management, and governance controls. This is similar to how Docker Hub manages container images, but with ML-specific metadata and approval workflows.</p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Component</th>\n<th>Acceptance Criteria</th>\n<th>Business Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model Registration</td>\n<td>Model Registry</td>\n<td>Register models with semantic versioning, accuracy metrics, and source experiment linkage</td>\n<td>Creates authoritative model catalog for organizational knowledge management</td>\n</tr>\n<tr>\n<td>Stage Management</td>\n<td>Model Registry</td>\n<td>Automated promotion through Development, Staging, Production, and Archived stages</td>\n<td>Enforces quality gates and reduces production deployment risk</td>\n</tr>\n<tr>\n<td>Approval Workflows</td>\n<td>Model Registry</td>\n<td>Configurable approval processes with role-based permissions for stage transitions</td>\n<td>Ensures model quality and maintains compliance audit trails</td>\n</tr>\n<tr>\n<td>Model Lineage</td>\n<td>Model Registry</td>\n<td>Trace models to training data versions, code commits, and experiment runs</td>\n<td>Enables impact analysis and supports regulatory compliance</td>\n</tr>\n<tr>\n<td>Metadata Search</td>\n<td>Model Registry</td>\n<td>Query models by name, version, stage, metrics, tags, and custom attributes</td>\n<td>Facilitates model discovery and reuse across teams</td>\n</tr>\n<tr>\n<td>Immutable Storage</td>\n<td>Model Registry</td>\n<td>Content-addressable storage with integrity validation and audit logging</td>\n<td>Guarantees model reproducibility and prevents unauthorized modifications</td>\n</tr>\n</tbody></table>\n<p>The model registry serves as the authoritative source of truth for all organizational models. It must enforce governance policies while remaining flexible enough to support diverse ML frameworks and deployment patterns. The approval workflow system needs to be configurable - some organizations require manual sign-offs for production models, while others prefer automated promotion based on performance thresholds.</p>\n<p><strong>Training Pipeline Orchestration</strong></p>\n<p>Pipeline orchestration requirements focus on coordinating complex, multi-step training workflows that span from data preparation through model evaluation. This is analogous to manufacturing process control - we need precise coordination, resource management, and quality checkpoints throughout the production line.</p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Component</th>\n<th>Training Pipeline</th>\n<th>Acceptance Criteria</th>\n<th>Business Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DAG Definition</td>\n<td>Training Pipeline</td>\n<td>Declarative pipeline specification with step dependencies and conditional execution</td>\n<td>Enables complex workflow automation and reduces manual coordination overhead</td>\n<td></td>\n</tr>\n<tr>\n<td>Resource Management</td>\n<td>Training Pipeline</td>\n<td>Dynamic allocation of CPU, memory, GPU resources with queue management</td>\n<td>Optimizes infrastructure utilization and reduces training costs</td>\n<td></td>\n</tr>\n<tr>\n<td>Distributed Training</td>\n<td>Training Pipeline</td>\n<td>Support for multi-GPU and multi-node training with parameter server coordination</td>\n<td>Enables large-scale model training that exceeds single-machine capabilities</td>\n<td></td>\n</tr>\n<tr>\n<td>Fault Tolerance</td>\n<td>Training Pipeline</td>\n<td>Automatic retry, checkpoint recovery, and partial failure handling</td>\n<td>Reduces pipeline maintenance overhead and improves training reliability</td>\n<td></td>\n</tr>\n<tr>\n<td>Data Validation</td>\n<td>Training Pipeline</td>\n<td>Schema validation, statistical profiling, and drift detection at pipeline ingestion</td>\n<td>Prevents silent training failures caused by data quality issues</td>\n<td></td>\n</tr>\n<tr>\n<td>Parallel Execution</td>\n<td>Training Pipeline</td>\n<td>Concurrent execution of independent pipeline steps with dependency tracking</td>\n<td>Reduces total pipeline runtime and improves resource efficiency</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>Training pipelines must handle the unique challenges of ML workloads - long-running processes, expensive compute resources, and complex data dependencies. The orchestration engine needs to be sophisticated enough to handle distributed training coordination while remaining simple enough for data scientists to define their workflows declaratively.</p>\n<p><strong>Model Deployment and Serving</strong></p>\n<p>Deployment requirements center on transforming trained models into production-ready services that meet enterprise reliability and performance standards. This involves more than simple model hosting - we&#39;re building a complete model serving infrastructure with traffic management, performance optimization, and operational controls.</p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Component</th>\n<th>Acceptance Criteria</th>\n<th>Business Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP API Generation</td>\n<td>Model Deployment</td>\n<td>Automatic REST endpoint creation with OpenAPI specifications and request validation</td>\n<td>Standardizes model serving interfaces and reduces integration complexity</td>\n</tr>\n<tr>\n<td>Auto-scaling</td>\n<td>Model Deployment</td>\n<td>Dynamic replica scaling based on request rate, latency percentiles, and resource utilization</td>\n<td>Maintains performance SLAs while optimizing infrastructure costs</td>\n</tr>\n<tr>\n<td>Canary Deployments</td>\n<td>Model Deployment</td>\n<td>Gradual traffic shifting with configurable rollout rates and automatic rollback triggers</td>\n<td>Reduces deployment risk and enables safe production updates</td>\n</tr>\n<tr>\n<td>A/B Testing</td>\n<td>Model Deployment</td>\n<td>Traffic splitting between model versions with statistical significance testing</td>\n<td>Enables data-driven model selection and performance validation</td>\n</tr>\n<tr>\n<td>Performance Optimization</td>\n<td>Model Deployment</td>\n<td>Model compilation, batching, caching, and hardware acceleration integration</td>\n<td>Achieves production latency requirements and maximizes throughput</td>\n</tr>\n<tr>\n<td>Multi-framework Support</td>\n<td>Model Deployment</td>\n<td>Integration with TensorFlow Serving, TorchServe, Triton, and custom serving containers</td>\n<td>Supports diverse ML frameworks and deployment patterns</td>\n</tr>\n</tbody></table>\n<p>Model deployment must bridge the gap between research models and production requirements. Research models often prioritize accuracy over inference speed, use frameworks optimized for experimentation rather than serving, and lack the error handling needed for production traffic. The deployment system must handle these transformations automatically while maintaining the model&#39;s predictive behavior.</p>\n<p><strong>Production Model Monitoring</strong></p>\n<p>Monitoring requirements focus on maintaining model performance and detecting degradation in production environments. This goes beyond traditional application monitoring to include ML-specific concerns like data drift, concept drift, and prediction quality assessment.</p>\n<table>\n<thead>\n<tr>\n<th>Requirement</th>\n<th>Component</th>\n<th>Acceptance Criteria</th>\n<th>Business Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Prediction Logging</td>\n<td>Model Monitoring</td>\n<td>Capture all inference requests and responses with configurable sampling rates</td>\n<td>Enables model performance analysis and debugging of production issues</td>\n</tr>\n<tr>\n<td>Performance Metrics</td>\n<td>Model Monitoring</td>\n<td>Track latency percentiles, throughput, error rates, and resource utilization</td>\n<td>Ensures SLA compliance and identifies performance bottlenecks</td>\n</tr>\n<tr>\n<td>Data Drift Detection</td>\n<td>Model Monitoring</td>\n<td>Statistical comparison of live and training feature distributions with alert thresholds</td>\n<td>Detects when model inputs change, indicating potential performance degradation</td>\n</tr>\n<tr>\n<td>Model Drift Monitoring</td>\n<td>Model Monitoring</td>\n<td>Track prediction distribution changes and accuracy degradation over time</td>\n<td>Identifies when models need retraining due to concept drift</td>\n</tr>\n<tr>\n<td>Alerting System</td>\n<td>Model Monitoring</td>\n<td>Configurable alerts based on performance metrics, drift scores, and business KPIs</td>\n<td>Enables proactive response to model degradation before business impact</td>\n</tr>\n<tr>\n<td>Dashboard Visualization</td>\n<td>Model Monitoring</td>\n<td>Real-time dashboards showing model health, prediction trends, and drift analysis</td>\n<td>Provides operational visibility and supports data-driven model management decisions</td>\n</tr>\n</tbody></table>\n<p>Production monitoring must detect subtle changes in model behavior that could indicate performance degradation. Unlike traditional software, ML models can silently degrade as the world changes around them. The monitoring system must be sensitive enough to detect these changes early while avoiding false alarms that lead to alert fatigue.</p>\n<h3 id=\"quality-attributes\">Quality Attributes</h3>\n<p>Quality attributes define how well the platform must perform its functional requirements. These non-functional requirements often determine platform adoption success more than feature completeness. Think of these as the platform&#39;s service level agreements with its users.</p>\n<p><strong>Performance Requirements</strong></p>\n<p>Performance requirements must account for the diverse workload patterns across the ML lifecycle. Experiment tracking deals with burst writes during model training, the model registry handles occasional large model uploads, pipelines require sustained compute orchestration, deployment demands low-latency serving, and monitoring processes continuous high-throughput logging.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Metric</th>\n<th>Target</th>\n<th>Measurement Method</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Tracking</td>\n<td>Metric logging latency</td>\n<td>&lt; 10ms p95</td>\n<td>Client-side timing instrumentation</td>\n<td>Must not interrupt training loops with slow logging calls</td>\n</tr>\n<tr>\n<td>Experiment Tracking</td>\n<td>Artifact upload throughput</td>\n<td>&gt; 100 MB/s per client</td>\n<td>Server-side transfer rate monitoring</td>\n<td>Large model artifacts require fast upload to maintain researcher productivity</td>\n</tr>\n<tr>\n<td>Model Registry</td>\n<td>Model download latency</td>\n<td>&lt; 500ms for models up to 1GB</td>\n<td>End-to-end deployment timing</td>\n<td>Deployment pipelines need fast model retrieval to minimize downtime</td>\n</tr>\n<tr>\n<td>Training Pipeline</td>\n<td>Step scheduling latency</td>\n<td>&lt; 30 seconds from trigger to execution</td>\n<td>Orchestrator internal metrics</td>\n<td>Quick pipeline response maintains development velocity</td>\n</tr>\n<tr>\n<td>Model Deployment</td>\n<td>Inference latency</td>\n<td>&lt; 100ms p99 including model execution</td>\n<td>Request timing at load balancer</td>\n<td>Production SLAs typically require sub-second response times</td>\n</tr>\n<tr>\n<td>Model Monitoring</td>\n<td>Log processing delay</td>\n<td>&lt; 5 minutes from request to dashboard</td>\n<td>Event timestamp comparison</td>\n<td>Timely monitoring enables rapid response to production issues</td>\n</tr>\n</tbody></table>\n<p>Performance targets must be realistic given the underlying infrastructure constraints while aggressive enough to support productive ML workflows. These targets assume reasonable hardware - cloud instances with NVMe storage, gigabit networking, and modern CPUs. The platform should gracefully degrade performance rather than failing completely when targets cannot be met.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Performance requirements for MLOps platforms differ fundamentally from traditional web applications. Experiment tracking has burst write patterns during training with long idle periods. Model deployment requires cold start optimization for auto-scaling. Monitoring systems must handle high-cardinality metrics from diverse model types. The platform must be architected to handle these unique performance characteristics.</p>\n</blockquote>\n<p><strong>Scalability Requirements</strong></p>\n<p>Scalability requirements must accommodate organizational growth patterns - more data scientists, larger datasets, increased model complexity, and higher production traffic. The platform should scale elastically with usage rather than requiring manual capacity planning.</p>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th>Current Target</th>\n<th>Growth Target</th>\n<th>Scaling Strategy</th>\n<th>Bottleneck Mitigation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent experiments</td>\n<td>50 simultaneous runs</td>\n<td>500 simultaneous runs</td>\n<td>Horizontal scaling of tracking workers</td>\n<td>Partition experiment data by user/team</td>\n</tr>\n<tr>\n<td>Model registry size</td>\n<td>10,000 models</td>\n<td>100,000 models</td>\n<td>Distributed storage with metadata sharding</td>\n<td>Content-addressable storage with deduplication</td>\n</tr>\n<tr>\n<td>Pipeline complexity</td>\n<td>50 steps per pipeline</td>\n<td>500 steps per pipeline</td>\n<td>Distributed DAG execution</td>\n<td>Step-level parallelization and resource isolation</td>\n</tr>\n<tr>\n<td>Serving throughput</td>\n<td>10,000 requests/second</td>\n<td>100,000 requests/second</td>\n<td>Auto-scaling with multi-region deployment</td>\n<td>Request batching and model compilation optimization</td>\n</tr>\n<tr>\n<td>Monitoring data volume</td>\n<td>1TB/day prediction logs</td>\n<td>10TB/day prediction logs</td>\n<td>Stream processing with data tiering</td>\n<td>Configurable retention and sampling policies</td>\n</tr>\n</tbody></table>\n<p>The platform must scale both vertically (handling larger individual workloads) and horizontally (serving more concurrent users). Vertical scaling supports larger models, longer training runs, and more complex pipelines. Horizontal scaling supports organizational growth and increased platform adoption.</p>\n<p><strong>Reliability Requirements</strong></p>\n<p>Reliability requirements ensure the platform remains available and consistent despite infrastructure failures, human errors, and unexpected load patterns. ML workflows often run for hours or days, making fault tolerance critical for productivity.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Availability Target</th>\n<th>Recovery Time</th>\n<th>Data Durability</th>\n<th>Consistency Model</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Tracking</td>\n<td>99.9% (8.7 hours downtime/year)</td>\n<td>&lt; 5 minutes</td>\n<td>99.999999999% (11 9&#39;s)</td>\n<td>Eventually consistent with conflict resolution</td>\n</tr>\n<tr>\n<td>Model Registry</td>\n<td>99.95% (4.4 hours downtime/year)</td>\n<td>&lt; 2 minutes</td>\n<td>99.999999999% (11 9&#39;s)</td>\n<td>Strong consistency for model versions</td>\n</tr>\n<tr>\n<td>Training Pipeline</td>\n<td>99.5% (43.8 hours downtime/year)</td>\n<td>&lt; 10 minutes</td>\n<td>99.9999999% (9 9&#39;s)</td>\n<td>Eventual consistency with checkpointing</td>\n</tr>\n<tr>\n<td>Model Deployment</td>\n<td>99.99% (52.6 minutes downtime/year)</td>\n<td>&lt; 1 minute</td>\n<td>N/A (stateless)</td>\n<td>Strong consistency for routing rules</td>\n</tr>\n<tr>\n<td>Model Monitoring</td>\n<td>99.9% (8.7 hours downtime/year)</td>\n<td>&lt; 5 minutes</td>\n<td>99.99999% (7 9&#39;s)</td>\n<td>Eventually consistent with time-series ordering</td>\n</tr>\n</tbody></table>\n<p>Different components have varying reliability requirements based on their impact on business operations. Model deployment requires the highest availability since it serves production traffic. Training pipelines can tolerate more downtime since they represent internal workflows, but they need strong checkpoint recovery to avoid losing hours of computation.</p>\n<blockquote>\n<p><strong>Critical Consideration</strong>: Reliability in MLOps platforms must account for long-running operations. A traditional web application can retry failed requests, but a training pipeline failure after 6 hours of computation represents significant lost work and compute costs. The platform must provide checkpoint recovery and partial failure handling to maintain productivity despite infrastructure instability.</p>\n</blockquote>\n<p><strong>Security and Compliance Requirements</strong></p>\n<p>Security requirements must address the sensitive nature of ML assets - proprietary models, confidential training data, and competitive intelligence embedded in experiment results. The platform handles intellectual property that could provide significant business advantage to competitors.</p>\n<table>\n<thead>\n<tr>\n<th>Security Domain</th>\n<th>Requirement</th>\n<th>Implementation Approach</th>\n<th>Compliance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Authentication</td>\n<td>Multi-factor authentication for all users</td>\n<td>Integration with enterprise identity providers (OIDC/SAML)</td>\n<td>Supports SOX, PCI-DSS access controls</td>\n</tr>\n<tr>\n<td>Authorization</td>\n<td>Role-based access control with fine-grained permissions</td>\n<td>Resource-level permissions with inheritance and delegation</td>\n<td>Enables GDPR data controller compliance</td>\n</tr>\n<tr>\n<td>Data Encryption</td>\n<td>Encryption at rest and in transit for all artifacts</td>\n<td>AES-256 for storage, TLS 1.3 for transport</td>\n<td>Meets HIPAA encryption requirements</td>\n</tr>\n<tr>\n<td>Audit Logging</td>\n<td>Complete audit trail for all platform operations</td>\n<td>Immutable audit logs with digital signatures</td>\n<td>Supports regulatory compliance reporting</td>\n</tr>\n<tr>\n<td>Model Protection</td>\n<td>Signed models with integrity validation</td>\n<td>Digital signatures and content hashing</td>\n<td>Prevents model tampering and IP theft</td>\n</tr>\n<tr>\n<td>Network Security</td>\n<td>Network segmentation and firewall controls</td>\n<td>VPC isolation with controlled ingress/egress</td>\n<td>Reduces attack surface and data exfiltration risk</td>\n</tr>\n</tbody></table>\n<p>Security must be built into the platform architecture rather than added as an afterthought. ML assets are particularly vulnerable because they&#39;re often accessed by automated systems, stored in object storage, and transmitted between distributed components. The platform must maintain security without impeding ML workflows.</p>\n<h3 id=\"what-we-won39t-build\">What We Won&#39;t Build</h3>\n<p>Explicit non-goals are critical for maintaining platform focus and avoiding feature creep. By clearly stating what we won&#39;t build, we establish boundaries that prevent the platform from expanding into adjacent problem domains where we lack expertise or resources.</p>\n<p><strong>Data Platform Capabilities</strong></p>\n<p>We will not build a comprehensive data platform or attempt to replace existing data infrastructure. The MLOps platform assumes that data engineering teams have already solved data ingestion, transformation, and quality problems using specialized tools.</p>\n<table>\n<thead>\n<tr>\n<th>Excluded Capability</th>\n<th>Rationale</th>\n<th>Alternative Approach</th>\n<th>Integration Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Lake Management</td>\n<td>Specialized tools like Apache Iceberg, Delta Lake handle this better</td>\n<td>Integrate with existing data lakes via standard APIs</td>\n<td>Read data via S3, HDFS, or database connectors</td>\n</tr>\n<tr>\n<td>ETL Pipeline Orchestration</td>\n<td>Airflow, Prefect, and similar tools are purpose-built for data workflows</td>\n<td>Support triggering ML pipelines from data pipeline completion</td>\n<td>Event-driven integration via webhooks or message queues</td>\n</tr>\n<tr>\n<td>Data Quality Monitoring</td>\n<td>Tools like Great Expectations, Monte Carlo specialize in data observability</td>\n<td>Consume data quality metrics from external systems</td>\n<td>Import data quality scores as pipeline validation inputs</td>\n</tr>\n<tr>\n<td>Stream Processing</td>\n<td>Apache Kafka, Apache Flink handle real-time data processing more effectively</td>\n<td>Connect to streaming platforms for real-time inference</td>\n<td>Subscribe to Kafka topics for live prediction requests</td>\n</tr>\n<tr>\n<td>Data Catalog Management</td>\n<td>Apache Atlas, DataHub provide comprehensive metadata management</td>\n<td>Integrate with existing catalogs for dataset discovery</td>\n<td>Query catalogs to validate training data lineage</td>\n</tr>\n</tbody></table>\n<p>Attempting to build data platform capabilities would create a massive, unfocused system that competes poorly with specialized tools. Instead, we&#39;ll design clean integration points that allow the MLOps platform to consume data from best-of-breed data infrastructure.</p>\n<p><strong>MLOps-Adjacent Development Tools</strong></p>\n<p>We will not build general-purpose development tools or attempt to replace the existing ML development ecosystem. Data scientists and ML engineers already have strong preferences for IDEs, notebooks, and experimentation environments.</p>\n<table>\n<thead>\n<tr>\n<th>Excluded Capability</th>\n<th>Rationale</th>\n<th>Alternative Approach</th>\n<th>Integration Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Notebook Environment</td>\n<td>JupyterLab, Google Colab, and Databricks provide superior notebook experiences</td>\n<td>Support any notebook environment via SDK integration</td>\n<td>Provide Python/R libraries for experiment tracking from notebooks</td>\n</tr>\n<tr>\n<td>IDE Integration</td>\n<td>VS Code, PyCharm, and specialized ML IDEs serve developer needs better</td>\n<td>Build plugins for popular IDEs</td>\n<td>Offer language server protocol support for autocomplete</td>\n</tr>\n<tr>\n<td>Code Version Control</td>\n<td>Git and platforms like GitHub/GitLab are the universal standard</td>\n<td>Integrate with existing version control systems</td>\n<td>Link experiments to Git commits automatically</td>\n</tr>\n<tr>\n<td>Collaborative Development</td>\n<td>GitHub, GitLab provide comprehensive collaboration features</td>\n<td>Leverage existing development platforms</td>\n<td>Import repository metadata and link to model lineage</td>\n</tr>\n<tr>\n<td>Code Quality Tools</td>\n<td>Linting, testing, and security scanning have mature specialized solutions</td>\n<td>Integrate quality gates into training pipelines</td>\n<td>Run existing tools as pipeline steps with result validation</td>\n</tr>\n</tbody></table>\n<p>Building development tools would distract from MLOps-specific problems while creating inferior alternatives to mature, widely-adopted solutions. The platform should integrate seamlessly with existing development workflows rather than replacing them.</p>\n<p><strong>Infrastructure and Platform Services</strong></p>\n<p>We will not build low-level infrastructure services or attempt to compete with cloud platforms and container orchestration systems. These are complex, specialized domains with mature solutions.</p>\n<table>\n<thead>\n<tr>\n<th>Excluded Capability</th>\n<th>Rationale</th>\n<th>Alternative Approach</th>\n<th>Integration Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Container Orchestration</td>\n<td>Kubernetes is the standard, with cloud-managed alternatives</td>\n<td>Deploy platform components on Kubernetes</td>\n<td>Use Kubernetes APIs for pipeline resource management</td>\n</tr>\n<tr>\n<td>Object Storage</td>\n<td>S3, GCS, and Azure Blob provide scalable, reliable storage</td>\n<td>Integrate with cloud object storage services</td>\n<td>Abstract storage behind pluggable interface</td>\n</tr>\n<tr>\n<td>Compute Provisioning</td>\n<td>Cloud auto-scaling groups and spot instances optimize cost and availability</td>\n<td>Leverage cloud-native compute management</td>\n<td>Request compute resources via cloud APIs</td>\n</tr>\n<tr>\n<td>Network Load Balancing</td>\n<td>Cloud load balancers provide global distribution and health checking</td>\n<td>Use existing load balancing infrastructure</td>\n<td>Configure health checks and traffic routing rules</td>\n</tr>\n<tr>\n<td>Monitoring Infrastructure</td>\n<td>Prometheus, DataDog, and cloud monitoring provide comprehensive observability</td>\n<td>Export metrics to existing monitoring systems</td>\n<td>Publish platform metrics in standard formats</td>\n</tr>\n<tr>\n<td>Secrets Management</td>\n<td>HashiCorp Vault, cloud KMS, and similar tools specialize in secret handling</td>\n<td>Integrate with existing secret management systems</td>\n<td>Retrieve API keys and certificates from secret stores</td>\n</tr>\n</tbody></table>\n<p>Infrastructure services require deep expertise in distributed systems, security, and operational procedures. Building these services would create significant maintenance overhead while duplicating capabilities that cloud providers deliver more reliably and cost-effectively.</p>\n<p><strong>Advanced ML Capabilities</strong></p>\n<p>We will not build advanced ML research capabilities or compete with specialized ML frameworks and libraries. Our focus is on operationalizing models built with existing ML tools.</p>\n<table>\n<thead>\n<tr>\n<th>Excluded Capability</th>\n<th>Rationale</th>\n<th>Alternative Approach</th>\n<th>Integration Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>AutoML Algorithms</td>\n<td>Tools like H2O.ai, AutoML tables provide sophisticated automated modeling</td>\n<td>Support AutoML tools through standard APIs</td>\n<td>Track AutoML experiments and deploy generated models</td>\n</tr>\n<tr>\n<td>Neural Architecture Search</td>\n<td>Research-focused tools and cloud services handle architecture optimization</td>\n<td>Integrate NAS results through experiment tracking</td>\n<td>Log architecture search results as model metadata</td>\n</tr>\n<tr>\n<td>Hyperparameter Optimization</td>\n<td>Optuna, Ray Tune, and Hyperopt provide advanced optimization algorithms</td>\n<td>Integrate HPO tools with experiment tracking</td>\n<td>Log optimization trials and visualize search spaces</td>\n</tr>\n<tr>\n<td>Model Interpretation</td>\n<td>LIME, SHAP, and specialized explainability tools provide superior analysis</td>\n<td>Store interpretation artifacts in experiment tracking</td>\n<td>Version explanation models alongside prediction models</td>\n</tr>\n<tr>\n<td>Feature Engineering</td>\n<td>Tools like Feast, Tecton specialize in feature store management</td>\n<td>Integrate with existing feature stores</td>\n<td>Track feature versions used in model training</td>\n</tr>\n<tr>\n<td>Federated Learning</td>\n<td>Specialized frameworks handle the complex coordination required</td>\n<td>Support federated learning outputs through standard interfaces</td>\n<td>Track federated models and distributed experiment metadata</td>\n</tr>\n</tbody></table>\n<p>Advanced ML capabilities require deep research expertise and constantly evolving algorithms. The platform should provide a stable foundation that supports innovation in these areas rather than attempting to implement cutting-edge algorithms internally.</p>\n<blockquote>\n<p><strong>Boundary Principle</strong>: The MLOps platform succeeds by excelling at the operational aspects of machine learning - tracking, versioning, orchestration, deployment, and monitoring. By integrating with best-of-breed tools in adjacent domains rather than replacing them, we create a focused, maintainable system that adds clear value to existing ML workflows.</p>\n</blockquote>\n<p>These non-goals ensure the platform remains focused on its core mission while providing clear integration strategies for adjacent capabilities. The boundaries may evolve over time based on user needs and market conditions, but they provide essential guidance for current development priorities.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The goals and non-goals established in this section translate into specific technology choices and architectural constraints that guide implementation across all platform components. This guidance helps developers make consistent decisions that align with our quality attributes and functional requirements.</p>\n<p><strong>Technology Recommendations Table</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metadata Storage</td>\n<td>PostgreSQL with JSONB columns</td>\n<td>Apache Cassandra with distributed architecture</td>\n<td>PostgreSQL sufficient for most organizations; Cassandra needed for extreme scale</td>\n</tr>\n<tr>\n<td>Artifact Storage</td>\n<td>MinIO (S3-compatible) on local storage</td>\n<td>Cloud object storage (S3, GCS, Azure Blob)</td>\n<td>Local MinIO for development; cloud storage for production reliability</td>\n</tr>\n<tr>\n<td>Message Queue</td>\n<td>Redis Pub/Sub with persistence</td>\n<td>Apache Kafka with topic partitioning</td>\n<td>Redis simpler for basic event coordination; Kafka for high-throughput event streaming</td>\n</tr>\n<tr>\n<td>Metrics Storage</td>\n<td>Prometheus with local storage</td>\n<td>InfluxDB with clustering and retention policies</td>\n<td>Prometheus integrated with Kubernetes; InfluxDB better for time-series analytics</td>\n</tr>\n<tr>\n<td>Container Orchestration</td>\n<td>Docker Compose for development</td>\n<td>Kubernetes with Helm charts</td>\n<td>Compose for local testing; Kubernetes required for production scalability</td>\n</tr>\n<tr>\n<td>API Framework</td>\n<td>FastAPI with automatic OpenAPI generation</td>\n<td>FastAPI with custom middleware and async workers</td>\n<td>FastAPI provides excellent performance and documentation for both scenarios</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Project Structure</strong></p>\n<p>The platform should be structured as a modular monorepo that can evolve into microservices as scale requirements demand. This structure supports the clear component boundaries established in our goals while maintaining development simplicity.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>mlops-platform/\n├── cmd/                              # Entry points for each service\n│   ├── experiment-tracker/\n│   │   └── main.py                   # Experiment tracking service\n│   ├── model-registry/\n│   │   └── main.py                   # Model registry service\n│   ├── pipeline-orchestrator/\n│   │   └── main.py                   # Training pipeline service\n│   ├── model-deployment/\n│   │   └── main.py                   # Deployment management service\n│   └── monitoring/\n│       └── main.py                   # Model monitoring service\n├── internal/                         # Shared internal libraries\n│   ├── metadata/                     # MetadataStore implementations\n│   │   ├── __init__.py\n│   │   ├── postgres_store.py         # PostgreSQL implementation\n│   │   └── cassandra_store.py        # Cassandra implementation\n│   ├── artifacts/                    # ArtifactStore implementations\n│   │   ├── __init__.py\n│   │   ├── s3_store.py              # S3-compatible storage\n│   │   └── local_store.py           # Local filesystem storage\n│   ├── events/                      # Event coordination system\n│   │   ├── __init__.py\n│   │   ├── coordinator.py           # EventCoordinator implementation\n│   │   └── handlers.py              # Event handler utilities\n│   └── health/                      # Health checking framework\n│       ├── __init__.py\n│       └── checker.py               # ComponentHealth implementation\n├── pkg/                             # Public SDK and client libraries\n│   ├── python-sdk/                  # Python client for data scientists\n│   │   ├── mlops_client/\n│   │   │   ├── __init__.py\n│   │   │   ├── experiment.py        # Experiment tracking client\n│   │   │   ├── registry.py          # Model registry client\n│   │   │   └── monitoring.py        # Monitoring client\n│   │   └── setup.py\n│   └── go-sdk/                      # Go client for infrastructure integration\n│       ├── client/\n│       │   ├── experiment.go\n│       │   ├── registry.go\n│       │   └── monitoring.go\n│       └── go.mod\n├── deployments/                     # Deployment configurations\n│   ├── docker-compose.yml           # Local development environment\n│   ├── kubernetes/                  # Kubernetes manifests\n│   │   ├── namespace.yml\n│   │   ├── experiment-tracker.yml\n│   │   ├── model-registry.yml\n│   │   └── ingress.yml\n│   └── terraform/                   # Infrastructure as code\n│       ├── main.tf\n│       └── variables.tf\n├── docs/                           # Documentation and design docs\n│   ├── api/                        # API documentation\n│   └── architecture/               # Architecture decision records\n├── tests/                          # Integration and end-to-end tests\n│   ├── integration/\n│   └── e2e/\n└── scripts/                        # Development and deployment scripts\n    ├── dev-setup.sh\n    └── deploy.sh</code></pre></div>\n\n<p><strong>Infrastructure Starter Code</strong></p>\n<p>The following starter code provides complete implementations of core infrastructure components that support the functional requirements while abstracting away non-essential complexity.</p>\n<p><strong>Metadata Store Abstraction</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># internal/metadata/__init__.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetadataStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for metadata storage across all platform components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Provides consistent CRUD operations with transaction support and optimistic</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    concurrency control. Implementations must ensure ACID properties for</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    critical operations like model version promotion and experiment finalization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> create_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new table with the specified schema.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            table_name: Name of the table to create</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            schema: Column definitions as {column_name: column_type}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            TableExistsError: If table already exists</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            InvalidSchemaError: If schema contains invalid type definitions</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Insert a new record and return the generated ID.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            table_name: Target table name</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: Record data as key-value pairs</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Generated unique ID for the inserted record</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValidationError: If data doesn't match table schema</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            DuplicateKeyError: If unique constraint is violated</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, record_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    version: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update an existing record with optimistic concurrency control.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            table_name: Target table name</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            record_id: ID of record to update</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: Updated field values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            version: Expected version for optimistic locking</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            RecordNotFoundError: If record doesn't exist</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            VersionConflictError: If version doesn't match current record</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            ValidationError: If updated data violates schema constraints</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, filters: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   sort_by: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, limit: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   offset: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Query records with filtering, sorting, and pagination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            table_name: Table to query</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            filters: WHERE clause conditions as {column: value}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            sort_by: Column name for sorting (prefix with '-' for descending)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            limit: Maximum number of records to return</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            offset: Number of records to skip for pagination</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of matching records as dictionaries</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            InvalidFilterError: If filter contains invalid column names</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            QueryTimeoutError: If query exceeds configured timeout</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get_by_id</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, record_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve a single record by its ID.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            table_name: Table containing the record</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            record_id: Unique identifier of the record</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Record data as dictionary, or None if not found</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># internal/metadata/postgres_store.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncpg</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .metadata_store </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MetadataStore</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PostgreSQLMetadataStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MetadataStore</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"PostgreSQL implementation of MetadataStore using JSONB for flexible schemas.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    This implementation provides ACID guarantees and supports complex queries</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    on JSON metadata. Suitable for most MLOps workloads with moderate scale</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    requirements (&#x3C; 10TB metadata, &#x3C; 1000 concurrent connections).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, connection_string: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_string </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_string</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize the connection pool and create system tables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._pool </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> asyncpg.create_pool(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_string)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Create system tables for metadata management</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._pool.acquire() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> conn.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE TABLE IF NOT EXISTS system_tables (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    table_name VARCHAR(255) PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    schema_definition JSONB NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    created_at TIMESTAMP DEFAULT NOW(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    version INTEGER DEFAULT 1</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> create_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._pool.acquire() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Store schema definition</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> conn.execute(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"INSERT INTO system_tables (table_name, schema_definition) VALUES ($1, $2)\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                table_name, json.dumps(schema)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Create actual table with flexible JSONB storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> conn.execute(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                CREATE TABLE </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    data JSONB NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    version INTEGER DEFAULT 1,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    created_at TIMESTAMP DEFAULT NOW(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    updated_at TIMESTAMP DEFAULT NOW()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Create indexes for common query patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> conn.execute(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"CREATE INDEX idx_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_data_gin ON </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> USING GIN (data)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._pool.acquire() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate data against table schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Insert record with generated UUID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return the generated ID as string</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Hint: Use RETURNING clause to get generated ID</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            record_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> conn.fetchval(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"INSERT INTO </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> (data) VALUES ($1) RETURNING id\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                json.dumps(data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(record_id)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, record_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    version: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._pool.acquire() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If version specified, check current version matches</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update record data and increment version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update the updated_at timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Raise VersionConflictError if optimistic lock fails</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> version </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> conn.execute(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"\"\"UPDATE </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                       SET data = $1, version = version + 1, updated_at = NOW()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                       WHERE id = $2 AND version = $3\"\"\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    json.dumps(data), record_id, version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> \"UPDATE 0\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    raise</span><span style=\"color:#E1E4E8\"> VersionConflictError(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Version conflict updating </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">record_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> conn.execute(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    f</span><span style=\"color:#9ECBFF\">\"\"\"UPDATE </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                       SET data = $1, version = version + 1, updated_at = NOW()</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                       WHERE id = $2\"\"\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    json.dumps(data), record_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, filters: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   sort_by: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, limit: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   offset: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build WHERE clause from filters using JSONB operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add ORDER BY clause if sort_by specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add LIMIT and OFFSET for pagination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Execute query and return list of records</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use data->>'field' for string comparison, data->'field' for JSON comparison</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get_by_id</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, record_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._pool.acquire() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            row </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> conn.fetchrow(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"SELECT data FROM </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> WHERE id = $1\"</span><span style=\"color:#E1E4E8\">, record_id)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> json.loads(row[</span><span style=\"color:#9ECBFF\">'data'</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> row </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> VersionConflictError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when optimistic concurrency control detects a version conflict.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Artifact Storage Abstraction</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># internal/artifacts/__init__.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional, BinaryIO</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ArtifactStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for storing and retrieving ML artifacts.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports versioned storage of binary artifacts like trained models,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    datasets, plots, and configuration files. Implementations should</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    provide content addressing and deduplication for efficiency.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">, metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store binary data with optional metadata.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            key: Unique identifier for the artifact</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            data: Binary content to store</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            metadata: Optional key-value metadata</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Content hash of the stored data for integrity verification</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            StorageError: If storage operation fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            InvalidKeyError: If key format is invalid</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve binary data by key.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            key: Unique identifier of the artifact</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            Binary content, or None if not found</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            StorageError: If retrieval operation fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            CorruptionError: If stored data fails integrity check</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Delete an artifact by key.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            key: Unique identifier of the artifact to delete</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            True if artifact was deleted, False if not found</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            StorageError: If deletion operation fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> list_keys</span><span style=\"color:#E1E4E8\">(self, prefix: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, limit: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"List artifact keys with optional prefix filtering.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            prefix: Optional key prefix for filtering</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            limit: Maximum number of keys to return</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of artifact keys matching the criteria</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            StorageError: If listing operation fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># internal/artifacts/s3_store.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> aioboto3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .artifact_store </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ArtifactStore</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> S3ArtifactStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ArtifactStore</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"S3-compatible artifact storage with content addressing and metadata support.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, bucket_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 aws_access_key_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, aws_secret_access_key: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.bucket_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.endpoint_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> endpoint_url</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.aws_access_key_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> aws_access_key_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.aws_secret_access_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> aws_secret_access_key</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._session </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> initialize</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize S3 session and create bucket if it doesn't exist.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._session </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> aioboto3.Session(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            aws_access_key_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.aws_access_key_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            aws_secret_access_key</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.aws_secret_access_key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._session.client(</span><span style=\"color:#9ECBFF\">'s3'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">endpoint_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.endpoint_url) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> s3:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> s3.head_bucket(</span><span style=\"color:#FFAB70\">Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                await</span><span style=\"color:#E1E4E8\"> s3.create_bucket(</span><span style=\"color:#FFAB70\">Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">, metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Compute SHA-256 hash of data for content addressing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Prepare S3 metadata headers (prefix with 'x-amz-meta-')</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Upload data to S3 with metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return content hash for integrity verification</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        content_hash </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hashlib.sha256(data).hexdigest()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s3_metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> metadata:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            s3_metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"x-amz-meta-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">k</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(v) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> metadata.items()}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        s3_metadata[</span><span style=\"color:#9ECBFF\">'x-amz-meta-content-hash'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> content_hash</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._session.client(</span><span style=\"color:#9ECBFF\">'s3'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">endpoint_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.endpoint_url) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> s3:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> s3.put_object(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                Key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">key,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                Body</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                Metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">s3_metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> content_hash</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[</span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve object from S3</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify content hash if available in metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return binary data or None if not found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Raise CorruptionError if hash verification fails</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._session.client(</span><span style=\"color:#9ECBFF\">'s3'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">endpoint_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.endpoint_url) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> s3:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                response </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> s3.get_object(</span><span style=\"color:#FFAB70\">Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name, </span><span style=\"color:#FFAB70\">Key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                data </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> response[</span><span style=\"color:#9ECBFF\">'Body'</span><span style=\"color:#E1E4E8\">].read()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Verify integrity if hash is available</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                stored_hash </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> response.get(</span><span style=\"color:#9ECBFF\">'Metadata'</span><span style=\"color:#E1E4E8\">, {}).get(</span><span style=\"color:#9ECBFF\">'content-hash'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> stored_hash:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    computed_hash </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hashlib.sha256(data).hexdigest()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> stored_hash </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> computed_hash:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        raise</span><span style=\"color:#E1E4E8\"> CorruptionError(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Content hash mismatch for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> s3.exceptions.NoSuchKey:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Delete object from S3</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return True if deleted, False if not found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle S3 exceptions appropriately</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> list_keys</span><span style=\"color:#E1E4E8\">(self, prefix: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, limit: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: List objects with optional prefix filter</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Implement pagination if limit specified</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return list of object keys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CorruptionError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">Exception</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Raised when stored artifact fails integrity verification.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>Event Coordination System</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># internal/events/__init__.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Callable, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Event</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Immutable event representing a state change in the MLOps platform.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Events enable loose coupling between components by providing an</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    asynchronous notification mechanism for important state transitions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, timestamp: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.type </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> type</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.source </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> source</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> timestamp</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.payload </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> payload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Event'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">event_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            source</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">source,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            payload</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">payload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize event to dictionary for transmission.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'id'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'type'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.type,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'source'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.source,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'payload'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.payload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Asynchronous event distribution system for inter-component communication.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports both synchronous and asynchronous event publishing with</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    error handling and dead letter queue functionality.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._handlers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._event_log: List[Event] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> publish</span><span style=\"color:#E1E4E8\">(self, event: Event, synchronous: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Publish event to all registered handlers.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            event: Event to publish</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            synchronous: If True, wait for all handlers to complete</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Raises:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            EventHandlerError: If synchronous=True and any handler fails</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Log event to internal event store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Find all handlers registered for this event type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If synchronous, await all handlers; otherwise fire-and-forget</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle handler exceptions appropriately</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._event_log.append(event)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        handlers </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._handlers.get(event.type, [])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> synchronous:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait for all handlers to complete</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            tasks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [asyncio.create_task(handler(event)) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> handlers]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> asyncio.gather(</span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">tasks)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Fire and forget - don't wait for handler completion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> handlers:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                asyncio.create_task(handler(event))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> subscribe</span><span style=\"color:#E1E4E8\">(self, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, handler: Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register event handler for specific event type.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            event_type: Type of events to handle</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            handler: Async function to call when events of this type occur</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Add handler to handlers dictionary for this event type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize empty list if this is the first handler for this type</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> event_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._handlers:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._handlers[event_type] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._handlers[event_type].append(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_recent_events</span><span style=\"color:#E1E4E8\">(self, event_type: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> List[Event]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve recent events for debugging and monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        events </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._event_log</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> event_type:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            events </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [e </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> e </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> events </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> e.type </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> event_type]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> events[</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">limit:]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Event type constants for consistent naming</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EXPERIMENT_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"experiment.completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MODEL_PROMOTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"model.promoted\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEPLOYMENT_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"deployment.failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DRIFT_DETECTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"monitoring.drift_detected\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">PIPELINE_STARTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pipeline.started\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">PIPELINE_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pipeline.completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MODEL_DEPLOYED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"model.deployed\"</span></span></code></pre></div>\n\n<p><strong>Health Checking Framework</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># internal/health/__init__.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Callable, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Health status enumeration for component health checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Individual health check result with status and diagnostic information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, status: HealthStatus, message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                timestamp: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> status</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.message </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> message</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> timestamp</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.details </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> details</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize health check to dictionary.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'name'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.name,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'status'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.status.value,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'message'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.message,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'timestamp'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.timestamp,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'details'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.details</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Health checking framework for monitoring component status.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Supports both active health checks (periodic execution) and</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    passive health indicators (updated by component logic).</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.component_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._checks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable[[], Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._last_results: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, HealthCheck] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_check</span><span style=\"color:#E1E4E8\">(self, check_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: Callable[[], Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register periodic health check function.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            check_name: Unique name for this health check</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            check_func: Async function returning status dict with 'status', 'message', 'details'</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store check function in internal registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate check_name is unique for this component</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._checks[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_func</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> run_checks</span><span style=\"color:#E1E4E8\">(self) -> List[HealthCheck]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all registered health checks and return results.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            List of HealthCheck objects with current status</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Execute all registered check functions concurrently</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert results to HealthCheck objects with timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store results in _last_results for caching</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle check function exceptions gracefully</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        check_tasks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> check_name, check_func </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._checks.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            check_tasks[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> asyncio.create_task(check_func())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> check_name, task </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> check_tasks.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> task</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                health_check </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">check_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus(result.get(</span><span style=\"color:#9ECBFF\">'status'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">result.get(</span><span style=\"color:#9ECBFF\">'message'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">result.get(</span><span style=\"color:#9ECBFF\">'details'</span><span style=\"color:#E1E4E8\">, {})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(health_check)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._last_results[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> health_check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                error_check </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">check_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Health check failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'error'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(error_check)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._last_results[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> error_check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_overall_status</span><span style=\"color:#E1E4E8\">(self) -> HealthStatus:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute overall component health from individual checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._last_results:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNKNOWN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        statuses </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [check.status </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> check </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._last_results.values()]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(s </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> s </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> statuses):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(s </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> s </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> statuses):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(s </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> s </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> statuses):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNKNOWN</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint</strong></p>\n<p>After implementing the infrastructure components provided above, verify the following behavior:</p>\n<ol>\n<li><strong>Metadata Store Verification</strong>: Create a test table, insert records, query with filters, and verify optimistic locking behavior</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Test command</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   python </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">m pytest tests</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">infrastructure</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">test_metadata_store.py </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Expected output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_create_table_success ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_insert_and_query ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_optimistic_locking ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_version_conflict_detection ✓</span></span></code></pre></div>\n\n<ol start=\"2\">\n<li><strong>Artifact Store Verification</strong>: Upload binary data, retrieve it, verify content integrity, and test listing operations</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Test command  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   python </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">m pytest tests</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">infrastructure</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">test_artifact_store.py </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Expected output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_put_and_get_artifact ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_content_integrity_verification ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_list_with_prefix_filter ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_delete_artifact ✓</span></span></code></pre></div>\n\n<ol start=\"3\">\n<li><strong>Event Coordination Verification</strong>: Publish events, verify handler execution, and test both synchronous and asynchronous modes</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Test command</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   python </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">m pytest tests</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">infrastructure</span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\">test_event_coordinator.py </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">v</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">   # Expected output</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_synchronous_event_publishing ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_asynchronous_event_publishing ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_multiple_handlers_per_event_type ✓</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   test_event_history_retention ✓</span></span></code></pre></div>\n\n<p><strong>Language-Specific Development Tips</strong></p>\n<ul>\n<li><strong>Database Connections</strong>: Use connection pooling (asyncpg.create_pool) to handle concurrent requests efficiently without exhausting database connections</li>\n<li><strong>S3 Integration</strong>: The aioboto3 library provides async S3 operations; always use context managers to ensure proper resource cleanup</li>\n<li><strong>Error Handling</strong>: Create custom exception classes for domain-specific errors (VersionConflictError, CorruptionError) rather than using generic exceptions</li>\n<li><strong>JSON Serialization</strong>: PostgreSQL JSONB provides better query performance than JSON for metadata storage; use appropriate operators (-&gt;&gt;, -&gt;) for different query types</li>\n<li><strong>Async Patterns</strong>: Use asyncio.gather() for concurrent operations, asyncio.create_task() for fire-and-forget operations, and proper exception handling in async contexts</li>\n<li><strong>Type Hints</strong>: Use typing.Optional, typing.Dict, and typing.List for better IDE support and runtime validation</li>\n<li><strong>Configuration Management</strong>: Use environment variables or configuration files for connection strings and API keys; never hard-code credentials</li>\n</ul>\n<p>These infrastructure components provide the foundation for implementing the specific MLOps components defined in our functional requirements. Each component builds on these abstractions to provide experiment tracking, model registry, pipeline orchestration, model deployment, and monitoring capabilities.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the foundational architecture that enables all milestones (1-5) by defining how experiment tracking, model registry, pipeline orchestration, model deployment, and monitoring components work together as a cohesive system.</p>\n</blockquote>\n<p>The MLOps platform architecture follows a <strong>microservices approach</strong> where each major component operates as an independent service with well-defined responsibilities and interfaces. Think of this architecture like a <strong>modern hospital system</strong> where different departments (emergency, surgery, radiology, pharmacy) each have specialized functions but coordinate seamlessly through shared patient records, standardized protocols, and real-time communication systems. Just as a patient&#39;s journey through the hospital involves multiple departments working together while maintaining their own expertise and tools, an ML model&#39;s lifecycle involves multiple specialized components that must coordinate while maintaining their distinct responsibilities.</p>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"MLOps Platform System Architecture\"></p>\n<p>The architecture centers around five core components that communicate through standardized APIs and event-driven coordination. Each component manages its own data stores optimized for its specific access patterns, while shared infrastructure services provide cross-cutting concerns like authentication, monitoring, and configuration management. The design emphasizes <strong>loose coupling</strong> between components to enable independent scaling, deployment, and evolution while ensuring strong data consistency and lineage tracking across the entire ML lifecycle.</p>\n<h3 id=\"component-responsibilities\">Component Responsibilities</h3>\n<p>Each component in the MLOps platform has clearly defined responsibilities and maintains specific types of state. The boundaries between components follow the principle of <strong>domain-driven design</strong>, where each component encapsulates a distinct area of MLOps functionality with minimal overlap. This separation enables teams to work independently on different aspects of the platform while ensuring that integration points remain stable and well-defined.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Primary Responsibility</th>\n<th>Data Ownership</th>\n<th>Key Interfaces</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Tracking</td>\n<td>Log and organize ML training runs with parameters, metrics, and artifacts</td>\n<td>Experiment metadata, run parameters, time-series metrics, artifact references</td>\n<td>Parameter logging, metric logging, artifact storage, run comparison</td>\n</tr>\n<tr>\n<td>Model Registry</td>\n<td>Version and manage trained models through their lifecycle stages</td>\n<td>Model metadata, version history, stage transitions, approval records</td>\n<td>Model registration, version promotion, lineage tracking, model download</td>\n</tr>\n<tr>\n<td>Pipeline Orchestration</td>\n<td>Execute multi-step training workflows with resource management</td>\n<td>Pipeline definitions, execution history, step dependencies, resource allocations</td>\n<td>Pipeline submission, execution monitoring, resource scheduling</td>\n</tr>\n<tr>\n<td>Model Deployment</td>\n<td>Serve models as scalable HTTP endpoints with traffic management</td>\n<td>Deployment configurations, endpoint metadata, traffic routing rules</td>\n<td>Model serving, canary deployments, auto-scaling, rollback</td>\n</tr>\n<tr>\n<td>Model Monitoring</td>\n<td>Track model performance and detect drift in production</td>\n<td>Prediction logs, performance metrics, drift statistics, alert history</td>\n<td>Prediction logging, drift detection, alerting, dashboard data</td>\n</tr>\n</tbody></table>\n<p>The <strong>Experiment Tracking</strong> component serves as the foundational layer where all ML training activity begins. It maintains a hierarchical organization of experiments containing multiple runs, where each run captures a complete snapshot of a training execution including hyperparameters, code version, data version, and all generated artifacts. The component provides both real-time logging APIs for active training jobs and batch analysis capabilities for comparing runs and identifying optimal configurations.</p>\n<p>The <strong>Model Registry</strong> acts as the authoritative source for all trained models in the organization. It implements a <strong>git-like versioning system</strong> where each model version is immutable and linked to its source experiment run. The registry manages model lifecycle stages (development, staging, production, archived) with approval workflows and automated promotion rules. Every model version maintains complete lineage information tracing back to the training data, code commit, and experiment parameters that produced it.</p>\n<p><strong>Pipeline Orchestration</strong> coordinates complex multi-step training workflows using directed acyclic graphs (DAGs) to express step dependencies and data flow. The component handles resource allocation across heterogeneous compute infrastructure, supports both sequential and parallel execution patterns, and provides fault tolerance through checkpoint-restart mechanisms. It integrates with container orchestration platforms to provide isolated execution environments for each pipeline step.</p>\n<p><strong>Model Deployment</strong> transforms registered model versions into production-ready HTTP endpoints with enterprise-grade capabilities. The component supports multiple deployment strategies including blue-green deployments for zero-downtime updates and canary releases for gradual traffic migration. It integrates with popular inference servers like TensorFlow Serving, TorchServe, and NVIDIA Triton while providing unified APIs for model loading, auto-scaling, and health monitoring.</p>\n<p><strong>Model Monitoring</strong> provides comprehensive observability for deployed models through continuous tracking of prediction quality, performance metrics, and data characteristics. The component implements statistical drift detection algorithms to identify when model assumptions no longer hold and provides automated alerting when model performance degrades below acceptable thresholds. It maintains detailed audit trails of all model predictions to support compliance requirements and debugging efforts.</p>\n<blockquote>\n<p><strong>Key Design Principle:</strong> Each component maintains <strong>single responsibility</strong> while providing rich APIs for cross-component integration. This enables teams to adopt components incrementally rather than requiring big-bang platform adoption.</p>\n</blockquote>\n<p>The components communicate through three primary integration patterns: <strong>synchronous API calls</strong> for immediate data retrieval, <strong>asynchronous event publishing</strong> for lifecycle notifications, and <strong>shared data access</strong> for read-heavy operations like model lineage queries. This hybrid approach balances consistency requirements with performance optimization, allowing each interaction to use the most appropriate communication pattern.</p>\n<h3 id=\"technology-stack\">Technology Stack</h3>\n<p>The technology stack balances <strong>proven reliability</strong> with <strong>modern cloud-native patterns</strong>, selecting mature technologies that can scale to enterprise requirements while remaining approachable for development teams. The stack emphasizes <strong>polyglot persistence</strong>, where each data store is optimized for its specific access patterns rather than forcing all data into a single database technology.</p>\n<table>\n<thead>\n<tr>\n<th>Layer</th>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Storage</strong></td>\n<td>Metadata</td>\n<td>PostgreSQL with JSONB</td>\n<td>PostgreSQL + Redis + Elasticsearch</td>\n<td>JSONB handles flexible ML metadata while maintaining ACID guarantees</td>\n</tr>\n<tr>\n<td><strong>Storage</strong></td>\n<td>Artifacts</td>\n<td>MinIO (S3-compatible)</td>\n<td>Multi-tier storage (S3 + Glacier)</td>\n<td>Object storage scales to petabytes with configurable retention policies</td>\n</tr>\n<tr>\n<td><strong>Storage</strong></td>\n<td>Time Series</td>\n<td>PostgreSQL TimescaleDB</td>\n<td>InfluxDB + Grafana</td>\n<td>TimescaleDB provides SQL familiarity with time-series optimization</td>\n</tr>\n<tr>\n<td><strong>Orchestration</strong></td>\n<td>Container Platform</td>\n<td>Docker Compose</td>\n<td>Kubernetes with Kubeflow</td>\n<td>Kubernetes provides production-grade scheduling and resource management</td>\n</tr>\n<tr>\n<td><strong>Orchestration</strong></td>\n<td>Workflow Engine</td>\n<td>Airflow</td>\n<td>Kubeflow Pipelines</td>\n<td>Airflow&#39;s mature ecosystem handles complex workflow dependencies</td>\n</tr>\n<tr>\n<td><strong>Messaging</strong></td>\n<td>Event Coordination</td>\n<td>Redis Pub/Sub</td>\n<td>Apache Kafka</td>\n<td>Redis provides low-latency coordination; Kafka handles high-volume streams</td>\n</tr>\n<tr>\n<td><strong>Serving</strong></td>\n<td>Model Inference</td>\n<td>Flask + Gunicorn</td>\n<td>Kubernetes + Istio Service Mesh</td>\n<td>Service mesh provides traffic management and observability at scale</td>\n</tr>\n<tr>\n<td><strong>Monitoring</strong></td>\n<td>Application Metrics</td>\n<td>Prometheus + Grafana</td>\n<td>Prometheus + Grafana + Jaeger</td>\n<td>Proven observability stack with distributed tracing for debugging</td>\n</tr>\n</tbody></table>\n<p><strong>Metadata Storage</strong> uses PostgreSQL as the primary system of record for all structured metadata across components. PostgreSQL&#39;s JSONB support enables flexible schema evolution for ML metadata while maintaining ACID guarantees for critical operations like model promotion and deployment rollbacks. The database schema normalizes core entities (experiments, models, deployments) while storing variable metadata (parameters, tags, configuration) in JSONB columns that can be efficiently queried and indexed.</p>\n<p><strong>Artifact Storage</strong> leverages object storage for all binary artifacts including trained models, datasets, plots, and logs. The system uses content-addressable storage where artifacts are identified by cryptographic hashes, enabling automatic deduplication and integrity verification. A tiered storage strategy automatically moves infrequently accessed artifacts to cheaper storage classes while maintaining fast access to recent artifacts.</p>\n<p><strong>Time Series Storage</strong> handles high-volume metric data from training runs, model serving, and monitoring systems. TimescaleDB extends PostgreSQL with time-series optimizations including automatic partitioning, compression, and retention policies. This choice maintains SQL compatibility while providing the performance characteristics needed for real-time dashboards and alerting systems.</p>\n<blockquote>\n<p><strong>Architecture Decision: PostgreSQL as Primary Database</strong></p>\n<ul>\n<li><strong>Context</strong>: MLOps platforms need to store diverse metadata with complex relationships and varying schemas</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>NoSQL document database (MongoDB) for schema flexibility</li>\n<li>Graph database (Neo4j) for lineage relationships  </li>\n<li>PostgreSQL with JSONB for hybrid approach</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: PostgreSQL with JSONB columns for variable metadata</li>\n<li><strong>Rationale</strong>: ACID guarantees critical for model promotion workflows, mature ecosystem, JSONB provides schema flexibility where needed, SQL familiarity reduces operational complexity</li>\n<li><strong>Consequences</strong>: Enables complex cross-component queries, requires careful index management for JSONB fields, may need sharding for extreme scale</li>\n</ul>\n</blockquote>\n<p><strong>Container Orchestration</strong> provides the runtime environment for all platform components and user workloads. Docker containers ensure consistent execution environments across development and production while Kubernetes provides production-grade features including resource allocation, health monitoring, and rolling updates. The platform uses Kubernetes Custom Resource Definitions (CRDs) to extend the API with ML-specific resources like training jobs and model deployments.</p>\n<p><strong>Workflow Orchestration</strong> handles complex multi-step ML workflows using Apache Airflow&#39;s mature DAG execution engine. Airflow&#39;s extensive operator ecosystem provides pre-built integrations with popular ML frameworks, cloud services, and data processing tools. The platform extends Airflow with custom operators for MLOps-specific tasks like model registration and deployment triggers.</p>\n<p><strong>Event Coordination</strong> enables loose coupling between components through publish-subscribe messaging patterns. Redis Pub/Sub provides low-latency coordination for interactive workflows while Apache Kafka handles high-volume event streams from production model serving. The event system implements at-least-once delivery semantics with idempotent handlers to ensure reliable processing.</p>\n<p><strong>Model Serving Infrastructure</strong> supports multiple inference frameworks through a unified abstraction layer. The platform integrates with specialized serving systems like TensorFlow Serving for TensorFlow models, TorchServe for PyTorch models, and NVIDIA Triton for multi-framework serving. Kubernetes provides the underlying container orchestration while Istio service mesh handles advanced traffic management features like canary deployments and circuit breaking.</p>\n<h3 id=\"codebase-organization\">Codebase Organization</h3>\n<p>The codebase follows a <strong>monorepo structure</strong> with clear module boundaries that align with component responsibilities. This organization balances the benefits of shared tooling and dependency management with the need for component independence and clear ownership boundaries. The structure supports both development-time productivity and production deployment flexibility.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>mlops-platform/\n├── cmd/                          # Application entry points\n│   ├── experiment-tracker/       # Experiment tracking service\n│   ├── model-registry/           # Model registry service  \n│   ├── pipeline-orchestrator/    # Pipeline orchestration service\n│   ├── model-deployment/         # Model deployment service\n│   └── model-monitoring/         # Model monitoring service\n├── internal/                     # Private application code\n│   ├── common/                   # Shared utilities and types\n│   │   ├── auth/                 # Authentication and authorization\n│   │   ├── config/               # Configuration management\n│   │   ├── database/             # Database connection utilities\n│   │   ├── events/               # Event coordination primitives\n│   │   ├── health/               # Health check framework\n│   │   ├── logging/              # Structured logging setup\n│   │   └── storage/              # Storage abstraction interfaces\n│   ├── experiment/               # Experiment tracking domain\n│   │   ├── api/                  # REST API handlers\n│   │   ├── models/               # Domain entities and business logic\n│   │   ├── repository/           # Data access layer\n│   │   └── service/              # Business logic and orchestration\n│   ├── registry/                 # Model registry domain\n│   │   ├── api/\n│   │   ├── models/\n│   │   ├── repository/\n│   │   └── service/\n│   ├── pipeline/                 # Pipeline orchestration domain\n│   │   ├── api/\n│   │   ├── executor/             # Pipeline execution engine\n│   │   ├── models/\n│   │   ├── repository/\n│   │   └── scheduler/            # Resource scheduling logic\n│   ├── deployment/               # Model deployment domain\n│   │   ├── api/\n│   │   ├── models/\n│   │   ├── repository/\n│   │   ├── serving/              # Model serving integrations\n│   │   └── traffic/              # Traffic management logic\n│   └── monitoring/               # Model monitoring domain\n│       ├── api/\n│       ├── collectors/           # Prediction data collection\n│       ├── detectors/            # Drift detection algorithms\n│       ├── models/\n│       └── repository/\n├── pkg/                          # Public library code\n│   ├── client/                   # SDK for external integrations\n│   │   ├── experiment/           # Experiment tracking client\n│   │   ├── registry/             # Model registry client\n│   │   └── monitoring/           # Monitoring client\n│   └── types/                    # Shared type definitions\n├── api/                          # API specifications\n│   ├── openapi/                  # OpenAPI/Swagger specifications\n│   └── proto/                    # Protocol buffer definitions\n├── deployments/                  # Deployment configurations\n│   ├── docker/                   # Docker configurations\n│   ├── kubernetes/               # Kubernetes manifests\n│   └── helm/                     # Helm charts\n├── docs/                         # Documentation\n│   ├── api/                      # API documentation\n│   ├── architecture/             # Architecture decision records\n│   └── user-guides/              # User documentation\n├── scripts/                      # Development and deployment scripts\n│   ├── dev/                      # Development environment setup\n│   ├── migration/                # Database migration scripts\n│   └── testing/                  # Testing utilities\n└── test/                         # Test suites\n    ├── integration/              # Cross-component integration tests\n    ├── load/                     # Performance and load tests\n    └── e2e/                      # End-to-end scenario tests</code></pre></div>\n\n<p>The <strong>hexagonal architecture pattern</strong> within each component separates business logic from external concerns through well-defined interfaces. The <code>api/</code> package handles HTTP request/response concerns, <code>service/</code> contains pure business logic, <code>repository/</code> abstracts data access, and <code>models/</code> defines domain entities with their invariants and behaviors. This separation enables comprehensive unit testing and makes it easy to swap out infrastructure dependencies.</p>\n<p><strong>Shared utilities</strong> in the <code>internal/common/</code> package provide consistent implementations of cross-cutting concerns while avoiding tight coupling between components. The <code>events/</code> package implements the event coordination system used for inter-component communication, while <code>storage/</code> provides abstract interfaces that components use to interact with databases and object storage without depending on specific implementations.</p>\n<p>The <strong>client SDK</strong> in the <code>pkg/client/</code> package enables external applications to integrate with the MLOps platform through idiomatic APIs. Each component provides a client library that handles authentication, request serialization, error handling, and retry logic. The SDK supports both synchronous and asynchronous usage patterns depending on the operation characteristics.</p>\n<p><strong>Configuration Management</strong> uses a hierarchical approach where default values are defined in code, overridden by configuration files, and finally by environment variables. This enables the same codebase to run across development, staging, and production environments with environment-specific configuration. Sensitive values like database credentials are injected through secure mechanisms like Kubernetes secrets.</p>\n<blockquote>\n<p><strong>Development Workflow:</strong> Each component can be developed and tested independently using interfaces and mocks for dependencies, but integration testing validates cross-component behavior using docker-compose environments that mirror production topology.</p>\n</blockquote>\n<p><strong>Database Schema Management</strong> uses versioned migrations to evolve the database schema safely across environments. Migration scripts are component-specific but coordinate through a shared migration tracking system to ensure consistent ordering. The system supports both forward migrations for schema evolution and rollback capabilities for deployment recovery scenarios.</p>\n<p><strong>Testing Strategy</strong> employs multiple testing levels with clear boundaries and responsibilities. Unit tests focus on individual component logic using mocks for external dependencies. Integration tests validate component interactions using test databases and message queues. End-to-end tests exercise complete user workflows across the entire platform using realistic data and scenarios.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The implementation follows a <strong>service-oriented architecture</strong> where each component runs as an independent service with clearly defined APIs and data boundaries. This approach enables incremental development where teams can build and deploy components independently while ensuring they integrate correctly through standardized interfaces.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n<th>Development Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP Framework</td>\n<td>Flask + Flask-RESTful</td>\n<td>FastAPI + Pydantic</td>\n<td>Flask for rapid prototyping, FastAPI for production</td>\n</tr>\n<tr>\n<td>Database ORM</td>\n<td>SQLAlchemy Core</td>\n<td>SQLAlchemy ORM + Alembic</td>\n<td>Core for complex queries, ORM for rapid development</td>\n</tr>\n<tr>\n<td>Task Queue</td>\n<td>Celery + Redis</td>\n<td>Celery + Redis + Flower</td>\n<td>Celery provides robust async task execution</td>\n</tr>\n<tr>\n<td>Configuration</td>\n<td>Python-dotenv + dataclasses</td>\n<td>Pydantic Settings + YAML</td>\n<td>Pydantic provides validation and type safety</td>\n</tr>\n<tr>\n<td>Testing</td>\n<td>pytest + pytest-asyncio</td>\n<td>pytest + testcontainers + factory_boy</td>\n<td>Testcontainers for realistic integration tests</td>\n</tr>\n<tr>\n<td>API Documentation</td>\n<td>Flask-RESTX</td>\n<td>FastAPI auto-docs + ReDoc</td>\n<td>FastAPI generates interactive documentation</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended Project Structure:</strong></p>\n<p>Start with this directory layout and expand as components grow in complexity:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>mlops-platform/\n├── requirements/\n│   ├── base.txt              # Core dependencies\n│   ├── dev.txt              # Development tools\n│   └── test.txt             # Testing dependencies\n├── src/\n│   ├── mlops_platform/\n│   │   ├── __init__.py\n│   │   ├── config.py        # Configuration management\n│   │   ├── database.py      # Database connection setup\n│   │   ├── events.py        # Event coordination system\n│   │   ├── health.py        # Health check framework\n│   │   └── storage.py       # Storage abstraction layer\n│   ├── experiment_tracking/\n│   │   ├── __init__.py\n│   │   ├── app.py          # Flask application factory\n│   │   ├── api.py          # REST API endpoints\n│   │   ├── models.py       # Database models\n│   │   ├── repository.py   # Data access layer\n│   │   └── service.py      # Business logic\n│   └── model_registry/     # Similar structure for each component\n├── tests/\n│   ├── conftest.py         # Pytest configuration and fixtures\n│   ├── unit/              # Component unit tests\n│   ├── integration/       # Cross-component tests\n│   └── e2e/              # End-to-end scenarios\n├── docker-compose.yml     # Development environment\n├── Dockerfile            # Container image definition\n└── pyproject.toml       # Python project configuration</code></pre></div>\n\n<p><strong>Core Infrastructure Starter Code:</strong></p>\n<p>The foundation provides essential infrastructure that all components use:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/mlops_platform/events.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Callable, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Event</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base event class for inter-component communication.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Event'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">event_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            source</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">source, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            payload</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">payload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates event publishing and subscription between components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._subscribers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> subscribe</span><span style=\"color:#E1E4E8\">(self, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, handler: Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register event handler for specific event type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> event_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._subscribers:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._subscribers[event_type] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._subscribers[event_type].append(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> publish</span><span style=\"color:#E1E4E8\">(self, event: Event, synchronous: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Publish event to subscribers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        handlers </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._subscribers.get(event.type, [])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> synchronous:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._notify_handlers_sync(event, handlers)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # For async implementation, use threading or async/await</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            threading.Thread(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._notify_handlers_sync,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                args</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(event, handlers),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                daemon</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ).start()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _notify_handlers_sync</span><span style=\"color:#E1E4E8\">(self, event: Event, handlers: List[Callable]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Notify all handlers synchronously with error isolation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> handlers:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                handler(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Event handler failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exc_info</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># src/mlops_platform/health.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Health check status values.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Health check result.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages health checks for a component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._checks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable[[], HealthCheck]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_check</span><span style=\"color:#E1E4E8\">(self, check_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: Callable[[], HealthCheck]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register periodic health check function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._checks[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_func</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_checks</span><span style=\"color:#E1E4E8\">(self) -> List[HealthCheck]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all health checks and return results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> name, check_func </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._checks.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_func()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(result)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Check failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># src/mlops_platform/storage.py  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetadataStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for metadata storage operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create table with specified schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Insert data and return generated ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update record by ID, return success status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, filters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">              limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Query records with filters and pagination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_by_id</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get single record by ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ArtifactStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for binary artifact storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">, metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store binary data with optional metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve binary data by key.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Delete artifact, return success status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_keys</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"List artifact keys with optional prefix filter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Component Skeleton Structure:</strong></p>\n<p>Each component follows this pattern for consistent organization:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># src/experiment_tracking/models.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Experiment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Experiment entity representing a group of related training runs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Define experiment fields based on data model section</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Run</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Training run entity with parameters, metrics, and artifacts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Define run fields based on data model section</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># src/experiment_tracking/service.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExperimentTrackingService</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Business logic for experiment tracking operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, metadata_store: MetadataStore, artifact_store: ArtifactStore,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 event_coordinator: EventCoordinator):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store dependencies for data access and event publishing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_parameter</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log parameter for a training run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate run_id exists and is in active state</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate parameter key format (no special characters)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store parameter in metadata store with run association</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update run's last_modified timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Publish parameter_logged event for real-time updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_metric</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log metric value for a training run at specific step.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate run_id exists and metric key format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If step is None, auto-increment from last step for this metric</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Store metric with timestamp in time-series optimized format  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update metric aggregations (min, max, latest) for run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Publish metric_logged event with real-time value</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># src/experiment_tracking/api.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> flask </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Flask, request, jsonify</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> mlops_platform.health </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ComponentHealth, HealthStatus, HealthCheck</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_app</span><span style=\"color:#E1E4E8\">(service: ExperimentTrackingService) -> Flask:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create Flask application with experiment tracking endpoints.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    app </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Flask(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    health </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ComponentHealth()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add health checks for database and storage connectivity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/health'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'GET'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> health_check</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Health check endpoint for monitoring and load balancers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Run all registered health checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return 200 if all healthy, 503 if any unhealthy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Include health check details in response body</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/experiments'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'POST'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_experiment</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new experiment for organizing training runs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract experiment name and metadata from request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate experiment name is unique and follows naming rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Call service to create experiment and return experiment ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return 201 with experiment details or 400 for validation errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @app.route</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">'/runs/&#x3C;run_id>/parameters'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">methods</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'POST'</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_parameter</span><span style=\"color:#E1E4E8\">(run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log parameter for specific training run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract parameter key and value from request body</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate request format and parameter value type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Call service to log parameter with error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return 200 on success or appropriate error status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> app</span></span></code></pre></div>\n\n<p><strong>Language-Specific Implementation Tips:</strong></p>\n<ul>\n<li><strong>Database Connections</strong>: Use connection pooling with SQLAlchemy&#39;s <code>create_engine(pool_size=20, pool_recycle=3600)</code> for production deployments</li>\n<li><strong>Async Operations</strong>: Consider using FastAPI with async/await for high-throughput endpoints, especially metric logging</li>\n<li><strong>Error Handling</strong>: Implement structured exception handling with custom exception types for domain errors vs infrastructure errors</li>\n<li><strong>Configuration</strong>: Use Pydantic BaseSettings for type-safe configuration with automatic validation and environment variable binding</li>\n<li><strong>Logging</strong>: Configure structured logging with correlation IDs to trace requests across components: <code>logging.basicConfig(format=&#39;%(asctime)s %(name)s %(levelname)s [%(correlation_id)s] %(message)s&#39;)</code></li>\n<li><strong>Testing</strong>: Use pytest fixtures for database setup/teardown and factory_boy for generating test data with realistic relationships</li>\n</ul>\n<p><strong>Development Environment Setup:</strong></p>\n<p>Create <code>docker-compose.dev.yml</code> for local development with all dependencies:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#85E89D\">version</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'3.8'</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">services</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  postgres</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    image</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">timescale/timescaledb:latest-pg14</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    environment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      POSTGRES_DB</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">mlops</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      POSTGRES_USER</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">mlops</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      POSTGRES_PASSWORD</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">development</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    ports</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"5432:5432\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    volumes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">postgres_data:/var/lib/postgresql/data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  redis</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    image</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">redis:7-alpine</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    ports</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"6379:6379\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  minio</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    image</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">minio/minio:latest</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    command</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">server /data --console-address \":9001\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    environment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      MINIO_ROOT_USER</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">minioadmin</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      MINIO_ROOT_PASSWORD</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">minioadmin</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    ports</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"9000:9000\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"9001:9001\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    volumes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">minio_data:/data</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#85E89D\">volumes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  postgres_data</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  minio_data</span><span style=\"color:#E1E4E8\">:</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the core infrastructure:</p>\n<ol>\n<li><strong>Run Infrastructure Tests</strong>: <code>python -m pytest tests/unit/test_events.py -v</code> should show all event coordination tests passing</li>\n<li><strong>Verify Database Connectivity</strong>: Start the development environment with <code>docker-compose -f docker-compose.dev.yml up -d</code> and run connection tests</li>\n<li><strong>Check Health Endpoints</strong>: Each component&#39;s <code>/health</code> endpoint should return 200 with status details when dependencies are available</li>\n<li><strong>Test Event Flow</strong>: Register event handlers and publish test events to verify cross-component communication works</li>\n<li><strong>Validate Configuration</strong>: Components should start successfully with environment variables and fail gracefully with helpful error messages for missing required configuration</li>\n</ol>\n<p><strong>Common Development Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Components can&#39;t find each other</td>\n<td>Service discovery misconfiguration</td>\n<td>Check component logs for connection errors</td>\n<td>Verify service names match docker-compose service definitions</td>\n</tr>\n<tr>\n<td>Database connection pool exhausted</td>\n<td>Too many concurrent connections without proper cleanup</td>\n<td>Monitor database connection count during load</td>\n<td>Use context managers for database sessions: <code>with get_session() as session:</code></td>\n</tr>\n<tr>\n<td>Events not being delivered</td>\n<td>Event coordinator not properly initialized</td>\n<td>Add debug logging to event handlers</td>\n<td>Ensure EventCoordinator is shared across component modules as singleton</td>\n</tr>\n<tr>\n<td>Health checks always failing</td>\n<td>Dependencies not ready during startup</td>\n<td>Check component startup order in logs</td>\n<td>Add retry logic with exponential backoff for dependency connections</td>\n</tr>\n<tr>\n<td>Slow API responses</td>\n<td>Database queries without proper indexing</td>\n<td>Enable SQL query logging and check execution plans</td>\n<td>Add database indexes for frequently queried columns (run_id, experiment_id, timestamp)</td>\n</tr>\n</tbody></table>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section establishes the core data structures and entity relationships that underpin all milestones (1-5), providing the foundational schema for experiment tracking, model registry, pipeline orchestration, deployment management, and monitoring systems.</p>\n</blockquote>\n<p>The data model serves as the backbone of our MLOps platform, defining how we represent and relate the core entities throughout the machine learning lifecycle. Think of this as the <strong>architectural blueprint</strong> for a modern research facility - just as a laboratory needs well-organized systems for tracking experiments, storing samples, managing equipment, and recording results, our platform needs structured data representations for experiments, models, pipelines, and deployments.</p>\n<p>The design follows a <strong>microservices approach</strong> where each component maintains its own data stores optimized for specific access patterns, a strategy known as <strong>polyglot persistence</strong>. This allows experiment tracking to use time-series optimized storage for metrics, while the model registry uses content-addressable storage for artifacts, and monitoring systems use real-time analytics databases.</p>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fdata-model-relationships.svg\" alt=\"Core Entity Relationships\"></p>\n<h3 id=\"mental-model-digital-laboratory-information-system\">Mental Model: Digital Laboratory Information System</h3>\n<p>Before diving into the technical schemas, consider how a modern pharmaceutical research laboratory organizes its information systems. The laboratory maintains several interconnected databases: an <strong>experiment logbook</strong> tracking research protocols and results, a <strong>compound registry</strong> managing chemical formulations and their versions, a <strong>workflow scheduler</strong> coordinating multi-step synthesis procedures, a <strong>production deployment system</strong> managing which compounds are in clinical trials, and a <strong>monitoring dashboard</strong> tracking patient outcomes and side effects.</p>\n<p>Our MLOps platform mirrors this organization. <strong>Experiment tracking</strong> serves as the digital logbook, capturing every training run with its hyperparameters, metrics, and generated artifacts. The <strong>model registry</strong> functions like the compound database, maintaining versioned models with their lineage and approval status. <strong>Pipeline orchestration</strong> coordinates complex training workflows like the synthesis scheduler. <strong>Model deployment</strong> manages which models serve production traffic, similar to clinical trial management. <strong>Model monitoring</strong> tracks real-world performance like patient outcome monitoring.</p>\n<p>Each system maintains its own specialized data structures while sharing common identifiers that enable <strong>model lineage</strong> - the ability to trace a production model back through its deployment, training pipeline, experiment run, and source data. This traceability proves essential for debugging, compliance, and understanding model behavior in production.</p>\n<blockquote>\n<p><strong>Design Principle: Immutable Core with Mutable Metadata</strong></p>\n<p>Core entities like experiment runs and model versions are immutable once created - their content cannot change, only their metadata (tags, descriptions, stage assignments) can be updated. This ensures reproducibility while allowing operational flexibility.</p>\n</blockquote>\n<h3 id=\"architecture-decision-entity-relationship-design\">Architecture Decision: Entity Relationship Design</h3>\n<blockquote>\n<p><strong>Decision: Hierarchical Entity Organization with Cross-Component References</strong></p>\n<ul>\n<li><strong>Context</strong>: MLOps platforms need to represent complex relationships between experiments, models, pipelines, and deployments while maintaining clear ownership boundaries for microservices</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Monolithic shared database with foreign key constraints</li>\n<li>Duplicated entity data in each component database</li>\n<li>Hierarchical ownership with cross-component reference IDs</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hierarchical ownership with cross-component reference IDs</li>\n<li><strong>Rationale</strong>: Allows each component to optimize its data store while maintaining loose coupling. Reference IDs enable lineage tracking without creating tight database dependencies that would compromise service autonomy</li>\n<li><strong>Consequences</strong>: Enables polyglot persistence and independent scaling, but requires eventual consistency patterns and careful handling of dangling references</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Scalability</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Monolithic Database</td>\n<td>Strong consistency, enforced relationships</td>\n<td>Tight coupling, single point of failure</td>\n<td>Limited</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Duplicated Data</td>\n<td>Complete service autonomy</td>\n<td>Data sync complexity, storage overhead</td>\n<td>High</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Reference IDs</td>\n<td>Loose coupling, optimized storage</td>\n<td>Eventual consistency, reference validation</td>\n<td>High</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<h2 id=\"experiment-tracking-entities\">Experiment Tracking Entities</h2>\n<p>The experiment tracking component organizes machine learning research using a three-level hierarchy that mirrors scientific research practices. This hierarchy provides progressively finer granularity for organizing and analyzing training efforts.</p>\n<h3 id=\"core-entity-hierarchy\">Core Entity Hierarchy</h3>\n<p>At the top level, an <strong>Experiment</strong> represents a research hypothesis or approach - for example, &quot;CNN architectures for image classification&quot; or &quot;BERT fine-tuning for sentiment analysis.&quot; Within each experiment, multiple <strong>Runs</strong> capture individual training attempts with specific hyperparameter configurations. Each run generates <strong>Parameters</strong>, <strong>Metrics</strong>, and <strong>Artifacts</strong> that collectively document the training process and results.</p>\n<table>\n<thead>\n<tr>\n<th>Entity</th>\n<th>Purpose</th>\n<th>Cardinality</th>\n<th>Lifespan</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment</td>\n<td>Group related research efforts</td>\n<td>1 to many Runs</td>\n<td>Indefinite</td>\n</tr>\n<tr>\n<td>Run</td>\n<td>Single training execution</td>\n<td>1 to many Parameters/Metrics/Artifacts</td>\n<td>Immutable after completion</td>\n</tr>\n<tr>\n<td>Parameter</td>\n<td>Input configuration value</td>\n<td>Many per Run</td>\n<td>Immutable</td>\n</tr>\n<tr>\n<td>Metric</td>\n<td>Measured training result</td>\n<td>Many per Run, many per training step</td>\n<td>Append-only</td>\n</tr>\n<tr>\n<td>Artifact</td>\n<td>Generated file or object</td>\n<td>Many per Run</td>\n<td>Immutable</td>\n</tr>\n</tbody></table>\n<h3 id=\"experiment-entity-schema\">Experiment Entity Schema</h3>\n<p>The <code>Experiment</code> entity serves as a logical container for related training runs, enabling researchers to organize their work by project, approach, or research question.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>experiment_id</td>\n<td>str</td>\n<td>Unique identifier for the experiment</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable experiment name</td>\n<td>Required, max 255 chars</td>\n</tr>\n<tr>\n<td>description</td>\n<td>str</td>\n<td>Detailed explanation of research goal</td>\n<td>Optional, max 2048 chars</td>\n</tr>\n<tr>\n<td>tags</td>\n<td>Dict[str, str]</td>\n<td>Key-value labels for organization</td>\n<td>Optional, max 20 tags</td>\n</tr>\n<tr>\n<td>creator_user_id</td>\n<td>str</td>\n<td>User who created the experiment</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of creation</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>updated_at</td>\n<td>float</td>\n<td>Unix timestamp of last modification</td>\n<td>Auto-updated</td>\n</tr>\n<tr>\n<td>run_count</td>\n<td>int</td>\n<td>Number of runs in this experiment</td>\n<td>Computed field</td>\n</tr>\n<tr>\n<td>lifecycle_stage</td>\n<td>str</td>\n<td>active, deleted, archived</td>\n<td>Default: active</td>\n</tr>\n</tbody></table>\n<p>Experiments support <strong>soft deletion</strong> through the <code>lifecycle_stage</code> field, allowing recovery of accidentally deleted experiments while hiding them from normal queries. The <code>tags</code> field enables flexible organization schemes - teams might tag experiments by model family (&quot;cnn&quot;, &quot;transformer&quot;), dataset (&quot;imagenet&quot;, &quot;coco&quot;), or business objective (&quot;accuracy&quot;, &quot;latency&quot;).</p>\n<h3 id=\"run-entity-schema\">Run Entity Schema</h3>\n<p>The <code>Run</code> entity captures a single training execution, serving as the central organizing unit for all training artifacts and measurements.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>run_id</td>\n<td>str</td>\n<td>Unique identifier for the run</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>experiment_id</td>\n<td>str</td>\n<td>Parent experiment reference</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>run_name</td>\n<td>str</td>\n<td>Human-readable run identifier</td>\n<td>Optional, max 255 chars</td>\n</tr>\n<tr>\n<td>status</td>\n<td>str</td>\n<td>RUNNING, COMPLETED, FAILED, KILLED</td>\n<td>Required, state machine</td>\n</tr>\n<tr>\n<td>start_time</td>\n<td>float</td>\n<td>Unix timestamp when run began</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>end_time</td>\n<td>float</td>\n<td>Unix timestamp when run finished</td>\n<td>Optional, set on completion</td>\n</tr>\n<tr>\n<td>source_type</td>\n<td>str</td>\n<td>Type of execution environment</td>\n<td>NOTEBOOK, SCRIPT, PIPELINE</td>\n</tr>\n<tr>\n<td>source_name</td>\n<td>str</td>\n<td>Specific source identifier</td>\n<td>File path, notebook name, etc.</td>\n</tr>\n<tr>\n<td>source_version</td>\n<td>str</td>\n<td>Code version or commit hash</td>\n<td>Optional, for reproducibility</td>\n</tr>\n<tr>\n<td>user_id</td>\n<td>str</td>\n<td>User who initiated the run</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>tags</td>\n<td>Dict[str, str]</td>\n<td>Run-specific key-value labels</td>\n<td>Optional, max 50 tags</td>\n</tr>\n<tr>\n<td>lifecycle_stage</td>\n<td>str</td>\n<td>active, deleted</td>\n<td>Default: active</td>\n</tr>\n</tbody></table>\n<p>The <code>status</code> field follows a strict state machine to track run progression:</p>\n<table>\n<thead>\n<tr>\n<th>Current Status</th>\n<th>Valid Transitions</th>\n<th>Trigger Events</th>\n<th>Automated Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>RUNNING</td>\n<td>COMPLETED, FAILED, KILLED</td>\n<td>Training completion, error, user termination</td>\n<td>Set end_time, finalize metrics</td>\n</tr>\n<tr>\n<td>COMPLETED</td>\n<td>deleted (via lifecycle_stage)</td>\n<td>User deletion</td>\n<td>Archive artifacts</td>\n</tr>\n<tr>\n<td>FAILED</td>\n<td>deleted (via lifecycle_stage)</td>\n<td>User deletion</td>\n<td>Preserve error logs</td>\n</tr>\n<tr>\n<td>KILLED</td>\n<td>deleted (via lifecycle_stage)</td>\n<td>User deletion</td>\n<td>Mark incomplete</td>\n</tr>\n</tbody></table>\n<h3 id=\"parameter-entity-schema\">Parameter Entity Schema</h3>\n<p>Parameters capture the input configuration for a training run, including hyperparameters, dataset specifications, and environment settings. The schema supports both simple scalar values and complex nested configurations.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>parameter_id</td>\n<td>str</td>\n<td>Unique identifier for this parameter</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>run_id</td>\n<td>str</td>\n<td>Parent run reference</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>key</td>\n<td>str</td>\n<td>Parameter name with optional nesting</td>\n<td>Required, dot notation supported</td>\n</tr>\n<tr>\n<td>value</td>\n<td>str</td>\n<td>Parameter value as string</td>\n<td>Required, JSON-serialized for complex types</td>\n</tr>\n<tr>\n<td>value_type</td>\n<td>str</td>\n<td>Original data type</td>\n<td>INT, FLOAT, STRING, BOOL, JSON</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of parameter logging</td>\n<td>Required, immutable</td>\n</tr>\n</tbody></table>\n<p>Parameters support <strong>hierarchical naming</strong> using dot notation to represent nested configurations. For example, a training configuration might include:</p>\n<ul>\n<li><code>model.type: &quot;cnn&quot;</code></li>\n<li><code>model.layers.conv1.filters: &quot;32&quot;</code></li>\n<li><code>model.layers.conv1.kernel_size: &quot;3&quot;</code></li>\n<li><code>optimizer.name: &quot;adam&quot;</code></li>\n<li><code>optimizer.learning_rate: &quot;0.001&quot;</code></li>\n</ul>\n<p>This structure enables efficient querying for parameter ranges (&quot;find all runs where <code>optimizer.learning_rate</code> &gt; 0.01&quot;) while maintaining the semantic structure of complex configurations.</p>\n<h3 id=\"metric-entity-schema\">Metric Entity Schema</h3>\n<p>Metrics capture quantitative measurements during training, supporting both scalar values logged at specific steps and aggregate statistics computed across runs.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>metric_id</td>\n<td>str</td>\n<td>Unique identifier for this metric</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>run_id</td>\n<td>str</td>\n<td>Parent run reference</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>key</td>\n<td>str</td>\n<td>Metric name (loss, accuracy, etc.)</td>\n<td>Required, max 255 chars</td>\n</tr>\n<tr>\n<td>value</td>\n<td>float</td>\n<td>Numeric measurement</td>\n<td>Required, supports NaN/Inf</td>\n</tr>\n<tr>\n<td>step</td>\n<td>int</td>\n<td>Training step or epoch number</td>\n<td>Optional, for time series</td>\n</tr>\n<tr>\n<td>timestamp</td>\n<td>float</td>\n<td>Unix timestamp of measurement</td>\n<td>Required, for temporal ordering</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp when logged</td>\n<td>Required, immutable</td>\n</tr>\n</tbody></table>\n<p>The dual timestamp system supports both <strong>logical ordering</strong> (via <code>step</code>) and <strong>wall-clock analysis</strong> (via <code>timestamp</code>). This proves essential for understanding training dynamics, especially in distributed training scenarios where logical steps might complete out of wall-clock order.</p>\n<p>Metrics support several logging patterns:</p>\n<ol>\n<li><strong>Step-based logging</strong>: <code>loss</code> and <code>accuracy</code> recorded at each training step</li>\n<li><strong>Epoch summarization</strong>: <code>epoch_loss_avg</code> and <code>validation_accuracy</code> recorded per epoch</li>\n<li><strong>Final aggregation</strong>: <code>best_validation_accuracy</code> and <code>total_training_time</code> recorded once per run</li>\n</ol>\n<h3 id=\"artifact-entity-schema\">Artifact Entity Schema</h3>\n<p>Artifacts represent files and binary objects generated during training runs, including model checkpoints, plots, datasets, and configuration files.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>artifact_id</td>\n<td>str</td>\n<td>Unique identifier for this artifact</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>run_id</td>\n<td>str</td>\n<td>Parent run reference</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>path</td>\n<td>str</td>\n<td>Logical path within run namespace</td>\n<td>Required, hierarchical</td>\n</tr>\n<tr>\n<td>artifact_uri</td>\n<td>str</td>\n<td>Physical storage location</td>\n<td>Required, URI format</td>\n</tr>\n<tr>\n<td>file_size</td>\n<td>int</td>\n<td>Size in bytes</td>\n<td>Optional, for storage tracking</td>\n</tr>\n<tr>\n<td>checksum</td>\n<td>str</td>\n<td>SHA-256 hash of contents</td>\n<td>Optional, for integrity verification</td>\n</tr>\n<tr>\n<td>artifact_type</td>\n<td>str</td>\n<td>MODEL, DATASET, PLOT, CONFIG, LOG</td>\n<td>Classification for UI organization</td>\n</tr>\n<tr>\n<td>mime_type</td>\n<td>str</td>\n<td>Content type hint</td>\n<td>Optional, for download handling</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of artifact creation</td>\n<td>Required, immutable</td>\n</tr>\n</tbody></table>\n<p>Artifacts use a <strong>logical path hierarchy</strong> that abstracts physical storage details. For example, a run might contain:</p>\n<ul>\n<li><code>model/checkpoint-final.pkl</code> → <code>s3://artifacts/runs/abc123/model/checkpoint-final.pkl</code></li>\n<li><code>plots/loss-curve.png</code> → <code>s3://artifacts/runs/abc123/plots/loss-curve.png</code></li>\n<li><code>data/train-dataset.parquet</code> → <code>s3://artifacts/runs/abc123/data/train-dataset.parquet</code></li>\n</ul>\n<p>The <code>checksum</code> field enables <strong>artifact deduplication</strong> - multiple runs producing identical model files can reference the same physical storage while maintaining separate logical paths.</p>\n<h3 id=\"querying-and-analysis-patterns\">Querying and Analysis Patterns</h3>\n<p>The experiment tracking schema supports several critical query patterns for ML research workflows:</p>\n<p><strong>Experiment comparison queries</strong> filter runs by parameter ranges and sort by metric values:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Find runs where optimizer.learning_rate BETWEEN 0.001 AND 0.01 \nAND model.type = &quot;transformer&quot; \nORDER BY best_validation_accuracy DESC LIMIT 10</code></pre></div>\n\n<p><strong>Time-series analysis queries</strong> retrieve metric evolution for specific runs:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Select step, value FROM metrics \nWHERE run_id = &quot;abc123&quot; AND key = &quot;validation_loss&quot; \nORDER BY step ASC</code></pre></div>\n\n<p><strong>Parameter correlation queries</strong> identify relationships between configuration and outcomes:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>SELECT parameters.value as batch_size, AVG(metrics.value) as avg_accuracy\nFROM parameters JOIN metrics ON parameters.run_id = metrics.run_id\nWHERE parameters.key = &quot;batch_size&quot; AND metrics.key = &quot;final_accuracy&quot;\nGROUP BY parameters.value</code></pre></div>\n\n<h2 id=\"model-registry-entities\">Model Registry Entities</h2>\n<p>The model registry manages trained models through their complete lifecycle, from initial registration through production deployment to eventual archival. The registry implements a <strong>software package registry</strong> pattern similar to npm or Docker Hub, providing versioning, metadata management, and stage-based promotion workflows.</p>\n<h3 id=\"model-lifecycle-overview\">Model Lifecycle Overview</h3>\n<p>Models progress through a structured lifecycle with explicit stage transitions and approval gates. This mirrors software release management practices, ensuring that only validated models reach production environments.</p>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fmodel-lifecycle.svg\" alt=\"Model Version State Machine\"></p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Purpose</th>\n<th>Entry Criteria</th>\n<th>Exit Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Development</td>\n<td>Initial model registration</td>\n<td>Completed training run</td>\n<td>Enable further testing</td>\n</tr>\n<tr>\n<td>Staging</td>\n<td>Pre-production validation</td>\n<td>Manual promotion or automated criteria</td>\n<td>Deploy to staging environment</td>\n</tr>\n<tr>\n<td>Production</td>\n<td>Live traffic serving</td>\n<td>Approval workflow completion</td>\n<td>Route production traffic</td>\n</tr>\n<tr>\n<td>Archived</td>\n<td>Historical preservation</td>\n<td>Superseded by newer version</td>\n<td>Remove from active serving</td>\n</tr>\n</tbody></table>\n<h3 id=\"model-entity-schema\">Model Entity Schema</h3>\n<p>The <code>Model</code> entity represents a named family of related model versions, analogous to a software package name that contains multiple versioned releases.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>model_id</td>\n<td>str</td>\n<td>Unique identifier for model family</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable model name</td>\n<td>Required, unique, max 255 chars</td>\n</tr>\n<tr>\n<td>description</td>\n<td>str</td>\n<td>Purpose and architecture description</td>\n<td>Optional, max 2048 chars</td>\n</tr>\n<tr>\n<td>tags</td>\n<td>Dict[str, str]</td>\n<td>Model family metadata</td>\n<td>Optional, max 20 tags</td>\n</tr>\n<tr>\n<td>creation_source</td>\n<td>str</td>\n<td>How model was created</td>\n<td>EXPERIMENT, IMPORT, PIPELINE</td>\n</tr>\n<tr>\n<td>creator_user_id</td>\n<td>str</td>\n<td>User who registered the model</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of initial registration</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>updated_at</td>\n<td>float</td>\n<td>Unix timestamp of last modification</td>\n<td>Auto-updated</td>\n</tr>\n<tr>\n<td>latest_version</td>\n<td>str</td>\n<td>Most recent version number</td>\n<td>Computed field</td>\n</tr>\n<tr>\n<td>current_stage_version</td>\n<td>Dict[str, str]</td>\n<td>Current version per stage</td>\n<td>Computed field</td>\n</tr>\n</tbody></table>\n<p>The <code>current_stage_version</code> field maintains a mapping from stage names to version numbers, enabling quick lookup of which version currently serves each environment:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"Development\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1.2.3\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"Staging\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1.2.1\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"Production\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"1.1.5\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"modelversion-entity-schema\">ModelVersion Entity Schema</h3>\n<p>The <code>ModelVersion</code> entity captures a specific iteration of a model with its artifacts, metadata, and stage assignment. Model versions are <strong>immutable</strong> once created - their core content cannot change, only their stage assignments and descriptive metadata.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>version_id</td>\n<td>str</td>\n<td>Unique identifier for this version</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>model_id</td>\n<td>str</td>\n<td>Parent model family reference</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>version</td>\n<td>str</td>\n<td>Semantic version number</td>\n<td>Required, immutable, semver format</td>\n</tr>\n<tr>\n<td>stage</td>\n<td>str</td>\n<td>Current lifecycle stage</td>\n<td>DEVELOPMENT, STAGING, PRODUCTION, ARCHIVED</td>\n</tr>\n<tr>\n<td>status</td>\n<td>str</td>\n<td>Version processing status</td>\n<td>CREATING, READY, FAILED</td>\n</tr>\n<tr>\n<td>source_run_id</td>\n<td>str</td>\n<td>Originating experiment run</td>\n<td>Optional, for lineage tracking</td>\n</tr>\n<tr>\n<td>model_uri</td>\n<td>str</td>\n<td>Primary model artifact location</td>\n<td>Required, URI format</td>\n</tr>\n<tr>\n<td>model_format</td>\n<td>str</td>\n<td>Serialization format</td>\n<td>PICKLE, ONNX, TENSORFLOW, PYTORCH</td>\n</tr>\n<tr>\n<td>model_signature</td>\n<td>Dict</td>\n<td>Input/output schema definition</td>\n<td>Optional, for compatibility validation</td>\n</tr>\n<tr>\n<td>model_metrics</td>\n<td>Dict[str, float]</td>\n<td>Performance measurements</td>\n<td>Optional, from training or validation</td>\n</tr>\n<tr>\n<td>description</td>\n<td>str</td>\n<td>Version-specific notes</td>\n<td>Optional, max 1024 chars</td>\n</tr>\n<tr>\n<td>tags</td>\n<td>Dict[str, str]</td>\n<td>Version-specific metadata</td>\n<td>Optional, max 50 tags</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of version creation</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>updated_at</td>\n<td>float</td>\n<td>Unix timestamp of last metadata update</td>\n<td>Auto-updated</td>\n</tr>\n<tr>\n<td>creator_user_id</td>\n<td>str</td>\n<td>User who created this version</td>\n<td>Required, immutable</td>\n</tr>\n</tbody></table>\n<p>The <code>model_signature</code> field captures the expected input and output schema for the model, enabling compatibility validation during deployment:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"inputs\"</span><span style=\"color:#E1E4E8\">: [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    {</span><span style=\"color:#79B8FF\">\"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"features\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"tensor\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"shape\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#79B8FF\">-1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">784</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">\"dtype\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"float32\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  ],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"outputs\"</span><span style=\"color:#E1E4E8\">: [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    {</span><span style=\"color:#79B8FF\">\"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"predictions\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"tensor\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"shape\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#79B8FF\">-1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">], </span><span style=\"color:#79B8FF\">\"dtype\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"float32\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"stage-transition-management\">Stage Transition Management</h3>\n<p>Model versions transition between stages through explicit promotion actions that can include approval workflows, automated testing, and rollback capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Transition</th>\n<th>Required Checks</th>\n<th>Approval Gates</th>\n<th>Automated Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Development → Staging</td>\n<td>Model signature validation</td>\n<td>Optional: Team lead approval</td>\n<td>Deploy to staging environment</td>\n</tr>\n<tr>\n<td>Staging → Production</td>\n<td>Performance benchmarks, compatibility tests</td>\n<td>Required: Production approval</td>\n<td>Create deployment, update routing</td>\n</tr>\n<tr>\n<td>Production → Archived</td>\n<td>Replacement version in production</td>\n<td>Optional: Cleanup approval</td>\n<td>Remove from serving, archive artifacts</td>\n</tr>\n<tr>\n<td>Any → Archived</td>\n<td>None</td>\n<td>User confirmation</td>\n<td>Stop all serving, preserve metadata</td>\n</tr>\n</tbody></table>\n<h3 id=\"model-lineage-tracking\">Model Lineage Tracking</h3>\n<p>The model registry maintains comprehensive <strong>model lineage</strong> by linking each model version back to its training context, enabling full reproducibility and debugging capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>source_run_id</td>\n<td>str</td>\n<td>Originating experiment run ID</td>\n<td>Link to training parameters and metrics</td>\n</tr>\n<tr>\n<td>data_version_hash</td>\n<td>str</td>\n<td>Training dataset fingerprint</td>\n<td>Detect data dependencies</td>\n</tr>\n<tr>\n<td>code_commit_hash</td>\n<td>str</td>\n<td>Source code version</td>\n<td>Enable code-level reproducibility</td>\n</tr>\n<tr>\n<td>training_pipeline_id</td>\n<td>str</td>\n<td>Pipeline that created model</td>\n<td>Link to orchestration context</td>\n</tr>\n<tr>\n<td>parent_model_ids</td>\n<td>List[str]</td>\n<td>Models used as inputs</td>\n<td>Track model composition and transfer learning</td>\n</tr>\n<tr>\n<td>derived_model_ids</td>\n<td>List[str]</td>\n<td>Models created from this version</td>\n<td>Forward lineage tracking</td>\n</tr>\n</tbody></table>\n<p>This lineage information supports critical MLOps workflows:</p>\n<ol>\n<li><strong>Root cause analysis</strong>: When a production model fails, trace back to the specific training data, code version, and hyperparameters that created it</li>\n<li><strong>Impact analysis</strong>: When a security issue is discovered in training data, identify all models that might be affected</li>\n<li><strong>Compliance auditing</strong>: Demonstrate the complete provenance of models used in regulated environments</li>\n<li><strong>Reproducibility</strong>: Recreate the exact conditions that produced a specific model version</li>\n</ol>\n<h3 id=\"architecture-decision-immutable-versions-with-mutable-metadata\">Architecture Decision: Immutable Versions with Mutable Metadata</h3>\n<blockquote>\n<p><strong>Decision: Separate Immutable Core from Mutable Operational Metadata</strong></p>\n<ul>\n<li><strong>Context</strong>: Model versions need both immutability for reproducibility and flexibility for operational management (stage assignments, descriptions, tags)</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Fully immutable versions requiring new versions for any changes</li>\n<li>Fully mutable versions allowing arbitrary modifications</li>\n<li>Split design with immutable core and mutable metadata</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Split design with immutable core and mutable metadata</li>\n<li><strong>Rationale</strong>: Preserves reproducibility for model content while enabling operational flexibility. Clearly separates what affects model behavior (immutable) from what affects model management (mutable)</li>\n<li><strong>Consequences</strong>: Requires careful field classification and schema design, but provides both reproducibility and operational flexibility</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Field Category</th>\n<th>Mutability</th>\n<th>Examples</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Core Identity</td>\n<td>Immutable</td>\n<td>version_id, model_id, version</td>\n<td>Never change to preserve references</td>\n</tr>\n<tr>\n<td>Model Content</td>\n<td>Immutable</td>\n<td>model_uri, model_format, model_signature</td>\n<td>Changes would create different model</td>\n</tr>\n<tr>\n<td>Training Context</td>\n<td>Immutable</td>\n<td>source_run_id, created_at, creator_user_id</td>\n<td>Historical facts cannot change</td>\n</tr>\n<tr>\n<td>Operational Metadata</td>\n<td>Mutable</td>\n<td>stage, description, tags</td>\n<td>Management info can evolve</td>\n</tr>\n<tr>\n<td>Computed Fields</td>\n<td>Auto-updated</td>\n<td>updated_at, status</td>\n<td>Reflect current state</td>\n</tr>\n</tbody></table>\n<h2 id=\"pipeline-entities\">Pipeline Entities</h2>\n<p>Pipeline orchestration manages complex multi-step training workflows through <strong>directed acyclic graph (DAG)</strong> representations that capture data dependencies, resource requirements, and execution constraints. The pipeline data model supports both template definitions for reusable workflows and execution instances that track specific runs.</p>\n<h3 id=\"mental-model-manufacturing-assembly-line\">Mental Model: Manufacturing Assembly Line</h3>\n<p>Think of ML training pipelines like a sophisticated manufacturing assembly line. The <strong>pipeline definition</strong> serves as the blueprint showing all stations, their sequence, and what resources each station needs. A <strong>pipeline execution</strong> represents running that assembly line for a specific order, tracking which station is currently active, what materials are flowing between stations, and any quality control checkpoints along the way.</p>\n<p>Each <strong>pipeline step</strong> corresponds to a manufacturing station with specific equipment requirements (CPU, memory, GPU), input materials (datasets, models), processing instructions (training code), and output products (trained models, evaluation metrics). The assembly line supervisor (pipeline orchestrator) ensures stations receive their inputs on time, allocate resources efficiently, and handle equipment failures gracefully.</p>\n<h3 id=\"pipeline-definition-schema\">Pipeline Definition Schema</h3>\n<p>The <code>PipelineDefinition</code> entity captures reusable workflow templates that can be executed multiple times with different parameters and data inputs.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>pipeline_id</td>\n<td>str</td>\n<td>Unique identifier for pipeline template</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable pipeline name</td>\n<td>Required, max 255 chars</td>\n</tr>\n<tr>\n<td>description</td>\n<td>str</td>\n<td>Purpose and workflow overview</td>\n<td>Optional, max 2048 chars</td>\n</tr>\n<tr>\n<td>version</td>\n<td>str</td>\n<td>Pipeline definition version</td>\n<td>Required, semver format</td>\n</tr>\n<tr>\n<td>dag_definition</td>\n<td>Dict</td>\n<td>Step definitions and dependencies</td>\n<td>Required, JSON schema validated</td>\n</tr>\n<tr>\n<td>parameters</td>\n<td>Dict</td>\n<td>Configurable pipeline parameters</td>\n<td>Optional, with default values</td>\n</tr>\n<tr>\n<td>resource_defaults</td>\n<td>Dict</td>\n<td>Default compute resource allocations</td>\n<td>Optional, inheritable by steps</td>\n</tr>\n<tr>\n<td>schedule</td>\n<td>str</td>\n<td>Optional automatic execution schedule</td>\n<td>Optional, cron format</td>\n</tr>\n<tr>\n<td>tags</td>\n<td>Dict[str, str]</td>\n<td>Pipeline metadata for organization</td>\n<td>Optional, max 20 tags</td>\n</tr>\n<tr>\n<td>creator_user_id</td>\n<td>str</td>\n<td>User who created pipeline</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of creation</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>updated_at</td>\n<td>float</td>\n<td>Unix timestamp of last modification</td>\n<td>Auto-updated</td>\n</tr>\n<tr>\n<td>is_active</td>\n<td>bool</td>\n<td>Whether pipeline can be executed</td>\n<td>Default: true</td>\n</tr>\n</tbody></table>\n<p>The <code>dag_definition</code> field contains the complete workflow specification including step definitions, dependencies, and conditional execution logic:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"steps\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"data_validation\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"python_script\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"script_path\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"scripts/validate_data.py\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"resources\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"cpu\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"memory\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"4Gi\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"outputs\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"validated_data\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"feature_engineering\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"python_script\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"script_path\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"scripts/build_features.py\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"depends_on\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"data_validation\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"resources\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"cpu\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"memory\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"8Gi\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"inputs\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"validated_data\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"outputs\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"feature_matrix\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"model_training\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"python_script\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"script_path\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"scripts/train_model.py\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"depends_on\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"feature_engineering\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"resources\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#79B8FF\">\"cpu\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"memory\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"16Gi\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">\"gpu\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"inputs\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"feature_matrix\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">      \"outputs\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#9ECBFF\">\"trained_model\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"training_metrics\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"conditions\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"model_training\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"feature_engineering.accuracy > 0.8\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"pipeline-execution-schema\">Pipeline Execution Schema</h3>\n<p>The <code>PipelineExecution</code> entity tracks specific runs of pipeline definitions, maintaining state for each step and capturing execution context.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>execution_id</td>\n<td>str</td>\n<td>Unique identifier for this execution</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>pipeline_id</td>\n<td>str</td>\n<td>Reference to pipeline definition</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>pipeline_version</td>\n<td>str</td>\n<td>Version of definition used</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>status</td>\n<td>str</td>\n<td>Overall execution status</td>\n<td>PENDING, RUNNING, COMPLETED, FAILED, CANCELLED</td>\n</tr>\n<tr>\n<td>start_time</td>\n<td>float</td>\n<td>Unix timestamp when execution began</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>end_time</td>\n<td>float</td>\n<td>Unix timestamp when execution finished</td>\n<td>Optional, set on completion</td>\n</tr>\n<tr>\n<td>parameters</td>\n<td>Dict</td>\n<td>Parameter values for this execution</td>\n<td>Required, merged with defaults</td>\n</tr>\n<tr>\n<td>trigger_type</td>\n<td>str</td>\n<td>How execution was initiated</td>\n<td>MANUAL, SCHEDULED, API, WEBHOOK</td>\n</tr>\n<tr>\n<td>trigger_user_id</td>\n<td>str</td>\n<td>User who initiated execution</td>\n<td>Optional, null for automated triggers</td>\n</tr>\n<tr>\n<td>execution_context</td>\n<td>Dict</td>\n<td>Environment and runtime metadata</td>\n<td>Optional, execution environment details</td>\n</tr>\n<tr>\n<td>step_executions</td>\n<td>List[str]</td>\n<td>References to step execution records</td>\n<td>Computed field</td>\n</tr>\n<tr>\n<td>artifacts</td>\n<td>Dict[str, str]</td>\n<td>Execution-level artifact URIs</td>\n<td>Optional, summary outputs</td>\n</tr>\n<tr>\n<td>tags</td>\n<td>Dict[str, str]</td>\n<td>Execution-specific metadata</td>\n<td>Optional, max 50 tags</td>\n</tr>\n</tbody></table>\n<p>The execution follows a state machine that coordinates step-level progress:</p>\n<table>\n<thead>\n<tr>\n<th>Current Status</th>\n<th>Valid Transitions</th>\n<th>Trigger Events</th>\n<th>Automated Actions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PENDING</td>\n<td>RUNNING, CANCELLED</td>\n<td>Resource allocation, user cancellation</td>\n<td>Initialize step queue</td>\n</tr>\n<tr>\n<td>RUNNING</td>\n<td>COMPLETED, FAILED, CANCELLED</td>\n<td>All steps complete, step failure, user action</td>\n<td>Update step states</td>\n</tr>\n<tr>\n<td>COMPLETED</td>\n<td>None</td>\n<td>N/A</td>\n<td>Archive artifacts, notify subscribers</td>\n</tr>\n<tr>\n<td>FAILED</td>\n<td>PENDING (retry)</td>\n<td>Retry command</td>\n<td>Reset failed steps</td>\n</tr>\n<tr>\n<td>CANCELLED</td>\n<td>PENDING (restart)</td>\n<td>Restart command</td>\n<td>Clean up resources</td>\n</tr>\n</tbody></table>\n<h3 id=\"pipeline-step-execution-schema\">Pipeline Step Execution Schema</h3>\n<p>The <code>PipelineStepExecution</code> entity tracks individual step executions within a pipeline run, providing detailed progress and resource usage information.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>step_execution_id</td>\n<td>str</td>\n<td>Unique identifier for step execution</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>execution_id</td>\n<td>str</td>\n<td>Parent pipeline execution reference</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>step_name</td>\n<td>str</td>\n<td>Step name from pipeline definition</td>\n<td>Required, matches DAG definition</td>\n</tr>\n<tr>\n<td>status</td>\n<td>str</td>\n<td>Step execution status</td>\n<td>PENDING, RUNNING, COMPLETED, FAILED, SKIPPED</td>\n</tr>\n<tr>\n<td>start_time</td>\n<td>float</td>\n<td>Unix timestamp when step began</td>\n<td>Optional, set when resources allocated</td>\n</tr>\n<tr>\n<td>end_time</td>\n<td>float</td>\n<td>Unix timestamp when step finished</td>\n<td>Optional, set on completion</td>\n</tr>\n<tr>\n<td>allocated_resources</td>\n<td>Dict</td>\n<td>Actual resources allocated to step</td>\n<td>Optional, may differ from requested</td>\n</tr>\n<tr>\n<td>resource_usage</td>\n<td>Dict</td>\n<td>Measured resource consumption</td>\n<td>Optional, collected during execution</td>\n</tr>\n<tr>\n<td>container_image</td>\n<td>str</td>\n<td>Docker image used for execution</td>\n<td>Optional, for containerized steps</td>\n</tr>\n<tr>\n<td>worker_node_id</td>\n<td>str</td>\n<td>Compute node where step executed</td>\n<td>Optional, for debugging</td>\n</tr>\n<tr>\n<td>exit_code</td>\n<td>int</td>\n<td>Process exit code</td>\n<td>Optional, for script-type steps</td>\n</tr>\n<tr>\n<td>logs_uri</td>\n<td>str</td>\n<td>Location of execution logs</td>\n<td>Optional, for debugging</td>\n</tr>\n<tr>\n<td>input_artifacts</td>\n<td>Dict[str, str]</td>\n<td>Input artifact URIs</td>\n<td>Required, from upstream steps</td>\n</tr>\n<tr>\n<td>output_artifacts</td>\n<td>Dict[str, str]</td>\n<td>Generated artifact URIs</td>\n<td>Optional, populated on completion</td>\n</tr>\n<tr>\n<td>error_message</td>\n<td>str</td>\n<td>Error details if step failed</td>\n<td>Optional, for failure diagnosis</td>\n</tr>\n<tr>\n<td>retry_count</td>\n<td>int</td>\n<td>Number of retry attempts</td>\n<td>Default: 0</td>\n</tr>\n</tbody></table>\n<p>Resource usage tracking captures detailed performance metrics for optimization and cost analysis:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"cpu_usage\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"max_cores\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">7.8</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"avg_cores\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">6.2</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"duration_seconds\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1800</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"memory_usage\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"max_bytes\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">15728640000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"avg_bytes\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">12884901888</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"gpu_usage\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"max_utilization\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.95</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"avg_utilization\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.82</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"memory_used_bytes\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">10737418240</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  },</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"io_stats\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"bytes_read\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">5368709120</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"bytes_written\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">2147483648</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"read_ops\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"write_ops\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">512</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"data-flow-and-artifact-passing\">Data Flow and Artifact Passing</h3>\n<p>Pipeline steps communicate through <strong>artifact passing</strong> where upstream steps produce outputs that become inputs for downstream steps. The pipeline orchestrator manages this data flow through a combination of object storage and metadata tracking.</p>\n<table>\n<thead>\n<tr>\n<th>Artifact Flow Stage</th>\n<th>Components</th>\n<th>Storage Location</th>\n<th>Metadata Updates</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Step Output Generation</td>\n<td>Step execution environment</td>\n<td>Temporary staging area</td>\n<td>Register artifact URI and metadata</td>\n</tr>\n<tr>\n<td>Artifact Registration</td>\n<td>Pipeline orchestrator</td>\n<td>Permanent object storage</td>\n<td>Update step execution output_artifacts</td>\n</tr>\n<tr>\n<td>Dependency Resolution</td>\n<td>Pipeline orchestrator</td>\n<td>Metadata store queries</td>\n<td>Populate downstream input_artifacts</td>\n</tr>\n<tr>\n<td>Input Provisioning</td>\n<td>Step execution environment</td>\n<td>Local cache or mount</td>\n<td>Download/mount artifacts for processing</td>\n</tr>\n</tbody></table>\n<p>The orchestrator implements several optimization strategies for artifact management:</p>\n<ol>\n<li><strong>Lazy Loading</strong>: Downloads artifacts only when steps are ready to execute</li>\n<li><strong>Caching</strong>: Reuses artifacts from previous executions when inputs haven&#39;t changed</li>\n<li><strong>Parallel Transfers</strong>: Downloads multiple input artifacts concurrently</li>\n<li><strong>Cleanup Policies</strong>: Removes temporary artifacts based on retention rules</li>\n</ol>\n<h3 id=\"conditional-execution-and-dynamic-workflows\">Conditional Execution and Dynamic Workflows</h3>\n<p>Pipeline definitions support <strong>conditional execution</strong> where steps run only when specific criteria are met, enabling dynamic workflows that adapt based on intermediate results.</p>\n<table>\n<thead>\n<tr>\n<th>Condition Type</th>\n<th>Evaluation Context</th>\n<th>Example</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Upstream Artifact</td>\n<td>Previous step outputs</td>\n<td><code>data_validation.row_count &gt; 10000</code></td>\n<td>Skip training on insufficient data</td>\n</tr>\n<tr>\n<td>Upstream Metrics</td>\n<td>Previous step metrics</td>\n<td><code>feature_engineering.feature_count &gt; 50</code></td>\n<td>Conditional dimensionality reduction</td>\n</tr>\n<tr>\n<td>Pipeline Parameters</td>\n<td>Execution parameters</td>\n<td><code>pipeline.mode == &quot;production&quot;</code></td>\n<td>Environment-specific behavior</td>\n</tr>\n<tr>\n<td>External State</td>\n<td>API calls or database queries</td>\n<td><code>model_registry.current_accuracy &lt; 0.9</code></td>\n<td>Trigger retraining workflows</td>\n</tr>\n</tbody></table>\n<p>Conditions are evaluated by the pipeline orchestrator using a sandboxed expression engine that prevents arbitrary code execution while supporting complex logical expressions.</p>\n<h2 id=\"deployment-and-monitoring-entities\">Deployment and Monitoring Entities</h2>\n<p>The deployment and monitoring components manage the transition from trained models to production services, tracking model serving infrastructure, traffic management, and real-time performance metrics. These entities capture both the <strong>deployment topology</strong> (how models are served) and <strong>observability data</strong> (how well they perform).</p>\n<h3 id=\"mental-model-restaurant-service-management\">Mental Model: Restaurant Service Management</h3>\n<p>Consider how a high-end restaurant manages its service operations. The <strong>deployment system</strong> acts like the kitchen management, coordinating which recipes (models) are prepared by which stations (serving infrastructure), how much of each dish to prepare (scaling policies), and how to roll out new menu items safely (canary deployments). The <strong>monitoring system</strong> functions like a combination of quality control and customer feedback analysis, tracking both kitchen performance (serving latency, resource usage) and diner satisfaction (prediction accuracy, data drift).</p>\n<p>Just as restaurants maintain detailed records of recipe versions, preparation techniques, customer preferences, and service quality, our platform tracks model versions, serving configurations, traffic patterns, and prediction performance with the same level of detail and operational rigor.</p>\n<h3 id=\"deployment-entity-schema\">Deployment Entity Schema</h3>\n<p>The <code>Deployment</code> entity represents a specific model version serving configuration, capturing how the model is exposed as an HTTP endpoint with its scaling and traffic management policies.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>deployment_id</td>\n<td>str</td>\n<td>Unique identifier for deployment</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable deployment name</td>\n<td>Required, max 255 chars</td>\n</tr>\n<tr>\n<td>model_id</td>\n<td>str</td>\n<td>Reference to deployed model</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>model_version</td>\n<td>str</td>\n<td>Specific model version being served</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>environment</td>\n<td>str</td>\n<td>Deployment environment</td>\n<td>DEVELOPMENT, STAGING, PRODUCTION</td>\n</tr>\n<tr>\n<td>status</td>\n<td>str</td>\n<td>Current deployment status</td>\n<td>CREATING, HEALTHY, DEGRADED, FAILED, TERMINATING</td>\n</tr>\n<tr>\n<td>endpoint_url</td>\n<td>str</td>\n<td>HTTP endpoint for inference requests</td>\n<td>Generated, unique per deployment</td>\n</tr>\n<tr>\n<td>serving_config</td>\n<td>Dict</td>\n<td>Model serving configuration</td>\n<td>Required, serving framework specific</td>\n</tr>\n<tr>\n<td>scaling_config</td>\n<td>Dict</td>\n<td>Auto-scaling policy configuration</td>\n<td>Required, min/max replicas and triggers</td>\n</tr>\n<tr>\n<td>traffic_config</td>\n<td>Dict</td>\n<td>Traffic routing and splitting rules</td>\n<td>Optional, for A/B testing</td>\n</tr>\n<tr>\n<td>resource_allocation</td>\n<td>Dict</td>\n<td>Compute resources per serving replica</td>\n<td>Required, CPU/memory/GPU specifications</td>\n</tr>\n<tr>\n<td>health_check_config</td>\n<td>Dict</td>\n<td>Health monitoring configuration</td>\n<td>Required, readiness/liveness checks</td>\n</tr>\n<tr>\n<td>created_by_user_id</td>\n<td>str</td>\n<td>User who created deployment</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of creation</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>updated_at</td>\n<td>float</td>\n<td>Unix timestamp of last modification</td>\n<td>Auto-updated</td>\n</tr>\n<tr>\n<td>last_health_check</td>\n<td>float</td>\n<td>Unix timestamp of latest health check</td>\n<td>Auto-updated</td>\n</tr>\n<tr>\n<td>tags</td>\n<td>Dict[str, str]</td>\n<td>Deployment metadata for organization</td>\n<td>Optional, max 20 tags</td>\n</tr>\n</tbody></table>\n<p>The <code>serving_config</code> field captures framework-specific configuration for model serving platforms like TensorFlow Serving, TorchServe, or Triton Inference Server:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">json</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"framework\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"tensorflow_serving\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"model_signature_name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"serving_default\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"batch_size\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"max_batch_delay\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"0.1s\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"enable_model_warmup\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  \"optimization\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"enable_batching\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"enable_mixed_precision\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">true</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    \"tensorrt_optimization\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">false</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">  }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">}</span></span></code></pre></div>\n\n<h3 id=\"deployment-revision-schema\">Deployment Revision Schema</h3>\n<p>The <code>DeploymentRevision</code> entity tracks changes to deployment configurations over time, enabling rollback capabilities and change auditing.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>revision_id</td>\n<td>str</td>\n<td>Unique identifier for this revision</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>deployment_id</td>\n<td>str</td>\n<td>Parent deployment reference</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>revision_number</td>\n<td>int</td>\n<td>Sequential revision number</td>\n<td>Required, auto-increment per deployment</td>\n</tr>\n<tr>\n<td>change_description</td>\n<td>str</td>\n<td>Human-readable change summary</td>\n<td>Required, max 512 chars</td>\n</tr>\n<tr>\n<td>config_diff</td>\n<td>Dict</td>\n<td>Changed configuration fields</td>\n<td>Required, shows before/after values</td>\n</tr>\n<tr>\n<td>deployment_strategy</td>\n<td>str</td>\n<td>How changes were applied</td>\n<td>BLUE_GREEN, CANARY, ROLLING, IMMEDIATE</td>\n</tr>\n<tr>\n<td>rollout_status</td>\n<td>str</td>\n<td>Rollout progress status</td>\n<td>PENDING, IN_PROGRESS, COMPLETED, FAILED, ROLLED_BACK</td>\n</tr>\n<tr>\n<td>rollout_start_time</td>\n<td>float</td>\n<td>Unix timestamp when rollout began</td>\n<td>Optional, set when rollout starts</td>\n</tr>\n<tr>\n<td>rollout_end_time</td>\n<td>float</td>\n<td>Unix timestamp when rollout finished</td>\n<td>Optional, set on completion</td>\n</tr>\n<tr>\n<td>health_metrics_snapshot</td>\n<td>Dict</td>\n<td>Key metrics captured during rollout</td>\n<td>Optional, for rollback decisions</td>\n</tr>\n<tr>\n<td>created_by_user_id</td>\n<td>str</td>\n<td>User who initiated the change</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of revision creation</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>is_active</td>\n<td>bool</td>\n<td>Whether this revision is currently deployed</td>\n<td>Computed field</td>\n</tr>\n</tbody></table>\n<p>Deployment revisions support sophisticated rollout strategies with automatic rollback triggers:</p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Traffic Pattern</th>\n<th>Rollback Triggers</th>\n<th>Completion Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>BLUE_GREEN</td>\n<td>Instant 100% switch</td>\n<td>Error rate &gt; 5%, latency &gt; 2x baseline</td>\n<td>New version stable for 10 minutes</td>\n</tr>\n<tr>\n<td>CANARY</td>\n<td>Gradual 5% → 25% → 50% → 100%</td>\n<td>Error rate &gt; 2%, accuracy drop &gt; 5%</td>\n<td>All traffic shifted successfully</td>\n</tr>\n<tr>\n<td>ROLLING</td>\n<td>Sequential replica replacement</td>\n<td>Replica failure rate &gt; 10%</td>\n<td>All replicas updated and healthy</td>\n</tr>\n<tr>\n<td>IMMEDIATE</td>\n<td>Instant replacement</td>\n<td>Any serving failure</td>\n<td>All replicas serving</td>\n</tr>\n</tbody></table>\n<h3 id=\"endpoint-metrics-schema\">Endpoint Metrics Schema</h3>\n<p>The <code>EndpointMetrics</code> entity captures real-time serving performance measurements, supporting both operational monitoring and business analytics.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>metric_id</td>\n<td>str</td>\n<td>Unique identifier for metric record</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>deployment_id</td>\n<td>str</td>\n<td>Reference to deployment</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>metric_name</td>\n<td>str</td>\n<td>Specific metric being measured</td>\n<td>Required, from predefined catalog</td>\n</tr>\n<tr>\n<td>metric_value</td>\n<td>float</td>\n<td>Measured value</td>\n<td>Required, supports NaN for missing data</td>\n</tr>\n<tr>\n<td>metric_unit</td>\n<td>str</td>\n<td>Unit of measurement</td>\n<td>Required, for proper aggregation</td>\n</tr>\n<tr>\n<td>aggregation_window</td>\n<td>str</td>\n<td>Time window for aggregated metrics</td>\n<td>Required, 1m, 5m, 1h, 1d</td>\n</tr>\n<tr>\n<td>timestamp</td>\n<td>float</td>\n<td>Unix timestamp of measurement</td>\n<td>Required, bucket-aligned for aggregation</td>\n</tr>\n<tr>\n<td>dimensions</td>\n<td>Dict[str, str]</td>\n<td>Metric dimensions for grouping</td>\n<td>Optional, region, replica_id, etc.</td>\n</tr>\n<tr>\n<td>percentile</td>\n<td>float</td>\n<td>Percentile for latency metrics</td>\n<td>Optional, 0.5, 0.95, 0.99</td>\n</tr>\n<tr>\n<td>sample_count</td>\n<td>int</td>\n<td>Number of samples in aggregation</td>\n<td>Required for rate calculations</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp when metric was recorded</td>\n<td>Required, immutable</td>\n</tr>\n</tbody></table>\n<p>Key endpoint metrics include operational and business measurements:</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Metric Names</th>\n<th>Units</th>\n<th>Aggregation</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Latency</td>\n<td>request_latency_p50, request_latency_p95, request_latency_p99</td>\n<td>milliseconds</td>\n<td>percentile</td>\n<td>SLA monitoring</td>\n</tr>\n<tr>\n<td>Throughput</td>\n<td>requests_per_second, predictions_per_second</td>\n<td>count/second</td>\n<td>rate</td>\n<td>Capacity planning</td>\n</tr>\n<tr>\n<td>Errors</td>\n<td>error_rate, timeout_rate</td>\n<td>percentage</td>\n<td>ratio</td>\n<td>Quality monitoring</td>\n</tr>\n<tr>\n<td>Resources</td>\n<td>cpu_utilization, memory_utilization, gpu_utilization</td>\n<td>percentage</td>\n<td>average</td>\n<td>Cost optimization</td>\n</tr>\n<tr>\n<td>Business</td>\n<td>prediction_confidence, feature_coverage</td>\n<td>various</td>\n<td>distribution</td>\n<td>Model quality</td>\n</tr>\n</tbody></table>\n<h3 id=\"prediction-log-schema\">Prediction Log Schema</h3>\n<p>The <code>PredictionLog</code> entity records individual inference requests and responses, enabling detailed analysis of model behavior and data drift detection.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>prediction_id</td>\n<td>str</td>\n<td>Unique identifier for this prediction</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>deployment_id</td>\n<td>str</td>\n<td>Reference to serving deployment</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>request_id</td>\n<td>str</td>\n<td>Client-provided request identifier</td>\n<td>Optional, for client correlation</td>\n</tr>\n<tr>\n<td>timestamp</td>\n<td>float</td>\n<td>Unix timestamp of prediction request</td>\n<td>Required, immutable</td>\n</tr>\n<tr>\n<td>model_version</td>\n<td>str</td>\n<td>Model version that generated prediction</td>\n<td>Required, for lineage tracking</td>\n</tr>\n<tr>\n<td>input_features</td>\n<td>Dict</td>\n<td>Input feature values (anonymized)</td>\n<td>Required, schema-validated</td>\n</tr>\n<tr>\n<td>prediction_output</td>\n<td>Dict</td>\n<td>Model prediction results</td>\n<td>Required, includes confidence scores</td>\n</tr>\n<tr>\n<td>response_time_ms</td>\n<td>float</td>\n<td>Total request processing time</td>\n<td>Required, includes queue time</td>\n</tr>\n<tr>\n<td>feature_hash</td>\n<td>str</td>\n<td>Hash of input features for drift detection</td>\n<td>Required, for distribution tracking</td>\n</tr>\n<tr>\n<td>prediction_hash</td>\n<td>str</td>\n<td>Hash of output for distribution tracking</td>\n<td>Required, for concept drift</td>\n</tr>\n<tr>\n<td>user_id</td>\n<td>str</td>\n<td>Anonymized user identifier</td>\n<td>Optional, for personalization analysis</td>\n</tr>\n<tr>\n<td>session_id</td>\n<td>str</td>\n<td>Session identifier for request grouping</td>\n<td>Optional, for multi-step interactions</td>\n</tr>\n<tr>\n<td>client_metadata</td>\n<td>Dict[str, str]</td>\n<td>Client-provided context</td>\n<td>Optional, geography, device, etc.</td>\n</tr>\n</tbody></table>\n<p>Prediction logs support <strong>privacy-preserving analytics</strong> through selective field hashing and configurable retention policies:</p>\n<ol>\n<li><strong>Feature anonymization</strong>: Raw feature values are hashed or tokenized to prevent PII exposure</li>\n<li><strong>Sampling policies</strong>: High-traffic models log only a percentage of predictions to control storage costs</li>\n<li><strong>Retention schedules</strong>: Different field categories have different retention periods (metrics forever, raw data 90 days)</li>\n<li><strong>Access controls</strong>: Different roles can access different subsets of logged data</li>\n</ol>\n<h3 id=\"data-drift-detection-schema\">Data Drift Detection Schema</h3>\n<p>The <code>DriftAnalysis</code> entity captures statistical analysis of input data and prediction distributions to detect model degradation over time.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n<th>Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>analysis_id</td>\n<td>str</td>\n<td>Unique identifier for drift analysis</td>\n<td>Primary key, UUID format</td>\n</tr>\n<tr>\n<td>deployment_id</td>\n<td>str</td>\n<td>Reference to monitored deployment</td>\n<td>Foreign key, immutable</td>\n</tr>\n<tr>\n<td>analysis_type</td>\n<td>str</td>\n<td>Type of drift being measured</td>\n<td>FEATURE_DRIFT, PREDICTION_DRIFT, CONCEPT_DRIFT</td>\n</tr>\n<tr>\n<td>analysis_window_start</td>\n<td>float</td>\n<td>Start time of analysis window</td>\n<td>Required, defines comparison period</td>\n</tr>\n<tr>\n<td>analysis_window_end</td>\n<td>float</td>\n<td>End time of analysis window</td>\n<td>Required, defines comparison period</td>\n</tr>\n<tr>\n<td>baseline_window_start</td>\n<td>float</td>\n<td>Start time of baseline comparison</td>\n<td>Required, often training data period</td>\n</tr>\n<tr>\n<td>baseline_window_end</td>\n<td>float</td>\n<td>End time of baseline comparison</td>\n<td>Required, often training data period</td>\n</tr>\n<tr>\n<td>drift_score</td>\n<td>float</td>\n<td>Statistical measure of distribution difference</td>\n<td>Required, algorithm-specific</td>\n</tr>\n<tr>\n<td>drift_method</td>\n<td>str</td>\n<td>Algorithm used for drift calculation</td>\n<td>PSI, KL_DIVERGENCE, WASSERSTEIN, KS_TEST</td>\n</tr>\n<tr>\n<td>significance_threshold</td>\n<td>float</td>\n<td>Threshold for significant drift detection</td>\n<td>Required, configurable per deployment</td>\n</tr>\n<tr>\n<td>is_drift_detected</td>\n<td>bool</td>\n<td>Whether drift exceeds threshold</td>\n<td>Computed field</td>\n</tr>\n<tr>\n<td>feature_drift_scores</td>\n<td>Dict[str, float]</td>\n<td>Per-feature drift measurements</td>\n<td>Optional, for feature-level analysis</td>\n</tr>\n<tr>\n<td>affected_features</td>\n<td>List[str]</td>\n<td>Features showing significant drift</td>\n<td>Computed field</td>\n</tr>\n<tr>\n<td>sample_sizes</td>\n<td>Dict[str, int]</td>\n<td>Sample counts for statistical validity</td>\n<td>Required, baseline and current</td>\n</tr>\n<tr>\n<td>analysis_metadata</td>\n<td>Dict</td>\n<td>Algorithm-specific analysis details</td>\n<td>Optional, confidence intervals, etc.</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Unix timestamp of analysis creation</td>\n<td>Required, immutable</td>\n</tr>\n</tbody></table>\n<p>Drift detection supports multiple statistical methods optimized for different data types and drift patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Drift Method</th>\n<th>Best For</th>\n<th>Sensitivity</th>\n<th>Computational Cost</th>\n<th>Interpretability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PSI (Population Stability Index)</td>\n<td>Categorical features</td>\n<td>Medium</td>\n<td>Low</td>\n<td>High</td>\n</tr>\n<tr>\n<td>KL Divergence</td>\n<td>Continuous distributions</td>\n<td>High</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Wasserstein Distance</td>\n<td>Distribution shape changes</td>\n<td>High</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Kolmogorov-Smirnov Test</td>\n<td>Ordinal data</td>\n<td>Medium</td>\n<td>Low</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Jensen-Shannon Divergence</td>\n<td>Probability distributions</td>\n<td>High</td>\n<td>Medium</td>\n<td>Low</td>\n</tr>\n</tbody></table>\n<h3 id=\"architecture-decision-real-time-vs-batch-analytics\">Architecture Decision: Real-Time vs. Batch Analytics</h3>\n<blockquote>\n<p><strong>Decision: Hybrid Real-Time and Batch Analytics Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Model monitoring requires both immediate alerting for critical issues and comprehensive analysis for trend detection, creating tension between latency and analytical depth</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Pure real-time streaming analytics with immediate processing</li>\n<li>Pure batch analytics with periodic comprehensive analysis  </li>\n<li>Hybrid approach with real-time alerting and batch analysis</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Hybrid approach with real-time alerting and batch analysis</li>\n<li><strong>Rationale</strong>: Critical serving issues need immediate detection (latency spikes, error rates), while statistical drift analysis requires larger sample sizes and complex computations better suited for batch processing</li>\n<li><strong>Consequences</strong>: Increases system complexity but provides both operational responsiveness and analytical depth. Requires careful data flow coordination between streaming and batch systems</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Analytics Type</th>\n<th>Latency</th>\n<th>Data Volume</th>\n<th>Algorithms</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Real-Time Streaming</td>\n<td>&lt; 1 minute</td>\n<td>Individual predictions</td>\n<td>Simple thresholds, sliding windows</td>\n<td>Error rate alerts, latency spikes</td>\n</tr>\n<tr>\n<td>Batch Analytics</td>\n<td>15 minutes - 24 hours</td>\n<td>Aggregated datasets</td>\n<td>Statistical tests, ML algorithms</td>\n<td>Drift detection, performance trends</td>\n</tr>\n<tr>\n<td>Interactive Queries</td>\n<td>&lt; 10 seconds</td>\n<td>Sampled datasets</td>\n<td>Aggregations, filters</td>\n<td>Dashboard updates, ad-hoc analysis</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Overlogging Prediction Details</strong></p>\n<p>A common mistake is logging every field of every prediction request, leading to explosive storage growth and privacy concerns. Instead, implement <strong>selective logging policies</strong> based on model criticality, traffic volume, and regulatory requirements. High-traffic models might log only 1% of predictions with full details, while critical models in regulated industries log everything with strong access controls.</p>\n<p>⚠️ <strong>Pitfall: Static Drift Thresholds</strong></p>\n<p>Setting fixed drift detection thresholds often leads to false alarms during expected seasonal patterns or insufficient sensitivity during gradual degradation. Implement <strong>adaptive baselines</strong> that adjust to normal variance patterns and <strong>time-aware comparisons</strong> that account for cyclical data patterns.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical code structures and technology recommendations for building the data layer of your MLOps platform.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metadata Storage</td>\n<td>PostgreSQL with SQLAlchemy</td>\n<td>MongoDB with change streams</td>\n</tr>\n<tr>\n<td>Artifact Storage</td>\n<td>Local filesystem with S3 API</td>\n<td>AWS S3 with CloudFront CDN</td>\n</tr>\n<tr>\n<td>Time-Series Metrics</td>\n<td>InfluxDB</td>\n<td>Prometheus + Grafana</td>\n</tr>\n<tr>\n<td>Search and Analytics</td>\n<td>Elasticsearch</td>\n<td>Apache Druid with Superset</td>\n</tr>\n<tr>\n<td>Event Streaming</td>\n<td>Redis Pub/Sub</td>\n<td>Apache Kafka with Schema Registry</td>\n</tr>\n<tr>\n<td>Data Validation</td>\n<td>Cerberus schema validation</td>\n<td>Great Expectations with profiling</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>mlops_platform/\n  core/\n    entities/\n      __init__.py                    ← Export all entity classes\n      base.py                        ← Base entity with common fields\n      experiment.py                  ← Experiment tracking entities\n      model.py                       ← Model registry entities\n      pipeline.py                    ← Pipeline orchestration entities\n      deployment.py                  ← Deployment and monitoring entities\n    storage/\n      __init__.py                    ← Storage interface definitions\n      metadata_store.py              ← MetadataStore implementation\n      artifact_store.py              ← ArtifactStore implementation\n      time_series_store.py           ← Metrics and monitoring data\n    validation/\n      __init__.py                    ← Schema validation utilities\n      schemas/                       ← JSON schemas for entity validation\n        experiment_schema.json\n        model_schema.json\n        pipeline_schema.json\n        deployment_schema.json\n  migrations/                        ← Database schema migrations\n  tests/\n    unit/\n      test_entities.py               ← Entity model tests\n      test_storage.py                ← Storage layer tests\n    integration/\n      test_end_to_end.py             ← Full workflow tests</code></pre></div>\n\n<h4 id=\"base-entity-infrastructure\">Base Entity Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Base entity infrastructure providing common functionality for all MLOps entities.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Health status enumeration for system components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Event</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"System event for cross-component coordination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Event'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">event_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            source</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">source, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            payload</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">payload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Health check result for monitoring component status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetadataStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for entity metadata storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create table with specified schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Insert data and return generated ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, entity_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update existing entity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, filters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">              limit: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Query entities with filters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_by_id</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, entity_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get single entity by ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ArtifactStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for artifact storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">, metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store binary data with optional metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve binary data by key.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Delete artifact by key.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_keys</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"List all keys with given prefix.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages health checks for a component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._checks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">callable</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_check</span><span style=\"color:#E1E4E8\">(self, check_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: </span><span style=\"color:#79B8FF\">callable</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register periodic health check function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._checks[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_func</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_checks</span><span style=\"color:#E1E4E8\">(self) -> List[HealthCheck]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all health checks and return results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> name, check_func </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._checks.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute check function with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse result into HealthCheck object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle check function exceptions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add performance timing to details</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create failed HealthCheck with error details</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Coordinates event publishing and subscription between components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._subscribers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">callable</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> publish</span><span style=\"color:#E1E4E8\">(self, event: Event, synchronous: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Publish event to subscribers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate event object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Find all subscribers for event.type  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If synchronous, call handlers directly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If asynchronous, queue event for background processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle handler exceptions gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log event publishing for audit trail</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> subscribe</span><span style=\"color:#E1E4E8\">(self, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, handler: </span><span style=\"color:#79B8FF\">callable</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register event handler for specific event type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate handler is callable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add handler to subscribers list for event_type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Support handler deregistration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate handler signature matches Event parameter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Event type constants for cross-component coordination</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EXPERIMENT_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"experiment.completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MODEL_PROMOTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"model.promoted\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEPLOYMENT_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"deployment.failed\"</span></span></code></pre></div>\n\n<h4 id=\"experiment-tracking-entities\">Experiment Tracking Entities</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Entity definitions for experiment tracking component.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExperimentLifecycleStage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Experiment lifecycle stages.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ACTIVE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DELETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"deleted\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARCHIVED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"archived\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RunStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Training run execution status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"running\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"completed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KILLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"killed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Experiment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a group of related ML training runs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    experiment_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    creator_user_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    updated_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lifecycle_stage: ExperimentLifecycleStage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ExperimentLifecycleStage.</span><span style=\"color:#79B8FF\">ACTIVE</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_dict</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Convert experiment to dictionary for storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Serialize all fields to JSON-compatible dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle enum conversion to string values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate required fields are present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply field length limits (name max 255 chars, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Run</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single ML training execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    experiment_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: RunStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RunStatus.</span><span style=\"color:#79B8FF\">RUNNING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # NOTEBOOK, SCRIPT, PIPELINE</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    user_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lifecycle_stage: ExperimentLifecycleStage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ExperimentLifecycleStage.</span><span style=\"color:#79B8FF\">ACTIVE</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> complete</span><span style=\"color:#E1E4E8\">(self, final_status: RunStatus) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Mark run as completed with final status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set end_time to current timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update status to final_status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate final_status is terminal (COMPLETED, FAILED, KILLED)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Publish EXPERIMENT_COMPLETED event</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Parameter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a training hyperparameter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameter_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # INT, FLOAT, STRING, BOOL, JSON</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Metric</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a training metric measurement.\"\"\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metric_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Artifact</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a file or object generated during training.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    artifact_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # Logical path within run</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    artifact_uri: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # Physical storage location</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    file_size: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checksum: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    artifact_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # MODEL, DATASET, PLOT, CONFIG, LOG</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mime_type: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExperimentTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main interface for experiment tracking operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, metadata_store: MetadataStore, artifact_store: ArtifactStore):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metadata_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.artifact_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> artifact_store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_experiment</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, description: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Experiment:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new experiment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate experiment name is unique</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create Experiment object with provided parameters  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store experiment in metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return created experiment object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_run</span><span style=\"color:#E1E4E8\">(self, experiment_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, run_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Run:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start a new training run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate experiment_id exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create Run object with RUNNING status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store run in metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Increment experiment run_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return created run object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_parameter</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log a hyperparameter for a run.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate run_id exists and is active</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Convert value to string representation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect value_type (int, float, string, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create Parameter object and store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Support hierarchical keys with dot notation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"model-registry-entities\">Model Registry Entities</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Entity definitions for model registry component.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelStage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Model version lifecycle stages.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEVELOPMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"development\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    STAGING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"staging\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PRODUCTION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"production\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ARCHIVED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"archived\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Model version processing status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CREATING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"creating\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    READY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"ready\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Model</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a named family of model versions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    creation_source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # EXPERIMENT, IMPORT, PIPELINE</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    creator_user_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    updated_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    latest_version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_stage_version: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelVersion</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a specific iteration of a model.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # Semantic version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    stage: ModelStage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ModelStage.</span><span style=\"color:#79B8FF\">DEVELOPMENT</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: ModelStatus </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ModelStatus.</span><span style=\"color:#79B8FF\">CREATING</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_run_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_uri: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_format: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#6A737D\">  # PICKLE, ONNX, TENSORFLOW, PYTORCH</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_signature: Optional[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    updated_at: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.now().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    creator_user_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Lineage tracking fields</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data_version_hash: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    code_commit_hash: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    training_pipeline_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parent_model_ids: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    derived_model_ids: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelRegistry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main interface for model registry operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, metadata_store: MetadataStore, artifact_store: ArtifactStore):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metadata_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.artifact_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> artifact_store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_model</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, description: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">) -> Model:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a new model family.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate model name is unique</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create Model object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store in metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return created model</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_model_version</span><span style=\"color:#E1E4E8\">(self, model_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           model_uri: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source_run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ModelVersion:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new model version.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate model_id exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate version follows semantic versioning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate model_uri points to valid artifact</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create ModelVersion object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update parent model's latest_version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store version in metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Publish MODEL_PROMOTED event if appropriate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> transition_stage</span><span style=\"color:#E1E4E8\">(self, model_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        new_stage: ModelStage) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Transition model version to new stage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate version exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check stage transition rules (dev->staging->prod)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update version stage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update model's current_stage_version mapping</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Publish MODEL_PROMOTED event</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing the data model foundations, verify the following behavior:</p>\n<p><strong>Experiment Tracking Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from core.entities.experiment import ExperimentTracker</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">tracker = ExperimentTracker(metadata_store, artifact_store)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">exp = tracker.create_experiment('test-cnn', 'CNN experiments')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">run = tracker.start_run(exp.experiment_id, 'baseline-run')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">tracker.log_parameter(run.run_id, 'learning_rate', 0.001)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Created run {run.run_id} in experiment {exp.name}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Model Registry Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from core.entities.model import ModelRegistry</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">registry = ModelRegistry(metadata_store, artifact_store)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">model = registry.register_model('image-classifier', 'CNN for image classification')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">version = registry.create_model_version(model.model_id, '1.0.0', 's3://models/v1.pkl')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print(f'Registered model {model.name} version {version.version}')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span></code></pre></div>\n\n<p><strong>Expected Database Tables:</strong></p>\n<ul>\n<li>experiments: experiment_id, name, description, tags, creator_user_id, created_at</li>\n<li>runs: run_id, experiment_id, status, start_time, end_time, tags</li>\n<li>parameters: parameter_id, run_id, key, value, value_type</li>\n<li>metrics: metric_id, run_id, key, value, step, timestamp</li>\n<li>artifacts: artifact_id, run_id, path, artifact_uri, file_size</li>\n<li>models: model_id, name, description, creation_source, created_at</li>\n<li>model_versions: version_id, model_id, version, stage, model_uri, created_at</li>\n</ul>\n<p><strong>Common Issues and Debugging:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>UUID generation fails</td>\n<td>Missing uuid import</td>\n<td>Check import statements</td>\n<td>Add <code>import uuid</code></td>\n</tr>\n<tr>\n<td>Timestamp errors</td>\n<td>Timezone issues</td>\n<td>Check datetime.now() usage</td>\n<td>Use UTC timestamps consistently</td>\n</tr>\n<tr>\n<td>Foreign key violations</td>\n<td>Missing parent entities</td>\n<td>Verify experiment exists before creating runs</td>\n<td>Add existence validation</td>\n</tr>\n<tr>\n<td>JSON serialization fails</td>\n<td>Non-serializable fields</td>\n<td>Check Enum and datetime fields</td>\n<td>Implement custom serializers</td>\n</tr>\n<tr>\n<td>Storage connection errors</td>\n<td>Missing store initialization</td>\n<td>Check MetadataStore setup</td>\n<td>Initialize stores in application startup</td>\n</tr>\n</tbody></table>\n<h2 id=\"experiment-tracking-component\">Experiment Tracking Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section primarily corresponds to Milestone 1 (Experiment Tracking), which focuses on tracking experiments, parameters, metrics, and artifacts, while also establishing foundations used throughout Milestones 2-5 for lineage tracking and reproducibility.</p>\n</blockquote>\n<h3 id=\"mental-model-research-laboratory\">Mental Model: Research Laboratory</h3>\n<p>Think of experiment tracking as transforming a chaotic research laboratory into a meticulously organized scientific facility. In traditional ML development, data scientists run experiments like researchers working in isolation—they might scribble notes on napkins, save models with cryptic names like &quot;model_final_v3_ACTUALLY_FINAL.pkl&quot;, and forget which hyperparameters produced their best results. This creates a digital equivalent of a messy lab where critical discoveries get lost, experiments can&#39;t be reproduced, and knowledge walks out the door with departing team members.</p>\n<p>The experiment tracking component functions as a <strong>digital laboratory notebook</strong> combined with a <strong>specimen archive</strong>. Just as a proper research lab maintains detailed records of every experiment—the hypothesis, methodology, observations, and results—our experiment tracking system captures every detail of ML training runs. The laboratory notebook records the &quot;what&quot; and &quot;why&quot; (parameters and metadata), while the time-series observation log captures the &quot;how it unfolded&quot; (metrics over time), and the specimen archive preserves the &quot;what was produced&quot; (artifacts and models).</p>\n<p>This mental model reveals why experiment tracking requires three distinct but connected storage systems: a <strong>metadata store</strong> for searchable experiment records (like a card catalog), a <strong>time-series store</strong> for metric evolution (like a monitoring chart), and an <strong>artifact store</strong> for binary outputs (like a specimen freezer). Each serves a different query pattern but must maintain referential integrity to preserve the complete experimental narrative.</p>\n<p>The hierarchical organization mirrors how research labs group related studies. <strong>Experiments</strong> represent research programs (like &quot;customer churn prediction&quot; or &quot;image classification v2&quot;), while individual <strong>runs</strong> represent specific trials within that program. This hierarchy enables both focused analysis (&quot;which learning rate worked best in this experiment?&quot;) and broad comparisons (&quot;how do our image models compare to our NLP models?&quot;).</p>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fexperiment-flow.svg\" alt=\"Experiment to Deployment Flow\"></p>\n<h3 id=\"logging-apis-and-storage\">Logging APIs and Storage</h3>\n<p>The experiment tracking component exposes three primary logging interfaces that capture different aspects of ML training runs. These APIs must handle the diverse data types generated during ML experiments while providing consistent correlation mechanisms that link related information across storage systems.</p>\n<h4 id=\"parameter-logging-interface\">Parameter Logging Interface</h4>\n<p>Parameter logging captures the <strong>configuration space</strong> of ML experiments—the hyperparameters, data preprocessing settings, and model architecture choices that define how training was conducted. Unlike metrics that change during training, parameters represent static configuration that remains constant throughout a run.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>log_param</code></td>\n<td>run_id: str, key: str, value: Any</td>\n<td>None</td>\n<td>Log a single parameter key-value pair for the specified run</td>\n</tr>\n<tr>\n<td><code>log_params</code></td>\n<td>run_id: str, params: Dict[str, Any]</td>\n<td>None</td>\n<td>Log multiple parameters in a single atomic operation</td>\n</tr>\n<tr>\n<td><code>get_run_params</code></td>\n<td>run_id: str</td>\n<td>Dict[str, Any]</td>\n<td>Retrieve all parameters logged for a specific run</td>\n</tr>\n<tr>\n<td><code>update_param</code></td>\n<td>run_id: str, key: str, value: Any</td>\n<td>None</td>\n<td>Update an existing parameter value (creates if not exists)</td>\n</tr>\n</tbody></table>\n<p>Parameter storage must handle <strong>nested configurations</strong> common in modern ML frameworks. Training configurations often contain hierarchical structures like optimizer settings, data augmentation pipelines, and model architecture definitions. The storage layer flattens these hierarchies using dot notation (e.g., <code>optimizer.learning_rate</code>, <code>model.layers.0.units</code>) while maintaining the ability to reconstruct the original structure for display and comparison.</p>\n<p>The parameter store implements <strong>type-aware serialization</strong> to preserve data types during storage and retrieval. String values, numeric types, boolean flags, and complex objects like lists require different handling to maintain semantic meaning. For example, a learning rate of 0.001 should remain a float, not convert to a string representation that breaks numerical comparisons.</p>\n<h4 id=\"metric-logging-interface\">Metric Logging Interface</h4>\n<p>Metric logging captures the <strong>training dynamics</strong> of ML experiments—how loss decreases, accuracy improves, and validation metrics evolve throughout the training process. This creates time-series data that reveals training behavior patterns and convergence characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>log_metric</code></td>\n<td>run_id: str, key: str, value: float, step: int, timestamp: float</td>\n<td>None</td>\n<td>Log a single metric value at a specific training step</td>\n</tr>\n<tr>\n<td><code>log_metrics</code></td>\n<td>run_id: str, metrics: Dict[str, float], step: int, timestamp: float</td>\n<td>None</td>\n<td>Log multiple metrics for the same training step atomically</td>\n</tr>\n<tr>\n<td><code>get_metric_history</code></td>\n<td>run_id: str, metric_key: str</td>\n<td>List[MetricPoint]</td>\n<td>Retrieve the complete time series for a specific metric</td>\n</tr>\n<tr>\n<td><code>get_run_metrics</code></td>\n<td>run_id: str, step: int</td>\n<td>Dict[str, float]</td>\n<td>Get all metrics logged at a specific training step</td>\n</tr>\n</tbody></table>\n<p>Each metric point contains four essential components that enable both temporal analysis and cross-run comparison:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>run_id</td>\n<td>str</td>\n<td>Links the metric to the specific experiment run</td>\n</tr>\n<tr>\n<td>key</td>\n<td>str</td>\n<td>Metric name (e.g., &quot;train_loss&quot;, &quot;val_accuracy&quot;, &quot;learning_rate&quot;)</td>\n</tr>\n<tr>\n<td>value</td>\n<td>float</td>\n<td>Numeric value of the metric at this point in training</td>\n</tr>\n<tr>\n<td>step</td>\n<td>int</td>\n<td>Training step number (epoch, batch, or iteration count)</td>\n</tr>\n<tr>\n<td>timestamp</td>\n<td>float</td>\n<td>Unix timestamp when the metric was recorded</td>\n</tr>\n</tbody></table>\n<p>The metric storage system must handle <strong>high-frequency logging</strong> efficiently. Modern training runs can generate thousands of metric points per run, especially when logging at batch-level granularity. The storage layer uses <strong>batch insertion</strong> and <strong>time-based partitioning</strong> to maintain write performance while supporting both real-time monitoring and historical analysis queries.</p>\n<h4 id=\"artifact-storage-interface\">Artifact Storage Interface</h4>\n<p>Artifact storage manages the <strong>physical outputs</strong> of ML experiments—trained models, evaluation plots, datasets, configuration files, and any other files generated during training. Unlike parameters and metrics, artifacts are binary objects that require object storage rather than relational databases.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>log_artifact</code></td>\n<td>run_id: str, local_path: str, artifact_path: str, metadata: Dict</td>\n<td>str</td>\n<td>Upload a local file as an experiment artifact</td>\n</tr>\n<tr>\n<td><code>log_artifacts</code></td>\n<td>run_id: str, local_dir: str, artifact_path: str</td>\n<td>List[str]</td>\n<td>Upload entire directory structure as experiment artifacts</td>\n</tr>\n<tr>\n<td><code>download_artifact</code></td>\n<td>run_id: str, artifact_path: str, local_path: str</td>\n<td>None</td>\n<td>Download an artifact to local filesystem</td>\n</tr>\n<tr>\n<td><code>list_artifacts</code></td>\n<td>run_id: str, path: str</td>\n<td>List[ArtifactInfo]</td>\n<td>List all artifacts under a specific path for a run</td>\n</tr>\n<tr>\n<td><code>get_artifact_uri</code></td>\n<td>run_id: str, artifact_path: str</td>\n<td>str</td>\n<td>Get downloadable URI for a specific artifact</td>\n</tr>\n</tbody></table>\n<p>Each artifact maintains metadata that enables discovery, validation, and efficient storage management:</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>run_id</td>\n<td>str</td>\n<td>Links the artifact to the specific experiment run</td>\n</tr>\n<tr>\n<td>path</td>\n<td>str</td>\n<td>Hierarchical path within the run&#39;s artifact space</td>\n</tr>\n<tr>\n<td>size_bytes</td>\n<td>int</td>\n<td>File size for storage cost tracking and transfer optimization</td>\n</tr>\n<tr>\n<td>checksum</td>\n<td>str</td>\n<td>SHA-256 hash for corruption detection and deduplication</td>\n</tr>\n<tr>\n<td>mime_type</td>\n<td>str</td>\n<td>Content type for proper handling and display</td>\n</tr>\n<tr>\n<td>created_at</td>\n<td>float</td>\n<td>Upload timestamp for version tracking</td>\n</tr>\n<tr>\n<td>metadata</td>\n<td>Dict[str, str]</td>\n<td>Custom key-value pairs for artifact classification</td>\n</tr>\n</tbody></table>\n<p>The artifact store implements <strong>content-addressed storage</strong> using file checksums to eliminate duplicate storage of identical files across runs. When multiple experiments save the same dataset or model checkpoint, only one physical copy exists, dramatically reducing storage costs while maintaining logical separation between experiments.</p>\n<h4 id=\"storage-architecture-decisions\">Storage Architecture Decisions</h4>\n<blockquote>\n<p><strong>Decision: Polyglot Persistence for Experiment Data</strong></p>\n<ul>\n<li><strong>Context</strong>: Experiment tracking requires storing three distinct data types (parameters, metrics, artifacts) with different access patterns, performance requirements, and scalability characteristics.</li>\n<li><strong>Options Considered</strong>: Single database for all data, separate specialized stores, hybrid approach</li>\n<li><strong>Decision</strong>: Use specialized storage systems optimized for each data type</li>\n<li><strong>Rationale</strong>: Parameters need flexible schema and complex queries (document store), metrics need time-series aggregation and fast writes (time-series DB), artifacts need blob storage with CDN capabilities (object store)</li>\n<li><strong>Consequences</strong>: Enables optimal performance for each workload but requires cross-store consistency mechanisms and more complex deployment</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Storage Type</th>\n<th>Parameter Store</th>\n<th>Metric Store</th>\n<th>Artifact Store</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Technology</strong></td>\n<td>PostgreSQL with JSONB</td>\n<td>InfluxDB or TimescaleDB</td>\n<td>S3-compatible object storage</td>\n</tr>\n<tr>\n<td><strong>Strengths</strong></td>\n<td>Complex queries, ACID guarantees, flexible schema</td>\n<td>High write throughput, time-series aggregation, automatic retention</td>\n<td>Unlimited scalability, CDN integration, cost-effective</td>\n</tr>\n<tr>\n<td><strong>Query Patterns</strong></td>\n<td>Search, filter, compare across runs</td>\n<td>Time-range queries, aggregation, downsampling</td>\n<td>Get/put by key, list with prefixes</td>\n</tr>\n<tr>\n<td><strong>Consistency Model</strong></td>\n<td>Strong consistency for metadata</td>\n<td>Eventual consistency acceptable</td>\n<td>Eventual consistency with versioning</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Correlation ID Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Related data (parameters, metrics, artifacts) for a single experiment run must be reliably associated across multiple storage systems</li>\n<li><strong>Options Considered</strong>: Run UUID as primary key, composite keys, foreign key relationships</li>\n<li><strong>Decision</strong>: Use globally unique run ID (UUID) as correlation key across all storage systems</li>\n<li><strong>Rationale</strong>: Simplifies cross-store queries, enables independent scaling of storage systems, provides clear data ownership boundaries</li>\n<li><strong>Consequences</strong>: Requires careful run ID generation and validation, but eliminates complex join operations across heterogeneous stores</li>\n</ul>\n</blockquote>\n<h3 id=\"querying-and-comparison\">Querying and Comparison</h3>\n<p>The experiment tracking component must transform raw experimental data into actionable insights through sophisticated querying and comparison capabilities. Data scientists need to answer questions like &quot;which runs achieved accuracy above 0.95?&quot;, &quot;how do learning rates affect convergence speed?&quot;, and &quot;what changed between my best and worst performing experiments?&quot;</p>\n<h4 id=\"run-search-and-filtering\">Run Search and Filtering</h4>\n<p>The search interface provides <strong>multi-dimensional filtering</strong> across the parameter space, metric outcomes, and execution metadata. This enables data scientists to slice their experimental data along any combination of dimensions to identify patterns and outliers.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>search_runs</code></td>\n<td>experiment_id: str, filter_string: str, order_by: List[str], max_results: int</td>\n<td>List[Run]</td>\n<td>Search runs with SQL-like filter expressions</td>\n</tr>\n<tr>\n<td><code>get_experiments</code></td>\n<td>view_type: str, max_results: int</td>\n<td>List[Experiment]</td>\n<td>List experiments with optional filtering by lifecycle stage</td>\n</tr>\n<tr>\n<td><code>get_run</code></td>\n<td>run_id: str</td>\n<td>Run</td>\n<td>Retrieve complete run information including params, metrics, and metadata</td>\n</tr>\n</tbody></table>\n<p>The filter string supports a <strong>domain-specific query language</strong> that combines SQL-like syntax with ML-specific operators:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>metrics.val_accuracy &gt; 0.9 AND params.learning_rate &lt;= 0.01 AND tags.model_type = 'transformer'</code></pre></div>\n\n<p>Common query patterns include:</p>\n<table>\n<thead>\n<tr>\n<th>Query Pattern</th>\n<th>Example Filter</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Metric Thresholds</strong></td>\n<td><code>metrics.accuracy &gt;= 0.95</code></td>\n<td>Find high-performing runs</td>\n</tr>\n<tr>\n<td><strong>Parameter Ranges</strong></td>\n<td><code>params.learning_rate BETWEEN 0.001 AND 0.1</code></td>\n<td>Analyze hyperparameter sensitivity</td>\n</tr>\n<tr>\n<td><strong>Combination Filters</strong></td>\n<td><code>metrics.val_loss &lt; 0.1 AND params.batch_size &gt; 32</code></td>\n<td>Multi-criteria optimization</td>\n</tr>\n<tr>\n<td><strong>Tag-based Grouping</strong></td>\n<td><code>tags.experiment_type = &#39;baseline&#39;</code></td>\n<td>Categorize experimental variants</td>\n</tr>\n<tr>\n<td><strong>Execution Metadata</strong></td>\n<td><code>status = &#39;FINISHED&#39; AND duration_ms &lt; 3600000</code></td>\n<td>Find fast, successful runs</td>\n</tr>\n</tbody></table>\n<h4 id=\"metric-comparison-and-visualization\">Metric Comparison and Visualization</h4>\n<p>The comparison engine aggregates and analyzes metric time-series data to reveal training patterns and performance differences across runs. This goes beyond simple final metric values to examine <strong>convergence behavior</strong>, <strong>training stability</strong>, and <strong>efficiency characteristics</strong>.</p>\n<table>\n<thead>\n<tr>\n<th>Method Name</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>compare_runs</code></td>\n<td>run_ids: List[str], metric_keys: List[str]</td>\n<td>ComparisonResult</td>\n<td>Generate statistical comparison across specified runs and metrics</td>\n</tr>\n<tr>\n<td><code>get_metric_summary</code></td>\n<td>run_id: str, metric_key: str</td>\n<td>MetricSummary</td>\n<td>Compute aggregation statistics for a metric time-series</td>\n</tr>\n<tr>\n<td><code>get_parallel_coordinates</code></td>\n<td>run_ids: List[str], param_keys: List[str], metric_keys: List[str]</td>\n<td>ParallelCoordinatesData</td>\n<td>Generate parallel coordinates plot data for parameter-outcome analysis</td>\n</tr>\n</tbody></table>\n<p>The comparison system computes multiple statistical measures that capture different aspects of experimental performance:</p>\n<table>\n<thead>\n<tr>\n<th>Comparison Metric</th>\n<th>Calculation</th>\n<th>Insight Provided</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Final Value</strong></td>\n<td>Last recorded metric value</td>\n<td>Ultimate performance achieved</td>\n</tr>\n<tr>\n<td><strong>Best Value</strong></td>\n<td>Maximum (for accuracy) or minimum (for loss)</td>\n<td>Peak performance during training</td>\n</tr>\n<tr>\n<td><strong>Convergence Step</strong></td>\n<td>Step where improvement stopped</td>\n<td>Training efficiency and stability</td>\n</tr>\n<tr>\n<td><strong>Area Under Curve</strong></td>\n<td>Integral of metric over training steps</td>\n<td>Overall training quality</td>\n</tr>\n<tr>\n<td><strong>Stability Score</strong></td>\n<td>Inverse of metric variance in final 10% of training</td>\n<td>Model reliability and robustness</td>\n</tr>\n</tbody></table>\n<h4 id=\"parameter-outcome-correlation-analysis\">Parameter-Outcome Correlation Analysis</h4>\n<p>Understanding how hyperparameters influence experimental outcomes requires sophisticated statistical analysis that goes beyond simple correlation coefficients. The tracking system implements <strong>multidimensional analysis</strong> that can identify complex parameter interactions and sensitivity patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Analysis Type</th>\n<th>Method</th>\n<th>Output</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Sensitivity Analysis</strong></td>\n<td>Partial correlation with other params held constant</td>\n<td>Ranking of parameter importance</td>\n<td>Hyperparameter prioritization</td>\n</tr>\n<tr>\n<td><strong>Interaction Detection</strong></td>\n<td>Two-way ANOVA across parameter combinations</td>\n<td>Significant parameter interactions</td>\n<td>Complex optimization strategies</td>\n</tr>\n<tr>\n<td><strong>Pareto Frontier</strong></td>\n<td>Multi-objective optimization analysis</td>\n<td>Non-dominated parameter combinations</td>\n<td>Trade-off analysis</td>\n</tr>\n<tr>\n<td><strong>Clustering Analysis</strong></td>\n<td>K-means on parameter vectors weighted by outcomes</td>\n<td>Groups of similar high-performing configurations</td>\n<td>Configuration templates</td>\n</tr>\n</tbody></table>\n<h4 id=\"query-optimization-strategies\">Query Optimization Strategies</h4>\n<p>Experiment tracking queries often involve complex joins across parameter, metric, and metadata stores, potentially scanning thousands of runs with millions of metric points. The system implements several optimization strategies to maintain sub-second response times even with large experimental datasets.</p>\n<blockquote>\n<p><strong>Decision: Materialized Views for Common Queries</strong></p>\n<ul>\n<li><strong>Context</strong>: Frequent queries like &quot;best run per experiment&quot; and &quot;parameter distribution analysis&quot; involve expensive aggregations across large datasets</li>\n<li><strong>Options Considered</strong>: Real-time aggregation, pre-computed materialized views, cached query results</li>\n<li><strong>Decision</strong>: Implement materialized views updated through event-driven triggers</li>\n<li><strong>Rationale</strong>: Provides consistent sub-second response for common queries while maintaining data freshness through incremental updates</li>\n<li><strong>Consequences</strong>: Increases storage requirements but dramatically improves user experience for exploratory data analysis</li>\n</ul>\n</blockquote>\n<p>The query optimization layer implements several key strategies:</p>\n<p><strong>Indexed Parameter Search</strong>: Parameters are indexed using <strong>GIN indexes</strong> on JSONB columns in PostgreSQL, enabling fast lookups even for nested parameter structures. The system maintains separate indexes for numeric and string parameters to optimize different query patterns.</p>\n<p><strong>Metric Aggregation Caching</strong>: Frequently accessed metric aggregations (final values, maximums, convergence points) are pre-computed and cached in Redis with <strong>time-based invalidation</strong>. This transforms expensive time-series scans into simple key-value lookups.</p>\n<p><strong>Query Result Pagination</strong>: Large result sets use <strong>cursor-based pagination</strong> to avoid memory exhaustion and provide consistent performance regardless of result set size. Each query returns a continuation token that enables stateless pagination.</p>\n<p><strong>Federated Query Planning</strong>: Queries spanning multiple storage systems use a <strong>cost-based query planner</strong> that determines optimal execution strategies based on estimated data volumes and network costs.</p>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The experiment tracking component embodies several key architectural decisions that influence both its internal design and its integration with other MLOps platform components. These decisions reflect trade-offs between consistency, performance, scalability, and operational complexity.</p>\n<blockquote>\n<p><strong>Decision: Event-Driven Architecture for Component Integration</strong></p>\n<ul>\n<li><strong>Context</strong>: Experiment tracking must notify other components (model registry, pipeline orchestration) about experiment completion, model artifact availability, and performance milestones</li>\n<li><strong>Options Considered</strong>: Synchronous API calls, database polling, event-driven messaging</li>\n<li><strong>Decision</strong>: Implement asynchronous event publishing using the <code>EventCoordinator</code> pattern</li>\n<li><strong>Rationale</strong>: Decouples components, enables independent scaling, supports complex workflows without tight coupling, provides audit trail of system interactions</li>\n<li><strong>Consequences</strong>: Requires eventual consistency handling but enables more resilient and scalable system architecture</li>\n</ul>\n</blockquote>\n<p>The event integration follows a structured pattern where significant experiment tracking events trigger notifications to interested components:</p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Trigger Condition</th>\n<th>Event Payload</th>\n<th>Consuming Components</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>EXPERIMENT_COMPLETED</code></td>\n<td>Run transitions to FINISHED status</td>\n<td>run_id, experiment_id, final_metrics, artifact_paths</td>\n<td>Model Registry, Pipeline Orchestration</td>\n</tr>\n<tr>\n<td><code>run.started</code></td>\n<td>New run begins logging</td>\n<td>run_id, experiment_id, parameters, start_time</td>\n<td>Monitoring dashboards, Resource management</td>\n</tr>\n<tr>\n<td><code>artifact.logged</code></td>\n<td>New artifact uploaded</td>\n<td>run_id, artifact_path, artifact_type, checksum</td>\n<td>Model Registry, Lineage tracking</td>\n</tr>\n<tr>\n<td><code>metric.threshold_exceeded</code></td>\n<td>Metric crosses configured threshold</td>\n<td>run_id, metric_name, threshold_value, current_value</td>\n<td>Auto-promotion workflows, Alert systems</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Immutable Event Log for Audit Trail</strong></p>\n<ul>\n<li><strong>Context</strong>: MLOps platforms require complete audit trails for compliance, debugging, and reproducibility, especially in regulated industries</li>\n<li><strong>Options Considered</strong>: Mutable records with timestamps, immutable append-only log, hybrid approach with versioning</li>\n<li><strong>Decision</strong>: Implement append-only event log where all parameter/metric updates create new timestamped entries rather than modifying existing records</li>\n<li><strong>Rationale</strong>: Provides complete audit trail, enables time-travel queries, supports compliance requirements, simplifies concurrent access patterns</li>\n<li><strong>Consequences</strong>: Increases storage requirements but provides invaluable debugging and compliance capabilities</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Hierarchical Artifact Namespacing</strong></p>\n<ul>\n<li><strong>Context</strong>: ML experiments generate diverse artifacts (models, plots, datasets, configs) that need logical organization and efficient access patterns</li>\n<li><strong>Options Considered</strong>: Flat key-value storage, hierarchical paths with metadata, database-driven catalog</li>\n<li><strong>Decision</strong>: Implement hierarchical path-based namespacing with conventional subdirectories</li>\n<li><strong>Rationale</strong>: Mirrors familiar filesystem semantics, enables efficient prefix-based queries, supports nested organization patterns, integrates naturally with object storage systems</li>\n<li><strong>Consequences</strong>: Provides intuitive organization but requires careful path validation and character encoding handling</li>\n</ul>\n</blockquote>\n<p>The artifact namespace follows conventional ML workflow patterns:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>/{run_id}/\n  ├── models/\n  │   ├── checkpoints/\n  │   │   ├── epoch_001.pt\n  │   │   └── epoch_010.pt\n  │   └── final_model.pkl\n  ├── data/\n  │   ├── train_features.parquet\n  │   └── validation_results.csv\n  ├── plots/\n  │   ├── learning_curves.png\n  │   └── confusion_matrix.png\n  └── configs/\n      ├── model_config.yaml\n      └── training_params.json</code></pre></div>\n\n<blockquote>\n<p><strong>Decision: Pluggable Storage Backend Interface</strong></p>\n<ul>\n<li><strong>Context</strong>: Different organizations have varying requirements for storage technology, compliance, cost optimization, and existing infrastructure integration</li>\n<li><strong>Options Considered</strong>: Fixed storage implementation, configuration-driven backends, pluggable interface with adapters</li>\n<li><strong>Decision</strong>: Define abstract storage interfaces (<code>MetadataStore</code>, <code>ArtifactStore</code>) with pluggable implementations</li>\n<li><strong>Rationale</strong>: Enables deployment flexibility, supports cloud-agnostic deployments, allows optimization for specific workloads, facilitates testing with mock implementations</li>\n<li><strong>Consequences</strong>: Requires careful interface design and adapter maintenance but provides crucial deployment flexibility</li>\n</ul>\n</blockquote>\n<p>The storage abstraction defines minimal interfaces that capture essential operations while allowing implementation-specific optimizations:</p>\n<table>\n<thead>\n<tr>\n<th>Interface</th>\n<th>Core Methods</th>\n<th>Implementation Examples</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MetadataStore</code></td>\n<td>create_table, insert, update, query, get_by_id</td>\n<td>PostgreSQL, MongoDB, DynamoDB</td>\n</tr>\n<tr>\n<td><code>ArtifactStore</code></td>\n<td>put, get, delete, list_keys</td>\n<td>S3, Azure Blob, Google Cloud Storage, MinIO</td>\n</tr>\n<tr>\n<td><code>EventStore</code></td>\n<td>append, read_stream, subscribe</td>\n<td>Kafka, Redis Streams, AWS Kinesis</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>Experiment tracking systems appear deceptively simple but contain numerous subtle complexities that frequently trip up implementers. Understanding these pitfalls helps avoid common mistakes that can compromise data integrity, performance, or usability.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Metric Naming Across Experiments</strong></p>\n<p>Data scientists often use slight variations in metric names (&quot;accuracy&quot;, &quot;val_acc&quot;, &quot;validation_accuracy&quot;) that fragment the metric namespace and break cross-experiment comparisons. This happens because metric names are typically generated programmatically by training scripts without central validation.</p>\n<p>The tracking system should implement <strong>metric name normalization</strong> and <strong>alias resolution</strong> to handle common variations. Maintain a registry of canonical metric names with known aliases, and warn users when logging metrics with new names that are similar to existing ones.</p>\n<p>⚠️ <strong>Pitfall: Logging High-Frequency Metrics Without Batching</strong></p>\n<p>Individual metric logging calls for each training step create enormous overhead when training models with thousands of iterations. This can slow training significantly and overwhelm the storage backend with small write operations.</p>\n<p>Implement <strong>automatic batching</strong> that accumulates metrics in memory and flushes to storage periodically or when batch size thresholds are reached. Provide explicit <code>flush()</code> methods for users who need immediate persistence at specific training milestones.</p>\n<p>⚠️ <strong>Pitfall: Storing Large Artifacts Directly in Metadata Database</strong></p>\n<p>Storing model binaries or large datasets as BLOBs in relational databases causes severe performance degradation, backup failures, and storage cost explosions. This often happens when implementers take the &quot;store everything together&quot; approach for simplicity.</p>\n<p>Always use object storage for artifacts larger than a few kilobytes. Store only artifact metadata (path, checksum, size) in the metadata database and maintain references to the actual object storage locations.</p>\n<p>⚠️ <strong>Pitfall: Missing Pagination for Experiment Queries</strong></p>\n<p>Experiment queries without pagination can return thousands of runs, causing browser crashes, memory exhaustion, and poor user experience. This is especially problematic for long-running projects with extensive experiment histories.</p>\n<p>Implement <strong>cursor-based pagination</strong> with reasonable default page sizes (50-100 runs). Provide total count estimates without full result set computation to avoid expensive COUNT(*) queries on large tables.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Parameter Type Validation</strong></p>\n<p>Storing parameters as strings without type preservation breaks numerical comparisons and range queries. For example, storing learning rates as strings makes &quot;0.1&quot; &gt; &quot;0.01&quot; evaluate to false in lexicographic ordering.</p>\n<p>Implement <strong>type-aware parameter storage</strong> that preserves numeric types, boolean values, and structured data. Use JSON schema validation to ensure parameter values match expected types and ranges before storage.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Concurrent Access Patterns</strong></p>\n<p>Multiple training processes logging to the same run simultaneously can cause race conditions, lost updates, and inconsistent experiment state. This is common in distributed training scenarios or when multiple team members accidentally use the same run ID.</p>\n<p>Implement <strong>optimistic concurrency control</strong> with version vectors or timestamps. Detect conflicting updates and provide clear error messages that help users understand and resolve concurrency issues.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Metadata Storage</strong></td>\n<td>SQLite with JSON columns</td>\n<td>PostgreSQL with JSONB indexing</td>\n</tr>\n<tr>\n<td><strong>Time-Series Storage</strong></td>\n<td>PostgreSQL with TimescaleDB extension</td>\n<td>InfluxDB or Prometheus</td>\n</tr>\n<tr>\n<td><strong>Object Storage</strong></td>\n<td>Local filesystem with organization</td>\n<td>AWS S3 or MinIO</td>\n</tr>\n<tr>\n<td><strong>Event Coordination</strong></td>\n<td>Direct function calls</td>\n<td>Redis Pub/Sub or Apache Kafka</td>\n</tr>\n<tr>\n<td><strong>API Framework</strong></td>\n<td>Flask with SQLAlchemy</td>\n<td>FastAPI with async database drivers</td>\n</tr>\n<tr>\n<td><strong>Serialization</strong></td>\n<td>JSON with custom encoders</td>\n<td>Protocol Buffers or MessagePack</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>experiment_tracking/\n├── __init__.py\n├── api/\n│   ├── __init__.py\n│   ├── runs.py              ← Run logging and retrieval endpoints\n│   ├── experiments.py       ← Experiment management endpoints\n│   └── search.py            ← Query and comparison endpoints\n├── storage/\n│   ├── __init__.py\n│   ├── interfaces.py        ← Abstract storage interfaces\n│   ├── metadata_store.py    ← MetadataStore implementations\n│   ├── artifact_store.py    ← ArtifactStore implementations\n│   └── migrations/          ← Database schema migrations\n├── models/\n│   ├── __init__.py\n│   ├── experiment.py        ← Experiment and Run data models\n│   ├── metric.py            ← Metric and parameter data models\n│   └── artifact.py          ← Artifact metadata models\n├── services/\n│   ├── __init__.py\n│   ├── tracking_service.py  ← Core experiment tracking logic\n│   ├── query_service.py     ← Search and comparison operations\n│   └── event_publisher.py   ← Event coordination logic\n└── utils/\n    ├── __init__.py\n    ├── validation.py        ← Input validation and normalization\n    └── serialization.py     ← Type-aware parameter serialization</code></pre></div>\n\n<h4 id=\"core-data-models-complete-implementation\">Core Data Models (Complete Implementation)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RunStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"RUNNING\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FINISHED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"FINISHED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"FAILED\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    KILLED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"KILLED\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Experiment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    experiment_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    lifecycle_stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"active\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    creation_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.utcnow().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_update_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: datetime.utcnow().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Experiment'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            experiment_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            tags</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">tags </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Run</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    experiment_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: RunStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source_version: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    entry_point: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    user_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, experiment_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source_version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Run'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            run_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            experiment_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">experiment_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RunStatus.</span><span style=\"color:#79B8FF\">RUNNING</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            start_time</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.utcnow().timestamp(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            source_version</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">source_version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetricPoint</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Parameter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    key: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    value_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ArtifactInfo</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    size_bytes: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    checksum: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mime_type: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_at: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"storage-interface-implementations\">Storage Interface Implementations</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetadataStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for storing experiment metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a table with the specified schema.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Insert data and return generated ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, record_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update existing record with new data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, filter_condition: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, order_by: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">              limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Query records with filtering and pagination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_by_id</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, record_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve a single record by its ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ArtifactStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for storing experiment artifacts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">, metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store binary data with optional metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve binary data by key.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Delete data by key.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_keys</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, max_keys: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"List keys with optional prefix filtering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_metadata</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve metadata for a stored object.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Simple PostgreSQL implementation for metadata storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psycopg2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PostgreSQLMetadataStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MetadataStore</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, connection_string: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_string </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_string</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_connection</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_string)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, schema: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate CREATE TABLE SQL from schema dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Execute DDL statement with proper error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add indexes for commonly queried columns (run_id, experiment_id)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> insert</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate INSERT statement with RETURNING clause for ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Serialize complex data types (dicts, lists) to JSON</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute insert and return generated ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle constraint violations and provide meaningful errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, record_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate UPDATE statement with WHERE clause on ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Handle partial updates (only provided fields)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate record exists before attempting update</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> query</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, filter_condition: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, order_by: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">              limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, offset: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse and validate filter_condition for SQL injection safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Build SELECT statement with WHERE, ORDER BY, LIMIT, OFFSET</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute query and convert rows to dictionaries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle JSON deserialization for complex fields</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_by_id</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, record_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Execute SELECT with WHERE id = %s</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return None if no record found, dict if found</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Deserialize JSON fields back to Python objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Simple filesystem implementation for artifact storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> shutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FilesystemArtifactStore</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">ArtifactStore</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, base_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Path(base_path)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_path.mkdir(</span><span style=\"color:#FFAB70\">parents</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">, metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate key format and prevent directory traversal attacks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create directory structure for key path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Write data to file atomically (write to temp, then rename)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute and store checksum for data integrity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Save metadata as separate .meta file alongside data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate key exists and build full file path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Read binary data from file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Optionally verify checksum if metadata exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Handle file not found with appropriate exception</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Remove both data file and metadata file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Clean up empty parent directories</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle missing file gracefully (idempotent operation)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_keys</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, max_keys: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Walk directory tree starting from prefix path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert file paths back to key format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply max_keys limit and return sorted results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Filter out metadata files from results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-tracking-service-skeleton\">Core Tracking Service (Skeleton)</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Experiment, Run, MetricPoint, Parameter, ArtifactInfo</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .storage.interfaces </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> MetadataStore, ArtifactStore</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExperimentTrackingService</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, metadata_store: MetadataStore, artifact_store: ArtifactStore):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metadata_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.artifact_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> artifact_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._initialize_tables()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _initialize_tables</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Define schema for experiments table (id, name, lifecycle_stage, creation_time, tags)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Define schema for runs table (id, experiment_id, status, start_time, end_time, user_id, tags)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Define schema for metrics table (run_id, key, value, step, timestamp)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Define schema for parameters table (run_id, key, value, value_type)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create all tables and indexes for optimal query performance</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_experiment</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Experiment:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate experiment name is unique</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Experiment object with generated ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Insert experiment into metadata store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return created experiment object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_run</span><span style=\"color:#E1E4E8\">(self, experiment_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   source_version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Run:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate experiment exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create Run object with generated ID and RUNNING status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Insert run into metadata store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Publish run.started event for monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return created run object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_param</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: Any) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate run exists and is in RUNNING status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Determine value type for type-aware storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if parameter already exists (warn about overwrites)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Insert parameter into metadata store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle type serialization for complex objects</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_metric</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   timestamp: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate run exists and metric value is numeric</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use current timestamp if not provided</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create MetricPoint object with all required fields</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Insert metric into time-series optimized storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check for metric threshold events and publish if needed</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_artifact</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, local_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, artifact_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> ArtifactInfo:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate run exists and local file exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Read file data and compute checksum</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Determine MIME type from file extension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store file in artifact store using run_id/artifact_path as key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create ArtifactInfo record and store metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Publish artifact.logged event with artifact details</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> finish_run</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, status: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"FINISHED\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate run exists and is currently RUNNING</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Update run status and end_time in metadata store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compute final metrics summary for quick access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Publish EXPERIMENT_COMPLETED event with run summary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Trigger any auto-promotion workflows if configured</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the experiment tracking component, verify the following behavior:</p>\n<ol>\n<li><strong>Create and Run Experiment Test</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">   python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/test_experiment_tracking.py::test_create_experiment_and_run</span></span></code></pre></div>\n<p>   Expected: New experiment and run created with valid UUIDs and timestamps</p>\n<ol start=\"2\">\n<li><strong>Parameter Logging Test</strong>:</li>\n</ol>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">   # Should successfully log various parameter types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   service.log_param(run_id, </span><span style=\"color:#9ECBFF\">\"learning_rate\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.001</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   service.log_param(run_id, </span><span style=\"color:#9ECBFF\">\"batch_size\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">)  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">   service.log_param(run_id, </span><span style=\"color:#9ECBFF\">\"optimizer\"</span><span style=\"color:#E1E4E8\">, {</span><span style=\"color:#9ECBFF\">\"type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"adam\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"beta1\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.9</span><span style=\"color:#E1E4E8\">})</span></span></code></pre></div>\n<p>   Expected: All parameters stored with correct types and retrievable</p>\n<ol start=\"3\">\n<li><p><strong>Metric Logging and Retrieval Test</strong>:\nLog 100 training steps with loss and accuracy metrics, then verify:</p>\n<ul>\n<li>All metric points stored with correct step numbers</li>\n<li>Time-series retrieval returns points in chronological order</li>\n<li>Metric comparison shows expected learning curves</li>\n</ul>\n</li>\n<li><p><strong>Artifact Storage Test</strong>:\nUpload a test model file and configuration, then verify:</p>\n<ul>\n<li>Artifacts appear in list_artifacts output</li>\n<li>Download produces identical file content</li>\n<li>Metadata includes correct file size and checksum</li>\n</ul>\n</li>\n<li><p><strong>Query and Search Test</strong>:\nCreate multiple runs with different parameters, then verify:</p>\n<ul>\n<li>Search filters work correctly (e.g., <code>params.learning_rate &gt; 0.001</code>)</li>\n<li>Run comparison shows parameter and metric differences</li>\n<li>Pagination handles large result sets properly</li>\n</ul>\n</li>\n</ol>\n<p><strong>Signs of Successful Implementation</strong>:</p>\n<ul>\n<li>Sub-second response times for typical queries (&lt; 100 runs)</li>\n<li>No data corruption under concurrent logging from multiple processes  </li>\n<li>Event publishing triggers can be verified in system logs</li>\n<li>Memory usage remains stable during long-running experiments</li>\n</ul>\n<p><strong>Common Issues and Debugging</strong>:</p>\n<ul>\n<li><p><strong>Symptom</strong>: &quot;Run not found&quot; errors during logging</p>\n<ul>\n<li><strong>Cause</strong>: Race condition between run creation and first log call</li>\n<li><strong>Fix</strong>: Add retry logic or ensure run creation completes before logging</li>\n</ul>\n</li>\n<li><p><strong>Symptom</strong>: Query timeouts on large experiments</p>\n<ul>\n<li><strong>Cause</strong>: Missing database indexes or inefficient filter conditions</li>\n<li><strong>Fix</strong>: Add indexes on run_id, experiment_id, and commonly filtered columns</li>\n</ul>\n</li>\n<li><p><strong>Symptom</strong>: Artifact upload failures with large files</p>\n<ul>\n<li><strong>Cause</strong>: Memory exhaustion from loading entire file</li>\n<li><strong>Fix</strong>: Implement streaming upload with chunked transfer</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"model-registry-component\">Model Registry Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section primarily corresponds to Milestone 2 (Model Registry), which focuses on versioning and managing trained models with metadata, stage transitions, lineage tracking, and discovery capabilities.</p>\n</blockquote>\n<h3 id=\"mental-model-software-package-registry\">Mental Model: Software Package Registry</h3>\n<p>Understanding model versioning and lifecycle management is best approached through the familiar analogy of software package registries like npm, PyPI, or Docker Hub. Just as these registries manage software artifacts through their lifecycle, a model registry manages machine learning models as versioned, deployable assets.</p>\n<p>Consider how npm works: developers publish package versions (1.0.0, 1.1.0, 2.0.0) with metadata describing dependencies, compatibility, and usage. Users discover packages through search, examine version history, and install specific versions. Critical packages go through testing stages before promotion to &quot;latest&quot; or &quot;stable&quot; tags. The registry tracks who published what, when, and maintains immutable storage ensuring that version 1.2.3 always contains exactly the same code.</p>\n<p>A model registry operates on identical principles but with ML-specific concerns. Instead of JavaScript libraries, we&#39;re managing trained neural networks, decision trees, or ensemble models. Instead of semantic versioning based on API compatibility, we version based on training data, algorithm changes, or performance improvements. Instead of npm tags like &quot;latest&quot; or &quot;beta&quot;, we have ML-specific stages like &quot;staging&quot;, &quot;production&quot;, or &quot;archived&quot;. The registry tracks model lineage back to training experiments rather than git commits, but the fundamental versioning and lifecycle concepts remain the same.</p>\n<p>This mental model is powerful because it immediately clarifies several design decisions. Just as package registries separate metadata (package.json) from artifacts (the actual code), model registries separate model metadata from the binary model files. Package registries enforce immutability—once published, a version never changes—and model registries must provide the same guarantee for reproducibility. Package registries support multiple simultaneous versions in production (different applications using different library versions), and model registries enable A/B testing by serving multiple model versions simultaneously.</p>\n<p>The key insight is that models are not just files to be stored, but <strong>versioned artifacts with rich metadata, lifecycle stages, and deployment semantics</strong>. This perspective guides every design decision in the model registry component.</p>\n<h3 id=\"version-management-and-stages\">Version Management and Stages</h3>\n<p>Model versioning requires a systematic approach to track changes, coordinate deployments, and maintain production stability. The version management system combines semantic versioning principles with ML-specific stage transitions to create a controlled path from experimental models to production deployments.</p>\n<p><strong>Version Numbering Strategy</strong></p>\n<p>Model versions follow a three-component semantic versioning scheme adapted for ML workflows: MAJOR.MINOR.PATCH. The major version increments when fundamental changes occur—new training data, different algorithms, or incompatible input/output schemas. The minor version increments for improvements that maintain compatibility—hyperparameter tuning, additional training epochs, or feature engineering changes. The patch version increments for metadata updates or bug fixes that don&#39;t affect model behavior.</p>\n<p>This versioning strategy provides immediate insight into compatibility and risk. A change from version 2.1.3 to 2.2.0 suggests performance improvements with maintained compatibility. A jump to 3.0.0 signals potential breaking changes requiring careful testing. Unlike software versioning based on API contracts, ML versioning considers data schemas, performance characteristics, and prediction distributions.</p>\n<table>\n<thead>\n<tr>\n<th>Version Component</th>\n<th>ML-Specific Meaning</th>\n<th>Example Triggers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MAJOR</td>\n<td>Breaking changes to model interface or fundamental algorithm</td>\n<td>New training dataset, schema changes, different model architecture</td>\n</tr>\n<tr>\n<td>MINOR</td>\n<td>Performance improvements maintaining compatibility</td>\n<td>Hyperparameter optimization, additional training data, feature engineering</td>\n</tr>\n<tr>\n<td>PATCH</td>\n<td>Metadata updates without behavioral changes</td>\n<td>Tag updates, description changes, ownership transfers</td>\n</tr>\n</tbody></table>\n<p><strong>Stage-Based Lifecycle Management</strong></p>\n<p>Each model version progresses through defined stages representing different levels of validation and approval. This stage-based approach prevents untested models from reaching production while enabling parallel development of multiple model variants.</p>\n<p>The <strong>Development</strong> stage contains newly registered models undergoing initial validation. Models in this stage are accessible for experimentation but carry no production guarantees. The registry allows rapid iteration, frequent uploads, and experimental comparisons without formal approval processes.</p>\n<p>The <strong>Staging</strong> stage represents models that have passed initial validation and are candidates for production deployment. Promotion from Development to Staging typically requires meeting accuracy thresholds, passing integration tests, and receiving approval from designated reviewers. Models in Staging undergo more rigorous testing including performance benchmarks, data compatibility checks, and shadow deployments.</p>\n<p>The <strong>Production</strong> stage contains models actively serving real traffic. Promotion to Production requires formal approval workflows, often involving multiple stakeholders reviewing performance metrics, business impact analysis, and rollback procedures. Only one model version per model name typically holds Production status at any given time, though A/B testing scenarios may temporarily promote multiple versions.</p>\n<p>The <strong>Archived</strong> stage stores models removed from active use but retained for historical analysis or regulatory compliance. Archived models remain immutable and queryable but are excluded from deployment workflows and discovery interfaces.</p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Purpose</th>\n<th>Promotion Requirements</th>\n<th>Access Control</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Development</td>\n<td>Initial experimentation</td>\n<td>Automatic on registration</td>\n<td>Model owner and team</td>\n</tr>\n<tr>\n<td>Staging</td>\n<td>Pre-production validation</td>\n<td>Accuracy thresholds, reviewer approval</td>\n<td>Extended team, QA personnel</td>\n</tr>\n<tr>\n<td>Production</td>\n<td>Active serving</td>\n<td>Formal approval workflow, performance validation</td>\n<td>Production engineers, designated approvers</td>\n</tr>\n<tr>\n<td>Archived</td>\n<td>Historical retention</td>\n<td>Manual archival or automated policies</td>\n<td>Read-only access for compliance</td>\n</tr>\n</tbody></table>\n<p><strong>Stage Transition Workflows</strong></p>\n<p>Stage transitions implement approval gates ensuring models meet quality and safety requirements before promotion. Each transition type defines specific validation criteria and approval mechanisms.</p>\n<p>Development to Staging transitions require automated validation checks: model artifact integrity, schema compatibility with existing pipelines, and baseline performance metrics. The system executes these checks automatically when promotion is requested, blocking the transition if any validation fails. Additional approvals from designated reviewers may be required based on organizational policies.</p>\n<p>Staging to Production transitions involve more rigorous validation including business stakeholder approval, performance benchmarking against current production models, and verification of rollback procedures. This transition often triggers automated deployment preparation, infrastructure provisioning, and monitoring configuration.</p>\n<p>Emergency rollback procedures enable rapid Production to Staging demotions when models exhibit unexpected behavior in production. These rollbacks bypass normal approval workflows but generate audit events and require post-incident review.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: Stage transitions are operations on model versions, not model names. This allows multiple versions of the same model to exist in different stages simultaneously, enabling gradual migration strategies and emergency rollbacks.</p>\n</blockquote>\n<p><strong>Immutability Guarantees</strong></p>\n<p>Once registered, model versions are immutable to ensure reproducibility and audit compliance. The registry enforces immutability at multiple levels: artifact content, metadata schemas, and version identifiers. This guarantee enables reliable rollbacks, regulatory compliance, and scientific reproducibility.</p>\n<p>Artifact immutability ensures that model version 2.1.3 always contains exactly the same trained weights, regardless of when it&#39;s accessed. The system computes cryptographic hashes of model artifacts during registration and validates these hashes during retrieval, detecting any corruption or tampering.</p>\n<p>Metadata immutability prevents unauthorized changes to model descriptions, performance metrics, or ownership information after registration. While some metadata fields like tags or descriptions might be updateable through controlled workflows, core metadata including training metrics, lineage information, and approval history remains frozen.</p>\n<p>Version identifier immutability guarantees that version numbers are never reused. Once version 2.1.3 is registered, no future model can claim that identifier, even if the original is deleted. This prevents confusion and maintains clear audit trails.</p>\n<blockquote>\n<p><strong>Architecture Decision: Content-Addressable Storage</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to guarantee model artifact immutability while supporting efficient storage and retrieval</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>File-based storage with access controls</li>\n<li>Content-addressable storage with cryptographic hashes</li>\n<li>Database blob storage with versioning</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Content-addressable storage using SHA-256 hashes as keys</li>\n<li><strong>Rationale</strong>: Provides automatic deduplication, tamper detection, and location-independent addressing. Hash-based keys make corruption immediately detectable and enable distributed caching.</li>\n<li><strong>Consequences</strong>: Requires careful garbage collection to avoid orphaned artifacts, but provides strongest immutability guarantees with efficient storage utilization.</li>\n</ul>\n</blockquote>\n<h3 id=\"model-lineage-and-metadata\">Model Lineage and Metadata</h3>\n<p>Model lineage tracking creates an auditable chain of provenance linking deployed models back to their training experiments, data sources, and code versions. This traceability is essential for debugging production issues, ensuring regulatory compliance, and understanding model behavior changes over time.</p>\n<p><strong>Lineage Graph Construction</strong></p>\n<p>The lineage graph represents dependencies between models, experiments, datasets, and code versions as a directed acyclic graph. Each model version serves as a root node with edges pointing to its dependencies: the experiment run that produced it, the training dataset version used, the code commit containing training logic, and any parent models in transfer learning scenarios.</p>\n<p>Experiment lineage links each model to its originating training run through the <code>source_run_id</code> field. This connection enables tracing model behavior back to specific hyperparameters, training metrics, and environmental conditions. The lineage system captures not just the final training run, but any preliminary experiments or hyperparameter sweeps that contributed to the final model configuration.</p>\n<p>Data lineage tracks the training and validation datasets used to create each model version. This includes dataset versions, preprocessing pipelines, and feature engineering transformations. The system records dataset checksums, transformation code hashes, and schema versions to enable precise reproduction of training conditions.</p>\n<p>Code lineage connects models to specific git commits, Docker images, or training environment snapshots. This linkage enables reproducing the exact training environment, including framework versions, system dependencies, and configuration files. The lineage system stores enough information to recreate the training environment, not just identify it.</p>\n<table>\n<thead>\n<tr>\n<th>Lineage Type</th>\n<th>Source</th>\n<th>Destination</th>\n<th>Information Captured</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment</td>\n<td>Model Version</td>\n<td>Training Run</td>\n<td>Run ID, experiment parameters, training metrics</td>\n</tr>\n<tr>\n<td>Data</td>\n<td>Model Version</td>\n<td>Dataset Version</td>\n<td>Dataset hash, schema version, preprocessing pipeline</td>\n</tr>\n<tr>\n<td>Code</td>\n<td>Model Version</td>\n<td>Code Version</td>\n<td>Git commit, Docker image, dependency manifest</td>\n</tr>\n<tr>\n<td>Model</td>\n<td>Model Version</td>\n<td>Parent Model</td>\n<td>Transfer learning base, fine-tuning checkpoint</td>\n</tr>\n</tbody></table>\n<p><strong>Metadata Schema Design</strong></p>\n<p>Model metadata encompasses both technical and business information required for model discovery, validation, and governance. The metadata schema balances completeness with flexibility, providing structured fields for common attributes while supporting extensible custom metadata.</p>\n<p>Core metadata includes model identification, versioning, and ownership information. Technical metadata captures model architecture, framework dependencies, input/output schemas, and performance characteristics. Business metadata includes model purpose, approved use cases, and regulatory classifications.</p>\n<p>Performance metadata records accuracy metrics, latency benchmarks, and resource requirements captured during model training and validation. This information guides deployment decisions and capacity planning. The schema supports both standard metrics (accuracy, F1 score, AUC) and custom metrics specific to the problem domain.</p>\n<p>Schema metadata describes model input and output formats using JSON Schema or Protocol Buffer definitions. This enables automatic compatibility checking, client code generation, and runtime validation. Schema evolution tracking identifies when models introduce breaking changes requiring coordinated client updates.</p>\n<table>\n<thead>\n<tr>\n<th>Metadata Category</th>\n<th>Fields</th>\n<th>Purpose</th>\n<th>Example Values</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Identification</td>\n<td>name, version, id, created_at</td>\n<td>Unique identification and discovery</td>\n<td>&quot;sentiment-classifier&quot;, &quot;2.1.3&quot;, &quot;uuid-123&quot;</td>\n</tr>\n<tr>\n<td>Ownership</td>\n<td>creator, team, maintainer</td>\n<td>Responsibility and access control</td>\n<td>&quot;data-science-team&quot;, &quot;<a href=\"mailto:alice@company.com\">alice@company.com</a>&quot;</td>\n</tr>\n<tr>\n<td>Technical</td>\n<td>framework, architecture, size_mb</td>\n<td>Deployment planning</td>\n<td>&quot;tensorflow&quot;, &quot;transformer&quot;, 1250</td>\n</tr>\n<tr>\n<td>Performance</td>\n<td>accuracy, latency_p99, throughput</td>\n<td>SLA planning and comparison</td>\n<td>0.94, &quot;15ms&quot;, &quot;1000 req/s&quot;</td>\n</tr>\n<tr>\n<td>Schema</td>\n<td>input_schema, output_schema</td>\n<td>Compatibility validation</td>\n<td>JSON Schema definitions</td>\n</tr>\n<tr>\n<td>Business</td>\n<td>purpose, use_cases, compliance</td>\n<td>Governance and approval</td>\n<td>&quot;customer sentiment&quot;, [&quot;marketing&quot;, &quot;support&quot;]</td>\n</tr>\n</tbody></table>\n<p><strong>Lineage Query Capabilities</strong></p>\n<p>The lineage system supports complex queries for impact analysis, compliance auditing, and debugging workflows. Query patterns include forward lineage (what models were derived from this dataset?), backward lineage (what data was used to train this model?), and impact analysis (if this dataset changes, which production models are affected?).</p>\n<p>Forward lineage queries start from data sources or code versions and identify all downstream models that could be affected by changes. These queries are essential for data governance, enabling teams to understand the impact of dataset updates, schema changes, or data quality issues on deployed models.</p>\n<p>Backward lineage queries start from deployed models and trace back to all contributing data sources, experiments, and code versions. These queries support debugging production issues by identifying potential root causes in training data or configuration changes.</p>\n<p>Cross-lineage queries combine multiple lineage types to answer complex questions like &quot;which models in production were trained on data from the compromised dataset collected between March 1-15?&quot; These queries require joining across experiment, data, and deployment records.</p>\n<p>Temporal lineage queries analyze how lineage relationships change over time, supporting questions like &quot;when did we start using the new feature engineering pipeline?&quot; or &quot;which models were affected by the data quality incident last month?&quot;</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: Lineage is not just about storage—it&#39;s about enabling queries that support critical operational workflows. The lineage schema must be optimized for the specific query patterns that model governance requires.</p>\n</blockquote>\n<p><strong>Automated Lineage Capture</strong></p>\n<p>Manual lineage tracking is error-prone and incomplete, so the registry implements automated lineage capture integrated with training workflows. The system uses experiment tracking integration, environment introspection, and policy-based validation to build comprehensive lineage graphs without manual intervention.</p>\n<p>Experiment integration automatically captures lineage when models are registered from training runs. The registration API accepts the source run ID and automatically populates data lineage, code lineage, and experiment metadata. This integration eliminates manual lineage entry while ensuring completeness.</p>\n<p>Environment introspection captures code versions, dependency manifests, and system configurations from training environments. The system can extract git commit hashes, Docker image SHAs, and package version lists from running training jobs. This automated capture ensures lineage accuracy and completeness.</p>\n<p>Policy enforcement validates lineage completeness before allowing model registration or promotion. Teams can define policies requiring specific lineage types (must include data version, code commit, and experiment run) and the system blocks registrations that don&#39;t meet these requirements.</p>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The model registry requires several critical architecture decisions around storage systems, consistency models, and API design. These decisions fundamentally shape the system&#39;s scalability, reliability, and operational characteristics.</p>\n<blockquote>\n<p><strong>Decision: Polyglot Persistence for Metadata and Artifacts</strong></p>\n<ul>\n<li><strong>Context</strong>: Model registry must store both structured metadata (for querying and discovery) and binary artifacts (model files, often gigabytes in size) with different access patterns and consistency requirements</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Single database storing everything (PostgreSQL with large object support)</li>\n<li>Metadata in relational database, artifacts in object storage (S3/GCS)</li>\n<li>Document database for everything (MongoDB GridFS)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Metadata in PostgreSQL, artifacts in S3-compatible object storage</li>\n<li><strong>Rationale</strong>: Relational databases excel at structured queries, joins, and transactions needed for metadata. Object storage provides scalability, durability, and cost-effectiveness for large binary files. Separation allows independent scaling and optimization.</li>\n<li><strong>Consequences</strong>: Enables efficient metadata queries and artifact storage, but requires consistency management across two storage systems and adds complexity for atomic operations spanning both stores.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Storage Option</th>\n<th>Metadata Performance</th>\n<th>Artifact Scalability</th>\n<th>Query Flexibility</th>\n<th>Consistency Guarantees</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single Database</td>\n<td>Good</td>\n<td>Poor (BLOB limits)</td>\n<td>Excellent</td>\n<td>Strong ACID</td>\n</tr>\n<tr>\n<td>Polyglot Persistence</td>\n<td>Excellent</td>\n<td>Excellent</td>\n<td>Excellent</td>\n<td>Eventual (cross-store)</td>\n</tr>\n<tr>\n<td>Document Database</td>\n<td>Good</td>\n<td>Good</td>\n<td>Limited</td>\n<td>Strong (single store)</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Immutable Model Versions with Soft Deletion</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support model lifecycle management while maintaining audit trails and enabling rollbacks to previously deployed models</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Mutable models with version history tracking</li>\n<li>Immutable versions with hard deletion capabilities</li>\n<li>Immutable versions with soft deletion only</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Immutable versions with soft deletion and configurable retention policies</li>\n<li><strong>Rationale</strong>: Immutability provides strongest reproducibility guarantees. Soft deletion maintains audit trails while supporting cleanup. Retention policies balance compliance needs with storage costs.</li>\n<li><strong>Consequences</strong>: Ensures reproducibility and supports compliance, but requires careful garbage collection and may increase storage costs. Prevents accidental data loss but requires explicit cleanup processes.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Stage-Based Model Lifecycle with Approval Gates</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance rapid model iteration with production stability and quality control</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>No formal stages - direct production deployment</li>\n<li>Simple staging/production stages</li>\n<li>Multi-stage lifecycle with approval workflows</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Four-stage lifecycle (Development, Staging, Production, Archived) with configurable approval gates</li>\n<li><strong>Rationale</strong>: Provides structured quality gates while maintaining flexibility. Approval workflows enable governance without blocking experimentation. Multiple stages support diverse organizational policies.</li>\n<li><strong>Consequences</strong>: Enables quality control and compliance, but adds complexity and potential bottlenecks. Requires workflow management but provides audit trails and risk mitigation.</li>\n</ul>\n</blockquote>\n<p><strong>API Design Strategy</strong></p>\n<p>The model registry API design balances RESTful conventions with ML-specific workflows. The API provides both imperative operations (register model, promote version) and declarative state management (desired model state, automated promotion).</p>\n<p>Resource hierarchy follows REST principles with models as top-level resources and versions as sub-resources: <code>/models/{model_name}/versions/{version}</code>. This structure naturally reflects the domain model and enables hierarchical permissions (model-level vs version-level access).</p>\n<p>State transition operations use POST verbs on sub-resources rather than PUT updates to the version resource. This design makes state changes explicit and auditable: <code>POST /models/{name}/versions/{version}/promote</code> rather than <code>PUT /models/{name}/versions/{version}</code> with a new stage field.</p>\n<p>Bulk operations support common workflows like comparing multiple model versions or promoting models across environments. The API provides endpoints like <code>POST /models/compare</code> accepting multiple model references and returning comparative analytics.</p>\n<table>\n<thead>\n<tr>\n<th>API Pattern</th>\n<th>Endpoint</th>\n<th>Purpose</th>\n<th>Request/Response</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Resource Management</td>\n<td><code>GET /models</code></td>\n<td>List and search models</td>\n<td>Query filters → model summaries</td>\n</tr>\n<tr>\n<td>Version Operations</td>\n<td><code>POST /models/{name}/versions</code></td>\n<td>Register new version</td>\n<td>Model artifact + metadata → version ID</td>\n</tr>\n<tr>\n<td>State Transitions</td>\n<td><code>POST /models/{name}/versions/{ver}/promote</code></td>\n<td>Promote to next stage</td>\n<td>Target stage → promotion status</td>\n</tr>\n<tr>\n<td>Lineage Queries</td>\n<td><code>GET /models/{name}/versions/{ver}/lineage</code></td>\n<td>Retrieve lineage graph</td>\n<td>Lineage direction → dependency graph</td>\n</tr>\n<tr>\n<td>Bulk Operations</td>\n<td><code>POST /models/compare</code></td>\n<td>Compare multiple versions</td>\n<td>Model version list → comparison matrix</td>\n</tr>\n</tbody></table>\n<p><strong>Consistency and Concurrency Model</strong></p>\n<p>The model registry implements eventual consistency across storage systems with strong consistency guarantees for critical operations. Metadata operations within PostgreSQL maintain ACID properties, while cross-system operations (metadata + artifacts) use compensation patterns for failure recovery.</p>\n<p>Model registration implements two-phase commit across metadata and artifact stores. The system first uploads artifacts to object storage, then creates metadata records with artifact references. Failure at either stage triggers compensation: orphaned artifacts are garbage collected, and incomplete metadata records are cleaned up.</p>\n<p>Concurrent version registration for the same model uses optimistic locking on the model resource. Version numbers are assigned atomically during metadata insertion, preventing duplicate versions even under concurrent load. Stage transitions use pessimistic locking to prevent conflicting promotions.</p>\n<p>Cache consistency maintains read performance while ensuring fresh data for critical operations. The system uses write-through caching for model metadata and lazy invalidation for artifact references. Time-sensitive operations like stage transitions bypass caches to ensure immediate consistency.</p>\n<blockquote>\n<p><strong>Key Design Insight</strong>: Model registries require different consistency guarantees for different operations. Artifact uploads can tolerate eventual consistency, but stage transitions affecting production deployments need immediate consistency across all system components.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metadata Storage</td>\n<td>SQLite with JSON columns</td>\n<td>PostgreSQL with JSONB indexing</td>\n</tr>\n<tr>\n<td>Artifact Storage</td>\n<td>Local filesystem with checksums</td>\n<td>S3-compatible object storage (MinIO/AWS)</td>\n</tr>\n<tr>\n<td>API Framework</td>\n<td>Flask-RESTful with SQLAlchemy</td>\n<td>FastAPI with async PostgreSQL driver</td>\n</tr>\n<tr>\n<td>Schema Validation</td>\n<td>JSON Schema with jsonschema library</td>\n<td>Pydantic models with automatic OpenAPI</td>\n</tr>\n<tr>\n<td>Lineage Queries</td>\n<td>Recursive SQL CTEs</td>\n<td>Graph database (Neo4j) for complex traversals</td>\n</tr>\n<tr>\n<td>Caching Layer</td>\n<td>In-memory Python dictionaries</td>\n<td>Redis with TTL-based invalidation</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>mlops-platform/\n  model-registry/\n    src/\n      registry/\n        __init__.py\n        models/\n          __init__.py\n          model.py              ← Model and ModelVersion entities\n          lineage.py            ← Lineage tracking classes\n          metadata.py           ← Metadata schema definitions\n        storage/\n          __init__.py\n          metadata_store.py     ← PostgreSQL metadata operations\n          artifact_store.py     ← S3 artifact operations\n          lineage_store.py      ← Lineage graph storage\n        api/\n          __init__.py\n          models_api.py         ← Model CRUD endpoints\n          versions_api.py       ← Version management endpoints\n          lineage_api.py        ← Lineage query endpoints\n          schemas.py            ← API request/response schemas\n        services/\n          __init__.py\n          registry_service.py   ← Core business logic\n          promotion_service.py  ← Stage transition workflows\n          lineage_service.py    ← Lineage analysis logic\n        migrations/\n          001_initial_schema.sql\n          002_add_lineage_tables.sql\n    tests/\n      unit/\n        test_models.py\n        test_storage.py\n        test_services.py\n      integration/\n        test_api_endpoints.py\n        test_lineage_queries.py\n    requirements.txt\n    docker-compose.yml          ← PostgreSQL + MinIO for development</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># storage/metadata_store.py - Complete PostgreSQL metadata storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psycopg2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> psycopg2.extras </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RealDictCursor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelMetadataStore</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"PostgreSQL-based metadata storage with JSONB support for flexible schemas.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, connection_string: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connection_string </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> connection_string</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._init_tables()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _init_tables</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create tables with proper indexes for common query patterns.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_string) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> conn.cursor() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cur:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Models table</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    CREATE TABLE IF NOT EXISTS models (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        name VARCHAR(255) PRIMARY KEY,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        description TEXT,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        tags JSONB DEFAULT '</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">',</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Model versions table</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    CREATE TABLE IF NOT EXISTS model_versions (</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        model_name VARCHAR(255) REFERENCES models(name),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        version VARCHAR(50) NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        stage VARCHAR(20) DEFAULT 'Development',</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        artifact_uri VARCHAR(500) NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        artifact_checksum VARCHAR(64) NOT NULL,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        metadata JSONB DEFAULT '</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">',</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        lineage JSONB DEFAULT '</span><span style=\"color:#79B8FF\">{}</span><span style=\"color:#9ECBFF\">',</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        UNIQUE(model_name, version)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    )</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Create indexes for common queries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"CREATE INDEX IF NOT EXISTS idx_versions_stage ON model_versions(stage)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"CREATE INDEX IF NOT EXISTS idx_versions_metadata ON model_versions USING gin(metadata)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"CREATE INDEX IF NOT EXISTS idx_models_tags ON models USING gin(tags)\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                conn.commit()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_model</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, description: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new model entry.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tags </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> tags </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_string) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> conn.cursor(</span><span style=\"color:#FFAB70\">cursor_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RealDictCursor) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cur:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    INSERT INTO models (name, description, tags)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    VALUES (</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    RETURNING *</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">, (name, description, json.dumps(tags)))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> dict</span><span style=\"color:#E1E4E8\">(cur.fetchone())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_version</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, artifact_uri: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      artifact_checksum: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      lineage: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new model version.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lineage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> lineage </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_string) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> conn.cursor(</span><span style=\"color:#FFAB70\">cursor_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RealDictCursor) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cur:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    INSERT INTO model_versions </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    (model_name, version, artifact_uri, artifact_checksum, metadata, lineage)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    VALUES (</span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">, </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">)</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    RETURNING *</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">, (model_name, version, artifact_uri, artifact_checksum, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     json.dumps(metadata), json.dumps(lineage)))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> dict</span><span style=\"color:#E1E4E8\">(cur.fetchone())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_version_stage</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, new_stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update model version stage with optimistic concurrency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_string) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> conn.cursor() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cur:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    UPDATE model_versions </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    SET stage = </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    WHERE model_name = </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> AND version = </span><span style=\"color:#79B8FF\">%s</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">, (new_stage, model_name, version))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> cur.rowcount </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> search_models</span><span style=\"color:#E1E4E8\">(self, name_filter: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, stage: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, limit: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Search models with flexible filtering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        conditions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        params </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> name_filter:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conditions.append(</span><span style=\"color:#9ECBFF\">\"m.name ILIKE </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            params.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"%</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">name_filter</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">%\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> stage:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conditions.append(</span><span style=\"color:#9ECBFF\">\"v.stage = </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            params.append(stage)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> tags:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> tags.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                conditions.append(</span><span style=\"color:#9ECBFF\">\"m.tags->></span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\"> = </span><span style=\"color:#79B8FF\">%s</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                params.extend([key, value])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        where_clause </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \" AND \"</span><span style=\"color:#E1E4E8\">.join(conditions) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> conditions </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"1=1\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#E1E4E8\"> psycopg2.connect(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connection_string) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> conn:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#E1E4E8\"> conn.cursor(</span><span style=\"color:#FFAB70\">cursor_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RealDictCursor) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> cur:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cur.execute(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    SELECT DISTINCT m.*, v.version, v.stage</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    FROM models m</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    LEFT JOIN model_versions v ON m.name = v.model_name</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    WHERE </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">where_clause</span><span style=\"color:#79B8FF\">}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    ORDER BY m.created_at DESC</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    LIMIT %s</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"\"\"</span><span style=\"color:#E1E4E8\">, params </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> [limit])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">(row) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> row </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> cur.fetchall()]</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># storage/artifact_store.py - Complete S3-compatible artifact storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> boto3</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> botocore.exceptions </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ClientError</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any, BinaryIO</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelArtifactStore</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"S3-compatible storage for model artifacts with checksum validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, bucket_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint_url: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 aws_access_key_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, aws_secret_access_key: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.bucket_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> bucket_name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Support both AWS S3 and MinIO</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        session </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> boto3.Session()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.s3_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> session.client(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            's3'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            endpoint_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">endpoint_url,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            aws_access_key_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">aws_access_key_id,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            aws_secret_access_key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">aws_secret_access_key</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._ensure_bucket_exists()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _ensure_bucket_exists</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create bucket if it doesn't exist.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.s3_client.head_bucket(</span><span style=\"color:#FFAB70\">Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> ClientError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> e.response[</span><span style=\"color:#9ECBFF\">'Error'</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'Code'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '404'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.s3_client.create_bucket(</span><span style=\"color:#FFAB70\">Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _compute_checksum</span><span style=\"color:#E1E4E8\">(self, data: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute SHA-256 checksum for data integrity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> hashlib.sha256(data).hexdigest()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put_artifact</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">, metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store artifact and return its checksum.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        checksum </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._compute_checksum(data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add checksum to metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata[</span><span style=\"color:#9ECBFF\">'checksum'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> checksum</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata[</span><span style=\"color:#9ECBFF\">'size'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(data))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.s3_client.put_object(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                Key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">key,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                Body</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">data,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                Metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">metadata</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> checksum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> ClientError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to store artifact </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put_file</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, file_path: Path, metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store file artifact and return its checksum.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(file_path, </span><span style=\"color:#9ECBFF\">'rb'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> f.read()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata[</span><span style=\"color:#9ECBFF\">'original_filename'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> file_path.name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        metadata[</span><span style=\"color:#9ECBFF\">'content_type'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._guess_content_type(file_path)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.put_artifact(key, data, metadata)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_artifact</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, validate_checksum: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bytes</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve artifact with optional checksum validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            response </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.s3_client.get_object(</span><span style=\"color:#FFAB70\">Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name, </span><span style=\"color:#FFAB70\">Key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> response[</span><span style=\"color:#9ECBFF\">'Body'</span><span style=\"color:#E1E4E8\">].read()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> validate_checksum </span><span style=\"color:#F97583\">and</span><span style=\"color:#9ECBFF\"> 'checksum'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> response.get(</span><span style=\"color:#9ECBFF\">'Metadata'</span><span style=\"color:#E1E4E8\">, {}):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                expected_checksum </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> response[</span><span style=\"color:#9ECBFF\">'Metadata'</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'checksum'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                actual_checksum </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._compute_checksum(data)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> expected_checksum </span><span style=\"color:#F97583\">!=</span><span style=\"color:#E1E4E8\"> actual_checksum:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    raise</span><span style=\"color:#79B8FF\"> ValueError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Checksum mismatch for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: expected </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">expected_checksum</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, got </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">actual_checksum</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> data</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> ClientError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> e.response[</span><span style=\"color:#9ECBFF\">'Error'</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'Code'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'NoSuchKey'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                raise</span><span style=\"color:#79B8FF\"> FileNotFoundError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Artifact not found: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to retrieve artifact </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">key</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> download_file</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, local_path: Path, validate_checksum: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Download artifact to local file.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_artifact(key, validate_checksum)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        local_path.parent.mkdir(</span><span style=\"color:#FFAB70\">parents</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">exist_ok</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(local_path, </span><span style=\"color:#9ECBFF\">'wb'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            f.write(data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> artifact_exists</span><span style=\"color:#E1E4E8\">(self, key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if artifact exists.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.s3_client.head_object(</span><span style=\"color:#FFAB70\">Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name, </span><span style=\"color:#FFAB70\">Key</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> ClientError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> e.response[</span><span style=\"color:#9ECBFF\">'Error'</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">'Code'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> '404'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> list_artifacts</span><span style=\"color:#E1E4E8\">(self, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"List artifacts with metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            response </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.s3_client.list_objects_v2(</span><span style=\"color:#FFAB70\">Bucket</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.bucket_name, </span><span style=\"color:#FFAB70\">Prefix</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">prefix)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'key'</span><span style=\"color:#E1E4E8\">: obj[</span><span style=\"color:#9ECBFF\">'Key'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'size'</span><span style=\"color:#E1E4E8\">: obj[</span><span style=\"color:#9ECBFF\">'Size'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'last_modified'</span><span style=\"color:#E1E4E8\">: obj[</span><span style=\"color:#9ECBFF\">'LastModified'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'etag'</span><span style=\"color:#E1E4E8\">: obj[</span><span style=\"color:#9ECBFF\">'ETag'</span><span style=\"color:#E1E4E8\">].strip(</span><span style=\"color:#9ECBFF\">'\"'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                for</span><span style=\"color:#E1E4E8\"> obj </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> response.get(</span><span style=\"color:#9ECBFF\">'Contents'</span><span style=\"color:#E1E4E8\">, [])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\"> ClientError </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to list artifacts: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _guess_content_type</span><span style=\"color:#E1E4E8\">(self, file_path: Path) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Guess content type from file extension.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        suffix </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> file_path.suffix.lower()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        content_types </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.pkl'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/octet-stream'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.joblib'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/octet-stream'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.pt'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/octet-stream'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.pth'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/octet-stream'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.h5'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/octet-stream'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.pb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/octet-stream'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.onnx'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/octet-stream'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.json'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/json'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.yaml'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/x-yaml'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            '.yml'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/x-yaml'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> content_types.get(suffix, </span><span style=\"color:#9ECBFF\">'application/octet-stream'</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># services/registry_service.py - Core model registry business logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> pathlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Path</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..models.model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Model, ModelVersion, ModelStage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..storage.metadata_store </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ModelMetadataStore</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..storage.artifact_store </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ModelArtifactStore</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelRegistryService</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Core business logic for model registration and lifecycle management.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, metadata_store: ModelMetadataStore, artifact_store: ModelArtifactStore):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metadata_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.artifact_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> artifact_store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_model_version</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              artifact_path: Path, run_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ModelVersion:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Register a new model version with artifact upload and lineage tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        This implements the two-phase commit pattern for consistency across storage systems.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate inputs - check model_name format, version format, artifact_path exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate artifact key using model_name/version/filename pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Upload artifact to storage and get checksum - handle upload failures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create metadata record with artifact reference - handle database failures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If run_id provided, fetch lineage info from experiment tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: On any failure after artifact upload, implement cleanup (delete orphaned artifact)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return populated ModelVersion object</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use try/except with cleanup in except block</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Artifact key format: f\"models/{model_name}/versions/{version}/{artifact_path.name}\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> promote_model_version</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             target_stage: ModelStage, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             approval_metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Promote model version to target stage with validation and approval tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Implements stage transition validation and approval workflow.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Fetch current model version and validate it exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate stage transition is allowed (Development->Staging->Production)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check if approval is required for this transition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If promoting to Production, demote current Production version to Archived</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update version stage in metadata store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Record approval metadata and transition timestamp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return success/failure boolean</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use database transactions for atomic stage updates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Stage transition rules: Dev->Staging->Prod->Archived</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Only one version per model can be in Production simultaneously</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> search_models</span><span style=\"color:#E1E4E8\">(self, query: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     stage: Optional[ModelStage] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     include_versions: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> List[Model]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Search models with flexible filtering and optional version inclusion.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Supports text search, stage filtering, and tag-based queries.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build search criteria from parameters - handle None values gracefully  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Query metadata store with constructed filters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If include_versions is True, fetch all versions for each model</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Convert database results to Model domain objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Apply any additional filtering that can't be done at database level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Sort results by relevance (text match quality, creation date)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return list of Model objects with populated versions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use database ILIKE for case-insensitive text search</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: JSONB queries for tag filtering in PostgreSQL</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_model_lineage</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                         depth: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Build lineage graph showing model dependencies up to specified depth.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns graph structure with nodes (experiments, datasets, models) and edges.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Fetch model version and validate it exists</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract lineage metadata from model version record</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Build graph structure with model version as root node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each dependency type (experiment, dataset, parent_model), add nodes and edges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Recursively follow parent model references up to depth limit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Query experiment tracking for experiment run details if run_id present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Format as graph structure: {nodes: [], edges: [], metadata: {}}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use breadth-first search to control depth</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Track visited nodes to prevent cycles</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Node format: {id, type, name, metadata}, Edge format: {source, target, relationship}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_model_versions</span><span style=\"color:#E1E4E8\">(self, version_refs: List[tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              metrics: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate comparison matrix for multiple model versions across specified metrics.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns statistical comparison including performance deltas and significance tests.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate all version references exist and are accessible</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Fetch metadata for all specified versions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If metrics not specified, find common metrics across all versions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Extract metric values and organize into comparison matrix</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate statistical comparisons (mean, std, relative differences)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Identify best/worst performing versions per metric</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return structured comparison with summary statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle missing metrics gracefully (some models may not have all metrics)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Return format: {versions: [], metrics: [], matrix: [][], summary: {}}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li><strong>Database Connections</strong>: Use connection pooling with <code>psycopg2.pool</code> for production deployments to handle concurrent requests efficiently</li>\n<li><strong>Object Storage</strong>: The <code>boto3</code> library works with both AWS S3 and MinIO - use environment variables for configuration flexibility</li>\n<li><strong>JSON Handling</strong>: PostgreSQL JSONB columns support efficient querying - use <code>@&gt;</code> operator for containment queries and GIN indexes for performance</li>\n<li><strong>Error Handling</strong>: Distinguish between retriable errors (network timeouts) and permanent failures (checksum mismatches) using specific exception types</li>\n<li><strong>Async Operations</strong>: Consider <code>asyncpg</code> and <code>aioboto3</code> for high-concurrency deployments, especially for artifact upload/download operations</li>\n<li><strong>Schema Validation</strong>: Use Pydantic models for API request/response validation and automatic OpenAPI documentation generation</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the Model Registry component, verify the following behavior:</p>\n<p><strong>Testing Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Run unit tests for core services</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_registry_service.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test metadata storage operations</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_metadata_store.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test artifact storage with MinIO</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">docker-compose</span><span style=\"color:#9ECBFF\"> up</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#6A737D\">  # Start PostgreSQL + MinIO</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/integration/test_model_registration.py</span><span style=\"color:#79B8FF\"> -v</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ol>\n<li><strong>Model Registration</strong>: Upload a test model file and verify it appears in both metadata database and object storage with correct checksums</li>\n<li><strong>Version Management</strong>: Register multiple versions of the same model and verify version numbering and immutability</li>\n<li><strong>Stage Transitions</strong>: Promote a model through Development → Staging → Production stages and verify only one Production version exists</li>\n<li><strong>Lineage Tracking</strong>: Register a model with run_id and verify lineage information is captured and queryable</li>\n<li><strong>Search Functionality</strong>: Search models by name, stage, and tags - verify filtering works correctly</li>\n</ol>\n<p><strong>Manual Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Check metadata in PostgreSQL</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">psql</span><span style=\"color:#79B8FF\"> -h</span><span style=\"color:#9ECBFF\"> localhost</span><span style=\"color:#79B8FF\"> -U</span><span style=\"color:#9ECBFF\"> postgres</span><span style=\"color:#79B8FF\"> -d</span><span style=\"color:#9ECBFF\"> mlops</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"SELECT * FROM model_versions ORDER BY created_at DESC LIMIT 5\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check artifacts in MinIO (using mc client)</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">mc</span><span style=\"color:#9ECBFF\"> ls</span><span style=\"color:#9ECBFF\"> local/models/</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">mc</span><span style=\"color:#9ECBFF\"> cat</span><span style=\"color:#9ECBFF\"> local/models/test-model/versions/1.0.0/model.pkl</span><span style=\"color:#F97583\"> |</span><span style=\"color:#B392F0\"> head</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#79B8FF\"> 100</span></span></code></pre></div>\n\n<p><strong>Signs of Problems:</strong></p>\n<ul>\n<li><strong>Checksum Mismatches</strong>: Usually indicates file corruption during upload/download or storage system issues</li>\n<li><strong>Orphaned Artifacts</strong>: Artifacts in object storage without metadata records suggest transaction rollback failures</li>\n<li><strong>Stage Transition Failures</strong>: Check approval workflow configuration and database constraints</li>\n<li><strong>Lineage Query Timeouts</strong>: May need database indexes on lineage JSONB columns or query optimization</li>\n</ul>\n<h2 id=\"training-pipeline-orchestration\">Training Pipeline Orchestration</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section primarily corresponds to Milestone 3 (Training Pipeline), which focuses on orchestrating training workflows with DAG-based execution, resource management, and fault tolerance.</p>\n</blockquote>\n<h3 id=\"mental-model-assembly-line\">Mental Model: Assembly Line</h3>\n<p>Think of training pipeline orchestration like a sophisticated manufacturing assembly line. In a traditional assembly line, raw materials flow through a sequence of workstations, where each station performs a specific operation and passes the result to the next station. Workers at each station need specific tools, workspace, and skills to perform their tasks. The assembly line manager ensures materials flow smoothly, workers have the resources they need, and if one station breaks down, the entire line doesn&#39;t grind to a halt.</p>\n<p>Training pipeline orchestration operates on the same principles but for machine learning workflows. Instead of physical materials, we have datasets, model artifacts, and intermediate computations flowing through the pipeline. Instead of workstations, we have pipeline steps like data preprocessing, feature engineering, model training, and evaluation. Instead of workers needing tools, our steps need computational resources like CPU cores, memory, and GPUs. And just like an assembly line manager, our orchestrator ensures data flows between steps, resources are allocated efficiently, and failures are handled gracefully.</p>\n<p>The key insight from the assembly line analogy is that orchestration is fundamentally about <strong>dependency management</strong> and <strong>resource coordination</strong>. A step cannot begin until its dependencies are satisfied (materials arrive from upstream), and it cannot proceed without adequate resources (workspace and tools). The orchestrator&#39;s job is to schedule work optimally while respecting these constraints.</p>\n<p>However, ML pipelines have additional complexities that manufacturing assembly lines don&#39;t face. Steps may need to process data in parallel across multiple machines, some steps may fail and need to be retried, and the computational requirements can vary dramatically between steps. This is where our orchestrator becomes more sophisticated than a simple assembly line manager.</p>\n<h3 id=\"dag-definition-and-execution\">DAG Definition and Execution</h3>\n<p>At the heart of pipeline orchestration lies the <strong>directed acyclic graph (DAG)</strong> representation of training workflows. A DAG captures the dependencies between pipeline steps while ensuring we never have circular dependencies that would create deadlocks. Each node in the DAG represents a computational step, and each edge represents a data dependency where the output of one step becomes the input to another.</p>\n<p>The DAG definition starts with individual pipeline steps, which are the atomic units of computation in our system. Each step encapsulates a specific piece of ML logic like data validation, feature transformation, model training, or evaluation. Steps declare their input and output schemas, resource requirements, and the container image needed to execute their code.</p>\n<table>\n<thead>\n<tr>\n<th>Step Attribute</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>step_id</td>\n<td>str</td>\n<td>Unique identifier for this step within the pipeline</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable name for the step</td>\n</tr>\n<tr>\n<td>container_image</td>\n<td>str</td>\n<td>Docker image containing the step&#39;s execution environment</td>\n</tr>\n<tr>\n<td>command</td>\n<td>List[str]</td>\n<td>Command and arguments to execute within the container</td>\n</tr>\n<tr>\n<td>inputs</td>\n<td>Dict[str, InputSpec]</td>\n<td>Declared input parameters and their types</td>\n</tr>\n<tr>\n<td>outputs</td>\n<td>Dict[str, OutputSpec]</td>\n<td>Declared output artifacts and their types</td>\n</tr>\n<tr>\n<td>resource_requirements</td>\n<td>ResourceSpec</td>\n<td>CPU, memory, GPU, and storage requirements</td>\n</tr>\n<tr>\n<td>retry_policy</td>\n<td>RetryPolicy</td>\n<td>Configuration for handling step failures</td>\n</tr>\n<tr>\n<td>timeout_seconds</td>\n<td>Optional[int]</td>\n<td>Maximum execution time before step is killed</td>\n</tr>\n<tr>\n<td>environment_variables</td>\n<td>Dict[str, str]</td>\n<td>Environment variables passed to the container</td>\n</tr>\n</tbody></table>\n<p>The <code>InputSpec</code> and <code>OutputSpec</code> types define the data contracts between steps. Input specifications declare what data a step expects to receive, including the data type, validation rules, and whether the input is required or optional. Output specifications declare what artifacts a step will produce upon successful completion.</p>\n<table>\n<thead>\n<tr>\n<th>InputSpec Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>input_type</td>\n<td>str</td>\n<td>Data type: &#39;dataset&#39;, &#39;model&#39;, &#39;parameter&#39;, &#39;artifact&#39;</td>\n</tr>\n<tr>\n<td>validation_schema</td>\n<td>Optional[Dict]</td>\n<td>JSON schema for validating input data structure</td>\n</tr>\n<tr>\n<td>required</td>\n<td>bool</td>\n<td>Whether this input must be provided for step to execute</td>\n</tr>\n<tr>\n<td>default_value</td>\n<td>Optional[Any]</td>\n<td>Default value used when input is not required and not provided</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th>OutputSpec Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>output_type</td>\n<td>str</td>\n<td>Type of artifact produced: &#39;dataset&#39;, &#39;model&#39;, &#39;metrics&#39;, &#39;artifact&#39;</td>\n</tr>\n<tr>\n<td>path_template</td>\n<td>str</td>\n<td>Template for where the output artifact will be stored</td>\n</tr>\n<tr>\n<td>metadata_schema</td>\n<td>Optional[Dict]</td>\n<td>Expected structure of output metadata</td>\n</tr>\n</tbody></table>\n<p>Pipeline definitions combine individual steps into a workflow by specifying the data flow connections between them. The pipeline definition is essentially a blueprint that the orchestrator uses to construct the execution DAG at runtime.</p>\n<table>\n<thead>\n<tr>\n<th>Pipeline Attribute</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>pipeline_id</td>\n<td>str</td>\n<td>Unique identifier for this pipeline definition</td>\n</tr>\n<tr>\n<td>version</td>\n<td>str</td>\n<td>Semantic version of the pipeline definition</td>\n</tr>\n<tr>\n<td>name</td>\n<td>str</td>\n<td>Human-readable pipeline name</td>\n</tr>\n<tr>\n<td>description</td>\n<td>str</td>\n<td>Documentation describing the pipeline&#39;s purpose</td>\n</tr>\n<tr>\n<td>steps</td>\n<td>Dict[str, Step]</td>\n<td>All steps in the pipeline keyed by step_id</td>\n</tr>\n<tr>\n<td>step_dependencies</td>\n<td>Dict[str, List[str]]</td>\n<td>Maps each step to its dependency steps</td>\n</tr>\n<tr>\n<td>data_flow</td>\n<td>Dict[str, Dict[str, str]]</td>\n<td>Maps step outputs to downstream step inputs</td>\n</tr>\n<tr>\n<td>global_parameters</td>\n<td>Dict[str, Any]</td>\n<td>Pipeline-level parameters available to all steps</td>\n</tr>\n<tr>\n<td>default_resources</td>\n<td>ResourceSpec</td>\n<td>Default resource allocation for steps that don&#39;t specify requirements</td>\n</tr>\n</tbody></table>\n<p>The <code>data_flow</code> mapping is crucial for understanding how information moves through the pipeline. Each entry specifies that a particular output from one step should be passed as input to another step. For example, <code>data_flow[&quot;preprocessing&quot;][&quot;dataset&quot;] = &quot;training.input_data&quot;</code> means the &quot;dataset&quot; output from the &quot;preprocessing&quot; step becomes the &quot;input_data&quot; input for the &quot;training&quot; step.</p>\n<blockquote>\n<p><strong>Key Design Principle</strong>: Data flow connections are explicit and declarative. Steps cannot access arbitrary outputs from other steps - they can only access data that is explicitly connected through the data flow specification. This ensures pipeline behavior is predictable and makes it easier to reason about data lineage.</p>\n</blockquote>\n<p>The orchestrator executes the pipeline by constructing an execution plan from the DAG definition. This involves several phases: dependency analysis, topological sorting, resource planning, and step scheduling.</p>\n<p><strong>Dependency Analysis Algorithm:</strong></p>\n<ol>\n<li>The orchestrator parses the <code>step_dependencies</code> and <code>data_flow</code> mappings to build a complete dependency graph</li>\n<li>It validates that the graph is acyclic by performing a depth-first search and checking for back edges</li>\n<li>It verifies that all data flow connections are valid by checking that output specifications from upstream steps match input specifications from downstream steps</li>\n<li>It identifies pipeline inputs (steps with no dependencies) and outputs (step outputs not consumed by any other step)</li>\n<li>It calculates the transitive closure of dependencies to determine which steps can potentially run in parallel</li>\n</ol>\n<p><strong>Topological Sorting for Execution Order:</strong></p>\n<ol>\n<li>Initialize a queue with all steps that have no unfulfilled dependencies (pipeline inputs)</li>\n<li>While the queue is not empty, remove a step and add it to the execution plan</li>\n<li>For each step that depends on the completed step, decrement its dependency count</li>\n<li>If any step&#39;s dependency count reaches zero, add it to the queue</li>\n<li>If the execution plan contains fewer steps than the original DAG, report a circular dependency error</li>\n</ol>\n<p><strong>Parallel Execution Identification:</strong></p>\n<p>The orchestrator identifies opportunities for parallel execution by analyzing the dependency structure. Steps that don&#39;t depend on each other (directly or transitively) can execute simultaneously, subject to resource constraints.</p>\n<table>\n<thead>\n<tr>\n<th>Execution Phase</th>\n<th>Description</th>\n<th>Steps Involved</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Independent Parallel</td>\n<td>Steps with no dependencies between them</td>\n<td>Data ingestion, parameter validation, environment setup</td>\n</tr>\n<tr>\n<td>Sequential Dependencies</td>\n<td>Steps that must run in order due to data flow</td>\n<td>Preprocessing → Training → Evaluation</td>\n</tr>\n<tr>\n<td>Fan-out Parallel</td>\n<td>Multiple steps consuming output from a single upstream step</td>\n<td>Training multiple model variants from same preprocessed data</td>\n</tr>\n<tr>\n<td>Fan-in Dependencies</td>\n<td>Single step consuming outputs from multiple upstream steps</td>\n<td>Model ensemble that combines predictions from multiple models</td>\n</tr>\n</tbody></table>\n<p><strong>Data Passing Between Steps:</strong></p>\n<p>When a step completes successfully, the orchestrator handles transferring its outputs to the appropriate downstream steps. This process involves artifact storage, metadata tracking, and input validation.</p>\n<ol>\n<li><p><strong>Artifact Storage</strong>: The orchestrator uploads step outputs to the configured artifact store using a standardized path structure: <code>pipelines/{pipeline_id}/runs/{run_id}/steps/{step_id}/outputs/{output_name}</code></p>\n</li>\n<li><p><strong>Metadata Registration</strong>: Each output artifact is registered with metadata including checksum, size, creation timestamp, and the step that produced it</p>\n</li>\n<li><p><strong>Input Preparation</strong>: For downstream steps, the orchestrator downloads required input artifacts to a local staging area and validates them against the step&#39;s input specifications</p>\n</li>\n<li><p><strong>Environment Variable Injection</strong>: Input artifact paths and metadata are made available to the step through environment variables following a naming convention: <code>MLOPS_INPUT_{INPUT_NAME}_PATH</code> and <code>MLOPS_INPUT_{INPUT_NAME}_METADATA</code></p>\n</li>\n</ol>\n<blockquote>\n<p><strong>Critical Implementation Detail</strong>: The orchestrator never passes data directly between step containers. All data exchange happens through the persistent artifact store, which provides durability guarantees and enables recovery from failures. This design trades some performance for reliability and debuggability.</p>\n</blockquote>\n<h3 id=\"resource-allocation-and-scheduling\">Resource Allocation and Scheduling</h3>\n<p>Effective resource management is essential for running training pipelines efficiently and cost-effectively. The orchestrator must allocate computational resources (CPU, memory, GPU, storage) to pipeline steps while respecting cluster capacity constraints and optimizing for throughput and cost.</p>\n<p>The foundation of resource management is the <code>ResourceSpec</code> type, which allows pipeline authors to declare the computational requirements for each step:</p>\n<table>\n<thead>\n<tr>\n<th>ResourceSpec Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>cpu_cores</td>\n<td>float</td>\n<td>Number of CPU cores (fractional values allowed)</td>\n</tr>\n<tr>\n<td>memory_gb</td>\n<td>float</td>\n<td>Amount of RAM in gigabytes</td>\n</tr>\n<tr>\n<td>gpu_count</td>\n<td>int</td>\n<td>Number of GPU devices required</td>\n</tr>\n<tr>\n<td>gpu_type</td>\n<td>Optional[str]</td>\n<td>Specific GPU model if required (e.g., &#39;V100&#39;, &#39;A100&#39;)</td>\n</tr>\n<tr>\n<td>storage_gb</td>\n<td>float</td>\n<td>Temporary storage space in gigabytes</td>\n</tr>\n<tr>\n<td>max_duration_hours</td>\n<td>Optional[float]</td>\n<td>Maximum execution time for resource reservation</td>\n</tr>\n<tr>\n<td>preemptible</td>\n<td>bool</td>\n<td>Whether this step can use preemptible/spot instances</td>\n</tr>\n<tr>\n<td>node_selector</td>\n<td>Dict[str, str]</td>\n<td>Key-value pairs for node selection (e.g., zone, instance type)</td>\n</tr>\n</tbody></table>\n<p>The orchestrator implements a <strong>multi-level resource scheduling</strong> approach that considers both immediate availability and longer-term resource optimization:</p>\n<p><strong>Level 1: Admission Control</strong></p>\n<p>Before starting pipeline execution, the orchestrator performs admission control to determine if the pipeline can be feasibly executed given current cluster state and resource reservations.</p>\n<ol>\n<li>Calculate the total resource requirements across all pipeline steps</li>\n<li>Check if peak resource usage (when independent steps run in parallel) exceeds cluster capacity</li>\n<li>Estimate execution cost based on resource requirements and current pricing</li>\n<li>If admission control fails, queue the pipeline execution with a priority score based on user quotas and historical usage</li>\n</ol>\n<p><strong>Level 2: Step-Level Scheduling</strong></p>\n<p>When a step becomes eligible for execution (all dependencies satisfied), the orchestrator schedules it on available cluster resources:</p>\n<ol>\n<li><strong>Resource Matching</strong>: Find nodes that have sufficient CPU, memory, GPU, and storage capacity for the step</li>\n<li><strong>Affinity Scheduling</strong>: Prefer nodes that already have the step&#39;s container image cached to reduce startup time</li>\n<li><strong>Data Locality</strong>: Consider proximity to input artifacts stored in distributed storage systems</li>\n<li><strong>Cost Optimization</strong>: For non-urgent steps, prefer cheaper preemptible instances when available</li>\n</ol>\n<p><strong>Level 3: Dynamic Resource Adjustment</strong></p>\n<p>During step execution, the orchestrator monitors resource usage and can make dynamic adjustments:</p>\n<table>\n<thead>\n<tr>\n<th>Adjustment Type</th>\n<th>Trigger Condition</th>\n<th>Action Taken</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Vertical Scaling</td>\n<td>Memory usage exceeds 80% of allocation</td>\n<td>Increase memory allocation if node capacity allows</td>\n</tr>\n<tr>\n<td>Early Termination</td>\n<td>Step exceeds maximum duration</td>\n<td>Kill step and mark as failed with timeout reason</td>\n</tr>\n<tr>\n<td>Resource Reclamation</td>\n<td>Step uses significantly less than allocated</td>\n<td>Release unused resources for other waiting steps</td>\n</tr>\n<tr>\n<td>Preemption Handling</td>\n<td>Spot instance receives preemption notice</td>\n<td>Checkpoint step state and migrate to different node</td>\n</tr>\n</tbody></table>\n<p><strong>Containerization and Isolation:</strong></p>\n<p>Each pipeline step executes within a containerized environment that provides process isolation, dependency management, and resource enforcement. The orchestrator integrates with container runtimes (Docker, containerd) and orchestration platforms (Kubernetes, Docker Swarm) to manage step execution.</p>\n<table>\n<thead>\n<tr>\n<th>Container Configuration</th>\n<th>Purpose</th>\n<th>Implementation Details</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Resource Limits</td>\n<td>Enforce CPU, memory, and GPU allocation</td>\n<td>Uses cgroups for CPU/memory, device plugins for GPU</td>\n</tr>\n<tr>\n<td>Network Isolation</td>\n<td>Prevent steps from accessing external services</td>\n<td>Custom network policies and firewall rules</td>\n</tr>\n<tr>\n<td>Filesystem Isolation</td>\n<td>Separate temporary storage per step</td>\n<td>Mounted volumes with per-step subdirectories</td>\n</tr>\n<tr>\n<td>Environment Variables</td>\n<td>Pass input paths and metadata to steps</td>\n<td>Standardized variable naming convention</td>\n</tr>\n<tr>\n<td>Security Context</td>\n<td>Run with minimal privileges</td>\n<td>Non-root user, read-only root filesystem where possible</td>\n</tr>\n</tbody></table>\n<p><strong>Distributed Training Support:</strong></p>\n<p>For training steps that require multiple nodes (distributed training), the orchestrator provides specialized scheduling capabilities:</p>\n<ol>\n<li><strong>Gang Scheduling</strong>: Ensures all nodes for a distributed training job are allocated simultaneously to prevent deadlocks</li>\n<li><strong>Communication Setup</strong>: Configures inter-node networking and service discovery for distributed training frameworks</li>\n<li><strong>Failure Handling</strong>: Implements all-or-nothing semantics where failure of any node causes the entire distributed job to be rescheduled</li>\n<li><strong>Resource Homogeneity</strong>: Ensures all nodes in a distributed training job have identical hardware configurations</li>\n</ol>\n<p>The orchestrator supports multiple distributed training patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th>Use Case</th>\n<th>Resource Allocation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Parallel</td>\n<td>Large datasets, model fits on single GPU</td>\n<td>Multiple nodes with identical GPU configurations</td>\n</tr>\n<tr>\n<td>Model Parallel</td>\n<td>Large models that don&#39;t fit on single GPU</td>\n<td>Nodes with high-bandwidth interconnect</td>\n</tr>\n<tr>\n<td>Pipeline Parallel</td>\n<td>Sequential model layers across nodes</td>\n<td>Nodes with balanced compute and network capacity</td>\n</tr>\n<tr>\n<td>Hybrid Parallel</td>\n<td>Combination of above approaches</td>\n<td>Heterogeneous allocation based on layer requirements</td>\n</tr>\n</tbody></table>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The design of the training pipeline orchestration component involves several critical architecture decisions that impact scalability, reliability, and usability. Each decision represents a trade-off between different quality attributes and operational concerns.</p>\n<blockquote>\n<p><strong>Decision: Orchestration Engine Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: We need to choose between building a custom orchestration engine versus adapting existing workflow engines like Argo Workflows, Apache Airflow, or Kubeflow Pipelines. Custom engines offer complete control but require significant development effort, while existing engines provide proven scalability but may not fit our ML-specific requirements.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Custom orchestrator built on Kubernetes controllers</li>\n<li>Argo Workflows with custom ML extensions</li>\n<li>Apache Airflow with ML plugins</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Build a custom orchestrator using Kubernetes controllers with pluggable execution backends</li>\n<li><strong>Rationale</strong>: ML pipelines have unique requirements like artifact lineage tracking, experiment correlation, and tight integration with model registry that are difficult to achieve with general-purpose workflow engines. A custom orchestrator allows us to optimize for ML workflows while still leveraging Kubernetes for resource management and scaling.</li>\n<li><strong>Consequences</strong>: Higher initial development cost but better long-term maintainability and ML-specific features. We maintain full control over execution semantics and can optimize performance for our specific use cases.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Orchestration Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Custom K8s Controller</td>\n<td>Full control, ML-optimized, tight integration</td>\n<td>High development cost, maintenance burden</td>\n</tr>\n<tr>\n<td>Argo Workflows</td>\n<td>Proven scalability, active community</td>\n<td>General-purpose design, complex ML integration</td>\n</tr>\n<tr>\n<td>Apache Airflow</td>\n<td>Rich ecosystem, familiar to many teams</td>\n<td>Python-centric, not optimized for containerized ML</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Resource Scheduling Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Pipeline steps have diverse resource requirements from lightweight data validation (100m CPU) to intensive model training (8 GPUs). We need to decide between time-sharing resources across multiple steps versus dedicating resources to single steps for their entire duration.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Time-sharing with preemption and checkpointing</li>\n<li>Dedicated resource allocation per step</li>\n<li>Hybrid approach with different strategies per step type</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Dedicated resource allocation with optional time-sharing for eligible steps</li>\n<li><strong>Rationale</strong>: ML training workloads are often GPU-intensive and don&#39;t checkpoint well, making preemption expensive. Dedicated allocation provides predictable performance and simplified failure handling. We allow opt-in time-sharing for CPU-only steps that can handle interruption.</li>\n<li><strong>Consequences</strong>: Higher resource efficiency for predictable workloads but potentially lower overall cluster utilization. Simplified scheduling logic and more predictable step execution times.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Scheduling Strategy</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time-sharing</td>\n<td>Higher resource utilization, cost efficiency</td>\n<td>Complex checkpointing, unpredictable performance</td>\n</tr>\n<tr>\n<td>Dedicated Allocation</td>\n<td>Predictable performance, simple failure handling</td>\n<td>Lower utilization, higher cost</td>\n</tr>\n<tr>\n<td>Hybrid Approach</td>\n<td>Best of both worlds for different step types</td>\n<td>Increased complexity, configuration overhead</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Fault Tolerance Mechanism</strong></p>\n<ul>\n<li><strong>Context</strong>: Pipeline steps can fail due to infrastructure issues (node failures, network partitions), resource exhaustion (OOM, timeout), or application errors (bad data, algorithm convergence issues). We need to decide how to handle failures while maintaining pipeline correctness and avoiding wasted computation.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Automatic retry with exponential backoff</li>\n<li>Checkpoint-based resumption from partial progress</li>\n<li>Pipeline-level rollback to last known good state</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Automatic retry with configurable policies plus optional checkpointing for long-running steps</li>\n<li><strong>Rationale</strong>: Most failures are transient infrastructure issues that resolve with retry. For expensive training steps, checkpointing allows resumption without losing hours of computation. Pipeline-level rollback is too coarse-grained and wastes too much work.</li>\n<li><strong>Consequences</strong>: Good balance of automatic recovery and computational efficiency. Requires step authors to implement checkpointing for long-running operations but provides significant cost savings.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Fault Tolerance Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Automatic Retry</td>\n<td>Simple to implement, handles transient failures</td>\n<td>Can waste computation on persistent failures</td>\n</tr>\n<tr>\n<td>Checkpoint Resumption</td>\n<td>Preserves expensive computation</td>\n<td>Requires step-level implementation, storage overhead</td>\n</tr>\n<tr>\n<td>Pipeline Rollback</td>\n<td>Simple failure model</td>\n<td>Wastes significant computation</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Data Passing Implementation</strong></p>\n<ul>\n<li><strong>Context</strong>: Pipeline steps need to exchange datasets, trained models, and intermediate artifacts. We must decide between in-memory passing (faster but limited by node memory), shared filesystem (requires distributed FS), or object storage (durable but higher latency).</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>In-memory passing through shared volumes</li>\n<li>Distributed filesystem (NFS, GFS) with path-based sharing</li>\n<li>Object storage (S3, GCS) with explicit upload/download</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Object storage with local caching for frequently accessed artifacts</li>\n<li><strong>Rationale</strong>: Object storage provides durability guarantees essential for reproducibility and debugging. Local caching mitigates latency concerns for artifacts accessed multiple times. Distributed filesystems add operational complexity and failure modes.</li>\n<li><strong>Consequences</strong>: Higher latency for small artifacts but better durability and debuggability. Simplified cluster setup without requiring distributed filesystem deployment.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Data Passing Option</th>\n<th>Pros</th>\n<th>Cons</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>In-memory Volumes</td>\n<td>Low latency, simple implementation</td>\n<td>Memory limitations, no durability</td>\n</tr>\n<tr>\n<td>Distributed Filesystem</td>\n<td>POSIX semantics, moderate latency</td>\n<td>Operational complexity, additional failure modes</td>\n</tr>\n<tr>\n<td>Object Storage</td>\n<td>Durability, scalability, vendor ecosystem</td>\n<td>Higher latency, eventual consistency issues</td>\n</tr>\n</tbody></table>\n<p><strong>Step Isolation and Security:</strong></p>\n<p>The orchestrator implements multiple layers of isolation to prevent steps from interfering with each other and to enforce security boundaries:</p>\n<table>\n<thead>\n<tr>\n<th>Isolation Layer</th>\n<th>Mechanism</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Process Isolation</td>\n<td>Container runtime (Docker/containerd)</td>\n<td>Prevent resource conflicts and crashes</td>\n</tr>\n<tr>\n<td>Network Isolation</td>\n<td>Kubernetes NetworkPolicies</td>\n<td>Prevent unauthorized communication between steps</td>\n</tr>\n<tr>\n<td>Filesystem Isolation</td>\n<td>Per-step mounted volumes</td>\n<td>Prevent data corruption and unauthorized access</td>\n</tr>\n<tr>\n<td>Resource Isolation</td>\n<td>cgroups and resource quotas</td>\n<td>Enforce resource limits and prevent noisy neighbor issues</td>\n</tr>\n<tr>\n<td>Privilege Isolation</td>\n<td>Non-root containers, seccomp profiles</td>\n<td>Minimize attack surface and prevent privilege escalation</td>\n</tr>\n</tbody></table>\n<p><strong>Pipeline State Management:</strong></p>\n<p>The orchestrator maintains comprehensive state information about pipeline executions to support monitoring, debugging, and recovery:</p>\n<table>\n<thead>\n<tr>\n<th>State Category</th>\n<th>Information Tracked</th>\n<th>Storage Location</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline Execution</td>\n<td>Status, start time, completion time, resource usage</td>\n<td>PostgreSQL metadata store</td>\n</tr>\n<tr>\n<td>Step Execution</td>\n<td>Individual step status, logs, resource consumption</td>\n<td>Combination of metadata store and log aggregation</td>\n</tr>\n<tr>\n<td>Artifact Lineage</td>\n<td>Input-output relationships, checksums, storage paths</td>\n<td>Metadata store with references to object storage</td>\n</tr>\n<tr>\n<td>Error Information</td>\n<td>Failure reasons, stack traces, retry attempts</td>\n<td>Structured logs in log aggregation system</td>\n</tr>\n</tbody></table>\n<p>The state management system ensures that pipeline executions can be resumed after orchestrator restarts and provides complete audit trails for compliance and debugging purposes.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Orchestration Backend</td>\n<td>Kubernetes Jobs with custom controller</td>\n<td>Argo Workflows with custom CRDs</td>\n</tr>\n<tr>\n<td>Resource Scheduling</td>\n<td>Native Kubernetes scheduler</td>\n<td>Volcano scheduler with gang scheduling</td>\n</tr>\n<tr>\n<td>Container Runtime</td>\n<td>Docker with containerd</td>\n<td>CRI-O with gVisor for enhanced security</td>\n</tr>\n<tr>\n<td>Artifact Storage</td>\n<td>MinIO (S3-compatible)</td>\n<td>Cloud object storage (S3, GCS, Azure Blob)</td>\n</tr>\n<tr>\n<td>Metadata Storage</td>\n<td>PostgreSQL with JSONB</td>\n<td>PostgreSQL with TimescaleDB for metrics</td>\n</tr>\n<tr>\n<td>Message Queue</td>\n<td>Redis with pub/sub</td>\n<td>Apache Kafka with persistent topics</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Prometheus with Grafana</td>\n<td>Full observability stack with tracing</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>internal/pipeline/\n  orchestrator/\n    orchestrator.go           ← Main orchestration engine\n    dag_executor.go          ← DAG parsing and execution logic\n    resource_scheduler.go    ← Resource allocation and scheduling\n    step_executor.go         ← Individual step execution management\n    state_manager.go         ← Pipeline state persistence\n    orchestrator_test.go     ← Comprehensive test suite\n  \n  models/\n    pipeline.go              ← Pipeline definition types\n    execution.go             ← Runtime execution types\n    resources.go             ← Resource specification types\n    \n  storage/\n    artifact_manager.go      ← Artifact upload/download handling\n    metadata_store.go        ← Pipeline metadata persistence\n    \n  executor/\n    kubernetes/\n      k8s_executor.go        ← Kubernetes-based step execution\n      job_manager.go         ← Kubernetes Job lifecycle management\n    local/\n      local_executor.go      ← Local execution for development/testing</code></pre></div>\n\n<p><strong>Infrastructure Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># internal/pipeline/models/pipeline.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StepStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RUNNING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"running\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUCCEEDED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"succeeded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SKIPPED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"skipped\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ResourceSpec</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cores: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_gb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    gpu_type: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    storage_gb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_duration_hours: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    preemptible: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node_selector: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InputSpec</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # 'dataset', 'model', 'parameter', 'artifact'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validation_schema: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    required: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    default_value: Optional[Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OutputSpec</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # 'dataset', 'model', 'metrics', 'artifact'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    path_template: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata_schema: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryPolicy</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_multiplier: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    initial_delay_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_delay_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Step</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    container_image: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    command: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inputs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, InputSpec] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    outputs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, OutputSpec] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_requirements: ResourceSpec </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">ResourceSpec)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retry_policy: RetryPolicy </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">RetryPolicy)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: Optional[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    environment_variables: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Pipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    steps: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Step] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step_dependencies: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    data_flow: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    global_parameters: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    default_resources: ResourceSpec </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">ResourceSpec)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StepExecution</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    step_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pipeline_run_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: StepStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    start_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    end_time: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    attempt_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_usage: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node_name: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    pod_name: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># internal/pipeline/orchestrator/dag_executor.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Set, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..models.pipeline </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Pipeline, Step, StepExecution</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DAGExecutor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Analyzes pipeline DAGs and coordinates step execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, artifact_store, metadata_store):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.artifact_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> artifact_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metadata_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metadata_store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_pipeline_dag</span><span style=\"color:#E1E4E8\">(self, pipeline: Pipeline) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validates pipeline DAG structure and returns list of validation errors.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns empty list if pipeline is valid.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Build adjacency list from step_dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check for circular dependencies using DFS with visit tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Validate that all data_flow connections reference valid steps and outputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify input/output type compatibility for connected steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check that all required inputs have either connections or default values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use three-color DFS (white/gray/black) to detect cycles</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compute_execution_order</span><span style=\"color:#E1E4E8\">(self, pipeline: Pipeline) -> List[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Computes execution order as list of step groups that can run in parallel.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns list where each inner list contains step_ids that can execute concurrently.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create dependency count map for each step</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize ready queue with steps that have zero dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: While ready queue not empty, collect all ready steps as parallel group</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each completed step, decrement dependency counts of downstream steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Add newly ready steps to queue for next iteration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return list of parallel execution groups</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> prepare_step_inputs</span><span style=\"color:#E1E4E8\">(self, step_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, pipeline: Pipeline, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          completed_steps: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, StepExecution]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Downloads required input artifacts and returns environment variables for step.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns dict of environment variables to pass to step container.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Look up required inputs from step definition</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each input, find the upstream step that produces it via data_flow</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Download artifact from artifact_store using upstream step's output path</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate downloaded artifact against input specification</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create environment variables with input paths and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Handle default values for optional inputs that aren't connected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use naming convention MLOPS_INPUT_{INPUT_NAME}_PATH for env vars</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle_step_completion</span><span style=\"color:#E1E4E8\">(self, step_execution: StepExecution, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             pipeline: Pipeline) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Processes step completion by uploading outputs and updating metadata.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Updates step execution record and handles artifact storage.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Read step outputs from container's output directory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate outputs against step's output specifications  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Upload artifacts to artifact_store with standardized paths</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute checksums and file metadata for each output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update step execution record with completion status and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Trigger downstream step eligibility check</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Kubernetes Integration Starter Code:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># internal/pipeline/executor/kubernetes/k8s_executor.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> kubernetes </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> client, config</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> yaml</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ...models.pipeline </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Step, ResourceSpec, StepExecution</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> KubernetesStepExecutor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Executes pipeline steps as Kubernetes Jobs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            config.load_incluster_config()  </span><span style=\"color:#6A737D\"># Running inside cluster</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            config.load_kube_config()  </span><span style=\"color:#6A737D\"># Development environment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.batch_v1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client.BatchV1Api()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.core_v1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> client.CoreV1Api()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_job_spec</span><span style=\"color:#E1E4E8\">(self, step: Step, step_execution: StepExecution,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                       environment_vars: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Creates Kubernetes Job specification for pipeline step.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Convert resource requirements to Kubernetes format</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        resources </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"requests\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"cpu\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step.resource_requirements.cpu_cores</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"memory\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step.resource_requirements.memory_gb</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">Gi\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"limits\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"cpu\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step.resource_requirements.cpu_cores</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"memory\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step.resource_requirements.memory_gb</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">Gi\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> step.resource_requirements.gpu_count </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            resources[</span><span style=\"color:#9ECBFF\">\"requests\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"nvidia.com/gpu\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> step.resource_requirements.gpu_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            resources[</span><span style=\"color:#9ECBFF\">\"limits\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"nvidia.com/gpu\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> step.resource_requirements.gpu_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Build environment variable list</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        env_vars </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> environment_vars.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            env_vars.append({</span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">: key, </span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">: value})</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> key, value </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> step.environment_variables.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            env_vars.append({</span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">: key, </span><span style=\"color:#9ECBFF\">\"value\"</span><span style=\"color:#E1E4E8\">: value})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job_spec </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"apiVersion\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"batch/v1\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"kind\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Job\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"metadata\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"mlops-step-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step.step_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">-</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step_execution.pipeline_run_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"labels\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"app\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"mlops-pipeline\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"step-id\"</span><span style=\"color:#E1E4E8\">: step.step_id,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"pipeline-run-id\"</span><span style=\"color:#E1E4E8\">: step_execution.pipeline_run_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            },</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"spec\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"backoffLimit\"</span><span style=\"color:#E1E4E8\">: step.retry_policy.max_attempts </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"template\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    \"spec\"</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"restartPolicy\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Never\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"containers\"</span><span style=\"color:#E1E4E8\">: [{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"step-container\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"image\"</span><span style=\"color:#E1E4E8\">: step.container_image,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"command\"</span><span style=\"color:#E1E4E8\">: step.command,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"env\"</span><span style=\"color:#E1E4E8\">: env_vars,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"resources\"</span><span style=\"color:#E1E4E8\">: resources,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"volumeMounts\"</span><span style=\"color:#E1E4E8\">: [{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                                \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"artifact-storage\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                                \"mountPath\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"/mlops/artifacts\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            }]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        }],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                        \"volumes\"</span><span style=\"color:#E1E4E8\">: [{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"name\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"artifact-storage\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                            \"emptyDir\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"sizeLimit\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">step.resource_requirements.storage_gb</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">Gi\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        }]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add node selector if specified</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> step.resource_requirements.node_selector:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_spec[</span><span style=\"color:#9ECBFF\">\"spec\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"template\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"spec\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"nodeSelector\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> step.resource_requirements.node_selector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add timeout if specified</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> step.timeout_seconds:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            job_spec[</span><span style=\"color:#9ECBFF\">\"spec\"</span><span style=\"color:#E1E4E8\">][</span><span style=\"color:#9ECBFF\">\"activeDeadlineSeconds\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> step.timeout_seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> job_spec</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> submit_job</span><span style=\"color:#E1E4E8\">(self, job_spec: Dict) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Submits job to Kubernetes and returns job name.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.batch_v1.create_namespaced_job(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"default\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            body</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_spec</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> response.metadata.name</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_job_status</span><span style=\"color:#E1E4E8\">(self, job_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Gets current status of Kubernetes job.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        job </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.batch_v1.read_namespaced_job_status(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">job_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            namespace</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"default\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"active\"</span><span style=\"color:#E1E4E8\">: job.status.active </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"succeeded\"</span><span style=\"color:#E1E4E8\">: job.status.succeeded </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"failed\"</span><span style=\"color:#E1E4E8\">: job.status.failed </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"conditions\"</span><span style=\"color:#E1E4E8\">: job.status.conditions </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span></code></pre></div>\n\n<p><strong>Language-Specific Implementation Hints:</strong></p>\n<ul>\n<li>Use <code>asyncio</code> for concurrent step monitoring and execution in Python</li>\n<li>Implement exponential backoff using <code>tenacity</code> library for retry logic  </li>\n<li>Use <code>kubernetes-python</code> client library for Kubernetes API interactions</li>\n<li>Store pipeline state in PostgreSQL using <code>asyncpg</code> for async database access</li>\n<li>Use <code>aiohttp</code> for non-blocking HTTP requests to artifact storage APIs</li>\n<li>Implement circuit breakers using <code>aiobreaker</code> for external service calls</li>\n<li>Use structured logging with <code>structlog</code> for correlation IDs and request tracing</li>\n</ul>\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the training pipeline orchestration:</p>\n<ol>\n<li><strong>Unit Tests</strong>: Run <code>python -m pytest tests/pipeline/ -v</code> to verify core logic</li>\n<li><strong>Integration Test</strong>: Create a simple 3-step pipeline (validate → preprocess → train) and verify it executes correctly with proper data flow</li>\n<li><strong>Expected Behavior</strong>: <ul>\n<li>Pipeline validates DAG structure and rejects circular dependencies</li>\n<li>Steps execute in correct dependency order with parallel execution where possible</li>\n<li>Artifacts flow correctly between steps through object storage</li>\n<li>Failed steps retry according to configured policies</li>\n<li>Resource limits are enforced at the Kubernetes level</li>\n</ul>\n</li>\n</ol>\n<p><strong>Debugging Tips:</strong></p>\n<p>| Symptom | Likely Cause | Diagnosis | Fix |\n|---|---|---|\n| Steps hang in pending state | Resource constraints or node selector mismatch | Check <code>kubectl get pods</code> and node capacity | Adjust resource requirements or node selectors |\n| Data flow failures between steps | Artifact path mismatch or corrupted upload | Examine artifact store logs and checksums | Verify output path templates and artifact validation |\n| Pipeline never completes | Circular dependency in DAG | Run DAG validation with detailed error logging | Fix step dependencies to create valid DAG |\n| High memory usage in orchestrator | Large pipeline state or insufficient garbage collection | Monitor orchestrator memory usage and state size | Implement state cleanup and optimize data structures |\n| Inconsistent pipeline execution | Race conditions in step scheduling | Add correlation IDs and trace execution order | Implement proper locking around shared state |</p>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fpipeline-execution.svg\" alt=\"Training Pipeline Execution\"></p>\n<h2 id=\"model-deployment-component\">Model Deployment Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section primarily corresponds to Milestone 4 (Model Deployment), which focuses on deploying models to production as HTTP endpoints with traffic management, canary releases, and auto-scaling. This section also establishes the foundation for Milestone 5 (Model Monitoring) by implementing the prediction logging and traffic routing infrastructure.</p>\n</blockquote>\n<p>The <strong>model deployment component</strong> transforms registered model versions from the Model Registry into scalable, production-ready inference endpoints. Think of this component as the bridge between the experimental world of model training and the demanding requirements of production systems, where millisecond latencies, high availability, and seamless updates determine business success.</p>\n<h3 id=\"mental-model-restaurant-service\">Mental Model: Restaurant Service</h3>\n<p>Understanding model deployment through restaurant service analogies helps build intuition for the complex orchestration required to serve models at scale. Consider how a successful restaurant must balance quality, speed, capacity, and customer satisfaction while introducing new menu items without disrupting ongoing service.</p>\n<p><strong>The Kitchen as Model Serving Infrastructure</strong>: Just as a restaurant kitchen contains different stations (grill, sauté, pastry) optimized for specific dishes, model serving infrastructure contains specialized inference servers (TensorFlow Serving, TorchServe, Triton) optimized for different model frameworks and use cases. Each station has specific equipment, trained staff, and procedures - similarly, each inference server has optimized runtimes, memory management, and preprocessing pipelines tuned for its target model types.</p>\n<p><strong>Menu Items as Model Versions</strong>: Restaurant menu items represent different model versions available for serving. Just as a restaurant might offer both the classic burger (stable, proven) and a seasonal special (new, experimental), production systems serve stable model versions alongside newer variants being evaluated. Each menu item has preparation instructions, ingredient requirements, and expected preparation time - model versions have inference code, resource requirements, and latency profiles.</p>\n<p><strong>Order Flow as Request Processing</strong>: When customers place orders, the restaurant&#39;s order management system routes requests to appropriate kitchen stations, manages preparation queues, and coordinates delivery timing. Similarly, model deployment systems route inference requests to appropriate model instances, manage request batching for efficiency, and coordinate response aggregation when using ensemble approaches.</p>\n<p><strong>Quality Control as Prediction Validation</strong>: Restaurants implement quality control checkpoints - temperature checks for food safety, taste testing for consistency, presentation review before serving. Model deployment systems implement analogous validation - input schema validation, prediction confidence thresholds, output format verification, and anomaly detection before returning results to clients.</p>\n<p><strong>Introducing New Dishes as Canary Deployments</strong>: When restaurants introduce new menu items, they often start with limited availability - offering the new dish to select customers or during specific hours to gather feedback without risking the entire operation. This mirrors canary deployment strategies where new model versions serve a small percentage of production traffic while monitoring performance metrics. If the new dish receives positive feedback, it becomes a regular menu item; if customers complain, it&#39;s quickly withdrawn. Similarly, successful canary deployments gradually increase traffic allocation, while problematic deployments trigger automatic rollbacks.</p>\n<p><strong>Kitchen Capacity Management as Auto-Scaling</strong>: Restaurants adjust staffing and station capacity based on expected demand - adding cooks during rush hours, opening additional grilling stations for burger-heavy periods, preparing mise en place during slow periods. Model deployment systems implement similar auto-scaling logic, monitoring request queues and response latencies to determine when additional model instances are needed, and scaling down during low-traffic periods to optimize resource costs.</p>\n<p><strong>Service Quality Monitoring as Performance Tracking</strong>: Successful restaurants continuously monitor service quality - order fulfillment times, customer satisfaction scores, ingredient freshness, equipment performance. They establish alert systems for critical issues (kitchen fire, equipment breakdown, food safety violations) that require immediate intervention. Model deployment systems implement comprehensive monitoring for inference latency, prediction accuracy, error rates, and resource utilization, with alert systems for degraded performance or system failures that threaten service availability.</p>\n<h3 id=\"model-serving-and-scaling\">Model Serving and Scaling</h3>\n<p>Model serving transforms static artifacts from the Model Registry into dynamic, responsive HTTP endpoints capable of handling production inference workloads. This process involves multiple sophisticated subsystems working together to optimize for latency, throughput, and resource efficiency while maintaining prediction quality and system reliability.</p>\n<p><strong>Inference Server Integration</strong> provides the foundation for model serving by wrapping trained models in high-performance runtime environments optimized for production inference workloads. The deployment component integrates with specialized inference servers rather than implementing model execution directly, leveraging years of optimization work in frameworks like TensorFlow Serving, TorchServe, NVIDIA Triton, and MLflow&#39;s built-in serving capabilities.</p>\n<p>The integration architecture uses a <strong>serving endpoint abstraction</strong> that encapsulates inference server specifics behind a common interface. When deploying a model version, the system examines the model&#39;s metadata to determine the appropriate inference server, generates server-specific configuration files, and manages the server lifecycle. For TensorFlow models, this involves creating SavedModel bundles and TensorFlow Serving configuration files specifying input/output tensor specifications. For PyTorch models, the system generates TorchServe model archives (MAR files) with custom preprocessing and postprocessing handlers when needed.</p>\n<table>\n<thead>\n<tr>\n<th>Inference Server</th>\n<th>Model Types</th>\n<th>Optimization Features</th>\n<th>Integration Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TensorFlow Serving</td>\n<td>TensorFlow, Keras</td>\n<td>Dynamic batching, GPU optimization, version management</td>\n<td>REST API + gRPC, model repository mounting</td>\n</tr>\n<tr>\n<td>TorchServe</td>\n<td>PyTorch, TorchScript</td>\n<td>Multi-worker inference, custom handlers, metrics</td>\n<td>REST management API, model store integration</td>\n</tr>\n<tr>\n<td>NVIDIA Triton</td>\n<td>Multi-framework</td>\n<td>Dynamic batching, model ensembles, backend optimization</td>\n<td>HTTP/gRPC inference, model repository</td>\n</tr>\n<tr>\n<td>MLflow Serving</td>\n<td>Scikit-learn, custom</td>\n<td>Unified interface, environment management</td>\n<td>REST API, conda environment packaging</td>\n</tr>\n<tr>\n<td>ONNX Runtime</td>\n<td>ONNX models</td>\n<td>Cross-platform optimization, hardware acceleration</td>\n<td>Python API wrapper, optimized execution providers</td>\n</tr>\n</tbody></table>\n<p><strong>Model Loading and Initialization</strong> represents a critical optimization point where the system balances startup time against memory efficiency. The deployment component implements <strong>model warming strategies</strong> that preload models into memory and execute initial inference requests to trigger JIT compilation and cache population. This prevents the &quot;cold start&quot; problem where the first production requests experience dramatically higher latency due to model loading overhead.</p>\n<p>The model loading process follows a structured sequence: First, the deployment controller downloads the model artifact from the Model Registry&#39;s artifact store, verifying checksums to ensure integrity. Next, it extracts the model files into the inference server&#39;s expected directory structure, applying any framework-specific transformations. The inference server then loads the model into memory, allocating GPU resources if specified in the model&#39;s resource requirements. Finally, the warming process sends synthetic inference requests through the model to trigger any lazy initialization and populate caches.</p>\n<p><strong>Auto-Scaling Policies</strong> enable model serving endpoints to adapt to changing demand patterns while optimizing for both performance and cost. The auto-scaling system monitors multiple metrics simultaneously and makes scaling decisions based on configurable thresholds and policies that account for the unique characteristics of ML inference workloads.</p>\n<p>The auto-scaling controller tracks <strong>request queue depth</strong> as the primary indicator of insufficient capacity. Unlike traditional web services where CPU utilization is often the primary metric, ML inference endpoints can become bottlenecked on specialized resources like GPU memory or model-specific preprocessing pipelines. The queue depth metric captures these bottlenecks regardless of their underlying cause - if requests are waiting longer than acceptable thresholds, additional capacity is needed.</p>\n<p><strong>Latency percentile tracking</strong> provides a quality-oriented scaling trigger that ensures user experience remains within acceptable bounds. The system monitors P95 and P99 response latencies, triggering scale-up when these percentiles exceed configured thresholds. This approach prevents the degraded user experience that can occur when average latency appears acceptable but tail latencies become problematic.</p>\n<table>\n<thead>\n<tr>\n<th>Scaling Metric</th>\n<th>Scale-Up Threshold</th>\n<th>Scale-Down Threshold</th>\n<th>Evaluation Window</th>\n<th>Rationale</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Request Queue Depth</td>\n<td>&gt; 10 requests</td>\n<td>&lt; 2 requests</td>\n<td>1 minute</td>\n<td>Direct capacity indicator regardless of bottleneck type</td>\n</tr>\n<tr>\n<td>P95 Latency</td>\n<td>&gt; 200ms</td>\n<td>&lt; 100ms</td>\n<td>2 minutes</td>\n<td>User experience quality maintenance</td>\n</tr>\n<tr>\n<td>P99 Latency</td>\n<td>&gt; 500ms</td>\n<td>&lt; 250ms</td>\n<td>3 minutes</td>\n<td>Tail latency protection for critical requests</td>\n</tr>\n<tr>\n<td>GPU Memory Usage</td>\n<td>&gt; 80%</td>\n<td>&lt; 40%</td>\n<td>30 seconds</td>\n<td>Hardware resource constraint prevention</td>\n</tr>\n<tr>\n<td>Request Rate</td>\n<td>&gt; 100 RPS</td>\n<td>&lt; 20 RPS</td>\n<td>5 minutes</td>\n<td>Predictive scaling based on traffic patterns</td>\n</tr>\n</tbody></table>\n<p><strong>Performance Optimization</strong> within model serving encompasses multiple layers of the inference pipeline, from request preprocessing through model execution to response serialization. The deployment component implements several optimization strategies that can significantly improve throughput and reduce latency without requiring changes to the underlying models.</p>\n<p><strong>Dynamic request batching</strong> groups multiple inference requests together to leverage vectorized operations and GPU parallelism. The batching system balances batch size against latency requirements - larger batches improve GPU utilization but increase waiting time for requests. The optimization algorithm considers the model&#39;s batch processing characteristics, available hardware resources, and latency SLAs to determine optimal batch sizes and timeout policies.</p>\n<p><strong>Result caching</strong> stores inference results for repeated inputs, particularly valuable for models that frequently receive identical or similar requests. The caching system implements intelligent cache key generation that accounts for input features while handling floating-point precision issues. Cache eviction policies prioritize frequently accessed results while ensuring cache size remains within memory constraints.</p>\n<p><strong>Model compilation optimizations</strong> leverage framework-specific compilation techniques to accelerate inference execution. For TensorFlow models, this includes TensorRT optimization for NVIDIA GPUs and XLA compilation for CPU and TPU workloads. For PyTorch models, the system applies TorchScript compilation and potentially ONNX conversion for cross-platform optimization.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: Model serving optimization requires understanding the specific computational patterns of ML inference workloads, which differ significantly from traditional web services. While web services are often I/O bound and benefit from connection pooling and caching, ML inference is typically compute-bound with predictable processing patterns that benefit from batching and specialized hardware acceleration.</p>\n</blockquote>\n<h3 id=\"traffic-management-and-rollouts\">Traffic Management and Rollouts</h3>\n<p>Traffic management enables safe, controlled deployment of new model versions while maintaining service availability and providing mechanisms for rapid rollback when issues arise. The traffic management subsystem implements sophisticated routing logic that supports multiple deployment strategies, each optimized for different risk profiles and operational requirements.</p>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fdeployment-traffic.svg\" alt=\"Deployment Traffic Management\"></p>\n<p><strong>Blue-Green Deployments</strong> provide the safest approach for model updates by maintaining two complete, identical production environments and switching traffic atomically between them. In the context of model serving, blue-green deployments mean running both the current model version and the new model version on separate infrastructure, then redirecting all traffic from the blue environment to the green environment instantaneously.</p>\n<p>The blue-green implementation maintains <strong>environment isolation</strong> to prevent any interference between model versions during the transition period. Each environment has dedicated inference server instances, separate monitoring dashboards, and independent resource allocations. This isolation ensures that performance testing of the green environment doesn&#39;t impact blue environment performance, and any issues in the green environment can&#39;t affect current production traffic.</p>\n<p>The <strong>traffic switch mechanism</strong> uses a load balancer configuration update to redirect all incoming requests from the blue environment to the green environment. The deployment controller implements this switch as an atomic operation - it updates the load balancer&#39;s target group configuration and waits for the configuration propagation to complete before considering the deployment successful. During the brief propagation period (typically 10-30 seconds), some requests may still route to the blue environment, but no requests are lost.</p>\n<p><strong>Canary Release Strategy</strong> provides a middle ground between the safety of blue-green deployments and the resource efficiency of in-place updates. Canary releases gradually shift traffic from the current model version to the new version while continuously monitoring performance metrics and automatically rolling back if degradation is detected.</p>\n<p>The canary implementation defines <strong>traffic splitting policies</strong> that specify what percentage of requests should route to the new model version at each stage of the rollout. A typical canary progression might start with 5% traffic to the new version, then increase to 25%, 50%, 75%, and finally 100% as confidence in the new version grows. Each stage includes automatic validation gates that must pass before proceeding to the next stage.</p>\n<table>\n<thead>\n<tr>\n<th>Canary Stage</th>\n<th>Traffic Percentage</th>\n<th>Duration</th>\n<th>Validation Criteria</th>\n<th>Rollback Triggers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Initial</td>\n<td>5%</td>\n<td>10 minutes</td>\n<td>Error rate &lt; 0.1%, P95 latency increase &lt; 10%</td>\n<td>Error rate &gt; 0.5%, latency increase &gt; 50%</td>\n</tr>\n<tr>\n<td>Expansion</td>\n<td>25%</td>\n<td>30 minutes</td>\n<td>Accuracy within 2% of baseline, no alerts fired</td>\n<td>Error rate &gt; 0.2%, accuracy drop &gt; 5%</td>\n</tr>\n<tr>\n<td>Majority</td>\n<td>50%</td>\n<td>60 minutes</td>\n<td>Full metric validation, A/B test significance</td>\n<td>Any metric degrades beyond threshold</td>\n</tr>\n<tr>\n<td>Near-Complete</td>\n<td>75%</td>\n<td>30 minutes</td>\n<td>Final validation before full rollout</td>\n<td>Last chance for manual intervention</td>\n</tr>\n<tr>\n<td>Complete</td>\n<td>100%</td>\n<td>Ongoing</td>\n<td>Continuous monitoring for delayed issues</td>\n<td>Standard production alerting</td>\n</tr>\n</tbody></table>\n<p><strong>A/B Testing Framework</strong> extends traffic splitting to support controlled experiments where different model versions serve production traffic simultaneously for statistical comparison of business metrics. Unlike canary deployments that aim to replace the old version with the new version, A/B tests maintain traffic splits for extended periods to gather sufficient data for statistical significance.</p>\n<p>The A/B testing system implements <strong>experiment assignment logic</strong> that ensures consistent user experiences by routing users to the same model version throughout their session or analysis period. This consistency prevents confusing user experiences where prediction behavior changes unexpectedly and ensures clean statistical analysis by avoiding cross-contamination between test groups.</p>\n<p><strong>Statistical significance monitoring</strong> tracks the key metrics for each model version and calculates confidence intervals to determine when sufficient evidence exists to declare a winner. The system implements sequential testing procedures that can detect statistically significant differences as early as possible while controlling for multiple comparisons and ensuring adequate statistical power.</p>\n<p><strong>Traffic Routing Implementation</strong> serves as the foundation for all traffic management strategies by implementing intelligent request routing logic that can direct requests to specific model versions based on various criteria including traffic split percentages, user attributes, request characteristics, and real-time performance metrics.</p>\n<p>The routing system maintains <strong>routing configuration state</strong> that specifies how traffic should be distributed across available model versions. This configuration includes traffic split percentages, routing rules based on request headers or user attributes, geographic routing preferences, and fallback policies for handling failures. The configuration updates propagate to all load balancers and API gateways within seconds to ensure consistent routing behavior.</p>\n<p><strong>Session affinity and consistency</strong> ensure that requests from the same user or session route to the same model version to prevent inconsistent prediction behavior. The system implements this through consistent hashing algorithms that map user identifiers to model versions, ensuring that adding or removing model versions doesn&#39;t disrupt existing user assignments more than necessary.</p>\n<blockquote>\n<p><strong>Decision: Traffic Management Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to balance deployment safety, resource efficiency, and operational complexity when rolling out new model versions to production traffic</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Blue-green deployments with atomic switching</li>\n<li>Canary releases with gradual traffic shifting  </li>\n<li>In-place updates with rolling restarts</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement canary releases as the primary deployment strategy with blue-green available for high-risk updates</li>\n<li><strong>Rationale</strong>: Canary releases provide excellent safety through gradual rollout and automatic rollback while being more resource-efficient than maintaining full duplicate environments. Blue-green deployments are available as an option for critical updates where maximum safety is required.</li>\n<li><strong>Consequences</strong>: Requires sophisticated traffic routing logic and comprehensive monitoring, but provides optimal balance of safety and efficiency for most model updates</li>\n</ul>\n</blockquote>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The architecture decisions for model deployment reflect the complex trade-offs between performance, reliability, cost, and operational complexity inherent in production ML systems. These decisions establish the foundation for scalable, maintainable deployment infrastructure that can evolve with changing requirements.</p>\n<blockquote>\n<p><strong>Decision: Inference Server Selection Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support multiple ML frameworks and model types while optimizing for performance and operational simplicity</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Single inference server (e.g., only TensorFlow Serving) with framework conversion</li>\n<li>Framework-specific servers (TensorFlow Serving, TorchServe, etc.) with routing logic</li>\n<li>Universal serving platform (e.g., NVIDIA Triton) for all model types</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Framework-specific inference servers with intelligent routing based on model metadata</li>\n<li><strong>Rationale</strong>: Framework-specific servers provide optimal performance for each model type and leverage extensive optimization work by framework teams. Conversion between frameworks often introduces performance penalties and compatibility issues. Universal platforms add complexity and may not optimize well for specific use cases.</li>\n<li><strong>Consequences</strong>: Requires maintaining expertise in multiple inference servers and more complex deployment logic, but delivers superior performance and maintains framework-specific optimizations</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Performance</th>\n<th>Operational Complexity</th>\n<th>Framework Support</th>\n<th>Chosen</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single inference server</td>\n<td>Medium</td>\n<td>Low</td>\n<td>Limited (requires conversion)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Framework-specific servers</td>\n<td>High</td>\n<td>Medium</td>\n<td>Native for each framework</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Universal serving platform</td>\n<td>Medium-High</td>\n<td>Medium</td>\n<td>Good but not native</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Auto-Scaling Metrics and Policies</strong></p>\n<ul>\n<li><strong>Context</strong>: ML inference workloads have different scaling characteristics than traditional web services due to GPU resource constraints and variable processing times</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>CPU/memory-based scaling like traditional web services</li>\n<li>Request queue depth and latency percentiles</li>\n<li>Predictive scaling based on historical patterns</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Multi-metric scaling using request queue depth, latency percentiles, and resource utilization with predictive elements</li>\n<li><strong>Rationale</strong>: CPU/memory metrics don&#39;t capture GPU bottlenecks or model-specific performance characteristics. Queue depth provides immediate capacity indicators while latency percentiles ensure user experience quality. Predictive elements help handle traffic spikes proactively.</li>\n<li><strong>Consequences</strong>: Requires more sophisticated monitoring infrastructure and tuning, but provides better scaling behavior for ML workloads</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>Decision: Deployment Strategy Framework</strong></p>\n<ul>\n<li><strong>Context</strong>: Different model updates have different risk profiles and require different deployment approaches</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Single deployment strategy for all updates</li>\n<li>Manual selection of deployment strategy per update</li>\n<li>Automatic strategy selection based on model metadata and change analysis</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Configurable deployment strategies with intelligent defaults based on model risk assessment</li>\n<li><strong>Rationale</strong>: Risk profiles vary significantly between model updates. Minor parameter updates may be safe for in-place deployment, while new model architectures require careful canary rollouts. Intelligent defaults reduce operational burden while allowing override for special cases.</li>\n<li><strong>Consequences</strong>: Requires developing risk assessment heuristics and maintaining multiple deployment code paths, but provides optimal safety and efficiency trade-offs</li>\n</ul>\n</blockquote>\n<p><strong>Resource Allocation and Scheduling Architecture</strong> determines how computational resources are assigned to model serving instances across the infrastructure. This decision impacts cost efficiency, performance predictability, and the ability to handle varying workloads.</p>\n<p>The resource allocation system implements <strong>multi-dimensional resource scheduling</strong> that considers CPU cores, memory, GPU devices, and storage requirements when placing model serving instances. Unlike traditional web services that primarily consume CPU and memory, ML inference often requires specialized resources like GPUs with specific memory capacities or tensor processing units with particular performance characteristics.</p>\n<p><strong>Resource isolation mechanisms</strong> prevent interference between different model serving instances running on shared infrastructure. The system uses containerization with resource limits to ensure that one model&#39;s resource consumption doesn&#39;t impact other models&#39; performance. For GPU resources, the system implements GPU sharing strategies when appropriate or dedicates entire GPU devices when models require exclusive access.</p>\n<p><strong>Cost optimization strategies</strong> balance performance requirements against infrastructure costs by implementing intelligent resource packing and scheduling policies. The system considers the cost implications of different resource allocation decisions, preferring to pack compatible workloads onto shared resources when performance requirements allow, while ensuring that performance-critical models receive dedicated resources when needed.</p>\n<table>\n<thead>\n<tr>\n<th>Resource Type</th>\n<th>Allocation Strategy</th>\n<th>Isolation Method</th>\n<th>Sharing Policy</th>\n<th>Cost Optimization</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU Cores</td>\n<td>Proportional share with limits</td>\n<td>cgroups CPU limits</td>\n<td>Multiple models per node</td>\n<td>Pack by CPU efficiency</td>\n</tr>\n<tr>\n<td>Memory</td>\n<td>Dedicated allocation with swap disabled</td>\n<td>cgroups memory limits</td>\n<td>Calculated based on model size</td>\n<td>Minimize memory waste</td>\n</tr>\n<tr>\n<td>GPU Devices</td>\n<td>Exclusive or shared based on model requirements</td>\n<td>NVIDIA MPS or dedicated allocation</td>\n<td>Single model for large models, shared for small</td>\n<td>Maximize GPU utilization</td>\n</tr>\n<tr>\n<td>Storage</td>\n<td>Shared model cache with dedicated scratch space</td>\n<td>Volume mounts and quotas</td>\n<td>Shared read-only, dedicated write</td>\n<td>Deduplication and compression</td>\n</tr>\n</tbody></table>\n<p><strong>Monitoring and Observability Integration</strong> establishes the foundation for understanding model serving performance and detecting issues before they impact users. The architecture integrates monitoring throughout the serving pipeline to provide comprehensive visibility into system behavior.</p>\n<p><strong>Metric collection strategies</strong> gather performance data at multiple levels including infrastructure metrics (CPU, memory, GPU utilization), application metrics (request latency, throughput, error rates), and ML-specific metrics (prediction confidence, feature distribution statistics). The system implements structured logging with correlation IDs that enable tracing individual requests through the entire serving pipeline.</p>\n<p><strong>Alert escalation policies</strong> define how different types of issues are handled, from automated responses for common problems to immediate human escalation for critical failures. The system implements intelligent alerting that considers context and severity when determining appropriate response actions.</p>\n<h3 id=\"common-pitfalls-in-model-deployment\">Common Pitfalls in Model Deployment</h3>\n<p>⚠️ <strong>Pitfall: Cold Start Performance Issues</strong>\nMany developers underestimate the model loading and initialization time required when scaling up model serving instances. They assume that spinning up new instances provides immediate capacity, but models often require significant time to load into memory, compile optimizations, and warm up caches. This leads to degraded performance during scaling events and poor user experience during traffic spikes. The fix involves implementing proper model warming strategies, maintaining warm spare instances during anticipated load increases, and using predictive scaling to start instance preparation before capacity is urgently needed.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Resource Specification</strong>\nTeams frequently specify insufficient or inappropriate resource requirements for model serving instances, leading to out-of-memory errors, GPU resource contention, or poor performance. This happens because resource requirements often differ significantly between training and inference workloads, and developers may not account for framework overhead, concurrent request processing, or peak memory usage during batch processing. The solution involves thorough performance testing with realistic traffic patterns, monitoring resource utilization during normal and peak loads, and implementing resource request optimization based on observed usage patterns.</p>\n<p>⚠️ <strong>Pitfall: Unsafe Traffic Routing During Deployments</strong>\nDevelopers often implement traffic routing logic that can lose requests or route them to unavailable model versions during deployment transitions. This occurs when routing configuration updates aren&#39;t atomic, when health checks don&#39;t properly validate model readiness, or when rollback procedures don&#39;t account for in-flight requests. The fix requires implementing proper health check endpoints that verify model loading and readiness, using atomic configuration updates for traffic routing, implementing graceful shutdown procedures that allow in-flight requests to complete, and testing rollback scenarios under load.</p>\n<p>⚠️ <strong>Pitfall: Missing Model Compatibility Validation</strong>\nTeams frequently deploy model versions without validating that they&#39;re compatible with the existing serving infrastructure and client expectations. This leads to runtime errors when input schemas change, output formats differ from client expectations, or model versions require different preprocessing pipelines. The solution involves implementing comprehensive compatibility testing that validates input/output schemas, response format consistency, and integration with upstream and downstream systems before deployment.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Performance Testing</strong>\nMany deployment implementations lack sufficient performance testing under realistic conditions, leading to surprising performance degradation or failures when models encounter production traffic patterns. This happens because development testing often uses synthetic data that doesn&#39;t match production characteristics, testing doesn&#39;t account for concurrent request processing, or testing environments don&#39;t match production infrastructure specifications. The fix involves implementing comprehensive performance testing with production-like data distributions, realistic concurrency levels, and infrastructure that matches production specifications.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides concrete code examples and infrastructure templates for building a production-ready model deployment system. The focus is on creating maintainable, scalable code that implements the architectural decisions and strategies described above.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Load Balancer</td>\n<td>HAProxy with configuration files</td>\n<td>NGINX Plus with dynamic configuration API</td>\n</tr>\n<tr>\n<td>Container Orchestration</td>\n<td>Docker Compose with health checks</td>\n<td>Kubernetes with custom operators</td>\n</tr>\n<tr>\n<td>Inference Servers</td>\n<td>MLflow serving for all models</td>\n<td>Framework-specific (TF Serving, TorchServe, Triton)</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Prometheus + Grafana</td>\n<td>Full observability stack (Jaeger, Prometheus, Grafana)</td>\n</tr>\n<tr>\n<td>Traffic Management</td>\n<td>Simple round-robin routing</td>\n<td>Istio service mesh with advanced traffic policies</td>\n</tr>\n<tr>\n<td>Auto-scaling</td>\n<td>Basic threshold-based scaling</td>\n<td>Kubernetes HPA with custom metrics</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>deployment/\n├── deployment_controller.py      ← Main deployment orchestration logic\n├── traffic_manager.py           ← Traffic routing and canary management\n├── serving_backend.py           ← Inference server integration\n├── auto_scaler.py              ← Auto-scaling policies and execution\n├── health_monitor.py           ← Health checking and readiness validation\n├── config/\n│   ├── serving_templates/      ← Server-specific config templates\n│   └── deployment_policies.yaml ← Default deployment strategies\n├── infrastructure/\n│   ├── kubernetes/             ← K8s manifests and operators\n│   ├── docker/                ← Container definitions and scripts\n│   └── monitoring/             ← Monitoring configuration\n└── tests/\n    ├── integration/            ← End-to-end deployment tests\n    └── performance/            ← Load testing and benchmarks</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Complete Health Monitoring System:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> aiohttp</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">any</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelServingHealthMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive health monitoring for model serving instances.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, check_interval: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.check_interval </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_interval</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.health_checks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_results: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, HealthCheck] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_check</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: Callable) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a health check function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.health_checks[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_func</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> check_model_endpoint</span><span style=\"color:#E1E4E8\">(self, endpoint_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> HealthCheck:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if model serving endpoint is responding correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#E1E4E8\"> aiohttp.ClientSession() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> session:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Health check endpoint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                health_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">endpoint_url</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/health\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#E1E4E8\"> session.get(health_url, </span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> response:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    if</span><span style=\"color:#E1E4E8\"> response.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        health_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> response.json()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            name</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"endpoint_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            message</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Endpoint responding\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"response_time\"</span><span style=\"color:#E1E4E8\">: response.headers.get(</span><span style=\"color:#9ECBFF\">'X-Response-Time'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                        return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            name</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"endpoint_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"HTTP </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">response.status</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                            details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"status_code\"</span><span style=\"color:#E1E4E8\">: response.status}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"endpoint_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">model_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Connection failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> check_resource_usage</span><span style=\"color:#E1E4E8\">(self, instance_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> HealthCheck:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check resource usage for serving instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Implementation would integrate with actual monitoring system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This is a simplified example</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Simulate resource check</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cpu_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 45.2</span><span style=\"color:#6A737D\">  # Would get from actual monitoring</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            memory_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 67.8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> cpu_usage </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 90</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> memory_usage </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 95</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                message </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"Resource exhaustion\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> cpu_usage </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 70</span><span style=\"color:#F97583\"> or</span><span style=\"color:#E1E4E8\"> memory_usage </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 80</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                message </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"High resource usage\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                message </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"Resources normal\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"resources_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">instance_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">status,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">message,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"cpu_percent\"</span><span style=\"color:#E1E4E8\">: cpu_usage, </span><span style=\"color:#9ECBFF\">\"memory_percent\"</span><span style=\"color:#E1E4E8\">: memory_usage}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"resources_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">instance_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNKNOWN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to check resources: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> run_all_checks</span><span style=\"color:#E1E4E8\">(self) -> List[HealthCheck]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all registered health checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> name, check_func </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.health_checks.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> check_func()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.last_results[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(result)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                error_result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNKNOWN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Check failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.last_results[name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> error_result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(error_result)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> start_monitoring</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start continuous health monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.run_all_checks()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            await</span><span style=\"color:#E1E4E8\"> asyncio.sleep(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.check_interval)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop_monitoring</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop health monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_overall_health</span><span style=\"color:#E1E4E8\">(self) -> HealthStatus:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine overall system health from individual checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_results:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNKNOWN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        statuses </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [check.status </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> check </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_results.values()]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> status </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> statuses):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> status </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> statuses):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> all</span><span style=\"color:#E1E4E8\">(status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> status </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> statuses):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNKNOWN</span></span></code></pre></div>\n\n<p><strong>Complete Traffic Routing System:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelEndpoint</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    endpoint_url: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    weight: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    health_status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RoutingRule</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    traffic_percentage: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_version: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    conditions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># Header-based routing conditions</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrafficRouter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract base for traffic routing strategies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> route_request</span><span style=\"color:#E1E4E8\">(self, request_context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     available_endpoints: List[ModelEndpoint]) -> Optional[ModelEndpoint]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CanaryTrafficRouter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">TrafficRouter</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Implements canary deployment traffic routing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, canary_percentage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.canary_percentage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> canary_percentage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> route_request</span><span style=\"color:#E1E4E8\">(self, request_context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     available_endpoints: List[ModelEndpoint]) -> Optional[ModelEndpoint]:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Filter to healthy endpoints only</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        healthy_endpoints </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [ep </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> ep </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> available_endpoints </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                           if</span><span style=\"color:#E1E4E8\"> ep.health_status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> healthy_endpoints:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Separate stable and canary versions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stable_endpoints </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [ep </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> ep </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> healthy_endpoints </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> ep.weight </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 90.0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        canary_endpoints </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [ep </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> ep </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> healthy_endpoints </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> ep.weight </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 90.0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Route based on traffic split</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> canary_endpoints </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> random.random() </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#F97583\"> &#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.canary_percentage:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> random.choice(canary_endpoints)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> stable_endpoints:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> random.choice(stable_endpoints)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> random.choice(healthy_endpoints)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ConsistentHashRouter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">TrafficRouter</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Routes requests consistently based on user ID for A/B testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> route_request</span><span style=\"color:#E1E4E8\">(self, request_context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     available_endpoints: List[ModelEndpoint]) -> Optional[ModelEndpoint]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        user_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> request_context.get(</span><span style=\"color:#9ECBFF\">'user_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'anonymous'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Filter to healthy endpoints</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        healthy_endpoints </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [ep </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> ep </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> available_endpoints </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                           if</span><span style=\"color:#E1E4E8\"> ep.health_status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> healthy_endpoints:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use consistent hashing to ensure same user gets same version</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hash_value </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(hashlib.md5(user_id.encode()).hexdigest(), </span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        endpoint_index </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> hash_value </span><span style=\"color:#F97583\">%</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(healthy_endpoints)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> healthy_endpoints[endpoint_index]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TrafficManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages traffic routing and deployment strategies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.endpoints: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[ModelEndpoint]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.routing_rules: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, RoutingRule] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.routers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TrafficRouter] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'canary'</span><span style=\"color:#E1E4E8\">: CanaryTrafficRouter(),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'consistent_hash'</span><span style=\"color:#E1E4E8\">: ConsistentHashRouter()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_endpoint</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: ModelEndpoint):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a new model serving endpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> model_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.endpoints:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.endpoints[model_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.endpoints[model_name].append(endpoint)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_endpoint_health</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              health_status: HealthStatus):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update health status for specific endpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> model_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.endpoints:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> endpoint </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.endpoints[model_name]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> endpoint.version </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> version:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    endpoint.health_status </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> health_status</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> set_traffic_split</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version_weights: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set traffic split percentages for model versions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> model_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.endpoints:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> endpoint </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.endpoints[model_name]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> endpoint.version </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> version_weights:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    endpoint.weight </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> version_weights[endpoint.version]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> route_request</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, request_context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     strategy: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'canary'</span><span style=\"color:#E1E4E8\">) -> Optional[ModelEndpoint]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Route request to appropriate model endpoint.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> model_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.endpoints:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> strategy </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.routers:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            strategy </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'canary'</span><span style=\"color:#6A737D\">  # Default fallback</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        router </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.routers[strategy]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> router.route_request(request_context, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.endpoints[model_name])</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton\">Core Logic Skeleton</h4>\n<p><strong>Deployment Controller (Core implementation for learners):</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DeploymentStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PENDING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pending\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEPLOYING</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"deploying\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    ROLLING_BACK</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"rolling_back\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DeploymentSpec</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_version: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    target_replicas: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_requirements: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    deployment_strategy: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    traffic_split: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelDeploymentController</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Core deployment controller - implement the TODOs below.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, traffic_manager: TrafficManager, health_monitor: ModelServingHealthMonitor):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.traffic_manager </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> traffic_manager</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.health_monitor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> health_monitor</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active_deployments: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, DeploymentSpec] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.deployment_history: List[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> deploy_model_version</span><span style=\"color:#E1E4E8\">(self, deployment_spec: DeploymentSpec) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deploy new model version using specified strategy.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns deployment_id for tracking.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate deployment spec (check model exists in registry, validate resources)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Generate unique deployment ID and store deployment spec</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Based on deployment_strategy, choose deployment method:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'blue_green': call _deploy_blue_green()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'canary': call _deploy_canary() </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'rolling': call _deploy_rolling_update()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create serving instances with specified resource requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Register health checks for new instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Wait for instances to pass health checks before proceeding</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Update traffic routing based on deployment strategy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return deployment ID</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.traffic_manager.register_endpoint() for new instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.health_monitor.register_check() for health monitoring</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _deploy_canary</span><span style=\"color:#E1E4E8\">(self, deployment_spec: DeploymentSpec) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Implement canary deployment strategy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start new model version with 5% traffic allocation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Monitor key metrics (latency, error rate, accuracy) for 10 minutes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If metrics are acceptable, increase traffic to 25%</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Continue monitoring and gradually increase: 50% -> 75% -> 100%</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: At each stage, validate metrics haven't degraded beyond thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If metrics degrade, immediately rollback to previous version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Update deployment status throughout the process</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return True if successful, False if rollback was needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use time.sleep() or asyncio.sleep() between traffic increase stages</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check self.health_monitor.get_overall_health() for validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _deploy_blue_green</span><span style=\"color:#E1E4E8\">(self, deployment_spec: DeploymentSpec) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Implement blue-green deployment strategy.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Deploy new version to separate \"green\" environment (0% traffic)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run full validation suite against green environment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If validation passes, switch 100% traffic to green environment atomically</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Monitor for 5 minutes to ensure no issues with traffic switch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If successful, mark blue environment for termination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: If issues detected, immediately switch traffic back to blue</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Update deployment status and return success/failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use self.traffic_manager.set_traffic_split() for atomic switch</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Keep blue environment running during monitoring period for fast rollback</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> rollback_deployment</span><span style=\"color:#E1E4E8\">(self, deployment_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Rollback failed deployment to previous version.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Look up deployment spec and current state using deployment_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify previous stable version from deployment history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Immediately route 100% traffic to previous stable version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Scale down failed version instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update deployment status to indicate rollback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Log rollback event with failure reason for analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return True if rollback successful, False if rollback failed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Rollback should be as fast as possible - don't wait for graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Keep detailed logs for post-incident analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scale_deployment</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, target_replicas: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Scale existing deployment to target replica count.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate current deployment exists and is healthy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate scaling direction (up or down) and number of replicas to change</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For scale-up: create new instances and wait for health checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For scale-down: gracefully shutdown excess instances after draining</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update traffic routing to include/exclude scaled instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Monitor overall deployment health during scaling operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Update deployment spec with new replica count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return success/failure status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: For scale-down, ensure in-flight requests complete before shutdown</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Scale gradually (e.g., change 25% of replicas at a time)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_deployment_status</span><span style=\"color:#E1E4E8\">(self, deployment_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get current status of deployment including health and metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Look up deployment using deployment_id</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Collect current health status from all instances</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Gather performance metrics (latency, throughput, error rate)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check traffic distribution across versions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Determine overall deployment health status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Format response with all relevant status information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return comprehensive status dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Include timestamps for all status information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Return None if deployment_id not found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing basic deployment:</strong></p>\n<ul>\n<li>Run: <code>python -m pytest tests/test_deployment_controller.py::test_basic_deployment</code></li>\n<li>Expected: All tests pass, showing successful model deployment and health checking</li>\n<li>Manual verification: Deploy a simple model and confirm it responds to inference requests</li>\n<li>Check logs for proper health check execution and traffic routing updates</li>\n</ul>\n<p><strong>After implementing canary deployments:</strong></p>\n<ul>\n<li>Run: <code>python test_canary_deployment.py</code> with a test that simulates traffic split</li>\n<li>Expected: Traffic gradually shifts from 5% → 25% → 50% → 75% → 100% over time</li>\n<li>Manual verification: Deploy new version and observe traffic split changes in monitoring dashboard</li>\n<li>Verify rollback functionality by introducing a &quot;bad&quot; model version that triggers alerts</li>\n</ul>\n<p><strong>After implementing auto-scaling:</strong></p>\n<ul>\n<li>Run: <code>python test_autoscaling.py</code> with load testing that generates traffic spikes  </li>\n<li>Expected: System automatically scales up during load, scales down when traffic decreases</li>\n<li>Manual verification: Generate load using <code>hey -n 10000 -c 50 http://model-endpoint/predict</code></li>\n<li>Check that P95 latency stays below configured thresholds during scaling events</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>New deployments stay in PENDING</td>\n<td>Resource allocation failure</td>\n<td>Check cluster capacity with <code>kubectl describe nodes</code></td>\n<td>Adjust resource requests or scale cluster</td>\n</tr>\n<tr>\n<td>Canary traffic not splitting correctly</td>\n<td>Routing configuration error</td>\n<td>Verify traffic split percentages in load balancer config</td>\n<td>Update routing rules and restart load balancer</td>\n</tr>\n<tr>\n<td>Health checks failing for running instances</td>\n<td>Model loading timeout</td>\n<td>Check container logs for model loading errors</td>\n<td>Increase health check timeout or optimize model loading</td>\n</tr>\n<tr>\n<td>Rollback takes too long</td>\n<td>Graceful shutdown blocking</td>\n<td>Monitor in-flight requests during rollback</td>\n<td>Implement forced termination after timeout</td>\n</tr>\n<tr>\n<td>Auto-scaling oscillating</td>\n<td>Thresholds too sensitive</td>\n<td>Analyze scaling metrics over time</td>\n<td>Adjust scaling thresholds and evaluation windows</td>\n</tr>\n<tr>\n<td>High latency during deployments</td>\n<td>Cold start issues</td>\n<td>Monitor instance startup times</td>\n<td>Implement model warming and keep spare capacity</td>\n</tr>\n</tbody></table>\n<h2 id=\"model-monitoring-component\">Model Monitoring Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section primarily corresponds to Milestone 5 (Model Monitoring), which focuses on monitoring model performance in production through prediction logging, drift detection, and automated alerting. This component integrates with the Model Deployment Component (Milestone 4) to observe deployed models and provides feedback that may influence future experiments (Milestone 1) and model promotions (Milestone 2).</p>\n</blockquote>\n<h3 id=\"mental-model-health-monitoring-system\">Mental Model: Health Monitoring System</h3>\n<p>Think of model monitoring as a comprehensive medical health monitoring system for deployed ML models. Just as a hospital continuously monitors a patient&#39;s vital signs—heart rate, blood pressure, temperature, oxygen levels—to detect early warning signs of health problems, model monitoring continuously tracks key &quot;vital signs&quot; of production ML models to detect performance degradation before it impacts business outcomes.</p>\n<p>In this analogy, <strong>prediction requests</strong> are like individual heartbeats—each one provides a data point about the model&#39;s current state. The <strong>prediction logging system</strong> acts like an electrocardiogram (EKG) machine, continuously recording every heartbeat with precise timestamps and measurements. <strong>Latency metrics</strong> are like blood pressure readings—high values indicate the system is under stress and may not be functioning optimally. <strong>Accuracy metrics</strong> are like core body temperature—significant deviations from the baseline indicate something is seriously wrong and requires immediate attention.</p>\n<p><strong>Data drift detection</strong> functions like blood chemistry analysis, comparing current blood samples (incoming feature distributions) against healthy baseline values (training data distributions). When the chemistry changes significantly—perhaps due to medication, diet changes, or illness—medical professionals investigate the root cause. Similarly, when input data distributions shift significantly from training distributions, data scientists investigate whether the model needs retraining or the data pipeline has issues.</p>\n<p><strong>Model drift detection</strong> is analogous to monitoring cognitive function over time. Just as doctors track whether a patient&#39;s mental responses and decision-making abilities remain consistent, model drift detection tracks whether the model&#39;s prediction patterns remain stable. Sudden changes in prediction distributions might indicate concept drift—the underlying relationship between inputs and outputs has changed in the real world.</p>\n<p>The <strong>alerting system</strong> acts like hospital alarm systems that trigger when vital signs move outside safe ranges. Different alert severities correspond to different urgency levels: a yellow alert for minor accuracy degradation is like a slightly elevated temperature (worth monitoring), while a red alert for severe data drift is like a cardiac emergency requiring immediate intervention.</p>\n<p><strong>Monitoring dashboards</strong> serve as the patient monitoring displays that medical staff use to track trends over time. They show recent values, historical patterns, and highlight anomalies that require attention. Just as medical professionals look for patterns across multiple vital signs to diagnose complex conditions, data scientists use monitoring dashboards to correlate multiple model health metrics to understand system-wide issues.</p>\n<p>This health monitoring metaphor is powerful because it emphasizes the <strong>continuous, automated, and proactive</strong> nature of effective model monitoring. You don&#39;t wait for a patient to collapse before checking their vital signs, and you shouldn&#39;t wait for business metrics to degrade before monitoring model performance. The monitoring system should detect problems early and escalate appropriately based on severity.</p>\n<h3 id=\"prediction-logging-and-metrics\">Prediction Logging and Metrics</h3>\n<p>The prediction logging system forms the foundation of model monitoring by capturing every inference request and response, along with contextual metadata necessary for performance analysis. This system must handle high-throughput production traffic while maintaining low latency overhead and ensuring data consistency for accurate metric computation.</p>\n<h4 id=\"request-and-response-capture\">Request and Response Capture</h4>\n<p>Every prediction request flowing through deployed model endpoints gets intercepted and logged by the monitoring system. The <strong>prediction logger</strong> operates as middleware in the inference serving stack, positioned between the load balancer and model serving instances to capture complete request context.</p>\n<p>The logging mechanism captures several categories of information for each prediction request. <strong>Input features</strong> are recorded exactly as they were sent to the model, preserving both feature names and values in their original data types. This enables downstream drift detection and feature importance analysis. <strong>Model outputs</strong> include both the final prediction and any intermediate outputs like confidence scores, class probabilities, or attention weights that provide insight into model decision-making.</p>\n<p><strong>Request metadata</strong> captures contextual information about the inference environment. This includes the model version that processed the request, the serving instance identifier, the geographic region where inference occurred, and client identification when available. <strong>Timing information</strong> records request arrival time, inference start and completion times, and any queueing delays that contribute to overall latency.</p>\n<p><strong>Correlation identifiers</strong> link prediction logs to broader request tracing systems, enabling end-to-end latency analysis across microservices. When a single user action triggers multiple model predictions—such as recommendation ranking followed by click-through prediction—correlation IDs help analyze the complete interaction sequence.</p>\n<p>The prediction logging system implements <strong>sampling strategies</strong> to manage storage costs for high-volume services while maintaining statistical validity. Random sampling captures a representative subset of all predictions, while stratified sampling ensures adequate coverage across different user segments, feature ranges, or time periods. Critical predictions—such as those triggering high-value business decisions—may be logged with 100% capture rate regardless of sampling configuration.</p>\n<table>\n<thead>\n<tr>\n<th>Logged Data Category</th>\n<th>Fields Captured</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Input Features</td>\n<td>feature_name, feature_value, feature_type, feature_schema_version</td>\n<td>Drift detection, feature importance analysis, model debugging</td>\n</tr>\n<tr>\n<td>Model Outputs</td>\n<td>prediction_value, confidence_score, class_probabilities, model_version</td>\n<td>Performance analysis, A/B testing, model comparison</td>\n</tr>\n<tr>\n<td>Request Metadata</td>\n<td>request_id, model_name, model_version, serving_instance_id, client_id</td>\n<td>Request tracing, capacity planning, error attribution</td>\n</tr>\n<tr>\n<td>Timing Information</td>\n<td>request_timestamp, inference_start_time, inference_end_time, queue_time</td>\n<td>Latency analysis, performance optimization, SLA monitoring</td>\n</tr>\n<tr>\n<td>Context Data</td>\n<td>geographical_region, user_segment, experiment_group, correlation_id</td>\n<td>Segmented analysis, A/B testing, multi-model workflows</td>\n</tr>\n</tbody></table>\n<h4 id=\"latency-and-throughput-measurement\">Latency and Throughput Measurement</h4>\n<p><strong>Latency tracking</strong> measures the time required to process prediction requests at multiple granularities. <strong>End-to-end latency</strong> captures the complete time from request arrival to response delivery, including network transmission, queueing delays, model inference, and response serialization. This metric directly impacts user experience and must stay below configured Service Level Objectives (SLOs).</p>\n<p><strong>Inference latency</strong> isolates the time spent in actual model computation, excluding network and queueing overhead. This metric helps distinguish between model performance issues and infrastructure capacity problems. When end-to-end latency increases but inference latency remains stable, the issue likely stems from resource contention or networking problems rather than model complexity.</p>\n<p>The system tracks latency using <strong>percentile distributions</strong> rather than simple averages, as averages can mask performance issues affecting a minority of requests. The P50 (median), P95, P99, and P99.9 percentiles provide insight into typical performance and tail latency behavior. A model with excellent average latency but poor P99 latency creates unpredictable user experiences that may impact business metrics.</p>\n<p><strong>Throughput measurement</strong> tracks the number of prediction requests processed per unit time, typically measured in requests per second (RPS) or predictions per minute. Peak throughput helps determine infrastructure capacity requirements, while sustained throughput indicates normal operational load. Throughput analysis identifies traffic patterns—daily cycles, seasonal variations, and sudden spikes—that inform auto-scaling decisions.</p>\n<p>The monitoring system correlates <strong>latency and throughput</strong> to identify performance characteristics under different load conditions. Some models maintain consistent latency until reaching a throughput threshold, then experience rapid latency degradation. Other models show gradual latency increases as throughput rises. Understanding these relationships helps set appropriate auto-scaling triggers and capacity planning decisions.</p>\n<table>\n<thead>\n<tr>\n<th>Latency Metric</th>\n<th>Measurement Method</th>\n<th>Alert Thresholds</th>\n<th>Business Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>P50 Latency</td>\n<td>Median request processing time</td>\n<td>&gt; 100ms warning, &gt; 200ms critical</td>\n<td>User experience degradation</td>\n</tr>\n<tr>\n<td>P95 Latency</td>\n<td>95th percentile processing time</td>\n<td>&gt; 300ms warning, &gt; 500ms critical</td>\n<td>Poor experience for some users</td>\n</tr>\n<tr>\n<td>P99 Latency</td>\n<td>99th percentile processing time</td>\n<td>&gt; 1000ms warning, &gt; 2000ms critical</td>\n<td>Unacceptable delays for edge cases</td>\n</tr>\n<tr>\n<td>Inference Latency</td>\n<td>Time in model computation only</td>\n<td>&gt; 50ms warning, &gt; 100ms critical</td>\n<td>Model complexity issues</td>\n</tr>\n<tr>\n<td>Queue Time</td>\n<td>Time waiting for available resources</td>\n<td>&gt; 10ms warning, &gt; 50ms critical</td>\n<td>Insufficient serving capacity</td>\n</tr>\n</tbody></table>\n<h4 id=\"performance-metric-aggregation\">Performance Metric Aggregation</h4>\n<p>The monitoring system computes <strong>aggregated performance metrics</strong> across multiple time windows to provide both real-time insights and historical trend analysis. <strong>Real-time metrics</strong> use sliding windows of recent predictions—typically the last 1, 5, and 15 minutes—to enable rapid detection of performance degradation. <strong>Historical metrics</strong> aggregate data over hourly, daily, and weekly periods to identify long-term trends and seasonal patterns.</p>\n<p><strong>Accuracy measurement</strong> in production environments faces the challenge that ground truth labels are rarely available immediately after prediction. The system implements several strategies to estimate model accuracy. <strong>Delayed feedback</strong> incorporates ground truth labels when they become available—such as user clicks, conversion events, or manual annotations—and retroactively computes accuracy metrics. <strong>Proxy metrics</strong> use correlated signals available in real-time, such as user engagement rates or downstream system responses, as approximate indicators of model performance.</p>\n<p><strong>Business metric correlation</strong> links model performance to measurable business outcomes. For recommendation models, this might include click-through rates, conversion rates, or revenue per impression. For fraud detection models, this could track false positive rates impacting customer experience and false negative rates impacting financial losses. These correlations help prioritize model improvements based on business impact rather than purely technical metrics.</p>\n<p>The aggregation system implements <strong>statistical significance testing</strong> for metric comparisons across time periods or model versions. When comparing current performance to historical baselines, the system computes confidence intervals and p-values to distinguish meaningful changes from normal statistical variation. This prevents alert fatigue from triggering on insignificant metric fluctuations.</p>\n<p><strong>Segmented analysis</strong> computes metrics across different user populations, geographic regions, or feature value ranges. A model may maintain overall accuracy while degrading for specific user segments, indicating dataset bias or insufficient training data coverage. Segmented metrics help identify these localized performance issues that might be masked in aggregate statistics.</p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Computation Method</th>\n<th>Update Frequency</th>\n<th>Retention Period</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Real-time Accuracy</td>\n<td>Sliding window with available labels</td>\n<td>Every 60 seconds</td>\n<td>7 days</td>\n</tr>\n<tr>\n<td>Business KPIs</td>\n<td>Correlation with downstream events</td>\n<td>Every 5 minutes</td>\n<td>6 months</td>\n</tr>\n<tr>\n<td>Latency Percentiles</td>\n<td>Quantile estimation over time windows</td>\n<td>Every 30 seconds</td>\n<td>30 days</td>\n</tr>\n<tr>\n<td>Error Rates</td>\n<td>Failure count / total request count</td>\n<td>Every 60 seconds</td>\n<td>90 days</td>\n</tr>\n<tr>\n<td>Throughput</td>\n<td>Request count per time window</td>\n<td>Every 15 seconds</td>\n<td>1 year</td>\n</tr>\n</tbody></table>\n<h3 id=\"data-and-model-drift-detection\">Data and Model Drift Detection</h3>\n<p>Drift detection identifies when the statistical properties of model inputs or outputs change significantly compared to training data or historical baselines. These changes can indicate underlying shifts in user behavior, data collection processes, or real-world phenomena that affect model validity.</p>\n<h4 id=\"statistical-drift-detection-methods\">Statistical Drift Detection Methods</h4>\n<p><strong>Data drift detection</strong> compares the distribution of incoming features against the distribution observed in training data. The system maintains <strong>reference distributions</strong> computed from the original training dataset, stored as statistical summaries rather than raw data to protect privacy and reduce storage requirements. For numerical features, reference distributions include mean, variance, quantiles, and histogram bins. For categorical features, they include class frequencies and unique value counts.</p>\n<p>The <strong>Kolmogorov-Smirnov (KS) test</strong> evaluates whether incoming numerical features follow the same distribution as training data. The KS statistic measures the maximum difference between cumulative distribution functions, with larger values indicating greater distributional differences. The system computes KS statistics over rolling windows of recent predictions and triggers drift alerts when values exceed calibrated thresholds.</p>\n<p><strong>Population Stability Index (PSI)</strong> provides a single metric summarizing drift across all features simultaneously. PSI compares the percentage of samples falling into each histogram bin between current and reference distributions. Values below 0.1 indicate minimal drift, values between 0.1 and 0.25 suggest moderate drift requiring investigation, and values above 0.25 indicate severe drift necessitating model retraining.</p>\n<p>For categorical features, <strong>chi-squared tests</strong> evaluate whether the observed frequency distribution matches expected frequencies from training data. The test statistic follows a known distribution, enabling p-value computation and statistical significance assessment. High chi-squared values with low p-values indicate significant distributional changes.</p>\n<p><strong>Jensen-Shannon divergence</strong> measures the difference between probability distributions in a symmetric, bounded metric. Unlike KL divergence, JS divergence remains finite even when distributions have non-overlapping support, making it robust for real-world data where new categorical values may appear in production. The system normalizes JS divergence scores to a 0-1 scale for consistent threshold setting across features.</p>\n<table>\n<thead>\n<tr>\n<th>Drift Detection Method</th>\n<th>Applicable Feature Types</th>\n<th>Computational Complexity</th>\n<th>Sensitivity</th>\n<th>Interpretability</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Kolmogorov-Smirnov</td>\n<td>Numerical continuous</td>\n<td>O(n log n)</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Population Stability Index</td>\n<td>Numerical binned</td>\n<td>O(b) where b = bins</td>\n<td>Medium</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Chi-squared Test</td>\n<td>Categorical</td>\n<td>O(k) where k = categories</td>\n<td>High</td>\n<td>High</td>\n</tr>\n<tr>\n<td>Jensen-Shannon Divergence</td>\n<td>Both numerical and categorical</td>\n<td>O(b) or O(k)</td>\n<td>Medium</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Two-sample t-test</td>\n<td>Numerical with normal distribution</td>\n<td>O(n)</td>\n<td>Medium for mean shifts</td>\n<td>High</td>\n</tr>\n</tbody></table>\n<h4 id=\"feature-distribution-monitoring\">Feature Distribution Monitoring</h4>\n<p>The monitoring system tracks <strong>univariate distributions</strong> for each input feature independently, computing drift metrics on a per-feature basis. This granular approach enables identification of specific features experiencing drift, facilitating targeted investigation and remediation. Feature-level drift scores aggregate into overall dataset drift scores using weighted averages based on feature importance or business relevance.</p>\n<p><strong>Multivariate drift detection</strong> identifies changes in feature correlations and interactions that univariate methods might miss. <strong>Principal Component Analysis (PCA)</strong> projects features into lower-dimensional spaces where distributional changes become more apparent. Drift in the first few principal components often indicates systematic changes in data collection or user behavior patterns.</p>\n<p><strong>Covariate shift detection</strong> specifically identifies changes in input feature distributions while assuming the underlying relationship between features and target variables remains constant. This type of drift is common when model deployment expands to new geographic markets, user segments, or time periods with different demographic characteristics but similar behavioral patterns.</p>\n<p>The system implements <strong>adaptive thresholds</strong> that adjust drift detection sensitivity based on historical patterns. Models serving highly seasonal businesses—such as retail or travel—experience predictable feature distribution changes throughout the year. Adaptive thresholds learn these seasonal patterns and avoid triggering false alerts during expected distribution shifts.</p>\n<p><strong>Feature importance weighting</strong> prioritizes drift detection for features that most strongly influence model predictions. Small drift in highly important features may be more concerning than large drift in features with minimal predictive power. The system computes feature importance scores using model-specific methods—such as SHAP values or permutation importance—and weights drift scores accordingly.</p>\n<h4 id=\"concept-drift-analysis\">Concept Drift Analysis</h4>\n<p><strong>Concept drift</strong> occurs when the relationship between input features and target variables changes over time, even if feature distributions remain stable. Unlike data drift, concept drift requires ground truth labels to detect, making it more challenging to identify in real-time production systems.</p>\n<p><strong>Prediction distribution monitoring</strong> provides an early signal of potential concept drift by tracking changes in model output distributions. Sudden shifts in prediction confidence, class probability distributions, or regression value ranges may indicate underlying concept changes before ground truth feedback becomes available. The system monitors prediction distributions using the same statistical methods applied to input features.</p>\n<p><strong>Performance degradation tracking</strong> identifies concept drift through declining model accuracy over time. The system maintains <strong>rolling accuracy estimates</strong> computed from available ground truth labels, accounting for label delay and coverage gaps. Significant accuracy decreases that cannot be explained by data drift suggest concept drift affecting model validity.</p>\n<p><strong>Temporal analysis</strong> examines drift patterns over different time scales to distinguish between temporary fluctuations and sustained changes. <strong>Short-term drift</strong> might indicate transient events like marketing campaigns or news cycles that affect user behavior temporarily. <strong>Long-term drift</strong> suggests fundamental changes in the underlying domain requiring model updates or retraining.</p>\n<p>The system implements <strong>change point detection</strong> algorithms that identify specific timestamps when drift began, helping correlate model performance changes with external events. <strong>CUSUM (Cumulative Sum)</strong> algorithms detect changes in statistical properties of time series data, while <strong>Bayesian change point detection</strong> provides probabilistic estimates of when distribution shifts occurred.</p>\n<table>\n<thead>\n<tr>\n<th>Concept Drift Type</th>\n<th>Detection Method</th>\n<th>Response Strategy</th>\n<th>Example Scenarios</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sudden Drift</td>\n<td>Change point detection on accuracy metrics</td>\n<td>Immediate model rollback</td>\n<td>Algorithm updates, policy changes</td>\n</tr>\n<tr>\n<td>Gradual Drift</td>\n<td>Linear trend analysis over time windows</td>\n<td>Scheduled retraining</td>\n<td>Seasonal behavior changes</td>\n</tr>\n<tr>\n<td>Incremental Drift</td>\n<td>Moving average convergence tracking</td>\n<td>Continuous learning updates</td>\n<td>User preference evolution</td>\n</tr>\n<tr>\n<td>Recurring Drift</td>\n<td>Seasonal decomposition analysis</td>\n<td>Scheduled model switching</td>\n<td>Holiday shopping patterns</td>\n</tr>\n<tr>\n<td>Blip Drift</td>\n<td>Outlier detection in performance metrics</td>\n<td>Temporary monitoring increase</td>\n<td>Marketing campaign effects</td>\n</tr>\n</tbody></table>\n<h3 id=\"architecture-decisions\">Architecture Decisions</h3>\n<p>The model monitoring component requires several critical architecture decisions around data collection, storage, analysis, and alerting. These decisions significantly impact system performance, cost, and reliability.</p>\n<blockquote>\n<p><strong>Decision: Real-time vs Batch Drift Detection</strong></p>\n<ul>\n<li><strong>Context</strong>: Model monitoring must detect drift quickly enough to prevent business impact while managing computational costs for high-volume services processing millions of predictions daily.</li>\n<li><strong>Options Considered</strong>: (1) Real-time drift computation on every prediction, (2) Micro-batch processing every few minutes, (3) Hourly batch processing</li>\n<li><strong>Decision</strong>: Hybrid approach with micro-batch processing for drift detection and real-time aggregation for latency metrics</li>\n<li><strong>Rationale</strong>: Real-time drift computation is computationally prohibitive at scale—computing KS statistics on millions of samples per second would require enormous compute resources. Hourly batches are too slow to catch rapid drift that could impact business outcomes. Micro-batches every 5-15 minutes provide good balance between detection speed and computational efficiency.</li>\n<li><strong>Consequences</strong>: Enables drift detection within 15 minutes of occurrence while keeping compute costs manageable. However, extremely rapid drift might not be caught before affecting user experience. Requires careful batch size tuning to maintain statistical power.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Detection Speed</th>\n<th>Computational Cost</th>\n<th>Statistical Power</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Real-time (per request)</td>\n<td>&lt; 1 second</td>\n<td>Very high</td>\n<td>Low (small samples)</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Micro-batch (5-15 min)</td>\n<td>5-15 minutes</td>\n<td>Medium</td>\n<td>Medium</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>Hourly batch</td>\n<td>60+ minutes</td>\n<td>Low</td>\n<td>High</td>\n<td>No</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Prediction Logging Storage Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: Production models may generate millions of prediction logs daily, requiring storage that supports both high-throughput writes and complex analytical queries for drift detection and performance analysis.</li>\n<li><strong>Options Considered</strong>: (1) Relational database with time-series optimization, (2) NoSQL document store, (3) Columnar analytics database, (4) Object storage with query engine</li>\n<li><strong>Decision</strong>: Columnar analytics database (ClickHouse) with object storage backup</li>\n<li><strong>Rationale</strong>: Columnar storage provides excellent compression for repetitive prediction data and fast analytical queries across time ranges. ClickHouse specifically handles time-series data well with automatic partitioning by timestamp. Object storage backup provides cost-effective long-term retention.</li>\n<li><strong>Consequences</strong>: Enables fast drift detection queries and flexible analytics. However, requires specialized database expertise and careful schema design for optimal performance. Higher infrastructure complexity than simple document storage.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Write Performance</th>\n<th>Query Performance</th>\n<th>Storage Cost</th>\n<th>Operational Complexity</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PostgreSQL + TimescaleDB</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Low</td>\n<td>No</td>\n</tr>\n<tr>\n<td>MongoDB</td>\n<td>High</td>\n<td>Low for analytics</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>No</td>\n</tr>\n<tr>\n<td>ClickHouse</td>\n<td>High</td>\n<td>High for analytics</td>\n<td>Low (compression)</td>\n<td>High</td>\n<td><strong>Yes</strong></td>\n</tr>\n<tr>\n<td>S3 + Athena</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>Very low</td>\n<td>Medium</td>\n<td>Backup only</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Drift Detection Algorithm Selection</strong></p>\n<ul>\n<li><strong>Context</strong>: Different drift detection algorithms have varying computational complexity, sensitivity, and interpretability characteristics. The system must detect meaningful drift while avoiding false positives from normal statistical variation.</li>\n<li><strong>Options Considered</strong>: (1) Single algorithm (KS test) for simplicity, (2) Ensemble of multiple algorithms with voting, (3) Feature-type-specific algorithm selection</li>\n<li><strong>Decision</strong>: Feature-type-specific algorithm selection with configurable sensitivity</li>\n<li><strong>Rationale</strong>: Different feature types require different statistical tests—KS tests work well for continuous variables but are inappropriate for categorical data. PSI provides intuitive interpretability for business users. Algorithm selection based on feature characteristics maximizes detection accuracy.</li>\n<li><strong>Consequences</strong>: Better drift detection accuracy and fewer false positives. However, requires more complex implementation and algorithm-specific parameter tuning. Need expertise in multiple statistical methods.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Implementation Complexity</th>\n<th>Detection Accuracy</th>\n<th>False Positive Rate</th>\n<th>Interpretability</th>\n<th>Chosen?</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single algorithm (KS only)</td>\n<td>Low</td>\n<td>Medium</td>\n<td>Medium</td>\n<td>High</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Algorithm ensemble</td>\n<td>High</td>\n<td>High</td>\n<td>Low</td>\n<td>Low</td>\n<td>No</td>\n</tr>\n<tr>\n<td>Feature-type-specific</td>\n<td>Medium</td>\n<td>High</td>\n<td>Low</td>\n<td>High</td>\n<td><strong>Yes</strong></td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Alerting Threshold Management</strong></p>\n<ul>\n<li><strong>Context</strong>: Static drift thresholds generate excessive false positives for models with natural seasonal patterns or business cycles, while adaptive thresholds may miss genuine drift during expected variation periods.</li>\n<li><strong>Options Considered</strong>: (1) Static thresholds set during model deployment, (2) Adaptive thresholds based on historical patterns, (3) Hierarchical thresholds with multiple severity levels</li>\n<li><strong>Decision</strong>: Hierarchical thresholds with seasonal adjustment factors</li>\n<li><strong>Rationale</strong>: Multiple threshold levels (warning/critical/emergency) enable appropriate response escalation. Seasonal adjustment factors accommodate predictable business patterns while maintaining sensitivity to unexpected changes. Provides balance between alerting precision and operational overhead.</li>\n<li><strong>Consequences</strong>: Reduces alert fatigue while maintaining drift detection sensitivity. However, requires careful threshold calibration and seasonal pattern analysis during deployment. May miss drift that coincides with expected seasonal changes.</li>\n</ul>\n</blockquote>\n<h4 id=\"data-retention-and-storage-optimization\">Data Retention and Storage Optimization</h4>\n<p>The monitoring system must balance data retention requirements with storage costs and query performance. <strong>Prediction logs</strong> contain detailed request/response data needed for debugging and detailed analysis, but storing every prediction indefinitely becomes prohibitively expensive for high-volume services.</p>\n<p><strong>Tiered storage strategy</strong> implements different retention policies based on data age and detail level. <strong>Hot storage</strong> keeps detailed prediction logs for the most recent 7-30 days in high-performance storage optimized for analytical queries. <strong>Warm storage</strong> retains aggregated metrics and sampled predictions for 3-12 months in cost-optimized storage. <strong>Cold storage</strong> preserves statistical summaries and drift detection results for multi-year retention in archive storage.</p>\n<p><strong>Data compression and aggregation</strong> reduces storage requirements while preserving analytical capability. <strong>Lossless compression</strong> leverages the repetitive nature of prediction logs—model names, versions, and feature names repeat across millions of records. <strong>Lossy aggregation</strong> replaces detailed logs with statistical summaries when full precision is no longer needed for analysis.</p>\n<p><strong>Partitioning strategy</strong> organizes data for efficient query performance and cost-effective deletion. <strong>Time-based partitioning</strong> allows efficient queries over date ranges and enables automatic partition dropping for data retention. <strong>Model-based partitioning</strong> enables model-specific analysis without scanning unrelated data.</p>\n<p>The system implements <strong>data lifecycle policies</strong> that automatically transition data between storage tiers and delete expired data. These policies integrate with cost monitoring to maintain target storage budgets while ensuring adequate data availability for monitoring functions.</p>\n<table>\n<thead>\n<tr>\n<th>Storage Tier</th>\n<th>Retention Period</th>\n<th>Data Detail Level</th>\n<th>Query Performance</th>\n<th>Storage Cost</th>\n<th>Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Hot</td>\n<td>7-30 days</td>\n<td>Full prediction logs</td>\n<td>Sub-second</td>\n<td>High</td>\n<td>Real-time monitoring, debugging</td>\n</tr>\n<tr>\n<td>Warm</td>\n<td>3-12 months</td>\n<td>Aggregated metrics + samples</td>\n<td>Few seconds</td>\n<td>Medium</td>\n<td>Trend analysis, drift investigation</td>\n</tr>\n<tr>\n<td>Cold</td>\n<td>1-5 years</td>\n<td>Statistical summaries</td>\n<td>Minutes</td>\n<td>Low</td>\n<td>Compliance, historical analysis</td>\n</tr>\n<tr>\n<td>Archive</td>\n<td>5+ years</td>\n<td>Metadata only</td>\n<td>N/A</td>\n<td>Very low</td>\n<td>Audit trails, legal requirements</td>\n</tr>\n</tbody></table>\n<h4 id=\"alert-configuration-and-escalation\">Alert Configuration and Escalation</h4>\n<p><strong>Multi-level alerting</strong> implements different response procedures based on drift severity and business impact. <strong>Warning alerts</strong> notify data science teams about moderate drift that requires investigation but doesn&#39;t threaten immediate model performance. <strong>Critical alerts</strong> trigger automated responses like traffic reduction or model rollback when drift exceeds acceptable thresholds. <strong>Emergency alerts</strong> page on-call engineers for severe drift indicating potential data corruption or system compromise.</p>\n<p><strong>Alert correlation</strong> prevents notification flooding when multiple related metrics trigger simultaneously. When both data drift and model performance degrade together, the system sends a single correlated alert rather than separate notifications for each metric. <strong>Alert suppression</strong> prevents repeated notifications for ongoing issues that haven&#39;t been resolved.</p>\n<p><strong>Escalation policies</strong> ensure appropriate response timing based on drift severity. Warning alerts may wait for business hours, while critical alerts notify team members immediately regardless of time. <strong>Automatic escalation</strong> promotes unacknowledged alerts to higher severity levels after configured time delays.</p>\n<p>The alerting system integrates with <strong>external notification systems</strong> including email, Slack, PagerDuty, and webhooks for custom integrations. <strong>Alert routing</strong> directs different alert types to appropriate teams—data drift alerts go to data science teams while infrastructure issues go to DevOps teams.</p>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: Logging Every Feature for Every Prediction</strong>\nMany monitoring implementations capture complete feature vectors for every prediction request, leading to enormous storage costs and query performance problems. For models with hundreds of features serving millions of requests daily, this approach can generate terabytes of logs weekly. Instead, implement selective logging that captures all features for a statistical sample (1-10% of requests) and only key features for the remainder. Use feature importance scores to identify which features require continuous monitoring versus periodic validation.</p>\n<p>⚠️ <strong>Pitfall: Using Fixed Drift Thresholds Across All Features</strong>\nSetting the same drift threshold for all features ignores their varying importance and natural variability. A 10% distribution change in a critical feature may be alarming, while the same change in a low-importance feature may be meaningless. Additionally, some features naturally have higher variance than others—user age distributions change slowly while behavioral features may fluctuate rapidly. Use feature-specific thresholds calibrated based on historical variability and feature importance scores from the model.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Label Delay in Accuracy Monitoring</strong>\nProduction model accuracy cannot be measured immediately because ground truth labels arrive with significant delays—sometimes hours, days, or weeks after prediction. Monitoring systems that assume immediate label availability will severely underestimate model performance or fail entirely. Implement delayed accuracy computation that accounts for realistic label arrival patterns, and use proxy metrics (user engagement, downstream conversion rates) for real-time performance estimation.</p>\n<p>⚠️ <strong>Pitfall: Computing Drift on Insufficiently Large Sample Sizes</strong>\nStatistical drift tests require adequate sample sizes to distinguish meaningful changes from random variation. Computing KS tests on 100 predictions may trigger false positives from normal statistical fluctuation, while computing on 10 million predictions may detect statistically significant but practically meaningless drift. Calibrate minimum sample sizes based on effect size you want to detect—typically 1,000-10,000 samples for meaningful business impact.</p>\n<p>⚠️ <strong>Pitfall: Not Accounting for Seasonal Patterns in Drift Detection</strong>\nBusiness applications often have predictable seasonal patterns—e-commerce shows different behavior during holidays, financial models vary by quarter, recommendation systems change with trends. Drift detection that doesn&#39;t account for these patterns will trigger false positives during every seasonal transition. Implement seasonal adjustment factors or separate baseline distributions for different time periods (weekday/weekend, monthly, quarterly patterns).</p>\n<p>⚠️ <strong>Pitfall: Storing Prediction Logs Without Proper Data Governance</strong>\nModel prediction logs often contain sensitive personal information and may be subject to privacy regulations like GDPR. Storing these logs indefinitely without proper anonymization, encryption, or retention policies creates compliance risks. Implement data governance policies that anonymize or pseudonymize personal identifiers, encrypt logs at rest and in transit, and automatically delete data according to retention policies. Consider differential privacy techniques for long-term analytical datasets.</p>\n<p>⚠️ <strong>Pitfall: Not Implementing Monitoring for the Monitoring System</strong>\nMonitoring systems themselves can fail—data ingestion may stop, drift computation may crash, or alert delivery may break. When monitoring fails silently, model degradation goes undetected until business impact becomes obvious. Implement meta-monitoring that tracks monitoring system health: data ingestion rates, computation job success rates, alert delivery confirmation, and end-to-end monitoring pipeline latency. Alert when the monitoring system itself shows signs of failure.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Prediction Logging</td>\n<td>PostgreSQL with time-series extension + structured JSON</td>\n<td>ClickHouse or Apache Druid with columnar compression</td>\n</tr>\n<tr>\n<td>Drift Computation</td>\n<td>Python scikit-learn + pandas batch processing</td>\n<td>Apache Kafka Streams + custom drift processors</td>\n</tr>\n<tr>\n<td>Metrics Storage</td>\n<td>TimescaleDB with automatic aggregation</td>\n<td>InfluxDB with downsampling policies</td>\n</tr>\n<tr>\n<td>Alerting</td>\n<td>Simple email/Slack webhooks</td>\n<td>Prometheus + AlertManager with PagerDuty integration</td>\n</tr>\n<tr>\n<td>Dashboard</td>\n<td>Grafana with PostgreSQL datasource</td>\n<td>Custom React dashboard with real-time updates</td>\n</tr>\n<tr>\n<td>Statistical Computing</td>\n<td>scipy.stats for standard tests</td>\n<td>Custom implementations optimized for streaming data</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>monitoring/\n├── __init__.py\n├── core/\n│   ├── __init__.py\n│   ├── prediction_logger.py     ← Request/response capture\n│   ├── metrics_calculator.py    ← Latency and accuracy computation\n│   └── drift_detector.py        ← Statistical drift analysis\n├── storage/\n│   ├── __init__.py\n│   ├── log_store.py            ← Prediction log storage interface\n│   ├── metrics_store.py        ← Aggregated metrics storage\n│   └── clickhouse_adapter.py   ← ClickHouse-specific implementation\n├── alerting/\n│   ├── __init__.py\n│   ├── alert_engine.py         ← Threshold evaluation and notification\n│   ├── escalation_manager.py   ← Alert routing and escalation policies\n│   └── notification_channels.py ← Email, Slack, webhook integrations\n├── api/\n│   ├── __init__.py\n│   ├── monitoring_service.py   ← HTTP API for monitoring queries\n│   └── health_checks.py       ← Monitoring system health validation\n└── scripts/\n    ├── drift_computation_job.py ← Batch drift analysis\n    └── metrics_aggregation_job.py ← Periodic metric summarization</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Prediction Logger Interface:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PredictionLogEntry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Single prediction log entry with all captured metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    log_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    model_version: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    input_features: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    prediction_output: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_score: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    latency_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    request_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    client_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, model_version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               input_features: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any], prediction_output: Any,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               confidence_score: Optional[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               latency_ms: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">, request_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">               client_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'PredictionLogEntry'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            log_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            model_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            model_version</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">model_version,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            input_features</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">input_features,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            prediction_output</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">prediction_output,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            confidence_score</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">confidence_score,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            latency_ms</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">latency_ms,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            request_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">request_id </span><span style=\"color:#F97583\">or</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            client_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">client_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PredictionLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for logging prediction requests and responses.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_prediction</span><span style=\"color:#E1E4E8\">(self, log_entry: PredictionLogEntry) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log a single prediction entry. Returns success status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_batch</span><span style=\"color:#E1E4E8\">(self, entries: List[PredictionLogEntry]) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log multiple entries efficiently. Returns count of successful logs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_recent_predictions</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">) -> List[PredictionLogEntry]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve recent predictions for drift analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MemoryPredictionLogger</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">PredictionLogger</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"In-memory prediction logger for development and testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_entries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._logs: List[PredictionLogEntry] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._max_entries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_entries</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_prediction</span><span style=\"color:#E1E4E8\">(self, log_entry: PredictionLogEntry) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._logs.append(log_entry)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._logs) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._max_entries:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._logs.pop(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)  </span><span style=\"color:#6A737D\"># Remove oldest entry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_batch</span><span style=\"color:#E1E4E8\">(self, entries: List[PredictionLogEntry]) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> entry </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> entries:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.log_prediction(entry)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(entries)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_recent_predictions</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">) -> List[PredictionLogEntry]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cutoff_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> (hours </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 3600</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> [log </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> log </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._logs </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> log.model_name </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> model_name </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> log.timestamp </span><span style=\"color:#F97583\">>=</span><span style=\"color:#E1E4E8\"> cutoff_time]</span></span></code></pre></div>\n\n<p><strong>Metrics Calculation Framework:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> scipy </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> stats</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DriftSeverity</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NONE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"none\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LOW</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"low\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MEDIUM</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"medium\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HIGH</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"high\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CRITICAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"critical\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DriftResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result from drift detection analysis.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    feature_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    drift_score: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    severity: DriftSeverity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    test_statistic: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p_value: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    detection_method: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sample_size: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MetricsCalculator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate performance and drift metrics from prediction logs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, drift_thresholds: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Initialize with configurable drift thresholds.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Args:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            drift_thresholds: Dict mapping severity levels to threshold values</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                             e.g., {\"low\": 0.1, \"medium\": 0.25, \"high\": 0.5}</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.drift_thresholds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> drift_thresholds </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"low\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"medium\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.25</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"high\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"critical\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.75</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compute_latency_percentiles</span><span style=\"color:#E1E4E8\">(self, predictions: List[PredictionLogEntry]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute latency percentiles from prediction logs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        latencies </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [p.latency_ms </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> p </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> predictions]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> latencies:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"p50\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(np.percentile(latencies, </span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"p95\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(np.percentile(latencies, </span><span style=\"color:#79B8FF\">95</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"p99\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(np.percentile(latencies, </span><span style=\"color:#79B8FF\">99</span><span style=\"color:#E1E4E8\">)),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"mean\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(np.mean(latencies)),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"std\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(np.std(latencies))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_numerical_drift</span><span style=\"color:#E1E4E8\">(self, current_values: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             baseline_values: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             feature_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> DriftResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect drift in numerical features using Kolmogorov-Smirnov test.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement sample size validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute KS statistic and p-value using scipy.stats.ks_2samp</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Classify drift severity based on KS statistic and thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return DriftResult with all computed metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_categorical_drift</span><span style=\"color:#E1E4E8\">(self, current_values: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                baseline_values: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                feature_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> DriftResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect drift in categorical features using chi-squared test.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Build frequency distributions for current and baseline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle case where current data has categories not in baseline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute chi-squared statistic using scipy.stats.chisquare</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Classify severity based on chi-squared value and degrees of freedom</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return DriftResult with test statistics and interpretation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compute_psi</span><span style=\"color:#E1E4E8\">(self, current_values: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">], baseline_values: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   bins: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute Population Stability Index for numerical features.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create histogram bins based on baseline distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compute percentage of samples in each bin for both distributions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle bins with zero samples (add small epsilon)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate PSI = sum((current_pct - baseline_pct) * ln(current_pct / baseline_pct))</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return PSI value (0 = no drift, >0.25 = significant drift)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Drift Detection Engine:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DriftDetectionEngine</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main engine for detecting data and model drift in production.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, prediction_logger: PredictionLogger, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 baseline_store: BaselineStore, metrics_calculator: MetricsCalculator):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.prediction_logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> prediction_logger</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.baseline_store </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> baseline_store</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics_calculator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> metrics_calculator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_model_drift</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, analysis_window_hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">) -> List[DriftResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze all features for drift compared to training baseline.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns list of DriftResult objects, one per feature analyzed.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Retrieve recent predictions from prediction_logger</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Extract feature values from predictions, organized by feature name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Load baseline feature distributions from baseline_store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each feature, determine if numerical or categorical</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Call appropriate drift detection method from metrics_calculator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Collect all drift results and return as list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Group predictions by feature name for efficient batch processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_prediction_drift</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              comparison_window_hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              baseline_window_hours: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 168</span><span style=\"color:#E1E4E8\">) -> DriftResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect concept drift by comparing prediction distributions over time.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get predictions from recent comparison window</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get predictions from baseline window (e.g., last week)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Extract prediction values from both time periods</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Use KS test to compare prediction distributions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compute drift severity based on test statistic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return DriftResult for prediction distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Consider confidence scores as well as raw predictions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compute_realtime_metrics</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, window_minutes: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 15</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute real-time performance metrics for monitoring dashboard.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Get predictions from the last window_minutes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute latency percentiles using metrics_calculator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate throughput (requests per second)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Compute error rate (failed predictions / total predictions)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate prediction confidence statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return all metrics as dictionary for dashboard consumption</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Handle case where no predictions exist in time window</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AlertManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manage drift alerts and escalation policies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, notification_channels: List[NotificationChannel]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.channels </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> notification_channels</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active_alerts: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Alert] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> evaluate_drift_alerts</span><span style=\"color:#E1E4E8\">(self, drift_results: List[DriftResult], model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[Alert]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate drift results against alert thresholds and create alerts.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through each drift result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check if drift severity exceeds alert thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create Alert objects for threshold violations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Correlate related alerts (avoid duplicate notifications)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Check against active_alerts to avoid repeat notifications</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return list of new alerts to send</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Different severity levels should route to different teams</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> send_alert</span><span style=\"color:#E1E4E8\">(self, alert: Alert) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Send alert through configured notification channels.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Determine appropriate notification channels based on alert severity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Format alert message for each channel (email vs Slack formatting)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Send notification through each selected channel</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record alert in active_alerts for tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle notification failures gracefully</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return True if at least one notification succeeded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Implement retry logic for failed notifications</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing the model monitoring component, verify correct operation with these checks:</p>\n<p><strong>1. Prediction Logging Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Start monitoring service</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> monitoring.api.monitoring_service</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Send test prediction and verify logging</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/monitor/log_prediction</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"model_name\": \"test_model\",</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"model_version\": \"v1.0\",</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"input_features\": {\"feature1\": 1.5, \"feature2\": \"category_a\"},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"prediction\": 0.85,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"latency_ms\": 12.5</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">  }'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Query recent predictions</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/monitor/predictions?model=test_model&#x26;hours=1\"</span></span></code></pre></div>\n\n<p><strong>Expected output:</strong> JSON response containing the logged prediction with all metadata fields populated.</p>\n<p><strong>2. Drift Detection Testing:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Generate synthetic drift data for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> monitoring.core.drift_detector </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> DriftDetectionEngine</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Create baseline data (normal distribution)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">baseline_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">).tolist()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Create drifted data (shifted distribution) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">drifted_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> np.random.normal(</span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">).tolist()  </span><span style=\"color:#6A737D\"># Mean shift</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test drift detection</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">engine </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DriftDetectionEngine(logger, baseline_store, calculator)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> engine.metrics_calculator.detect_numerical_drift(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    drifted_data, baseline_data, </span><span style=\"color:#9ECBFF\">\"test_feature\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Drift detected: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.severity</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"KS statistic: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.test_statistic</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"P-value: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.p_value</span><span style=\"color:#F97583\">:.4f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Expected behavior:</strong> Drift severity should be &quot;MEDIUM&quot; or &quot;HIGH&quot; for the shifted distribution, with p-value &lt; 0.05 indicating statistical significance.</p>\n<p><strong>3. Alert System Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Configure test alert thresholds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">export</span><span style=\"color:#E1E4E8\"> DRIFT_ALERT_THRESHOLD</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.2</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">export</span><span style=\"color:#E1E4E8\"> SLACK_WEBHOOK_URL</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"https://hooks.slack.com/test\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Trigger alert with high drift</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#9ECBFF\"> scripts/test_alert_system.py</span><span style=\"color:#79B8FF\"> --drift-score=0.8</span><span style=\"color:#79B8FF\"> --model=test_model</span></span></code></pre></div>\n\n<p><strong>Expected behavior:</strong> Alert notification sent to configured channels (Slack message, email) with drift details and recommended actions.</p>\n<p><strong>Signs of correct implementation:</strong></p>\n<ul>\n<li>Prediction logs captured with sub-millisecond timestamp precision</li>\n<li>Latency percentiles computed correctly across time windows  </li>\n<li>Drift detection produces consistent results for identical input data</li>\n<li>Alert thresholds properly calibrated to avoid false positives</li>\n<li>Monitoring system health checks pass consistently</li>\n</ul>\n<p><strong>Signs of problems:</strong></p>\n<ul>\n<li>Memory usage grows unbounded (indicates logging without retention policies)</li>\n<li>Drift scores vary significantly between runs on same data (suggests statistical bugs)</li>\n<li>Alerts fire repeatedly for same issue (missing alert correlation)</li>\n<li>Query performance degrades over time (needs storage optimization)</li>\n</ul>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fmonitoring-architecture.svg\" alt=\"Model Monitoring Data Flow\"></p>\n<h2 id=\"component-interactions-and-data-flow\">Component Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section integrates all milestones (1-5) by describing how experiment tracking, model registry, training pipeline orchestration, model deployment, and model monitoring communicate through APIs and events to create a cohesive MLOps workflow from experiment to production monitoring.</p>\n</blockquote>\n<p>The MLOps platform components work together as a distributed system where each component maintains its own data and business logic while communicating with others through well-defined APIs and asynchronous events. Understanding these interactions is crucial because the platform&#39;s value comes not from individual components but from their seamless integration throughout the machine learning lifecycle.</p>\n<h3 id=\"mental-model-orchestra-coordination\">Mental Model: Orchestra Coordination</h3>\n<p>Think of the MLOps platform like a symphony orchestra where each component is a different instrument section. Just as musicians follow sheet music and respond to the conductor&#39;s signals to create harmonious music, our components follow API contracts and respond to events to orchestrate ML workflows. The conductor (event coordinator) doesn&#39;t control every note each musician plays, but ensures they start, stop, and transition together at the right moments. Each section (component) has specialized skills—strings handle melody, percussion provides rhythm—just as our components have specialized responsibilities for tracking, versioning, orchestration, deployment, and monitoring.</p>\n<p>The sheet music represents our API specifications and data contracts, while the conductor&#39;s gestures represent the events that trigger coordinated actions across components. When the violins finish their solo (experiment completes), they signal the conductor, who then cues the brass section (model registry) to begin their part (model registration). This coordination happens without each section needing to know the internal details of how other sections operate—they just need to respond to the agreed-upon signals and timing.</p>\n<h3 id=\"inter-component-apis\">Inter-Component APIs</h3>\n<p>The MLOps platform uses REST APIs as the primary communication mechanism between components, with each component exposing specific endpoints for cross-component operations while maintaining internal APIs for client interactions. This approach provides clear service boundaries, enables independent scaling, and supports polyglot implementation where different components can use different programming languages optimized for their specific requirements.</p>\n<h4 id=\"core-api-design-principles\">Core API Design Principles</h4>\n<p>Each component exposes two types of APIs: <strong>external APIs</strong> for client interactions (data scientists, ML engineers) and <strong>internal APIs</strong> for inter-component communication. The internal APIs focus on data exchange and lifecycle notifications, while external APIs provide rich query capabilities and user-friendly interfaces. All APIs follow REST principles with resource-based URLs, HTTP status codes for error handling, and JSON payloads with consistent field naming conventions.</p>\n<p>The API design emphasizes <strong>idempotency</strong> for all mutation operations, meaning that repeating the same request multiple times produces the same result. This is crucial in distributed systems where network failures can cause request retries. For example, registering a model version with the same name, version, and checksum always returns the same model version ID, whether it&#39;s a new registration or a retry of a previous request.</p>\n<p><strong>Authentication and authorization</strong> flow through all APIs using JWT tokens that contain user identity and permissions. Each component validates tokens independently and makes authorization decisions based on resource ownership and role-based access control. This distributed authorization model eliminates the need for a central authorization service while ensuring consistent security policies.</p>\n<h4 id=\"experiment-tracking-apis\">Experiment Tracking APIs</h4>\n<p>The Experiment Tracking component exposes APIs for logging training data and querying experiment results, with specific endpoints designed for integration with other components that need to access experiment metadata and artifacts.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Endpoint</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>POST /api/v1/experiments</code></td>\n<td>name, description, tags</td>\n<td>Experiment</td>\n<td>Create new experiment</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/runs</code></td>\n<td>experiment_id, run_name, tags</td>\n<td>Run</td>\n<td>Start new training run</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/runs/{run_id}/params</code></td>\n<td>key, value, value_type</td>\n<td>Parameter</td>\n<td>Log parameter for run</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/runs/{run_id}/metrics</code></td>\n<td>key, value, step, timestamp</td>\n<td>MetricPoint</td>\n<td>Log metric point for run</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/runs/{run_id}/artifacts</code></td>\n<td>path, file_data, metadata</td>\n<td>ArtifactInfo</td>\n<td>Upload artifact for run</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/runs/{run_id}</code></td>\n<td>include_params, include_metrics</td>\n<td>Run</td>\n<td>Get run details with optional related data</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/runs/{run_id}/artifacts/{path}</code></td>\n<td>None</td>\n<td>Binary data</td>\n<td>Download artifact by path</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/runs/search</code></td>\n<td>experiment_ids, filter, order_by, limit</td>\n<td>List[Run]</td>\n<td>Search runs with filtering</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/runs/compare</code></td>\n<td>run_ids, metric_keys</td>\n<td>Comparison</td>\n<td>Compare metrics across runs</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The <strong>inter-component integration points</strong> focus on providing model registry and pipeline orchestration access to experiment data without exposing all experiment tracking functionality. The model registry calls the experiment tracking API to retrieve run metadata when registering models, ensuring complete lineage information. Pipeline orchestration queries experiment results to determine the best model versions for automated model selection workflows.</p>\n<h4 id=\"model-registry-apis\">Model Registry APIs</h4>\n<p>The Model Registry component provides APIs for model lifecycle management with emphasis on version control, stage transitions, and lineage tracking that other components need for deployment and monitoring workflows.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Endpoint</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>POST /api/v1/models</code></td>\n<td>name, description, tags</td>\n<td>Model</td>\n<td>Create new model entry</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/models/{name}/versions</code></td>\n<td>version, artifact_uri, run_id, metadata</td>\n<td>ModelVersion</td>\n<td>Register new model version</td>\n<td></td>\n</tr>\n<tr>\n<td><code>PUT /api/v1/models/{name}/versions/{version}/stage</code></td>\n<td>new_stage, approval_metadata</td>\n<td>ModelVersion</td>\n<td>Update version stage</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/models/{name}/versions/{version}</code></td>\n<td>include_lineage</td>\n<td>ModelVersion</td>\n<td>Get version details with optional lineage</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/models/{name}/versions/latest</code></td>\n<td>stage</td>\n<td>ModelVersion</td>\n<td>Get latest version for stage</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/models/{name}/lineage</code></td>\n<td>version, depth</td>\n<td>Lineage graph</td>\n<td>Build lineage graph for version</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/models/search</code></td>\n<td>stage, tags, metrics_filter, limit</td>\n<td>List[ModelVersion]</td>\n<td>Search models by criteria</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/models/{name}/versions/{version}/artifact</code></td>\n<td>path</td>\n<td>Binary data</td>\n<td>Download model artifact</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The <strong>deployment integration</strong> relies heavily on the &quot;get latest version for stage&quot; endpoint, which allows deployment components to automatically deploy the latest production model without hardcoding version numbers. The monitoring component uses the search API to discover all deployed model versions and establish monitoring for each one. This loose coupling means that promoting a model to production automatically triggers deployment workflows without explicit coordination.</p>\n<h4 id=\"pipeline-orchestration-apis\">Pipeline Orchestration APIs</h4>\n<p>The Training Pipeline component exposes APIs for pipeline definition, execution control, and status monitoring, with integration points for automated model training and deployment workflows.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Endpoint</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>POST /api/v1/pipelines</code></td>\n<td>pipeline_definition</td>\n<td>Pipeline</td>\n<td>Create or update pipeline</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/pipelines/{pipeline_id}/runs</code></td>\n<td>parameters, trigger_type</td>\n<td>PipelineRun</td>\n<td>Start pipeline execution</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/pipelines/{pipeline_id}/runs/{run_id}</code></td>\n<td>include_steps</td>\n<td>PipelineRun</td>\n<td>Get run status with step details</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/pipelines/{pipeline_id}/runs/{run_id}/cancel</code></td>\n<td>reason</td>\n<td>Success status</td>\n<td>Cancel running pipeline</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/pipelines/{pipeline_id}/runs/{run_id}/logs</code></td>\n<td>step_id, follow</td>\n<td>Log stream</td>\n<td>Get execution logs for debugging</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/pipelines/{pipeline_id}/runs/{run_id}/artifacts</code></td>\n<td>step_id, path</td>\n<td>List[ArtifactInfo]</td>\n<td>List artifacts produced by step</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/pipelines/trigger</code></td>\n<td>event_type, payload</td>\n<td>List[PipelineRun]</td>\n<td>Trigger pipelines based on events</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The <strong>event-driven integration</strong> through the trigger endpoint allows other components to automatically start training pipelines when specific conditions are met. For example, when new training data becomes available or when model performance degrades below thresholds, monitoring components can trigger retraining pipelines without manual intervention.</p>\n<h4 id=\"model-deployment-apis\">Model Deployment APIs</h4>\n<p>The Model Deployment component provides APIs for deploying models as serving endpoints with traffic management, health monitoring, and rollback capabilities that integrate with model registry and monitoring components.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Endpoint</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>POST /api/v1/deployments</code></td>\n<td>deployment_spec</td>\n<td>Deployment</td>\n<td>Deploy model version to endpoint</td>\n<td></td>\n</tr>\n<tr>\n<td><code>PUT /api/v1/deployments/{deployment_id}/traffic</code></td>\n<td>version_weights</td>\n<td>Success status</td>\n<td>Update traffic split between versions</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/deployments/{deployment_id}/rollback</code></td>\n<td>target_version</td>\n<td>Success status</td>\n<td>Rollback to previous version</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/deployments/{deployment_id}/health</code></td>\n<td>None</td>\n<td>HealthCheck</td>\n<td>Get deployment health status</td>\n<td></td>\n</tr>\n<tr>\n<td><code>PUT /api/v1/deployments/{deployment_id}/scale</code></td>\n<td>target_replicas</td>\n<td>Success status</td>\n<td>Scale deployment replicas</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/deployments/endpoints</code></td>\n<td>model_name, stage</td>\n<td>List[ModelEndpoint]</td>\n<td>List active endpoints for model</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/deployments/predict</code></td>\n<td>model_name, input_data, version</td>\n<td>Prediction</td>\n<td>Make prediction request</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The <strong>monitoring integration</strong> happens through the health and endpoints APIs, which allow monitoring components to discover all deployed models and track their serving status. The prediction API includes metadata that enables request logging for drift detection and performance analysis.</p>\n<h4 id=\"model-monitoring-apis\">Model Monitoring APIs</h4>\n<p>The Model Monitoring component exposes APIs for prediction logging, drift analysis, and alert management that close the loop by providing feedback to trigger retraining or rollback decisions.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Endpoint</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>POST /api/v1/monitoring/predictions</code></td>\n<td>log_entry</td>\n<td>Success status</td>\n<td>Log prediction for analysis</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/monitoring/predictions/batch</code></td>\n<td>entries</td>\n<td>Batch status</td>\n<td>Log multiple predictions efficiently</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/monitoring/models/{name}/metrics</code></td>\n<td>time_range, aggregation</td>\n<td>Metrics summary</td>\n<td>Get model performance metrics</td>\n<td></td>\n</tr>\n<tr>\n<td><code>POST /api/v1/monitoring/models/{name}/drift/analyze</code></td>\n<td>analysis_config</td>\n<td>List[DriftResult]</td>\n<td>Run drift analysis on demand</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/monitoring/models/{name}/alerts</code></td>\n<td>severity, status</td>\n<td>List[Alert]</td>\n<td>Get active alerts for model</td>\n<td></td>\n</tr>\n<tr>\n<td><code>PUT /api/v1/monitoring/models/{name}/thresholds</code></td>\n<td>metric_thresholds</td>\n<td>Success status</td>\n<td>Update alert thresholds</td>\n<td></td>\n</tr>\n<tr>\n<td><code>GET /api/v1/monitoring/models/{name}/baseline</code></td>\n<td>feature_names</td>\n<td>Baseline data</td>\n<td>Get baseline distributions for comparison</td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The <strong>feedback loop integration</strong> enables monitoring to trigger actions in other components when issues are detected. High drift scores can automatically trigger retraining pipelines, while severe performance degradation can trigger deployment rollbacks through the respective component APIs.</p>\n<h3 id=\"end-to-end-workflows\">End-to-End Workflows</h3>\n<p>Understanding how components coordinate during complete ML workflows reveals the platform&#39;s true capabilities. These workflows demonstrate the API interactions and event flows that transform isolated training experiments into production-ready ML systems with continuous monitoring and improvement.</p>\n<h4 id=\"experiment-to-deployment-workflow\">Experiment to Deployment Workflow</h4>\n<p>This workflow represents the core path from initial model development to production deployment, showcasing how experiment tracking, model registry, and deployment components work together to maintain lineage and enable reproducible deployments.</p>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Fexperiment-flow.svg\" alt=\"Experiment to Deployment Flow\"></p>\n<p>The workflow begins when a data scientist starts a training experiment and concludes with an automatically deployed and monitored production model. This end-to-end process typically takes several API calls across multiple components, but the platform coordinates these interactions to make the experience seamless for users while maintaining complete audit trails.</p>\n<p><strong>Phase 1: Experiment Execution and Tracking</strong></p>\n<p>The workflow starts when a data scientist initiates a training run through the experiment tracking component. The training code makes API calls to log parameters, metrics, and artifacts throughout the training process, building a complete record of the experiment.</p>\n<ol>\n<li><strong>Experiment Creation</strong>: Data scientist calls <code>POST /api/v1/experiments</code> with experiment name &quot;credit-fraud-detection-v2&quot; and description &quot;Testing new feature engineering approach with SMOTE balancing&quot;</li>\n<li><strong>Run Initialization</strong>: Training script calls <code>POST /api/v1/runs</code> with experiment ID and run name &quot;gradient-boosting-run-1&quot;, receiving run ID &quot;run_abc123&quot;</li>\n<li><strong>Parameter Logging</strong>: Script logs hyperparameters via <code>POST /api/v1/runs/run_abc123/params</code> including learning_rate=0.1, max_depth=6, n_estimators=100</li>\n<li><strong>Metric Tracking</strong>: During training, script logs metrics via <code>POST /api/v1/runs/run_abc123/metrics</code> including accuracy, precision, recall at each epoch</li>\n<li><strong>Artifact Upload</strong>: After training completes, script uploads model file via <code>POST /api/v1/runs/run_abc123/artifacts</code> with path &quot;model/fraud_detector.pkl&quot; and metadata including framework=&quot;scikit-learn&quot;</li>\n<li><strong>Run Completion</strong>: Script calls <code>PUT /api/v1/runs/run_abc123/status</code> to mark run as FINISHED with final metrics summary</li>\n</ol>\n<p>This phase creates a complete experiment record that includes all information needed for model reproduction and regulatory compliance. The artifact upload generates a content hash that ensures model integrity throughout the lifecycle.</p>\n<p><strong>Phase 2: Model Registration and Promotion</strong></p>\n<p>Once the experiment produces a satisfactory model, the data scientist registers it in the model registry, beginning the formal model lifecycle management process.</p>\n<ol>\n<li><strong>Model Registration</strong>: Data scientist calls <code>POST /api/v1/models/credit-fraud-detector/versions</code> with version=&quot;1.2.0&quot;, run_id=&quot;run_abc123&quot;, artifact_uri pointing to uploaded model</li>\n<li><strong>Lineage Establishment</strong>: Model registry calls <code>GET /api/v1/runs/run_abc123</code> to retrieve complete experiment metadata and establish lineage linkage</li>\n<li><strong>Validation</strong>: Model registry validates model artifact integrity by verifying checksum matches upload record from experiment tracking</li>\n<li><strong>Development Stage</strong>: New model version starts in Development stage, allowing testing and validation before production promotion</li>\n<li><strong>Stage Promotion</strong>: After validation, ML engineer calls <code>PUT /api/v1/models/credit-fraud-detector/versions/1.2.0/stage</code> with new_stage=&quot;Production&quot; and approval metadata</li>\n<li><strong>Approval Workflow</strong>: Model registry executes approval workflow, potentially requiring additional approvals based on organization policies</li>\n</ol>\n<p>This phase transforms an experiment artifact into a managed model version with formal lifecycle controls. The lineage linking ensures that production models can always be traced back to their training experiments for debugging and compliance.</p>\n<p><strong>Phase 3: Automated Deployment</strong></p>\n<p>Model promotion to Production stage triggers automated deployment workflows that create serving endpoints without manual intervention, ensuring consistent deployment practices.</p>\n<ol>\n<li><strong>Promotion Event</strong>: Model registry publishes <code>MODEL_PROMOTED</code> event with payload including model_name=&quot;credit-fraud-detector&quot;, version=&quot;1.2.0&quot;, stage=&quot;Production&quot;</li>\n<li><strong>Deployment Trigger</strong>: Deployment component receives event and calls <code>GET /api/v1/models/credit-fraud-detector/versions/1.2.0</code> to retrieve model metadata</li>\n<li><strong>Artifact Download</strong>: Deployment component calls <code>GET /api/v1/models/credit-fraud-detector/versions/1.2.0/artifact</code> to download model file for deployment</li>\n<li><strong>Deployment Creation</strong>: Deployment component creates deployment spec with auto-scaling configuration and calls <code>POST /api/v1/deployments</code> to start deployment</li>\n<li><strong>Health Verification</strong>: Deployment component monitors deployment health via <code>GET /api/v1/deployments/{deployment_id}/health</code> until all replicas are healthy</li>\n<li><strong>Traffic Routing</strong>: Once healthy, deployment component calls <code>PUT /api/v1/deployments/{deployment_id}/traffic</code> to route production traffic to new version</li>\n</ol>\n<p>This phase demonstrates how event-driven architecture enables automated deployment while maintaining safety through health checks and gradual traffic shifting.</p>\n<p><strong>Phase 4: Monitoring Setup and Feedback</strong></p>\n<p>The final phase establishes production monitoring for the newly deployed model and sets up feedback loops that can trigger retraining or rollback when issues are detected.</p>\n<ol>\n<li><strong>Endpoint Discovery</strong>: Monitoring component calls <code>GET /api/v1/deployments/endpoints</code> with model_name=&quot;credit-fraud-detector&quot; to discover new production endpoint</li>\n<li><strong>Baseline Establishment</strong>: Monitoring component retrieves training data distributions from experiment artifacts to establish drift detection baseline</li>\n<li><strong>Prediction Logging</strong>: Production inference requests automatically log prediction data via <code>POST /api/v1/monitoring/predictions</code> including input features and model outputs</li>\n<li><strong>Drift Analysis</strong>: Monitoring component periodically calls <code>POST /api/v1/monitoring/models/credit-fraud-detector/drift/analyze</code> to detect data or concept drift</li>\n<li><strong>Performance Tracking</strong>: Monitoring component tracks model accuracy, latency, and throughput via <code>GET /api/v1/monitoring/models/credit-fraud-detector/metrics</code></li>\n<li><strong>Alert Configuration</strong>: ML operations team configures alert thresholds via <code>PUT /api/v1/monitoring/models/credit-fraud-detector/thresholds</code> for automated issue detection</li>\n</ol>\n<p>This phase closes the loop by establishing continuous monitoring that provides feedback about model performance and can trigger new experiment cycles when retraining becomes necessary.</p>\n<h4 id=\"automated-retraining-workflow\">Automated Retraining Workflow</h4>\n<p>This workflow demonstrates how the platform supports continuous learning by automatically detecting when model performance degrades and triggering retraining pipelines to maintain model quality without manual intervention.</p>\n<p><strong>Trigger Detection and Pipeline Initialization</strong></p>\n<p>The retraining workflow begins when monitoring components detect performance degradation or data drift beyond acceptable thresholds. This automated detection prevents model quality degradation from impacting business operations.</p>\n<ol>\n<li><strong>Drift Detection</strong>: Monitoring component analyzes recent predictions and detects significant data drift with PSI score 0.35 (above threshold 0.25)</li>\n<li><strong>Alert Generation</strong>: Monitoring component creates alert with severity=HIGH and calls alert manager to evaluate escalation policies</li>\n<li><strong>Retraining Decision</strong>: Alert manager determines that drift severity requires automated retraining based on configured policies</li>\n<li><strong>Pipeline Trigger</strong>: Alert manager calls <code>POST /api/v1/pipelines/trigger</code> with event_type=&quot;model_drift_detected&quot; and payload containing model_name and drift_metrics</li>\n<li><strong>Pipeline Selection</strong>: Pipeline orchestration component matches event to registered retraining pipeline &quot;credit-fraud-detector-retrain-v1&quot;</li>\n<li><strong>Pipeline Execution</strong>: Pipeline component calls <code>POST /api/v1/pipelines/credit-fraud-detector-retrain-v1/runs</code> with parameters including drift_context and target_model_version</li>\n</ol>\n<p>This trigger mechanism demonstrates how monitoring provides actionable feedback that drives automated improvement without requiring manual intervention to detect and respond to model quality issues.</p>\n<p><strong>Data Pipeline and Model Training</strong></p>\n<p>The retraining pipeline executes a series of coordinated steps that fetch fresh training data, preprocess features, train new models, and evaluate their performance against current production baselines.</p>\n<ol>\n<li><strong>Data Collection</strong>: Pipeline step calls external data sources to fetch training data updated since last training run, including new fraud patterns</li>\n<li><strong>Feature Engineering</strong>: Pipeline step applies same feature transformations used in original training, ensuring consistency with production serving</li>\n<li><strong>Data Validation</strong>: Pipeline step compares new training data distribution against baseline to detect training data quality issues</li>\n<li><strong>Model Training</strong>: Pipeline step executes training with hyperparameters from best previous run plus automated hyperparameter optimization</li>\n<li><strong>Evaluation</strong>: Pipeline step evaluates new model against held-out test set and compares performance metrics to current production model</li>\n<li><strong>Experiment Logging</strong>: Each pipeline step logs parameters, metrics, and artifacts via experiment tracking APIs to maintain complete lineage</li>\n</ol>\n<p>This phase ensures that retraining maintains the same quality standards as manual model development while incorporating the latest available data and potentially improved hyperparameters.</p>\n<p><strong>Model Selection and Deployment</strong></p>\n<p>The pipeline concludes by automatically selecting the best model variant and deploying it to production if it meets quality criteria, or alerting human operators if manual review is required.</p>\n<ol>\n<li><strong>Performance Comparison</strong>: Pipeline step retrieves current production model metrics and compares against newly trained models</li>\n<li><strong>Model Registration</strong>: If new model shows improvement, pipeline step calls model registry to register new version with lineage pointing to retraining run</li>\n<li><strong>Automated Testing</strong>: Pipeline step deploys new model to staging environment and runs automated test suite against known fraud patterns</li>\n<li><strong>Canary Deployment</strong>: If tests pass, pipeline step calls deployment component to start canary deployment routing 5% traffic to new model version</li>\n<li><strong>Performance Monitoring</strong>: Pipeline step monitors canary performance for specified duration, comparing key metrics against main production version</li>\n<li><strong>Full Rollout</strong>: If canary shows improved performance with statistical significance, pipeline step completes rollout by shifting 100% traffic to new version</li>\n</ol>\n<p>This automated deployment phase maintains safety through staged rollouts while enabling continuous model improvement without manual oversight for routine quality improvements.</p>\n<h4 id=\"model-rollback-workflow\">Model Rollback Workflow</h4>\n<p>This critical workflow demonstrates how the platform handles production incidents by rapidly reverting to previous model versions when performance degradation or system issues are detected.</p>\n<p><strong>Incident Detection and Response Initiation</strong></p>\n<p>Model rollback workflows typically begin with automated detection of severe performance degradation or system alerts indicating serving failures that require immediate remediation.</p>\n<ol>\n<li><strong>Performance Degradation</strong>: Monitoring component detects accuracy drop from 94% to 78% over 30-minute window, exceeding emergency threshold</li>\n<li><strong>Alert Escalation</strong>: Monitoring component generates CRITICAL severity alert and immediately notifies on-call engineering team</li>\n<li><strong>Incident Response</strong>: On-call engineer receives alert and reviews monitoring dashboard showing performance timeline and potential causes</li>\n<li><strong>Rollback Decision</strong>: Engineer determines that issue requires immediate rollback to previous stable model version to restore service quality</li>\n<li><strong>Version Identification</strong>: Engineer calls <code>GET /api/v1/models/credit-fraud-detector/versions</code> to identify last known good version in production</li>\n<li><strong>Rollback Initiation</strong>: Engineer calls <code>POST /api/v1/deployments/{deployment_id}/rollback</code> with target_version=&quot;1.1.0&quot; to begin rollback process</li>\n</ol>\n<p>This rapid response process minimizes the time between incident detection and remediation, reducing business impact from model quality issues.</p>\n<p><strong>Traffic Shifting and Validation</strong></p>\n<p>The rollback process carefully manages traffic shifting to ensure service continuity while validating that the rollback resolves the detected issues.</p>\n<ol>\n<li><strong>Traffic Diversion</strong>: Deployment component immediately shifts 100% traffic from problematic version 1.2.0 to stable version 1.1.0</li>\n<li><strong>Health Verification</strong>: Deployment component monitors rolled-back deployment health via continuous health checks ensuring all replicas serve correctly</li>\n<li><strong>Performance Validation</strong>: Monitoring component tracks post-rollback metrics to verify that accuracy returns to expected baseline levels</li>\n<li><strong>Service Continuity</strong>: Deployment component ensures zero-downtime rollback by maintaining sufficient capacity in previous version before traffic shift</li>\n<li><strong>Confirmation Monitoring</strong>: Engineering team monitors service for 30 minutes post-rollback to confirm that issue resolution is stable</li>\n<li><strong>Incident Documentation</strong>: System automatically logs rollback event with timestamps, reasons, and performance impact metrics for post-incident analysis</li>\n</ol>\n<p>This controlled rollback process prioritizes service restoration while collecting information needed for root cause analysis and process improvement.</p>\n<h3 id=\"event-driven-coordination\">Event-Driven Coordination</h3>\n<p>The MLOps platform uses asynchronous events to coordinate complex workflows across components without tight coupling, enabling scalable and resilient operations. Events decouple components by allowing them to react to state changes without direct API dependencies, supporting scenarios where multiple components need to respond to single actions or where response timing varies significantly.</p>\n<h4 id=\"event-architecture-and-patterns\">Event Architecture and Patterns</h4>\n<p><strong>Event-Driven Coordination Mental Model: Newspaper Publishing</strong></p>\n<p>Think of the event system like a newspaper publishing operation. Each component is like a different department—newsroom (experiment tracking), editorial (model registry), printing press (pipeline orchestration), distribution (deployment), and circulation analytics (monitoring). When a major story breaks (experiment completes), the newsroom doesn&#39;t call each department individually. Instead, they publish the story to the central editorial system (event coordinator), which then distributes it to all departments that need to know. The editorial department decides which stories get promoted to front page (production stage), the printing press schedules special editions (deployment pipelines), and circulation tracks reader response (monitoring metrics). Each department operates independently but stays coordinated through shared information flow.</p>\n<p>The event coordinator serves as the central newsroom editorial desk, ensuring that important information reaches all relevant parties without requiring each department to know about all others. When the sports department (training pipeline) finishes a major feature story (model training), they publish it once, and the editorial desk handles distributing it to layout (model registry), printing (deployment), and marketing (monitoring) automatically.</p>\n<p>The platform implements <strong>event sourcing</strong> patterns where important state changes are captured as immutable events that can be replayed to reconstruct system state or debug workflow issues. Each event includes correlation IDs that link related activities across components, enabling end-to-end tracing of complex workflows.</p>\n<p><strong>Event Schema and Metadata</strong></p>\n<p>All platform events follow a consistent schema that includes sufficient metadata for routing, filtering, and audit trail reconstruction. The standardized event structure enables generic event processing infrastructure while supporting component-specific payload formats.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>id</code></td>\n<td>str</td>\n<td>Unique identifier for event deduplication and tracking</td>\n</tr>\n<tr>\n<td><code>type</code></td>\n<td>str</td>\n<td>Hierarchical event type for filtering (e.g., &quot;model.promoted&quot;, &quot;pipeline.completed&quot;)</td>\n</tr>\n<tr>\n<td><code>source</code></td>\n<td>str</td>\n<td>Component that generated event for audit and debugging</td>\n</tr>\n<tr>\n<td><code>timestamp</code></td>\n<td>float</td>\n<td>Unix timestamp when event occurred for ordering and correlation</td>\n</tr>\n<tr>\n<td><code>payload</code></td>\n<td>Dict[str, Any]</td>\n<td>Event-specific data containing relevant state information</td>\n</tr>\n<tr>\n<td><code>correlation_id</code></td>\n<td>str</td>\n<td>Links related events across workflow execution</td>\n</tr>\n<tr>\n<td><code>version</code></td>\n<td>str</td>\n<td>Event schema version for backward compatibility</td>\n</tr>\n<tr>\n<td><code>metadata</code></td>\n<td>Dict[str, str]</td>\n<td>Additional context like user_id, tenant_id for multi-tenant deployments</td>\n</tr>\n</tbody></table>\n<p>The event payload structure varies by event type but follows consistent naming conventions and includes sufficient information for downstream components to take appropriate actions without additional API calls.</p>\n<h4 id=\"core-platform-events\">Core Platform Events</h4>\n<p><strong>Model Lifecycle Events</strong></p>\n<p>Model registry events coordinate promotion workflows and trigger deployment automations when model stages change or new versions become available.</p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Payload Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>MODEL_PROMOTED</code></td>\n<td>model_name, version, old_stage, new_stage, approval_metadata</td>\n<td>Model version promoted to new stage</td>\n</tr>\n<tr>\n<td><code>model.registered</code></td>\n<td>model_name, version, artifact_uri, run_id, metadata</td>\n<td>New model version registered</td>\n</tr>\n<tr>\n<td><code>model.deprecated</code></td>\n<td>model_name, version, reason, replacement_version</td>\n<td>Model version marked as deprecated</td>\n</tr>\n<tr>\n<td><code>model.deleted</code></td>\n<td>model_name, version, deletion_reason</td>\n<td>Model version removed from registry</td>\n</tr>\n</tbody></table>\n<p>The <code>MODEL_PROMOTED</code> event is particularly important as it triggers automated deployment workflows when models reach production readiness. The payload includes approval metadata that deployment components can use to apply appropriate deployment strategies based on risk assessment.</p>\n<p><strong>Pipeline Execution Events</strong></p>\n<p>Pipeline orchestration events coordinate training workflows and provide status updates that other components use for scheduling and resource management decisions.</p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Payload Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>PIPELINE_COMPLETED</code></td>\n<td>pipeline_id, run_id, status, artifacts, metrics, duration</td>\n<td>Pipeline execution finished</td>\n</tr>\n<tr>\n<td><code>STEP_FAILED</code></td>\n<td>pipeline_id, run_id, step_id, error_message, retry_count</td>\n<td>Individual step failed with error details</td>\n</tr>\n<tr>\n<td><code>pipeline.started</code></td>\n<td>pipeline_id, run_id, trigger_type, parameters</td>\n<td>Pipeline execution initiated</td>\n</tr>\n<tr>\n<td><code>RESOURCE_EXHAUSTED</code></td>\n<td>pipeline_id, run_id, step_id, resource_type, requested, available</td>\n<td>Insufficient resources for step</td>\n</tr>\n</tbody></table>\n<p>Pipeline completion events often trigger model evaluation and registration workflows, while failure events may trigger automated retry policies or alert escalation depending on the failure type and pipeline criticality.</p>\n<p><strong>Deployment Lifecycle Events</strong></p>\n<p>Deployment events coordinate model serving operations and provide status updates that monitoring components use to establish baseline measurements and alert configurations.</p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Payload Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>deployment.started</code></td>\n<td>deployment_id, model_name, version, endpoint_url, strategy</td>\n<td>Model deployment initiated</td>\n</tr>\n<tr>\n<td><code>deployment.healthy</code></td>\n<td>deployment_id, model_name, version, replica_count, health_metrics</td>\n<td>Deployment reached healthy state</td>\n</tr>\n<tr>\n<td><code>DEPLOYMENT_FAILED</code></td>\n<td>deployment_id, model_name, version, error_message, rollback_version</td>\n<td>Deployment failed with rollback info</td>\n</tr>\n<tr>\n<td><code>traffic.shifted</code></td>\n<td>deployment_id, model_name, old_weights, new_weights, reason</td>\n<td>Traffic routing updated</td>\n</tr>\n</tbody></table>\n<p>Deployment health events trigger monitoring setup, while failure events may trigger automatic rollback workflows or escalation to on-call teams depending on the deployment strategy and business criticality.</p>\n<p><strong>Monitoring and Alert Events</strong></p>\n<p>Monitoring events provide feedback that drives retraining decisions and alert escalation, closing the loop between production performance and model improvement workflows.</p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Payload Fields</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>drift.detected</code></td>\n<td>model_name, drift_type, severity, features, metrics, threshold</td>\n<td>Data or concept drift detected</td>\n</tr>\n<tr>\n<td><code>performance.degraded</code></td>\n<td>model_name, metric_name, current_value, baseline_value, severity</td>\n<td>Model performance below threshold</td>\n</tr>\n<tr>\n<td><code>alert.triggered</code></td>\n<td>alert_id, model_name, severity, message, escalation_level</td>\n<td>Alert condition met</td>\n</tr>\n<tr>\n<td><code>baseline.updated</code></td>\n<td>model_name, feature_names, distribution_metrics, version</td>\n<td>Baseline distributions updated</td>\n</tr>\n</tbody></table>\n<p>Drift detection events often trigger automated retraining pipelines, while performance degradation events may trigger deployment rollbacks or manual investigation workflows depending on the severity and configured response policies.</p>\n<h4 id=\"event-coordination-workflows\">Event Coordination Workflows</h4>\n<p><strong>Experiment-to-Production Coordination</strong></p>\n<p>This event sequence demonstrates how asynchronous events coordinate the complete workflow from experiment completion to production monitoring without requiring components to directly orchestrate each other.</p>\n<ol>\n<li><strong>Experiment Completion</strong>: Experiment tracking publishes <code>experiment.completed</code> event with run_id, metrics, and artifact locations</li>\n<li><strong>Model Evaluation</strong>: Model registry subscribes to experiment completion and evaluates whether results meet promotion criteria</li>\n<li><strong>Automatic Registration</strong>: If criteria are met, model registry automatically registers new version and publishes <code>model.registered</code> event</li>\n<li><strong>Deployment Trigger</strong>: Deployment component receives <code>model.registered</code> event and checks if model should be deployed based on stage and policies</li>\n<li><strong>Monitoring Setup</strong>: Monitoring component receives <code>deployment.healthy</code> event and automatically establishes baseline monitoring for new model</li>\n<li><strong>Feedback Loop</strong>: Monitoring publishes <code>drift.detected</code> or <code>performance.degraded</code> events that can trigger new experiment cycles</li>\n</ol>\n<p>This event-driven coordination means that data scientists only need to run experiments and mark them as successful—the platform handles model registration, deployment, and monitoring setup automatically based on configured policies.</p>\n<p><strong>Incident Response Coordination</strong></p>\n<p>Event-driven incident response demonstrates how the platform can automatically respond to production issues while maintaining audit trails and escalation policies.</p>\n<ol>\n<li><strong>Performance Detection</strong>: Monitoring component detects accuracy degradation and publishes <code>performance.degraded</code> event with severity=CRITICAL</li>\n<li><strong>Automated Response</strong>: Deployment component receives performance degradation event and evaluates rollback policies for the affected model</li>\n<li><strong>Rollback Execution</strong>: If policies indicate automatic rollback, deployment component executes rollback and publishes <code>deployment.rollback</code> event</li>\n<li><strong>Alert Escalation</strong>: Alert manager receives rollback event and escalates to on-call team with context about automatic actions taken</li>\n<li><strong>Retraining Trigger</strong>: Pipeline orchestration receives performance degradation event and schedules emergency retraining pipeline</li>\n<li><strong>Resolution Tracking</strong>: All components log their responses to the original correlation_id, enabling complete incident timeline reconstruction</li>\n</ol>\n<p>This automated incident response reduces mean time to recovery while ensuring that human operators receive complete context about automatic actions taken during the incident.</p>\n<blockquote>\n<p><strong>Decision: Event-Driven Architecture vs Direct API Orchestration</strong></p>\n<ul>\n<li><strong>Context</strong>: Components need to coordinate complex workflows involving multiple stages and potential retry/rollback scenarios</li>\n<li><strong>Options Considered</strong>: Direct API calls between components, centralized workflow orchestrator, event-driven coordination</li>\n<li><strong>Decision</strong>: Event-driven coordination with centralized event coordinator</li>\n<li><strong>Rationale</strong>: Events provide loose coupling that enables independent component scaling and development, support complex workflow patterns like fan-out/fan-in, and create natural audit trails for compliance and debugging</li>\n<li><strong>Consequences</strong>: Adds complexity in event ordering and delivery guarantees, but enables more resilient and scalable coordination patterns</li>\n</ul>\n</blockquote>\n<h4 id=\"event-delivery-and-reliability\">Event Delivery and Reliability</h4>\n<p><strong>Event Coordinator Implementation</strong></p>\n<p>The EventCoordinator provides reliable event delivery with ordering guarantees and failure handling that ensures workflow coordination remains consistent even during component failures or network partitions.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>publish</code></td>\n<td>event, synchronous</td>\n<td>bool</td>\n<td>Publish event to registered subscribers</td>\n</tr>\n<tr>\n<td><code>subscribe</code></td>\n<td>event_type, handler</td>\n<td>subscription_id</td>\n<td>Register event handler function</td>\n</tr>\n<tr>\n<td><code>unsubscribe</code></td>\n<td>subscription_id</td>\n<td>bool</td>\n<td>Remove event subscription</td>\n</tr>\n<tr>\n<td><code>replay_events</code></td>\n<td>start_time, end_time, event_types</td>\n<td>List[Event]</td>\n<td>Replay events for debugging or recovery</td>\n</tr>\n<tr>\n<td><code>get_event_history</code></td>\n<td>correlation_id</td>\n<td>List[Event]</td>\n<td>Get all events for workflow tracing</td>\n</tr>\n</tbody></table>\n<p>The event coordinator implements <strong>at-least-once delivery</strong> guarantees by persisting events to durable storage before notifying subscribers. Subscribers must implement idempotent event handling to manage potential duplicate deliveries during failure scenarios.</p>\n<p><strong>Event Ordering and Causality</strong></p>\n<p>The platform maintains <strong>causal ordering</strong> for events that affect the same resources (model, pipeline, deployment) while allowing concurrent processing of independent events. This ensures that model promotion events are processed before deployment events for the same model version, preventing race conditions that could deploy outdated versions.</p>\n<table>\n<thead>\n<tr>\n<th>Ordering Guarantee</th>\n<th>Scope</th>\n<th>Implementation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Total order</td>\n<td>Events affecting same model version</td>\n<td>Sequential processing per model partition</td>\n</tr>\n<tr>\n<td>Causal order</td>\n<td>Events in same workflow correlation</td>\n<td>Vector clock timestamps</td>\n</tr>\n<tr>\n<td>No ordering</td>\n<td>Independent model workflows</td>\n<td>Parallel processing across partitions</td>\n</tr>\n</tbody></table>\n<p><strong>Event Storage and Replay</strong></p>\n<p>All events are persisted to an append-only event log that enables replay for debugging, audit compliance, and disaster recovery scenarios. The event log includes event metadata and payload data with retention policies based on regulatory requirements and operational needs.</p>\n<table>\n<thead>\n<tr>\n<th>Retention Policy</th>\n<th>Duration</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Real-time processing</td>\n<td>7 days</td>\n<td>Active workflow coordination</td>\n</tr>\n<tr>\n<td>Audit compliance</td>\n<td>7 years</td>\n<td>Regulatory audit trails</td>\n</tr>\n<tr>\n<td>Debugging replay</td>\n<td>90 days</td>\n<td>Incident investigation and troubleshooting</td>\n</tr>\n<tr>\n<td>Performance analysis</td>\n<td>1 year</td>\n<td>Workflow optimization and capacity planning</td>\n</tr>\n</tbody></table>\n<h3 id=\"common-pitfalls\">Common Pitfalls</h3>\n<p>⚠️ <strong>Pitfall: API Versioning Inconsistency</strong>\nComponent APIs evolve independently, leading to compatibility issues when different components expect different API versions. This manifests as cryptic errors when new model registry versions return additional fields that older deployment components don&#39;t expect, causing deployment failures. Fix this by implementing explicit API version headers in all requests and maintaining backward compatibility for at least two major versions. Use content negotiation to allow clients to specify which response format they support.</p>\n<p>⚠️ <strong>Pitfall: Event Ordering Race Conditions</strong>\nPublishing model promotion and deployment events simultaneously can cause deployments to start before model artifacts are fully replicated across storage systems. This results in deployment failures with &quot;artifact not found&quot; errors that resolve when retried later. Prevent this by implementing event dependencies where deployment events include artifact readiness checks, or use event sequencing where model registry confirms artifact replication before publishing promotion events.</p>\n<p>⚠️ <strong>Pitfall: Missing Correlation ID Propagation</strong>\nEvents related to the same workflow use different correlation IDs, making it impossible to trace complete workflow execution during debugging or compliance audits. This happens when components generate new correlation IDs instead of propagating existing ones from upstream events. Fix this by requiring all API calls to accept optional correlation IDs and propagating them through all downstream events and API calls.</p>\n<p>⚠️ <strong>Pitfall: Event Payload Size Explosion</strong>\nIncluding complete model metadata or large artifact lists in event payloads causes event delivery timeouts and memory issues in event processing systems. Events should contain only essential identifiers and lightweight metadata, with consumers making additional API calls to fetch detailed information when needed. Use event payload size limits (e.g., 1MB) and design events to reference resources rather than embedding them.</p>\n<p>⚠️ <strong>Pitfall: Synchronous Event Processing Blocking</strong>\nComponents that process events synchronously can block event delivery when processing takes significant time, such as downloading large model artifacts or running complex validation checks. This causes event backlogs and workflow delays. Implement asynchronous event processing where event handlers quickly acknowledge receipt and perform actual work in background tasks.</p>\n<p>⚠️ <strong>Pitfall: Missing Event Deduplication</strong>\nNetwork retries and component restarts can cause duplicate event delivery, leading to duplicate model registrations, multiple deployment attempts, or redundant monitoring setup. Implement idempotent event handlers that check if the requested action has already been completed before proceeding, using resource checksums or unique request identifiers to detect duplicates.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>API Framework</td>\n<td>Flask/FastAPI with OpenAPI specs</td>\n<td>Django REST Framework with auto-generated clients</td>\n</tr>\n<tr>\n<td>Event Broker</td>\n<td>Redis Streams with consumer groups</td>\n<td>Apache Kafka with Schema Registry</td>\n</tr>\n<tr>\n<td>API Gateway</td>\n<td>nginx reverse proxy with basic routing</td>\n<td>Kong or Envoy with rate limiting and authentication</td>\n</tr>\n<tr>\n<td>Service Discovery</td>\n<td>Static configuration files</td>\n<td>Consul or etcd with health checking</td>\n</tr>\n<tr>\n<td>Event Schema</td>\n<td>JSON with manual validation</td>\n<td>Protocol Buffers with automatic validation</td>\n</tr>\n<tr>\n<td>API Authentication</td>\n<td>JWT tokens with shared secrets</td>\n<td>OAuth2 with PKCE and token introspection</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>mlops-platform/\n├── services/\n│   ├── experiment-tracking/\n│   │   ├── src/api/\n│   │   │   ├── external_api.py      # Client-facing REST endpoints\n│   │   │   ├── internal_api.py      # Inter-component endpoints\n│   │   │   └── schemas.py           # Request/response models\n│   │   ├── src/events/\n│   │   │   ├── handlers.py          # Event processing logic\n│   │   │   └── publishers.py        # Event publishing utilities\n│   │   └── tests/integration/\n│   │       └── api_integration_test.py\n│   ├── model-registry/\n│   │   ├── src/api/\n│   │   ├── src/events/\n│   │   └── tests/\n│   └── shared/\n│       ├── event_coordinator/\n│       │   ├── coordinator.py       # Event broker abstraction\n│       │   ├── event_schemas.py     # Common event definitions\n│       │   └── delivery.py          # Reliability and ordering\n│       ├── api_client/\n│       │   ├── base_client.py       # Common HTTP client functionality\n│       │   ├── auth.py              # JWT handling and refresh\n│       │   └── retry.py             # Retry policies and circuit breakers\n│       └── monitoring/\n│           ├── health_checks.py     # Component health monitoring\n│           ├── metrics.py           # Prometheus metrics collection\n│           └── tracing.py           # Request correlation and tracing</code></pre></div>\n\n<h4 id=\"event-coordinator-infrastructure-code\">Event Coordinator Infrastructure Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Event coordination infrastructure providing reliable delivery and ordering guarantees.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">This module implements the central event distribution system that enables loose coupling</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">between MLOps platform components while maintaining workflow consistency.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, asdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> queue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Event</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Immutable event object with standardized metadata for platform coordination.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlation_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"1.0\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Event'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            id</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">event_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            source</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">source,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            payload</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">payload,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            correlation_id</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">payload.get(</span><span style=\"color:#9ECBFF\">'correlation_id'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            metadata</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> to_json</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Serialize event for storage and transmission.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> json.dumps(asdict(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_json</span><span style=\"color:#E1E4E8\">(cls, json_str: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'Event'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deserialize event from JSON string.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.loads(json_str)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">data)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventHandler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Wrapper for event handler functions with error handling and retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, handler_func: Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">], max_retries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.handler_func </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> handler_func</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_retries</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"event_handler.</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">handler_func.</span><span style=\"color:#79B8FF\">__name__}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> handle</span><span style=\"color:#E1E4E8\">(self, event: Event) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute handler with retry logic, returns True if successful.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.handler_func(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Handler attempt </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">attempt </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1}</span><span style=\"color:#9ECBFF\"> failed: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_retries:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Handler failed after </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1}</span><span style=\"color:#9ECBFF\"> attempts: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                time.sleep(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#F97583\"> **</span><span style=\"color:#E1E4E8\"> attempt)  </span><span style=\"color:#6A737D\"># Exponential backoff</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Abstract interface for event persistence supporting audit and replay capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store_event</span><span style=\"color:#E1E4E8\">(self, event: Event) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Persist event to durable storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_events</span><span style=\"color:#E1E4E8\">(self, start_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, end_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   event_types: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Event]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve events for replay or audit purposes.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_events_by_correlation</span><span style=\"color:#E1E4E8\">(self, correlation_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[Event]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get all events for a specific workflow or operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> InMemoryEventStorage</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">EventStorage</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Simple in-memory event storage for development and testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.events: List[Event] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store_event</span><span style=\"color:#E1E4E8\">(self, event: Event) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store event in memory with thread safety.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.events.append(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_events</span><span style=\"color:#E1E4E8\">(self, start_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, end_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   event_types: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> List[Event]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Filter events by time range and optional type filter.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            filtered </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [e </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> e </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.events </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                       if</span><span style=\"color:#E1E4E8\"> start_time </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> e.timestamp </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#E1E4E8\"> end_time]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> event_types:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                filtered </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [e </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> e </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> filtered </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> e.type </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> event_types]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> filtered</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_events_by_correlation</span><span style=\"color:#E1E4E8\">(self, correlation_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[Event]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get all events with matching correlation ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> [e </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> e </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.events </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> e.correlation_id </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> correlation_id]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Central event coordination system managing subscription and delivery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, storage: EventStorage):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.subscriptions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[EventHandler]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.event_queue </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> queue.Queue()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.processing_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#9ECBFF\">\"event_coordinator\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start event processing thread.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.processing_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Thread(</span><span style=\"color:#FFAB70\">target</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._process_events)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.processing_thread.daemon </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.processing_thread.start()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#9ECBFF\">\"Event coordinator started\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> stop</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Stop event processing and wait for thread completion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.running </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.processing_thread:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.processing_thread.join(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#9ECBFF\">\"Event coordinator stopped\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> publish</span><span style=\"color:#E1E4E8\">(self, event: Event, synchronous: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Publish event to subscribers with optional synchronous processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Store event for audit trail</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.storage.store_event(event):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Failed to store event </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> synchronous:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._deliver_event(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.event_queue.put(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> subscribe</span><span style=\"color:#E1E4E8\">(self, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, handler: Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register event handler for specific event type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        subscription_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(uuid.uuid4())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        event_handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> EventHandler(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> event_type </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.subscriptions:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.subscriptions[event_type] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.subscriptions[event_type].append(event_handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Registered handler for </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, subscription </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">subscription_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> subscription_id</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _process_events</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Background thread that processes events from queue.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        while</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.running:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                event </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.event_queue.get(</span><span style=\"color:#FFAB70\">timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._deliver_event(event)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.event_queue.task_done()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> queue.Empty:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                continue</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Error processing event: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _deliver_event</span><span style=\"color:#E1E4E8\">(self, event: Event) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Deliver event to all registered handlers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        handlers </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.subscriptions.get(event.type, [])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> handlers:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.debug(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"No handlers registered for event type </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        success_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> handlers:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> handler.handle(event):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                success_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Delivered event </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> to </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">success_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(handlers)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> handlers\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> success_count </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(handlers)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> replay_events</span><span style=\"color:#E1E4E8\">(self, start_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, end_time: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                     event_types: Optional[List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Replay stored events for debugging or recovery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        events </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.storage.get_events(start_time, end_time, event_types)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        replayed </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> event </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> events:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._deliver_event(event):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                replayed </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Replayed </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">replayed</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(events)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> events\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> replayed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_event_history</span><span style=\"color:#E1E4E8\">(self, correlation_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[Event]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get complete event history for workflow tracing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.storage.get_events_by_correlation(correlation_id)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Event type constants for type safety and consistency</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MODEL_PROMOTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"model.promoted\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">PIPELINE_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pipeline.completed\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEPLOYMENT_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"deployment.failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">STEP_FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pipeline.step.failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RESOURCE_EXHAUSTED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"pipeline.resource.exhausted\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">EXPERIMENT_COMPLETED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"experiment.completed\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Global event coordinator instance (initialized by main application)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">_event_coordinator: Optional[EventCoordinator] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_event_coordinator</span><span style=\"color:#E1E4E8\">() -> EventCoordinator:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Get global event coordinator instance.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    global</span><span style=\"color:#E1E4E8\"> _event_coordinator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> _event_coordinator </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> RuntimeError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Event coordinator not initialized\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> _event_coordinator</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> initialize_event_coordinator</span><span style=\"color:#E1E4E8\">(storage: EventStorage) -> EventCoordinator:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Initialize global event coordinator with storage backend.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    global</span><span style=\"color:#E1E4E8\"> _event_coordinator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _event_coordinator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> EventCoordinator(storage)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    _event_coordinator.start()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> _event_coordinator</span></span></code></pre></div>\n\n<h4 id=\"api-client-infrastructure-code\">API Client Infrastructure Code</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Common API client infrastructure for inter-component communication.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides authentication, retry logic, and circuit breakers for reliable</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">distributed system communication between MLOps platform components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Circuit breaker states for handling downstream service failures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CLOSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"closed\"</span><span style=\"color:#6A737D\">      # Normal operation, requests pass through</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"open\"</span><span style=\"color:#6A737D\">          # Failing fast, requests rejected immediately</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HALF_OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"half_open\"</span><span style=\"color:#6A737D\">  # Testing if downstream service recovered</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RetryConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for exponential backoff retry policies.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    initial_delay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_delay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    backoff_multiplier: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    retryable_status_codes: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __post_init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.retryable_status_codes </span><span style=\"color:#F97583\">is</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.retryable_status_codes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">502</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">503</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">504</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">429</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Circuit breaker implementation preventing cascade failures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, failure_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span><span style=\"color:#E1E4E8\">, reset_timeout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> failure_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.reset_timeout </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> reset_timeout</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#9ECBFF\">\"circuit_breaker\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> can_execute</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if request should be allowed through circuit breaker.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.reset_timeout:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#9ECBFF\">\"Circuit breaker entering half-open state\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_success</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record successful request, potentially closing circuit.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">HALF_OPEN</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#9ECBFF\">\"Circuit breaker closed after successful request\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_failure</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record failed request, potentially opening circuit.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.failure_threshold:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitState.</span><span style=\"color:#79B8FF\">OPEN</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Circuit breaker opened after </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.failure_count</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> failures\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> APIClient</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base HTTP client with authentication, retries, and circuit breaking.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, base_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, auth_token: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 retry_config: Optional[RetryConfig] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> base_url.rstrip(</span><span style=\"color:#9ECBFF\">'/'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.auth_token </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> auth_token</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.retry_config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retry_config </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> RetryConfig()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.circuit_breaker </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreaker()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.session </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> requests.Session()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#9ECBFF\">\"api_client\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Set default headers</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.session.headers.update({</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'Content-Type'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/json'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'Accept'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'application/json'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'User-Agent'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'MLOps-Platform/1.0'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.auth_token:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.session.headers[</span><span style=\"color:#9ECBFF\">'Authorization'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">'Bearer </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.auth_token</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _should_retry</span><span style=\"color:#E1E4E8\">(self, response: requests.Response, attempt: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if request should be retried based on response and attempt count.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.retry_config.max_attempts:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (response.status_code </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.retry_config.retryable_status_codes </span><span style=\"color:#F97583\">or</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                response.status_code </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> requests.codes.request_timeout)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_delay</span><span style=\"color:#E1E4E8\">(self, attempt: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate exponential backoff delay for retry attempt.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.retry_config.initial_delay </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.retry_config.backoff_multiplier </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\"> attempt)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(delay, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.retry_config.max_delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> request</span><span style=\"color:#E1E4E8\">(self, method: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs) -> requests.Response:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Make HTTP request with retry logic and circuit breaker protection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        url </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.base_url</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">endpoint</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.circuit_breaker.can_execute():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#E1E4E8\"> requests.exceptions.HTTPError(</span><span style=\"color:#9ECBFF\">\"Circuit breaker is open\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        last_exception </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.retry_config.max_attempts):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                response </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.session.request(method, url, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Record success for circuit breaker</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> response.status_code </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 500</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    self</span><span style=\"color:#E1E4E8\">.circuit_breaker.record_success()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Check if we should retry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._should_retry(response, attempt):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                    return</span><span style=\"color:#E1E4E8\"> response</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Request failed with status </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">response.status_code</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                                  f</span><span style=\"color:#9ECBFF\">\"attempt </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">attempt </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1}</span><span style=\"color:#9ECBFF\">/</span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.retry_config.max_attempts</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> requests.exceptions.RequestException </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                last_exception </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> e</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.circuit_breaker.record_failure()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Request exception on attempt </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">attempt </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Wait before retry (except on last attempt)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> attempt </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.retry_config.max_attempts </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._calculate_delay(attempt)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                time.sleep(delay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # All retries exhausted</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> last_exception:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#E1E4E8\"> last_exception</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            raise</span><span style=\"color:#E1E4E8\"> requests.exceptions.HTTPError(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Request failed after </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.retry_config.max_attempts</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> attempts\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get</span><span style=\"color:#E1E4E8\">(self, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, params: Optional[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> requests.Response:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Make GET request with error handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.request(</span><span style=\"color:#9ECBFF\">'GET'</span><span style=\"color:#E1E4E8\">, endpoint, </span><span style=\"color:#FFAB70\">params</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">params)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> post</span><span style=\"color:#E1E4E8\">(self, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Optional[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> requests.Response:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Make POST request with JSON payload.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        json_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.dumps(data) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> data </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.request(</span><span style=\"color:#9ECBFF\">'POST'</span><span style=\"color:#E1E4E8\">, endpoint, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">json_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> put</span><span style=\"color:#E1E4E8\">(self, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, data: Optional[Dict] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> requests.Response:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Make PUT request with JSON payload.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        json_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.dumps(data) </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> data </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.request(</span><span style=\"color:#9ECBFF\">'PUT'</span><span style=\"color:#E1E4E8\">, endpoint, </span><span style=\"color:#FFAB70\">data</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">json_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> delete</span><span style=\"color:#E1E4E8\">(self, endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> requests.Response:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Make DELETE request.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.request(</span><span style=\"color:#9ECBFF\">'DELETE'</span><span style=\"color:#E1E4E8\">, endpoint)</span></span></code></pre></div>\n\n<h4 id=\"component-integration-skeleton\">Component Integration Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Example integration patterns for MLOps platform components.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Demonstrates how components should interact through APIs and events</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">while maintaining loose coupling and error resilience.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Import shared infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> shared.event_coordinator </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Event, get_event_coordinator, </span><span style=\"color:#79B8FF\">MODEL_PROMOTED</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">PIPELINE_COMPLETED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> shared.api_client </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> APIClient</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComponentConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configuration for component integration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    base_url: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    auth_token: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    event_subscriptions: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MLOpsComponent</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for MLOps platform components with common integration patterns.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: ComponentConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"component.</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">config.component_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.api_clients: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, APIClient] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.event_coordinator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> get_event_coordinator()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._setup_event_subscriptions()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _setup_event_subscriptions</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register event handlers for component-specific events.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> event_type </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.config.event_subscriptions:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.event_coordinator.subscribe(event_type, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._handle_event)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Subscribed to event type: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_event</span><span style=\"color:#E1E4E8\">(self, event: Event):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Route events to specific handler methods based on event type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        handler_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"_handle_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.type.replace(</span><span style=\"color:#9ECBFF\">'.'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'_'</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> getattr</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">, handler_name, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> handler:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                handler(event)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.logger.error(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Error handling event </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.warning(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"No handler found for event type: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event.type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_api_client</span><span style=\"color:#E1E4E8\">(self, service_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, base_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> APIClient:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get or create API client for external service.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> service_name </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.api_clients:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.api_clients[service_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> APIClient(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                base_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">base_url,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                auth_token</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.auth_token</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.api_clients[service_name]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> publish_event</span><span style=\"color:#E1E4E8\">(self, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Publish event with component source information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        event </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> Event.create(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            event_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">event_type,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            source</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.config.component_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            payload</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">payload</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.event_coordinator.publish(event)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Published event: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">event_type</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ModelRegistryIntegration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MLOpsComponent</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Example integration for Model Registry component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: ComponentConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(config)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.experiment_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_api_client(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'experiment_tracking'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'http://experiment-service:8080'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_model_from_experiment</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                     run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Register model version with lineage to experiment run.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Implement complete model registration workflow</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Validate run_id exists in experiment tracking</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Download and validate model artifact</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Create model version entry with metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Publish model.registered event for downstream components</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Get experiment run metadata for lineage</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        response </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.experiment_client.get(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">'/api/v1/runs/</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">run_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle response errors and missing runs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        run_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> response.json()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract relevant metadata (parameters, metrics, artifacts)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Register model version with extracted metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return model version details</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_experiment_completed</span><span style=\"color:#E1E4E8\">(self, event: Event):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handle experiment completion events for automatic model registration.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Evaluate if experiment results meet registration criteria</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Check if automatic registration is configured for experiment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Call register_model_from_experiment if criteria are met</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Handle registration failures gracefully</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DeploymentIntegration</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MLOpsComponent</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Example integration for Model Deployment component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: ComponentConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(config)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.model_registry_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_api_client(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'model_registry'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'http://model-registry:8080'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring_client </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.get_api_client(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'monitoring'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'http://monitoring:8080'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _handle_model_promoted</span><span style=\"color:#E1E4E8\">(self, event: Event):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Handle model promotion events for automatic deployment.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Check if promoted stage requires automatic deployment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Retrieve model version details from registry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Create deployment specification based on model metadata</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Execute deployment using configured strategy (blue-green, canary)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Monitor deployment health and publish deployment.healthy event</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Setup monitoring baseline for newly deployed model</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event.payload[</span><span style=\"color:#9ECBFF\">'model_name'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        version </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event.payload[</span><span style=\"color:#9ECBFF\">'version'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        new_stage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> event.payload[</span><span style=\"color:#9ECBFF\">'new_stage'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> new_stage </span><span style=\"color:#F97583\">==</span><span style=\"color:#9ECBFF\"> 'Production'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement automatic production deployment</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> deploy_model_version</span><span style=\"color:#E1E4E8\">(self, model_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           deployment_spec: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Deploy specific model version with given specification.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Validate deployment specification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Download model artifacts from registry</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Create serving container with model</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Configure auto-scaling and health checks</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Set up traffic routing (canary or blue-green)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        TODO</span><span style=\"color:#9ECBFF\">: Return deployment ID for tracking</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Example usage and testing patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> setup_component_integration</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Example setup for component integration in main application.</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Load configuration from environment variables</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Initialize event coordinator with production storage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Create and start all component instances</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    TODO</span><span style=\"color:#9ECBFF\">: Setup health check endpoints for each component</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Milestone checkpoint: After implementing component integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_integration_workflow</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Integration test demonstrating complete workflow coordination.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Expected behavior:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    1. Start experiment tracking and model registry components</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    2. Create experiment and log training run with model artifact</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    3. Verify model.registered event is published automatically</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    4. Verify deployment component receives and processes event</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    5. Check that model endpoint becomes available and healthy</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Run this test to verify: python -m pytest tests/integration/test_component_coordination.py</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Expected output: All workflow steps complete successfully with events logged</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips-for-component-interactions\">Debugging Tips for Component Interactions</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Events published but not received</td>\n<td>Subscription registration failed or wrong event type</td>\n<td>Check event coordinator logs for subscription confirmations</td>\n<td>Verify event type strings match exactly between publishers and subscribers</td>\n</tr>\n<tr>\n<td>API calls timeout during high load</td>\n<td>Circuit breaker opening due to downstream failures</td>\n<td>Check API client circuit breaker state and failure counts</td>\n<td>Increase circuit breaker thresholds or improve downstream service performance</td>\n</tr>\n<tr>\n<td>Workflow steps execute out of order</td>\n<td>Event processing happening concurrently without ordering</td>\n<td>Review event timestamps and correlation IDs</td>\n<td>Implement event sequencing for same-resource operations</td>\n</tr>\n<tr>\n<td>Duplicate actions on workflow retry</td>\n<td>Event handlers not idempotent</td>\n<td>Check for duplicate model registrations or deployments</td>\n<td>Add idempotency checks using resource checksums or unique identifiers</td>\n</tr>\n<tr>\n<td>Missing correlation between workflow steps</td>\n<td>Correlation IDs not propagated</td>\n<td>Search event logs for missing correlation ID fields</td>\n<td>Ensure all API calls and events include correlation_id from upstream requests</td>\n</tr>\n<tr>\n<td>Component integration failures</td>\n<td>Authentication tokens expired or invalid</td>\n<td>Check API response status codes and authentication headers</td>\n<td>Implement token refresh logic or verify token scope permissions</td>\n</tr>\n</tbody></table>\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing component interactions and data flow:</p>\n<p><strong>Verification Command</strong>: <code>python -m pytest tests/integration/test_end_to_end_workflow.py -v</code></p>\n<p><strong>Expected Behavior</strong>:</p>\n<ol>\n<li><strong>Event Coordination</strong>: Events published by one component are received and processed by subscribed components within 5 seconds</li>\n<li><strong>API Integration</strong>: Components can successfully call each other&#39;s APIs with proper authentication and retry handling</li>\n<li><strong>Workflow Completion</strong>: Complete experiment-to-deployment workflow completes successfully with all intermediate events logged</li>\n<li><strong>Error Recovery</strong>: Failed API calls are retried according to configured policies, and circuit breakers prevent cascade failures</li>\n<li><strong>Audit Trail</strong>: All workflow steps can be traced using correlation IDs through event and API logs</li>\n</ol>\n<p><strong>Manual Testing</strong>:</p>\n<ol>\n<li>Start all components: <code>docker-compose up -d</code></li>\n<li>Create experiment and run training: <code>curl -X POST http://localhost:8080/api/v1/experiments -d &#39;{&quot;name&quot;: &quot;test-workflow&quot;}&#39;</code></li>\n<li>Verify model registration: Check model registry UI shows new model version</li>\n<li>Verify deployment: Check that model endpoint responds to prediction requests</li>\n<li>Verify monitoring: Check that prediction requests are logged and baseline is established</li>\n</ol>\n<p><strong>Troubleshooting</strong>: If workflow steps fail, check event coordinator logs for delivery failures and component logs for API integration errors. Use correlation IDs to trace specific workflow execution through all components.</p>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-5) by providing comprehensive failure mode analysis and recovery mechanisms that ensure the platform remains operational despite component failures, data corruption, and edge cases.</p>\n</blockquote>\n<p>Building a robust MLOps platform requires anticipating and gracefully handling the myriad ways distributed systems can fail. Think of error handling in an MLOps platform like designing a hospital&#39;s emergency response system - you need to identify every possible crisis, establish detection procedures, and create recovery protocols that minimize harm while restoring normal operations. Unlike simple applications that might crash and restart, an MLOps platform manages long-running training jobs, production model endpoints serving live traffic, and valuable experiment data that cannot be lost.</p>\n<p>The complexity of error handling in MLOps stems from the platform&#39;s distributed nature and the diverse types of failures that can occur. Training pipelines might fail due to resource constraints, model deployments might encounter version incompatibilities, and monitoring systems might detect data drift requiring immediate intervention. Each component must handle both internal failures and cascading failures from dependent components, while maintaining data consistency and enabling automated recovery wherever possible.</p>\n<h3 id=\"system-failure-modes\">System Failure Modes</h3>\n<p>Understanding failure modes requires examining each component&#39;s critical dependencies and the ways they can break. Like a medical diagnosis guide, we categorize failures by symptoms, root causes, and affected systems to enable rapid identification and response.</p>\n<h4 id=\"experiment-tracking-failures\">Experiment Tracking Failures</h4>\n<p>The experiment tracking component faces several critical failure modes that can disrupt the research workflow and cause data loss. These failures typically manifest as inability to log new experiments, missing historical data, or corrupted artifact storage.</p>\n<p><strong>Mental Model: Research Laboratory Breakdown</strong>\nThink of experiment tracking failures like equipment failures in a research laboratory. A broken scale means you can&#39;t measure new samples, but existing measurements remain valid. A fire in the storage room destroys historical samples but doesn&#39;t prevent new experiments. A power outage stops all work until restored. Each type of failure requires different emergency procedures and recovery strategies.</p>\n<p><strong>Database Connection Failures</strong> occur when the metadata store becomes unreachable due to network partitions, database server crashes, or connection pool exhaustion. These failures prevent logging new experiments while leaving existing data intact.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Symptom</th>\n<th>Root Cause</th>\n<th>Immediate Impact</th>\n<th>Downstream Effects</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Connection timeout on log_param calls</td>\n<td>Database server overload</td>\n<td>Cannot log new parameters</td>\n<td>Training scripts hang waiting for logging</td>\n</tr>\n<tr>\n<td>&quot;Too many connections&quot; errors</td>\n<td>Connection pool exhaustion</td>\n<td>New experiment creation fails</td>\n<td>Researchers cannot start new runs</td>\n</tr>\n<tr>\n<td>Intermittent query failures</td>\n<td>Network partition to database</td>\n<td>Inconsistent data retrieval</td>\n<td>Run comparison views show incomplete results</td>\n</tr>\n<tr>\n<td>Database lock timeouts</td>\n<td>Concurrent write conflicts</td>\n<td>Metric logging operations fail</td>\n<td>Training progress not tracked</td>\n</tr>\n</tbody></table>\n<p><strong>Artifact Storage Failures</strong> manifest when the object storage system experiences outages, quota exhaustion, or corruption. Unlike metadata failures, artifact failures can cause permanent data loss if not handled properly.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Type</th>\n<th>Detection Method</th>\n<th>Data at Risk</th>\n<th>Recovery Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>S3 bucket unreachable</td>\n<td>HTTP 503 responses</td>\n<td>New artifacts only</td>\n<td>Retry with exponential backoff</td>\n</tr>\n<tr>\n<td>Storage quota exceeded</td>\n<td>HTTP 413 responses</td>\n<td>All new artifacts</td>\n<td>Trigger artifact cleanup policies</td>\n</tr>\n<tr>\n<td>Corrupted artifact downloads</td>\n<td>Checksum validation failure</td>\n<td>Specific artifacts</td>\n<td>Re-upload from training environment</td>\n</tr>\n<tr>\n<td>Permission denied errors</td>\n<td>HTTP 403 responses</td>\n<td>Component-specific artifacts</td>\n<td>Update IAM policies and rotate credentials</td>\n</tr>\n</tbody></table>\n<p><strong>Metadata Corruption</strong> represents the most serious failure mode, where stored experiment data becomes inconsistent or unreadable. This typically occurs during partial writes, concurrent modifications, or storage system bugs.</p>\n<p>The experiment tracking system must detect corruption early through consistency checks and provide recovery mechanisms that minimize data loss. Corruption scenarios include orphaned artifacts (metadata points to non-existent files), missing foreign keys (runs reference non-existent experiments), and inconsistent timestamps (end time before start time).</p>\n<h4 id=\"model-registry-failures\">Model Registry Failures</h4>\n<p>The model registry&#39;s failure modes center around version consistency, artifact integrity, and stage transition workflows. Since the registry serves as the authoritative source for production model deployments, failures can directly impact live systems.</p>\n<p><strong>Mental Model: Bank Vault Security Breach</strong>\nThink of model registry failures like security breaches in a bank vault. A broken lock means you can&#39;t access your assets temporarily. Corrupted records mean you can&#39;t prove ownership. A compromised vault means the integrity of all assets is questionable. Each scenario requires different containment and recovery procedures.</p>\n<p><strong>Version Consistency Failures</strong> occur when model metadata becomes disconnected from actual model artifacts, or when the version history becomes corrupted. These failures threaten the fundamental guarantees that model versions are immutable and traceable.</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Violation</th>\n<th>Detection Method</th>\n<th>Risk Level</th>\n<th>Mitigation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model artifact missing for registered version</td>\n<td>Checksum validation during retrieval</td>\n<td>High - deployment failure</td>\n<td>Maintain redundant artifact storage</td>\n</tr>\n<tr>\n<td>Multiple versions claiming same artifact hash</td>\n<td>Content-addressable storage verification</td>\n<td>Medium - lineage confusion</td>\n<td>Implement atomic registration transactions</td>\n</tr>\n<tr>\n<td>Stage transition without approval workflow</td>\n<td>Audit log verification</td>\n<td>High - unauthorized production deployment</td>\n<td>Enforce approval gates in API layer</td>\n</tr>\n<tr>\n<td>Orphaned model versions after experiment deletion</td>\n<td>Foreign key constraint violations</td>\n<td>Low - storage waste</td>\n<td>Cascade deletion policies with grace periods</td>\n</tr>\n</tbody></table>\n<p><strong>Stage Transition Failures</strong> disrupt the model promotion workflow, potentially blocking production deployments or allowing unauthorized model releases. These failures require immediate intervention to maintain deployment governance.</p>\n<p>The registry must handle scenarios where approval workflows fail mid-transition, multiple users attempt simultaneous promotions, and external validation systems become unavailable during promotion checks. Recovery procedures must ensure that partially completed transitions are either completed or cleanly rolled back.</p>\n<p><strong>Lineage Tracking Corruption</strong> breaks the traceability links between models and their source experiments, training data, and code versions. While not immediately fatal, lineage corruption undermines reproducibility and compliance requirements.</p>\n<table>\n<thead>\n<tr>\n<th>Lineage Break</th>\n<th>Impact</th>\n<th>Detection</th>\n<th>Recovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing experiment run reference</td>\n<td>Cannot reproduce model training</td>\n<td>Periodic lineage validation</td>\n<td>Manual reconstruction from logs</td>\n</tr>\n<tr>\n<td>Broken training data hash links</td>\n<td>Cannot verify data provenance</td>\n<td>Data integrity checks</td>\n<td>Re-compute hashes from source data</td>\n</tr>\n<tr>\n<td>Invalid code commit references</td>\n<td>Cannot access training code</td>\n<td>Git repository validation</td>\n<td>Update references or mark as unrecoverable</td>\n</tr>\n<tr>\n<td>Circular dependency in lineage graph</td>\n<td>Infinite loops in dependency traversal</td>\n<td>Graph cycle detection</td>\n<td>Break cycles at newest dependency</td>\n</tr>\n</tbody></table>\n<h4 id=\"training-pipeline-failures\">Training Pipeline Failures</h4>\n<p>Training pipelines face the most complex failure scenarios due to their distributed nature, resource dependencies, and long execution times. Failures can occur at the orchestration level, individual step level, or resource management level.</p>\n<p><strong>Mental Model: Assembly Line Disruption</strong>\nThink of pipeline failures like disruptions in a manufacturing assembly line. A broken machine stops one station but shouldn&#39;t shut down the entire line. A power outage affects everything temporarily. A defective component early in the line creates waste downstream. Each disruption type requires different containment and recovery strategies.</p>\n<p><strong>Orchestration Engine Failures</strong> occur when the pipeline scheduler becomes unavailable, loses track of running jobs, or encounters resource allocation conflicts. These failures can leave jobs running without supervision or prevent new pipelines from starting.</p>\n<table>\n<thead>\n<tr>\n<th>Orchestration Issue</th>\n<th>Manifestation</th>\n<th>Immediate Action</th>\n<th>Long-term Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scheduler pod crash</td>\n<td>New pipelines stuck in PENDING</td>\n<td>Restart scheduler with state recovery</td>\n<td>Delayed pipeline starts</td>\n</tr>\n<tr>\n<td>Lost job tracking state</td>\n<td>Running jobs become &quot;orphaned&quot;</td>\n<td>Reconcile actual vs. recorded job state</td>\n<td>Resource leaks from untracked jobs</td>\n</tr>\n<tr>\n<td>Resource quota exhaustion</td>\n<td>Steps fail with insufficient resources</td>\n<td>Trigger resource cleanup and queuing</td>\n<td>Cascading delays across pipelines</td>\n</tr>\n<tr>\n<td>Dead node with running steps</td>\n<td>Steps marked RUNNING but not progressing</td>\n<td>Detect node failure and reschedule</td>\n<td>Partial work loss requiring restart</td>\n</tr>\n</tbody></table>\n<p><strong>Step Execution Failures</strong> happen when individual pipeline steps crash, encounter data validation errors, or exceed resource limits. The pipeline orchestrator must decide whether to retry, skip, or abort the entire pipeline based on the failure type and configured policies.</p>\n<p>Step failures require sophisticated error classification to determine appropriate recovery actions. Transient errors (network timeouts, temporary resource unavailability) warrant automatic retry with exponential backoff. Data errors (schema validation failures, corrupt input files) require human intervention to fix upstream issues. Resource errors (out-of-memory, disk full) need resource reconfiguration before retry.</p>\n<p><strong>Data Dependency Violations</strong> occur when pipeline steps receive invalid or missing input data, breaking the expected data flow between steps. These violations can cascade through the pipeline, corrupting downstream processing.</p>\n<table>\n<thead>\n<tr>\n<th>Dependency Violation</th>\n<th>Root Cause</th>\n<th>Detection Point</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing input artifact</td>\n<td>Upstream step failure or cleanup</td>\n<td>Step startup validation</td>\n<td>Re-run upstream dependencies</td>\n</tr>\n<tr>\n<td>Schema validation failure</td>\n<td>Data format change or corruption</td>\n<td>Input processing stage</td>\n<td>Fail fast with clear error message</td>\n</tr>\n<tr>\n<td>Stale data dependencies</td>\n<td>Clock skew or caching issues</td>\n<td>Timestamp validation</td>\n<td>Force refresh of cached dependencies</td>\n</tr>\n<tr>\n<td>Cross-pipeline data conflicts</td>\n<td>Concurrent modifications to shared data</td>\n<td>File lock conflicts</td>\n<td>Implement data versioning and isolation</td>\n</tr>\n</tbody></table>\n<h4 id=\"model-deployment-failures\">Model Deployment Failures</h4>\n<p>Model deployment failures can directly impact production traffic and user-facing applications. These failures require immediate detection and automated recovery to minimize service disruption.</p>\n<p><strong>Mental Model: Restaurant Service Breakdown</strong>\nThink of deployment failures like service breakdowns in a restaurant. A chef getting sick means one station slows down but others continue. A power outage stops all cooking until restored. A food safety issue requires immediate shutdown and cleanup. Each scenario has different urgency levels and recovery procedures.</p>\n<p><strong>Health Check Failures</strong> indicate that deployed model endpoints are not responding correctly to requests, either due to model loading issues, resource constraints, or infrastructure problems.</p>\n<table>\n<thead>\n<tr>\n<th>Health Check Failure</th>\n<th>Probable Cause</th>\n<th>Service Impact</th>\n<th>Auto-Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HTTP 503 responses</td>\n<td>Model loading timeout or memory pressure</td>\n<td>Partial traffic loss</td>\n<td>Scale up replicas and retry</td>\n</tr>\n<tr>\n<td>High latency responses</td>\n<td>CPU throttling or model complexity</td>\n<td>Degraded user experience</td>\n<td>Implement request queuing</td>\n</tr>\n<tr>\n<td>Prediction accuracy drop</td>\n<td>Model serving infrastructure bug</td>\n<td>Incorrect results to users</td>\n<td>Rollback to previous version</td>\n</tr>\n<tr>\n<td>Memory leak detection</td>\n<td>Model or serving framework bug</td>\n<td>Gradually degrading performance</td>\n<td>Rolling restart of serving pods</td>\n</tr>\n</tbody></table>\n<p><strong>Traffic Routing Failures</strong> disrupt the careful traffic management required for canary deployments and A/B testing. These failures can route traffic to wrong model versions or fail to balance load appropriately.</p>\n<p>Routing failures often manifest as traffic imbalances (all traffic to one version), routing loops (requests bounce between endpoints), or version confusion (requests served by wrong model version). The deployment system must detect these issues quickly and implement safeguards to restore proper traffic flow.</p>\n<p><strong>Rollback Failures</strong> represent the worst-case scenario where both the new model version and the rollback mechanism fail simultaneously. This leaves the deployment in an inconsistent state with no clear recovery path.</p>\n<table>\n<thead>\n<tr>\n<th>Rollback Scenario</th>\n<th>Failure Point</th>\n<th>Remaining Options</th>\n<th>Prevention Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Previous version artifacts deleted</td>\n<td>Artifact cleanup policy too aggressive</td>\n<td>Deploy older known-good version</td>\n<td>Implement version retention policies</td>\n</tr>\n<tr>\n<td>Configuration drift during rollback</td>\n<td>Infrastructure changes since last deployment</td>\n<td>Manual infrastructure reconciliation</td>\n<td>Configuration drift detection</td>\n</tr>\n<tr>\n<td>Database migration incompatibility</td>\n<td>Schema changes not backward compatible</td>\n<td>Emergency maintenance mode</td>\n<td>Backward compatibility testing</td>\n</tr>\n<tr>\n<td>Cascading dependency failures</td>\n<td>Related services expect new model schema</td>\n<td>Service mesh circuit breakers</td>\n<td>Dependency impact analysis</td>\n</tr>\n</tbody></table>\n<h4 id=\"model-monitoring-failures\">Model Monitoring Failures</h4>\n<p>Monitoring system failures create blind spots that hide model performance degradation and drift, potentially allowing serious issues to persist undetected.</p>\n<p><strong>Mental Model: Medical Monitoring Equipment Failure</strong>\nThink of monitoring failures like vital sign monitors failing in an intensive care unit. A broken heart rate monitor doesn&#39;t stop the heart, but doctors can&#39;t detect dangerous changes. Multiple monitor failures create dangerous blind spots. Backup monitoring systems and manual checks become critical for patient safety.</p>\n<p><strong>Prediction Logging Failures</strong> prevent the collection of inference data needed for drift detection and performance analysis. These failures can occur due to storage system issues, high request volumes, or logging pipeline bugs.</p>\n<table>\n<thead>\n<tr>\n<th>Logging Issue</th>\n<th>Data Loss Risk</th>\n<th>Detection Method</th>\n<th>Mitigation Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Log ingestion backpressure</td>\n<td>Recent predictions dropped</td>\n<td>Queue depth monitoring</td>\n<td>Scale logging infrastructure</td>\n</tr>\n<tr>\n<td>Storage quota exhaustion</td>\n<td>All new logs rejected</td>\n<td>Storage utilization alerts</td>\n<td>Implement data retention policies</td>\n</tr>\n<tr>\n<td>Schema evolution conflicts</td>\n<td>Logs with new fields rejected</td>\n<td>Schema validation errors</td>\n<td>Deploy backward-compatible schemas</td>\n</tr>\n<tr>\n<td>Batch processing failures</td>\n<td>Delayed availability of metrics</td>\n<td>Processing job status monitoring</td>\n<td>Implement streaming analytics backup</td>\n</tr>\n</tbody></table>\n<p><strong>Drift Detection Algorithm Failures</strong> occur when statistical analysis components encounter edge cases, insufficient data, or numerical instability. These failures can produce false alerts or miss genuine drift events.</p>\n<p>Drift detection failures often result from assumptions violated by real-world data: non-normal distributions breaking statistical tests, seasonal patterns triggering false alarms, or insufficient historical data preventing baseline establishment. The monitoring system must validate its own assumptions and gracefully degrade when conditions don&#39;t meet requirements.</p>\n<p><strong>Alert Escalation Failures</strong> prevent critical notifications from reaching the appropriate teams, allowing serious issues to persist without intervention. These failures can occur in notification systems, communication channels, or alert routing logic.</p>\n<h3 id=\"detection-and-recovery-strategies\">Detection and Recovery Strategies</h3>\n<p>Effective detection and recovery requires a layered approach that combines proactive health monitoring, automated recovery procedures, and human escalation pathways. Think of this like a tiered emergency response system where automated systems handle routine issues, escalate complex problems to specialists, and always maintain situational awareness through comprehensive monitoring.</p>\n<h4 id=\"health-check-framework\">Health Check Framework</h4>\n<p>The health check framework provides the foundation for failure detection across all components. Each component implements standardized health checks that assess both its own functionality and its dependencies.</p>\n<p><strong>Health Check Categories</strong> organize monitoring into distinct areas with different urgency levels and escalation procedures. Critical health checks indicate immediate service impact requiring automatic recovery actions. Warning-level checks indicate degraded performance that may require scaling or attention. Informational checks provide operational insights without triggering alerts.</p>\n<table>\n<thead>\n<tr>\n<th>Health Check Type</th>\n<th>Check Frequency</th>\n<th>Failure Threshold</th>\n<th>Auto-Recovery Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Liveness probe</td>\n<td>10 seconds</td>\n<td>3 consecutive failures</td>\n<td>Container restart</td>\n</tr>\n<tr>\n<td>Readiness probe</td>\n<td>5 seconds</td>\n<td>1 failure</td>\n<td>Remove from load balancer</td>\n</tr>\n<tr>\n<td>Deep dependency check</td>\n<td>60 seconds</td>\n<td>5 failures in 5 minutes</td>\n<td>Escalate to operations team</td>\n</tr>\n<tr>\n<td>Performance baseline</td>\n<td>300 seconds</td>\n<td>20% degradation sustained</td>\n<td>Trigger auto-scaling</td>\n</tr>\n</tbody></table>\n<p><strong>Component-Specific Health Checks</strong> verify the unique functionality and dependencies of each platform component. The experiment tracking component checks database connectivity and artifact storage availability. The model registry validates artifact integrity and stage transition workflows. Training pipelines monitor resource availability and job scheduling capability.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Experiment Tracking Health Checks:\n1. Database connection test with simple query execution\n2. Artifact storage write/read/delete cycle test\n3. Metadata consistency validation for recent experiments\n4. Query performance benchmark against baseline latency\n5. Storage quota verification with buffer thresholds\n\nModel Registry Health Checks:\n1. Model artifact checksum validation for recent versions\n2. Stage transition workflow simulation\n3. Lineage graph traversal performance test\n4. Approval workflow integration connectivity\n5. Version immutability constraint verification\n\nTraining Pipeline Health Checks:\n1. Kubernetes cluster resource availability check\n2. Container image registry accessibility test\n3. Persistent volume claim creation and mounting test\n4. Inter-node network connectivity validation\n5. GPU resource detection and allocation test\n\nModel Deployment Health Checks:\n1. Model endpoint response time and accuracy test\n2. Traffic routing configuration validation\n3. Auto-scaling trigger and response verification\n4. Load balancer health and configuration check\n5. Canary deployment traffic split accuracy\n\nModel Monitoring Health Checks:\n1. Prediction log ingestion rate and latency check\n2. Drift detection algorithm execution and accuracy\n3. Alert routing and escalation pathway test\n4. Dashboard data freshness and query performance\n5. Storage retention policy execution validation</code></pre></div>\n\n<h4 id=\"circuit-breaker-implementation\">Circuit Breaker Implementation</h4>\n<p>Circuit breakers prevent cascading failures by isolating failing components and providing fallback behavior during outages. The circuit breaker pattern monitors failure rates and response times, automatically opening to prevent further damage when thresholds are exceeded.</p>\n<p><strong>Mental Model: Electrical Circuit Protection</strong>\nThink of software circuit breakers like electrical circuit breakers in your home. When a device draws too much current, the breaker trips to prevent fire damage. The breaker can be manually reset once the problem is fixed. Software circuit breakers work similarly - they &quot;trip&quot; when error rates exceed thresholds, preventing cascading failures until the underlying issue is resolved.</p>\n<p><strong>Circuit Breaker States</strong> define the operational behavior and transition conditions. The closed state allows normal operation while monitoring failure rates. The open state blocks requests and returns immediate failures. The half-open state allows limited testing to determine if the underlying issue has been resolved.</p>\n<table>\n<thead>\n<tr>\n<th>State</th>\n<th>Request Behavior</th>\n<th>Monitoring Actions</th>\n<th>Transition Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Closed</td>\n<td>Forward all requests to backend</td>\n<td>Track success/failure rates</td>\n<td>Failure rate &gt; threshold → Open</td>\n</tr>\n<tr>\n<td>Open</td>\n<td>Immediately return circuit breaker error</td>\n<td>Monitor for timeout expiration</td>\n<td>Timeout elapsed → Half-Open</td>\n</tr>\n<tr>\n<td>Half-Open</td>\n<td>Forward limited test requests</td>\n<td>Evaluate test request results</td>\n<td>All tests succeed → Closed, Any test fails → Open</td>\n</tr>\n</tbody></table>\n<p><strong>Component Integration Points</strong> identify where circuit breakers provide maximum protection against cascading failures. Critical integration points include database connections, external service calls, and inter-component API communications.</p>\n<p>The experiment tracking component uses circuit breakers around database queries and artifact storage operations. When the database becomes unavailable, the circuit breaker prevents connection pool exhaustion by immediately failing requests with clear error messages. Similarly, artifact upload operations fail fast when storage systems experience outages, allowing training scripts to save artifacts locally for later upload.</p>\n<p><strong>Fallback Strategies</strong> define alternative behavior when circuit breakers open. Effective fallback strategies maintain essential functionality while clearly communicating degraded service state. Read operations might serve stale cached data with appropriate warnings. Write operations might queue requests for later processing or store data locally.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Circuit Breaker Location</th>\n<th>Fallback Strategy</th>\n<th>Degraded Service Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Tracking</td>\n<td>Database connections</td>\n<td>Cache recent experiment data</td>\n<td>Read-only access to recent experiments</td>\n</tr>\n<tr>\n<td>Model Registry</td>\n<td>Artifact storage</td>\n<td>Return metadata only, defer downloads</td>\n<td>Model information available, artifacts delayed</td>\n</tr>\n<tr>\n<td>Training Pipeline</td>\n<td>Kubernetes API</td>\n<td>Queue pipeline submissions</td>\n<td>Delayed pipeline execution</td>\n</tr>\n<tr>\n<td>Model Deployment</td>\n<td>Model serving endpoints</td>\n<td>Route to previous version</td>\n<td>Gradual traffic shift to stable version</td>\n</tr>\n<tr>\n<td>Model Monitoring</td>\n<td>Prediction logging</td>\n<td>Local buffering with delayed upload</td>\n<td>Temporary gap in real-time monitoring</td>\n</tr>\n</tbody></table>\n<h4 id=\"automated-recovery-procedures\">Automated Recovery Procedures</h4>\n<p>Automated recovery procedures handle common failure scenarios without human intervention, reducing mean time to recovery and operational burden. These procedures must be carefully designed to avoid making failures worse through inappropriate automated actions.</p>\n<p><strong>Recovery Procedure Categories</strong> organize automated responses by failure type and required intervention complexity. Immediate recovery procedures activate within seconds to handle transient issues. Scheduled recovery procedures run periodically to address accumulated issues. Escalation procedures engage human operators when automated recovery fails.</p>\n<p><strong>Database Recovery Procedures</strong> handle common database connectivity and performance issues that affect the metadata storage layer. These procedures include connection pool reset, query optimization, and failover coordination.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Database Connection Recovery Procedure:\n1. Detect connection failure through health check or operation timeout\n2. Verify network connectivity to database host using ping and port checks\n3. Attempt connection pool refresh with exponential backoff\n4. If pool refresh fails, check for connection limit exhaustion\n5. Implement circuit breaker to prevent further connection attempts\n6. Switch to read-only replica if available for degraded service\n7. Alert operations team if primary database remains unavailable\n8. Monitor recovery and gradually increase connection attempts</code></pre></div>\n\n<p><strong>Storage Recovery Procedures</strong> address artifact storage failures that can prevent model versioning and experiment artifact management. Recovery includes retry logic, alternative storage backends, and cleanup procedures.</p>\n<p>Storage failures often resolve automatically through retry with exponential backoff, particularly for network-related timeouts. However, quota exhaustion requires active cleanup of old artifacts based on retention policies. The recovery system maintains multiple storage backends when possible, automatically failing over to secondary storage during outages.</p>\n<p><strong>Resource Allocation Recovery</strong> handles training pipeline failures related to insufficient compute resources, node failures, and scheduling conflicts. These procedures coordinate with cluster management systems to restore service capability.</p>\n<table>\n<thead>\n<tr>\n<th>Recovery Scenario</th>\n<th>Detection Signal</th>\n<th>Automated Actions</th>\n<th>Escalation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Node failure with running jobs</td>\n<td>Kubernetes node NotReady event</td>\n<td>Reschedule affected jobs to available nodes</td>\n<td>Job rescheduling fails repeatedly</td>\n</tr>\n<tr>\n<td>GPU resource exhaustion</td>\n<td>Job pending with unschedulable reason</td>\n<td>Trigger cluster auto-scaling, queue jobs</td>\n<td>Auto-scaling limit reached</td>\n</tr>\n<tr>\n<td>Persistent volume failures</td>\n<td>Pod stuck in ContainerCreating</td>\n<td>Attempt PV repair, schedule on different node</td>\n<td>PV remains unrecoverable</td>\n</tr>\n<tr>\n<td>Container image pull failures</td>\n<td>Pod ImagePullBackOff status</td>\n<td>Clear image cache, retry pull from backup registry</td>\n<td>Image not available in any registry</td>\n</tr>\n</tbody></table>\n<h4 id=\"event-driven-coordination\">Event-Driven Coordination</h4>\n<p>Event-driven coordination enables components to respond to failures and recovery actions throughout the platform without tight coupling. Components publish events about their state changes and subscribe to events that require their attention.</p>\n<p><strong>Mental Model: Hospital Emergency Communication System</strong>\nThink of event-driven coordination like a hospital&#39;s emergency communication system. When a patient codes, the alert goes to all relevant departments simultaneously. The cardiac team responds immediately, pharmacy prepares emergency medications, and the lab prioritizes stat tests. Each department knows their role and acts based on the alert type without requiring central coordination.</p>\n<p><strong>Event Types for Error Handling</strong> define the categories of failure and recovery events that components must publish and handle. These events carry sufficient context for subscribers to determine appropriate responses.</p>\n<table>\n<thead>\n<tr>\n<th>Event Type</th>\n<th>Publishing Component</th>\n<th>Event Payload</th>\n<th>Typical Subscribers</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>COMPONENT_HEALTH_DEGRADED</code></td>\n<td>Any component health check</td>\n<td>Component name, health status, error details</td>\n<td>Monitoring dashboard, alert manager</td>\n</tr>\n<tr>\n<td><code>STORAGE_QUOTA_WARNING</code></td>\n<td>Experiment tracking, model registry</td>\n<td>Storage backend, usage percentage, projection</td>\n<td>Cleanup services, capacity planning</td>\n</tr>\n<tr>\n<td><code>DEPLOYMENT_FAILED</code></td>\n<td>Model deployment</td>\n<td>Model name, version, error details, rollback needed</td>\n<td>Model registry, monitoring, alerting</td>\n</tr>\n<tr>\n<td><code>PIPELINE_STEP_RETRY_EXHAUSTED</code></td>\n<td>Training pipeline orchestrator</td>\n<td>Pipeline ID, step name, error summary</td>\n<td>Pipeline monitoring, error analysis</td>\n</tr>\n<tr>\n<td><code>DRIFT_ALERT_CRITICAL</code></td>\n<td>Model monitoring</td>\n<td>Model name, drift metric, severity level</td>\n<td>Model registry, deployment service</td>\n</tr>\n</tbody></table>\n<p><strong>Event Ordering and Consistency</strong> ensures that components process related events in the correct sequence and maintain consistent state despite asynchronous delivery. Critical events use causal ordering to prevent race conditions between related state changes.</p>\n<p>Event processing implements idempotent handlers that produce the same result regardless of how many times they execute. This prevents duplicate processing when events are redelivered due to network issues or processing failures. Each event includes a correlation ID that links related events and enables end-to-end tracing of failure and recovery workflows.</p>\n<p><strong>Recovery Workflow Coordination</strong> orchestrates complex recovery procedures that require coordination between multiple components. For example, rolling back a failed model deployment involves the deployment service, model registry, monitoring system, and traffic routing components.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Model Deployment Rollback Coordination:\n1. Deployment service publishes DEPLOYMENT_FAILED event with rollback request\n2. Model registry subscribes to event and prepares previous version metadata\n3. Traffic routing service receives event and prepares traffic shifting plan\n4. Monitoring service pauses drift detection during rollback window\n5. Deployment service coordinates rollback execution across subscribers\n6. Each component publishes completion events for overall workflow tracking\n7. Final DEPLOYMENT_ROLLBACK_COMPLETE event signals successful recovery</code></pre></div>\n\n<h3 id=\"data-consistency-guarantees\">Data Consistency Guarantees</h3>\n<p>Maintaining data consistency across distributed MLOps components requires careful transaction design, conflict resolution strategies, and consistency level management. Unlike traditional applications with single-database transactions, MLOps platforms must coordinate state across metadata stores, artifact storage, container registries, and external services.</p>\n<p><strong>Mental Model: Bank Transaction Processing</strong>\nThink of MLOps data consistency like bank transaction processing. When you transfer money between accounts, the system must ensure both accounts are updated or neither is changed - you can&#39;t have money disappear or appear from nowhere. Similarly, when registering a model version, the metadata and artifacts must remain synchronized, even if storage systems experience failures during the process.</p>\n<h4 id=\"transaction-boundaries-and-acid-properties\">Transaction Boundaries and ACID Properties</h4>\n<p>Transaction boundaries define the scope of operations that must complete atomically to maintain platform consistency. Each component establishes transaction boundaries around operations that modify multiple related pieces of state.</p>\n<p><strong>Experiment Tracking Transactions</strong> encompass parameter logging, metric recording, and artifact upload operations that belong to a single experiment run. These transactions ensure that experiment state remains consistent even during concurrent updates from distributed training jobs.</p>\n<table>\n<thead>\n<tr>\n<th>Transaction Scope</th>\n<th>ACID Property Implementation</th>\n<th>Consistency Invariant</th>\n<th>Failure Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single run parameter batch</td>\n<td>Atomicity through database transaction</td>\n<td>All parameters logged or none</td>\n<td>Rollback on any parameter validation failure</td>\n</tr>\n<tr>\n<td>Metric time series update</td>\n<td>Consistency through monotonic timestamps</td>\n<td>Metrics never decrease in step number</td>\n<td>Reject out-of-order metric updates</td>\n</tr>\n<tr>\n<td>Artifact upload with metadata</td>\n<td>Isolation through file staging</td>\n<td>Metadata references only uploaded artifacts</td>\n<td>Clean up staged files on metadata failure</td>\n</tr>\n<tr>\n<td>Run completion marking</td>\n<td>Durability through WAL flushing</td>\n<td>Run status reflects actual completion</td>\n<td>Mark failed if artifacts missing</td>\n</tr>\n</tbody></table>\n<p><strong>Model Registry Transactions</strong> coordinate model version registration with artifact storage and lineage tracking. These transactions implement the immutability guarantees that production deployments depend on.</p>\n<p>Model registration transactions use a two-phase approach: first validate and stage all artifacts, then atomically update registry metadata. If artifact validation fails during staging, the transaction aborts without creating registry entries. If metadata updates fail after successful staging, the system retries the metadata operation using staged artifacts.</p>\n<p><strong>Cross-Component Transactions</strong> handle operations that span multiple platform components, such as promoting a model from experiment tracking through registry to deployment. These transactions use event sourcing and compensation patterns since traditional ACID transactions cannot span independent services.</p>\n<table>\n<thead>\n<tr>\n<th>Cross-Component Operation</th>\n<th>Transaction Pattern</th>\n<th>Consistency Mechanism</th>\n<th>Compensation Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model promotion from experiment to registry</td>\n<td>Saga pattern with events</td>\n<td>Event log ordering guarantees</td>\n<td>Reverse compensation events</td>\n</tr>\n<tr>\n<td>Pipeline completion with model registration</td>\n<td>Two-phase commit across services</td>\n<td>Coordinator service with participant votes</td>\n<td>Automated retry with timeout</td>\n</tr>\n<tr>\n<td>Deployment rollback with monitoring pause</td>\n<td>Event-driven coordination</td>\n<td>Causal event ordering</td>\n<td>Forward compensation to final state</td>\n</tr>\n</tbody></table>\n<h4 id=\"conflict-resolution-strategies\">Conflict Resolution Strategies</h4>\n<p>Conflict resolution handles situations where concurrent operations attempt to modify the same resources in incompatible ways. MLOps platforms face unique conflicts around model versioning, experiment naming, and resource allocation.</p>\n<p><strong>Model Version Conflicts</strong> occur when multiple processes attempt to register the same model version simultaneously, or when stage transitions conflict with ongoing operations. The registry implements optimistic concurrency control with version vectors to detect and resolve these conflicts.</p>\n<p>Version conflicts use a deterministic resolution strategy based on timestamps and content hashes. When two processes register the same model version with different artifacts, the system compares creation timestamps and artifact checksums. If the artifacts are identical (same checksum), the later registration succeeds but references the existing artifact. If artifacts differ, the registration fails with a clear error message requiring manual resolution.</p>\n<p><strong>Experiment Naming Conflicts</strong> arise when researchers create experiments with duplicate names or when automated systems generate conflicting experiment identifiers. The experiment tracking system resolves these conflicts through hierarchical namespacing and automatic disambiguation.</p>\n<table>\n<thead>\n<tr>\n<th>Conflict Type</th>\n<th>Detection Method</th>\n<th>Resolution Strategy</th>\n<th>User Experience</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Duplicate experiment name</td>\n<td>Unique constraint violation</td>\n<td>Append timestamp suffix</td>\n<td>Experiment created as &quot;model-tuning-2023-10-15-143022&quot;</td>\n</tr>\n<tr>\n<td>Concurrent run creation</td>\n<td>Run ID collision</td>\n<td>Regenerate ID with retry</td>\n<td>Transparent to user, automatic retry</td>\n</tr>\n<tr>\n<td>Parameter key conflicts within run</td>\n<td>Duplicate key insertion</td>\n<td>Last write wins with warning</td>\n<td>Parameter overwritten, warning logged</td>\n</tr>\n<tr>\n<td>Artifact path conflicts</td>\n<td>Path already exists check</td>\n<td>Generate unique suffix</td>\n<td>Artifact stored with disambiguation suffix</td>\n</tr>\n</tbody></table>\n<p><strong>Resource Allocation Conflicts</strong> happen when multiple training pipelines compete for limited cluster resources, or when deployment scaling conflicts with resource quotas. The platform implements fair scheduling and resource reservation to minimize conflicts.</p>\n<p>Resource conflicts use a combination of preemption and queuing strategies. High-priority jobs can preempt lower-priority jobs with sufficient notice for checkpoint saving. Jobs that cannot be scheduled immediately enter a priority queue with estimated wait times. The scheduler periodically rebalances allocations to ensure fair resource distribution across users and teams.</p>\n<h4 id=\"eventual-consistency-and-convergence\">Eventual Consistency and Convergence</h4>\n<p>Some MLOps operations can tolerate eventual consistency in exchange for higher availability and performance. The platform implements eventual consistency for operations where immediate consistency is not critical for correctness.</p>\n<p><strong>Mental Model: News Distribution Network</strong>\nThink of eventual consistency like news distribution in a global network. A breaking news story published in New York doesn&#39;t instantly appear in Tokyo newspapers, but the information eventually propagates everywhere. Readers might see slightly different versions temporarily, but the final story converges to the same content once distribution completes.</p>\n<p><strong>Metrics Aggregation Consistency</strong> allows experiment metrics to propagate through caching layers and materialized views with eventual convergence. Real-time dashboards might show slightly stale data during high write loads, but views eventually converge to consistent state.</p>\n<p>Metrics aggregation implements conflict-free replicated data types (CRDTs) for operations like counting experiment runs and computing performance statistics. These data types guarantee convergence without requiring coordination, enabling high write throughput during intensive training periods.</p>\n<p><strong>Artifact Replication Consistency</strong> manages the propagation of model artifacts across multiple storage regions and caching layers. Downloads might occasionally receive stale versions during propagation, but checksums ensure detection of inconsistencies.</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Level</th>\n<th>Use Case</th>\n<th>Convergence Time</th>\n<th>Detection Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Strong consistency</td>\n<td>Model version registration</td>\n<td>Immediate</td>\n<td>Synchronous validation</td>\n</tr>\n<tr>\n<td>Sequential consistency</td>\n<td>Experiment run ordering</td>\n<td>&lt; 1 second</td>\n<td>Vector clocks</td>\n</tr>\n<tr>\n<td>Eventual consistency</td>\n<td>Dashboard metrics</td>\n<td>&lt; 30 seconds</td>\n<td>Background reconciliation</td>\n</tr>\n<tr>\n<td>Weak consistency</td>\n<td>Usage statistics</td>\n<td>&lt; 5 minutes</td>\n<td>Periodic aggregation</td>\n</tr>\n</tbody></table>\n<p><strong>Monitoring Data Consistency</strong> handles the high-volume prediction logging and drift detection data that can tolerate some inconsistency for performance. The monitoring system implements lambda architecture with real-time and batch processing layers that eventually converge.</p>\n<p>Real-time monitoring provides approximate metrics with low latency for immediate alerting. Batch processing computes authoritative metrics periodically and corrects any inconsistencies detected in the real-time layer. This approach enables responsive alerting while maintaining data accuracy for compliance and auditing requirements.</p>\n<p><img src=\"/api/project/mlops-platform/architecture-doc/asset?path=diagrams%2Ferror-handling-flows.svg\" alt=\"Failure Recovery Flows\"></p>\n<h3 id=\"common-recovery-scenarios\">Common Recovery Scenarios</h3>\n<p>Understanding how the platform handles common failure scenarios helps operators troubleshoot issues and validates the robustness of recovery procedures. Each scenario includes the failure sequence, detection methods, automated recovery actions, and manual intervention requirements.</p>\n<h4 id=\"training-pipeline-catastrophic-failure\">Training Pipeline Catastrophic Failure</h4>\n<p>A catastrophic training pipeline failure occurs when the orchestration engine crashes during active job execution, potentially leaving running containers without supervision and consuming resources without progress tracking.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Failure Sequence:\n1. Training pipeline orchestrator pod crashes due to memory pressure\n2. Kubernetes reschedules orchestrator to new node with state loss\n3. Previously running training jobs continue executing but become &quot;orphaned&quot;\n4. New job submissions fail due to missing orchestrator state\n5. Resource quotas fill up with untracked jobs preventing new work\n6. Monitoring alerts fire due to job submission failures</code></pre></div>\n\n<p><strong>Detection and Recovery Process:</strong>\nThe platform detects this failure through health check timeouts and job submission error rates. Automated recovery includes state reconciliation, orphaned job cleanup, and orchestrator restart with recovered state.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Recovery Procedure:\n1. Detect orchestrator failure through health check timeout\n2. Query Kubernetes API for all running jobs matching orchestrator labels\n3. Cross-reference running jobs against expected pipeline executions\n4. For orphaned jobs: attempt graceful termination with artifact preservation\n5. Rebuild orchestrator state from persisted pipeline definitions and job history\n6. Resume monitoring of recovered jobs and accept new job submissions\n7. Send notification summarizing recovery actions and any data loss</code></pre></div>\n\n<p><strong>Manual Intervention Requirements:</strong>\nOperators must validate that recovered state correctly reflects actual cluster state and manually resolve any pipelines that cannot be automatically recovered. Long-running training jobs may need manual checkpoint restoration if automatic state recovery fails.</p>\n<h4 id=\"model-deployment-rollback-cascade\">Model Deployment Rollback Cascade</h4>\n<p>A deployment rollback cascade occurs when rolling back a failed model deployment triggers failures in the previous version, creating a situation where no stable model version is available for production traffic.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Failure Sequence:\n1. New model version deployed successfully but produces incorrect predictions\n2. Automated monitoring detects accuracy degradation and triggers rollback\n3. Rollback to previous version fails due to artifact corruption\n4. Traffic routing attempts to find stable version but all recent versions problematic\n5. Model serving endpoints become unavailable causing customer impact\n6. Manual intervention required to deploy known-good version from older backup</code></pre></div>\n\n<p>This scenario requires sophisticated rollback strategies that maintain multiple stable versions and validate rollback targets before traffic switching. The deployment system must implement health validation for rollback targets and maintain emergency deployment procedures for crisis scenarios.</p>\n<h4 id=\"data-corruption-during-experiment-migration\">Data Corruption During Experiment Migration</h4>\n<p>Data corruption during experiment migration represents a complex scenario where database schema changes or data migration scripts corrupt historical experiment data, affecting research reproducibility.</p>\n<p><strong>Corruption Detection:</strong>\nThe system detects corruption through periodic consistency checks that validate foreign key relationships, timestamp ordering, and artifact checksums. Corruption manifests as missing experiment runs, unreachable artifacts, or inconsistent metric time series.</p>\n<p><strong>Recovery Strategy:</strong>\nRecovery requires restoring from validated backups while preserving recent uncorrupted data. The process involves identifying the corruption scope, isolating affected data, and merging clean historical data with recent additions.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Migration Recovery Process:\n1. Identify corruption scope through consistency validation queries\n2. Stop all write operations to prevent further corruption spread\n3. Restore database from last known-good backup to isolated environment\n4. Extract uncorrupted recent data from production database\n5. Merge clean historical data with validated recent data\n6. Perform full consistency validation on merged dataset\n7. Replace production database with merged data after validation\n8. Resume operations with enhanced validation during recovery period</code></pre></div>\n\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Building robust error handling requires implementing the health check framework, circuit breaker patterns, and recovery procedures described above. The following guidance provides concrete implementation strategies for each component.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Health Checks</td>\n<td>HTTP endpoints with JSON responses</td>\n<td>Kubernetes liveness/readiness probes</td>\n</tr>\n<tr>\n<td>Circuit Breakers</td>\n<td>Simple threshold-based implementation</td>\n<td>Netflix Hystrix or similar library</td>\n</tr>\n<tr>\n<td>Event Coordination</td>\n<td>Redis pub/sub with message queues</td>\n<td>Apache Kafka with event sourcing</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Prometheus metrics with Grafana dashboards</td>\n<td>Full observability stack with distributed tracing</td>\n</tr>\n<tr>\n<td>Recovery Automation</td>\n<td>Shell scripts with cron scheduling</td>\n<td>Kubernetes operators with custom resource definitions</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure\">File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>platform/\n├── internal/\n│   ├── health/\n│   │   ├── checker.go              # ComponentHealth implementation\n│   │   ├── checks.go               # Standard health check functions\n│   │   └── registry.go             # Health check registration\n│   ├── circuit/\n│   │   ├── breaker.go              # CircuitBreaker implementation\n│   │   ├── config.go               # Configuration structures\n│   │   └── metrics.go              # Circuit breaker metrics\n│   ├── events/\n│   │   ├── coordinator.go          # EventCoordinator implementation\n│   │   ├── handlers.go             # Event handler utilities\n│   │   └── storage.go              # Event persistence layer\n│   ├── recovery/\n│   │   ├── procedures.go           # Automated recovery procedures\n│   │   ├── detection.go            # Failure detection algorithms\n│   │   └── coordination.go         # Multi-component recovery coordination\n│   └── consistency/\n│       ├── transactions.go         # Cross-component transaction patterns\n│       ├── conflicts.go            # Conflict resolution strategies\n│       └── convergence.go          # Eventual consistency mechanisms\n└── pkg/\n    ├── errors/\n    │   ├── types.go                # Error type definitions\n    │   ├── classification.go       # Error classification utilities\n    │   └── context.go              # Error context enrichment\n    └── monitoring/\n        ├── alerts.go               # Alert management\n        └── dashboards.go           # Health dashboard utilities</code></pre></div>\n\n<h4 id=\"health-check-infrastructure\">Health Check Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Callable, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages health checks for a platform component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.component_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable[[], HealthCheck]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.check_intervals: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_results: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, HealthCheck] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_check</span><span style=\"color:#E1E4E8\">(self, check_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: Callable[[], HealthCheck], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  interval_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a periodic health check function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store check function in self.checks dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store check interval in self.check_intervals</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize last_results entry with UNKNOWN status</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_checks</span><span style=\"color:#E1E4E8\">(self) -> List[HealthCheck]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all health checks and return results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Iterate through all registered checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each check, call the check function safely with exception handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update last_results with new results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Append result to results list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return complete results list</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_overall_status</span><span style=\"color:#E1E4E8\">(self) -> HealthStatus:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compute overall component health from individual checks.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If no checks registered, return UNKNOWN</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If any check is UNHEALTHY, return UNHEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If any check is DEGRADED, return DEGRADED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If all checks are HEALTHY, return HEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Otherwise return UNKNOWN</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Example health check implementations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> database_connectivity_check</span><span style=\"color:#E1E4E8\">() -> HealthCheck:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check database connectivity and response time.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Attempt simple database query with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Measure query execution time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return HEALTHY if query succeeds within threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return DEGRADED if query slow but successful</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return UNHEALTHY if query fails or times out</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> artifact_storage_check</span><span style=\"color:#E1E4E8\">() -> HealthCheck:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Check artifact storage availability through write/read cycle.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate test artifact with unique key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Attempt to upload test artifact</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Attempt to download and verify test artifact</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Clean up test artifact</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return appropriate status based on operation success</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"circuit-breaker-implementation\">Circuit Breaker Implementation</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Any, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreakerState</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CLOSED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"closed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"open\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HALF_OPEN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"half_open\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreakerConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failure_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 60.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    success_threshold: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    call_timeout: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CircuitBreaker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Circuit breaker preventing cascade failures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: CircuitBreakerConfig):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.config </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.state </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CircuitBreakerState.</span><span style=\"color:#79B8FF\">CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.failure_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.success_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_failure_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> can_execute</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if request should be allowed through circuit breaker.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: If state is CLOSED, return True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If state is OPEN, check if timeout period has elapsed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If timeout elapsed, transition to HALF_OPEN and return True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If state is HALF_OPEN, return True (allow test requests)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Otherwise return False</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_success</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record successful operation result.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Reset failure_count to 0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If state is HALF_OPEN, increment success_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If success_count >= success_threshold, transition to CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If state is CLOSED, ensure it remains CLOSED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_failure</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record failed operation result.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.lock:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Increment failure_count</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Record current timestamp as last_failure_time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If failure_count >= failure_threshold, transition to OPEN</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If state is HALF_OPEN, transition back to OPEN and reset success_count</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> circuit_breaker_wrapper</span><span style=\"color:#E1E4E8\">(breaker: CircuitBreaker, func: Callable, </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\">args, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">kwargs):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Wrapper function that applies circuit breaker to function calls.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if breaker.can_execute() returns True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If not, raise CircuitBreakerOpenError immediately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Try executing func(*args, **kwargs) with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If successful, call breaker.record_success() and return result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: If failed, call breaker.record_failure() and re-raise exception</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"event-driven-coordination-system\">Event-Driven Coordination System</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional, Callable, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Event</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    id</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    type</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    source: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlation_id: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    version: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"1.0\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    metadata: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create</span><span style=\"color:#E1E4E8\">(cls, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, payload: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#9ECBFF\">'Event'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate unique event ID using uuid.uuid4()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set timestamp to current time using time.time()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create and return Event instance with provided parameters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventHandler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Wrapper for event handler functions with retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, handler_func: Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 max_retries: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#E1E4E8\">, retry_delay: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.handler_func </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> handler_func</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_retries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_retries</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.retry_delay </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> retry_delay</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> handle_event</span><span style=\"color:#E1E4E8\">(self, event: Event) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute event handler with retry logic.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Attempt to call handler_func(event) with try/except</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If successful, return True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If exception occurs, implement exponential backoff retry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Log retry attempts and final success/failure</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return False if all retries exhausted</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> EventCoordinator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Central event coordination system.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.subscribers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[EventHandler]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.event_storage: Optional[EventStorage] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> subscribe</span><span style=\"color:#E1E4E8\">(self, event_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, handler: Callable[[Event], </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register event handler for specific event type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create EventHandler wrapper around handler function</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add handler to subscribers[event_type] list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate and return subscription ID for later unsubscription</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> publish</span><span style=\"color:#E1E4E8\">(self, event: Event, synchronous: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Publish event to subscribers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store event in event_storage if configured</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get list of subscribers for event.type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If synchronous=True, await all handler executions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: If synchronous=False, schedule handlers as background tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return True if all handlers succeeded (synchronous) or scheduled (async)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Example event handlers for MLOps platform</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> handle_deployment_failed</span><span style=\"color:#E1E4E8\">(event: Event):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handle deployment failure by triggering rollback.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract model name and version from event payload</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Look up previous stable version from model registry</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initiate rollback deployment to previous version</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update deployment status and send notifications</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> handle_drift_alert</span><span style=\"color:#E1E4E8\">(event: Event):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Handle drift detection alert by triggering retraining.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract drift metrics and model information from event</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Evaluate drift severity against configured thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: If severe drift, trigger retraining pipeline</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update model status and alert appropriate teams</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"recovery-procedure-framework\">Recovery Procedure Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RecoveryResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUCCESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"success\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PARTIAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"partial\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MANUAL_REQUIRED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"manual_required\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RecoveryProcedure</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    detection_criteria: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_attempts: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 3</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timeout_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AutomatedRecovery</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Framework for automated failure recovery procedures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.procedures: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Callable] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_history: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_procedure</span><span style=\"color:#E1E4E8\">(self, failure_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, procedure: Callable):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register recovery procedure for specific failure type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Store procedure function in procedures dictionary</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate procedure signature matches expected interface</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Log registration of new recovery procedure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> attempt_recovery</span><span style=\"color:#E1E4E8\">(self, failure_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> RecoveryResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Attempt automated recovery for detected failure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Look up registered procedure for failure_type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If no procedure found, return MANUAL_REQUIRED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute procedure with retry logic and timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Record recovery attempt in recovery_history</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return appropriate RecoveryResult based on procedure outcome</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Example recovery procedures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> recover_database_connection</span><span style=\"color:#E1E4E8\">(context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> RecoveryResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Recover from database connectivity issues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test current database connectivity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: If connection works, return SUCCESS</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Attempt connection pool reset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Try connecting to read replica if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return appropriate result based on recovery success</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> recover_storage_quota_exhaustion</span><span style=\"color:#E1E4E8\">(context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> RecoveryResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Recover from storage quota exhaustion.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate current storage usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Identify oldest artifacts eligible for cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Execute retention policy cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify sufficient space available after cleanup</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return SUCCESS if space recovered, MANUAL_REQUIRED if not</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> recover_orphaned_training_jobs</span><span style=\"color:#E1E4E8\">(context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> RecoveryResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Recover from orchestrator failure with orphaned jobs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Query Kubernetes for all jobs with orchestrator labels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Cross-reference with expected pipeline executions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify truly orphaned jobs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Attempt graceful termination with checkpoint preservation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Rebuild orchestrator state from remaining valid jobs</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p>After implementing the error handling framework, verify the following behavior:</p>\n<p><strong>Health Check Validation:</strong></p>\n<ul>\n<li>Run <code>python -m platform.health.checker</code> to execute all component health checks</li>\n<li>Verify that healthy components return status &quot;healthy&quot; with appropriate details</li>\n<li>Simulate database disconnection and confirm health checks detect degraded state</li>\n<li>Check that health check results are properly cached and timestamped</li>\n</ul>\n<p><strong>Circuit Breaker Testing:</strong></p>\n<ul>\n<li>Implement a test service that fails after N requests</li>\n<li>Verify circuit breaker opens after threshold failures reached</li>\n<li>Confirm circuit breaker prevents further requests when open</li>\n<li>Test that circuit breaker transitions to half-open after timeout</li>\n<li>Validate successful requests close circuit breaker from half-open state</li>\n</ul>\n<p><strong>Event Coordination Verification:</strong></p>\n<ul>\n<li>Publish test events and verify subscribers receive them</li>\n<li>Test both synchronous and asynchronous event delivery</li>\n<li>Simulate handler failures and confirm retry logic works</li>\n<li>Verify event ordering is preserved for related events</li>\n</ul>\n<p><strong>Recovery Procedure Testing:</strong></p>\n<ul>\n<li>Trigger controlled failures (disconnect database, fill storage)</li>\n<li>Verify automated recovery procedures detect and respond appropriately</li>\n<li>Test that recovery procedures respect timeout and retry limits</li>\n<li>Confirm manual escalation occurs when automated recovery fails</li>\n</ul>\n<h4 id=\"debugging-guide\">Debugging Guide</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Steps</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Health checks always return UNKNOWN</td>\n<td>Health check functions not registered</td>\n<td>Check ComponentHealth.add_check() calls</td>\n<td>Register health checks in component initialization</td>\n</tr>\n<tr>\n<td>Circuit breaker stuck in OPEN state</td>\n<td>Underlying service not recovered or timeout too short</td>\n<td>Check service health and circuit breaker timeout configuration</td>\n<td>Fix underlying service or increase timeout</td>\n</tr>\n<tr>\n<td>Events not reaching subscribers</td>\n<td>Event type mismatch or subscriber registration failure</td>\n<td>Check event.type matches subscription and verify subscriber list</td>\n<td>Ensure exact string match and re-register subscribers</td>\n</tr>\n<tr>\n<td>Recovery procedures not triggering</td>\n<td>Detection criteria not matching actual failures</td>\n<td>Review failure detection logic and criteria</td>\n<td>Update detection criteria or add missing failure patterns</td>\n</tr>\n<tr>\n<td>Inconsistent data after recovery</td>\n<td>Recovery procedures not atomic or concurrent modifications</td>\n<td>Check transaction boundaries and locking</td>\n<td>Implement proper transaction isolation</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-5) by providing comprehensive testing approaches that validate correct implementation and integration across experiment tracking, model registry, training pipeline orchestration, model deployment, and model monitoring components.</p>\n</blockquote>\n<p>Testing an MLOps platform requires a multi-layered approach that validates both individual component correctness and end-to-end workflow integration. Think of testing like a <strong>quality assurance factory</strong> - you need inspection checkpoints at every stage of the assembly line (unit tests), integration testing where components connect (API tests), and final quality validation of the complete product (end-to-end scenarios). Each milestone introduces new complexity layers that must be thoroughly validated before proceeding to the next phase.</p>\n<p>The testing strategy addresses three critical dimensions: <strong>functional correctness</strong> (does each component do what it&#39;s supposed to do), <strong>integration reliability</strong> (do components work together correctly), and <strong>operational resilience</strong> (does the system handle failures gracefully). Unlike traditional web applications, MLOps platforms must also validate data science workflows, model artifacts, distributed training coordination, and production monitoring - each with unique testing challenges.</p>\n<h3 id=\"component-testing\">Component Testing</h3>\n<p>Component testing forms the foundation of our testing pyramid, validating individual component logic and their interactions through well-defined interfaces. Think of this like <strong>testing individual instruments in an orchestra</strong> before bringing them together for a full symphony performance. Each component must prove it can handle its responsibilities correctly in isolation before we test how they harmonize together.</p>\n<h4 id=\"unit-testing-strategy\">Unit Testing Strategy</h4>\n<p>Unit tests focus on the core business logic within each component, testing pure functions and isolated behaviors without external dependencies. These tests should run quickly (under 100ms each) and provide immediate feedback during development.</p>\n<p><strong>Experiment Tracking Unit Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Cases</th>\n<th>Key Assertions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parameter Logging</td>\n<td>Valid parameter types, nested parameters, parameter overwriting</td>\n<td>Parameter correctly stored with run correlation, type validation works, overwrites generate warnings</td>\n</tr>\n<tr>\n<td>Metric Validation</td>\n<td>Numeric metrics, step ordering, timestamp handling</td>\n<td>Metrics stored with correct precision, steps are monotonic, timestamps are realistic</td>\n</tr>\n<tr>\n<td>Artifact Management</td>\n<td>File upload, checksum validation, metadata attachment</td>\n<td>Checksums match uploaded content, metadata correctly associated, duplicate detection works</td>\n</tr>\n<tr>\n<td>Query Engine</td>\n<td>Filter parsing, sorting logic, pagination</td>\n<td>SQL-like filters compile correctly, sorting handles null values, pagination maintains consistency</td>\n</tr>\n<tr>\n<td>Run Comparison</td>\n<td>Statistical comparison, missing metrics handling</td>\n<td>Statistical tests use correct formulas, missing values handled gracefully, confidence intervals computed</td>\n</tr>\n</tbody></table>\n<p><strong>Model Registry Unit Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Cases</th>\n<th>Key Assertions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Version Management</td>\n<td>Semantic versioning, version conflicts, automatic incrementing</td>\n<td>Versions follow semantic rules, conflicts detected and resolved, auto-increment preserves ordering</td>\n</tr>\n<tr>\n<td>Stage Transitions</td>\n<td>Valid transitions, approval gates, rollback scenarios</td>\n<td>Only valid transitions allowed, approval metadata captured, rollbacks restore previous state</td>\n</tr>\n<tr>\n<td>Lineage Tracking</td>\n<td>Parent-child relationships, circular dependency detection, depth limits</td>\n<td>Lineage graphs are acyclic, circular references rejected, depth limits prevent infinite recursion</td>\n</tr>\n<tr>\n<td>Artifact Storage</td>\n<td>Content addressing, deduplication, integrity verification</td>\n<td>Identical artifacts share storage, checksums prevent corruption, retrieval validates integrity</td>\n</tr>\n<tr>\n<td>Metadata Validation</td>\n<td>Schema enforcement, tag normalization, search indexing</td>\n<td>Invalid metadata rejected, tags normalized consistently, search indexes updated correctly</td>\n</tr>\n</tbody></table>\n<p><strong>Pipeline Orchestration Unit Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Cases</th>\n<th>Key Assertions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DAG Validation</td>\n<td>Cycle detection, unreachable nodes, dependency resolution</td>\n<td>Cycles detected and rejected, all nodes reachable, dependencies form valid execution order</td>\n</tr>\n<tr>\n<td>Resource Allocation</td>\n<td>CPU/memory/GPU requests, constraint satisfaction, resource conflicts</td>\n<td>Resource requests validated, constraints satisfied, conflicts detected before execution</td>\n</tr>\n<tr>\n<td>Step Execution</td>\n<td>Input preparation, output processing, error handling</td>\n<td>Inputs correctly mapped, outputs properly captured, errors propagated with context</td>\n</tr>\n<tr>\n<td>Retry Logic</td>\n<td>Exponential backoff, maximum attempts, transient vs permanent failures</td>\n<td>Backoff calculations correct, attempt limits enforced, failure types classified correctly</td>\n</tr>\n<tr>\n<td>Data Flow</td>\n<td>Artifact passing, schema validation, data lineage</td>\n<td>Artifacts flow between steps correctly, schemas validated at boundaries, lineage tracked accurately</td>\n</tr>\n</tbody></table>\n<p><strong>Model Deployment Unit Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Cases</th>\n<th>Key Assertions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Traffic Management</td>\n<td>Weight calculation, routing logic, canary progression</td>\n<td>Traffic weights sum to 100%, routing respects weights, canary progression follows schedule</td>\n</tr>\n<tr>\n<td>Health Monitoring</td>\n<td>Endpoint validation, failure detection, recovery triggers</td>\n<td>Health checks validate correctly, failures trigger appropriate responses, recovery procedures execute</td>\n</tr>\n<tr>\n<td>Scaling Logic</td>\n<td>Replica calculation, resource limits, scaling policies</td>\n<td>Replica counts calculated correctly, resource limits respected, scaling policies enforced</td>\n</tr>\n<tr>\n<td>Rollback Mechanisms</td>\n<td>Trigger conditions, state preservation, recovery validation</td>\n<td>Rollbacks trigger on correct conditions, previous state restored, recovery validated before completion</td>\n</tr>\n<tr>\n<td>Model Loading</td>\n<td>Artifact download, model initialization, warming procedures</td>\n<td>Artifacts downloaded correctly, models initialize successfully, warming procedures complete</td>\n</tr>\n</tbody></table>\n<p><strong>Model Monitoring Unit Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Cases</th>\n<th>Key Assertions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Drift Detection</td>\n<td>Statistical test calculations, threshold evaluation, severity classification</td>\n<td>Statistical tests use correct formulas, thresholds evaluated properly, severity levels assigned correctly</td>\n</tr>\n<tr>\n<td>Prediction Logging</td>\n<td>Request capturing, response correlation, metadata extraction</td>\n<td>Requests captured completely, responses correlated correctly, metadata extracted accurately</td>\n</tr>\n<tr>\n<td>Metrics Aggregation</td>\n<td>Time window calculations, percentile computation, trend analysis</td>\n<td>Time windows aligned correctly, percentiles computed accurately, trends calculated properly</td>\n</tr>\n<tr>\n<td>Alert Generation</td>\n<td>Threshold evaluation, escalation logic, notification formatting</td>\n<td>Thresholds evaluated correctly, escalation follows policy, notifications formatted properly</td>\n</tr>\n<tr>\n<td>Performance Tracking</td>\n<td>Latency measurement, throughput calculation, resource utilization</td>\n<td>Latency measured accurately, throughput calculated correctly, resource utilization tracked</td>\n</tr>\n</tbody></table>\n<h4 id=\"integration-testing-approach\">Integration Testing Approach</h4>\n<p>Integration tests validate how components communicate through their APIs and event interfaces. These tests use real network communication but mock external dependencies like databases and cloud services.</p>\n<p><strong>API Integration Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component Pair</th>\n<th>Test Scenarios</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Tracking ↔ Model Registry</td>\n<td>Run completion triggers model registration, artifact linking, metadata transfer</td>\n<td>Model created with correct run reference, artifacts accessible from registry, metadata synchronized</td>\n</tr>\n<tr>\n<td>Model Registry ↔ Training Pipeline</td>\n<td>Model download for fine-tuning, version updates from training, lineage tracking</td>\n<td>Models downloaded successfully, versions updated with training results, lineage preserved through fine-tuning</td>\n</tr>\n<tr>\n<td>Training Pipeline ↔ Model Deployment</td>\n<td>Model artifact handoff, deployment triggering, resource coordination</td>\n<td>Artifacts transferred correctly, deployments triggered automatically, resources allocated without conflicts</td>\n</tr>\n<tr>\n<td>Model Deployment ↔ Model Monitoring</td>\n<td>Prediction logging setup, health status reporting, performance feedback</td>\n<td>Logging configured correctly, health status synchronized, performance metrics flow back to deployment</td>\n</tr>\n</tbody></table>\n<p><strong>Event-Driven Integration Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Event Flow</th>\n<th>Trigger</th>\n<th>Expected Propagation</th>\n<th>Validation Points</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>EXPERIMENT_COMPLETED</code></td>\n<td>Experiment finishes with acceptable metrics</td>\n<td>Model registration triggered, pipeline deployment initiated</td>\n<td>Model appears in registry, deployment starts automatically, monitoring configured</td>\n</tr>\n<tr>\n<td><code>MODEL_PROMOTED</code></td>\n<td>Model version promoted to production stage</td>\n<td>Deployment updated, monitoring activated, alerts configured</td>\n<td>Production deployment reflects new version, monitoring active, alert rules applied</td>\n</tr>\n<tr>\n<td><code>DEPLOYMENT_FAILED</code></td>\n<td>Model deployment encounters errors</td>\n<td>Rollback initiated, alerts fired, incident recorded</td>\n<td>Previous version restored, alerts sent to correct channels, incident logged with context</td>\n</tr>\n<tr>\n<td><code>DRIFT_ALERT_CRITICAL</code></td>\n<td>Critical drift detected in production model</td>\n<td>Deployment scaling paused, escalation triggered, investigation initiated</td>\n<td>Scaling policies updated, escalation follows defined workflow, investigation tools activated</td>\n</tr>\n</tbody></table>\n<p><strong>Data Consistency Integration Tests:</strong></p>\n<p>These tests verify that data remains consistent across component boundaries, especially during concurrent operations and failure scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Consistency Scenario</th>\n<th>Test Setup</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent Model Registration</td>\n<td>Multiple training runs register models simultaneously</td>\n<td>All models registered correctly, no version conflicts, lineage preserved</td>\n</tr>\n<tr>\n<td>Pipeline Interruption Recovery</td>\n<td>Training pipeline interrupted mid-execution</td>\n<td>Partial results preserved, restart from checkpoint, no data corruption</td>\n</tr>\n<tr>\n<td>Deployment Traffic Split Consistency</td>\n<td>Traffic weights updated during active requests</td>\n<td>Request routing remains consistent, no dropped requests, weights eventually consistent</td>\n</tr>\n<tr>\n<td>Monitoring Data Pipeline Integrity</td>\n<td>High-volume prediction logging with processing lag</td>\n<td>All predictions processed exactly once, metrics aggregated correctly, no data loss</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: Integration tests should focus on the <strong>contract boundaries</strong> between components rather than internal implementation details. Test what each component promises to deliver to its collaborators, not how it delivers it internally.</p>\n</blockquote>\n<h4 id=\"database-and-storage-testing\">Database and Storage Testing</h4>\n<p>Data persistence testing validates that each component correctly stores and retrieves data through its storage abstractions, ensuring data integrity across application restarts.</p>\n<p><strong>Metadata Storage Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Storage Operation</th>\n<th>Test Cases</th>\n<th>Integrity Checks</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Run Storage</td>\n<td>Parameter insertion, metric updates, concurrent writes</td>\n<td>Parameters stored with correct types, metrics maintain temporal ordering, concurrent writes don&#39;t corrupt data</td>\n</tr>\n<tr>\n<td>Model Version Storage</td>\n<td>Version creation, stage updates, lineage recording</td>\n<td>Versions are immutable after creation, stage transitions follow rules, lineage relationships preserved</td>\n</tr>\n<tr>\n<td>Pipeline Execution Storage</td>\n<td>Step status updates, resource tracking, failure recording</td>\n<td>Step statuses reflect actual execution, resource usage accurately recorded, failures captured with full context</td>\n</tr>\n<tr>\n<td>Deployment Configuration Storage</td>\n<td>Traffic split updates, health status changes, configuration versioning</td>\n<td>Traffic splits are atomic updates, health status changes timestamped correctly, configuration history preserved</td>\n</tr>\n</tbody></table>\n<p><strong>Artifact Storage Tests:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Artifact Operation</th>\n<th>Test Cases</th>\n<th>Validation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model Artifact Upload</td>\n<td>Large model files, concurrent uploads, checksum validation</td>\n<td>Files uploaded completely, concurrent uploads don&#39;t interfere, checksums prevent corruption</td>\n</tr>\n<tr>\n<td>Pipeline Artifact Passing</td>\n<td>Inter-step data transfer, schema validation, cleanup procedures</td>\n<td>Data transfers completely between steps, schemas validated at boundaries, temporary artifacts cleaned up</td>\n</tr>\n<tr>\n<td>Deployment Artifact Download</td>\n<td>Model loading, caching behavior, update detection</td>\n<td>Models download correctly for serving, caching reduces redundant downloads, updates detected reliably</td>\n</tr>\n<tr>\n<td>Monitoring Data Archival</td>\n<td>Log rotation, compression, retention policies</td>\n<td>Logs rotated without data loss, compression maintains data integrity, retention policies enforced correctly</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Testing Database Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Components need database testing but setting up full databases for every test is slow and complex</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>In-memory SQLite for all tests</li>\n<li>Docker containers with real databases</li>\n<li>Database mocking with interface validation</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Use in-memory SQLite for unit tests, Docker containers for integration tests</li>\n<li><strong>Rationale</strong>: SQLite provides real SQL semantics without setup overhead for fast unit tests, while Docker containers test against production database types for integration scenarios</li>\n<li><strong>Consequences</strong>: Unit tests run quickly in CI/CD, integration tests catch database-specific issues, but requires maintaining test data setup scripts</li>\n</ul>\n</blockquote>\n<h3 id=\"end-to-end-scenarios\">End-to-End Scenarios</h3>\n<p>End-to-end scenarios validate complete workflows from data ingestion through model deployment and monitoring, testing the platform as data scientists would actually use it. Think of these tests like <strong>full dress rehearsals</strong> before opening night - everything must work together seamlessly under realistic conditions.</p>\n<h4 id=\"complete-ml-workflow-scenarios\">Complete ML Workflow Scenarios</h4>\n<p><strong>Scenario 1: New Model Development and Deployment</strong></p>\n<p>This scenario tests the complete journey from initial experiment to production deployment, validating that all components work together to support a typical data science workflow.</p>\n<p><em>Setup Requirements:</em></p>\n<ul>\n<li>Sample training dataset (100MB CSV with realistic ML features)</li>\n<li>Training script that logs parameters, metrics, and artifacts</li>\n<li>Model serving container image with health check endpoint</li>\n<li>Monitoring configuration with drift detection rules</li>\n</ul>\n<p><em>Workflow Steps:</em></p>\n<ol>\n<li><p><strong>Experiment Tracking Phase</strong></p>\n<ul>\n<li>Data scientist starts new experiment with tagged dataset version</li>\n<li>Training script logs hyperparameters (learning_rate, batch_size, model_architecture)</li>\n<li>Training loop logs metrics every 100 steps (loss, accuracy, validation_score)</li>\n<li>Model artifacts (weights, config, visualization plots) uploaded on completion</li>\n<li>Experiment marked as completed with final model accuracy above threshold</li>\n</ul>\n</li>\n<li><p><strong>Model Registration Phase</strong></p>\n<ul>\n<li>Training run automatically triggers model registration</li>\n<li>Model version created with semantic version increment (1.2.3 → 1.3.0)</li>\n<li>Lineage links model to training run, dataset version, and code commit</li>\n<li>Model stage initialized to Development with registration metadata</li>\n<li>Artifact integrity validated through checksum verification</li>\n</ul>\n</li>\n<li><p><strong>Pipeline Orchestration Phase</strong></p>\n<ul>\n<li>Model validation pipeline triggered for registered model</li>\n<li>Validation steps include: accuracy testing, bias analysis, performance benchmarking</li>\n<li>Pipeline executes on dedicated validation cluster with GPU resources</li>\n<li>Validation results stored as model annotations and pipeline artifacts</li>\n<li>Successful validation automatically promotes model to Staging stage</li>\n</ul>\n</li>\n<li><p><strong>Deployment Phase</strong></p>\n<ul>\n<li>Staging deployment created with single replica for internal testing</li>\n<li>Model loaded into inference server with health check validation</li>\n<li>Internal testing generates sample predictions with acceptable latency</li>\n<li>A/B testing configuration prepared for production canary deployment</li>\n<li>Production deployment initiated with 5% traffic allocation</li>\n</ul>\n</li>\n<li><p><strong>Monitoring Phase</strong></p>\n<ul>\n<li>Prediction logging activated for both staging and production endpoints</li>\n<li>Baseline feature distributions captured from training data</li>\n<li>Real-time monitoring dashboard configured with key metrics</li>\n<li>Drift detection rules configured with warning and critical thresholds</li>\n<li>Performance alerts configured for latency, error rate, and throughput</li>\n</ul>\n</li>\n</ol>\n<p><em>Expected Outcomes:</em></p>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Success Criteria</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Tracking</td>\n<td>All parameters/metrics logged correctly, artifacts downloadable</td>\n<td>Query API returns complete run data, artifacts download without corruption</td>\n</tr>\n<tr>\n<td>Model Registration</td>\n<td>Model registered with correct version and lineage</td>\n<td>Registry API returns model with complete metadata and valid artifact links</td>\n</tr>\n<tr>\n<td>Pipeline Orchestration</td>\n<td>Validation pipeline completes successfully</td>\n<td>Pipeline status shows all steps succeeded, validation metrics meet thresholds</td>\n</tr>\n<tr>\n<td>Deployment</td>\n<td>Model serving responds correctly to test requests</td>\n<td>Health checks pass, test predictions return expected format with acceptable latency</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Monitoring captures predictions and computes metrics</td>\n<td>Dashboard shows real-time metrics, drift detection processes sample data correctly</td>\n</tr>\n</tbody></table>\n<p><strong>Scenario 2: Model Update with Canary Deployment</strong></p>\n<p>This scenario tests updating an existing production model through a controlled canary deployment process, validating traffic management and rollback capabilities.</p>\n<p><em>Workflow Steps:</em></p>\n<ol>\n<li><p><strong>Model Improvement Cycle</strong></p>\n<ul>\n<li>Existing production model (v1.3.0) serves 100% of traffic</li>\n<li>New training experiment produces improved model (v1.4.0) with better accuracy</li>\n<li>Model registry shows clear lineage from previous version</li>\n<li>Staging deployment validates new model performance</li>\n</ul>\n</li>\n<li><p><strong>Canary Deployment Initiation</strong></p>\n<ul>\n<li>Canary deployment configured: 95% traffic to v1.3.0, 5% to v1.4.0</li>\n<li>Traffic routing rules deployed to load balancer</li>\n<li>Both model versions receive real production traffic</li>\n<li>Monitoring tracks per-version metrics separately</li>\n</ul>\n</li>\n<li><p><strong>Performance Validation</strong></p>\n<ul>\n<li>Canary version shows improved accuracy on validation metrics</li>\n<li>Latency remains within acceptable bounds for both versions</li>\n<li>Error rates comparable between versions</li>\n<li>No significant drift detected in prediction distributions</li>\n</ul>\n</li>\n<li><p><strong>Gradual Traffic Increase</strong></p>\n<ul>\n<li>Traffic shifted incrementally: 80%/20%, then 60%/40%, then 20%/80%</li>\n<li>Each traffic shift monitored for 2 hours before next increase</li>\n<li>Performance metrics remain stable throughout progression</li>\n<li>A/B testing shows statistical significance in favor of new version</li>\n</ul>\n</li>\n<li><p><strong>Full Deployment</strong></p>\n<ul>\n<li>Traffic shifted to 100% new version after successful validation</li>\n<li>Old version kept running for 24 hours as rollback safety net</li>\n<li>Monitoring continues to validate production performance</li>\n<li>Old version finally decommissioned after confirmation</li>\n</ul>\n</li>\n</ol>\n<p><em>Validation Points:</em></p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Metrics Tracked</th>\n<th>Success Thresholds</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Canary 5%</td>\n<td>Latency p99, error rate, prediction accuracy</td>\n<td>p99 &lt; 100ms, error rate &lt; 0.1%, accuracy improvement &gt; 2%</td>\n</tr>\n<tr>\n<td>Traffic 20%</td>\n<td>Business metrics, user satisfaction, system load</td>\n<td>Conversion rate stable, complaints &lt; baseline, CPU utilization &lt; 80%</td>\n</tr>\n<tr>\n<td>Traffic 50%</td>\n<td>Statistical significance, drift detection, capacity</td>\n<td>A/B test p-value &lt; 0.05, drift score &lt; 0.3, no capacity bottlenecks</td>\n</tr>\n<tr>\n<td>Full Deployment</td>\n<td>Production stability, rollback readiness</td>\n<td>Error rate &lt; production baseline, rollback tested and ready</td>\n</tr>\n</tbody></table>\n<p><strong>Scenario 3: Drift Detection and Model Retraining</strong></p>\n<p>This scenario tests the platform&#39;s ability to detect performance degradation and coordinate automated retraining workflows.</p>\n<p><em>Workflow Steps:</em></p>\n<ol>\n<li><p><strong>Baseline Establishment</strong></p>\n<ul>\n<li>Production model deployed with comprehensive monitoring</li>\n<li>Baseline feature distributions captured from training data</li>\n<li>Performance thresholds configured for accuracy and drift scores</li>\n<li>Automated retraining pipeline prepared but not activated</li>\n</ul>\n</li>\n<li><p><strong>Gradual Data Drift Introduction</strong></p>\n<ul>\n<li>Simulated drift through gradually shifting input distributions</li>\n<li>Feature drift scores increase over two-week period</li>\n<li>Prediction accuracy begins declining from baseline levels</li>\n<li>Monitoring dashboard shows increasing drift severity</li>\n</ul>\n</li>\n<li><p><strong>Alert Escalation</strong></p>\n<ul>\n<li>Warning alerts triggered when drift scores exceed low thresholds</li>\n<li>Critical alerts triggered when multiple features show significant drift</li>\n<li>Escalation procedures notify data science team</li>\n<li>Automated retraining pipeline activation considered</li>\n</ul>\n</li>\n<li><p><strong>Automated Retraining Decision</strong></p>\n<ul>\n<li>Platform evaluates retraining triggers: drift severity, accuracy decline, data availability</li>\n<li>Sufficient recent data available for retraining (last 30 days)</li>\n<li>Retraining pipeline automatically initiated with updated dataset</li>\n<li>Original model continues serving while retraining proceeds</li>\n</ul>\n</li>\n<li><p><strong>Model Update Cycle</strong></p>\n<ul>\n<li>Retraining completes with improved model adapted to recent data</li>\n<li>New model registered with lineage showing drift-triggered retraining</li>\n<li>Automated validation confirms improved performance on recent data</li>\n<li>Canary deployment initiated with drift-adapted model</li>\n</ul>\n</li>\n</ol>\n<p><em>Monitoring Validation:</em></p>\n<table>\n<thead>\n<tr>\n<th>Drift Type</th>\n<th>Detection Method</th>\n<th>Threshold</th>\n<th>Alert Action</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Feature Drift</td>\n<td>Population Stability Index (PSI)</td>\n<td>PSI &gt; 0.2 warning, PSI &gt; 0.4 critical</td>\n<td>Warning: dashboard notification, Critical: team alert + retraining evaluation</td>\n</tr>\n<tr>\n<td>Concept Drift</td>\n<td>Prediction distribution change</td>\n<td>KL divergence &gt; 0.3</td>\n<td>Accuracy validation triggered, retraining pipeline evaluated</td>\n</tr>\n<tr>\n<td>Performance Drift</td>\n<td>Accuracy decline on validation set</td>\n<td>&gt;5% decline from baseline</td>\n<td>Immediate investigation, expedited retraining if cause unclear</td>\n</tr>\n<tr>\n<td>Input Data Quality</td>\n<td>Missing features, invalid ranges</td>\n<td>&gt;1% invalid requests</td>\n<td>Data pipeline investigation, input validation rule updates</td>\n</tr>\n</tbody></table>\n<h4 id=\"integration-testing-with-external-systems\">Integration Testing with External Systems</h4>\n<p>MLOps platforms integrate with numerous external systems that must be tested through realistic interfaces rather than simple mocks.</p>\n<p><strong>Cloud Storage Integration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Integration Point</th>\n<th>Test Scenarios</th>\n<th>Validation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Artifact Storage (S3/GCS)</td>\n<td>Large model upload/download, concurrent access, network failures</td>\n<td>Files transferred completely, concurrent operations don&#39;t corrupt data, failures handled gracefully with retries</td>\n</tr>\n<tr>\n<td>Data Lake Access</td>\n<td>Training data ingestion, schema evolution, access control</td>\n<td>Data loaded correctly with schema validation, schema changes handled appropriately, unauthorized access denied</td>\n</tr>\n<tr>\n<td>Backup and Disaster Recovery</td>\n<td>Metadata backup, artifact replication, cross-region restore</td>\n<td>Backups complete and restorable, artifacts replicated correctly, cross-region restore preserves all functionality</td>\n</tr>\n</tbody></table>\n<p><strong>Container Orchestration Integration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Integration Point</th>\n<th>Test Scenarios</th>\n<th>Validation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Kubernetes Training Jobs</td>\n<td>GPU scheduling, distributed training, resource limits</td>\n<td>GPUs allocated correctly, distributed training coordinates properly, resource limits enforced</td>\n</tr>\n<tr>\n<td>Model Serving Deployment</td>\n<td>Auto-scaling, rolling updates, health checks</td>\n<td>Auto-scaling responds to load changes, rolling updates maintain availability, health checks prevent bad deployments</td>\n</tr>\n<tr>\n<td>Pipeline Step Execution</td>\n<td>Container lifecycle, secret management, persistent volumes</td>\n<td>Containers start/stop cleanly, secrets mounted securely, data persists across container restarts</td>\n</tr>\n</tbody></table>\n<p><strong>Monitoring System Integration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Integration Point</th>\n<th>Test Scenarios</th>\n<th>Validation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metrics Collection (Prometheus)</td>\n<td>High-frequency metrics, metric cardinality, retention policies</td>\n<td>Metrics collected at required frequency, cardinality stays within limits, retention policies applied correctly</td>\n</tr>\n<tr>\n<td>Alerting (AlertManager)</td>\n<td>Alert routing, escalation policies, notification delivery</td>\n<td>Alerts routed to correct teams, escalation follows defined policies, notifications delivered reliably</td>\n</tr>\n<tr>\n<td>Observability (Jaeger/Zipkin)</td>\n<td>Distributed tracing, performance profiling, error correlation</td>\n<td>Traces capture complete request flows, performance bottlenecks identified, errors correlated across services</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Common Pitfalls in End-to-End Testing:</strong></p>\n<p>⚠️ <strong>Pitfall: Test Data Pollution</strong>: Using the same test data repeatedly leads to unrealistic scenarios where models memorize test patterns rather than learning generalizable behaviors.\n<strong>Fix</strong>: Generate fresh test data for each scenario or use data versioning to ensure tests use appropriate datasets for their validation goals.</p>\n<p>⚠️ <strong>Pitfall: Timing Dependencies</strong>: Tests that depend on specific timing (e.g., expecting metrics to update within exactly 5 seconds) become flaky in different environments or under load.\n<strong>Fix</strong>: Use event-driven validation where possible, and implement exponential backoff polling for time-sensitive assertions with reasonable timeout bounds.</p>\n<p>⚠️ <strong>Pitfall: Resource Cleanup</strong>: Failed tests leave behind cloud resources, test models, or storage artifacts that interfere with subsequent test runs and accumulate costs.\n<strong>Fix</strong>: Implement comprehensive cleanup in test teardown with unique resource naming (test-run-id prefixes) and scheduled cleanup jobs that remove orphaned test resources.</p>\n</blockquote>\n<h3 id=\"milestone-verification\">Milestone Verification</h3>\n<p>After each milestone implementation, specific verification procedures ensure the platform meets acceptance criteria and integrates correctly with existing components. Think of milestone verification like <strong>quality gates in a manufacturing process</strong> - each stage must pass inspection before proceeding to the next phase of development.</p>\n<h4 id=\"milestone-1-experiment-tracking-verification\">Milestone 1: Experiment Tracking Verification</h4>\n<p><strong>Functional Verification Checklist:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Verification Method</th>\n<th>Expected Behavior</th>\n<th>Pass Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parameter Logging</td>\n<td>Log nested parameters with mixed types</td>\n<td>Parameters stored with correct types, nested structure preserved</td>\n<td>Query API returns parameters exactly as logged, type information maintained</td>\n</tr>\n<tr>\n<td>Metric Tracking</td>\n<td>Log metrics at irregular intervals with duplicate steps</td>\n<td>Metrics stored with step correlation, duplicates handled correctly</td>\n<td>Time series queries return metrics in step order, duplicate handling documented behavior</td>\n</tr>\n<tr>\n<td>Artifact Upload</td>\n<td>Upload 100MB model file with metadata</td>\n<td>File stored with integrity verification, metadata searchable</td>\n<td>Download matches upload exactly, metadata query returns correct results</td>\n</tr>\n<tr>\n<td>Run Comparison</td>\n<td>Compare 5 runs with different parameter sets</td>\n<td>Statistical comparison computed correctly, missing data handled</td>\n<td>Comparison view shows parameter differences, statistical tests use appropriate methods</td>\n</tr>\n</tbody></table>\n<p><strong>Performance Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Load Scenario</th>\n<th>Test Configuration</th>\n<th>Performance Target</th>\n<th>Measurement Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High-Frequency Logging</td>\n<td>1000 metrics/second for 10 minutes</td>\n<td>&lt;100ms p99 latency for metric logging</td>\n<td>Load testing with concurrent clients, measure response times</td>\n</tr>\n<tr>\n<td>Large Artifact Upload</td>\n<td>1GB model file upload</td>\n<td>&lt;10 minutes upload time on 100Mbps connection</td>\n<td>Time upload operation, verify integrity after completion</td>\n</tr>\n<tr>\n<td>Query Performance</td>\n<td>Search across 10,000 runs with complex filters</td>\n<td>&lt;2 seconds for complex queries</td>\n<td>Execute representative queries, measure database response times</td>\n</tr>\n<tr>\n<td>Concurrent Access</td>\n<td>50 simultaneous users logging to different runs</td>\n<td>No data corruption or lost updates</td>\n<td>Verify all logged data appears correctly in final state</td>\n</tr>\n</tbody></table>\n<p><strong>Integration Verification:</strong></p>\n<p>Verify that experiment tracking correctly publishes events and provides APIs that downstream components can consume.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Event Publication Test</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/runs/complete</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"run_id\": \"test-run-123\", \"final_metrics\": {\"accuracy\": 0.95}}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Verify EXPERIMENT_COMPLETED event published</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/events?type=experiment.completed</span><span style=\"color:#E1E4E8\">&#x26;since</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">2024-01-01</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># API Integration Test  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/runs/test-run-123/artifacts</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Authorization: Bearer test-token\"</span></span></code></pre></div>\n\n<p><strong>Verification Outputs:</strong></p>\n<p>After successful milestone 1 verification, the following artifacts should be available:</p>\n<ul>\n<li><strong>Experiment Dashboard</strong>: Web interface showing experiment list with sortable columns for key metrics</li>\n<li><strong>API Documentation</strong>: Complete OpenAPI specification with example requests/responses  </li>\n<li><strong>Performance Report</strong>: Load testing results demonstrating throughput and latency targets</li>\n<li><strong>Event Integration</strong>: Documented event schemas with example payloads for downstream consumers</li>\n</ul>\n<h4 id=\"milestone-2-model-registry-verification\">Milestone 2: Model Registry Verification</h4>\n<p><strong>Functional Verification Checklist:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Verification Method</th>\n<th>Expected Behavior</th>\n<th>Pass Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model Registration</td>\n<td>Register model from experiment run with artifacts</td>\n<td>Model created with version 1.0.0, linked to run</td>\n<td>Registry API returns model with correct lineage, artifacts downloadable</td>\n</tr>\n<tr>\n<td>Stage Transitions</td>\n<td>Promote model through Development → Staging → Production</td>\n<td>Each transition recorded with timestamp and metadata</td>\n<td>Stage history shows complete transition log, current stage accurate</td>\n</tr>\n<tr>\n<td>Version Management</td>\n<td>Register multiple versions of same model</td>\n<td>Automatic version incrementing, parallel development branches</td>\n<td>Semantic versioning rules followed, version conflicts prevented</td>\n</tr>\n<tr>\n<td>Lineage Tracking</td>\n<td>Trace model back to training data and code commit</td>\n<td>Complete lineage graph from model to source data</td>\n<td>Lineage API returns full dependency chain, graph visualization possible</td>\n</tr>\n</tbody></table>\n<p><strong>Registry Operations Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Operation</th>\n<th>Test Scenario</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Concurrent Registration</td>\n<td>10 simultaneous model registrations</td>\n<td>All models registered successfully, no version conflicts</td>\n</tr>\n<tr>\n<td>Large Model Handling</td>\n<td>5GB model artifact registration</td>\n<td>Registration completes successfully, checksums validated</td>\n</tr>\n<tr>\n<td>Stage Approval Workflow</td>\n<td>Model promotion requiring manual approval</td>\n<td>Approval metadata captured, unauthorized promotions blocked</td>\n</tr>\n<tr>\n<td>Registry Migration</td>\n<td>Export/import registry to new deployment</td>\n<td>All models, versions, and lineage preserved exactly</td>\n</tr>\n</tbody></table>\n<p><strong>Model Registry API Verification:</strong></p>\n<p>Test the model registry API endpoints comprehensively:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Model Registration</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/models</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"name\": \"fraud-detection\", \"description\": \"Credit card fraud detection model\"}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Version Registration  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/models/fraud-detection/versions</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"version\": \"1.0.0\", \"run_id\": \"test-run-123\", \"description\": \"Initial production model\"}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Stage Promotion</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> PUT</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/models/fraud-detection/versions/1.0.0/stage</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"stage\": \"Staging\", \"approval_metadata\": {\"approver\": \"data-science-lead\"}}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Lineage Query</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/models/fraud-detection/versions/1.0.0/lineage?depth=3\"</span></span></code></pre></div>\n\n<h4 id=\"milestone-3-training-pipeline-verification\">Milestone 3: Training Pipeline Verification</h4>\n<p><strong>Pipeline Execution Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Pipeline Type</th>\n<th>Test Configuration</th>\n<th>Success Criteria</th>\n<th>Verification Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Linear Pipeline</td>\n<td>5 sequential steps with data dependencies</td>\n<td>All steps complete successfully in order</td>\n<td>Check step execution logs, verify data flow between steps</td>\n</tr>\n<tr>\n<td>Parallel Pipeline</td>\n<td>3 parallel training branches merging to evaluation</td>\n<td>Parallel steps execute concurrently, merge step waits for all</td>\n<td>Monitor resource utilization, verify merge step input timing</td>\n</tr>\n<tr>\n<td>Distributed Training</td>\n<td>Multi-GPU training across 4 nodes</td>\n<td>Training completes with gradient synchronization</td>\n<td>Verify model convergence, check distributed training logs</td>\n</tr>\n<tr>\n<td>Failure Recovery</td>\n<td>Pipeline with intentional step failure and retry</td>\n<td>Failed step retries with exponential backoff</td>\n<td>Monitor retry attempts, verify eventual success or failure after max attempts</td>\n</tr>\n</tbody></table>\n<p><strong>Resource Management Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Resource Scenario</th>\n<th>Test Configuration</th>\n<th>Expected Behavior</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>GPU Allocation</td>\n<td>Request 2 GPUs for training step</td>\n<td>GPUs allocated exclusively to step</td>\n<td>Check GPU utilization, verify no sharing with other processes</td>\n</tr>\n<tr>\n<td>Memory Limits</td>\n<td>Step requiring 16GB RAM on 8GB node</td>\n<td>Step fails with clear resource error</td>\n<td>Verify error message indicates insufficient memory</td>\n</tr>\n<tr>\n<td>Storage Requirements</td>\n<td>Step generating 50GB temporary data</td>\n<td>Sufficient storage allocated and cleaned up</td>\n<td>Monitor disk usage during and after step execution</td>\n</tr>\n<tr>\n<td>Preemptible Instances</td>\n<td>Pipeline on spot instances with interruption</td>\n<td>Graceful handling of node preemption</td>\n<td>Verify checkpoint/resume behavior on node replacement</td>\n</tr>\n</tbody></table>\n<p><strong>Pipeline Orchestration API Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Pipeline Definition Upload</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/pipelines</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -F</span><span style=\"color:#9ECBFF\"> \"pipeline=@fraud-detection-pipeline.yaml\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Pipeline Execution</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/pipelines/fraud-detection/runs</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"parameters\": {\"dataset_version\": \"v2.1\", \"learning_rate\": 0.001}}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Execution Monitoring</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/pipelines/fraud-detection/runs/run-456/status</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Step Log Retrieval  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/pipelines/runs/run-456/steps/training/logs</span></span></code></pre></div>\n\n<h4 id=\"milestone-4-model-deployment-verification\">Milestone 4: Model Deployment Verification</h4>\n<p><strong>Deployment Strategy Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Deployment Type</th>\n<th>Test Configuration</th>\n<th>Success Criteria</th>\n<th>Verification Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Blue-Green Deployment</td>\n<td>Switch traffic between two model versions</td>\n<td>Zero-downtime traffic switch, rollback capability</td>\n<td>Monitor request success rate during switch, verify rollback restores previous version</td>\n</tr>\n<tr>\n<td>Canary Deployment</td>\n<td>Gradual 5%→25%→50%→100% traffic shift</td>\n<td>Traffic split accurately according to configuration</td>\n<td>Measure actual traffic distribution, verify gradual progression</td>\n</tr>\n<tr>\n<td>A/B Testing</td>\n<td>Split traffic 50/50 between two models for comparison</td>\n<td>Statistical significance calculation, performance comparison</td>\n<td>Collect sufficient samples for statistical validity, compare conversion rates</td>\n</tr>\n</tbody></table>\n<p><strong>Auto-scaling Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Load Pattern</th>\n<th>Configuration</th>\n<th>Expected Behavior</th>\n<th>Measurement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Gradual Load Increase</td>\n<td>Scale up when p99 latency &gt; 100ms</td>\n<td>Replicas increase before latency degrades</td>\n<td>Monitor latency and replica count during load ramp</td>\n</tr>\n<tr>\n<td>Traffic Spike</td>\n<td>10x traffic increase in 1 minute</td>\n<td>Rapid scale-up maintains service availability</td>\n<td>Verify no request failures during scaling event</td>\n</tr>\n<tr>\n<td>Load Decrease</td>\n<td>Traffic drops to 10% of peak</td>\n<td>Scale down to minimum replicas after cooldown</td>\n<td>Confirm scale-down occurs after appropriate delay</td>\n</tr>\n</tbody></table>\n<p><strong>Model Serving Integration Verification:</strong></p>\n<p>Test integration with various model serving frameworks:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># TensorFlow Serving Integration</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/api/v1/deploy</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\ </span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">  -d</span><span style=\"color:#9ECBFF\"> '{\"model_name\": \"fraud-detection\", \"version\": \"1.0.0\", \"serving_framework\": \"tensorflow-serving\"}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Model Inference Test</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> POST</span><span style=\"color:#9ECBFF\"> http://localhost:8080/v1/models/fraud-detection:predict</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -H</span><span style=\"color:#9ECBFF\"> \"Content-Type: application/json\"</span><span style=\"color:#79B8FF\"> \\</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">  -d</span><span style=\"color:#9ECBFF\"> '{\"instances\": [{\"transaction_amount\": 1500.0, \"merchant_category\": \"restaurant\"}]}'</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Health Check Verification</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> http://localhost:8080/v1/models/fraud-detection/metadata</span></span></code></pre></div>\n\n<h4 id=\"milestone-5-model-monitoring-verification\">Milestone 5: Model Monitoring Verification</h4>\n<p><strong>Drift Detection Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Drift Type</th>\n<th>Simulation Method</th>\n<th>Detection Threshold</th>\n<th>Validation Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Feature Drift</td>\n<td>Gradually shift input feature distributions</td>\n<td>PSI &gt; 0.2 warning, PSI &gt; 0.4 critical</td>\n<td>Alerts triggered at correct thresholds, drift scores calculated accurately</td>\n</tr>\n<tr>\n<td>Concept Drift</td>\n<td>Change relationship between features and predictions</td>\n<td>Prediction distribution divergence &gt; 0.3</td>\n<td>Concept drift detected before significant accuracy loss</td>\n</tr>\n<tr>\n<td>Data Quality Issues</td>\n<td>Introduce missing values and outliers</td>\n<td>&gt;1% invalid requests</td>\n<td>Data quality alerts trigger investigation workflows</td>\n</tr>\n</tbody></table>\n<p><strong>Performance Monitoring Verification:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Metric Category</th>\n<th>Measurement</th>\n<th>Target</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Prediction Latency</td>\n<td>p50, p90, p99 latency percentiles</td>\n<td>p99 &lt; 100ms</td>\n<td>Load test with realistic request patterns</td>\n</tr>\n<tr>\n<td>Throughput</td>\n<td>Requests per second capacity</td>\n<td>&gt;1000 RPS sustained</td>\n<td>Sustained load test for 30 minutes</td>\n</tr>\n<tr>\n<td>Accuracy Tracking</td>\n<td>Online accuracy vs batch validation</td>\n<td>&lt;5% difference</td>\n<td>Compare online predictions with known labels</td>\n</tr>\n</tbody></table>\n<p><strong>Monitoring Dashboard Verification:</strong></p>\n<p>The monitoring dashboard should display real-time and historical metrics:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Dashboard API Test</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/monitoring/fraud-detection/metrics?start_time=2024-01-01&#x26;end_time=2024-01-02\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Real-time Metrics</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/monitoring/fraud-detection/realtime\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Drift Analysis  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/monitoring/fraud-detection/drift?analysis_window=24h\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Alert History</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">curl</span><span style=\"color:#79B8FF\"> -X</span><span style=\"color:#9ECBFF\"> GET</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/monitoring/fraud-detection/alerts?severity=critical&#x26;since=7d\"</span></span></code></pre></div>\n\n<blockquote>\n<p><strong>Design Insight</strong>: Milestone verification should be <strong>cumulative</strong> - each milestone should continue to validate all previous functionality while adding new capabilities. This ensures that integration work doesn&#39;t break existing features and that the platform maintains coherent functionality as complexity increases.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The testing strategy requires careful coordination between fast-running unit tests and comprehensive integration scenarios. Focus on building a <strong>testing pyramid</strong> where fast unit tests provide immediate feedback during development, while slower integration tests validate realistic workflows.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Testing Layer</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing</td>\n<td>pytest with fixtures</td>\n<td>pytest + hypothesis for property-based testing</td>\n</tr>\n<tr>\n<td>API Testing</td>\n<td>requests library with custom helpers</td>\n<td>tavern for API contract testing</td>\n</tr>\n<tr>\n<td>Database Testing</td>\n<td>SQLite in-memory for fast tests</td>\n<td>testcontainers for realistic database testing</td>\n</tr>\n<tr>\n<td>Load Testing</td>\n<td>Simple threading with concurrent.futures</td>\n<td>locust for distributed load testing</td>\n</tr>\n<tr>\n<td>End-to-End Testing</td>\n<td>selenium for web UI testing</td>\n<td>playwright for modern web automation</td>\n</tr>\n<tr>\n<td>Test Data Management</td>\n<td>JSON fixtures in test files</td>\n<td>factory-boy for realistic data generation</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<p>Organize test files to mirror the application structure while providing clear separation between test types:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>mlops-platform/\n├── tests/\n│   ├── unit/                          # Fast unit tests\n│   │   ├── experiment_tracking/       # Component-specific unit tests\n│   │   │   ├── test_parameter_logging.py\n│   │   │   ├── test_metric_tracking.py\n│   │   │   └── test_artifact_storage.py\n│   │   ├── model_registry/\n│   │   │   ├── test_version_management.py\n│   │   │   ├── test_stage_transitions.py\n│   │   │   └── test_lineage_tracking.py\n│   │   └── shared/                     # Shared test utilities\n│   │       ├── fixtures.py\n│   │       └── test_helpers.py\n│   ├── integration/                    # Component integration tests\n│   │   ├── test_api_integration.py\n│   │   ├── test_event_coordination.py\n│   │   └── test_database_integration.py\n│   ├── end_to_end/                     # Complete workflow tests\n│   │   ├── test_model_development_flow.py\n│   │   ├── test_canary_deployment.py\n│   │   └── test_drift_detection.py\n│   ├── performance/                    # Load and performance tests\n│   │   ├── test_experiment_tracking_load.py\n│   │   ├── test_serving_throughput.py\n│   │   └── test_monitoring_scalability.py\n│   └── fixtures/                       # Test data and configuration\n│       ├── sample_datasets/\n│       ├── model_artifacts/\n│       └── pipeline_definitions/\n├── src/\n│   └── mlops_platform/                # Application code\n└── docker-compose.test.yml            # Test infrastructure</code></pre></div>\n\n<h4 id=\"test-infrastructure-setup\">Test Infrastructure Setup</h4>\n<p><strong>Docker Compose Test Environment:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">yaml</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># docker-compose.test.yml - Complete test infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">version</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'3.8'</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">services</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  postgres-test</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    image</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">postgres:14</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    environment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      POSTGRES_DB</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">mlops_test</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      POSTGRES_USER</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">test_user</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      POSTGRES_PASSWORD</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">test_pass</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    ports</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"5433:5432\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  minio-test</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    image</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">minio/minio</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    command</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">server /data --console-address \":9001\"</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    environment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      MINIO_ROOT_USER</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">test_access_key</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">      MINIO_ROOT_PASSWORD</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">test_secret_key</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    ports</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"9000:9000\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"9001:9001\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">  redis-test</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    image</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">redis:7-alpine</span></span>\n<span class=\"line\"><span style=\"color:#85E89D\">    ports</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">      - </span><span style=\"color:#9ECBFF\">\"6380:6379\"</span></span></code></pre></div>\n\n<p><strong>Test Configuration Management:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/shared/config.py - Centralized test configuration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> os</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestConfig</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Centralized configuration for all test environments.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Database configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    postgres_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"postgresql://test_user:test_pass@localhost:5433/mlops_test\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Object storage configuration  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    minio_endpoint: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"localhost:9000\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    minio_access_key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"test_access_key\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    minio_secret_key: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"test_secret_key\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Redis configuration</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    redis_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"redis://localhost:6380/0\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # API base URLs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    experiment_api_base: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/experiments\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    registry_api_base: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"http://localhost:8080/api/v1/models\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test data paths</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    fixtures_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"tests/fixtures\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sample_model_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"tests/fixtures/model_artifacts/sample_model.pkl\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_environment</span><span style=\"color:#E1E4E8\">(cls) -> </span><span style=\"color:#9ECBFF\">'TestConfig'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load configuration from environment variables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> cls</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            postgres_url</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'TEST_POSTGRES_URL'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">cls</span><span style=\"color:#E1E4E8\">.postgres_url),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            minio_endpoint</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">os.getenv(</span><span style=\"color:#9ECBFF\">'TEST_MINIO_ENDPOINT'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">cls</span><span style=\"color:#E1E4E8\">.minio_endpoint),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # ... other environment variable mappings</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<h4 id=\"test-utilities-and-fixtures\">Test Utilities and Fixtures</h4>\n<p><strong>Database Test Utilities:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/shared/database.py - Database testing utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pytest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncpg</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Generator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextlib </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asynccontextmanager</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DatabaseTestHelper</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper class for database testing operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, postgres_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.postgres_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> postgres_url</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @asynccontextmanager</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> temporary_database</span><span style=\"color:#E1E4E8\">(self) -> Generator[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a temporary database for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate unique database name with test prefix</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create database using asyncpg connection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Run schema migrations on new database  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Yield database URL for test use</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Clean up database after test completion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> reset_database</span><span style=\"color:#E1E4E8\">(self, database_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Reset database to clean state between tests.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Connect to database</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Truncate all tables in correct dependency order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Reset auto-increment sequences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify database is in clean state</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@pytest.fixture</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> clean_database</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Provides a clean database for each test.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    helper </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> DatabaseTestHelper(TestConfig().postgres_url)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> with</span><span style=\"color:#E1E4E8\"> helper.temporary_database() </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> db_url:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        yield</span><span style=\"color:#E1E4E8\"> db_url</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Database automatically cleaned up by context manager</span></span></code></pre></div>\n\n<p><strong>API Testing Utilities:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/shared/api_client.py - API testing client</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> requests</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> APIResponse</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Structured API response for testing assertions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status_code: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    json_data: Optional[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    headers: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    elapsed_seconds: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MLOpsAPIClient</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test client for MLOps platform APIs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, base_url: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, auth_token: Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.base_url </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> base_url.rstrip(</span><span style=\"color:#9ECBFF\">'/'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.session </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> requests.Session()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> auth_token:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.session.headers[</span><span style=\"color:#9ECBFF\">'Authorization'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">'Bearer </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">auth_token</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_experiment</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, tags: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> APIResponse:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a new experiment.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Prepare request payload with experiment data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Make POST request to experiments endpoint</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Parse response and measure response time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return structured APIResponse object</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Handle request failures with descriptive errors</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> log_metrics</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">], step: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> APIResponse:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Log metrics for a specific run and step.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate metric names and values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Format metrics according to API schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Make POST request with batch metric logging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify response indicates successful logging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return response with timing information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> wait_for_experiment_completion</span><span style=\"color:#E1E4E8\">(self, run_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 300</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Wait for experiment run to complete with polling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start polling timer for timeout handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Poll run status every 5 seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return True when run status becomes FINISHED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return False if timeout exceeded or run FAILED</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Log polling progress for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"test-data-generation\">Test Data Generation</h4>\n<p><strong>Realistic ML Data Factories:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/shared/data_factories.py - Test data generation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> factory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> random</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExperimentFactory</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">factory</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Factory</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Factory for creating realistic experiment test data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    class</span><span style=\"color:#B392F0\"> Meta</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    experiment_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> factory.LazyFunction(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"exp_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">random.randint(</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">9999</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> factory.Sequence(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\"> n: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"fraud_detection_experiment_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">n</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    creation_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> factory.LazyFunction(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: datetime.utcnow().timestamp())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tags </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> factory.LazyFunction(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"team\"</span><span style=\"color:#E1E4E8\">: random.choice([</span><span style=\"color:#9ECBFF\">\"data-science\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"ml-engineering\"</span><span style=\"color:#E1E4E8\">]),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"priority\"</span><span style=\"color:#E1E4E8\">: random.choice([</span><span style=\"color:#9ECBFF\">\"high\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"medium\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"low\"</span><span style=\"color:#E1E4E8\">]),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"dataset_version\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"v</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">random.randint(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">.</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">random.randint(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">9</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RunFactory</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">factory</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Factory</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Factory for creating ML training run data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    class</span><span style=\"color:#B392F0\"> Meta</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        model </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> dict</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    run_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> factory.LazyFunction(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"run_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">random.randint(</span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">99999</span><span style=\"color:#E1E4E8\">)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    experiment_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> factory.SubFactory(ExperimentFactory)[</span><span style=\"color:#9ECBFF\">'experiment_id'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> factory.LazyFunction(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"learning_rate\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">round</span><span style=\"color:#E1E4E8\">(random.uniform(</span><span style=\"color:#79B8FF\">0.0001</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#79B8FF\">6</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"batch_size\"</span><span style=\"color:#E1E4E8\">: random.choice([</span><span style=\"color:#79B8FF\">16</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">32</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">64</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">128</span><span style=\"color:#E1E4E8\">]),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"epochs\"</span><span style=\"color:#E1E4E8\">: random.randint(</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"model_type\"</span><span style=\"color:#E1E4E8\">: random.choice([</span><span style=\"color:#9ECBFF\">\"random_forest\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"xgboost\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"neural_network\"</span><span style=\"color:#E1E4E8\">])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    })</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @factory.LazyAttribute</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> metrics</span><span style=\"color:#E1E4E8\">(obj):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate realistic training metrics time series.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        epochs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> obj.parameters[</span><span style=\"color:#9ECBFF\">\"epochs\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"accuracy\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#79B8FF\">round</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.5</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 0.4</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> np.exp(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">10</span><span style=\"color:#E1E4E8\">)), </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(epochs)],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"loss\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#79B8FF\">round</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> np.exp(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">15</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(epochs)],</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"val_accuracy\"</span><span style=\"color:#E1E4E8\">: [</span><span style=\"color:#79B8FF\">round</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.45</span><span style=\"color:#F97583\"> +</span><span style=\"color:#79B8FF\"> 0.35</span><span style=\"color:#F97583\"> *</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> np.exp(</span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\">i</span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">)), </span><span style=\"color:#79B8FF\">4</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(epochs)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_realistic_training_data</span><span style=\"color:#E1E4E8\">(num_samples: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate realistic training dataset for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create feature columns with realistic distributions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Add correlations between features that ML models would learn</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Generate target variable with realistic class imbalance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Add some missing values and outliers for robustness testing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Package data in format expected by training pipelines</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint-implementation\">Milestone Checkpoint Implementation</h4>\n<p><strong>Automated Milestone Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/milestones/milestone_validator.py - Automated milestone validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ValidationResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Result of a milestone validation check.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    check_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    passed: </span><span style=\"color:#79B8FF\">bool</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_message: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MilestoneValidator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for milestone validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate</span><span style=\"color:#E1E4E8\">(self) -> List[ValidationResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run all validation checks for this milestone.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> Milestone1Validator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MilestoneValidator</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validator for Experiment Tracking milestone.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, api_client: MLOpsAPIClient):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.api </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> api_client</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate</span><span style=\"color:#E1E4E8\">(self) -> List[ValidationResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate experiment tracking functionality.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test parameter logging with various data types</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._test_parameter_logging())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Test metric tracking with time series data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._test_metric_tracking())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test artifact upload and download integrity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._test_artifact_handling())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test experiment querying and filtering</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._test_experiment_queries())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test run comparison functionality</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results.append(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._test_run_comparison())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _test_parameter_logging</span><span style=\"color:#E1E4E8\">(self) -> ValidationResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test parameter logging with various data types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create test experiment and run</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Log parameters with different types (int, float, str, dict, list)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Retrieve parameters and verify exact match</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test parameter overwriting behavior</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return validation result with details</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> run_milestone_validation</span><span style=\"color:#E1E4E8\">(milestone: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Run validation for specified milestone.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    validators </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        1</span><span style=\"color:#E1E4E8\">: Milestone1Validator,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        2</span><span style=\"color:#E1E4E8\">: Milestone2Validator,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        3</span><span style=\"color:#E1E4E8\">: Milestone3Validator,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        4</span><span style=\"color:#E1E4E8\">: Milestone4Validator,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        5</span><span style=\"color:#E1E4E8\">: Milestone5Validator</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#E1E4E8\"> milestone </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> validators:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Unknown milestone: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">milestone</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize validator with appropriate configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Run all validation checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Print detailed results for failed checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return overall pass/fail status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Generate validation report for documentation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># CLI entry point for milestone validation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    milestone </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(sys.argv[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]) </span><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(sys.argv) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    success </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> run_milestone_validation(milestone)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sys.exit(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> success </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"performance-testing-framework\">Performance Testing Framework</h4>\n<p><strong>Load Testing Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># tests/performance/load_test_framework.py - Load testing framework</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Callable, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> concurrent.futures </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ThreadPoolExecutor, as_completed</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoadTestResult</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Results from a load testing scenario.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_requests: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    successful_requests: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    failed_requests: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    response_times: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    throughput_rps: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p50_latency: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p95_latency: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    p99_latency: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    error_rate: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">classmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> from_measurements</span><span style=\"color:#E1E4E8\">(cls, response_times: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">], errors: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], duration: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate load test metrics from raw measurements.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate basic counts and rates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compute latency percentiles using statistics module</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate throughput as requests per second</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Determine error rate from failed requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return structured result object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> LoadTestRunner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Framework for running load tests against MLOps APIs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_workers: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 50</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_workers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_workers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> run_load_test</span><span style=\"color:#E1E4E8\">(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        test_function: Callable[[], Any],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        target_rps: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        duration_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        warmup_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ) -> LoadTestResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Run load test with specified parameters.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Execute warmup period to stabilize system</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate request timing to achieve target RPS</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use ThreadPoolExecutor for concurrent request execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Collect response times and errors from all requests</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate and return performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_experiment_tracking_load</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Load test for experiment tracking API.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> single_request</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute single API request for load testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate unique test data for this request</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Make API call to log parameters and metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Measure response time and capture any errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return timing and success information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Include resource usage if available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    runner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> LoadTestRunner(</span><span style=\"color:#FFAB70\">max_workers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test scenarios with different load patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scenarios </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#9ECBFF\">\"target_rps\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"duration\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">300</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"description\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Sustained load\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#9ECBFF\">\"target_rps\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">500</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"duration\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">60</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"description\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Peak load burst\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        {</span><span style=\"color:#9ECBFF\">\"target_rps\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"duration\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"description\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"Stress test\"</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> scenario </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> scenarios:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Running </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">scenario[</span><span style=\"color:#9ECBFF\">'description'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> scenario...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> runner.run_load_test(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            single_request,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            scenario[</span><span style=\"color:#9ECBFF\">\"target_rps\"</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            scenario[</span><span style=\"color:#9ECBFF\">\"duration\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Validate performance targets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> result.p99_latency </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 100.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"p99 latency too high: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.p99_latency</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        assert</span><span style=\"color:#E1E4E8\"> result.error_rate </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Error rate too high: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.error_rate</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Results: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.throughput_rps</span><span style=\"color:#F97583\">:.1f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> RPS, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">              f</span><span style=\"color:#9ECBFF\">\"p99: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.p99_latency</span><span style=\"color:#F97583\">:.1f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">ms, \"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">              f</span><span style=\"color:#9ECBFF\">\"errors: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">result.error_rate</span><span style=\"color:#F97583\">:.2%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-5) by providing comprehensive debugging strategies that help developers diagnose and fix issues across experiment tracking, model registry, training pipeline orchestration, model deployment, and model monitoring components.</p>\n</blockquote>\n<p>Building an MLOps platform involves coordinating multiple distributed components, each with its own failure modes and debugging challenges. Think of debugging a distributed MLOps system like being a detective investigating a crime scene where the evidence is scattered across multiple locations, timestamps don&#39;t always align, and witnesses (logs) might be unreliable or missing. Unlike debugging a single-threaded application where you can step through code linearly, MLOps debugging requires understanding how components interact asynchronously, how data flows between systems, and how failures cascade across service boundaries.</p>\n<p>The complexity of MLOps debugging stems from the inherent distributed nature of the system. An experiment might fail due to a parameter logging issue in the tracking component, but the symptoms appear as missing metrics in the dashboard. A model deployment might succeed technically but fail functionally due to data drift that wasn&#39;t detected by the monitoring component. A training pipeline might hang indefinitely due to resource contention in the orchestration layer, but the user only sees a &quot;pending&quot; status in the UI.</p>\n<p>This section provides structured approaches for diagnosing and fixing these complex, multi-component issues. We&#39;ll start with symptom-based diagnosis tables that help identify root causes, then cover debugging tools and techniques for gaining visibility into system behavior, and finally address performance troubleshooting for identifying and eliminating bottlenecks.</p>\n<h3 id=\"symptom-based-diagnosis\">Symptom-Based Diagnosis</h3>\n<p>The key to effective MLOps debugging is recognizing that symptoms often appear far from their root causes. A deployment failure might be caused by an experiment tracking issue, or a monitoring alert might indicate a problem with the model registry. This subsection provides comprehensive symptom-to-cause mappings organized by the component where symptoms typically appear.</p>\n<p><strong>Experiment Tracking Issues</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Causes</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment run stuck in RUNNING status</td>\n<td>Database connection timeout, missing end_time update, process crash during logging</td>\n<td>Check database connectivity with <code>ComponentHealth.run_checks()</code>, query recent <code>Run</code> records for missing <code>end_time</code> values, examine application logs for crash stack traces</td>\n<td>Implement automated cleanup job that sets abandoned runs to FAILED after timeout, add database connection pooling, ensure <code>log_artifact</code> calls include proper exception handling</td>\n</tr>\n<tr>\n<td>Parameters not appearing in experiment comparison</td>\n<td>Parameter value serialization failure, metadata store write timeout, incorrect run_id correlation</td>\n<td>Validate parameter values with <code>Parameter</code> type constraints, check <code>MetadataStore.insert</code> return codes, verify run_id exists in experiments table</td>\n<td>Add parameter validation before storage, implement retry logic for metadata writes, create foreign key constraints for referential integrity</td>\n</tr>\n<tr>\n<td>Artifact upload fails intermittently</td>\n<td>Object storage network timeouts, insufficient storage permissions, concurrent write conflicts</td>\n<td>Test <code>ArtifactStore.put</code> with sample data, verify IAM permissions for storage bucket, check for duplicate artifact paths</td>\n<td>Implement exponential backoff retry for storage operations, add artifact path uniqueness validation, create separate storage prefixes per run</td>\n</tr>\n<tr>\n<td>Metric comparison shows inconsistent results</td>\n<td>Clock skew between training nodes, metric aggregation errors, floating-point precision issues</td>\n<td>Compare timestamps across <code>MetricPoint</code> entries, validate metric calculations with known test data, check for NaN or infinity values</td>\n<td>Synchronize clocks using NTP, implement consistent metric aggregation with stable sorting, add numerical validation for metric values</td>\n</tr>\n<tr>\n<td>Search queries return incomplete results</td>\n<td>Database query timeouts, missing indexes on search columns, pagination implementation bugs</td>\n<td>Execute search query directly against database, check query execution plan for full table scans, validate <code>search_runs</code> pagination logic</td>\n<td>Add composite indexes on commonly searched columns, implement query result caching, optimize pagination with cursor-based navigation</td>\n</tr>\n</tbody></table>\n<p><strong>Model Registry Issues</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Causes</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model version promotion fails silently</td>\n<td>Stage transition validation errors, approval workflow misconfiguration, database constraint violations</td>\n<td>Check <code>ModelStage</code> enum constraints, validate approval metadata format, examine database transaction logs</td>\n<td>Implement explicit validation for stage transitions, add approval workflow status tracking, create audit log for all stage changes</td>\n</tr>\n<tr>\n<td>Model lineage graph shows broken connections</td>\n<td>Missing foreign key relationships, experiment run cleanup affecting lineage, metadata corruption</td>\n<td>Trace lineage path using <code>get_model_lineage</code>, verify experiment run still exists, check for orphaned model versions</td>\n<td>Implement soft deletion for experiment runs, add referential integrity constraints, create lineage validation procedures</td>\n</tr>\n<tr>\n<td>Model artifact download corruption</td>\n<td>Network transmission errors, storage checksum mismatches, concurrent modification during download</td>\n<td>Verify artifact checksum with <code>get_artifact</code> validation, test download with different network conditions, check for write operations during download</td>\n<td>Implement artifact immutability guarantees, add download retry with integrity verification, use atomic storage operations</td>\n</tr>\n<tr>\n<td>Version registration succeeds but model missing</td>\n<td>Asynchronous processing delays, metadata store inconsistency, artifact storage failures after metadata commit</td>\n<td>Check <code>ModelVersion</code> creation timestamp vs artifact upload time, verify artifact exists in storage, examine event processing logs</td>\n<td>Implement two-phase commit for version registration, add artifact existence validation before metadata commit, create reconciliation jobs</td>\n</tr>\n<tr>\n<td>Model search returns stale results</td>\n<td>Metadata caching inconsistencies, database replica lag, index update delays</td>\n<td>Compare direct database query with search API results, check cache invalidation logs, verify database replication status</td>\n<td>Implement cache invalidation on model updates, add read-after-write consistency checks, create cache warming procedures</td>\n</tr>\n</tbody></table>\n<p><strong>Training Pipeline Issues</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Causes</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Pipeline execution hangs indefinitely</td>\n<td>Resource allocation deadlock, step dependency cycles, Kubernetes job scheduling failures</td>\n<td>Check <code>StepExecution</code> status for all pipeline steps, validate DAG structure with <code>validate_pipeline_dag</code>, examine Kubernetes events</td>\n<td>Implement pipeline execution timeouts, add dependency cycle detection, create resource allocation monitoring</td>\n</tr>\n<tr>\n<td>Step fails with &quot;resource exhausted&quot; error</td>\n<td>Insufficient cluster capacity, resource quota exceeded, memory leak in step container</td>\n<td>Check cluster resource utilization, verify resource quotas with <code>kubectl describe quota</code>, examine container memory usage patterns</td>\n<td>Implement dynamic resource scaling, add resource usage monitoring per step, optimize container resource requests</td>\n</tr>\n<tr>\n<td>Data passing between steps corrupted</td>\n<td>Network failures during artifact transfer, concurrent write/read operations, serialization format changes</td>\n<td>Verify artifact integrity with checksums, check network connectivity between nodes, validate data schema consistency</td>\n<td>Implement atomic data transfer operations, add data validation at step boundaries, create backup data paths</td>\n</tr>\n<tr>\n<td>Parallel step execution produces inconsistent results</td>\n<td>Race conditions in shared resource access, non-deterministic processing order, inadequate step isolation</td>\n<td>Execute steps sequentially to verify correctness, check for shared file system access patterns, validate container isolation settings</td>\n<td>Implement proper step isolation, add shared resource locking mechanisms, create deterministic execution ordering</td>\n</tr>\n<tr>\n<td>Pipeline restart from checkpoint fails</td>\n<td>Checkpoint data corruption, missing intermediate artifacts, version incompatibility between runs</td>\n<td>Verify checkpoint file integrity, check artifact availability for restart point, compare pipeline version with checkpoint metadata</td>\n<td>Implement checkpoint validation before restart, add backward compatibility for pipeline versions, create checkpoint repair procedures</td>\n</tr>\n</tbody></table>\n<p><strong>Model Deployment Issues</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Causes</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model endpoint returns 5xx errors</td>\n<td>Model loading failures, insufficient memory allocation, inference server misconfiguration</td>\n<td>Check container logs for model loading errors, verify memory usage against allocation, test inference server configuration locally</td>\n<td>Implement model loading validation, add memory usage monitoring, create inference server health checks</td>\n</tr>\n<tr>\n<td>Traffic splitting not working correctly</td>\n<td>Load balancer misconfiguration, endpoint weight calculation errors, routing rule conflicts</td>\n<td>Verify traffic percentages sum to 100%, check load balancer configuration, examine routing rule precedence</td>\n<td>Implement traffic split validation, add routing rule conflict detection, create traffic distribution monitoring</td>\n</tr>\n<tr>\n<td>Canary deployment stuck in progress</td>\n<td>Health check failures for new version, automatic rollback trigger conditions, deployment orchestration bugs</td>\n<td>Check <code>DeploymentStatus</code> for new version, verify health check results, examine deployment orchestration logs</td>\n<td>Implement deployment timeout mechanisms, add health check debugging tools, create manual deployment control overrides</td>\n</tr>\n<tr>\n<td>Auto-scaling thrashing</td>\n<td>Inappropriate scaling metrics, too aggressive scaling policies, resource allocation delays</td>\n<td>Monitor scaling decision logs, check metric collection frequency, verify resource allocation timing</td>\n<td>Implement scaling decision smoothing, add scaling policy validation, create resource allocation monitoring</td>\n</tr>\n<tr>\n<td>Model serving latency spikes</td>\n<td>Cold start delays, resource contention, network connectivity issues</td>\n<td>Measure model loading time, check resource utilization during spikes, test network connectivity to inference servers</td>\n<td>Implement model warming procedures, add resource reservation for serving, create network connectivity monitoring</td>\n</tr>\n</tbody></table>\n<p><strong>Model Monitoring Issues</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Causes</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Drift detection shows false positives</td>\n<td>Inappropriate statistical thresholds, seasonal data patterns, insufficient baseline data</td>\n<td>Review drift detection parameters, analyze data patterns over longer time periods, verify baseline data quality</td>\n<td>Implement adaptive thresholds, add seasonal pattern detection, create baseline data validation procedures</td>\n</tr>\n<tr>\n<td>Prediction logging missing entries</td>\n<td>Network failures during logging, storage quota exceeded, logging buffer overflow</td>\n<td>Check network connectivity to prediction logger, verify storage space availability, examine logging buffer configuration</td>\n<td>Implement prediction logging retry mechanisms, add storage monitoring, create logging buffer scaling policies</td>\n</tr>\n<tr>\n<td>Performance metrics calculation errors</td>\n<td>Timestamp synchronization issues, missing ground truth data, aggregation window configuration errors</td>\n<td>Verify timestamp consistency across logs, check ground truth data availability, validate aggregation window settings</td>\n<td>Implement timestamp normalization, add ground truth data validation, create aggregation window optimization</td>\n</tr>\n<tr>\n<td>Alerts not firing for known issues</td>\n<td>Alert threshold misconfiguration, notification delivery failures, alert rule evaluation errors</td>\n<td>Manually trigger alert conditions, check notification channel configuration, examine alert rule evaluation logs</td>\n<td>Implement alert rule testing procedures, add notification delivery confirmation, create alert escalation mechanisms</td>\n</tr>\n<tr>\n<td>Dashboard shows stale monitoring data</td>\n<td>Data pipeline processing delays, cache invalidation issues, metric aggregation job failures</td>\n<td>Check data pipeline processing logs, verify cache invalidation timing, examine metric aggregation job status</td>\n<td>Implement real-time data processing, add cache invalidation monitoring, create metric aggregation job recovery</td>\n</tr>\n</tbody></table>\n<p><strong>Cross-Component Integration Issues</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Causes</th>\n<th>Diagnostic Steps</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Events not propagating between components</td>\n<td>Message queue failures, event serialization errors, subscriber registration issues</td>\n<td>Check event queue health, verify event payload format, examine subscriber registration logs</td>\n<td>Implement event delivery confirmation, add event serialization validation, create subscriber health monitoring</td>\n</tr>\n<tr>\n<td>API calls timing out between components</td>\n<td>Network connectivity issues, service overload, authentication token expiration</td>\n<td>Test network connectivity between services, check service resource utilization, verify authentication token validity</td>\n<td>Implement API call retry mechanisms, add service load monitoring, create token refresh automation</td>\n</tr>\n<tr>\n<td>Data inconsistency across components</td>\n<td>Transaction boundary violations, eventual consistency delays, concurrent modification conflicts</td>\n<td>Compare data state across components, check transaction isolation levels, examine conflict resolution logs</td>\n<td>Implement proper transaction boundaries, add consistency validation procedures, create conflict resolution mechanisms</td>\n</tr>\n<tr>\n<td>Circuit breaker preventing valid requests</td>\n<td>Overly aggressive failure detection, insufficient recovery time, health check implementation issues</td>\n<td>Review circuit breaker configuration, check failure detection logs, verify health check accuracy</td>\n<td>Implement circuit breaker tuning procedures, add failure pattern analysis, create health check debugging tools</td>\n</tr>\n</tbody></table>\n<h3 id=\"debugging-tools-and-techniques\">Debugging Tools and Techniques</h3>\n<p>Effective MLOps debugging requires a comprehensive toolkit that provides visibility into system behavior across multiple components and abstraction layers. Think of debugging tools like a diagnostic laboratory for distributed systems - you need different instruments to examine different types of evidence, from microscopic code-level behavior to macroscopic system-wide patterns.</p>\n<p><strong>Structured Logging Strategy</strong></p>\n<p>The foundation of MLOps debugging is structured logging that correlates activity across components using correlation IDs. Every operation should generate logs with consistent format, appropriate verbosity levels, and contextual information that helps reconstruct system behavior during incidents.</p>\n<table>\n<thead>\n<tr>\n<th>Log Level</th>\n<th>Use Cases</th>\n<th>Information Included</th>\n<th>Retention Policy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DEBUG</td>\n<td>Detailed execution flow, parameter values, intermediate calculations</td>\n<td>Function entry/exit, variable values, algorithm steps</td>\n<td>7 days, high volume</td>\n</tr>\n<tr>\n<td>INFO</td>\n<td>Normal operations, successful completions, state transitions</td>\n<td>Operation results, timing information, resource usage</td>\n<td>30 days, moderate volume</td>\n</tr>\n<tr>\n<td>WARN</td>\n<td>Recoverable errors, degraded performance, configuration issues</td>\n<td>Error conditions, fallback mechanisms, performance metrics</td>\n<td>90 days, low volume</td>\n</tr>\n<tr>\n<td>ERROR</td>\n<td>Operation failures, data corruption, service unavailability</td>\n<td>Error messages, stack traces, recovery actions</td>\n<td>1 year, critical preservation</td>\n</tr>\n<tr>\n<td>FATAL</td>\n<td>System-wide failures, data loss, security breaches</td>\n<td>Complete context, system state, emergency procedures</td>\n<td>Permanent, immediate escalation</td>\n</tr>\n</tbody></table>\n<p>The logging implementation should include correlation IDs that trace operations across component boundaries. When an experiment run begins, generate a unique correlation ID that appears in all related logs across experiment tracking, model registry, training pipeline, deployment, and monitoring components. This enables reconstructing the complete flow of operations even when failures occur in different components at different times.</p>\n<table>\n<thead>\n<tr>\n<th>Log Field</th>\n<th>Type</th>\n<th>Purpose</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>timestamp</td>\n<td>float</td>\n<td>Precise timing for sequence reconstruction</td>\n<td>1640995200.123456</td>\n</tr>\n<tr>\n<td>level</td>\n<td>str</td>\n<td>Log severity for filtering and alerting</td>\n<td>&quot;ERROR&quot;</td>\n</tr>\n<tr>\n<td>component</td>\n<td>str</td>\n<td>Source component for distributed tracing</td>\n<td>&quot;experiment-tracker&quot;</td>\n</tr>\n<tr>\n<td>correlation_id</td>\n<td>str</td>\n<td>Operation correlation across components</td>\n<td>&quot;exp_20220101_abc123&quot;</td>\n</tr>\n<tr>\n<td>operation</td>\n<td>str</td>\n<td>Specific operation being performed</td>\n<td>&quot;log_metric&quot;</td>\n</tr>\n<tr>\n<td>message</td>\n<td>str</td>\n<td>Human-readable description</td>\n<td>&quot;Failed to store metric value&quot;</td>\n</tr>\n<tr>\n<td>context</td>\n<td>dict</td>\n<td>Structured context for debugging</td>\n<td>{&quot;run_id&quot;: &quot;run_123&quot;, &quot;metric&quot;: &quot;accuracy&quot;}</td>\n</tr>\n<tr>\n<td>stack_trace</td>\n<td>str</td>\n<td>Error stack trace when applicable</td>\n<td>Full Python/Go stack trace</td>\n</tr>\n</tbody></table>\n<p><strong>Health Check and Monitoring Systems</strong></p>\n<p>Comprehensive health checking provides real-time visibility into component status and enables proactive issue detection. The <code>ComponentHealth</code> system should implement both shallow and deep health checks that validate different aspects of system functionality.</p>\n<table>\n<thead>\n<tr>\n<th>Health Check Type</th>\n<th>Validation Scope</th>\n<th>Check Frequency</th>\n<th>Failure Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Shallow</td>\n<td>Basic connectivity, process status</td>\n<td>Every 30 seconds</td>\n<td>3 consecutive failures</td>\n</tr>\n<tr>\n<td>Deep</td>\n<td>End-to-end functionality, data consistency</td>\n<td>Every 5 minutes</td>\n<td>2 consecutive failures</td>\n</tr>\n<tr>\n<td>External</td>\n<td>Dependent service availability</td>\n<td>Every 2 minutes</td>\n<td>5 consecutive failures</td>\n</tr>\n<tr>\n<td>Resource</td>\n<td>Memory, disk, CPU utilization</td>\n<td>Every 1 minute</td>\n<td>Threshold-based alerts</td>\n</tr>\n</tbody></table>\n<p>The health check implementation should provide detailed diagnostic information when checks fail, not just binary pass/fail status. For example, a database health check should report connection pool status, query response times, and any constraint violations.</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Health Check Method</th>\n<th>Success Criteria</th>\n<th>Diagnostic Information</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Tracker</td>\n<td><code>check_experiment_tracking()</code></td>\n<td>Can create run and log metric within 5 seconds</td>\n<td>Database connection status, storage availability, recent operation latency</td>\n</tr>\n<tr>\n<td>Model Registry</td>\n<td><code>check_model_registry()</code></td>\n<td>Can retrieve model version and download artifact within 10 seconds</td>\n<td>Metadata store responsiveness, artifact storage connectivity, lineage graph integrity</td>\n</tr>\n<tr>\n<td>Training Pipeline</td>\n<td><code>check_pipeline_orchestration()</code></td>\n<td>Can submit test job and receive status update within 30 seconds</td>\n<td>Kubernetes cluster status, resource availability, job queue depth</td>\n</tr>\n<tr>\n<td>Model Deployment</td>\n<td><code>check_model_serving()</code></td>\n<td>Can route request and receive prediction within 2 seconds</td>\n<td>Endpoint health, traffic routing accuracy, auto-scaler responsiveness</td>\n</tr>\n<tr>\n<td>Model Monitoring</td>\n<td><code>check_model_monitoring()</code></td>\n<td>Can log prediction and compute metrics within 5 seconds</td>\n<td>Prediction logging pipeline status, drift detection job health, alerting system connectivity</td>\n</tr>\n</tbody></table>\n<p><strong>Distributed Tracing Implementation</strong></p>\n<p>Distributed tracing provides end-to-end visibility into request flows across multiple components. Unlike logs which provide point-in-time snapshots, traces show the complete journey of operations through the system, including timing, dependencies, and error propagation paths.</p>\n<p>The tracing system should capture spans for all major operations, with parent-child relationships that reflect the actual call hierarchy. Each span should include operation metadata, timing information, and any errors encountered.</p>\n<table>\n<thead>\n<tr>\n<th>Trace Component</th>\n<th>Span Operations</th>\n<th>Metadata Captured</th>\n<th>Error Information</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>API Gateway</td>\n<td>Request routing, authentication, rate limiting</td>\n<td>HTTP method, endpoint, client ID, response code</td>\n<td>Authentication failures, rate limit violations, routing errors</td>\n</tr>\n<tr>\n<td>Experiment Tracker</td>\n<td>Parameter logging, metric storage, artifact upload</td>\n<td>Run ID, parameter count, artifact size, storage location</td>\n<td>Serialization errors, storage failures, validation errors</td>\n</tr>\n<tr>\n<td>Model Registry</td>\n<td>Version creation, stage promotion, lineage tracking</td>\n<td>Model name, version number, stage transition, lineage depth</td>\n<td>Version conflicts, approval failures, lineage corruption</td>\n</tr>\n<tr>\n<td>Pipeline Orchestrator</td>\n<td>Job submission, resource allocation, step execution</td>\n<td>Pipeline ID, step count, resource requirements, execution time</td>\n<td>Resource allocation failures, step execution errors, timeout conditions</td>\n</tr>\n<tr>\n<td>Model Deployment</td>\n<td>Traffic routing, scaling decisions, health checks</td>\n<td>Model version, traffic percentage, replica count, response time</td>\n<td>Routing failures, scaling errors, health check failures</td>\n</tr>\n<tr>\n<td>Model Monitoring</td>\n<td>Prediction logging, drift detection, alert evaluation</td>\n<td>Model name, prediction count, drift score, alert status</td>\n<td>Logging failures, calculation errors, alert delivery issues</td>\n</tr>\n</tbody></table>\n<p><strong>Performance Profiling Tools</strong></p>\n<p>Performance profiling helps identify bottlenecks and resource utilization patterns across the MLOps platform. Profiling should cover both individual component performance and cross-component interaction patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Profiling Type</th>\n<th>Measurement Focus</th>\n<th>Collection Method</th>\n<th>Analysis Tools</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU Profiling</td>\n<td>Function execution time, hot paths, blocking operations</td>\n<td>Statistical sampling every 10ms</td>\n<td>Flame graphs, call trees, execution histograms</td>\n</tr>\n<tr>\n<td>Memory Profiling</td>\n<td>Allocation patterns, memory leaks, garbage collection</td>\n<td>Heap snapshots every 5 minutes</td>\n<td>Memory growth analysis, allocation tracking, leak detection</td>\n</tr>\n<tr>\n<td>I/O Profiling</td>\n<td>Database queries, storage operations, network requests</td>\n<td>Operation timing and volume</td>\n<td>Query optimization, storage access patterns, network utilization</td>\n</tr>\n<tr>\n<td>Resource Profiling</td>\n<td>Container resource usage, cluster utilization</td>\n<td>System metrics every 30 seconds</td>\n<td>Resource efficiency analysis, capacity planning, cost optimization</td>\n</tr>\n</tbody></table>\n<p>The profiling system should automatically detect performance anomalies and generate actionable insights. For example, when database query latency increases significantly, the profiler should identify which specific queries are affected and suggest optimization strategies.</p>\n<p><strong>Debugging Dashboards and Visualization</strong></p>\n<p>Effective debugging requires visual representations of system behavior that help identify patterns, anomalies, and correlations across multiple metrics and timeframes. The debugging dashboard should provide both real-time monitoring and historical analysis capabilities.</p>\n<table>\n<thead>\n<tr>\n<th>Dashboard Section</th>\n<th>Visualization Types</th>\n<th>Data Sources</th>\n<th>Interaction Features</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>System Overview</td>\n<td>Service topology, health status, traffic flow</td>\n<td>Health checks, API metrics, trace data</td>\n<td>Component drill-down, time range selection, alert correlation</td>\n</tr>\n<tr>\n<td>Component Deep Dive</td>\n<td>Metric time series, error rates, resource usage</td>\n<td>Component logs, performance metrics, profiling data</td>\n<td>Metric correlation, anomaly detection, performance baseline comparison</td>\n</tr>\n<tr>\n<td>Operation Tracing</td>\n<td>Request flow diagrams, timing waterfalls, dependency graphs</td>\n<td>Distributed traces, span data, error propagation</td>\n<td>Trace filtering, span inspection, error root cause analysis</td>\n</tr>\n<tr>\n<td>Trend Analysis</td>\n<td>Historical patterns, capacity trends, performance degradation</td>\n<td>Aggregated metrics, long-term storage, statistical analysis</td>\n<td>Trend prediction, seasonal pattern detection, capacity planning alerts</td>\n</tr>\n</tbody></table>\n<p>The dashboard implementation should support collaborative debugging by allowing users to save and share specific views, annotate incidents with debugging notes, and create custom alerts based on complex conditions.</p>\n<p><strong>Automated Incident Response</strong></p>\n<p>Automated incident response reduces the mean time to recovery by implementing predefined procedures for common failure scenarios. The system should detect specific failure patterns and execute appropriate recovery actions without human intervention.</p>\n<table>\n<thead>\n<tr>\n<th>Incident Type</th>\n<th>Detection Criteria</th>\n<th>Automated Response</th>\n<th>Escalation Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Database Connection Loss</td>\n<td>Health check failures &gt; 3 minutes</td>\n<td>Restart database connection pool, switch to read replica</td>\n<td>Connection restoration fails after 10 minutes</td>\n</tr>\n<tr>\n<td>Storage Space Exhaustion</td>\n<td>Available space &lt; 10%</td>\n<td>Clean up temporary files, archive old artifacts</td>\n<td>Space cannot be recovered to &gt; 20%</td>\n</tr>\n<tr>\n<td>Memory Leak Detection</td>\n<td>Memory usage growth &gt; 50% in 1 hour</td>\n<td>Restart affected service instances</td>\n<td>Memory usage continues growing after restart</td>\n</tr>\n<tr>\n<td>Service Overload</td>\n<td>Error rate &gt; 10% for 5 minutes</td>\n<td>Enable circuit breakers, scale up replicas</td>\n<td>Error rate remains high after scaling</td>\n</tr>\n<tr>\n<td>Data Pipeline Failure</td>\n<td>Processing lag &gt; 2 hours</td>\n<td>Restart pipeline jobs, switch to backup processing</td>\n<td>Pipeline cannot catch up within 6 hours</td>\n</tr>\n</tbody></table>\n<h3 id=\"performance-troubleshooting\">Performance Troubleshooting</h3>\n<p>Performance troubleshooting in MLOps platforms requires understanding how different components interact under load and identifying bottlenecks that may not be apparent during normal operation. Think of performance troubleshooting like optimizing a complex assembly line - you need to identify the slowest station, understand how delays propagate downstream, and optimize the entire flow rather than just individual components.</p>\n<p><strong>Experiment Tracking Performance Issues</strong></p>\n<p>Experiment tracking performance typically degrades due to high-frequency metric logging, large artifact uploads, or inefficient query patterns. The key insight is that most performance issues stem from treating the tracking system like a high-throughput streaming system rather than a structured data repository with different access patterns.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Issue</th>\n<th>Symptoms</th>\n<th>Root Cause Analysis</th>\n<th>Optimization Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Metric logging latency spikes</td>\n<td><code>log_metric</code> calls taking &gt; 5 seconds intermittently</td>\n<td>Check database connection pool exhaustion, examine metric insertion batch sizes, analyze index usage patterns</td>\n<td>Implement metric batching, increase connection pool size, add composite indexes on (run_id, step, timestamp)</td>\n</tr>\n<tr>\n<td>Artifact upload timeouts</td>\n<td><code>log_artifact</code> fails with network timeouts, incomplete uploads</td>\n<td>Measure network bandwidth utilization, check object storage rate limits, examine concurrent upload patterns</td>\n<td>Implement chunked upload with retry, add upload progress tracking, create artifact upload queuing</td>\n</tr>\n<tr>\n<td>Experiment comparison slow queries</td>\n<td><code>compare_runs</code> takes &gt; 30 seconds for large experiments</td>\n<td>Analyze query execution plan, check for full table scans, examine result set sizes</td>\n<td>Add materialized views for common comparisons, implement query result caching, optimize metric aggregation queries</td>\n</tr>\n<tr>\n<td>Memory usage growth during long runs</td>\n<td>Python process memory usage increases continuously</td>\n<td>Profile memory allocation patterns, check for metric data accumulation, examine artifact reference retention</td>\n<td>Implement periodic metric flushing, add memory usage monitoring, create garbage collection tuning</td>\n</tr>\n<tr>\n<td>Parameter logging bottlenecks</td>\n<td>High parameter count runs causing storage delays</td>\n<td>Check parameter serialization overhead, examine database transaction sizes, analyze concurrent run impact</td>\n<td>Implement parameter compression, batch parameter storage operations, add parameter storage optimization</td>\n</tr>\n</tbody></table>\n<p>The experiment tracking component should implement adaptive performance optimizations based on usage patterns. For example, when detecting high-frequency metric logging, automatically enable batching mode to reduce database transaction overhead.</p>\n<p><strong>Model Registry Performance Bottlenecks</strong></p>\n<p>Model registry performance issues typically manifest during model version queries, artifact downloads, or lineage graph construction. The challenge is balancing data consistency with query performance, especially when dealing with large model artifacts and complex lineage relationships.</p>\n<table>\n<thead>\n<tr>\n<th>Performance Bottleneck</th>\n<th>Manifestation</th>\n<th>Diagnostic Approach</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model search query slowness</td>\n<td><code>search_models</code> taking &gt; 10 seconds with filters</td>\n<td>Profile query execution against model metadata tables, examine index coverage, analyze filter selectivity</td>\n<td>Create composite indexes on commonly filtered fields, implement search result pagination, add query optimization hints</td>\n</tr>\n<tr>\n<td>Large artifact download delays</td>\n<td>Model artifact downloads failing or taking &gt; 5 minutes</td>\n<td>Measure network throughput to storage backend, check artifact compression effectiveness, examine concurrent download impact</td>\n<td>Implement artifact compression, add download resumption capability, create content delivery network caching</td>\n</tr>\n<tr>\n<td>Lineage graph construction timeouts</td>\n<td><code>get_model_lineage</code> timing out for deep lineage chains</td>\n<td>Analyze recursive query performance, check for circular references, examine graph traversal algorithm efficiency</td>\n<td>Implement lineage graph caching, add graph depth limits, optimize recursive query structure</td>\n</tr>\n<tr>\n<td>Version creation transaction delays</td>\n<td><code>create_version</code> hanging during concurrent registrations</td>\n<td>Check for database lock contention, examine transaction isolation levels, analyze version numbering conflicts</td>\n<td>Implement optimistic locking, add version creation queuing, optimize database schema for concurrency</td>\n</tr>\n<tr>\n<td>Stage transition processing overhead</td>\n<td>Model promotion operations taking &gt; 2 minutes</td>\n<td>Profile approval workflow execution, check for synchronous processing bottlenecks, examine notification delivery delays</td>\n<td>Implement asynchronous stage transitions, add workflow step parallelization, optimize approval process structure</td>\n</tr>\n</tbody></table>\n<p>Model registry optimization should focus on separating metadata operations from artifact operations, allowing metadata queries to execute quickly while artifact operations happen asynchronously in the background.</p>\n<p><strong>Training Pipeline Orchestration Bottlenecks</strong></p>\n<p>Pipeline orchestration performance issues typically occur during resource allocation, step scheduling, or data transfer between steps. The key challenge is optimizing resource utilization while maintaining step isolation and fault tolerance.</p>\n<table>\n<thead>\n<tr>\n<th>Orchestration Bottleneck</th>\n<th>Symptoms</th>\n<th>Analysis Methodology</th>\n<th>Optimization Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Step scheduling delays</td>\n<td>Pipeline steps remaining in PENDING status for &gt; 10 minutes</td>\n<td>Examine Kubernetes cluster resource availability, check job queue depth, analyze resource request patterns</td>\n<td>Implement resource pre-allocation, add cluster autoscaling, optimize resource request sizing</td>\n</tr>\n<tr>\n<td>Data transfer between steps</td>\n<td>Inter-step data passing taking &gt; 30 minutes</td>\n<td>Profile network bandwidth usage, check storage backend performance, examine data serialization overhead</td>\n<td>Implement data streaming between steps, add data compression, optimize storage backend configuration</td>\n</tr>\n<tr>\n<td>Resource allocation conflicts</td>\n<td>Steps failing with &quot;resource exhausted&quot; errors</td>\n<td>Monitor cluster resource utilization patterns, check resource quota enforcement, analyze concurrent job impact</td>\n<td>Implement resource reservation system, add priority-based scheduling, create resource utilization forecasting</td>\n</tr>\n<tr>\n<td>Pipeline execution parallelization limits</td>\n<td>Independent steps executing sequentially instead of parallel</td>\n<td>Analyze DAG execution scheduling, check for artificial dependencies, examine resource allocation constraints</td>\n<td>Optimize DAG execution algorithm, remove unnecessary dependencies, implement gang scheduling for distributed training</td>\n</tr>\n<tr>\n<td>Checkpoint and recovery overhead</td>\n<td>Pipeline restart taking &gt; 1 hour from checkpoints</td>\n<td>Profile checkpoint size and complexity, check recovery process efficiency, examine state reconstruction time</td>\n<td>Implement incremental checkpointing, add parallel recovery processing, optimize checkpoint data structure</td>\n</tr>\n</tbody></table>\n<p>Pipeline performance optimization should focus on resource utilization efficiency and minimizing data movement overhead. The goal is to keep compute resources busy with actual training work rather than waiting for scheduling or data transfer operations.</p>\n<p><strong>Model Deployment Performance Issues</strong></p>\n<p>Model deployment performance issues typically involve inference latency, scaling responsiveness, or traffic routing overhead. The challenge is maintaining low latency and high throughput while supporting advanced deployment patterns like canary releases and A/B testing.</p>\n<table>\n<thead>\n<tr>\n<th>Deployment Performance Issue</th>\n<th>Observable Symptoms</th>\n<th>Investigation Steps</th>\n<th>Performance Tuning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inference latency degradation</td>\n<td>P99 latency &gt; 500ms for simple models</td>\n<td>Profile model loading and inference pipeline, check for resource contention, examine batching effectiveness</td>\n<td>Implement model warming procedures, optimize inference batching, add GPU utilization monitoring</td>\n</tr>\n<tr>\n<td>Auto-scaling responsiveness delays</td>\n<td>Replica scaling taking &gt; 5 minutes to respond to load</td>\n<td>Analyze scaling metric collection frequency, check scaling decision logic, examine resource allocation timing</td>\n<td>Implement predictive scaling, optimize scaling metric calculation, add scaling decision pre-warming</td>\n</tr>\n<tr>\n<td>Traffic routing overhead</td>\n<td>Load balancer adding &gt; 50ms latency</td>\n<td>Profile routing rule evaluation, check for routing table size impact, examine health check frequency</td>\n<td>Optimize routing rule structure, implement routing table caching, add health check result batching</td>\n</tr>\n<tr>\n<td>Model serving cold starts</td>\n<td>New replica startup taking &gt; 2 minutes</td>\n<td>Profile container startup time, check model loading process, examine dependency initialization overhead</td>\n<td>Implement container image optimization, add model preloading, create replica warm pool</td>\n</tr>\n<tr>\n<td>Canary deployment traffic splitting accuracy</td>\n<td>Traffic not splitting according to configured percentages</td>\n<td>Analyze load balancer session affinity, check routing algorithm implementation, examine request distribution patterns</td>\n<td>Implement consistent hashing for traffic distribution, add traffic split monitoring, optimize routing algorithm</td>\n</tr>\n</tbody></table>\n<p>Model deployment optimization should prioritize inference latency while ensuring scaling operations don&#39;t disrupt ongoing request processing. The key is implementing proper request queuing and load balancing during scaling transitions.</p>\n<p><strong>Model Monitoring Performance Challenges</strong></p>\n<p>Model monitoring performance issues typically involve prediction logging throughput, drift detection processing time, or real-time metrics calculation overhead. The challenge is processing high-volume prediction streams while maintaining low-latency alerting for critical issues.</p>\n<table>\n<thead>\n<tr>\n<th>Monitoring Performance Challenge</th>\n<th>Impact on System</th>\n<th>Diagnosis Procedure</th>\n<th>Performance Enhancement</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Prediction logging backlog</td>\n<td>Predictions not appearing in monitoring dashboard for &gt; 15 minutes</td>\n<td>Check prediction log processing queue depth, examine batch processing efficiency, analyze storage write performance</td>\n<td>Implement prediction log batching, add parallel processing pipelines, optimize storage backend for high throughput</td>\n</tr>\n<tr>\n<td>Drift detection computation delays</td>\n<td>Drift analysis results delayed by &gt; 1 hour</td>\n<td>Profile statistical calculation performance, check for algorithm optimization opportunities, examine data preprocessing overhead</td>\n<td>Implement incremental drift calculation, add computation result caching, optimize statistical algorithm implementation</td>\n</tr>\n<tr>\n<td>Real-time metrics aggregation overhead</td>\n<td>Monitoring dashboard updates delayed by &gt; 5 minutes</td>\n<td>Analyze metric aggregation query performance, check for computation bottlenecks, examine data freshness requirements</td>\n<td>Implement streaming metric aggregation, add pre-computed metric materialization, optimize aggregation time windows</td>\n</tr>\n<tr>\n<td>Alert evaluation processing delays</td>\n<td>Critical alerts delayed by &gt; 2 minutes</td>\n<td>Profile alert rule evaluation performance, check for rule complexity overhead, examine notification delivery bottlenecks</td>\n<td>Implement alert rule optimization, add alert evaluation parallelization, optimize notification delivery batching</td>\n</tr>\n<tr>\n<td>Monitoring data retention overhead</td>\n<td>Historical data queries taking &gt; 30 seconds</td>\n<td>Analyze data storage schema efficiency, check for query optimization opportunities, examine data archival effectiveness</td>\n<td>Implement data partitioning strategy, add query result caching, optimize data retention policies</td>\n</tr>\n</tbody></table>\n<p>Model monitoring optimization should focus on stream processing efficiency and ensuring critical alerts have priority over historical analysis queries. The system should maintain real-time responsiveness for active monitoring while handling batch processing for historical analysis.</p>\n<p><strong>Cross-Component Performance Integration</strong></p>\n<p>Performance issues often span multiple components, requiring system-wide optimization rather than individual component tuning. Understanding these interactions is crucial for achieving overall platform performance goals.</p>\n<table>\n<thead>\n<tr>\n<th>Integration Performance Issue</th>\n<th>Cross-Component Impact</th>\n<th>Analysis Strategy</th>\n<th>System-Wide Optimization</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Event propagation delays</td>\n<td>Operations completing in one component not reflected in others for &gt; 5 minutes</td>\n<td>Trace event flow across components, check message queue performance, examine event processing bottlenecks</td>\n<td>Implement event prioritization, add event processing parallelization, optimize message serialization</td>\n</tr>\n<tr>\n<td>API call cascading delays</td>\n<td>Single user operation triggering multiple slow API calls</td>\n<td>Profile API call chains, check for synchronous vs asynchronous processing, examine timeout and retry behavior</td>\n<td>Implement asynchronous operation processing, add API call result caching, optimize inter-component protocols</td>\n</tr>\n<tr>\n<td>Database connection pool exhaustion</td>\n<td>Multiple components competing for limited database connections</td>\n<td>Monitor connection pool utilization across components, check for connection leak patterns, examine transaction duration distribution</td>\n<td>Implement connection pool sharing, add connection usage monitoring, optimize database transaction boundaries</td>\n</tr>\n<tr>\n<td>Storage backend overload</td>\n<td>Concurrent operations from multiple components overwhelming storage systems</td>\n<td>Analyze storage operation patterns, check for hot spot identification, examine load distribution effectiveness</td>\n<td>Implement storage operation queuing, add storage backend load balancing, optimize data access patterns</td>\n</tr>\n<tr>\n<td>Resource contention during peak loads</td>\n<td>System performance degrading when multiple components under high load simultaneously</td>\n<td>Profile system resource usage patterns, check for resource allocation conflicts, examine performance isolation effectiveness</td>\n<td>Implement resource allocation prioritization, add component performance isolation, optimize resource sharing policies</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This implementation guidance provides practical tools and code frameworks for building effective debugging capabilities into your MLOps platform. The focus is on creating debugging infrastructure that integrates seamlessly with your components while providing comprehensive visibility into system behavior.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Debugging Capability</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Structured Logging</td>\n<td>Python logging module with JSON formatter</td>\n<td>Structured logging with OpenTelemetry and log correlation</td>\n</tr>\n<tr>\n<td>Health Checks</td>\n<td>Simple HTTP endpoints returning status</td>\n<td>Comprehensive health check framework with dependency validation</td>\n</tr>\n<tr>\n<td>Distributed Tracing</td>\n<td>Manual correlation ID propagation</td>\n<td>OpenTelemetry distributed tracing with Jaeger backend</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td>Python cProfile with custom analysis</td>\n<td>Continuous profiling with Pyroscope or similar tools</td>\n</tr>\n<tr>\n<td>Metrics Collection</td>\n<td>Custom metrics with Prometheus client</td>\n<td>Full observability stack with Grafana, Prometheus, and AlertManager</td>\n</tr>\n<tr>\n<td>Error Tracking</td>\n<td>Log aggregation with ELK stack</td>\n<td>Dedicated error tracking with Sentry or Rollbar integration</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  mlops_platform/\n    debugging/\n      __init__.py\n      health_checks.py          ← Component health validation\n      structured_logging.py     ← Correlation ID logging framework  \n      performance_profiler.py   ← Performance monitoring utilities\n      tracing.py               ← Distributed tracing implementation\n      incident_response.py      ← Automated recovery procedures\n      debugging_dashboard.py    ← Debugging visualization endpoints\n    components/\n      experiment_tracker/\n        health_checks.py        ← Component-specific health validation\n        performance_monitor.py   ← Component performance monitoring\n      model_registry/\n        health_checks.py\n        performance_monitor.py\n      # ... other components\n    tests/\n      debugging/\n        test_health_checks.py\n        test_incident_response.py</code></pre></div>\n\n<p><strong>Infrastructure Starter Code</strong></p>\n<p>Here&#39;s complete, production-ready infrastructure for debugging capabilities:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># mlops_platform/debugging/structured_logging.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> contextvars </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ContextVar</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Context variable for correlation ID propagation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">correlation_id_var: ContextVar[Optional[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ContextVar(</span><span style=\"color:#9ECBFF\">'correlation_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">default</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CorrelationIDFilter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">logging</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Filter</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Add correlation ID to log records.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> filter</span><span style=\"color:#E1E4E8\">(self, record):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        record.correlation_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> correlation_id_var.get() </span><span style=\"color:#F97583\">or</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StructuredFormatter</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">logging</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">Formatter</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Format logs as structured JSON with consistent fields.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> format</span><span style=\"color:#E1E4E8\">(self, record):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        log_entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"timestamp\"</span><span style=\"color:#E1E4E8\">: record.created,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"level\"</span><span style=\"color:#E1E4E8\">: record.levelname,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"component\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">getattr</span><span style=\"color:#E1E4E8\">(record, </span><span style=\"color:#9ECBFF\">'component'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"correlation_id\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">getattr</span><span style=\"color:#E1E4E8\">(record, </span><span style=\"color:#9ECBFF\">'correlation_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"operation\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">getattr</span><span style=\"color:#E1E4E8\">(record, </span><span style=\"color:#9ECBFF\">'operation'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'unknown'</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"message\"</span><span style=\"color:#E1E4E8\">: record.getMessage(),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(record, </span><span style=\"color:#9ECBFF\">'context'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log_entry[</span><span style=\"color:#9ECBFF\">\"context\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> record.context</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> record.exc_info:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            log_entry[</span><span style=\"color:#9ECBFF\">\"stack_trace\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.formatException(record.exc_info)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> json.dumps(log_entry)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> setup_structured_logging</span><span style=\"color:#E1E4E8\">(component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, log_level: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"INFO\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Configure structured logging for a component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(component_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger.setLevel(</span><span style=\"color:#79B8FF\">getattr</span><span style=\"color:#E1E4E8\">(logging, log_level))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Remove existing handlers</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> handler </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> logger.handlers[:]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logger.removeHandler(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Add structured handler</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    handler </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.StreamHandler()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    handler.setFormatter(StructuredFormatter())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    handler.addFilter(CorrelationIDFilter())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    logger.addHandler(handler)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> logger</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> set_correlation_id</span><span style=\"color:#E1E4E8\">(correlation_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Set correlation ID for current context.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlation_id_var.set(correlation_id)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generate_correlation_id</span><span style=\"color:#E1E4E8\">(prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"mlops\"</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate unique correlation ID.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(time.time())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    unique_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> str</span><span style=\"color:#E1E4E8\">(uuid.uuid4())[:</span><span style=\"color:#79B8FF\">8</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prefix</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">timestamp</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">_</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">unique_id</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperationLogger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Context manager for logging operations with correlation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, logger: logging.Logger, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">context):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logger</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> operation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> context</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __enter__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger.info(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            f</span><span style=\"color:#9ECBFF\">\"Starting </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            extra</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"operation\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.operation, </span><span style=\"color:#9ECBFF\">\"context\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.context}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __exit__</span><span style=\"color:#E1E4E8\">(self, exc_type, exc_val, exc_tb):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        duration </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.start_time</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.context[</span><span style=\"color:#9ECBFF\">\"duration_seconds\"</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> round</span><span style=\"color:#E1E4E8\">(duration, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> exc_type:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.error(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"Failed </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(exc_val)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                extra</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"operation\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.operation, </span><span style=\"color:#9ECBFF\">\"context\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.context},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                exc_info</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(exc_type, exc_val, exc_tb)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.logger.info(</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                f</span><span style=\"color:#9ECBFF\">\"Completed </span><span style=\"color:#79B8FF\">{self</span><span style=\"color:#E1E4E8\">.operation</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                extra</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"operation\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.operation, </span><span style=\"color:#9ECBFF\">\"context\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.context}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># mlops_platform/debugging/health_checks.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> asyncio</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Callable, Dict, List, Optional</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthStatus</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"healthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    DEGRADED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"degraded\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNHEALTHY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unhealthy\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNKNOWN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"unknown\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheck</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    status: HealthStatus</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    message: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    details: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HealthCheckFunction</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Wrapper for health check functions with metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: Callable[[], HealthCheck], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                 timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">, critical: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.check_func </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_func</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.timeout_seconds </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> timeout_seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.critical </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> critical</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ComponentHealth</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Manages health checks for a component.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.component_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checks: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, HealthCheckFunction] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_results: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, HealthCheck] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_check</span><span style=\"color:#E1E4E8\">(self, check_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, check_func: Callable[[], HealthCheck],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  timeout_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">, critical: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a health check function.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.checks[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthCheckFunction(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            check_name, check_func, timeout_seconds, critical</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    async</span><span style=\"color:#F97583\"> def</span><span style=\"color:#B392F0\"> run_checks</span><span style=\"color:#E1E4E8\">(self) -> List[HealthCheck]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute all health checks and return results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        results </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> check_name, check_func </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checks.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Run check with timeout</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> await</span><span style=\"color:#E1E4E8\"> asyncio.wait_for(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    asyncio.get_event_loop().run_in_executor(</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                        None</span><span style=\"color:#E1E4E8\">, check_func.check_func</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    ),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timeout</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">check_func.timeout_seconds</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(result)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.last_results[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#E1E4E8\"> asyncio.TimeoutError:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">check_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Health check timed out after </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">check_func.timeout_seconds</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">s\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"timeout_seconds\"</span><span style=\"color:#E1E4E8\">: check_func.timeout_seconds}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(result)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.last_results[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">check_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Health check failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e), </span><span style=\"color:#9ECBFF\">\"error_type\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">type</span><span style=\"color:#E1E4E8\">(e).</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                results.append(result)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.last_results[check_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> results</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_overall_status</span><span style=\"color:#E1E4E8\">(self) -> HealthStatus:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine overall component health status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_results:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNKNOWN</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        critical_checks </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> name, result </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.last_results.items()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.checks[name].critical</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(check.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> check </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> critical_checks):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#79B8FF\"> any</span><span style=\"color:#E1E4E8\">(check.status </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> check </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> critical_checks):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Example health check implementations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_database_health_check</span><span style=\"color:#E1E4E8\">(db_connection):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create health check for database connectivity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_database</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Execute simple query to verify connectivity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> db_connection.cursor()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cursor.execute(</span><span style=\"color:#9ECBFF\">\"SELECT 1\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cursor.fetchone()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            response_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> result </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> response_time </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 5.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"database_connectivity\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Database connection healthy\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"response_time_seconds\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">round</span><span style=\"color:#E1E4E8\">(response_time, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"database_connectivity\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Database response slow\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"response_time_seconds\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">round</span><span style=\"color:#E1E4E8\">(response_time, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"database_connectivity\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Database connection failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> check_database</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> create_storage_health_check</span><span style=\"color:#E1E4E8\">(storage_client):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Create health check for object storage connectivity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_storage</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test storage operations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            test_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"health_check_</span><span style=\"color:#79B8FF\">{int</span><span style=\"color:#E1E4E8\">(time.time())</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            test_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> b</span><span style=\"color:#9ECBFF\">\"health_check_data\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test write operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            storage_client.put(test_key, test_data)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test read operation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            retrieved_data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage_client.get(test_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Clean up test data</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            storage_client.delete(test_key)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            response_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time.time() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> retrieved_data </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> test_data </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> response_time </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 10.0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"storage_connectivity\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">HEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Storage operations healthy\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"response_time_seconds\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">round</span><span style=\"color:#E1E4E8\">(response_time, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"storage_connectivity\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">DEGRADED</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    message</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Storage operations slow or data mismatch\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                    details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"response_time_seconds\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">round</span><span style=\"color:#E1E4E8\">(response_time, </span><span style=\"color:#79B8FF\">3</span><span style=\"color:#E1E4E8\">)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> HealthCheck(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"storage_connectivity\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                status</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">HealthStatus.</span><span style=\"color:#79B8FF\">UNHEALTHY</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                message</span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Storage operations failed: </span><span style=\"color:#79B8FF\">{str</span><span style=\"color:#E1E4E8\">(e)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                timestamp</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">time.time(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                details</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"error\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(e)}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> check_storage</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code</strong></p>\n<p>Here are the skeleton implementations for the main debugging components:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># mlops_platform/debugging/incident_response.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> abc </span><span style=\"color:#F97583\">import</span><span style=\"color:#79B8FF\"> ABC</span><span style=\"color:#E1E4E8\">, abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RecoveryResult</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SUCCESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"success\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PARTIAL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"partial\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FAILED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"failed\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    MANUAL_REQUIRED</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"manual_required\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> RecoveryProcedure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">ABC</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for automated recovery procedures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> can_handle</span><span style=\"color:#E1E4E8\">(self, failure_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if this procedure can handle the failure type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @abstractmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_recovery</span><span style=\"color:#E1E4E8\">(self, failure_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> RecoveryResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute the recovery procedure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AutomatedRecovery</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Framework for automated failure recovery procedures.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.procedures: List[RecoveryProcedure] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.recovery_history: List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_procedure</span><span style=\"color:#E1E4E8\">(self, procedure: RecoveryProcedure):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register a recovery procedure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add procedure to the procedures list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate procedure implements required methods</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log procedure registration for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> attempt_recovery</span><span style=\"color:#E1E4E8\">(self, failure_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> RecoveryResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Attempt automated recovery for detected failure.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Find procedures that can handle this failure type using can_handle()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute procedures in priority order (implement priority system)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record recovery attempt in history with timestamp and context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return SUCCESS if any procedure succeeds, FAILED if all fail</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return MANUAL_REQUIRED for complex failures requiring human intervention</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DatabaseRecoveryProcedure</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">RecoveryProcedure</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Recovery procedure for database connection issues.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> can_handle</span><span style=\"color:#E1E4E8\">(self, failure_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True for database-related failure types</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check context contains required database connection information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> execute_recovery</span><span style=\"color:#E1E4E8\">(self, failure_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, context: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]) -> RecoveryResult:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Execute database recovery steps.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Attempt to recreate database connection pool</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test connection with simple query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: If primary database fails, attempt connection to read replica</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return SUCCESS if connection restored, FAILED otherwise</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log all recovery steps for debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># mlops_platform/debugging/performance_profiler.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> psutil</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict, deque</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Any, Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Container for performance measurement data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operation_times: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, deque] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.resource_usage: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, deque] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#F97583\">lambda</span><span style=\"color:#E1E4E8\">: deque(</span><span style=\"color:#FFAB70\">maxlen</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.error_counts: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.Lock()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operation_time</span><span style=\"color:#E1E4E8\">(self, operation: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, duration_seconds: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record operation execution time.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add duration to operation_times[operation] deque with thread safety</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update operation statistics (avg, p95, p99 percentiles)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect performance anomalies (duration > 3x average)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log performance warnings for slow operations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_resource_usage</span><span style=\"color:#E1E4E8\">(self, cpu_percent: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, memory_mb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, disk_io_mb: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record system resource utilization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add resource measurements to appropriate deques with timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate resource usage trends over time windows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Detect resource exhaustion conditions (CPU > 90%, memory > 85%)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate resource utilization alerts when thresholds exceeded</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PerformanceProfiler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive performance monitoring and profiling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, component_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.component_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> component_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.metrics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PerformanceMetrics()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring_active </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.monitoring_thread </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_monitoring</span><span style=\"color:#E1E4E8\">(self, interval_seconds: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start continuous performance monitoring.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set monitoring_active to True and create monitoring thread</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: In monitoring loop, collect CPU, memory, disk I/O every interval_seconds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use psutil to gather system resource information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record metrics using record_resource_usage method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle thread lifecycle and graceful shutdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> profile_operation</span><span style=\"color:#E1E4E8\">(self, operation_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Context manager for profiling individual operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return context manager that measures operation execution time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record start time on enter, calculate duration on exit</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle exceptions during profiled operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record operation metrics including success/failure status</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add operation context information (parameters, result size, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_performance_report</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate comprehensive performance analysis report.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate operation statistics (min, max, avg, percentiles)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze resource usage patterns and trends</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify performance bottlenecks and anomalies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate actionable performance optimization recommendations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format report as structured data for dashboard consumption</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoints</strong></p>\n<p>After implementing debugging infrastructure for each milestone:</p>\n<p><strong>Milestone 1 (Experiment Tracking) - Debugging Verification:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test structured logging</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from mlops_platform.debugging.structured_logging import setup_structured_logging, set_correlation_id</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">logger = setup_structured_logging('test-component')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">set_correlation_id('test-123')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">logger.info('Test message', extra={'operation': 'test', 'context': {'key': 'value'}})</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: JSON log entry with correlation_id, timestamp, and structured fields</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Test health checks</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">from mlops_platform.debugging.health_checks import ComponentHealth</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">import asyncio</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">health = ComponentHealth('experiment-tracker')</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">health.add_check('test', lambda: HealthCheck('test', HealthStatus.HEALTHY, 'OK', time.time(), {}))</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">results = asyncio.run(health.run_checks())</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">print([r.status.value for r in results])</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: ['healthy']</span></span></code></pre></div>\n\n<p><strong>Load Testing Checkpoint:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test experiment tracking under load</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> concurrent.futures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> load_test_experiment_tracking</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create multiple concurrent experiment runs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log parameters and metrics at high frequency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure logging latency and system resource usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify all operations complete successfully under load</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for memory leaks or resource exhaustion</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: All operations complete within latency targets, no resource exhaustion</span></span></code></pre></div>\n\n<p><strong>Debugging Tips for Common Issues:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Command</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logs missing correlation IDs</td>\n<td>Context variable not propagated across async boundaries</td>\n<td><code>grep &quot;correlation_id.*unknown&quot; /var/log/mlops.log</code></td>\n<td>Ensure <code>set_correlation_id()</code> called before async operations</td>\n</tr>\n<tr>\n<td>Health checks timing out</td>\n<td>Database/storage connectivity issues</td>\n<td><code>python -m mlops_platform.debugging.health_checks --component experiment-tracker</code></td>\n<td>Check network connectivity, increase timeout values</td>\n</tr>\n<tr>\n<td>Performance metrics missing</td>\n<td>Monitoring thread not started</td>\n<td><code>ps aux | grep performance_monitor</code></td>\n<td>Call <code>start_monitoring()</code> during component initialization</td>\n</tr>\n<tr>\n<td>Recovery procedures not executing</td>\n<td>Failure detection not triggering procedures</td>\n<td><code>tail -f /var/log/mlops.log | grep &quot;recovery_attempt&quot;</code></td>\n<td>Verify failure detection thresholds and procedure registration</td>\n</tr>\n</tbody></table>\n<p>The debugging infrastructure should integrate seamlessly with your MLOps components, providing comprehensive visibility into system behavior while maintaining performance under production loads.</p>\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section applies to all milestones (1-5) by identifying potential enhancements and scale-out scenarios that the current architecture supports. Understanding these extensions helps validate the architectural decisions and guides future development planning.</p>\n</blockquote>\n<p>The MLOps platform architecture we&#39;ve designed provides a solid foundation for enterprise machine learning operations, but the ML landscape continues to evolve rapidly. This section explores how the platform can be extended to support advanced MLOps features, scale to enterprise requirements, and integrate with the broader ML ecosystem. Think of this as a <strong>growth roadmap</strong> — just as a well-designed building can support additional floors and renovations, our modular architecture can accommodate new capabilities without fundamental restructuring.</p>\n<p>Each extension leverages the existing architectural patterns: the event-driven coordination system enables loose coupling between new and existing components, the polyglot persistence approach allows optimal data stores for new use cases, and the hexagonal architecture ensures clean integration boundaries. Understanding these potential extensions validates our design decisions and provides guidance for prioritizing future development efforts.</p>\n<h3 id=\"advanced-mlops-features\">Advanced MLOps Features</h3>\n<p>The platform&#39;s current feature set addresses core MLOps workflows, but production ML systems often require additional capabilities for data management, automated optimization, and multi-tenant operations. These advanced features build upon the existing components while introducing new architectural patterns and data flows.</p>\n<h4 id=\"feature-store-integration\">Feature Store Integration</h4>\n<p>Modern ML systems require consistent feature engineering and serving across training and inference workloads. A <strong>feature store</strong> acts like a <strong>data warehouse specifically designed for ML features</strong> — it provides a centralized repository for feature definitions, transformations, and both batch and real-time feature serving. Think of it as a <strong>feature cafeteria</strong> where data scientists can discover, reuse, and serve high-quality features without rebuilding the same transformations repeatedly.</p>\n<p>The feature store extends our architecture by introducing new entities and data flows that integrate with existing experiment tracking and model serving components. Features become first-class citizens with their own versioning, lineage tracking, and monitoring capabilities.</p>\n<p><strong>Feature Store Entity Model:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Entity</th>\n<th>Fields</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>FeatureGroup</code></td>\n<td>group_id str, name str, description str, source_table str, entity_key str, timestamp_key str, features List[FeatureDefinition], owner str, tags Dict[str, str]</td>\n<td>Groups related features from same data source</td>\n</tr>\n<tr>\n<td><code>FeatureDefinition</code></td>\n<td>feature_name str, data_type str, transformation str, validation_rules List[Rule], description str, creation_time float, last_update_time float</td>\n<td>Defines individual feature with transformation logic</td>\n</tr>\n<tr>\n<td><code>FeatureView</code></td>\n<td>view_id str, name str, feature_groups List[str], join_keys List[str], filters Dict[str, Any], ttl_hours int, description str</td>\n<td>Defines logical view joining features for specific use case</td>\n</tr>\n<tr>\n<td><code>FeatureValue</code></td>\n<td>feature_name str, entity_id str, timestamp float, value Any, feature_group_id str</td>\n<td>Individual feature value for specific entity and time</td>\n</tr>\n<tr>\n<td><code>FeatureLineage</code></td>\n<td>feature_name str, source_tables List[str], transformation_code str, dependencies List[str], created_by str</td>\n<td>Tracks feature provenance and dependencies</td>\n</tr>\n</tbody></table>\n<p>The feature store integrates with existing components through event-driven coordination. When a training pipeline requests features, the feature store publishes <code>FEATURES_REQUESTED</code> events that trigger batch feature computation. During model serving, real-time feature requests generate <code>FEATURE_SERVED</code> events that feed into monitoring for feature drift detection.</p>\n<p><strong>Feature Store API Integration:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>create_feature_group(name, source_config, features)</code></td>\n<td>name str, source_config Dict, features List[FeatureDefinition]</td>\n<td>str</td>\n<td>Register new feature group with data source</td>\n</tr>\n<tr>\n<td><code>get_training_features(feature_view, entity_ids, timestamp_range)</code></td>\n<td>feature_view str, entity_ids List[str], timestamp_range Tuple[float, float]</td>\n<td>DataFrame</td>\n<td>Retrieve point-in-time correct features for training</td>\n</tr>\n<tr>\n<td><code>get_online_features(feature_view, entity_ids)</code></td>\n<td>feature_view str, entity_ids List[str]</td>\n<td>Dict[str, Any]</td>\n<td>Retrieve latest feature values for inference</td>\n</tr>\n<tr>\n<td><code>compute_feature_statistics(feature_group, time_range)</code></td>\n<td>feature_group str, time_range Tuple[float, float]</td>\n<td>Dict[str, Any]</td>\n<td>Compute feature distribution statistics</td>\n</tr>\n<tr>\n<td><code>validate_feature_schema(feature_group, data)</code></td>\n<td>feature_group str, data Any</td>\n<td>List[ValidationError]</td>\n<td>Validate data against feature schema</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Insight</strong>: The feature store leverages the same event-driven architecture and polyglot persistence patterns as core components. Feature computation triggers use the pipeline orchestration engine, while online feature serving uses low-latency key-value stores. This architectural consistency simplifies operation and reduces cognitive load for developers.</p>\n</blockquote>\n<p><strong>Feature Store Architecture Decision:</strong></p>\n<blockquote>\n<p><strong>Decision: Hybrid Online/Offline Feature Architecture</strong></p>\n<ul>\n<li><strong>Context</strong>: ML systems need both batch features for training and real-time features for serving, with consistency requirements between the two modes</li>\n<li><strong>Options Considered</strong>: <ul>\n<li>Separate online and offline stores with manual synchronization</li>\n<li>Single unified store serving both batch and online workloads</li>\n<li>Hybrid architecture with dual writes and consistency validation</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement hybrid architecture with materialization pipelines keeping online and offline stores synchronized</li>\n<li><strong>Rationale</strong>: Provides optimal performance for each use case while maintaining consistency through automated synchronization and drift detection</li>\n<li><strong>Consequences</strong>: Requires additional infrastructure complexity but ensures feature consistency and enables point-in-time correctness</li>\n</ul>\n</blockquote>\n<h4 id=\"automated-model-selection-and-hyperparameter-optimization\">Automated Model Selection and Hyperparameter Optimization</h4>\n<p>Advanced ML teams often train hundreds of model variants to find optimal configurations. <strong>Automated Machine Learning (AutoML)</strong> capabilities transform the platform from a passive tracking system into an <strong>active optimization engine</strong> that systematically explores the model space. Think of this as evolving from a <strong>manual laboratory</strong> where scientists conduct individual experiments to an <strong>automated research facility</strong> that runs systematic optimization protocols.</p>\n<p>AutoML extends the experiment tracking and training pipeline components by introducing optimization algorithms, search space definitions, and automated resource allocation. The system becomes capable of generating experiment configurations, launching training runs, and converging on optimal model architectures.</p>\n<p><strong>AutoML Entity Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Entity</th>\n<th>Fields</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SearchSpace</code></td>\n<td>search_id str, parameter_ranges Dict[str, ParameterRange], constraints List[Constraint], optimization_metric str, direction str</td>\n<td>Defines hyperparameter search boundaries</td>\n</tr>\n<tr>\n<td><code>ParameterRange</code></td>\n<td>name str, type str, min_value Optional[float], max_value Optional[float], choices Optional[List[Any]], distribution str</td>\n<td>Individual parameter search range</td>\n</tr>\n<tr>\n<td><code>OptimizationRun</code></td>\n<td>optimization_id str, search_space_id str, algorithm str, budget_hours float, best_score float, completed_trials int, status OptimizationStatus</td>\n<td>Tracks overall optimization process</td>\n</tr>\n<tr>\n<td><code>Trial</code></td>\n<td>trial_id str, optimization_id str, parameters Dict[str, Any], score Optional[float], status TrialStatus, start_time float, end_time Optional[float]</td>\n<td>Individual model training attempt</td>\n</tr>\n<tr>\n<td><code>OptimizationStatus</code></td>\n<td>enum: RUNNING, COMPLETED, FAILED, STOPPED</td>\n<td>Status of optimization run</td>\n</tr>\n</tbody></table>\n<p>The automated optimization system integrates with existing pipeline orchestration by generating <code>Pipeline</code> definitions dynamically based on optimization algorithm recommendations. Each trial becomes a standard training pipeline execution with additional metadata linking it to the optimization run.</p>\n<p><strong>AutoML Integration APIs:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>create_search_space(name, parameter_ranges, constraints)</code></td>\n<td>name str, parameter_ranges Dict, constraints List</td>\n<td>str</td>\n<td>Define hyperparameter search space</td>\n</tr>\n<tr>\n<td><code>start_optimization(search_space_id, algorithm, budget)</code></td>\n<td>search_space_id str, algorithm str, budget ResourceBudget</td>\n<td>str</td>\n<td>Launch automated optimization run</td>\n</tr>\n<tr>\n<td><code>suggest_trial_parameters(optimization_id)</code></td>\n<td>optimization_id str</td>\n<td>Dict[str, Any]</td>\n<td>Get next parameter configuration to try</td>\n</tr>\n<tr>\n<td><code>report_trial_result(trial_id, score, metadata)</code></td>\n<td>trial_id str, score float, metadata Dict</td>\n<td>bool</td>\n<td>Report trial completion and performance</td>\n</tr>\n<tr>\n<td><code>get_optimization_status(optimization_id)</code></td>\n<td>optimization_id str</td>\n<td>OptimizationRun</td>\n<td>Check optimization progress and best results</td>\n</tr>\n</tbody></table>\n<p>The optimization algorithms leverage Bayesian optimization, evolutionary strategies, or neural architecture search depending on the problem characteristics. The system maintains a <strong>surrogate model</strong> of the parameter-performance relationship that guides exploration toward promising regions of the search space.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: AutoML capabilities transform the platform from reactive to proactive. Instead of just tracking what users do, the system actively suggests improvements and automates tedious hyperparameter tuning. This shift requires careful resource management to prevent optimization runs from consuming all cluster capacity.</p>\n</blockquote>\n<h4 id=\"multi-tenant-support-and-resource-isolation\">Multi-Tenant Support and Resource Isolation</h4>\n<p>Enterprise MLOps platforms serve multiple teams with varying security, compliance, and resource requirements. <strong>Multi-tenancy</strong> transforms the platform from a single-team tool into a <strong>shared service bureau</strong> that provides isolated environments while maximizing resource utilization. Think of this evolution like moving from a <strong>private workshop</strong> to a <strong>co-working space</strong> with private offices, shared facilities, and usage-based billing.</p>\n<p>Multi-tenancy introduces hierarchical resource management, fine-grained access controls, and tenant-specific configuration while maintaining the unified operational model that makes the platform valuable.</p>\n<p><strong>Multi-Tenancy Entity Model:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Entity</th>\n<th>Fields</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>Tenant</code></td>\n<td>tenant_id str, name str, subscription_tier str, resource_quotas Dict[str, float], billing_account str, created_at float, status TenantStatus</td>\n<td>Top-level tenant organization</td>\n</tr>\n<tr>\n<td><code>Workspace</code></td>\n<td>workspace_id str, tenant_id str, name str, description str, members List[WorkspaceMember], resource_allocation Dict[str, float]</td>\n<td>Project workspace within tenant</td>\n</tr>\n<tr>\n<td><code>WorkspaceMember</code></td>\n<td>user_id str, workspace_id str, role WorkspaceRole, permissions List[Permission], added_at float</td>\n<td>User access to workspace</td>\n</tr>\n<tr>\n<td><code>ResourceQuota</code></td>\n<td>quota_id str, tenant_id str, resource_type str, limit_value float, current_usage float, enforcement_policy str</td>\n<td>Resource usage limits and tracking</td>\n</tr>\n<tr>\n<td><code>TenantStatus</code></td>\n<td>enum: ACTIVE, SUSPENDED, TRIAL, DEACTIVATED</td>\n<td>Tenant account status</td>\n</tr>\n<tr>\n<td><code>WorkspaceRole</code></td>\n<td>enum: OWNER, ADMIN, CONTRIBUTOR, VIEWER</td>\n<td>User role in workspace</td>\n</tr>\n</tbody></table>\n<p>Multi-tenant isolation operates at multiple architectural layers. Data isolation ensures tenants cannot access each other&#39;s experiments, models, or pipelines. Resource isolation prevents one tenant from monopolizing compute resources. Network isolation restricts inter-tenant communication in shared infrastructure.</p>\n<p><strong>Multi-Tenancy Access Control:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>create_tenant(name, subscription_config, quotas)</code></td>\n<td>name str, subscription_config Dict, quotas Dict[str, float]</td>\n<td>str</td>\n<td>Create new tenant with resource limits</td>\n</tr>\n<tr>\n<td><code>create_workspace(tenant_id, name, initial_members)</code></td>\n<td>tenant_id str, name str, initial_members List[WorkspaceMember]</td>\n<td>str</td>\n<td>Create workspace within tenant</td>\n</tr>\n<tr>\n<td><code>check_access(user_id, resource_type, resource_id, action)</code></td>\n<td>user_id str, resource_type str, resource_id str, action str</td>\n<td>bool</td>\n<td>Verify user permissions for resource action</td>\n</tr>\n<tr>\n<td><code>allocate_resources(workspace_id, resource_request)</code></td>\n<td>workspace_id str, resource_request ResourceSpec</td>\n<td>ResourceAllocation</td>\n<td>Reserve resources with quota enforcement</td>\n</tr>\n<tr>\n<td><code>track_usage(tenant_id, resource_type, usage_amount)</code></td>\n<td>tenant_id str, resource_type str, usage_amount float</td>\n<td>bool</td>\n<td>Record resource consumption for billing</td>\n</tr>\n</tbody></table>\n<p>The multi-tenant architecture introduces <strong>hierarchical namespacing</strong> where all resources include tenant and workspace identifiers in their paths. For example, model artifacts are stored as <code>tenants/{tenant_id}/workspaces/{workspace_id}/models/{model_name}/versions/{version}/artifacts/</code> ensuring complete isolation while maintaining the familiar model registry interface.</p>\n<blockquote>\n<p><strong>Architecture Decision: Shared Infrastructure with Logical Isolation</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to support multiple tenants while controlling infrastructure costs and operational complexity</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Physical isolation with dedicated infrastructure per tenant</li>\n<li>Logical isolation with shared infrastructure and access controls</li>\n<li>Hybrid approach with dedicated resources for sensitive workloads</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement logical isolation with namespace-based separation and resource quotas</li>\n<li><strong>Rationale</strong>: Maximizes resource utilization while providing adequate security through access controls and audit logging</li>\n<li><strong>Consequences</strong>: Requires sophisticated access control implementation but enables cost-effective multi-tenancy with flexibility for dedicated resources when needed</li>\n</ul>\n</blockquote>\n<p><strong>Common Pitfalls in Advanced Features:</strong></p>\n<p>⚠️ <strong>Pitfall: Feature Store Data Consistency</strong>\nAdding a feature store without proper consistency guarantees can lead to training-serving skew where models see different feature values during training versus inference. This happens when online and offline feature stores drift apart due to failed synchronization or timing differences. Prevent this by implementing feature store drift monitoring that compares online and offline feature distributions, and use feature lineage tracking to ensure the same transformation code runs in both batch and streaming contexts.</p>\n<p>⚠️ <strong>Pitfall: AutoML Resource Exhaustion</strong>\nAutomated hyperparameter optimization can consume unlimited cluster resources if not properly constrained, starving other users of compute capacity. This occurs when optimization algorithms launch too many parallel trials or don&#39;t respect resource quotas. Prevent this by implementing dynamic resource budgets that adjust based on cluster utilization, setting maximum parallel trial limits per optimization run, and integrating with the multi-tenant resource quota system.</p>\n<p>⚠️ <strong>Pitfall: Multi-Tenant Data Leakage</strong>\nImproper tenant isolation can leak sensitive data between organizations through shared caches, logging systems, or artifact storage. This happens when tenant identifiers aren&#39;t properly validated at every access point or when aggregated metrics inadvertently reveal tenant-specific information. Prevent this by implementing defense-in-depth with tenant validation at API boundaries, database row-level security, and regular security audits of cross-tenant data flows.</p>\n<h3 id=\"scale-and-performance-extensions\">Scale and Performance Extensions</h3>\n<p>As ML teams grow and model complexity increases, the platform must scale beyond single-datacenter deployments to support global operations, edge computing, and massive training workloads. These extensions stress-test the architectural decisions and often require fundamental changes to data distribution and coordination patterns.</p>\n<h4 id=\"multi-region-deployments-and-global-model-serving\">Multi-Region Deployments and Global Model Serving</h4>\n<p>Global organizations need ML models deployed close to users for optimal latency while maintaining consistency across regions. <strong>Multi-region deployments</strong> transform the platform from a <strong>centralized service</strong> to a <strong>distributed federation</strong> of regional clusters with sophisticated coordination mechanisms. Think of this like evolving from a <strong>single headquarters</strong> to a <strong>multinational corporation</strong> with regional offices that operate independently while maintaining global coordination.</p>\n<p>Multi-region architecture introduces challenges around data replication, eventual consistency, conflict resolution, and cross-region network partitions that don&#39;t exist in single-region deployments.</p>\n<p><strong>Multi-Region Architecture Components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Regional Scope</th>\n<th>Global Scope</th>\n<th>Coordination Mechanism</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model Registry</td>\n<td>Regional cache with async sync</td>\n<td>Global authoritative store</td>\n<td>Event-driven replication with conflict resolution</td>\n</tr>\n<tr>\n<td>Experiment Tracking</td>\n<td>Regional storage</td>\n<td>Global aggregation</td>\n<td>Eventual consistency with merge strategies</td>\n</tr>\n<tr>\n<td>Pipeline Orchestration</td>\n<td>Regional execution clusters</td>\n<td>Global scheduling coordination</td>\n<td>Leader election with regional failover</td>\n</tr>\n<tr>\n<td>Model Serving</td>\n<td>Regional endpoints</td>\n<td>Global traffic routing</td>\n<td>DNS-based routing with health monitoring</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Regional data collection</td>\n<td>Global alerting and dashboards</td>\n<td>Cross-region metric aggregation</td>\n</tr>\n</tbody></table>\n<p>The multi-region coordination system extends the existing <code>EventCoordinator</code> to handle cross-region message delivery with retries, ordering guarantees, and partition tolerance. Regional failures cannot block other regions, but eventual consistency ensures global coherence when connectivity is restored.</p>\n<p><strong>Global Coordination APIs:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>replicate_model_version(model_name, version, target_regions)</code></td>\n<td>model_name str, version str, target_regions List[str]</td>\n<td>ReplicationStatus</td>\n<td>Replicate model to specified regions</td>\n</tr>\n<tr>\n<td><code>get_nearest_endpoint(model_name, client_location)</code></td>\n<td>model_name str, client_location Location</td>\n<td>ModelEndpoint</td>\n<td>Return closest healthy model endpoint</td>\n</tr>\n<tr>\n<td><code>sync_experiment_metadata(experiment_id, source_region)</code></td>\n<td>experiment_id str, source_region str</td>\n<td>SyncResult</td>\n<td>Synchronize experiment data across regions</td>\n</tr>\n<tr>\n<td><code>resolve_version_conflict(model_name, conflicting_versions)</code></td>\n<td>model_name str, conflicting_versions List[ModelVersion]</td>\n<td>ModelVersion</td>\n<td>Resolve concurrent model updates</td>\n</tr>\n<tr>\n<td><code>check_global_consistency(resource_type, resource_id)</code></td>\n<td>resource_type str, resource_id str</td>\n<td>ConsistencyReport</td>\n<td>Verify consistency across regions</td>\n</tr>\n</tbody></table>\n<p>Multi-region deployments require <strong>split-brain protection</strong> to handle network partitions where regions cannot communicate. The system uses consensus protocols for critical operations like model promotion while allowing regions to operate independently for read-heavy workloads like experiment tracking and model serving.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Multi-region deployments expose the tension between consistency and availability. The platform must gracefully degrade during network partitions while ensuring critical safety properties like preventing conflicting model versions from serving simultaneously in different regions.</p>\n</blockquote>\n<h4 id=\"edge-computing-and-model-deployment\">Edge Computing and Model Deployment</h4>\n<p>IoT devices, mobile applications, and low-latency scenarios require ML models deployed at the <strong>network edge</strong> rather than centralized cloud infrastructure. <strong>Edge deployment</strong> transforms the platform from a <strong>cloud-centric service</strong> to a <strong>hierarchical distribution network</strong> that pushes intelligence closer to data sources. Think of this like evolving from <strong>centralized broadcasting</strong> to a <strong>content delivery network</strong> with local caching and adaptive streaming.</p>\n<p>Edge deployments introduce constraints around limited compute resources, intermittent connectivity, model size restrictions, and autonomous operation when disconnected from the central platform.</p>\n<p><strong>Edge Deployment Architecture:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Edge Capability</th>\n<th>Sync Requirements</th>\n<th>Offline Operation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Model Serving</td>\n<td>Optimized inference engines</td>\n<td>Model updates via sync protocol</td>\n<td>Full autonomy with cached models</td>\n</tr>\n<tr>\n<td>Prediction Logging</td>\n<td>Local buffering with batch upload</td>\n<td>Upload when connectivity available</td>\n<td>Store-and-forward with compression</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Local health checks and basic metrics</td>\n<td>Aggregate metrics upload</td>\n<td>Alert on local thresholds only</td>\n</tr>\n<tr>\n<td>Model Updates</td>\n<td>Incremental model patching</td>\n<td>Delta synchronization</td>\n<td>Version rollback capability</td>\n</tr>\n</tbody></table>\n<p>Edge model serving requires <strong>model optimization</strong> techniques that reduce memory footprint and inference latency while maintaining acceptable accuracy. The platform automatically generates optimized model variants using quantization, pruning, and knowledge distillation based on edge device capabilities.</p>\n<p><strong>Edge Optimization Pipeline:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Optimization</th>\n<th>Input Requirements</th>\n<th>Output Characteristics</th>\n<th>Quality Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Quantization</td>\n<td>Float32 model weights</td>\n<td>Int8 weights with 4x size reduction</td>\n<td>1-3% accuracy loss typical</td>\n</tr>\n<tr>\n<td>Pruning</td>\n<td>Dense neural network</td>\n<td>Sparse network with 50-90% weight reduction</td>\n<td>2-5% accuracy loss with careful tuning</td>\n</tr>\n<tr>\n<td>Knowledge Distillation</td>\n<td>Large teacher model</td>\n<td>Small student model with similar behavior</td>\n<td>5-15% accuracy loss for 10x size reduction</td>\n</tr>\n<tr>\n<td>Model Compilation</td>\n<td>Framework-specific model</td>\n<td>Optimized binary for target hardware</td>\n<td>No accuracy loss, 2-5x speed improvement</td>\n</tr>\n</tbody></table>\n<p>The edge synchronization protocol handles intermittent connectivity by batching updates, compressing data transfers, and implementing conflict-free replicated data types (CRDTs) for prediction logs and monitoring metrics.</p>\n<p><strong>Edge Deployment APIs:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>create_edge_deployment(model_name, edge_config, optimization_spec)</code></td>\n<td>model_name str, edge_config EdgeConfig, optimization_spec OptimizationSpec</td>\n<td>str</td>\n<td>Deploy optimized model to edge devices</td>\n</tr>\n<tr>\n<td><code>sync_edge_data(edge_device_id, data_batch)</code></td>\n<td>edge_device_id str, data_batch CompressedData</td>\n<td>SyncAcknowledgment</td>\n<td>Upload batched data from edge device</td>\n</tr>\n<tr>\n<td><code>push_model_update(device_group, model_delta)</code></td>\n<td>device_group str, model_delta ModelDelta</td>\n<td>PushStatus</td>\n<td>Send incremental model update to device group</td>\n</tr>\n<tr>\n<td><code>check_edge_health(device_id)</code></td>\n<td>device_id str</td>\n<td>EdgeHealthStatus</td>\n<td>Monitor edge device operational status</td>\n</tr>\n<tr>\n<td><code>rollback_edge_model(device_id, target_version)</code></td>\n<td>device_id str, target_version str</td>\n<td>RollbackResult</td>\n<td>Revert edge device to previous model version</td>\n</tr>\n</tbody></table>\n<h4 id=\"large-scale-training-orchestration\">Large-Scale Training Orchestration</h4>\n<p>Advanced ML models require training across hundreds or thousands of GPUs with sophisticated parallelization strategies. <strong>Large-scale training</strong> transforms the platform from supporting <strong>individual researchers</strong> to enabling <strong>industrial-scale model development</strong> comparable to training foundation models. Think of this evolution like moving from a <strong>university chemistry lab</strong> to a <strong>pharmaceutical manufacturing plant</strong> with automated processes and quality controls.</p>\n<p>Large-scale training introduces challenges around gang scheduling, gradient synchronization, fault tolerance at scale, and dynamic resource allocation that stress-test the pipeline orchestration component.</p>\n<p><strong>Large-Scale Training Components:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Single-Node</th>\n<th>Multi-Node</th>\n<th>Large-Scale (100+ nodes)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Resource Scheduling</td>\n<td>Simple container allocation</td>\n<td>Gang scheduling for distributed jobs</td>\n<td>Hierarchical scheduling with priority queues</td>\n</tr>\n<tr>\n<td>Gradient Synchronization</td>\n<td>In-memory parameter updates</td>\n<td>AllReduce communication patterns</td>\n<td>Hierarchical AllReduce with compression</td>\n</tr>\n<tr>\n<td>Fault Tolerance</td>\n<td>Checkpoint to persistent storage</td>\n<td>Coordinated checkpointing</td>\n<td>Automatic failure detection and recovery</td>\n</tr>\n<tr>\n<td>Data Pipeline</td>\n<td>Local data loading</td>\n<td>Distributed data sharding</td>\n<td>Parallel data preprocessing with caching</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Basic GPU utilization</td>\n<td>Per-node communication metrics</td>\n<td>System-wide bottleneck detection</td>\n</tr>\n</tbody></table>\n<p>The large-scale training orchestrator extends the existing <code>Pipeline</code> and <code>Step</code> abstractions with distributed execution primitives and collective communication operations. Training steps become <strong>distributed operations</strong> with explicit parallelization strategies rather than single-container executions.</p>\n<p><strong>Distributed Training Extensions:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Entity</th>\n<th>Fields</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>DistributedStep</code></td>\n<td>step_id str, parallelism_strategy str, node_count int, processes_per_node int, communication_backend str, synchronization_mode str</td>\n<td>Training step with distributed execution</td>\n</tr>\n<tr>\n<td><code>CollectiveOperation</code></td>\n<td>operation_type str, participants List[str], data_size_bytes int, compression str, timeout_seconds int</td>\n<td>Coordinated multi-node operation</td>\n</tr>\n<tr>\n<td><code>TrainingTopology</code></td>\n<td>topology_type str, node_assignments Dict[str, List[str]], bandwidth_matrix Dict[str, Dict[str, float]]</td>\n<td>Physical layout of training cluster</td>\n</tr>\n<tr>\n<td><code>CheckpointStrategy</code></td>\n<td>frequency_steps int, storage_location str, compression bool, async_upload bool, retention_policy str</td>\n<td>Fault tolerance configuration</td>\n</tr>\n</tbody></table>\n<p>Large-scale training requires <strong>hierarchical fault tolerance</strong> where node failures don&#39;t restart the entire job. The system implements elastic training that can continue with reduced parallelism when nodes fail and scale back up when replacement resources become available.</p>\n<p><strong>Large-Scale Training APIs:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>schedule_distributed_training(training_spec, resource_requirements)</code></td>\n<td>training_spec DistributedTrainingSpec, resource_requirements ResourceSpec</td>\n<td>str</td>\n<td>Schedule multi-node training job</td>\n</tr>\n<tr>\n<td><code>create_checkpoint(job_id, checkpoint_metadata)</code></td>\n<td>job_id str, checkpoint_metadata Dict</td>\n<td>CheckpointInfo</td>\n<td>Save distributed training state</td>\n</tr>\n<tr>\n<td><code>resume_from_checkpoint(checkpoint_id, new_resource_spec)</code></td>\n<td>checkpoint_id str, new_resource_spec ResourceSpec</td>\n<td>str</td>\n<td>Restart training from saved checkpoint</td>\n</tr>\n<tr>\n<td><code>monitor_collective_operations(job_id)</code></td>\n<td>job_id str</td>\n<td>CollectiveMetrics</td>\n<td>Track communication performance</td>\n</tr>\n<tr>\n<td><code>handle_node_failure(job_id, failed_nodes)</code></td>\n<td>job_id str, failed_nodes List[str]</td>\n<td>RecoveryPlan</td>\n<td>Respond to node failures during training</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Hierarchical Training Coordination</strong></p>\n<ul>\n<li><strong>Context</strong>: Large-scale training requires coordination across hundreds of nodes while maintaining fault tolerance and performance</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Centralized parameter server architecture with bottleneck risks</li>\n<li>Fully decentralized peer-to-peer coordination with complex failure handling</li>\n<li>Hierarchical coordination with regional aggregators and global coordination</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement hierarchical coordination with tree-based aggregation and elastic scaling</li>\n<li><strong>Rationale</strong>: Balances coordination efficiency with fault tolerance while supporting elastic scaling based on resource availability</li>\n<li><strong>Consequences</strong>: Requires sophisticated topology-aware scheduling but provides optimal performance and resilience for large-scale workloads</li>\n</ul>\n</blockquote>\n<p><strong>Common Pitfalls in Scale Extensions:</strong></p>\n<p>⚠️ <strong>Pitfall: Cross-Region Consistency Violations</strong>\nMulti-region deployments can serve inconsistent model versions if replication delays cause some regions to lag behind during model updates. This creates subtle bugs where the same input produces different outputs depending on which region serves the request. Prevent this by implementing global model version coordination with rollout controls that prevent serving until all target regions confirm successful deployment.</p>\n<p>⚠️ <strong>Pitfall: Edge Device Resource Exhaustion</strong>\nEdge deployments often fail when optimized models still exceed device memory or compute capabilities, especially when multiple models run simultaneously on the same device. This leads to out-of-memory crashes or unacceptably slow inference times. Prevent this by implementing device capability profiling that measures actual resource consumption and automatically selects appropriate optimization levels based on measured device performance.</p>\n<p>⚠️ <strong>Pitfall: Large-Scale Training Communication Bottlenecks</strong>\nLarge-scale training can become communication-bound when gradient synchronization dominates training time, especially with high-dimensional models or slow network connections. This manifests as poor GPU utilization despite abundant compute resources. Prevent this by implementing gradient compression, overlapping communication with computation, and adaptive batching based on measured network bandwidth.</p>\n<h3 id=\"ecosystem-integrations\">Ecosystem Integrations</h3>\n<p>The MLOps platform operates within a broader ecosystem of ML frameworks, cloud services, and third-party tools. <strong>Deep ecosystem integration</strong> transforms the platform from an <strong>isolated solution</strong> to a <strong>central hub</strong> that orchestrates the entire ML toolchain. Think of this evolution like moving from a <strong>standalone application</strong> to an <strong>operating system</strong> that provides infrastructure for diverse applications while maintaining compatibility and interoperability.</p>\n<p>Successful ecosystem integration requires understanding the interaction patterns, data formats, and operational models of popular ML tools while maintaining the platform&#39;s architectural integrity and avoiding tight coupling to specific vendor solutions.</p>\n<h4 id=\"ml-framework-integration\">ML Framework Integration</h4>\n<p>Modern ML teams use diverse frameworks like TensorFlow, PyTorch, Scikit-learn, XGBoost, and emerging frameworks for specific domains. <strong>Framework integration</strong> ensures the platform provides value regardless of the underlying ML technology choices. This requires <strong>framework-agnostic abstractions</strong> while supporting framework-specific optimizations.</p>\n<p>The integration strategy uses <strong>adapter patterns</strong> that translate between the platform&#39;s common interfaces and framework-specific APIs. Each framework adapter handles model serialization, metadata extraction, and execution environment setup while exposing a consistent interface to the core platform components.</p>\n<p><strong>Framework Integration Architecture:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Framework</th>\n<th>Model Format</th>\n<th>Metadata Extraction</th>\n<th>Runtime Requirements</th>\n<th>Serving Integration</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>TensorFlow</td>\n<td>SavedModel format</td>\n<td>TensorFlow metadata API</td>\n<td>TensorFlow Serving</td>\n<td>Native TF Serving support</td>\n</tr>\n<tr>\n<td>PyTorch</td>\n<td>TorchScript or pickle</td>\n<td>Manual metadata registration</td>\n<td>TorchServe container</td>\n<td>TorchServe integration</td>\n</tr>\n<tr>\n<td>Scikit-learn</td>\n<td>Pickle with joblib</td>\n<td>Scikit-learn introspection</td>\n<td>Python runtime</td>\n<td>Custom serving wrapper</td>\n</tr>\n<tr>\n<td>XGBoost</td>\n<td>XGB binary format</td>\n<td>Booster introspection</td>\n<td>XGBoost library</td>\n<td>Native XGBoost prediction</td>\n</tr>\n<tr>\n<td>ONNX</td>\n<td>ONNX model format</td>\n<td>ONNX metadata</td>\n<td>ONNX Runtime</td>\n<td>Universal ONNX serving</td>\n</tr>\n</tbody></table>\n<p>Framework adapters implement the standard <code>ModelArtifactStore</code> interface while handling framework-specific serialization and validation. This allows the model registry to support any framework through a pluggable adapter system without modifying core platform code.</p>\n<p><strong>Framework Adapter Interface:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>serialize_model(model_object, metadata)</code></td>\n<td>model_object Any, metadata Dict</td>\n<td>SerializedModel</td>\n<td>Convert framework model to storage format</td>\n</tr>\n<tr>\n<td><code>deserialize_model(model_data, target_format)</code></td>\n<td>model_data bytes, target_format str</td>\n<td>Any</td>\n<td>Load model from storage in requested format</td>\n</tr>\n<tr>\n<td><code>extract_model_signature(model_object)</code></td>\n<td>model_object Any</td>\n<td>ModelSignature</td>\n<td>Extract input/output schema from model</td>\n</tr>\n<tr>\n<td><code>validate_model_compatibility(model_data, runtime_env)</code></td>\n<td>model_data bytes, runtime_env Dict</td>\n<td>ValidationResult</td>\n<td>Check if model can run in target environment</td>\n</tr>\n<tr>\n<td><code>optimize_for_serving(model_data, optimization_config)</code></td>\n<td>model_data bytes, optimization_config Dict</td>\n<td>bytes</td>\n<td>Apply serving optimizations like quantization</td>\n</tr>\n</tbody></table>\n<p>The framework integration system automatically detects model types and selects appropriate adapters based on file extensions, metadata markers, or explicit framework specifications. This enables seamless workflows where data scientists can register models without worrying about platform-specific conversion requirements.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Framework adapters provide a translation layer that preserves framework-specific optimizations while exposing platform-standard interfaces. This pattern enables supporting new frameworks through plugin development without modifying core platform components.</p>\n</blockquote>\n<h4 id=\"cloud-platform-integration\">Cloud Platform Integration</h4>\n<p>Enterprise ML teams often use cloud-managed services for specific capabilities like data warehouses, managed Kubernetes clusters, or specialized ML services. <strong>Cloud integration</strong> extends the platform&#39;s reach by <strong>federating with external services</strong> rather than rebuilding equivalent capabilities. Think of this like <strong>diplomatic relations</strong> where the platform maintains sovereignty while establishing treaties for specific collaborations.</p>\n<p>Cloud integrations follow the <strong>adapter pattern</strong> similar to framework integrations, but focus on operational concerns like authentication, resource provisioning, and service lifecycle management rather than data format conversion.</p>\n<p><strong>Cloud Service Integration Patterns:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Integration Type</th>\n<th>Authentication</th>\n<th>Resource Management</th>\n<th>Data Flow</th>\n<th>Service Discovery</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Sources</td>\n<td>Cloud IAM roles</td>\n<td>Query-based access</td>\n<td>Pull data for training</td>\n<td>Service endpoint configuration</td>\n</tr>\n<tr>\n<td>Compute Clusters</td>\n<td>Kubernetes RBAC</td>\n<td>Node pool management</td>\n<td>Push workloads to cluster</td>\n<td>Cluster API integration</td>\n</tr>\n<tr>\n<td>Storage Services</td>\n<td>Cloud credentials</td>\n<td>Bucket lifecycle policies</td>\n<td>Stream artifacts bidirectionally</td>\n<td>SDK-based discovery</td>\n</tr>\n<tr>\n<td>ML Services</td>\n<td>API key management</td>\n<td>Usage quota monitoring</td>\n<td>REST API integration</td>\n<td>Service catalog lookup</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td>Service account tokens</td>\n<td>Dashboard provisioning</td>\n<td>Push metrics and logs</td>\n<td>Metrics endpoint registration</td>\n</tr>\n</tbody></table>\n<p>The cloud integration framework provides <strong>credential management</strong>, <strong>service discovery</strong>, and <strong>lifecycle coordination</strong> capabilities that individual adapters can leverage. This prevents each cloud adapter from implementing its own authentication and configuration management.</p>\n<p><strong>Cloud Integration APIs:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>register_cloud_service(service_type, credentials, config)</code></td>\n<td>service_type str, credentials CloudCredentials, config Dict</td>\n<td>str</td>\n<td>Register cloud service for platform use</td>\n</tr>\n<tr>\n<td><code>provision_compute_cluster(cloud_provider, cluster_spec)</code></td>\n<td>cloud_provider str, cluster_spec ClusterSpec</td>\n<td>ClusterInfo</td>\n<td>Create managed compute cluster</td>\n</tr>\n<tr>\n<td><code>sync_data_from_warehouse(warehouse_config, query, destination)</code></td>\n<td>warehouse_config Dict, query str, destination str</td>\n<td>SyncJob</td>\n<td>Pull training data from cloud warehouse</td>\n</tr>\n<tr>\n<td><code>push_metrics_to_service(service_name, metrics_batch)</code></td>\n<td>service_name str, metrics_batch List[Metric]</td>\n<td>bool</td>\n<td>Send metrics to cloud monitoring</td>\n</tr>\n<tr>\n<td><code>backup_artifacts_to_cloud(artifact_paths, cloud_storage_config)</code></td>\n<td>artifact_paths List[str], cloud_storage_config Dict</td>\n<td>BackupJob</td>\n<td>Replicate artifacts to cloud storage</td>\n</tr>\n</tbody></table>\n<p>Cloud integrations handle <strong>credential rotation</strong>, <strong>service health monitoring</strong>, and <strong>cost optimization</strong> through automated policies. For example, the compute cluster integration can automatically scale down expensive GPU nodes during periods of low utilization while maintaining rapid scale-up capability.</p>\n<blockquote>\n<p><strong>Architecture Decision: Federation Over Replication</strong></p>\n<ul>\n<li><strong>Context</strong>: Cloud providers offer specialized ML services that would be expensive and time-consuming to replicate within the platform</li>\n<li><strong>Options Considered</strong>:<ul>\n<li>Build equivalent capabilities within the platform for complete independence</li>\n<li>Integrate with cloud services through APIs while maintaining platform control</li>\n<li>Use cloud services as the primary platform with custom extensions</li>\n</ul>\n</li>\n<li><strong>Decision</strong>: Implement federation through standardized adapters that preserve platform workflows while leveraging cloud capabilities</li>\n<li><strong>Rationale</strong>: Maximizes value from cloud investments while maintaining operational consistency and avoiding vendor lock-in</li>\n<li><strong>Consequences</strong>: Requires sophisticated adapter development but provides flexibility and cost optimization opportunities</li>\n</ul>\n</blockquote>\n<h4 id=\"third-party-tool-integration\">Third-Party Tool Integration</h4>\n<p>ML teams use diverse tools for data preparation, model development, deployment automation, and operational monitoring. <strong>Tool ecosystem integration</strong> creates <strong>workflows that span multiple tools</strong> while maintaining the platform as the <strong>system of record</strong> for ML artifacts and metadata. Think of this like <strong>API orchestration</strong> where the platform conducts a symphony of specialized tools rather than replacing every instrument.</p>\n<p>Third-party integrations focus on <strong>data synchronization</strong>, <strong>workflow triggering</strong>, and <strong>metadata federation</strong> to ensure information flows seamlessly between tools while avoiding duplicate work or inconsistent states.</p>\n<p><strong>Common Integration Categories:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Tool Category</th>\n<th>Integration Pattern</th>\n<th>Data Synchronization</th>\n<th>Event Coordination</th>\n<th>Examples</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Data Preparation</td>\n<td>Pipeline triggers</td>\n<td>Export feature engineering</td>\n<td>Trigger on data updates</td>\n<td>dbt, Apache Airflow, Prefect</td>\n</tr>\n<tr>\n<td>Model Development</td>\n<td>Artifact sync</td>\n<td>Import notebooks and models</td>\n<td>Sync on experiment completion</td>\n<td>Jupyter, Databricks, SageMaker</td>\n</tr>\n<tr>\n<td>Deployment Automation</td>\n<td>CD pipeline triggers</td>\n<td>Export deployment specs</td>\n<td>Trigger on model promotion</td>\n<td>GitLab CI, GitHub Actions, Jenkins</td>\n</tr>\n<tr>\n<td>Security Scanning</td>\n<td>Validation hooks</td>\n<td>Send models for analysis</td>\n<td>Block on security failures</td>\n<td>Twistlock, Aqua Security</td>\n</tr>\n<tr>\n<td>Business Intelligence</td>\n<td>Metrics federation</td>\n<td>Export model performance</td>\n<td>Schedule report updates</td>\n<td>Tableau, PowerBI, Looker</td>\n</tr>\n</tbody></table>\n<p>The integration framework provides <strong>webhook infrastructure</strong>, <strong>event transformation</strong>, and <strong>credential management</strong> that individual tool integrations can leverage. This enables rapid integration development for new tools without rebuilding common infrastructure.</p>\n<p><strong>Third-Party Integration APIs:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>register_webhook_endpoint(tool_name, event_types, endpoint_config)</code></td>\n<td>tool_name str, event_types List[str], endpoint_config WebhookConfig</td>\n<td>str</td>\n<td>Register webhook for tool notifications</td>\n</tr>\n<tr>\n<td><code>trigger_external_workflow(tool_name, workflow_id, parameters)</code></td>\n<td>tool_name str, workflow_id str, parameters Dict</td>\n<td>TriggerResult</td>\n<td>Start workflow in external tool</td>\n</tr>\n<tr>\n<td><code>sync_metadata_to_tool(tool_name, metadata_type, data)</code></td>\n<td>tool_name str, metadata_type str, data Any</td>\n<td>SyncResult</td>\n<td>Export metadata to external tool</td>\n</tr>\n<tr>\n<td><code>import_artifacts_from_tool(tool_name, import_spec)</code></td>\n<td>tool_name str, import_spec ImportSpec</td>\n<td>ImportResult</td>\n<td>Import artifacts from external tool</td>\n</tr>\n<tr>\n<td><code>federate_metrics(tool_name, metric_mapping)</code></td>\n<td>tool_name str, metric_mapping Dict</td>\n<td>FederationSetup</td>\n<td>Set up bidirectional metric sharing</td>\n</tr>\n</tbody></table>\n<p>Third-party integrations implement <strong>idempotent synchronization</strong> to handle network failures, <strong>conflict resolution</strong> for concurrent updates, and <strong>audit logging</strong> to track data provenance across tool boundaries.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: Successful third-party integrations preserve each tool&#39;s strengths while ensuring the MLOps platform remains the authoritative source for model lineage and deployment decisions. This requires careful interface design that respects tool boundaries while enabling seamless workflows.</p>\n</blockquote>\n<p><strong>Common Pitfalls in Ecosystem Integrations:</strong></p>\n<p>⚠️ <strong>Pitfall: Framework Lock-in Through Tight Coupling</strong>\nIntegrating too deeply with specific ML frameworks can create hidden dependencies that make it difficult to support new frameworks or upgrade existing ones. This happens when platform code directly imports framework libraries or relies on framework-specific data structures. Prevent this by using adapter patterns with well-defined interfaces, serializing models to framework-agnostic formats when possible, and testing framework upgrades in isolated environments.</p>\n<p>⚠️ <strong>Pitfall: Cloud Credential Sprawl and Security Risks</strong>\nCloud integrations often accumulate credentials and permissions over time, creating security risks and operational complexity. This occurs when each integration manages its own credentials without centralized policies or rotation procedures. Prevent this by implementing centralized credential management with automatic rotation, principle of least privilege access controls, and regular security audits of cloud service permissions.</p>\n<p>⚠️ <strong>Pitfall: Third-Party Integration Cascade Failures</strong>\nExternal tool failures can cascade into platform failures when integrations don&#39;t handle service unavailability gracefully. This manifests as platform operations blocking on external API calls or failing when third-party webhooks are unreachable. Prevent this by implementing circuit breakers for external service calls, asynchronous integration patterns with retry queues, and graceful degradation when external tools are unavailable.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The future extensions outlined in this section demonstrate the platform&#39;s architectural flexibility while providing concrete guidance for prioritizing and implementing advanced capabilities. This implementation guidance focuses on the foundational patterns that enable extension development rather than complete implementations of specific features.</p>\n<h4 id=\"technology-recommendations-for-extensions\">Technology Recommendations for Extensions</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Feature Store</td>\n<td>SQLite with Pandas integration</td>\n<td>Apache Feast with Redis online store</td>\n</tr>\n<tr>\n<td>AutoML Optimization</td>\n<td>Grid search with multiprocessing</td>\n<td>Optuna with distributed trials</td>\n</tr>\n<tr>\n<td>Multi-Tenant Storage</td>\n<td>PostgreSQL schemas with RLS</td>\n<td>Dedicated databases with federation</td>\n</tr>\n<tr>\n<td>Edge Deployment</td>\n<td>Docker containers with sync scripts</td>\n<td>Kubernetes edge clusters with GitOps</td>\n</tr>\n<tr>\n<td>Large-Scale Training</td>\n<td>Horovod with MPI backend</td>\n<td>Ray Train with elastic scaling</td>\n</tr>\n<tr>\n<td>Cloud Integration</td>\n<td>Direct SDK calls with retry logic</td>\n<td>Terraform providers with state management</td>\n</tr>\n</tbody></table>\n<h4 id=\"extension-development-framework\">Extension Development Framework</h4>\n<p>The platform provides a standardized framework for developing extensions that maintains architectural consistency while supporting diverse integration requirements. Extensions should follow these patterns:</p>\n<p><strong>Extension Base Classes:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># MLOps platform extension framework</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MLOpsExtension</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Base class for platform extensions with standard lifecycle.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: ExtensionConfig, event_coordinator: EventCoordinator):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize extension with configuration and event system access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Register extension-specific health checks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up extension-specific metrics collection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Initialize any required external service connections</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Start extension services and begin processing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate configuration and external dependencies</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Subscribe to relevant platform events</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Start any background processing threads or tasks</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Register extension APIs with the platform router</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Publish extension ready event</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> health_check</span><span style=\"color:#E1E4E8\">(self) -> HealthCheck:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Report extension health status.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check external service connectivity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate critical configuration is still valid</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify background processes are running correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return detailed health status with failure reasons</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> FeatureStoreExtension</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">MLOpsExtension</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Feature store extension providing feature management capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_feature_group</span><span style=\"color:#E1E4E8\">(self, name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, source_config: Dict, features: List[FeatureDefinition]) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Validate feature definitions and source configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create feature group metadata in extension storage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Set up data synchronization from source to feature store</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Register feature group with platform model registry for lineage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Publish feature group created event</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Extension Integration Helpers:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Standard patterns for extension development</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionEventHandler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper for handling platform events in extensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, extension_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, event_coordinator: EventCoordinator):</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Register extension as event source</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up structured logging with extension context</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize event processing metrics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> subscribe_to_events</span><span style=\"color:#E1E4E8\">(self, event_mappings: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">callable</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Subscribe to platform events with extension-specific handlers.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Register event handlers with error handling and retries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up event processing metrics and monitoring</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Implement graceful shutdown for event processing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionAPIRouter</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper for exposing extension APIs through platform routing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_endpoints</span><span style=\"color:#E1E4E8\">(self, extension_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, endpoints: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">callable</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Register extension HTTP endpoints with platform API gateway.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Add authentication and authorization middleware</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up request logging and metrics collection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add input validation and error handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Register endpoints with API documentation system</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"extension-development-guidelines\">Extension Development Guidelines</h4>\n<p><strong>Milestone Checkpoint for Extension Development:</strong></p>\n<p>After implementing an extension using the framework, verify the following behavior:</p>\n<ol>\n<li><strong>Extension Lifecycle</strong>: Start the extension and confirm it publishes a ready event and responds to health checks</li>\n<li><strong>Event Integration</strong>: Trigger a relevant platform event and verify the extension receives and processes it correctly</li>\n<li><strong>API Exposure</strong>: Make HTTP requests to extension endpoints through the platform API gateway</li>\n<li><strong>Error Handling</strong>: Simulate external service failures and confirm graceful degradation</li>\n<li><strong>Monitoring</strong>: Check that extension metrics appear in platform monitoring dashboards</li>\n</ol>\n<p><strong>Extension Testing Pattern:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test framework for extensions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensionTestHelper</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Helper for testing extension integrations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_test_environment</span><span style=\"color:#E1E4E8\">(self, extension_config: Dict) -> TestEnvironment:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set up isolated test environment for extension development.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create temporary database and storage for extension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set up mock external services with configurable responses</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Initialize extension with test configuration</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Provide access to platform test utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> simulate_platform_events</span><span style=\"color:#E1E4E8\">(self, events: List[Event]) -> List[EventResult]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Send test events to extension and collect responses.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Publish events through test event coordinator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Wait for extension processing with timeout</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Collect any events published by extension</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return processing results and timing information</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p>The extension development framework ensures new capabilities integrate cleanly with existing platform components while maintaining operational consistency and debuggability across the entire system.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> This section provides essential definitions and terminology that apply to all milestones (1-5), ensuring consistent understanding of technical terms, MLOps concepts, and domain-specific vocabulary used throughout the platform architecture.</p>\n</blockquote>\n<p>The MLOps platform introduces numerous technical concepts, architectural patterns, and domain-specific terminology that span machine learning, distributed systems, and software engineering. Understanding these terms is crucial for implementing and maintaining the platform effectively. This glossary provides comprehensive definitions organized by functional areas, with cross-references to related concepts and concrete examples from the platform&#39;s implementation.</p>\n<h3 id=\"core-mlops-concepts\">Core MLOps Concepts</h3>\n<p><strong>Experiment tracking</strong> refers to the systematic logging and organization of ML training runs, capturing parameters, metrics, and artifacts to enable reproducibility and comparison across different training attempts. The experiment tracking component maintains a hierarchical relationship where experiments group related runs, and each run captures the complete context of a training session including hyperparameters, performance metrics at each step, and generated artifacts like model files and visualizations.</p>\n<p><strong>Model registry</strong> provides versioned storage and lifecycle management for trained ML models, implementing semantic versioning with stage transitions through development, staging, and production environments. The registry enforces immutability guarantees ensuring that once a model version is registered, its artifacts and metadata cannot be modified, while maintaining complete model lineage that traces each version back to its source training run, data dependencies, and code commit.</p>\n<p><strong>Pipeline orchestration</strong> coordinates multi-step ML workflows using directed acyclic graphs (DAGs) that define data dependencies between steps. The orchestration engine handles resource allocation, step isolation through containerization, and fault tolerance through retry policies and checkpointing. Each pipeline execution creates a complete audit trail of step executions, resource usage, and data flow that enables debugging and performance optimization.</p>\n<p><strong>Model deployment</strong> encompasses the process of serving ML models as scalable HTTP endpoints in production environments, supporting advanced deployment patterns like blue-green deployments for zero-downtime updates and canary deployments for gradual traffic shifting. The deployment component integrates with specialized inference servers and implements auto-scaling policies that adjust replica counts based on request load and latency requirements.</p>\n<p><strong>Model monitoring</strong> tracks ML model performance and data characteristics in production through comprehensive prediction logging and statistical analysis. The monitoring system detects data drift by comparing incoming feature distributions against training baselines, identifies concept drift through prediction distribution analysis, and maintains real-time performance metrics including latency percentiles and throughput measurements.</p>\n<h3 id=\"architecture-and-design-patterns\">Architecture and Design Patterns</h3>\n<p><strong>Microservices approach</strong> structures the platform as independent services that communicate through well-defined APIs, enabling independent scaling, deployment, and technology choices for each component. Each service maintains its own data store and implements clear boundaries that prevent tight coupling while supporting platform-wide coordination through event-driven patterns.</p>\n<p><strong>Hexagonal architecture</strong> separates business logic from external concerns by defining explicit interfaces for all dependencies, allowing components to swap implementations without affecting core functionality. This pattern enables the platform to support multiple storage backends, inference servers, and orchestration engines while maintaining consistent internal APIs.</p>\n<p><strong>Polyglot persistence</strong> employs different data storage technologies optimized for specific access patterns, using PostgreSQL for structured metadata with complex queries, object storage for large binary artifacts, and time-series databases for metrics and monitoring data. This approach maximizes performance while ensuring data consistency across the platform.</p>\n<p><strong>Event-driven coordination</strong> implements asynchronous communication between components using immutable events that capture state changes and trigger downstream processing. The event system supports at-least-once delivery guarantees and maintains causal ordering for events affecting the same resources, enabling reliable workflow coordination without tight coupling.</p>\n<h3 id=\"data-management-and-storage\">Data Management and Storage</h3>\n<p><strong>Correlation ID</strong> provides a unique identifier that links related data and operations across multiple components, enabling distributed tracing and debugging by following the flow of requests and events through the entire system. Each API request, pipeline execution, and model deployment receives a correlation ID that appears in all related logs and database records.</p>\n<p><strong>Artifact</strong> refers to binary files produced during ML workflows, including trained model files, evaluation plots, configuration files, and dataset snapshots. The platform implements content-addressable storage for artifacts using cryptographic hashes as keys, enabling deduplication and ensuring data integrity through checksum validation.</p>\n<p><strong>Model lineage</strong> creates a directed acyclic graph showing the complete provenance of a trained model, linking it to the specific training dataset version, code commit, hyperparameters, and experiment run that produced it. This lineage graph enables impact analysis when data or code changes and supports regulatory compliance requirements for model traceability.</p>\n<p><strong>Hierarchical namespacing</strong> organizes platform resources using path-based structures that separate tenants, workspaces, and individual resources, enabling fine-grained access control and resource quotas. The namespace hierarchy supports multi-tenant deployments while maintaining strict isolation between different organizations or teams.</p>\n<h3 id=\"model-lifecycle-management\">Model Lifecycle Management</h3>\n<p><strong>Semantic versioning</strong> adapts the MAJOR.MINOR.PATCH version scheme for ML workflows, where major versions indicate breaking changes to model inputs or outputs, minor versions represent significant algorithmic improvements, and patch versions capture bug fixes or retraining with additional data.</p>\n<p><strong>Stage transitions</strong> implement a promotion workflow where model versions progress through predefined stages (Development, Staging, Production, Archived) with approval gates and validation requirements at each transition. The system maintains a complete audit trail of stage changes including approval metadata and rollback capabilities.</p>\n<p><strong>Immutability guarantees</strong> ensure that once a model version is registered in the model registry, its artifacts and core metadata cannot be modified, preventing accidental corruption of production models while allowing non-breaking metadata updates like tags and descriptions.</p>\n<p><strong>Approval workflows</strong> define structured processes requiring validation and authorization before model versions can be promoted to higher stages, supporting both automated checks (performance thresholds, integration tests) and manual approvals from designated reviewers.</p>\n<h3 id=\"pipeline-and-training-concepts\">Pipeline and Training Concepts</h3>\n<p><strong>DAG (Directed Acyclic Graph)</strong> represents pipeline step dependencies as a mathematical graph structure where nodes are computational steps and edges represent data dependencies, ensuring that upstream steps complete before downstream steps begin execution. The pipeline orchestrator computes execution order by performing topological sorting on the DAG structure.</p>\n<p><strong>Resource allocation</strong> assigns computational resources (CPU cores, memory, GPU units, storage) to pipeline steps based on declared requirements and cluster availability, supporting both guaranteed resource reservations and best-effort scheduling for cost optimization.</p>\n<p><strong>Step isolation</strong> executes each pipeline step within a separate container environment with dedicated resource limits, preventing interference between steps and enabling precise resource accounting. Containerization also ensures reproducible execution environments across different cluster nodes.</p>\n<p><strong>Fault tolerance</strong> handles various failure scenarios through retry policies with exponential backoff, checkpoint-restart mechanisms for long-running steps, and graceful degradation when non-critical steps fail. The system maintains detailed failure logs and supports both automatic recovery and manual intervention.</p>\n<p><strong>Distributed training</strong> coordinates model training across multiple GPUs or compute nodes using parameter server architectures or all-reduce communication patterns, requiring careful gang scheduling to ensure all resources are allocated simultaneously and handling node failures through checkpointing and restart mechanisms.</p>\n<p><strong>Gang scheduling</strong> allocates all required resources for a distributed training job simultaneously rather than incrementally, preventing deadlock situations where partially allocated jobs block resources needed by other jobs waiting in the queue.</p>\n<p><strong>Artifact lineage</strong> tracks the flow of data artifacts through pipeline steps, recording which outputs were generated from which inputs and maintaining a complete dependency graph that enables impact analysis and debugging of data quality issues.</p>\n<h3 id=\"deployment-and-serving\">Deployment and Serving</h3>\n<p><strong>Blue-green deployments</strong> maintain two complete production environments (blue and green) and switch traffic atomically between them during model updates, enabling zero-downtime deployments with instant rollback capabilities if issues are detected with the new version.</p>\n<p><strong>Canary deployments</strong> gradually shift traffic from the current model version to a new version by routing a small percentage of requests to the new version initially and increasing the percentage based on performance metrics and error rates, providing risk mitigation for production deployments.</p>\n<p><strong>Auto-scaling</strong> automatically adjusts the number of model serving replicas based on observed metrics like request rate, latency percentiles, and resource utilization, ensuring adequate capacity to handle traffic spikes while minimizing costs during low-traffic periods.</p>\n<p><strong>Traffic management</strong> controls the routing of inference requests between different model versions using configurable rules based on request headers, client properties, or random sampling, enabling A/B testing and gradual rollouts with precise control over traffic distribution.</p>\n<p><strong>Inference servers</strong> are specialized systems optimized for serving ML models in production, including TensorFlow Serving, TorchServe, and NVIDIA Triton, providing features like dynamic batching, model optimization, and GPU memory management that maximize serving performance.</p>\n<p><strong>Model warming</strong> involves preloading models into memory and executing initial inference requests to trigger just-in-time compilation and optimization before routing production traffic to new serving instances, reducing cold start latency and ensuring consistent performance.</p>\n<p><strong>Health checks</strong> implement validation endpoints that verify model serving instances are ready to handle requests, checking model loading status, dependency availability, and basic inference functionality to support load balancer configuration and auto-scaling decisions.</p>\n<p><strong>Traffic splitting</strong> distributes incoming requests across multiple model versions according to configured percentages, enabling controlled experiments and gradual rollouts while maintaining detailed metrics for each version to support decision-making.</p>\n<p><strong>Rollback</strong> provides mechanisms to revert to a previous model version when issues are detected, including automated rollback based on error rate thresholds and manual rollback procedures that preserve traffic routing configurations and monitoring baselines.</p>\n<h3 id=\"monitoring-and-observability\">Monitoring and Observability</h3>\n<p><strong>Data drift</strong> represents statistical changes in input data distribution compared to the training dataset, detected using techniques like the Kolmogorov-Smirnov test for continuous features and chi-squared tests for categorical features, indicating potential degradation in model performance.</p>\n<p><strong>Concept drift</strong> refers to changes in the underlying relationship between input features and target variables, typically detected by monitoring changes in prediction distributions or performance metrics over time, requiring model retraining to maintain accuracy.</p>\n<p><strong>Prediction logging</strong> captures comprehensive information about each inference request including input features, model outputs, confidence scores, latency measurements, and request metadata, providing the foundation for performance analysis and drift detection.</p>\n<p><strong>Population Stability Index (PSI)</strong> measures the stability of feature distributions between two time periods by comparing the proportion of samples in different bins, with values above 0.2 typically indicating significant distribution shifts that may affect model performance.</p>\n<p><strong>Kolmogorov-Smirnov test</strong> compares two continuous distributions by measuring the maximum difference between their cumulative distribution functions, providing a statistical test for detecting changes in feature distributions with quantified confidence levels.</p>\n<p><strong>Chi-squared test</strong> evaluates whether categorical feature distributions differ significantly between two samples by comparing observed versus expected frequencies across categories, supporting drift detection for discrete and ordinal features.</p>\n<p><strong>Alert escalation</strong> implements tiered notification systems that route alerts to appropriate teams based on severity levels and response times, ensuring critical issues receive immediate attention while preventing alert fatigue through intelligent filtering and grouping.</p>\n<p><strong>Statistical significance</strong> quantifies the probability that observed differences in model performance or data distributions are not due to random variation, supporting data-driven decisions about model updates and drift response actions.</p>\n<h3 id=\"system-integration-and-communication\">System Integration and Communication</h3>\n<p><strong>Inter-component APIs</strong> define RESTful interfaces for communication between platform components, specifying request/response formats, authentication requirements, error handling patterns, and versioning strategies that enable independent component evolution while maintaining compatibility.</p>\n<p><strong>Circuit breaker</strong> implements a pattern that prevents cascade failures in distributed systems by monitoring error rates and response times, automatically routing requests away from failing services and periodically testing recovery to restore normal operation when services become healthy again.</p>\n<p><strong>Event sourcing</strong> captures all state changes as immutable events stored in an append-only log, enabling complete system state reconstruction, audit trails, and support for complex queries about historical system behavior and data lineage.</p>\n<p><strong>Idempotent event handlers</strong> process events in a way that produces the same result regardless of how many times the event is delivered, supporting at-least-once delivery guarantees while preventing duplicate processing and maintaining system consistency.</p>\n<p><strong>Causal ordering</strong> ensures that events affecting the same resources are processed in dependency order, preventing race conditions and maintaining data consistency in distributed systems where events may arrive out of order due to network delays or system failures.</p>\n<p><strong>At-least-once delivery</strong> guarantees that published events will be delivered to all registered subscribers, implementing retry mechanisms and persistent event storage to handle temporary system failures while requiring subscribers to implement idempotent processing.</p>\n<p><strong>Workflow coordination</strong> orchestrates complex multi-component operations through a combination of synchronous API calls for immediate feedback and asynchronous events for long-running processes, maintaining clear transaction boundaries and consistent error handling.</p>\n<h3 id=\"error-handling-and-recovery\">Error Handling and Recovery</h3>\n<p><strong>Automated recovery</strong> implements procedures that detect and resolve common failure scenarios without human intervention, including service restarts, data inconsistency repairs, and resource cleanup, with clear escalation paths when automated approaches are insufficient.</p>\n<p><strong>Failure detection</strong> monitors system health through comprehensive health checks, metric thresholds, and log analysis to identify component failures, performance degradation, and data quality issues as quickly as possible to minimize impact.</p>\n<p><strong>Data consistency</strong> maintains synchronized state across distributed components through careful transaction boundary design, conflict resolution mechanisms, and eventual consistency guarantees that ensure the system converges to a correct state even after failures.</p>\n<p><strong>Transaction boundaries</strong> define the scope of operations that must complete atomically, using database transactions, compensating actions, and saga patterns to maintain data integrity across multiple components and external systems.</p>\n<p><strong>Conflict resolution</strong> handles concurrent operations that modify the same resources through optimistic locking, version vectors, and merge strategies that preserve user intent while maintaining system consistency and preventing data corruption.</p>\n<p><strong>Eventual consistency</strong> provides guarantees that the distributed system will converge to a consistent state within a bounded time period, even in the presence of network partitions and component failures, enabling high availability while ensuring data integrity.</p>\n<p><strong>Recovery procedures</strong> define systematic approaches to restoring normal system operation after failures, including data restoration from backups, service restart sequences, and validation steps to confirm successful recovery.</p>\n<p><strong>Escalation</strong> routes complex issues to human operators when automated recovery procedures are insufficient, providing comprehensive context, suggested actions, and clear procedures for manual intervention while maintaining detailed audit trails.</p>\n<h3 id=\"testing-and-quality-assurance\">Testing and Quality Assurance</h3>\n<p><strong>Testing pyramid</strong> structures the testing strategy with many fast, focused unit tests at the base, fewer integration tests in the middle, and minimal slow end-to-end tests at the top, optimizing for quick feedback during development while ensuring comprehensive coverage.</p>\n<p><strong>Milestone verification</strong> implements validation procedures that ensure the platform meets acceptance criteria after each development phase, including automated test suites, performance benchmarks, and functional validation scenarios that demonstrate correct behavior.</p>\n<p><strong>Load testing</strong> evaluates system performance under realistic traffic patterns by simulating concurrent users, varying request rates, and peak load scenarios to identify bottlenecks, validate auto-scaling behavior, and establish performance baselines.</p>\n<p><strong>Integration testing</strong> validates component interactions through their public APIs and event interfaces, using test doubles and contract testing to ensure components work correctly together while maintaining independent development and deployment.</p>\n<p><strong>End-to-end testing</strong> verifies complete workflows from experiment tracking through model deployment and monitoring using realistic datasets and scenarios, ensuring the platform delivers value to users while catching integration issues that unit tests might miss.</p>\n<p><strong>Test fixtures</strong> provide reusable test data, configuration, and infrastructure setup that enables consistent testing environments and reduces test maintenance overhead while supporting both local development and continuous integration pipelines.</p>\n<p><strong>Performance benchmarks</strong> establish quantitative targets for system performance including request latency, throughput, resource utilization, and scalability limits that guide development priorities and validate optimization efforts.</p>\n<h3 id=\"observability-and-operations\">Observability and Operations</h3>\n<p><strong>Structured logging</strong> implements consistent log formats with correlation IDs, contextual metadata, and standardized severity levels that enable effective log aggregation, searching, and analysis across all platform components.</p>\n<p><strong>Distributed tracing</strong> provides end-to-end visibility into request flows across multiple components by propagating trace contexts and recording timing, dependencies, and errors to support performance optimization and debugging of complex workflows.</p>\n<p><strong>Performance profiling</strong> identifies bottlenecks and resource utilization patterns through systematic measurement of CPU usage, memory allocation, I/O patterns, and database query performance to guide optimization efforts and capacity planning.</p>\n<p><strong>Incident response</strong> establishes systematic procedures for detecting, investigating, and resolving system failures through runbooks, escalation procedures, and post-incident reviews that capture lessons learned and prevent recurrence.</p>\n<h3 id=\"advanced-mlops-features\">Advanced MLOps Features</h3>\n<p><strong>Feature store</strong> provides a centralized repository for feature definitions, transformations, and serving infrastructure that enables feature reuse across different models while maintaining consistency between training and inference data processing pipelines.</p>\n<p><strong>Automated machine learning (AutoML)</strong> systematically explores model architectures, hyperparameter configurations, and feature engineering approaches to optimize model performance with minimal human intervention, requiring sophisticated search algorithms and resource management.</p>\n<p><strong>Multi-tenancy</strong> enables a shared platform to serve multiple isolated organizations or teams through hierarchical resource organization, access control policies, and resource quotas that ensure security and fair resource allocation.</p>\n<p><strong>Edge deployment</strong> optimizes and deploys ML models at network edge locations to minimize latency for end users, requiring model compression, specialized runtime environments, and synchronization mechanisms for model updates and telemetry collection.</p>\n<p><strong>Large-scale training</strong> coordinates model training across hundreds or thousands of compute nodes using advanced parallelization strategies, fault-tolerant communication protocols, and sophisticated scheduling algorithms that maximize resource utilization while handling node failures.</p>\n<h3 id=\"integration-and-ecosystem\">Integration and Ecosystem</h3>\n<p><strong>Framework integration</strong> supports multiple ML frameworks (TensorFlow, PyTorch, scikit-learn, XGBoost) through adapter patterns and standardized interfaces that abstract framework-specific details while preserving access to advanced features and optimizations.</p>\n<p><strong>Cloud integration</strong> federates with external cloud services like managed training platforms, serving infrastructure, and storage systems through standardized APIs and credential management that enables hybrid deployment scenarios and vendor flexibility.</p>\n<p><strong>Model optimization</strong> applies techniques like quantization, pruning, knowledge distillation, and specialized compilation to reduce model size and improve inference performance while maintaining acceptable accuracy levels for production deployment constraints.</p>\n<p><strong>Federation over replication</strong> integrates with existing external services rather than rebuilding equivalent functionality, using APIs, webhooks, and data synchronization to leverage specialized tools while maintaining platform coherence and user experience.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The platform&#39;s comprehensive terminology reflects the complexity of building production-grade MLOps systems that must handle the full lifecycle of machine learning applications. Understanding these concepts is essential for several reasons: they provide precise vocabulary for discussing system behavior and requirements, they establish clear boundaries between different concerns and components, and they enable effective communication between team members working on different aspects of the platform.</p>\n<p>When implementing the platform, developers should internalize these concepts progressively, starting with core MLOps terminology (experiments, models, pipelines, deployments, monitoring) before moving to advanced distributed systems concepts (consistency, fault tolerance, observability). Each milestone introduces terminology relevant to its specific domain while building on concepts from previous milestones.</p>\n<p>The glossary serves as both a reference during implementation and a validation tool to ensure consistent understanding across the development team. When design discussions arise, referring to these standardized definitions helps maintain clarity and prevents misunderstandings that could lead to architectural inconsistencies.</p>\n<p><strong>Technology Integration Reference:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Concept Category</th>\n<th>Core Technologies</th>\n<th>Integration Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Experiment Tracking</td>\n<td>PostgreSQL, MLflow, S3</td>\n<td>Database storage with object storage for artifacts</td>\n</tr>\n<tr>\n<td>Model Registry</td>\n<td>Docker Registry patterns, Git semantics</td>\n<td>Immutable storage with semantic versioning</td>\n</tr>\n<tr>\n<td>Pipeline Orchestration</td>\n<td>Kubernetes, Apache Airflow</td>\n<td>Container orchestration with DAG execution</td>\n</tr>\n<tr>\n<td>Model Deployment</td>\n<td>Kubernetes, NGINX, Triton</td>\n<td>Service mesh with specialized inference servers</td>\n</tr>\n<tr>\n<td>Model Monitoring</td>\n<td>Prometheus, Grafana, Kafka</td>\n<td>Time-series metrics with stream processing</td>\n</tr>\n<tr>\n<td>System Integration</td>\n<td>REST APIs, CloudEvents, OpenAPI</td>\n<td>Event-driven architecture with standard protocols</td>\n</tr>\n</tbody></table>\n<p><strong>Common Terminology Pitfalls:</strong></p>\n<p>⚠️ <strong>Pitfall: Confusing experiments and runs</strong>\nMany developers initially treat experiments and runs as the same concept, leading to flat organizational structures that become unwieldy at scale. Remember that experiments are logical groupings (like &quot;hyperparameter tuning for ResNet model&quot;) while runs are individual training executions within those experiments.</p>\n<p>⚠️ <strong>Pitfall: Mixing deployment patterns</strong>\nUsing inconsistent terminology for deployment strategies (calling canary deployments &quot;rolling updates&quot; or blue-green deployments &quot;staged rollouts&quot;) creates confusion during incident response. Stick to standard industry terminology to ensure clear communication.</p>\n<p>⚠️ <strong>Pitfall: Overloading &quot;pipeline&quot; terminology</strong>\nThe term &quot;pipeline&quot; appears in multiple contexts (training pipelines, data pipelines, CI/CD pipelines, inference pipelines). Always specify the context or use more precise terms like &quot;training workflow&quot; or &quot;feature pipeline&quot; to avoid ambiguity.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent drift terminology</strong>\nDistinguish clearly between data drift (input distribution changes) and concept drift (relationship changes between inputs and outputs). Using &quot;drift&quot; generically makes it difficult to implement appropriate detection and response strategies.</p>\n<p>This comprehensive glossary ensures that all stakeholders in the MLOps platform development and operation share a common understanding of critical concepts, enabling more effective collaboration and reducing miscommunication that could lead to implementation errors or architectural inconsistencies.</p>\n","toc":[{"level":1,"text":"MLOps Platform: Design Document","id":"mlops-platform-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"The ML Development Chaos","id":"the-ml-development-chaos"},{"level":3,"text":"Existing MLOps Solutions","id":"existing-mlops-solutions"},{"level":3,"text":"Core Technical Challenges","id":"core-technical-challenges"},{"level":4,"text":"State Management Complexity","id":"state-management-complexity"},{"level":4,"text":"Distributed Coordination Challenges","id":"distributed-coordination-challenges"},{"level":4,"text":"Data Consistency Guarantees","id":"data-consistency-guarantees"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended Project Structure","id":"recommended-project-structure"},{"level":4,"text":"Foundation Infrastructure Code","id":"foundation-infrastructure-code"},{"level":4,"text":"Core Component Integration Patterns","id":"core-component-integration-patterns"},{"level":4,"text":"Language-Specific Implementation Hints","id":"language-specific-implementation-hints"},{"level":4,"text":"Platform Development Checkpoints","id":"platform-development-checkpoints"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Functional Requirements","id":"functional-requirements"},{"level":3,"text":"Quality Attributes","id":"quality-attributes"},{"level":3,"text":"What We Won&#39;t Build","id":"what-we-won39t-build"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Responsibilities","id":"component-responsibilities"},{"level":3,"text":"Technology Stack","id":"technology-stack"},{"level":3,"text":"Codebase Organization","id":"codebase-organization"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Mental Model: Digital Laboratory Information System","id":"mental-model-digital-laboratory-information-system"},{"level":3,"text":"Architecture Decision: Entity Relationship Design","id":"architecture-decision-entity-relationship-design"},{"level":2,"text":"Experiment Tracking Entities","id":"experiment-tracking-entities"},{"level":3,"text":"Core Entity Hierarchy","id":"core-entity-hierarchy"},{"level":3,"text":"Experiment Entity Schema","id":"experiment-entity-schema"},{"level":3,"text":"Run Entity Schema","id":"run-entity-schema"},{"level":3,"text":"Parameter Entity Schema","id":"parameter-entity-schema"},{"level":3,"text":"Metric Entity Schema","id":"metric-entity-schema"},{"level":3,"text":"Artifact Entity Schema","id":"artifact-entity-schema"},{"level":3,"text":"Querying and Analysis Patterns","id":"querying-and-analysis-patterns"},{"level":2,"text":"Model Registry Entities","id":"model-registry-entities"},{"level":3,"text":"Model Lifecycle Overview","id":"model-lifecycle-overview"},{"level":3,"text":"Model Entity Schema","id":"model-entity-schema"},{"level":3,"text":"ModelVersion Entity Schema","id":"modelversion-entity-schema"},{"level":3,"text":"Stage Transition Management","id":"stage-transition-management"},{"level":3,"text":"Model Lineage Tracking","id":"model-lineage-tracking"},{"level":3,"text":"Architecture Decision: Immutable Versions with Mutable Metadata","id":"architecture-decision-immutable-versions-with-mutable-metadata"},{"level":2,"text":"Pipeline Entities","id":"pipeline-entities"},{"level":3,"text":"Mental Model: Manufacturing Assembly Line","id":"mental-model-manufacturing-assembly-line"},{"level":3,"text":"Pipeline Definition Schema","id":"pipeline-definition-schema"},{"level":3,"text":"Pipeline Execution Schema","id":"pipeline-execution-schema"},{"level":3,"text":"Pipeline Step Execution Schema","id":"pipeline-step-execution-schema"},{"level":3,"text":"Data Flow and Artifact Passing","id":"data-flow-and-artifact-passing"},{"level":3,"text":"Conditional Execution and Dynamic Workflows","id":"conditional-execution-and-dynamic-workflows"},{"level":2,"text":"Deployment and Monitoring Entities","id":"deployment-and-monitoring-entities"},{"level":3,"text":"Mental Model: Restaurant Service Management","id":"mental-model-restaurant-service-management"},{"level":3,"text":"Deployment Entity Schema","id":"deployment-entity-schema"},{"level":3,"text":"Deployment Revision Schema","id":"deployment-revision-schema"},{"level":3,"text":"Endpoint Metrics Schema","id":"endpoint-metrics-schema"},{"level":3,"text":"Prediction Log Schema","id":"prediction-log-schema"},{"level":3,"text":"Data Drift Detection Schema","id":"data-drift-detection-schema"},{"level":3,"text":"Architecture Decision: Real-Time vs. Batch Analytics","id":"architecture-decision-real-time-vs-batch-analytics"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Base Entity Infrastructure","id":"base-entity-infrastructure"},{"level":4,"text":"Experiment Tracking Entities","id":"experiment-tracking-entities"},{"level":4,"text":"Model Registry Entities","id":"model-registry-entities"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":2,"text":"Experiment Tracking Component","id":"experiment-tracking-component"},{"level":3,"text":"Mental Model: Research Laboratory","id":"mental-model-research-laboratory"},{"level":3,"text":"Logging APIs and Storage","id":"logging-apis-and-storage"},{"level":4,"text":"Parameter Logging Interface","id":"parameter-logging-interface"},{"level":4,"text":"Metric Logging Interface","id":"metric-logging-interface"},{"level":4,"text":"Artifact Storage Interface","id":"artifact-storage-interface"},{"level":4,"text":"Storage Architecture Decisions","id":"storage-architecture-decisions"},{"level":3,"text":"Querying and Comparison","id":"querying-and-comparison"},{"level":4,"text":"Run Search and Filtering","id":"run-search-and-filtering"},{"level":4,"text":"Metric Comparison and Visualization","id":"metric-comparison-and-visualization"},{"level":4,"text":"Parameter-Outcome Correlation Analysis","id":"parameter-outcome-correlation-analysis"},{"level":4,"text":"Query Optimization Strategies","id":"query-optimization-strategies"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Core Data Models (Complete Implementation)","id":"core-data-models-complete-implementation"},{"level":4,"text":"Storage Interface Implementations","id":"storage-interface-implementations"},{"level":4,"text":"Core Tracking Service (Skeleton)","id":"core-tracking-service-skeleton"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Model Registry Component","id":"model-registry-component"},{"level":3,"text":"Mental Model: Software Package Registry","id":"mental-model-software-package-registry"},{"level":3,"text":"Version Management and Stages","id":"version-management-and-stages"},{"level":3,"text":"Model Lineage and Metadata","id":"model-lineage-and-metadata"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Training Pipeline Orchestration","id":"training-pipeline-orchestration"},{"level":3,"text":"Mental Model: Assembly Line","id":"mental-model-assembly-line"},{"level":3,"text":"DAG Definition and Execution","id":"dag-definition-and-execution"},{"level":3,"text":"Resource Allocation and Scheduling","id":"resource-allocation-and-scheduling"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Model Deployment Component","id":"model-deployment-component"},{"level":3,"text":"Mental Model: Restaurant Service","id":"mental-model-restaurant-service"},{"level":3,"text":"Model Serving and Scaling","id":"model-serving-and-scaling"},{"level":3,"text":"Traffic Management and Rollouts","id":"traffic-management-and-rollouts"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":3,"text":"Common Pitfalls in Model Deployment","id":"common-pitfalls-in-model-deployment"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton","id":"core-logic-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Model Monitoring Component","id":"model-monitoring-component"},{"level":3,"text":"Mental Model: Health Monitoring System","id":"mental-model-health-monitoring-system"},{"level":3,"text":"Prediction Logging and Metrics","id":"prediction-logging-and-metrics"},{"level":4,"text":"Request and Response Capture","id":"request-and-response-capture"},{"level":4,"text":"Latency and Throughput Measurement","id":"latency-and-throughput-measurement"},{"level":4,"text":"Performance Metric Aggregation","id":"performance-metric-aggregation"},{"level":3,"text":"Data and Model Drift Detection","id":"data-and-model-drift-detection"},{"level":4,"text":"Statistical Drift Detection Methods","id":"statistical-drift-detection-methods"},{"level":4,"text":"Feature Distribution Monitoring","id":"feature-distribution-monitoring"},{"level":4,"text":"Concept Drift Analysis","id":"concept-drift-analysis"},{"level":3,"text":"Architecture Decisions","id":"architecture-decisions"},{"level":4,"text":"Data Retention and Storage Optimization","id":"data-retention-and-storage-optimization"},{"level":4,"text":"Alert Configuration and Escalation","id":"alert-configuration-and-escalation"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Component Interactions and Data Flow","id":"component-interactions-and-data-flow"},{"level":3,"text":"Mental Model: Orchestra Coordination","id":"mental-model-orchestra-coordination"},{"level":3,"text":"Inter-Component APIs","id":"inter-component-apis"},{"level":4,"text":"Core API Design Principles","id":"core-api-design-principles"},{"level":4,"text":"Experiment Tracking APIs","id":"experiment-tracking-apis"},{"level":4,"text":"Model Registry APIs","id":"model-registry-apis"},{"level":4,"text":"Pipeline Orchestration APIs","id":"pipeline-orchestration-apis"},{"level":4,"text":"Model Deployment APIs","id":"model-deployment-apis"},{"level":4,"text":"Model Monitoring APIs","id":"model-monitoring-apis"},{"level":3,"text":"End-to-End Workflows","id":"end-to-end-workflows"},{"level":4,"text":"Experiment to Deployment Workflow","id":"experiment-to-deployment-workflow"},{"level":4,"text":"Automated Retraining Workflow","id":"automated-retraining-workflow"},{"level":4,"text":"Model Rollback Workflow","id":"model-rollback-workflow"},{"level":3,"text":"Event-Driven Coordination","id":"event-driven-coordination"},{"level":4,"text":"Event Architecture and Patterns","id":"event-architecture-and-patterns"},{"level":4,"text":"Core Platform Events","id":"core-platform-events"},{"level":4,"text":"Event Coordination Workflows","id":"event-coordination-workflows"},{"level":4,"text":"Event Delivery and Reliability","id":"event-delivery-and-reliability"},{"level":3,"text":"Common Pitfalls","id":"common-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Event Coordinator Infrastructure Code","id":"event-coordinator-infrastructure-code"},{"level":4,"text":"API Client Infrastructure Code","id":"api-client-infrastructure-code"},{"level":4,"text":"Component Integration Skeleton","id":"component-integration-skeleton"},{"level":4,"text":"Debugging Tips for Component Interactions","id":"debugging-tips-for-component-interactions"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"System Failure Modes","id":"system-failure-modes"},{"level":4,"text":"Experiment Tracking Failures","id":"experiment-tracking-failures"},{"level":4,"text":"Model Registry Failures","id":"model-registry-failures"},{"level":4,"text":"Training Pipeline Failures","id":"training-pipeline-failures"},{"level":4,"text":"Model Deployment Failures","id":"model-deployment-failures"},{"level":4,"text":"Model Monitoring Failures","id":"model-monitoring-failures"},{"level":3,"text":"Detection and Recovery Strategies","id":"detection-and-recovery-strategies"},{"level":4,"text":"Health Check Framework","id":"health-check-framework"},{"level":4,"text":"Circuit Breaker Implementation","id":"circuit-breaker-implementation"},{"level":4,"text":"Automated Recovery Procedures","id":"automated-recovery-procedures"},{"level":4,"text":"Event-Driven Coordination","id":"event-driven-coordination"},{"level":3,"text":"Data Consistency Guarantees","id":"data-consistency-guarantees"},{"level":4,"text":"Transaction Boundaries and ACID Properties","id":"transaction-boundaries-and-acid-properties"},{"level":4,"text":"Conflict Resolution Strategies","id":"conflict-resolution-strategies"},{"level":4,"text":"Eventual Consistency and Convergence","id":"eventual-consistency-and-convergence"},{"level":3,"text":"Common Recovery Scenarios","id":"common-recovery-scenarios"},{"level":4,"text":"Training Pipeline Catastrophic Failure","id":"training-pipeline-catastrophic-failure"},{"level":4,"text":"Model Deployment Rollback Cascade","id":"model-deployment-rollback-cascade"},{"level":4,"text":"Data Corruption During Experiment Migration","id":"data-corruption-during-experiment-migration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure","id":"file-structure"},{"level":4,"text":"Health Check Infrastructure","id":"health-check-infrastructure"},{"level":4,"text":"Circuit Breaker Implementation","id":"circuit-breaker-implementation"},{"level":4,"text":"Event-Driven Coordination System","id":"event-driven-coordination-system"},{"level":4,"text":"Recovery Procedure Framework","id":"recovery-procedure-framework"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Guide","id":"debugging-guide"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Component Testing","id":"component-testing"},{"level":4,"text":"Unit Testing Strategy","id":"unit-testing-strategy"},{"level":4,"text":"Integration Testing Approach","id":"integration-testing-approach"},{"level":4,"text":"Database and Storage Testing","id":"database-and-storage-testing"},{"level":3,"text":"End-to-End Scenarios","id":"end-to-end-scenarios"},{"level":4,"text":"Complete ML Workflow Scenarios","id":"complete-ml-workflow-scenarios"},{"level":4,"text":"Integration Testing with External Systems","id":"integration-testing-with-external-systems"},{"level":3,"text":"Milestone Verification","id":"milestone-verification"},{"level":4,"text":"Milestone 1: Experiment Tracking Verification","id":"milestone-1-experiment-tracking-verification"},{"level":4,"text":"Milestone 2: Model Registry Verification","id":"milestone-2-model-registry-verification"},{"level":4,"text":"Milestone 3: Training Pipeline Verification","id":"milestone-3-training-pipeline-verification"},{"level":4,"text":"Milestone 4: Model Deployment Verification","id":"milestone-4-model-deployment-verification"},{"level":4,"text":"Milestone 5: Model Monitoring Verification","id":"milestone-5-model-monitoring-verification"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Test Infrastructure Setup","id":"test-infrastructure-setup"},{"level":4,"text":"Test Utilities and Fixtures","id":"test-utilities-and-fixtures"},{"level":4,"text":"Test Data Generation","id":"test-data-generation"},{"level":4,"text":"Milestone Checkpoint Implementation","id":"milestone-checkpoint-implementation"},{"level":4,"text":"Performance Testing Framework","id":"performance-testing-framework"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Symptom-Based Diagnosis","id":"symptom-based-diagnosis"},{"level":3,"text":"Debugging Tools and Techniques","id":"debugging-tools-and-techniques"},{"level":3,"text":"Performance Troubleshooting","id":"performance-troubleshooting"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Advanced MLOps Features","id":"advanced-mlops-features"},{"level":4,"text":"Feature Store Integration","id":"feature-store-integration"},{"level":4,"text":"Automated Model Selection and Hyperparameter Optimization","id":"automated-model-selection-and-hyperparameter-optimization"},{"level":4,"text":"Multi-Tenant Support and Resource Isolation","id":"multi-tenant-support-and-resource-isolation"},{"level":3,"text":"Scale and Performance Extensions","id":"scale-and-performance-extensions"},{"level":4,"text":"Multi-Region Deployments and Global Model Serving","id":"multi-region-deployments-and-global-model-serving"},{"level":4,"text":"Edge Computing and Model Deployment","id":"edge-computing-and-model-deployment"},{"level":4,"text":"Large-Scale Training Orchestration","id":"large-scale-training-orchestration"},{"level":3,"text":"Ecosystem Integrations","id":"ecosystem-integrations"},{"level":4,"text":"ML Framework Integration","id":"ml-framework-integration"},{"level":4,"text":"Cloud Platform Integration","id":"cloud-platform-integration"},{"level":4,"text":"Third-Party Tool Integration","id":"third-party-tool-integration"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations for Extensions","id":"technology-recommendations-for-extensions"},{"level":4,"text":"Extension Development Framework","id":"extension-development-framework"},{"level":4,"text":"Extension Development Guidelines","id":"extension-development-guidelines"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Core MLOps Concepts","id":"core-mlops-concepts"},{"level":3,"text":"Architecture and Design Patterns","id":"architecture-and-design-patterns"},{"level":3,"text":"Data Management and Storage","id":"data-management-and-storage"},{"level":3,"text":"Model Lifecycle Management","id":"model-lifecycle-management"},{"level":3,"text":"Pipeline and Training Concepts","id":"pipeline-and-training-concepts"},{"level":3,"text":"Deployment and Serving","id":"deployment-and-serving"},{"level":3,"text":"Monitoring and Observability","id":"monitoring-and-observability"},{"level":3,"text":"System Integration and Communication","id":"system-integration-and-communication"},{"level":3,"text":"Error Handling and Recovery","id":"error-handling-and-recovery"},{"level":3,"text":"Testing and Quality Assurance","id":"testing-and-quality-assurance"},{"level":3,"text":"Observability and Operations","id":"observability-and-operations"},{"level":3,"text":"Advanced MLOps Features","id":"advanced-mlops-features"},{"level":3,"text":"Integration and Ecosystem","id":"integration-and-ecosystem"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"}],"title":"MLOps Platform: Design Document","markdown":"# MLOps Platform: Design Document\n\n\n## Overview\n\nAn end-to-end MLOps platform that provides a unified workflow for machine learning teams to track experiments, version models, orchestrate training pipelines, deploy models to production, and monitor their performance. The key architectural challenge is building a scalable, fault-tolerant system that integrates diverse ML tools while maintaining data lineage and reproducibility across the entire ML lifecycle.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** This foundational section provides context for all milestones (1-5) by establishing why integrated MLOps platforms are necessary.\n\nMachine learning development in most organizations resembles a chaotic research laboratory where brilliant scientists work in isolation, each maintaining their own experimental notebooks, storing samples in unmarked containers, and using incompatible equipment. While individual experiments might succeed, the lack of coordination creates a system where breakthroughs cannot be reliably reproduced, promising discoveries get lost in transition from research to production, and teams spend more time wrestling with infrastructure than advancing the science. The transformation from this ad-hoc approach to a coordinated MLOps platform mirrors the evolution from individual laboratory benches to modern pharmaceutical research facilities with standardized protocols, centralized sample management, and automated quality control systems.\n\nThe core challenge lies not in the sophistication of any single ML algorithm, but in orchestrating the complex dependencies between data preparation, model training, validation, deployment, and monitoring across teams and time. Unlike traditional software development where code artifacts are largely deterministic, machine learning introduces probabilistic models trained on evolving datasets, creating a web of dependencies that traditional DevOps tools cannot adequately manage. The platform must handle the inherent uncertainty and experimentation nature of ML development while providing the reliability and reproducibility guarantees that production systems demand.\n\n### The ML Development Chaos\n\n![MLOps Platform System Architecture](./diagrams/system-architecture.svg)\n\nConsider a typical machine learning team six months into a project. Data scientists have trained dozens of model variants, each stored on individual laptops with handwritten notes about hyperparameters. The most promising model achieved 87% accuracy, but nobody can remember which exact combination of data preprocessing, feature engineering, and training parameters produced that result. The engineering team has been waiting three weeks for a production-ready model, but every attempt to reproduce the data scientist's results yields slightly different accuracy scores. Meanwhile, the business stakeholders are asking why the model that worked perfectly in the demo now fails on real customer data, and the compliance team needs to audit the training process for a model that was deployed two months ago.\n\nThis scenario illustrates the **experiment tracking crisis** that emerges when teams scale beyond individual contributors. Without structured logging of experiments, parameters become tribal knowledge that disappears when team members change roles or forget details. The lack of **artifact management** means that models, datasets, and preprocessing code exist in dozens of versions across different machines, with no authoritative source of truth. **Reproducibility failures** compound over time as subtle environment differences, library version mismatches, and undocumented manual steps make it impossible to recreate previous results.\n\nThe **deployment bottleneck** represents another critical failure mode. In traditional software, deployment means copying stateless code to production servers. In machine learning, deployment requires coordinating model artifacts, inference serving infrastructure, data preprocessing pipelines, and monitoring systems. Teams often resort to manual deployment processes that take weeks to complete, during which time the model's performance may have already degraded due to data drift. The lack of **version management** means that rolling back a problematic model becomes a manual archaeology project to reconstruct the previous deployment state.\n\n| Problem Category | Symptoms | Root Causes | Business Impact |\n|-----------------|----------|-------------|-----------------|\n| Experiment Chaos | Lost high-performing models, unreproducible results, duplicate work | No structured logging, inconsistent environments, manual tracking | Wasted research effort, delayed model delivery |\n| Deployment Friction | Weeks-long deployment cycles, manual model updates, rollback failures | Ad-hoc serving infrastructure, no version control, manual processes | Delayed value realization, production incidents |\n| Monitoring Blindness | Silent model degradation, undetected data drift, surprise accuracy drops | No performance tracking, missing alerting, reactive debugging | Revenue loss, customer satisfaction issues |\n| Team Coordination | Duplicate experiments, incompatible toolchains, knowledge silos | No shared infrastructure, inconsistent processes, individual workflows | Reduced team velocity, knowledge loss |\n\nThe **collaboration breakdown** occurs when team members use incompatible tools and workflows. Data scientists prefer Jupyter notebooks and Python libraries, while ML engineers favor containerized services and deployment automation. Data engineers work with batch processing systems, while platform engineers focus on real-time serving infrastructure. Without a unified platform that bridges these different working styles and tool preferences, teams develop parallel systems that cannot interoperate, leading to costly integration projects and duplicated effort.\n\n### Existing MLOps Solutions\n\nThe current landscape of MLOps tools reflects the evolution from individual solutions addressing specific pain points to comprehensive platforms attempting to cover the entire ML lifecycle. Understanding the strengths and limitations of existing approaches provides crucial context for architectural decisions in building a new platform.\n\n**MLflow** represents the experiment tracking generation of tools, born from Databricks' experience with large-scale ML projects. Its core strength lies in lightweight experiment logging that requires minimal changes to existing training code. Data scientists can add a few lines of `mlflow.log_param()` and `mlflow.log_metric()` calls to their existing scripts and immediately gain experiment tracking capabilities. The model registry provides basic versioning with manual stage transitions (None → Staging → Production → Archived), while the model serving component offers simple REST API deployment for common frameworks.\n\nHowever, MLflow's simplicity becomes a limitation at enterprise scale. The deployment capabilities are minimal compared to production requirements for auto-scaling, traffic management, and high availability. The pipeline orchestration is rudimentary, essentially a thin wrapper around existing workflow tools rather than a purpose-built ML orchestration engine. Security and multi-tenancy support lag behind enterprise requirements, and the monitoring capabilities focus primarily on system metrics rather than ML-specific concerns like data drift and model performance degradation.\n\n**Kubeflow** takes the opposite approach, building a comprehensive ML platform on Kubernetes from the ground up. Its strength lies in scalability and integration with cloud-native infrastructure patterns. Kubeflow Pipelines provides sophisticated DAG-based orchestration with support for complex data dependencies and resource management. The multi-framework support through Kubeflow Training Operators enables distributed training across different ML libraries. KFServing (now KServe) offers production-grade model serving with advanced traffic management and auto-scaling capabilities.\n\nThe complexity that enables Kubeflow's power also creates adoption barriers. Teams need deep Kubernetes expertise to operate Kubeflow effectively, making it inaccessible to data science teams focused on model development rather than infrastructure management. The component ecosystem is extensive but can be overwhelming, with multiple overlapping solutions for similar problems. Configuration complexity grows exponentially as teams customize components for their specific requirements.\n\n**Cloud-native solutions** like SageMaker, Azure ML, and Vertex AI provide managed MLOps platforms that eliminate infrastructure management overhead. These platforms excel at integration with their respective cloud ecosystems, offering seamless data access, compute scaling, and billing integration. The managed nature means that teams can focus on ML development rather than platform operations. Built-in compliance features, security controls, and enterprise governance capabilities address requirements that open-source solutions often leave as integration challenges.\n\nThe trade-off for convenience is vendor lock-in and reduced flexibility. Customization options are limited to what the cloud provider supports, which may not align with specialized ML workflows or existing tool preferences. Cost optimization becomes challenging when compute resources are bundled with platform features. Migration between cloud providers requires significant reworking of ML pipelines and training code.\n\n| Solution Category | Strengths | Limitations | Best Fit Scenarios |\n|------------------|-----------|-------------|-------------------|\n| MLflow | Lightweight adoption, broad framework support, strong experiment tracking | Limited deployment capabilities, basic orchestration, minimal monitoring | Small to medium teams, research-focused organizations, rapid prototyping |\n| Kubeflow | Comprehensive ML capabilities, cloud-native architecture, scalable infrastructure | High complexity, Kubernetes expertise required, steep learning curve | Large engineering teams, cloud-native organizations, complex ML workflows |\n| Cloud Platforms | Managed infrastructure, enterprise features, ecosystem integration | Vendor lock-in, limited customization, cost optimization challenges | Enterprise organizations, cloud-first strategies, compliance-heavy industries |\n| Custom Solutions | Complete control, tailored to specific needs, no vendor dependencies | High development cost, ongoing maintenance burden, expertise requirements | Large tech companies, unique ML requirements, strong engineering teams |\n\n> **The Platform Integration Challenge**: Most organizations end up with a combination of tools that address different aspects of the ML lifecycle, but lack integration between components. Data flows through manual handoffs between experiment tracking, model registry, deployment, and monitoring systems, creating opportunities for errors and inconsistencies. The challenge is not choosing the best individual tool for each function, but designing a cohesive system where components work together seamlessly.\n\n### Core Technical Challenges\n\nBuilding an integrated MLOps platform requires solving three fundamental distributed systems challenges that emerge from the unique characteristics of machine learning workflows: **state management** across long-running experiments and training jobs, **distributed coordination** of pipeline steps with complex data dependencies, and **data consistency** guarantees in a system where artifacts, metadata, and model performance evolve continuously.\n\n#### State Management Complexity\n\nMachine learning workflows differ fundamentally from traditional web services in their state management requirements. While web applications typically handle stateless requests that complete in milliseconds, ML training jobs run for hours or days while continuously generating state that must be preserved, queried, and correlated across multiple dimensions.\n\nConsider the state that accumulates during a single training experiment: hyperparameters selected at the beginning, training metrics logged at each epoch, model checkpoints saved periodically, validation scores computed on holdout data, and artifacts like feature importance plots generated at completion. This state has several challenging characteristics that traditional databases struggle to handle efficiently.\n\nThe **temporal dimension** creates query complexity that standard relational models handle poorly. Analysts want to visualize how training loss decreased over time across multiple experiments, comparing learning curves between different hyperparameter configurations. This requires efficient time-series queries across potentially millions of metric points, with grouping and aggregation capabilities that span experiment boundaries. The naive approach of storing each metric point as a database row quickly becomes prohibitive at scale, while specialized time-series solutions often lack the metadata correlation capabilities needed for experiment analysis.\n\nThe **hierarchical experiment organization** adds another layer of complexity. Teams organize experiments into projects, with runs grouped by model architecture or dataset version. Individual runs contain multiple training phases (preprocessing, training, evaluation), each generating its own metrics and artifacts. The metadata relationships form a complex graph rather than simple tree structures, as models may inherit preprocessed datasets from previous experiments or use transfer learning from models trained in different projects.\n\n**Artifact lifecycle management** presents unique challenges because ML artifacts have different access patterns than typical file storage. Model checkpoints may be large (gigabytes) but accessed infrequently after training completion. Training datasets are read-heavy during active experimentation but rarely modified. Experiment notebooks and plots are small but require fast access for interactive analysis. The storage system must optimize for these mixed workloads while maintaining strong consistency guarantees for metadata queries.\n\n> **Design Insight**: The state management challenge requires treating experiments as long-lived, stateful entities rather than fire-and-forget jobs. The platform must provide transactional guarantees for experiment metadata updates while supporting high-throughput writes for metric logging and efficient reads for analysis queries.\n\n#### Distributed Coordination Challenges\n\nML pipeline orchestration introduces coordination complexity that traditional workflow engines struggle to handle effectively. Unlike business process workflows where task dependencies are primarily sequential, ML pipelines often involve complex data dependencies, resource contention, and conditional execution paths that must be coordinated across distributed compute resources.\n\n**Data dependency coordination** represents the most complex aspect of ML pipeline orchestration. Consider a training pipeline that preprocesses data, trains multiple model variants in parallel, evaluates each variant on different test sets, and selects the best performer for deployment. The data dependencies form a complex graph where downstream steps may require outputs from multiple upstream steps, but those outputs may be generated at different times and on different compute nodes.\n\nThe naive approach of copying data between steps quickly becomes prohibitive when working with large datasets. Instead, the orchestration system must coordinate **data locality** to minimize transfer overhead while ensuring that compute resources are available when data dependencies are satisfied. This requires sophisticated scheduling that considers both data placement and resource availability across the cluster.\n\n**Resource allocation complexity** emerges from the heterogeneous compute requirements of ML workloads. Data preprocessing steps may require high-memory nodes, training steps need GPU clusters, and evaluation steps can run on standard CPU instances. The resource requirements may not be known until runtime, as they depend on data characteristics discovered during preprocessing. The orchestration system must support **dynamic resource allocation** while preventing resource starvation and ensuring fair sharing across concurrent pipelines.\n\n**Fault tolerance** in ML pipelines requires more sophisticated recovery strategies than traditional workflows. Training jobs may run for days, making simple restart-from-beginning policies prohibitively expensive. Instead, the system must support **checkpoint-based recovery** where failed training steps can resume from the last saved checkpoint rather than restarting completely. However, checkpoint consistency across distributed training jobs introduces additional complexity, as the system must ensure that all workers reach checkpoints synchronously to maintain model consistency.\n\nThe **conditional execution** patterns common in ML workflows challenge traditional DAG-based orchestration models. Hyperparameter search involves executing training steps with different parameters until convergence criteria are met. Early stopping may terminate training steps before completion based on validation metrics. Model selection may choose between alternative preprocessing or training approaches based on data characteristics discovered at runtime. These patterns require **dynamic pipeline modification** capabilities that go beyond static DAG execution.\n\n| Coordination Challenge | Traditional Workflow Approach | ML-Specific Requirements | Technical Solutions Needed |\n|----------------------|------------------------------|------------------------|---------------------------|\n| Data Dependencies | File-based handoffs between steps | Efficient large dataset transfer, data lineage tracking | Distributed storage integration, metadata-driven coordination |\n| Resource Management | Static resource allocation | Dynamic GPU/CPU requirements, long-running jobs | Resource scheduling with ML-aware policies |\n| Fault Tolerance | Restart failed tasks | Checkpoint recovery, distributed training consistency | Stateful step recovery, coordinated checkpointing |\n| Conditional Execution | Static DAG execution | Hyperparameter search, early stopping, model selection | Dynamic pipeline modification, event-driven orchestration |\n\n#### Data Consistency Guarantees\n\nThe distributed nature of MLOps platforms creates consistency challenges that span multiple storage systems and involve both structured metadata and unstructured artifacts. Unlike traditional applications where consistency requirements are localized to a single database, MLOps platforms must maintain consistency across experiment tracking databases, artifact storage systems, model registries, and deployment infrastructure.\n\n**Cross-system consistency** emerges when an experiment run generates metadata stored in a relational database, artifacts stored in object storage, and model versions registered in a separate registry system. These updates must appear atomic from the perspective of platform users, even though they involve multiple storage systems with different consistency guarantees. Consider the failure scenario where experiment metadata is successfully written but artifact upload fails due to network issues. Without proper coordination, the experiment appears complete in queries but missing critical artifacts needed for reproducibility.\n\nThe challenge intensifies with **model promotion workflows** that must coordinate updates across multiple systems. Promoting a model from staging to production involves updating the model registry metadata, deploying new serving infrastructure, updating traffic routing configuration, and initializing monitoring data collection. These updates must appear atomic to prevent inconsistent states where traffic routes to an undeployed model or monitoring systems track the wrong model version.\n\n**Eventual consistency trade-offs** become critical in distributed MLOps systems that span multiple geographic regions or cloud providers. Experiment data logged in one region must be available for analysis in other regions, but strict consistency requirements would make the system unusable during network partitions. The platform must carefully choose which consistency guarantees are essential for correctness and which operations can tolerate eventual consistency.\n\n**Metadata vs. Artifact consistency** requires different strategies due to the size and access pattern differences between structured metadata and large binary artifacts. Experiment metadata must be immediately consistent to support real-time analysis and comparison queries. Artifact storage can tolerate eventual consistency as long as metadata accurately reflects artifact availability. This suggests a **two-tier consistency model** where metadata operations require strong consistency while artifact operations can be eventually consistent.\n\nThe **lineage consistency** challenge ensures that model lineage information remains accurate as experiments, models, and deployments evolve. Model lineage links deployed models back to their source experiments, training data versions, and code commits. As these entities are updated or archived, the lineage relationships must be maintained consistently to support compliance and debugging requirements. This requires **referential integrity** across multiple storage systems and careful handling of deletion operations.\n\n> **Decision: Event-Driven Consistency Architecture**\n> - **Context**: MLOps platforms must maintain consistency across multiple storage systems while supporting high-throughput operations and geographic distribution.\n> - **Options Considered**: \n>   1. Two-phase commit across all storage systems\n>   2. Event-driven eventual consistency with compensation\n>   3. Single-system consistency with data duplication\n> - **Decision**: Implement event-driven consistency with compensation for non-critical operations and two-phase commit only for critical state transitions\n> - **Rationale**: Two-phase commit across all systems would create availability and performance bottlenecks. Single-system approaches limit scalability and increase vendor lock-in. Event-driven consistency allows optimizing consistency vs. performance trade-offs per operation type.\n> - **Consequences**: Enables high-throughput experiment logging with eventual consistency while maintaining strong consistency for critical operations like model promotion. Requires sophisticated event handling and compensation logic.\n\n⚠️ **Pitfall: Ignoring Consistency Boundaries**\n\nA common mistake in MLOps platform design is treating all operations as requiring the same consistency guarantees. Teams often either apply strong consistency everywhere (creating performance bottlenecks) or eventual consistency everywhere (creating correctness issues). The correct approach is carefully identifying which operations require strong consistency (model promotion, experiment completion) and which can tolerate eventual consistency (metric logging, artifact availability). This requires explicit consistency boundary design rather than relying on default database settings.\n\n### Implementation Guidance\n\nUnderstanding the context and problems that MLOps platforms solve provides the foundation for making informed architectural decisions. The following technical recommendations help translate these insights into concrete implementation choices.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Experiment Metadata | PostgreSQL with JSONB columns | Time-series DB (InfluxDB) + metadata DB (PostgreSQL) |\n| Artifact Storage | Local filesystem with backup | Object storage (S3/GCS/Azure Blob) with CDN |\n| Pipeline Orchestration | Simple job queue (Redis/RabbitMQ) | Kubernetes-native workflows (Argo/Tekton) |\n| Model Serving | HTTP REST with Python Flask/FastAPI | Kubernetes + Istio with traffic splitting |\n| Monitoring | Application logs + basic metrics | Dedicated observability platform (Prometheus/Grafana) |\n| Message Coordination | HTTP APIs for synchronous coordination | Event streaming (Kafka/Pulsar) for async coordination |\n\n#### Recommended Project Structure\n\n```\nmlops-platform/\n├── cmd/                          # Entry points for different services\n│   ├── experiment-server/        # Experiment tracking HTTP API\n│   ├── model-registry/           # Model registry service\n│   ├── pipeline-orchestrator/    # Training pipeline scheduler\n│   ├── deployment-manager/       # Model deployment controller\n│   └── monitoring-collector/     # Model performance monitoring\n├── internal/                     # Private application code\n│   ├── storage/                  # Storage layer abstractions\n│   │   ├── metadata/             # Experiment and model metadata\n│   │   ├── artifacts/            # Model and artifact storage\n│   │   └── timeseries/           # Metrics and monitoring data\n│   ├── coordination/             # Cross-component communication\n│   │   ├── events/               # Event-driven messaging\n│   │   └── apis/                 # Internal API definitions\n│   └── common/                   # Shared utilities and types\n├── api/                          # Public API definitions\n│   ├── rest/                     # REST API specifications\n│   └── proto/                    # Protocol buffer definitions (if using gRPC)\n├── web/                          # Web dashboard and UI\n├── deploy/                       # Deployment configurations\n│   ├── docker/                   # Docker configurations\n│   ├── k8s/                      # Kubernetes manifests\n│   └── terraform/                # Infrastructure as code\n└── docs/                         # Documentation and design docs\n```\n\n#### Foundation Infrastructure Code\n\nThe following complete implementations provide the infrastructure foundation that subsequent components will build upon:\n\n**Event Coordination System** (`internal/coordination/events/coordinator.go`):\n\n```python\n\"\"\"\nEvent coordination system for cross-component communication.\nProvides reliable event publishing and subscription for MLOps workflows.\n\"\"\"\n\nimport json\nimport threading\nimport time\nfrom typing import Dict, List, Callable, Any\nfrom dataclasses import dataclass, asdict\nfrom queue import Queue, Empty\nimport uuid\n\n@dataclass\nclass Event:\n    \"\"\"Represents a system event with metadata and payload.\"\"\"\n    id: str\n    type: str\n    source: str\n    timestamp: float\n    payload: Dict[str, Any]\n    \n    @classmethod\n    def create(cls, event_type: str, source: str, payload: Dict[str, Any]):\n        \"\"\"Create a new event with auto-generated ID and timestamp.\"\"\"\n        return cls(\n            id=str(uuid.uuid4()),\n            type=event_type,\n            source=source,\n            timestamp=time.time(),\n            payload=payload\n        )\n\nclass EventCoordinator:\n    \"\"\"\n    Coordinates events between MLOps platform components.\n    Supports both synchronous and asynchronous event handling patterns.\n    \"\"\"\n    \n    def __init__(self):\n        self._subscribers: Dict[str, List[Callable]] = {}\n        self._event_queue = Queue()\n        self._running = False\n        self._worker_thread = None\n        self._lock = threading.RLock()\n    \n    def start(self):\n        \"\"\"Start the event processing worker thread.\"\"\"\n        with self._lock:\n            if not self._running:\n                self._running = True\n                self._worker_thread = threading.Thread(target=self._process_events)\n                self._worker_thread.daemon = True\n                self._worker_thread.start()\n    \n    def stop(self):\n        \"\"\"Stop event processing and wait for worker thread completion.\"\"\"\n        with self._lock:\n            self._running = False\n            if self._worker_thread:\n                self._worker_thread.join(timeout=5.0)\n    \n    def publish(self, event: Event, synchronous: bool = False):\n        \"\"\"\n        Publish an event to all registered subscribers.\n        \n        Args:\n            event: Event to publish\n            synchronous: If True, process immediately. If False, queue for async processing.\n        \"\"\"\n        if synchronous:\n            self._deliver_event(event)\n        else:\n            self._event_queue.put(event)\n    \n    def subscribe(self, event_type: str, handler: Callable[[Event], None]):\n        \"\"\"\n        Subscribe to events of a specific type.\n        \n        Args:\n            event_type: Type of event to subscribe to (e.g., \"experiment.completed\")\n            handler: Function to call when event is received\n        \"\"\"\n        with self._lock:\n            if event_type not in self._subscribers:\n                self._subscribers[event_type] = []\n            self._subscribers[event_type].append(handler)\n    \n    def _process_events(self):\n        \"\"\"Worker thread function that processes queued events.\"\"\"\n        while self._running:\n            try:\n                event = self._event_queue.get(timeout=1.0)\n                self._deliver_event(event)\n            except Empty:\n                continue\n    \n    def _deliver_event(self, event: Event):\n        \"\"\"Deliver event to all registered subscribers for the event type.\"\"\"\n        with self._lock:\n            subscribers = self._subscribers.get(event.type, [])\n            for handler in subscribers:\n                try:\n                    handler(event)\n                except Exception as e:\n                    # Log error but continue processing other subscribers\n                    print(f\"Error in event handler for {event.type}: {e}\")\n\n# Global event coordinator instance\nevent_coordinator = EventCoordinator()\n```\n\n**Storage Abstraction Layer** (`internal/storage/base.py`):\n\n```python\n\"\"\"\nStorage abstraction layer providing unified interface for different storage backends.\nSupports both metadata (structured) and artifact (binary) storage patterns.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional, BinaryIO\nimport os\nimport json\nimport sqlite3\nfrom pathlib import Path\n\nclass MetadataStore(ABC):\n    \"\"\"Abstract interface for structured metadata storage.\"\"\"\n    \n    @abstractmethod\n    def create_table(self, table_name: str, schema: Dict[str, str]):\n        \"\"\"Create a table with the specified schema.\"\"\"\n        pass\n    \n    @abstractmethod\n    def insert(self, table_name: str, data: Dict[str, Any]) -> str:\n        \"\"\"Insert data and return the generated ID.\"\"\"\n        pass\n    \n    @abstractmethod\n    def update(self, table_name: str, id: str, data: Dict[str, Any]) -> bool:\n        \"\"\"Update existing record by ID.\"\"\"\n        pass\n    \n    @abstractmethod\n    def query(self, table_name: str, filters: Dict[str, Any] = None, \n             order_by: str = None, limit: int = None) -> List[Dict[str, Any]]:\n        \"\"\"Query records with optional filtering, ordering, and limiting.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_by_id(self, table_name: str, id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get a single record by ID.\"\"\"\n        pass\n\nclass SQLiteMetadataStore(MetadataStore):\n    \"\"\"SQLite implementation of metadata storage for development/testing.\"\"\"\n    \n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        os.makedirs(os.path.dirname(db_path), exist_ok=True)\n        self._conn = sqlite3.connect(db_path, check_same_thread=False)\n        self._conn.row_factory = sqlite3.Row  # Enable column access by name\n    \n    def create_table(self, table_name: str, schema: Dict[str, str]):\n        \"\"\"Create table with columns defined in schema dict.\"\"\"\n        columns = [f\"{name} {type_def}\" for name, type_def in schema.items()]\n        sql = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(columns)})\"\n        self._conn.execute(sql)\n        self._conn.commit()\n    \n    def insert(self, table_name: str, data: Dict[str, Any]) -> str:\n        \"\"\"Insert data and return the row ID as string.\"\"\"\n        columns = list(data.keys())\n        placeholders = ['?' for _ in columns]\n        values = [data[col] for col in columns]\n        \n        sql = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({', '.join(placeholders)})\"\n        cursor = self._conn.execute(sql, values)\n        self._conn.commit()\n        return str(cursor.lastrowid)\n    \n    def update(self, table_name: str, id: str, data: Dict[str, Any]) -> bool:\n        \"\"\"Update record by ID, return True if successful.\"\"\"\n        set_clause = ', '.join([f\"{col} = ?\" for col in data.keys()])\n        values = list(data.values()) + [id]\n        \n        sql = f\"UPDATE {table_name} SET {set_clause} WHERE id = ?\"\n        cursor = self._conn.execute(sql, values)\n        self._conn.commit()\n        return cursor.rowcount > 0\n    \n    def query(self, table_name: str, filters: Dict[str, Any] = None, \n             order_by: str = None, limit: int = None) -> List[Dict[str, Any]]:\n        \"\"\"Query with optional WHERE, ORDER BY, and LIMIT clauses.\"\"\"\n        sql = f\"SELECT * FROM {table_name}\"\n        values = []\n        \n        if filters:\n            where_clause = ' AND '.join([f\"{col} = ?\" for col in filters.keys()])\n            sql += f\" WHERE {where_clause}\"\n            values.extend(filters.values())\n        \n        if order_by:\n            sql += f\" ORDER BY {order_by}\"\n        \n        if limit:\n            sql += f\" LIMIT {limit}\"\n        \n        cursor = self._conn.execute(sql, values)\n        return [dict(row) for row in cursor.fetchall()]\n    \n    def get_by_id(self, table_name: str, id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get single record by ID.\"\"\"\n        cursor = self._conn.execute(f\"SELECT * FROM {table_name} WHERE id = ?\", [id])\n        row = cursor.fetchone()\n        return dict(row) if row else None\n\nclass ArtifactStore(ABC):\n    \"\"\"Abstract interface for binary artifact storage.\"\"\"\n    \n    @abstractmethod\n    def put(self, key: str, data: BinaryIO, metadata: Dict[str, str] = None) -> bool:\n        \"\"\"Store binary data with optional metadata.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get(self, key: str) -> Optional[BinaryIO]:\n        \"\"\"Retrieve binary data by key.\"\"\"\n        pass\n    \n    @abstractmethod\n    def delete(self, key: str) -> bool:\n        \"\"\"Delete artifact by key.\"\"\"\n        pass\n    \n    @abstractmethod\n    def list_keys(self, prefix: str = \"\") -> List[str]:\n        \"\"\"List artifact keys with optional prefix filter.\"\"\"\n        pass\n\nclass FilesystemArtifactStore(ArtifactStore):\n    \"\"\"Local filesystem implementation for artifact storage.\"\"\"\n    \n    def __init__(self, base_path: str):\n        self.base_path = Path(base_path)\n        self.base_path.mkdir(parents=True, exist_ok=True)\n    \n    def put(self, key: str, data: BinaryIO, metadata: Dict[str, str] = None) -> bool:\n        \"\"\"Store data to filesystem with metadata as JSON sidecar file.\"\"\"\n        try:\n            artifact_path = self.base_path / key\n            artifact_path.parent.mkdir(parents=True, exist_ok=True)\n            \n            # Write binary data\n            with open(artifact_path, 'wb') as f:\n                f.write(data.read())\n            \n            # Write metadata sidecar file\n            if metadata:\n                metadata_path = artifact_path.with_suffix('.metadata.json')\n                with open(metadata_path, 'w') as f:\n                    json.dump(metadata, f)\n            \n            return True\n        except Exception:\n            return False\n    \n    def get(self, key: str) -> Optional[BinaryIO]:\n        \"\"\"Read binary data from filesystem.\"\"\"\n        artifact_path = self.base_path / key\n        if artifact_path.exists():\n            return open(artifact_path, 'rb')\n        return None\n    \n    def delete(self, key: str) -> bool:\n        \"\"\"Delete artifact and metadata files.\"\"\"\n        try:\n            artifact_path = self.base_path / key\n            metadata_path = artifact_path.with_suffix('.metadata.json')\n            \n            if artifact_path.exists():\n                artifact_path.unlink()\n            if metadata_path.exists():\n                metadata_path.unlink()\n            \n            return True\n        except Exception:\n            return False\n    \n    def list_keys(self, prefix: str = \"\") -> List[str]:\n        \"\"\"List all artifact keys with optional prefix filter.\"\"\"\n        pattern = f\"{prefix}*\" if prefix else \"*\"\n        paths = self.base_path.glob(pattern)\n        # Filter out metadata files and return relative paths\n        return [str(p.relative_to(self.base_path)) for p in paths \n                if p.is_file() and not p.name.endswith('.metadata.json')]\n```\n\n#### Core Component Integration Patterns\n\n**Component Health Monitoring** (`internal/common/health.py`):\n\n```python\n\"\"\"\nHealth monitoring utilities for component coordination and debugging.\nProvides standardized health checks and status reporting across components.\n\"\"\"\n\nimport time\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass HealthStatus(Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass HealthCheck:\n    \"\"\"Individual health check result.\"\"\"\n    name: str\n    status: HealthStatus\n    message: str\n    timestamp: float\n    details: Dict[str, Any]\n\nclass ComponentHealth:\n    \"\"\"\n    Manages health checks for a single component.\n    Used by each major component to report its health status.\n    \"\"\"\n    \n    def __init__(self, component_name: str):\n        self.component_name = component_name\n        self._checks: Dict[str, HealthCheck] = {}\n        self._last_update = time.time()\n    \n    def add_check(self, check_name: str, check_func: callable):\n        \"\"\"Register a health check function.\"\"\"\n        # TODO 1: Store the check function for periodic execution\n        # TODO 2: Add validation that check_func returns HealthCheck object\n        # TODO 3: Set up periodic execution timer (every 30 seconds)\n        pass\n    \n    def run_checks(self) -> Dict[str, HealthCheck]:\n        \"\"\"Execute all registered health checks and return results.\"\"\"\n        # TODO 1: Iterate through all registered check functions\n        # TODO 2: Execute each function and capture HealthCheck result\n        # TODO 3: Handle exceptions by creating UNHEALTHY HealthCheck\n        # TODO 4: Update self._checks dict with results\n        # TODO 5: Update self._last_update timestamp\n        # TODO 6: Return the updated checks dictionary\n        pass\n    \n    def get_overall_status(self) -> HealthStatus:\n        \"\"\"Determine overall component health based on individual checks.\"\"\"\n        # TODO 1: If no checks registered, return UNKNOWN\n        # TODO 2: If any check is UNHEALTHY, return UNHEALTHY\n        # TODO 3: If any check is DEGRADED, return DEGRADED\n        # TODO 4: If all checks are HEALTHY, return HEALTHY\n        # TODO 5: Consider check staleness (> 5 minutes old = UNKNOWN)\n        pass\n```\n\n#### Language-Specific Implementation Hints\n\nFor Python-based MLOps platform development:\n\n- **Use SQLAlchemy for metadata storage** to support multiple database backends while maintaining type safety with declarative models\n- **Implement async/await patterns** for I/O-heavy operations like artifact uploads and database queries to improve concurrent throughput\n- **Use Pydantic models** for API request/response validation and serialization, ensuring type safety across component boundaries  \n- **Leverage pytest fixtures** for creating test data factories that generate realistic experiment data for component testing\n- **Use structlog for structured logging** to enable correlation of log messages across distributed components using trace IDs\n- **Implement circuit breaker patterns** using libraries like `pybreaker` for external service calls to prevent cascade failures\n- **Use celery or RQ for background tasks** like artifact processing, model training orchestration, and monitoring data aggregation\n\n#### Platform Development Checkpoints\n\nAfter implementing the foundational infrastructure:\n\n**Checkpoint 1: Event System**\n- Run: `python -m pytest tests/test_event_coordinator.py`\n- Verify: Events published asynchronously are delivered to subscribers within 1 second\n- Manual Test: Start coordinator, subscribe to \"test.event\", publish event, confirm handler execution\n- Debug: If events aren't delivered, check that `start()` was called and worker thread is running\n\n**Checkpoint 2: Storage Layer**\n- Run: `python -m pytest tests/test_storage.py`\n- Verify: Metadata operations support concurrent access without data corruption\n- Manual Test: Store and retrieve a binary artifact, confirm metadata sidecar file creation\n- Debug: If queries fail, check table schema matches expected column types in test data\n\n**Common Integration Issues**\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| Events not delivered | Event coordinator not started | Check `_running` flag and worker thread status | Call `event_coordinator.start()` before publishing |\n| Storage queries timeout | Database connection pool exhaustion | Monitor open connection count | Implement connection pooling with max limits |\n| Artifact uploads fail | Insufficient filesystem permissions | Check directory write permissions | Ensure artifact storage directory has write access |\n| Component health unknown | Health checks not registered | Verify `add_check()` calls in component initialization | Add health checks in component `__init__` method |\n\nThis foundational infrastructure provides the building blocks for implementing the experiment tracking, model registry, pipeline orchestration, deployment, and monitoring components. Each component will extend these base patterns while adding domain-specific functionality for their particular aspect of the ML lifecycle.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** This foundational section defines the scope and success criteria for all milestones (1-5) by establishing clear boundaries and requirements that guide implementation decisions across the entire platform.\n\nBuilding an MLOps platform is like designing a city's infrastructure - you need clear zoning laws, building codes, and service level agreements before breaking ground. Without explicit goals and boundaries, feature creep transforms a focused platform into an unwieldy monolith that serves no one well. This section establishes the platform's charter: what we commit to building, how we measure success, and equally important, what we explicitly won't attempt.\n\nThe challenge with MLOps platforms lies in their inherent complexity - they sit at the intersection of data engineering, machine learning, software engineering, and DevOps. Each discipline brings its own requirements, tools, and expectations. A data scientist wants seamless experiment tracking that doesn't interrupt their research flow. An ML engineer needs reliable pipeline orchestration that handles distributed training workloads. A production engineer demands robust model serving with sub-100ms latency guarantees. Platform engineers require clear APIs, comprehensive monitoring, and straightforward operational procedures.\n\nOur goal setting process must balance these competing demands while maintaining architectural coherence. We'll define functional requirements that capture what the platform must accomplish, quality attributes that specify how well it must perform, and explicit non-goals that prevent scope expansion into adjacent problem domains.\n\n### Functional Requirements\n\nThink of functional requirements as the platform's job description - the specific tasks it must complete successfully across the machine learning lifecycle. Each requirement maps to one or more platform components and defines measurable acceptance criteria.\n\n**Experiment Tracking Capabilities**\n\nThe platform must provide comprehensive experiment tracking that captures the full context of machine learning experiments. This goes beyond simple logging - we're building a research laboratory information management system that maintains scientific rigor while supporting rapid experimentation.\n\n| Requirement | Component | Acceptance Criteria | Business Value |\n|-------------|-----------|-------------------|----------------|\n| Parameter Logging | Experiment Tracking | Record hyperparameter key-value pairs with automatic type inference and validation | Enables systematic hyperparameter optimization and reproducible research |\n| Metric Tracking | Experiment Tracking | Store time-series metrics with step numbers, timestamps, and statistical aggregations | Supports learning curve analysis and model performance comparison |\n| Artifact Management | Experiment Tracking | Version and store binary artifacts (models, plots, datasets) with content hashing | Ensures experiment reproducibility and enables artifact reuse |\n| Run Comparison | Experiment Tracking | Side-by-side comparison of parameters, metrics, and artifacts across multiple runs | Accelerates model development through systematic analysis |\n| Experiment Organization | Experiment Tracking | Hierarchical grouping of related runs with tagging and search capabilities | Improves research organization and knowledge sharing |\n| Lineage Tracking | Experiment Tracking | Link experiments to source code commits, data versions, and environmental metadata | Enables complete experiment reproduction and debugging |\n\nThe experiment tracking system must handle the chaotic nature of ML research while maintaining data integrity. Data scientists often run dozens of experiments daily, each generating megabytes of artifacts and thousands of metric data points. The system must capture this information automatically without disrupting the research workflow.\n\n**Model Registry and Versioning**\n\nModel registry requirements center on treating trained models as first-class software artifacts with proper versioning, lifecycle management, and governance controls. This is similar to how Docker Hub manages container images, but with ML-specific metadata and approval workflows.\n\n| Requirement | Component | Acceptance Criteria | Business Value |\n|-------------|-----------|-------------------|----------------|\n| Model Registration | Model Registry | Register models with semantic versioning, accuracy metrics, and source experiment linkage | Creates authoritative model catalog for organizational knowledge management |\n| Stage Management | Model Registry | Automated promotion through Development, Staging, Production, and Archived stages | Enforces quality gates and reduces production deployment risk |\n| Approval Workflows | Model Registry | Configurable approval processes with role-based permissions for stage transitions | Ensures model quality and maintains compliance audit trails |\n| Model Lineage | Model Registry | Trace models to training data versions, code commits, and experiment runs | Enables impact analysis and supports regulatory compliance |\n| Metadata Search | Model Registry | Query models by name, version, stage, metrics, tags, and custom attributes | Facilitates model discovery and reuse across teams |\n| Immutable Storage | Model Registry | Content-addressable storage with integrity validation and audit logging | Guarantees model reproducibility and prevents unauthorized modifications |\n\nThe model registry serves as the authoritative source of truth for all organizational models. It must enforce governance policies while remaining flexible enough to support diverse ML frameworks and deployment patterns. The approval workflow system needs to be configurable - some organizations require manual sign-offs for production models, while others prefer automated promotion based on performance thresholds.\n\n**Training Pipeline Orchestration**\n\nPipeline orchestration requirements focus on coordinating complex, multi-step training workflows that span from data preparation through model evaluation. This is analogous to manufacturing process control - we need precise coordination, resource management, and quality checkpoints throughout the production line.\n\n| Requirement | Component | Training Pipeline | Acceptance Criteria | Business Value |\n|-------------|-----------|------------------|-------------------|----------------|\n| DAG Definition | Training Pipeline | Declarative pipeline specification with step dependencies and conditional execution | Enables complex workflow automation and reduces manual coordination overhead |\n| Resource Management | Training Pipeline | Dynamic allocation of CPU, memory, GPU resources with queue management | Optimizes infrastructure utilization and reduces training costs |\n| Distributed Training | Training Pipeline | Support for multi-GPU and multi-node training with parameter server coordination | Enables large-scale model training that exceeds single-machine capabilities |\n| Fault Tolerance | Training Pipeline | Automatic retry, checkpoint recovery, and partial failure handling | Reduces pipeline maintenance overhead and improves training reliability |\n| Data Validation | Training Pipeline | Schema validation, statistical profiling, and drift detection at pipeline ingestion | Prevents silent training failures caused by data quality issues |\n| Parallel Execution | Training Pipeline | Concurrent execution of independent pipeline steps with dependency tracking | Reduces total pipeline runtime and improves resource efficiency |\n\nTraining pipelines must handle the unique challenges of ML workloads - long-running processes, expensive compute resources, and complex data dependencies. The orchestration engine needs to be sophisticated enough to handle distributed training coordination while remaining simple enough for data scientists to define their workflows declaratively.\n\n**Model Deployment and Serving**\n\nDeployment requirements center on transforming trained models into production-ready services that meet enterprise reliability and performance standards. This involves more than simple model hosting - we're building a complete model serving infrastructure with traffic management, performance optimization, and operational controls.\n\n| Requirement | Component | Acceptance Criteria | Business Value |\n|-------------|-----------|-------------------|----------------|\n| HTTP API Generation | Model Deployment | Automatic REST endpoint creation with OpenAPI specifications and request validation | Standardizes model serving interfaces and reduces integration complexity |\n| Auto-scaling | Model Deployment | Dynamic replica scaling based on request rate, latency percentiles, and resource utilization | Maintains performance SLAs while optimizing infrastructure costs |\n| Canary Deployments | Model Deployment | Gradual traffic shifting with configurable rollout rates and automatic rollback triggers | Reduces deployment risk and enables safe production updates |\n| A/B Testing | Model Deployment | Traffic splitting between model versions with statistical significance testing | Enables data-driven model selection and performance validation |\n| Performance Optimization | Model Deployment | Model compilation, batching, caching, and hardware acceleration integration | Achieves production latency requirements and maximizes throughput |\n| Multi-framework Support | Model Deployment | Integration with TensorFlow Serving, TorchServe, Triton, and custom serving containers | Supports diverse ML frameworks and deployment patterns |\n\nModel deployment must bridge the gap between research models and production requirements. Research models often prioritize accuracy over inference speed, use frameworks optimized for experimentation rather than serving, and lack the error handling needed for production traffic. The deployment system must handle these transformations automatically while maintaining the model's predictive behavior.\n\n**Production Model Monitoring**\n\nMonitoring requirements focus on maintaining model performance and detecting degradation in production environments. This goes beyond traditional application monitoring to include ML-specific concerns like data drift, concept drift, and prediction quality assessment.\n\n| Requirement | Component | Acceptance Criteria | Business Value |\n|-------------|-----------|-------------------|----------------|\n| Prediction Logging | Model Monitoring | Capture all inference requests and responses with configurable sampling rates | Enables model performance analysis and debugging of production issues |\n| Performance Metrics | Model Monitoring | Track latency percentiles, throughput, error rates, and resource utilization | Ensures SLA compliance and identifies performance bottlenecks |\n| Data Drift Detection | Model Monitoring | Statistical comparison of live and training feature distributions with alert thresholds | Detects when model inputs change, indicating potential performance degradation |\n| Model Drift Monitoring | Model Monitoring | Track prediction distribution changes and accuracy degradation over time | Identifies when models need retraining due to concept drift |\n| Alerting System | Model Monitoring | Configurable alerts based on performance metrics, drift scores, and business KPIs | Enables proactive response to model degradation before business impact |\n| Dashboard Visualization | Model Monitoring | Real-time dashboards showing model health, prediction trends, and drift analysis | Provides operational visibility and supports data-driven model management decisions |\n\nProduction monitoring must detect subtle changes in model behavior that could indicate performance degradation. Unlike traditional software, ML models can silently degrade as the world changes around them. The monitoring system must be sensitive enough to detect these changes early while avoiding false alarms that lead to alert fatigue.\n\n### Quality Attributes\n\nQuality attributes define how well the platform must perform its functional requirements. These non-functional requirements often determine platform adoption success more than feature completeness. Think of these as the platform's service level agreements with its users.\n\n**Performance Requirements**\n\nPerformance requirements must account for the diverse workload patterns across the ML lifecycle. Experiment tracking deals with burst writes during model training, the model registry handles occasional large model uploads, pipelines require sustained compute orchestration, deployment demands low-latency serving, and monitoring processes continuous high-throughput logging.\n\n| Component | Metric | Target | Measurement Method | Rationale |\n|-----------|--------|--------|-------------------|-----------|\n| Experiment Tracking | Metric logging latency | < 10ms p95 | Client-side timing instrumentation | Must not interrupt training loops with slow logging calls |\n| Experiment Tracking | Artifact upload throughput | > 100 MB/s per client | Server-side transfer rate monitoring | Large model artifacts require fast upload to maintain researcher productivity |\n| Model Registry | Model download latency | < 500ms for models up to 1GB | End-to-end deployment timing | Deployment pipelines need fast model retrieval to minimize downtime |\n| Training Pipeline | Step scheduling latency | < 30 seconds from trigger to execution | Orchestrator internal metrics | Quick pipeline response maintains development velocity |\n| Model Deployment | Inference latency | < 100ms p99 including model execution | Request timing at load balancer | Production SLAs typically require sub-second response times |\n| Model Monitoring | Log processing delay | < 5 minutes from request to dashboard | Event timestamp comparison | Timely monitoring enables rapid response to production issues |\n\nPerformance targets must be realistic given the underlying infrastructure constraints while aggressive enough to support productive ML workflows. These targets assume reasonable hardware - cloud instances with NVMe storage, gigabit networking, and modern CPUs. The platform should gracefully degrade performance rather than failing completely when targets cannot be met.\n\n> **Design Insight**: Performance requirements for MLOps platforms differ fundamentally from traditional web applications. Experiment tracking has burst write patterns during training with long idle periods. Model deployment requires cold start optimization for auto-scaling. Monitoring systems must handle high-cardinality metrics from diverse model types. The platform must be architected to handle these unique performance characteristics.\n\n**Scalability Requirements**\n\nScalability requirements must accommodate organizational growth patterns - more data scientists, larger datasets, increased model complexity, and higher production traffic. The platform should scale elastically with usage rather than requiring manual capacity planning.\n\n| Dimension | Current Target | Growth Target | Scaling Strategy | Bottleneck Mitigation |\n|-----------|---------------|---------------|------------------|---------------------|\n| Concurrent experiments | 50 simultaneous runs | 500 simultaneous runs | Horizontal scaling of tracking workers | Partition experiment data by user/team |\n| Model registry size | 10,000 models | 100,000 models | Distributed storage with metadata sharding | Content-addressable storage with deduplication |\n| Pipeline complexity | 50 steps per pipeline | 500 steps per pipeline | Distributed DAG execution | Step-level parallelization and resource isolation |\n| Serving throughput | 10,000 requests/second | 100,000 requests/second | Auto-scaling with multi-region deployment | Request batching and model compilation optimization |\n| Monitoring data volume | 1TB/day prediction logs | 10TB/day prediction logs | Stream processing with data tiering | Configurable retention and sampling policies |\n\nThe platform must scale both vertically (handling larger individual workloads) and horizontally (serving more concurrent users). Vertical scaling supports larger models, longer training runs, and more complex pipelines. Horizontal scaling supports organizational growth and increased platform adoption.\n\n**Reliability Requirements**\n\nReliability requirements ensure the platform remains available and consistent despite infrastructure failures, human errors, and unexpected load patterns. ML workflows often run for hours or days, making fault tolerance critical for productivity.\n\n| Component | Availability Target | Recovery Time | Data Durability | Consistency Model |\n|-----------|-------------------|---------------|-----------------|-------------------|\n| Experiment Tracking | 99.9% (8.7 hours downtime/year) | < 5 minutes | 99.999999999% (11 9's) | Eventually consistent with conflict resolution |\n| Model Registry | 99.95% (4.4 hours downtime/year) | < 2 minutes | 99.999999999% (11 9's) | Strong consistency for model versions |\n| Training Pipeline | 99.5% (43.8 hours downtime/year) | < 10 minutes | 99.9999999% (9 9's) | Eventual consistency with checkpointing |\n| Model Deployment | 99.99% (52.6 minutes downtime/year) | < 1 minute | N/A (stateless) | Strong consistency for routing rules |\n| Model Monitoring | 99.9% (8.7 hours downtime/year) | < 5 minutes | 99.99999% (7 9's) | Eventually consistent with time-series ordering |\n\nDifferent components have varying reliability requirements based on their impact on business operations. Model deployment requires the highest availability since it serves production traffic. Training pipelines can tolerate more downtime since they represent internal workflows, but they need strong checkpoint recovery to avoid losing hours of computation.\n\n> **Critical Consideration**: Reliability in MLOps platforms must account for long-running operations. A traditional web application can retry failed requests, but a training pipeline failure after 6 hours of computation represents significant lost work and compute costs. The platform must provide checkpoint recovery and partial failure handling to maintain productivity despite infrastructure instability.\n\n**Security and Compliance Requirements**\n\nSecurity requirements must address the sensitive nature of ML assets - proprietary models, confidential training data, and competitive intelligence embedded in experiment results. The platform handles intellectual property that could provide significant business advantage to competitors.\n\n| Security Domain | Requirement | Implementation Approach | Compliance Impact |\n|-----------------|-------------|------------------------|-------------------|\n| Authentication | Multi-factor authentication for all users | Integration with enterprise identity providers (OIDC/SAML) | Supports SOX, PCI-DSS access controls |\n| Authorization | Role-based access control with fine-grained permissions | Resource-level permissions with inheritance and delegation | Enables GDPR data controller compliance |\n| Data Encryption | Encryption at rest and in transit for all artifacts | AES-256 for storage, TLS 1.3 for transport | Meets HIPAA encryption requirements |\n| Audit Logging | Complete audit trail for all platform operations | Immutable audit logs with digital signatures | Supports regulatory compliance reporting |\n| Model Protection | Signed models with integrity validation | Digital signatures and content hashing | Prevents model tampering and IP theft |\n| Network Security | Network segmentation and firewall controls | VPC isolation with controlled ingress/egress | Reduces attack surface and data exfiltration risk |\n\nSecurity must be built into the platform architecture rather than added as an afterthought. ML assets are particularly vulnerable because they're often accessed by automated systems, stored in object storage, and transmitted between distributed components. The platform must maintain security without impeding ML workflows.\n\n### What We Won't Build\n\nExplicit non-goals are critical for maintaining platform focus and avoiding feature creep. By clearly stating what we won't build, we establish boundaries that prevent the platform from expanding into adjacent problem domains where we lack expertise or resources.\n\n**Data Platform Capabilities**\n\nWe will not build a comprehensive data platform or attempt to replace existing data infrastructure. The MLOps platform assumes that data engineering teams have already solved data ingestion, transformation, and quality problems using specialized tools.\n\n| Excluded Capability | Rationale | Alternative Approach | Integration Points |\n|---------------------|-----------|---------------------|-------------------|\n| Data Lake Management | Specialized tools like Apache Iceberg, Delta Lake handle this better | Integrate with existing data lakes via standard APIs | Read data via S3, HDFS, or database connectors |\n| ETL Pipeline Orchestration | Airflow, Prefect, and similar tools are purpose-built for data workflows | Support triggering ML pipelines from data pipeline completion | Event-driven integration via webhooks or message queues |\n| Data Quality Monitoring | Tools like Great Expectations, Monte Carlo specialize in data observability | Consume data quality metrics from external systems | Import data quality scores as pipeline validation inputs |\n| Stream Processing | Apache Kafka, Apache Flink handle real-time data processing more effectively | Connect to streaming platforms for real-time inference | Subscribe to Kafka topics for live prediction requests |\n| Data Catalog Management | Apache Atlas, DataHub provide comprehensive metadata management | Integrate with existing catalogs for dataset discovery | Query catalogs to validate training data lineage |\n\nAttempting to build data platform capabilities would create a massive, unfocused system that competes poorly with specialized tools. Instead, we'll design clean integration points that allow the MLOps platform to consume data from best-of-breed data infrastructure.\n\n**MLOps-Adjacent Development Tools**\n\nWe will not build general-purpose development tools or attempt to replace the existing ML development ecosystem. Data scientists and ML engineers already have strong preferences for IDEs, notebooks, and experimentation environments.\n\n| Excluded Capability | Rationale | Alternative Approach | Integration Points |\n|---------------------|-----------|---------------------|-------------------|\n| Notebook Environment | JupyterLab, Google Colab, and Databricks provide superior notebook experiences | Support any notebook environment via SDK integration | Provide Python/R libraries for experiment tracking from notebooks |\n| IDE Integration | VS Code, PyCharm, and specialized ML IDEs serve developer needs better | Build plugins for popular IDEs | Offer language server protocol support for autocomplete |\n| Code Version Control | Git and platforms like GitHub/GitLab are the universal standard | Integrate with existing version control systems | Link experiments to Git commits automatically |\n| Collaborative Development | GitHub, GitLab provide comprehensive collaboration features | Leverage existing development platforms | Import repository metadata and link to model lineage |\n| Code Quality Tools | Linting, testing, and security scanning have mature specialized solutions | Integrate quality gates into training pipelines | Run existing tools as pipeline steps with result validation |\n\nBuilding development tools would distract from MLOps-specific problems while creating inferior alternatives to mature, widely-adopted solutions. The platform should integrate seamlessly with existing development workflows rather than replacing them.\n\n**Infrastructure and Platform Services**\n\nWe will not build low-level infrastructure services or attempt to compete with cloud platforms and container orchestration systems. These are complex, specialized domains with mature solutions.\n\n| Excluded Capability | Rationale | Alternative Approach | Integration Points |\n|---------------------|-----------|---------------------|-------------------|\n| Container Orchestration | Kubernetes is the standard, with cloud-managed alternatives | Deploy platform components on Kubernetes | Use Kubernetes APIs for pipeline resource management |\n| Object Storage | S3, GCS, and Azure Blob provide scalable, reliable storage | Integrate with cloud object storage services | Abstract storage behind pluggable interface |\n| Compute Provisioning | Cloud auto-scaling groups and spot instances optimize cost and availability | Leverage cloud-native compute management | Request compute resources via cloud APIs |\n| Network Load Balancing | Cloud load balancers provide global distribution and health checking | Use existing load balancing infrastructure | Configure health checks and traffic routing rules |\n| Monitoring Infrastructure | Prometheus, DataDog, and cloud monitoring provide comprehensive observability | Export metrics to existing monitoring systems | Publish platform metrics in standard formats |\n| Secrets Management | HashiCorp Vault, cloud KMS, and similar tools specialize in secret handling | Integrate with existing secret management systems | Retrieve API keys and certificates from secret stores |\n\nInfrastructure services require deep expertise in distributed systems, security, and operational procedures. Building these services would create significant maintenance overhead while duplicating capabilities that cloud providers deliver more reliably and cost-effectively.\n\n**Advanced ML Capabilities**\n\nWe will not build advanced ML research capabilities or compete with specialized ML frameworks and libraries. Our focus is on operationalizing models built with existing ML tools.\n\n| Excluded Capability | Rationale | Alternative Approach | Integration Points |\n|---------------------|-----------|---------------------|-------------------|\n| AutoML Algorithms | Tools like H2O.ai, AutoML tables provide sophisticated automated modeling | Support AutoML tools through standard APIs | Track AutoML experiments and deploy generated models |\n| Neural Architecture Search | Research-focused tools and cloud services handle architecture optimization | Integrate NAS results through experiment tracking | Log architecture search results as model metadata |\n| Hyperparameter Optimization | Optuna, Ray Tune, and Hyperopt provide advanced optimization algorithms | Integrate HPO tools with experiment tracking | Log optimization trials and visualize search spaces |\n| Model Interpretation | LIME, SHAP, and specialized explainability tools provide superior analysis | Store interpretation artifacts in experiment tracking | Version explanation models alongside prediction models |\n| Feature Engineering | Tools like Feast, Tecton specialize in feature store management | Integrate with existing feature stores | Track feature versions used in model training |\n| Federated Learning | Specialized frameworks handle the complex coordination required | Support federated learning outputs through standard interfaces | Track federated models and distributed experiment metadata |\n\nAdvanced ML capabilities require deep research expertise and constantly evolving algorithms. The platform should provide a stable foundation that supports innovation in these areas rather than attempting to implement cutting-edge algorithms internally.\n\n> **Boundary Principle**: The MLOps platform succeeds by excelling at the operational aspects of machine learning - tracking, versioning, orchestration, deployment, and monitoring. By integrating with best-of-breed tools in adjacent domains rather than replacing them, we create a focused, maintainable system that adds clear value to existing ML workflows.\n\nThese non-goals ensure the platform remains focused on its core mission while providing clear integration strategies for adjacent capabilities. The boundaries may evolve over time based on user needs and market conditions, but they provide essential guidance for current development priorities.\n\n### Implementation Guidance\n\nThe goals and non-goals established in this section translate into specific technology choices and architectural constraints that guide implementation across all platform components. This guidance helps developers make consistent decisions that align with our quality attributes and functional requirements.\n\n**Technology Recommendations Table**\n\n| Component | Simple Option | Advanced Option | Rationale |\n|-----------|---------------|------------------|-----------|\n| Metadata Storage | PostgreSQL with JSONB columns | Apache Cassandra with distributed architecture | PostgreSQL sufficient for most organizations; Cassandra needed for extreme scale |\n| Artifact Storage | MinIO (S3-compatible) on local storage | Cloud object storage (S3, GCS, Azure Blob) | Local MinIO for development; cloud storage for production reliability |\n| Message Queue | Redis Pub/Sub with persistence | Apache Kafka with topic partitioning | Redis simpler for basic event coordination; Kafka for high-throughput event streaming |\n| Metrics Storage | Prometheus with local storage | InfluxDB with clustering and retention policies | Prometheus integrated with Kubernetes; InfluxDB better for time-series analytics |\n| Container Orchestration | Docker Compose for development | Kubernetes with Helm charts | Compose for local testing; Kubernetes required for production scalability |\n| API Framework | FastAPI with automatic OpenAPI generation | FastAPI with custom middleware and async workers | FastAPI provides excellent performance and documentation for both scenarios |\n\n**Recommended Project Structure**\n\nThe platform should be structured as a modular monorepo that can evolve into microservices as scale requirements demand. This structure supports the clear component boundaries established in our goals while maintaining development simplicity.\n\n```\nmlops-platform/\n├── cmd/                              # Entry points for each service\n│   ├── experiment-tracker/\n│   │   └── main.py                   # Experiment tracking service\n│   ├── model-registry/\n│   │   └── main.py                   # Model registry service\n│   ├── pipeline-orchestrator/\n│   │   └── main.py                   # Training pipeline service\n│   ├── model-deployment/\n│   │   └── main.py                   # Deployment management service\n│   └── monitoring/\n│       └── main.py                   # Model monitoring service\n├── internal/                         # Shared internal libraries\n│   ├── metadata/                     # MetadataStore implementations\n│   │   ├── __init__.py\n│   │   ├── postgres_store.py         # PostgreSQL implementation\n│   │   └── cassandra_store.py        # Cassandra implementation\n│   ├── artifacts/                    # ArtifactStore implementations\n│   │   ├── __init__.py\n│   │   ├── s3_store.py              # S3-compatible storage\n│   │   └── local_store.py           # Local filesystem storage\n│   ├── events/                      # Event coordination system\n│   │   ├── __init__.py\n│   │   ├── coordinator.py           # EventCoordinator implementation\n│   │   └── handlers.py              # Event handler utilities\n│   └── health/                      # Health checking framework\n│       ├── __init__.py\n│       └── checker.py               # ComponentHealth implementation\n├── pkg/                             # Public SDK and client libraries\n│   ├── python-sdk/                  # Python client for data scientists\n│   │   ├── mlops_client/\n│   │   │   ├── __init__.py\n│   │   │   ├── experiment.py        # Experiment tracking client\n│   │   │   ├── registry.py          # Model registry client\n│   │   │   └── monitoring.py        # Monitoring client\n│   │   └── setup.py\n│   └── go-sdk/                      # Go client for infrastructure integration\n│       ├── client/\n│       │   ├── experiment.go\n│       │   ├── registry.go\n│       │   └── monitoring.go\n│       └── go.mod\n├── deployments/                     # Deployment configurations\n│   ├── docker-compose.yml           # Local development environment\n│   ├── kubernetes/                  # Kubernetes manifests\n│   │   ├── namespace.yml\n│   │   ├── experiment-tracker.yml\n│   │   ├── model-registry.yml\n│   │   └── ingress.yml\n│   └── terraform/                   # Infrastructure as code\n│       ├── main.tf\n│       └── variables.tf\n├── docs/                           # Documentation and design docs\n│   ├── api/                        # API documentation\n│   └── architecture/               # Architecture decision records\n├── tests/                          # Integration and end-to-end tests\n│   ├── integration/\n│   └── e2e/\n└── scripts/                        # Development and deployment scripts\n    ├── dev-setup.sh\n    └── deploy.sh\n```\n\n**Infrastructure Starter Code**\n\nThe following starter code provides complete implementations of core infrastructure components that support the functional requirements while abstracting away non-essential complexity.\n\n**Metadata Store Abstraction**\n\n```python\n# internal/metadata/__init__.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional, Union\nfrom enum import Enum\nimport uuid\nimport time\n\nclass MetadataStore(ABC):\n    \"\"\"Abstract interface for metadata storage across all platform components.\n    \n    Provides consistent CRUD operations with transaction support and optimistic\n    concurrency control. Implementations must ensure ACID properties for\n    critical operations like model version promotion and experiment finalization.\n    \"\"\"\n    \n    @abstractmethod\n    async def create_table(self, table_name: str, schema: Dict[str, str]) -> None:\n        \"\"\"Create a new table with the specified schema.\n        \n        Args:\n            table_name: Name of the table to create\n            schema: Column definitions as {column_name: column_type}\n            \n        Raises:\n            TableExistsError: If table already exists\n            InvalidSchemaError: If schema contains invalid type definitions\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def insert(self, table_name: str, data: Dict[str, Any]) -> str:\n        \"\"\"Insert a new record and return the generated ID.\n        \n        Args:\n            table_name: Target table name\n            data: Record data as key-value pairs\n            \n        Returns:\n            Generated unique ID for the inserted record\n            \n        Raises:\n            ValidationError: If data doesn't match table schema\n            DuplicateKeyError: If unique constraint is violated\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def update(self, table_name: str, record_id: str, data: Dict[str, Any], \n                    version: Optional[int] = None) -> None:\n        \"\"\"Update an existing record with optimistic concurrency control.\n        \n        Args:\n            table_name: Target table name\n            record_id: ID of record to update\n            data: Updated field values\n            version: Expected version for optimistic locking\n            \n        Raises:\n            RecordNotFoundError: If record doesn't exist\n            VersionConflictError: If version doesn't match current record\n            ValidationError: If updated data violates schema constraints\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def query(self, table_name: str, filters: Optional[Dict[str, Any]] = None,\n                   sort_by: Optional[str] = None, limit: Optional[int] = None,\n                   offset: Optional[int] = None) -> List[Dict[str, Any]]:\n        \"\"\"Query records with filtering, sorting, and pagination.\n        \n        Args:\n            table_name: Table to query\n            filters: WHERE clause conditions as {column: value}\n            sort_by: Column name for sorting (prefix with '-' for descending)\n            limit: Maximum number of records to return\n            offset: Number of records to skip for pagination\n            \n        Returns:\n            List of matching records as dictionaries\n            \n        Raises:\n            InvalidFilterError: If filter contains invalid column names\n            QueryTimeoutError: If query exceeds configured timeout\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def get_by_id(self, table_name: str, record_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve a single record by its ID.\n        \n        Args:\n            table_name: Table containing the record\n            record_id: Unique identifier of the record\n            \n        Returns:\n            Record data as dictionary, or None if not found\n        \"\"\"\n        pass\n\n# internal/metadata/postgres_store.py\nimport asyncpg\nimport json\nfrom typing import Dict, Any, List, Optional\nfrom .metadata_store import MetadataStore\n\nclass PostgreSQLMetadataStore(MetadataStore):\n    \"\"\"PostgreSQL implementation of MetadataStore using JSONB for flexible schemas.\n    \n    This implementation provides ACID guarantees and supports complex queries\n    on JSON metadata. Suitable for most MLOps workloads with moderate scale\n    requirements (< 10TB metadata, < 1000 concurrent connections).\n    \"\"\"\n    \n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n        self._pool = None\n    \n    async def initialize(self):\n        \"\"\"Initialize the connection pool and create system tables.\"\"\"\n        self._pool = await asyncpg.create_pool(self.connection_string)\n        \n        # Create system tables for metadata management\n        async with self._pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS system_tables (\n                    table_name VARCHAR(255) PRIMARY KEY,\n                    schema_definition JSONB NOT NULL,\n                    created_at TIMESTAMP DEFAULT NOW(),\n                    version INTEGER DEFAULT 1\n                )\n            \"\"\")\n    \n    async def create_table(self, table_name: str, schema: Dict[str, str]) -> None:\n        async with self._pool.acquire() as conn:\n            # Store schema definition\n            await conn.execute(\n                \"INSERT INTO system_tables (table_name, schema_definition) VALUES ($1, $2)\",\n                table_name, json.dumps(schema)\n            )\n            \n            # Create actual table with flexible JSONB storage\n            await conn.execute(f\"\"\"\n                CREATE TABLE {table_name} (\n                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n                    data JSONB NOT NULL,\n                    version INTEGER DEFAULT 1,\n                    created_at TIMESTAMP DEFAULT NOW(),\n                    updated_at TIMESTAMP DEFAULT NOW()\n                )\n            \"\"\")\n            \n            # Create indexes for common query patterns\n            await conn.execute(f\"CREATE INDEX idx_{table_name}_data_gin ON {table_name} USING GIN (data)\")\n    \n    async def insert(self, table_name: str, data: Dict[str, Any]) -> str:\n        async with self._pool.acquire() as conn:\n            # TODO 1: Validate data against table schema\n            # TODO 2: Insert record with generated UUID\n            # TODO 3: Return the generated ID as string\n            # Hint: Use RETURNING clause to get generated ID\n            record_id = await conn.fetchval(\n                f\"INSERT INTO {table_name} (data) VALUES ($1) RETURNING id\",\n                json.dumps(data)\n            )\n            return str(record_id)\n    \n    async def update(self, table_name: str, record_id: str, data: Dict[str, Any], \n                    version: Optional[int] = None) -> None:\n        async with self._pool.acquire() as conn:\n            # TODO 1: If version specified, check current version matches\n            # TODO 2: Update record data and increment version\n            # TODO 3: Update the updated_at timestamp\n            # TODO 4: Raise VersionConflictError if optimistic lock fails\n            if version is not None:\n                result = await conn.execute(\n                    f\"\"\"UPDATE {table_name} \n                       SET data = $1, version = version + 1, updated_at = NOW()\n                       WHERE id = $2 AND version = $3\"\"\",\n                    json.dumps(data), record_id, version\n                )\n                if result == \"UPDATE 0\":\n                    raise VersionConflictError(f\"Version conflict updating {record_id}\")\n            else:\n                await conn.execute(\n                    f\"\"\"UPDATE {table_name} \n                       SET data = $1, version = version + 1, updated_at = NOW()\n                       WHERE id = $2\"\"\",\n                    json.dumps(data), record_id\n                )\n    \n    async def query(self, table_name: str, filters: Optional[Dict[str, Any]] = None,\n                   sort_by: Optional[str] = None, limit: Optional[int] = None,\n                   offset: Optional[int] = None) -> List[Dict[str, Any]]:\n        # TODO 1: Build WHERE clause from filters using JSONB operators\n        # TODO 2: Add ORDER BY clause if sort_by specified\n        # TODO 3: Add LIMIT and OFFSET for pagination\n        # TODO 4: Execute query and return list of records\n        # Hint: Use data->>'field' for string comparison, data->'field' for JSON comparison\n        pass\n    \n    async def get_by_id(self, table_name: str, record_id: str) -> Optional[Dict[str, Any]]:\n        async with self._pool.acquire() as conn:\n            row = await conn.fetchrow(f\"SELECT data FROM {table_name} WHERE id = $1\", record_id)\n            return json.loads(row['data']) if row else None\n\nclass VersionConflictError(Exception):\n    \"\"\"Raised when optimistic concurrency control detects a version conflict.\"\"\"\n    pass\n```\n\n**Artifact Storage Abstraction**\n\n```python\n# internal/artifacts/__init__.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional, BinaryIO\nimport hashlib\nimport time\n\nclass ArtifactStore(ABC):\n    \"\"\"Abstract interface for storing and retrieving ML artifacts.\n    \n    Supports versioned storage of binary artifacts like trained models,\n    datasets, plots, and configuration files. Implementations should\n    provide content addressing and deduplication for efficiency.\n    \"\"\"\n    \n    @abstractmethod\n    async def put(self, key: str, data: bytes, metadata: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Store binary data with optional metadata.\n        \n        Args:\n            key: Unique identifier for the artifact\n            data: Binary content to store\n            metadata: Optional key-value metadata\n            \n        Returns:\n            Content hash of the stored data for integrity verification\n            \n        Raises:\n            StorageError: If storage operation fails\n            InvalidKeyError: If key format is invalid\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def get(self, key: str) -> Optional[bytes]:\n        \"\"\"Retrieve binary data by key.\n        \n        Args:\n            key: Unique identifier of the artifact\n            \n        Returns:\n            Binary content, or None if not found\n            \n        Raises:\n            StorageError: If retrieval operation fails\n            CorruptionError: If stored data fails integrity check\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def delete(self, key: str) -> bool:\n        \"\"\"Delete an artifact by key.\n        \n        Args:\n            key: Unique identifier of the artifact to delete\n            \n        Returns:\n            True if artifact was deleted, False if not found\n            \n        Raises:\n            StorageError: If deletion operation fails\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def list_keys(self, prefix: Optional[str] = None, limit: Optional[int] = None) -> List[str]:\n        \"\"\"List artifact keys with optional prefix filtering.\n        \n        Args:\n            prefix: Optional key prefix for filtering\n            limit: Maximum number of keys to return\n            \n        Returns:\n            List of artifact keys matching the criteria\n            \n        Raises:\n            StorageError: If listing operation fails\n        \"\"\"\n        pass\n\n# internal/artifacts/s3_store.py\nimport aioboto3\nimport hashlib\nfrom typing import Dict, Any, List, Optional\nfrom .artifact_store import ArtifactStore\n\nclass S3ArtifactStore(ArtifactStore):\n    \"\"\"S3-compatible artifact storage with content addressing and metadata support.\"\"\"\n    \n    def __init__(self, bucket_name: str, endpoint_url: Optional[str] = None,\n                 aws_access_key_id: Optional[str] = None, aws_secret_access_key: Optional[str] = None):\n        self.bucket_name = bucket_name\n        self.endpoint_url = endpoint_url\n        self.aws_access_key_id = aws_access_key_id\n        self.aws_secret_access_key = aws_secret_access_key\n        self._session = None\n    \n    async def initialize(self):\n        \"\"\"Initialize S3 session and create bucket if it doesn't exist.\"\"\"\n        self._session = aioboto3.Session(\n            aws_access_key_id=self.aws_access_key_id,\n            aws_secret_access_key=self.aws_secret_access_key\n        )\n        \n        async with self._session.client('s3', endpoint_url=self.endpoint_url) as s3:\n            try:\n                await s3.head_bucket(Bucket=self.bucket_name)\n            except:\n                await s3.create_bucket(Bucket=self.bucket_name)\n    \n    async def put(self, key: str, data: bytes, metadata: Optional[Dict[str, Any]] = None) -> str:\n        # TODO 1: Compute SHA-256 hash of data for content addressing\n        # TODO 2: Prepare S3 metadata headers (prefix with 'x-amz-meta-')\n        # TODO 3: Upload data to S3 with metadata\n        # TODO 4: Return content hash for integrity verification\n        content_hash = hashlib.sha256(data).hexdigest()\n        \n        s3_metadata = {}\n        if metadata:\n            s3_metadata = {f\"x-amz-meta-{k}\": str(v) for k, v in metadata.items()}\n        s3_metadata['x-amz-meta-content-hash'] = content_hash\n        \n        async with self._session.client('s3', endpoint_url=self.endpoint_url) as s3:\n            await s3.put_object(\n                Bucket=self.bucket_name,\n                Key=key,\n                Body=data,\n                Metadata=s3_metadata\n            )\n        \n        return content_hash\n    \n    async def get(self, key: str) -> Optional[bytes]:\n        # TODO 1: Retrieve object from S3\n        # TODO 2: Verify content hash if available in metadata\n        # TODO 3: Return binary data or None if not found\n        # TODO 4: Raise CorruptionError if hash verification fails\n        try:\n            async with self._session.client('s3', endpoint_url=self.endpoint_url) as s3:\n                response = await s3.get_object(Bucket=self.bucket_name, Key=key)\n                data = await response['Body'].read()\n                \n                # Verify integrity if hash is available\n                stored_hash = response.get('Metadata', {}).get('content-hash')\n                if stored_hash:\n                    computed_hash = hashlib.sha256(data).hexdigest()\n                    if stored_hash != computed_hash:\n                        raise CorruptionError(f\"Content hash mismatch for {key}\")\n                \n                return data\n        except s3.exceptions.NoSuchKey:\n            return None\n    \n    async def delete(self, key: str) -> bool:\n        # TODO 1: Delete object from S3\n        # TODO 2: Return True if deleted, False if not found\n        # TODO 3: Handle S3 exceptions appropriately\n        pass\n    \n    async def list_keys(self, prefix: Optional[str] = None, limit: Optional[int] = None) -> List[str]:\n        # TODO 1: List objects with optional prefix filter\n        # TODO 2: Implement pagination if limit specified\n        # TODO 3: Return list of object keys\n        pass\n\nclass CorruptionError(Exception):\n    \"\"\"Raised when stored artifact fails integrity verification.\"\"\"\n    pass\n```\n\n**Event Coordination System**\n\n```python\n# internal/events/__init__.py\nfrom typing import Dict, Any, Callable, List\nfrom enum import Enum\nimport uuid\nimport time\nimport asyncio\nimport json\n\nclass Event:\n    \"\"\"Immutable event representing a state change in the MLOps platform.\n    \n    Events enable loose coupling between components by providing an\n    asynchronous notification mechanism for important state transitions.\n    \"\"\"\n    \n    def __init__(self, id: str, type: str, source: str, timestamp: float, payload: Dict[str, Any]):\n        self.id = id\n        self.type = type\n        self.source = source\n        self.timestamp = timestamp\n        self.payload = payload\n    \n    @classmethod\n    def create(cls, event_type: str, source: str, payload: Dict[str, Any]) -> 'Event':\n        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"\n        return cls(\n            id=str(uuid.uuid4()),\n            type=event_type,\n            source=source,\n            timestamp=time.time(),\n            payload=payload\n        )\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Serialize event to dictionary for transmission.\"\"\"\n        return {\n            'id': self.id,\n            'type': self.type,\n            'source': self.source,\n            'timestamp': self.timestamp,\n            'payload': self.payload\n        }\n\nclass EventCoordinator:\n    \"\"\"Asynchronous event distribution system for inter-component communication.\n    \n    Supports both synchronous and asynchronous event publishing with\n    error handling and dead letter queue functionality.\n    \"\"\"\n    \n    def __init__(self):\n        self._handlers: Dict[str, List[Callable[[Event], None]]] = {}\n        self._event_log: List[Event] = []\n    \n    async def publish(self, event: Event, synchronous: bool = False) -> None:\n        \"\"\"Publish event to all registered handlers.\n        \n        Args:\n            event: Event to publish\n            synchronous: If True, wait for all handlers to complete\n            \n        Raises:\n            EventHandlerError: If synchronous=True and any handler fails\n        \"\"\"\n        # TODO 1: Log event to internal event store\n        # TODO 2: Find all handlers registered for this event type\n        # TODO 3: If synchronous, await all handlers; otherwise fire-and-forget\n        # TODO 4: Handle handler exceptions appropriately\n        self._event_log.append(event)\n        \n        handlers = self._handlers.get(event.type, [])\n        if synchronous:\n            # Wait for all handlers to complete\n            tasks = [asyncio.create_task(handler(event)) for handler in handlers]\n            await asyncio.gather(*tasks)\n        else:\n            # Fire and forget - don't wait for handler completion\n            for handler in handlers:\n                asyncio.create_task(handler(event))\n    \n    def subscribe(self, event_type: str, handler: Callable[[Event], None]) -> None:\n        \"\"\"Register event handler for specific event type.\n        \n        Args:\n            event_type: Type of events to handle\n            handler: Async function to call when events of this type occur\n        \"\"\"\n        # TODO 1: Add handler to handlers dictionary for this event type\n        # TODO 2: Initialize empty list if this is the first handler for this type\n        if event_type not in self._handlers:\n            self._handlers[event_type] = []\n        self._handlers[event_type].append(handler)\n    \n    def get_recent_events(self, event_type: Optional[str] = None, limit: int = 100) -> List[Event]:\n        \"\"\"Retrieve recent events for debugging and monitoring.\"\"\"\n        events = self._event_log\n        if event_type:\n            events = [e for e in events if e.type == event_type]\n        return events[-limit:]\n\n# Event type constants for consistent naming\nEXPERIMENT_COMPLETED = \"experiment.completed\"\nMODEL_PROMOTED = \"model.promoted\"\nDEPLOYMENT_FAILED = \"deployment.failed\"\nDRIFT_DETECTED = \"monitoring.drift_detected\"\nPIPELINE_STARTED = \"pipeline.started\"\nPIPELINE_COMPLETED = \"pipeline.completed\"\nMODEL_DEPLOYED = \"model.deployed\"\n```\n\n**Health Checking Framework**\n\n```python\n# internal/health/__init__.py\nfrom typing import Dict, Any, Callable, List, Optional\nfrom enum import Enum\nimport asyncio\nimport time\n\nclass HealthStatus(Enum):\n    \"\"\"Health status enumeration for component health checks.\"\"\"\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"  \n    UNHEALTHY = \"unhealthy\"\n    UNKNOWN = \"unknown\"\n\nclass HealthCheck:\n    \"\"\"Individual health check result with status and diagnostic information.\"\"\"\n    \n    def __init__(self, name: str, status: HealthStatus, message: str, \n                timestamp: float, details: Dict[str, Any]):\n        self.name = name\n        self.status = status\n        self.message = message\n        self.timestamp = timestamp\n        self.details = details\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Serialize health check to dictionary.\"\"\"\n        return {\n            'name': self.name,\n            'status': self.status.value,\n            'message': self.message,\n            'timestamp': self.timestamp,\n            'details': self.details\n        }\n\nclass ComponentHealth:\n    \"\"\"Health checking framework for monitoring component status.\n    \n    Supports both active health checks (periodic execution) and\n    passive health indicators (updated by component logic).\n    \"\"\"\n    \n    def __init__(self, component_name: str):\n        self.component_name = component_name\n        self._checks: Dict[str, Callable[[], Dict[str, Any]]] = {}\n        self._last_results: Dict[str, HealthCheck] = {}\n    \n    def add_check(self, check_name: str, check_func: Callable[[], Dict[str, Any]]) -> None:\n        \"\"\"Register periodic health check function.\n        \n        Args:\n            check_name: Unique name for this health check\n            check_func: Async function returning status dict with 'status', 'message', 'details'\n        \"\"\"\n        # TODO 1: Store check function in internal registry\n        # TODO 2: Validate check_name is unique for this component\n        self._checks[check_name] = check_func\n    \n    async def run_checks(self) -> List[HealthCheck]:\n        \"\"\"Execute all registered health checks and return results.\n        \n        Returns:\n            List of HealthCheck objects with current status\n        \"\"\"\n        # TODO 1: Execute all registered check functions concurrently\n        # TODO 2: Convert results to HealthCheck objects with timestamps\n        # TODO 3: Store results in _last_results for caching\n        # TODO 4: Handle check function exceptions gracefully\n        results = []\n        check_tasks = {}\n        \n        for check_name, check_func in self._checks.items():\n            check_tasks[check_name] = asyncio.create_task(check_func())\n        \n        for check_name, task in check_tasks.items():\n            try:\n                result = await task\n                health_check = HealthCheck(\n                    name=check_name,\n                    status=HealthStatus(result.get('status', 'unknown')),\n                    message=result.get('message', ''),\n                    timestamp=time.time(),\n                    details=result.get('details', {})\n                )\n                results.append(health_check)\n                self._last_results[check_name] = health_check\n            except Exception as e:\n                error_check = HealthCheck(\n                    name=check_name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=f\"Health check failed: {str(e)}\",\n                    timestamp=time.time(),\n                    details={'error': str(e)}\n                )\n                results.append(error_check)\n                self._last_results[check_name] = error_check\n        \n        return results\n    \n    def get_overall_status(self) -> HealthStatus:\n        \"\"\"Compute overall component health from individual checks.\"\"\"\n        if not self._last_results:\n            return HealthStatus.UNKNOWN\n        \n        statuses = [check.status for check in self._last_results.values()]\n        \n        if any(s == HealthStatus.UNHEALTHY for s in statuses):\n            return HealthStatus.UNHEALTHY\n        elif any(s == HealthStatus.DEGRADED for s in statuses):\n            return HealthStatus.DEGRADED\n        elif all(s == HealthStatus.HEALTHY for s in statuses):\n            return HealthStatus.HEALTHY\n        else:\n            return HealthStatus.UNKNOWN\n```\n\n**Milestone Checkpoint**\n\nAfter implementing the infrastructure components provided above, verify the following behavior:\n\n1. **Metadata Store Verification**: Create a test table, insert records, query with filters, and verify optimistic locking behavior\n   ```python\n   # Test command\n   python -m pytest tests/infrastructure/test_metadata_store.py -v\n   \n   # Expected output\n   test_create_table_success ✓\n   test_insert_and_query ✓\n   test_optimistic_locking ✓\n   test_version_conflict_detection ✓\n   ```\n\n2. **Artifact Store Verification**: Upload binary data, retrieve it, verify content integrity, and test listing operations\n   ```python\n   # Test command  \n   python -m pytest tests/infrastructure/test_artifact_store.py -v\n   \n   # Expected output\n   test_put_and_get_artifact ✓\n   test_content_integrity_verification ✓\n   test_list_with_prefix_filter ✓\n   test_delete_artifact ✓\n   ```\n\n3. **Event Coordination Verification**: Publish events, verify handler execution, and test both synchronous and asynchronous modes\n   ```python\n   # Test command\n   python -m pytest tests/infrastructure/test_event_coordinator.py -v\n   \n   # Expected output\n   test_synchronous_event_publishing ✓\n   test_asynchronous_event_publishing ✓\n   test_multiple_handlers_per_event_type ✓\n   test_event_history_retention ✓\n   ```\n\n**Language-Specific Development Tips**\n\n- **Database Connections**: Use connection pooling (asyncpg.create_pool) to handle concurrent requests efficiently without exhausting database connections\n- **S3 Integration**: The aioboto3 library provides async S3 operations; always use context managers to ensure proper resource cleanup\n- **Error Handling**: Create custom exception classes for domain-specific errors (VersionConflictError, CorruptionError) rather than using generic exceptions\n- **JSON Serialization**: PostgreSQL JSONB provides better query performance than JSON for metadata storage; use appropriate operators (->>, ->) for different query types\n- **Async Patterns**: Use asyncio.gather() for concurrent operations, asyncio.create_task() for fire-and-forget operations, and proper exception handling in async contexts\n- **Type Hints**: Use typing.Optional, typing.Dict, and typing.List for better IDE support and runtime validation\n- **Configuration Management**: Use environment variables or configuration files for connection strings and API keys; never hard-code credentials\n\nThese infrastructure components provide the foundation for implementing the specific MLOps components defined in our functional requirements. Each component builds on these abstractions to provide experiment tracking, model registry, pipeline orchestration, model deployment, and monitoring capabilities.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** This section establishes the foundational architecture that enables all milestones (1-5) by defining how experiment tracking, model registry, pipeline orchestration, model deployment, and monitoring components work together as a cohesive system.\n\nThe MLOps platform architecture follows a **microservices approach** where each major component operates as an independent service with well-defined responsibilities and interfaces. Think of this architecture like a **modern hospital system** where different departments (emergency, surgery, radiology, pharmacy) each have specialized functions but coordinate seamlessly through shared patient records, standardized protocols, and real-time communication systems. Just as a patient's journey through the hospital involves multiple departments working together while maintaining their own expertise and tools, an ML model's lifecycle involves multiple specialized components that must coordinate while maintaining their distinct responsibilities.\n\n![MLOps Platform System Architecture](./diagrams/system-architecture.svg)\n\nThe architecture centers around five core components that communicate through standardized APIs and event-driven coordination. Each component manages its own data stores optimized for its specific access patterns, while shared infrastructure services provide cross-cutting concerns like authentication, monitoring, and configuration management. The design emphasizes **loose coupling** between components to enable independent scaling, deployment, and evolution while ensuring strong data consistency and lineage tracking across the entire ML lifecycle.\n\n### Component Responsibilities\n\nEach component in the MLOps platform has clearly defined responsibilities and maintains specific types of state. The boundaries between components follow the principle of **domain-driven design**, where each component encapsulates a distinct area of MLOps functionality with minimal overlap. This separation enables teams to work independently on different aspects of the platform while ensuring that integration points remain stable and well-defined.\n\n| Component | Primary Responsibility | Data Ownership | Key Interfaces |\n|-----------|----------------------|----------------|----------------|\n| Experiment Tracking | Log and organize ML training runs with parameters, metrics, and artifacts | Experiment metadata, run parameters, time-series metrics, artifact references | Parameter logging, metric logging, artifact storage, run comparison |\n| Model Registry | Version and manage trained models through their lifecycle stages | Model metadata, version history, stage transitions, approval records | Model registration, version promotion, lineage tracking, model download |\n| Pipeline Orchestration | Execute multi-step training workflows with resource management | Pipeline definitions, execution history, step dependencies, resource allocations | Pipeline submission, execution monitoring, resource scheduling |\n| Model Deployment | Serve models as scalable HTTP endpoints with traffic management | Deployment configurations, endpoint metadata, traffic routing rules | Model serving, canary deployments, auto-scaling, rollback |\n| Model Monitoring | Track model performance and detect drift in production | Prediction logs, performance metrics, drift statistics, alert history | Prediction logging, drift detection, alerting, dashboard data |\n\nThe **Experiment Tracking** component serves as the foundational layer where all ML training activity begins. It maintains a hierarchical organization of experiments containing multiple runs, where each run captures a complete snapshot of a training execution including hyperparameters, code version, data version, and all generated artifacts. The component provides both real-time logging APIs for active training jobs and batch analysis capabilities for comparing runs and identifying optimal configurations.\n\nThe **Model Registry** acts as the authoritative source for all trained models in the organization. It implements a **git-like versioning system** where each model version is immutable and linked to its source experiment run. The registry manages model lifecycle stages (development, staging, production, archived) with approval workflows and automated promotion rules. Every model version maintains complete lineage information tracing back to the training data, code commit, and experiment parameters that produced it.\n\n**Pipeline Orchestration** coordinates complex multi-step training workflows using directed acyclic graphs (DAGs) to express step dependencies and data flow. The component handles resource allocation across heterogeneous compute infrastructure, supports both sequential and parallel execution patterns, and provides fault tolerance through checkpoint-restart mechanisms. It integrates with container orchestration platforms to provide isolated execution environments for each pipeline step.\n\n**Model Deployment** transforms registered model versions into production-ready HTTP endpoints with enterprise-grade capabilities. The component supports multiple deployment strategies including blue-green deployments for zero-downtime updates and canary releases for gradual traffic migration. It integrates with popular inference servers like TensorFlow Serving, TorchServe, and NVIDIA Triton while providing unified APIs for model loading, auto-scaling, and health monitoring.\n\n**Model Monitoring** provides comprehensive observability for deployed models through continuous tracking of prediction quality, performance metrics, and data characteristics. The component implements statistical drift detection algorithms to identify when model assumptions no longer hold and provides automated alerting when model performance degrades below acceptable thresholds. It maintains detailed audit trails of all model predictions to support compliance requirements and debugging efforts.\n\n> **Key Design Principle:** Each component maintains **single responsibility** while providing rich APIs for cross-component integration. This enables teams to adopt components incrementally rather than requiring big-bang platform adoption.\n\nThe components communicate through three primary integration patterns: **synchronous API calls** for immediate data retrieval, **asynchronous event publishing** for lifecycle notifications, and **shared data access** for read-heavy operations like model lineage queries. This hybrid approach balances consistency requirements with performance optimization, allowing each interaction to use the most appropriate communication pattern.\n\n### Technology Stack\n\nThe technology stack balances **proven reliability** with **modern cloud-native patterns**, selecting mature technologies that can scale to enterprise requirements while remaining approachable for development teams. The stack emphasizes **polyglot persistence**, where each data store is optimized for its specific access patterns rather than forcing all data into a single database technology.\n\n| Layer | Component | Simple Option | Advanced Option | Rationale |\n|-------|-----------|---------------|-----------------|-----------|\n| **Storage** | Metadata | PostgreSQL with JSONB | PostgreSQL + Redis + Elasticsearch | JSONB handles flexible ML metadata while maintaining ACID guarantees |\n| **Storage** | Artifacts | MinIO (S3-compatible) | Multi-tier storage (S3 + Glacier) | Object storage scales to petabytes with configurable retention policies |\n| **Storage** | Time Series | PostgreSQL TimescaleDB | InfluxDB + Grafana | TimescaleDB provides SQL familiarity with time-series optimization |\n| **Orchestration** | Container Platform | Docker Compose | Kubernetes with Kubeflow | Kubernetes provides production-grade scheduling and resource management |\n| **Orchestration** | Workflow Engine | Airflow | Kubeflow Pipelines | Airflow's mature ecosystem handles complex workflow dependencies |\n| **Messaging** | Event Coordination | Redis Pub/Sub | Apache Kafka | Redis provides low-latency coordination; Kafka handles high-volume streams |\n| **Serving** | Model Inference | Flask + Gunicorn | Kubernetes + Istio Service Mesh | Service mesh provides traffic management and observability at scale |\n| **Monitoring** | Application Metrics | Prometheus + Grafana | Prometheus + Grafana + Jaeger | Proven observability stack with distributed tracing for debugging |\n\n**Metadata Storage** uses PostgreSQL as the primary system of record for all structured metadata across components. PostgreSQL's JSONB support enables flexible schema evolution for ML metadata while maintaining ACID guarantees for critical operations like model promotion and deployment rollbacks. The database schema normalizes core entities (experiments, models, deployments) while storing variable metadata (parameters, tags, configuration) in JSONB columns that can be efficiently queried and indexed.\n\n**Artifact Storage** leverages object storage for all binary artifacts including trained models, datasets, plots, and logs. The system uses content-addressable storage where artifacts are identified by cryptographic hashes, enabling automatic deduplication and integrity verification. A tiered storage strategy automatically moves infrequently accessed artifacts to cheaper storage classes while maintaining fast access to recent artifacts.\n\n**Time Series Storage** handles high-volume metric data from training runs, model serving, and monitoring systems. TimescaleDB extends PostgreSQL with time-series optimizations including automatic partitioning, compression, and retention policies. This choice maintains SQL compatibility while providing the performance characteristics needed for real-time dashboards and alerting systems.\n\n> **Architecture Decision: PostgreSQL as Primary Database**\n> - **Context**: MLOps platforms need to store diverse metadata with complex relationships and varying schemas\n> - **Options Considered**: \n>   1. NoSQL document database (MongoDB) for schema flexibility\n>   2. Graph database (Neo4j) for lineage relationships  \n>   3. PostgreSQL with JSONB for hybrid approach\n> - **Decision**: PostgreSQL with JSONB columns for variable metadata\n> - **Rationale**: ACID guarantees critical for model promotion workflows, mature ecosystem, JSONB provides schema flexibility where needed, SQL familiarity reduces operational complexity\n> - **Consequences**: Enables complex cross-component queries, requires careful index management for JSONB fields, may need sharding for extreme scale\n\n**Container Orchestration** provides the runtime environment for all platform components and user workloads. Docker containers ensure consistent execution environments across development and production while Kubernetes provides production-grade features including resource allocation, health monitoring, and rolling updates. The platform uses Kubernetes Custom Resource Definitions (CRDs) to extend the API with ML-specific resources like training jobs and model deployments.\n\n**Workflow Orchestration** handles complex multi-step ML workflows using Apache Airflow's mature DAG execution engine. Airflow's extensive operator ecosystem provides pre-built integrations with popular ML frameworks, cloud services, and data processing tools. The platform extends Airflow with custom operators for MLOps-specific tasks like model registration and deployment triggers.\n\n**Event Coordination** enables loose coupling between components through publish-subscribe messaging patterns. Redis Pub/Sub provides low-latency coordination for interactive workflows while Apache Kafka handles high-volume event streams from production model serving. The event system implements at-least-once delivery semantics with idempotent handlers to ensure reliable processing.\n\n**Model Serving Infrastructure** supports multiple inference frameworks through a unified abstraction layer. The platform integrates with specialized serving systems like TensorFlow Serving for TensorFlow models, TorchServe for PyTorch models, and NVIDIA Triton for multi-framework serving. Kubernetes provides the underlying container orchestration while Istio service mesh handles advanced traffic management features like canary deployments and circuit breaking.\n\n### Codebase Organization\n\nThe codebase follows a **monorepo structure** with clear module boundaries that align with component responsibilities. This organization balances the benefits of shared tooling and dependency management with the need for component independence and clear ownership boundaries. The structure supports both development-time productivity and production deployment flexibility.\n\n```\nmlops-platform/\n├── cmd/                          # Application entry points\n│   ├── experiment-tracker/       # Experiment tracking service\n│   ├── model-registry/           # Model registry service  \n│   ├── pipeline-orchestrator/    # Pipeline orchestration service\n│   ├── model-deployment/         # Model deployment service\n│   └── model-monitoring/         # Model monitoring service\n├── internal/                     # Private application code\n│   ├── common/                   # Shared utilities and types\n│   │   ├── auth/                 # Authentication and authorization\n│   │   ├── config/               # Configuration management\n│   │   ├── database/             # Database connection utilities\n│   │   ├── events/               # Event coordination primitives\n│   │   ├── health/               # Health check framework\n│   │   ├── logging/              # Structured logging setup\n│   │   └── storage/              # Storage abstraction interfaces\n│   ├── experiment/               # Experiment tracking domain\n│   │   ├── api/                  # REST API handlers\n│   │   ├── models/               # Domain entities and business logic\n│   │   ├── repository/           # Data access layer\n│   │   └── service/              # Business logic and orchestration\n│   ├── registry/                 # Model registry domain\n│   │   ├── api/\n│   │   ├── models/\n│   │   ├── repository/\n│   │   └── service/\n│   ├── pipeline/                 # Pipeline orchestration domain\n│   │   ├── api/\n│   │   ├── executor/             # Pipeline execution engine\n│   │   ├── models/\n│   │   ├── repository/\n│   │   └── scheduler/            # Resource scheduling logic\n│   ├── deployment/               # Model deployment domain\n│   │   ├── api/\n│   │   ├── models/\n│   │   ├── repository/\n│   │   ├── serving/              # Model serving integrations\n│   │   └── traffic/              # Traffic management logic\n│   └── monitoring/               # Model monitoring domain\n│       ├── api/\n│       ├── collectors/           # Prediction data collection\n│       ├── detectors/            # Drift detection algorithms\n│       ├── models/\n│       └── repository/\n├── pkg/                          # Public library code\n│   ├── client/                   # SDK for external integrations\n│   │   ├── experiment/           # Experiment tracking client\n│   │   ├── registry/             # Model registry client\n│   │   └── monitoring/           # Monitoring client\n│   └── types/                    # Shared type definitions\n├── api/                          # API specifications\n│   ├── openapi/                  # OpenAPI/Swagger specifications\n│   └── proto/                    # Protocol buffer definitions\n├── deployments/                  # Deployment configurations\n│   ├── docker/                   # Docker configurations\n│   ├── kubernetes/               # Kubernetes manifests\n│   └── helm/                     # Helm charts\n├── docs/                         # Documentation\n│   ├── api/                      # API documentation\n│   ├── architecture/             # Architecture decision records\n│   └── user-guides/              # User documentation\n├── scripts/                      # Development and deployment scripts\n│   ├── dev/                      # Development environment setup\n│   ├── migration/                # Database migration scripts\n│   └── testing/                  # Testing utilities\n└── test/                         # Test suites\n    ├── integration/              # Cross-component integration tests\n    ├── load/                     # Performance and load tests\n    └── e2e/                      # End-to-end scenario tests\n```\n\nThe **hexagonal architecture pattern** within each component separates business logic from external concerns through well-defined interfaces. The `api/` package handles HTTP request/response concerns, `service/` contains pure business logic, `repository/` abstracts data access, and `models/` defines domain entities with their invariants and behaviors. This separation enables comprehensive unit testing and makes it easy to swap out infrastructure dependencies.\n\n**Shared utilities** in the `internal/common/` package provide consistent implementations of cross-cutting concerns while avoiding tight coupling between components. The `events/` package implements the event coordination system used for inter-component communication, while `storage/` provides abstract interfaces that components use to interact with databases and object storage without depending on specific implementations.\n\nThe **client SDK** in the `pkg/client/` package enables external applications to integrate with the MLOps platform through idiomatic APIs. Each component provides a client library that handles authentication, request serialization, error handling, and retry logic. The SDK supports both synchronous and asynchronous usage patterns depending on the operation characteristics.\n\n**Configuration Management** uses a hierarchical approach where default values are defined in code, overridden by configuration files, and finally by environment variables. This enables the same codebase to run across development, staging, and production environments with environment-specific configuration. Sensitive values like database credentials are injected through secure mechanisms like Kubernetes secrets.\n\n> **Development Workflow:** Each component can be developed and tested independently using interfaces and mocks for dependencies, but integration testing validates cross-component behavior using docker-compose environments that mirror production topology.\n\n**Database Schema Management** uses versioned migrations to evolve the database schema safely across environments. Migration scripts are component-specific but coordinate through a shared migration tracking system to ensure consistent ordering. The system supports both forward migrations for schema evolution and rollback capabilities for deployment recovery scenarios.\n\n**Testing Strategy** employs multiple testing levels with clear boundaries and responsibilities. Unit tests focus on individual component logic using mocks for external dependencies. Integration tests validate component interactions using test databases and message queues. End-to-end tests exercise complete user workflows across the entire platform using realistic data and scenarios.\n\n### Implementation Guidance\n\nThe implementation follows a **service-oriented architecture** where each component runs as an independent service with clearly defined APIs and data boundaries. This approach enables incremental development where teams can build and deploy components independently while ensuring they integrate correctly through standardized interfaces.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option | Development Complexity |\n|-----------|---------------|-----------------|----------------------|\n| HTTP Framework | Flask + Flask-RESTful | FastAPI + Pydantic | Flask for rapid prototyping, FastAPI for production |\n| Database ORM | SQLAlchemy Core | SQLAlchemy ORM + Alembic | Core for complex queries, ORM for rapid development |\n| Task Queue | Celery + Redis | Celery + Redis + Flower | Celery provides robust async task execution |\n| Configuration | Python-dotenv + dataclasses | Pydantic Settings + YAML | Pydantic provides validation and type safety |\n| Testing | pytest + pytest-asyncio | pytest + testcontainers + factory_boy | Testcontainers for realistic integration tests |\n| API Documentation | Flask-RESTX | FastAPI auto-docs + ReDoc | FastAPI generates interactive documentation |\n\n**Recommended Project Structure:**\n\nStart with this directory layout and expand as components grow in complexity:\n\n```\nmlops-platform/\n├── requirements/\n│   ├── base.txt              # Core dependencies\n│   ├── dev.txt              # Development tools\n│   └── test.txt             # Testing dependencies\n├── src/\n│   ├── mlops_platform/\n│   │   ├── __init__.py\n│   │   ├── config.py        # Configuration management\n│   │   ├── database.py      # Database connection setup\n│   │   ├── events.py        # Event coordination system\n│   │   ├── health.py        # Health check framework\n│   │   └── storage.py       # Storage abstraction layer\n│   ├── experiment_tracking/\n│   │   ├── __init__.py\n│   │   ├── app.py          # Flask application factory\n│   │   ├── api.py          # REST API endpoints\n│   │   ├── models.py       # Database models\n│   │   ├── repository.py   # Data access layer\n│   │   └── service.py      # Business logic\n│   └── model_registry/     # Similar structure for each component\n├── tests/\n│   ├── conftest.py         # Pytest configuration and fixtures\n│   ├── unit/              # Component unit tests\n│   ├── integration/       # Cross-component tests\n│   └── e2e/              # End-to-end scenarios\n├── docker-compose.yml     # Development environment\n├── Dockerfile            # Container image definition\n└── pyproject.toml       # Python project configuration\n```\n\n**Core Infrastructure Starter Code:**\n\nThe foundation provides essential infrastructure that all components use:\n\n```python\n# src/mlops_platform/events.py\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any, Callable, List\nfrom enum import Enum\nimport uuid\nimport time\nimport threading\nimport logging\n\n@dataclass\nclass Event:\n    \"\"\"Base event class for inter-component communication.\"\"\"\n    id: str\n    type: str  \n    source: str\n    timestamp: float\n    payload: Dict[str, Any]\n    \n    @classmethod\n    def create(cls, event_type: str, source: str, payload: Dict[str, Any]) -> 'Event':\n        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"\n        return cls(\n            id=str(uuid.uuid4()),\n            type=event_type,\n            source=source, \n            timestamp=time.time(),\n            payload=payload\n        )\n\nclass EventCoordinator:\n    \"\"\"Coordinates event publishing and subscription between components.\"\"\"\n    \n    def __init__(self):\n        self._subscribers: Dict[str, List[Callable[[Event], None]]] = {}\n        self._lock = threading.RLock()\n        self._logger = logging.getLogger(__name__)\n    \n    def subscribe(self, event_type: str, handler: Callable[[Event], None]) -> None:\n        \"\"\"Register event handler for specific event type.\"\"\"\n        with self._lock:\n            if event_type not in self._subscribers:\n                self._subscribers[event_type] = []\n            self._subscribers[event_type].append(handler)\n    \n    def publish(self, event: Event, synchronous: bool = False) -> None:\n        \"\"\"Publish event to subscribers.\"\"\"\n        handlers = self._subscribers.get(event.type, [])\n        \n        if synchronous:\n            self._notify_handlers_sync(event, handlers)\n        else:\n            # For async implementation, use threading or async/await\n            threading.Thread(\n                target=self._notify_handlers_sync,\n                args=(event, handlers),\n                daemon=True\n            ).start()\n    \n    def _notify_handlers_sync(self, event: Event, handlers: List[Callable]) -> None:\n        \"\"\"Notify all handlers synchronously with error isolation.\"\"\"\n        for handler in handlers:\n            try:\n                handler(event)\n            except Exception as e:\n                self._logger.error(f\"Event handler failed: {e}\", exc_info=True)\n\n# src/mlops_platform/health.py\nclass HealthStatus(Enum):\n    \"\"\"Health check status values.\"\"\"\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\" \n    UNHEALTHY = \"unhealthy\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass HealthCheck:\n    \"\"\"Health check result.\"\"\"\n    name: str\n    status: HealthStatus\n    message: str\n    timestamp: float\n    details: Dict[str, Any] = field(default_factory=dict)\n\nclass ComponentHealth:\n    \"\"\"Manages health checks for a component.\"\"\"\n    \n    def __init__(self):\n        self._checks: Dict[str, Callable[[], HealthCheck]] = {}\n    \n    def add_check(self, check_name: str, check_func: Callable[[], HealthCheck]) -> None:\n        \"\"\"Register periodic health check function.\"\"\"\n        self._checks[check_name] = check_func\n    \n    def run_checks(self) -> List[HealthCheck]:\n        \"\"\"Execute all health checks and return results.\"\"\"\n        results = []\n        for name, check_func in self._checks.items():\n            try:\n                result = check_func()\n                results.append(result)\n            except Exception as e:\n                results.append(HealthCheck(\n                    name=name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=f\"Check failed: {str(e)}\",\n                    timestamp=time.time(),\n                    details={\"error\": str(e)}\n                ))\n        return results\n\n# src/mlops_platform/storage.py  \nclass MetadataStore(ABC):\n    \"\"\"Abstract interface for metadata storage operations.\"\"\"\n    \n    @abstractmethod\n    def create_table(self, table_name: str, schema: Dict[str, Any]) -> None:\n        \"\"\"Create table with specified schema.\"\"\"\n        pass\n    \n    @abstractmethod  \n    def insert(self, table_name: str, data: Dict[str, Any]) -> str:\n        \"\"\"Insert data and return generated ID.\"\"\"\n        pass\n    \n    @abstractmethod\n    def update(self, table_name: str, id: str, data: Dict[str, Any]) -> bool:\n        \"\"\"Update record by ID, return success status.\"\"\"\n        pass\n    \n    @abstractmethod\n    def query(self, table_name: str, filters: Dict[str, Any], \n              limit: int = 100, offset: int = 0) -> List[Dict[str, Any]]:\n        \"\"\"Query records with filters and pagination.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_by_id(self, table_name: str, id: str) -> Dict[str, Any]:\n        \"\"\"Get single record by ID.\"\"\"\n        pass\n\nclass ArtifactStore(ABC):\n    \"\"\"Abstract interface for binary artifact storage.\"\"\"\n    \n    @abstractmethod\n    def put(self, key: str, data: bytes, metadata: Dict[str, Any] = None) -> str:\n        \"\"\"Store binary data with optional metadata.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get(self, key: str) -> bytes:\n        \"\"\"Retrieve binary data by key.\"\"\"\n        pass\n    \n    @abstractmethod  \n    def delete(self, key: str) -> bool:\n        \"\"\"Delete artifact, return success status.\"\"\"\n        pass\n    \n    @abstractmethod\n    def list_keys(self, prefix: str = \"\") -> List[str]:\n        \"\"\"List artifact keys with optional prefix filter.\"\"\"\n        pass\n```\n\n**Component Skeleton Structure:**\n\nEach component follows this pattern for consistent organization:\n\n```python\n# src/experiment_tracking/models.py\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, Optional, List\nfrom datetime import datetime\n\n@dataclass\nclass Experiment:\n    \"\"\"Experiment entity representing a group of related training runs.\"\"\"\n    # TODO: Define experiment fields based on data model section\n    pass\n\n@dataclass  \nclass Run:\n    \"\"\"Training run entity with parameters, metrics, and artifacts.\"\"\"\n    # TODO: Define run fields based on data model section\n    pass\n\n# src/experiment_tracking/service.py\nclass ExperimentTrackingService:\n    \"\"\"Business logic for experiment tracking operations.\"\"\"\n    \n    def __init__(self, metadata_store: MetadataStore, artifact_store: ArtifactStore,\n                 event_coordinator: EventCoordinator):\n        # TODO 1: Store dependencies for data access and event publishing\n        pass\n    \n    def log_parameter(self, run_id: str, key: str, value: Any) -> None:\n        \"\"\"Log parameter for a training run.\"\"\"\n        # TODO 1: Validate run_id exists and is in active state\n        # TODO 2: Validate parameter key format (no special characters)\n        # TODO 3: Store parameter in metadata store with run association\n        # TODO 4: Update run's last_modified timestamp\n        # TODO 5: Publish parameter_logged event for real-time updates\n        pass\n    \n    def log_metric(self, run_id: str, key: str, value: float, step: int = None) -> None:\n        \"\"\"Log metric value for a training run at specific step.\"\"\"\n        # TODO 1: Validate run_id exists and metric key format\n        # TODO 2: If step is None, auto-increment from last step for this metric\n        # TODO 3: Store metric with timestamp in time-series optimized format  \n        # TODO 4: Update metric aggregations (min, max, latest) for run\n        # TODO 5: Publish metric_logged event with real-time value\n        pass\n\n# src/experiment_tracking/api.py\nfrom flask import Flask, request, jsonify\nfrom mlops_platform.health import ComponentHealth, HealthStatus, HealthCheck\n\ndef create_app(service: ExperimentTrackingService) -> Flask:\n    \"\"\"Create Flask application with experiment tracking endpoints.\"\"\"\n    app = Flask(__name__)\n    health = ComponentHealth()\n    \n    # TODO: Add health checks for database and storage connectivity\n    \n    @app.route('/health', methods=['GET'])\n    def health_check():\n        \"\"\"Health check endpoint for monitoring and load balancers.\"\"\"\n        # TODO 1: Run all registered health checks\n        # TODO 2: Return 200 if all healthy, 503 if any unhealthy\n        # TODO 3: Include health check details in response body\n        pass\n    \n    @app.route('/experiments', methods=['POST'])\n    def create_experiment():\n        \"\"\"Create new experiment for organizing training runs.\"\"\"\n        # TODO 1: Extract experiment name and metadata from request\n        # TODO 2: Validate experiment name is unique and follows naming rules\n        # TODO 3: Call service to create experiment and return experiment ID\n        # TODO 4: Return 201 with experiment details or 400 for validation errors\n        pass\n    \n    @app.route('/runs/<run_id>/parameters', methods=['POST'])\n    def log_parameter(run_id: str):\n        \"\"\"Log parameter for specific training run.\"\"\"\n        # TODO 1: Extract parameter key and value from request body\n        # TODO 2: Validate request format and parameter value type\n        # TODO 3: Call service to log parameter with error handling\n        # TODO 4: Return 200 on success or appropriate error status\n        pass\n    \n    return app\n```\n\n**Language-Specific Implementation Tips:**\n\n- **Database Connections**: Use connection pooling with SQLAlchemy's `create_engine(pool_size=20, pool_recycle=3600)` for production deployments\n- **Async Operations**: Consider using FastAPI with async/await for high-throughput endpoints, especially metric logging\n- **Error Handling**: Implement structured exception handling with custom exception types for domain errors vs infrastructure errors\n- **Configuration**: Use Pydantic BaseSettings for type-safe configuration with automatic validation and environment variable binding\n- **Logging**: Configure structured logging with correlation IDs to trace requests across components: `logging.basicConfig(format='%(asctime)s %(name)s %(levelname)s [%(correlation_id)s] %(message)s')`\n- **Testing**: Use pytest fixtures for database setup/teardown and factory_boy for generating test data with realistic relationships\n\n**Development Environment Setup:**\n\nCreate `docker-compose.dev.yml` for local development with all dependencies:\n\n```yaml\nversion: '3.8'\nservices:\n  postgres:\n    image: timescale/timescaledb:latest-pg14\n    environment:\n      POSTGRES_DB: mlops\n      POSTGRES_USER: mlops  \n      POSTGRES_PASSWORD: development\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      \n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n      \n  minio:\n    image: minio/minio:latest\n    command: server /data --console-address \":9001\"\n    environment:\n      MINIO_ROOT_USER: minioadmin\n      MINIO_ROOT_PASSWORD: minioadmin\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    volumes:\n      - minio_data:/data\n\nvolumes:\n  postgres_data:\n  minio_data:\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the core infrastructure:\n\n1. **Run Infrastructure Tests**: `python -m pytest tests/unit/test_events.py -v` should show all event coordination tests passing\n2. **Verify Database Connectivity**: Start the development environment with `docker-compose -f docker-compose.dev.yml up -d` and run connection tests\n3. **Check Health Endpoints**: Each component's `/health` endpoint should return 200 with status details when dependencies are available\n4. **Test Event Flow**: Register event handlers and publish test events to verify cross-component communication works\n5. **Validate Configuration**: Components should start successfully with environment variables and fail gracefully with helpful error messages for missing required configuration\n\n**Common Development Issues:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Components can't find each other | Service discovery misconfiguration | Check component logs for connection errors | Verify service names match docker-compose service definitions |\n| Database connection pool exhausted | Too many concurrent connections without proper cleanup | Monitor database connection count during load | Use context managers for database sessions: `with get_session() as session:` |\n| Events not being delivered | Event coordinator not properly initialized | Add debug logging to event handlers | Ensure EventCoordinator is shared across component modules as singleton |\n| Health checks always failing | Dependencies not ready during startup | Check component startup order in logs | Add retry logic with exponential backoff for dependency connections |\n| Slow API responses | Database queries without proper indexing | Enable SQL query logging and check execution plans | Add database indexes for frequently queried columns (run_id, experiment_id, timestamp) |\n\n\n## Data Model\n\n> **Milestone(s):** This section establishes the core data structures and entity relationships that underpin all milestones (1-5), providing the foundational schema for experiment tracking, model registry, pipeline orchestration, deployment management, and monitoring systems.\n\nThe data model serves as the backbone of our MLOps platform, defining how we represent and relate the core entities throughout the machine learning lifecycle. Think of this as the **architectural blueprint** for a modern research facility - just as a laboratory needs well-organized systems for tracking experiments, storing samples, managing equipment, and recording results, our platform needs structured data representations for experiments, models, pipelines, and deployments.\n\nThe design follows a **microservices approach** where each component maintains its own data stores optimized for specific access patterns, a strategy known as **polyglot persistence**. This allows experiment tracking to use time-series optimized storage for metrics, while the model registry uses content-addressable storage for artifacts, and monitoring systems use real-time analytics databases.\n\n![Core Entity Relationships](./diagrams/data-model-relationships.svg)\n\n### Mental Model: Digital Laboratory Information System\n\nBefore diving into the technical schemas, consider how a modern pharmaceutical research laboratory organizes its information systems. The laboratory maintains several interconnected databases: an **experiment logbook** tracking research protocols and results, a **compound registry** managing chemical formulations and their versions, a **workflow scheduler** coordinating multi-step synthesis procedures, a **production deployment system** managing which compounds are in clinical trials, and a **monitoring dashboard** tracking patient outcomes and side effects.\n\nOur MLOps platform mirrors this organization. **Experiment tracking** serves as the digital logbook, capturing every training run with its hyperparameters, metrics, and generated artifacts. The **model registry** functions like the compound database, maintaining versioned models with their lineage and approval status. **Pipeline orchestration** coordinates complex training workflows like the synthesis scheduler. **Model deployment** manages which models serve production traffic, similar to clinical trial management. **Model monitoring** tracks real-world performance like patient outcome monitoring.\n\nEach system maintains its own specialized data structures while sharing common identifiers that enable **model lineage** - the ability to trace a production model back through its deployment, training pipeline, experiment run, and source data. This traceability proves essential for debugging, compliance, and understanding model behavior in production.\n\n> **Design Principle: Immutable Core with Mutable Metadata**\n> \n> Core entities like experiment runs and model versions are immutable once created - their content cannot change, only their metadata (tags, descriptions, stage assignments) can be updated. This ensures reproducibility while allowing operational flexibility.\n\n### Architecture Decision: Entity Relationship Design\n\n> **Decision: Hierarchical Entity Organization with Cross-Component References**\n> - **Context**: MLOps platforms need to represent complex relationships between experiments, models, pipelines, and deployments while maintaining clear ownership boundaries for microservices\n> - **Options Considered**: \n>   1. Monolithic shared database with foreign key constraints\n>   2. Duplicated entity data in each component database\n>   3. Hierarchical ownership with cross-component reference IDs\n> - **Decision**: Hierarchical ownership with cross-component reference IDs\n> - **Rationale**: Allows each component to optimize its data store while maintaining loose coupling. Reference IDs enable lineage tracking without creating tight database dependencies that would compromise service autonomy\n> - **Consequences**: Enables polyglot persistence and independent scaling, but requires eventual consistency patterns and careful handling of dangling references\n\n| Option | Pros | Cons | Scalability | Chosen |\n|--------|------|------|-------------|---------|\n| Monolithic Database | Strong consistency, enforced relationships | Tight coupling, single point of failure | Limited | No |\n| Duplicated Data | Complete service autonomy | Data sync complexity, storage overhead | High | No |\n| Reference IDs | Loose coupling, optimized storage | Eventual consistency, reference validation | High | **Yes** |\n\n## Experiment Tracking Entities\n\nThe experiment tracking component organizes machine learning research using a three-level hierarchy that mirrors scientific research practices. This hierarchy provides progressively finer granularity for organizing and analyzing training efforts.\n\n### Core Entity Hierarchy\n\nAt the top level, an **Experiment** represents a research hypothesis or approach - for example, \"CNN architectures for image classification\" or \"BERT fine-tuning for sentiment analysis.\" Within each experiment, multiple **Runs** capture individual training attempts with specific hyperparameter configurations. Each run generates **Parameters**, **Metrics**, and **Artifacts** that collectively document the training process and results.\n\n| Entity | Purpose | Cardinality | Lifespan |\n|--------|---------|-------------|----------|\n| Experiment | Group related research efforts | 1 to many Runs | Indefinite |\n| Run | Single training execution | 1 to many Parameters/Metrics/Artifacts | Immutable after completion |\n| Parameter | Input configuration value | Many per Run | Immutable |\n| Metric | Measured training result | Many per Run, many per training step | Append-only |\n| Artifact | Generated file or object | Many per Run | Immutable |\n\n### Experiment Entity Schema\n\nThe `Experiment` entity serves as a logical container for related training runs, enabling researchers to organize their work by project, approach, or research question.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| experiment_id | str | Unique identifier for the experiment | Primary key, UUID format |\n| name | str | Human-readable experiment name | Required, max 255 chars |\n| description | str | Detailed explanation of research goal | Optional, max 2048 chars |\n| tags | Dict[str, str] | Key-value labels for organization | Optional, max 20 tags |\n| creator_user_id | str | User who created the experiment | Required, immutable |\n| created_at | float | Unix timestamp of creation | Required, immutable |\n| updated_at | float | Unix timestamp of last modification | Auto-updated |\n| run_count | int | Number of runs in this experiment | Computed field |\n| lifecycle_stage | str | active, deleted, archived | Default: active |\n\nExperiments support **soft deletion** through the `lifecycle_stage` field, allowing recovery of accidentally deleted experiments while hiding them from normal queries. The `tags` field enables flexible organization schemes - teams might tag experiments by model family (\"cnn\", \"transformer\"), dataset (\"imagenet\", \"coco\"), or business objective (\"accuracy\", \"latency\").\n\n### Run Entity Schema\n\nThe `Run` entity captures a single training execution, serving as the central organizing unit for all training artifacts and measurements.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| run_id | str | Unique identifier for the run | Primary key, UUID format |\n| experiment_id | str | Parent experiment reference | Foreign key, immutable |\n| run_name | str | Human-readable run identifier | Optional, max 255 chars |\n| status | str | RUNNING, COMPLETED, FAILED, KILLED | Required, state machine |\n| start_time | float | Unix timestamp when run began | Required, immutable |\n| end_time | float | Unix timestamp when run finished | Optional, set on completion |\n| source_type | str | Type of execution environment | NOTEBOOK, SCRIPT, PIPELINE |\n| source_name | str | Specific source identifier | File path, notebook name, etc. |\n| source_version | str | Code version or commit hash | Optional, for reproducibility |\n| user_id | str | User who initiated the run | Required, immutable |\n| tags | Dict[str, str] | Run-specific key-value labels | Optional, max 50 tags |\n| lifecycle_stage | str | active, deleted | Default: active |\n\nThe `status` field follows a strict state machine to track run progression:\n\n| Current Status | Valid Transitions | Trigger Events | Automated Actions |\n|---------------|------------------|----------------|-------------------|\n| RUNNING | COMPLETED, FAILED, KILLED | Training completion, error, user termination | Set end_time, finalize metrics |\n| COMPLETED | deleted (via lifecycle_stage) | User deletion | Archive artifacts |\n| FAILED | deleted (via lifecycle_stage) | User deletion | Preserve error logs |\n| KILLED | deleted (via lifecycle_stage) | User deletion | Mark incomplete |\n\n### Parameter Entity Schema\n\nParameters capture the input configuration for a training run, including hyperparameters, dataset specifications, and environment settings. The schema supports both simple scalar values and complex nested configurations.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| parameter_id | str | Unique identifier for this parameter | Primary key, UUID format |\n| run_id | str | Parent run reference | Foreign key, immutable |\n| key | str | Parameter name with optional nesting | Required, dot notation supported |\n| value | str | Parameter value as string | Required, JSON-serialized for complex types |\n| value_type | str | Original data type | INT, FLOAT, STRING, BOOL, JSON |\n| created_at | float | Unix timestamp of parameter logging | Required, immutable |\n\nParameters support **hierarchical naming** using dot notation to represent nested configurations. For example, a training configuration might include:\n\n- `model.type: \"cnn\"`\n- `model.layers.conv1.filters: \"32\"`\n- `model.layers.conv1.kernel_size: \"3\"`\n- `optimizer.name: \"adam\"`\n- `optimizer.learning_rate: \"0.001\"`\n\nThis structure enables efficient querying for parameter ranges (\"find all runs where `optimizer.learning_rate` > 0.01\") while maintaining the semantic structure of complex configurations.\n\n### Metric Entity Schema\n\nMetrics capture quantitative measurements during training, supporting both scalar values logged at specific steps and aggregate statistics computed across runs.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| metric_id | str | Unique identifier for this metric | Primary key, UUID format |\n| run_id | str | Parent run reference | Foreign key, immutable |\n| key | str | Metric name (loss, accuracy, etc.) | Required, max 255 chars |\n| value | float | Numeric measurement | Required, supports NaN/Inf |\n| step | int | Training step or epoch number | Optional, for time series |\n| timestamp | float | Unix timestamp of measurement | Required, for temporal ordering |\n| created_at | float | Unix timestamp when logged | Required, immutable |\n\nThe dual timestamp system supports both **logical ordering** (via `step`) and **wall-clock analysis** (via `timestamp`). This proves essential for understanding training dynamics, especially in distributed training scenarios where logical steps might complete out of wall-clock order.\n\nMetrics support several logging patterns:\n\n1. **Step-based logging**: `loss` and `accuracy` recorded at each training step\n2. **Epoch summarization**: `epoch_loss_avg` and `validation_accuracy` recorded per epoch\n3. **Final aggregation**: `best_validation_accuracy` and `total_training_time` recorded once per run\n\n### Artifact Entity Schema\n\nArtifacts represent files and binary objects generated during training runs, including model checkpoints, plots, datasets, and configuration files.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| artifact_id | str | Unique identifier for this artifact | Primary key, UUID format |\n| run_id | str | Parent run reference | Foreign key, immutable |\n| path | str | Logical path within run namespace | Required, hierarchical |\n| artifact_uri | str | Physical storage location | Required, URI format |\n| file_size | int | Size in bytes | Optional, for storage tracking |\n| checksum | str | SHA-256 hash of contents | Optional, for integrity verification |\n| artifact_type | str | MODEL, DATASET, PLOT, CONFIG, LOG | Classification for UI organization |\n| mime_type | str | Content type hint | Optional, for download handling |\n| created_at | float | Unix timestamp of artifact creation | Required, immutable |\n\nArtifacts use a **logical path hierarchy** that abstracts physical storage details. For example, a run might contain:\n\n- `model/checkpoint-final.pkl` → `s3://artifacts/runs/abc123/model/checkpoint-final.pkl`\n- `plots/loss-curve.png` → `s3://artifacts/runs/abc123/plots/loss-curve.png`\n- `data/train-dataset.parquet` → `s3://artifacts/runs/abc123/data/train-dataset.parquet`\n\nThe `checksum` field enables **artifact deduplication** - multiple runs producing identical model files can reference the same physical storage while maintaining separate logical paths.\n\n### Querying and Analysis Patterns\n\nThe experiment tracking schema supports several critical query patterns for ML research workflows:\n\n**Experiment comparison queries** filter runs by parameter ranges and sort by metric values:\n```\nFind runs where optimizer.learning_rate BETWEEN 0.001 AND 0.01 \nAND model.type = \"transformer\" \nORDER BY best_validation_accuracy DESC LIMIT 10\n```\n\n**Time-series analysis queries** retrieve metric evolution for specific runs:\n```\nSelect step, value FROM metrics \nWHERE run_id = \"abc123\" AND key = \"validation_loss\" \nORDER BY step ASC\n```\n\n**Parameter correlation queries** identify relationships between configuration and outcomes:\n```\nSELECT parameters.value as batch_size, AVG(metrics.value) as avg_accuracy\nFROM parameters JOIN metrics ON parameters.run_id = metrics.run_id\nWHERE parameters.key = \"batch_size\" AND metrics.key = \"final_accuracy\"\nGROUP BY parameters.value\n```\n\n## Model Registry Entities\n\nThe model registry manages trained models through their complete lifecycle, from initial registration through production deployment to eventual archival. The registry implements a **software package registry** pattern similar to npm or Docker Hub, providing versioning, metadata management, and stage-based promotion workflows.\n\n### Model Lifecycle Overview\n\nModels progress through a structured lifecycle with explicit stage transitions and approval gates. This mirrors software release management practices, ensuring that only validated models reach production environments.\n\n![Model Version State Machine](./diagrams/model-lifecycle.svg)\n\n| Stage | Purpose | Entry Criteria | Exit Actions |\n|-------|---------|----------------|--------------|\n| Development | Initial model registration | Completed training run | Enable further testing |\n| Staging | Pre-production validation | Manual promotion or automated criteria | Deploy to staging environment |\n| Production | Live traffic serving | Approval workflow completion | Route production traffic |\n| Archived | Historical preservation | Superseded by newer version | Remove from active serving |\n\n### Model Entity Schema\n\nThe `Model` entity represents a named family of related model versions, analogous to a software package name that contains multiple versioned releases.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| model_id | str | Unique identifier for model family | Primary key, UUID format |\n| name | str | Human-readable model name | Required, unique, max 255 chars |\n| description | str | Purpose and architecture description | Optional, max 2048 chars |\n| tags | Dict[str, str] | Model family metadata | Optional, max 20 tags |\n| creation_source | str | How model was created | EXPERIMENT, IMPORT, PIPELINE |\n| creator_user_id | str | User who registered the model | Required, immutable |\n| created_at | float | Unix timestamp of initial registration | Required, immutable |\n| updated_at | float | Unix timestamp of last modification | Auto-updated |\n| latest_version | str | Most recent version number | Computed field |\n| current_stage_version | Dict[str, str] | Current version per stage | Computed field |\n\nThe `current_stage_version` field maintains a mapping from stage names to version numbers, enabling quick lookup of which version currently serves each environment:\n```json\n{\n  \"Development\": \"1.2.3\", \n  \"Staging\": \"1.2.1\", \n  \"Production\": \"1.1.5\"\n}\n```\n\n### ModelVersion Entity Schema\n\nThe `ModelVersion` entity captures a specific iteration of a model with its artifacts, metadata, and stage assignment. Model versions are **immutable** once created - their core content cannot change, only their stage assignments and descriptive metadata.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| version_id | str | Unique identifier for this version | Primary key, UUID format |\n| model_id | str | Parent model family reference | Foreign key, immutable |\n| version | str | Semantic version number | Required, immutable, semver format |\n| stage | str | Current lifecycle stage | DEVELOPMENT, STAGING, PRODUCTION, ARCHIVED |\n| status | str | Version processing status | CREATING, READY, FAILED |\n| source_run_id | str | Originating experiment run | Optional, for lineage tracking |\n| model_uri | str | Primary model artifact location | Required, URI format |\n| model_format | str | Serialization format | PICKLE, ONNX, TENSORFLOW, PYTORCH |\n| model_signature | Dict | Input/output schema definition | Optional, for compatibility validation |\n| model_metrics | Dict[str, float] | Performance measurements | Optional, from training or validation |\n| description | str | Version-specific notes | Optional, max 1024 chars |\n| tags | Dict[str, str] | Version-specific metadata | Optional, max 50 tags |\n| created_at | float | Unix timestamp of version creation | Required, immutable |\n| updated_at | float | Unix timestamp of last metadata update | Auto-updated |\n| creator_user_id | str | User who created this version | Required, immutable |\n\nThe `model_signature` field captures the expected input and output schema for the model, enabling compatibility validation during deployment:\n\n```json\n{\n  \"inputs\": [\n    {\"name\": \"features\", \"type\": \"tensor\", \"shape\": [-1, 784], \"dtype\": \"float32\"}\n  ],\n  \"outputs\": [\n    {\"name\": \"predictions\", \"type\": \"tensor\", \"shape\": [-1, 10], \"dtype\": \"float32\"}\n  ]\n}\n```\n\n### Stage Transition Management\n\nModel versions transition between stages through explicit promotion actions that can include approval workflows, automated testing, and rollback capabilities.\n\n| Transition | Required Checks | Approval Gates | Automated Actions |\n|------------|----------------|----------------|-------------------|\n| Development → Staging | Model signature validation | Optional: Team lead approval | Deploy to staging environment |\n| Staging → Production | Performance benchmarks, compatibility tests | Required: Production approval | Create deployment, update routing |\n| Production → Archived | Replacement version in production | Optional: Cleanup approval | Remove from serving, archive artifacts |\n| Any → Archived | None | User confirmation | Stop all serving, preserve metadata |\n\n### Model Lineage Tracking\n\nThe model registry maintains comprehensive **model lineage** by linking each model version back to its training context, enabling full reproducibility and debugging capabilities.\n\n| Field | Type | Description | Usage |\n|-------|------|-------------|-------|\n| source_run_id | str | Originating experiment run ID | Link to training parameters and metrics |\n| data_version_hash | str | Training dataset fingerprint | Detect data dependencies |\n| code_commit_hash | str | Source code version | Enable code-level reproducibility |\n| training_pipeline_id | str | Pipeline that created model | Link to orchestration context |\n| parent_model_ids | List[str] | Models used as inputs | Track model composition and transfer learning |\n| derived_model_ids | List[str] | Models created from this version | Forward lineage tracking |\n\nThis lineage information supports critical MLOps workflows:\n\n1. **Root cause analysis**: When a production model fails, trace back to the specific training data, code version, and hyperparameters that created it\n2. **Impact analysis**: When a security issue is discovered in training data, identify all models that might be affected\n3. **Compliance auditing**: Demonstrate the complete provenance of models used in regulated environments\n4. **Reproducibility**: Recreate the exact conditions that produced a specific model version\n\n### Architecture Decision: Immutable Versions with Mutable Metadata\n\n> **Decision: Separate Immutable Core from Mutable Operational Metadata**\n> - **Context**: Model versions need both immutability for reproducibility and flexibility for operational management (stage assignments, descriptions, tags)\n> - **Options Considered**:\n>   1. Fully immutable versions requiring new versions for any changes\n>   2. Fully mutable versions allowing arbitrary modifications\n>   3. Split design with immutable core and mutable metadata\n> - **Decision**: Split design with immutable core and mutable metadata\n> - **Rationale**: Preserves reproducibility for model content while enabling operational flexibility. Clearly separates what affects model behavior (immutable) from what affects model management (mutable)\n> - **Consequences**: Requires careful field classification and schema design, but provides both reproducibility and operational flexibility\n\n| Field Category | Mutability | Examples | Rationale |\n|----------------|------------|----------|-----------|\n| Core Identity | Immutable | version_id, model_id, version | Never change to preserve references |\n| Model Content | Immutable | model_uri, model_format, model_signature | Changes would create different model |\n| Training Context | Immutable | source_run_id, created_at, creator_user_id | Historical facts cannot change |\n| Operational Metadata | Mutable | stage, description, tags | Management info can evolve |\n| Computed Fields | Auto-updated | updated_at, status | Reflect current state |\n\n## Pipeline Entities\n\nPipeline orchestration manages complex multi-step training workflows through **directed acyclic graph (DAG)** representations that capture data dependencies, resource requirements, and execution constraints. The pipeline data model supports both template definitions for reusable workflows and execution instances that track specific runs.\n\n### Mental Model: Manufacturing Assembly Line\n\nThink of ML training pipelines like a sophisticated manufacturing assembly line. The **pipeline definition** serves as the blueprint showing all stations, their sequence, and what resources each station needs. A **pipeline execution** represents running that assembly line for a specific order, tracking which station is currently active, what materials are flowing between stations, and any quality control checkpoints along the way.\n\nEach **pipeline step** corresponds to a manufacturing station with specific equipment requirements (CPU, memory, GPU), input materials (datasets, models), processing instructions (training code), and output products (trained models, evaluation metrics). The assembly line supervisor (pipeline orchestrator) ensures stations receive their inputs on time, allocate resources efficiently, and handle equipment failures gracefully.\n\n### Pipeline Definition Schema\n\nThe `PipelineDefinition` entity captures reusable workflow templates that can be executed multiple times with different parameters and data inputs.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| pipeline_id | str | Unique identifier for pipeline template | Primary key, UUID format |\n| name | str | Human-readable pipeline name | Required, max 255 chars |\n| description | str | Purpose and workflow overview | Optional, max 2048 chars |\n| version | str | Pipeline definition version | Required, semver format |\n| dag_definition | Dict | Step definitions and dependencies | Required, JSON schema validated |\n| parameters | Dict | Configurable pipeline parameters | Optional, with default values |\n| resource_defaults | Dict | Default compute resource allocations | Optional, inheritable by steps |\n| schedule | str | Optional automatic execution schedule | Optional, cron format |\n| tags | Dict[str, str] | Pipeline metadata for organization | Optional, max 20 tags |\n| creator_user_id | str | User who created pipeline | Required, immutable |\n| created_at | float | Unix timestamp of creation | Required, immutable |\n| updated_at | float | Unix timestamp of last modification | Auto-updated |\n| is_active | bool | Whether pipeline can be executed | Default: true |\n\nThe `dag_definition` field contains the complete workflow specification including step definitions, dependencies, and conditional execution logic:\n\n```json\n{\n  \"steps\": {\n    \"data_validation\": {\n      \"type\": \"python_script\",\n      \"script_path\": \"scripts/validate_data.py\",\n      \"resources\": {\"cpu\": 2, \"memory\": \"4Gi\"},\n      \"outputs\": [\"validated_data\"]\n    },\n    \"feature_engineering\": {\n      \"type\": \"python_script\", \n      \"script_path\": \"scripts/build_features.py\",\n      \"depends_on\": [\"data_validation\"],\n      \"resources\": {\"cpu\": 4, \"memory\": \"8Gi\"},\n      \"inputs\": [\"validated_data\"],\n      \"outputs\": [\"feature_matrix\"]\n    },\n    \"model_training\": {\n      \"type\": \"python_script\",\n      \"script_path\": \"scripts/train_model.py\", \n      \"depends_on\": [\"feature_engineering\"],\n      \"resources\": {\"cpu\": 8, \"memory\": \"16Gi\", \"gpu\": 1},\n      \"inputs\": [\"feature_matrix\"],\n      \"outputs\": [\"trained_model\", \"training_metrics\"]\n    }\n  },\n  \"conditions\": {\n    \"model_training\": \"feature_engineering.accuracy > 0.8\"\n  }\n}\n```\n\n### Pipeline Execution Schema\n\nThe `PipelineExecution` entity tracks specific runs of pipeline definitions, maintaining state for each step and capturing execution context.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| execution_id | str | Unique identifier for this execution | Primary key, UUID format |\n| pipeline_id | str | Reference to pipeline definition | Foreign key, immutable |\n| pipeline_version | str | Version of definition used | Required, immutable |\n| status | str | Overall execution status | PENDING, RUNNING, COMPLETED, FAILED, CANCELLED |\n| start_time | float | Unix timestamp when execution began | Required, immutable |\n| end_time | float | Unix timestamp when execution finished | Optional, set on completion |\n| parameters | Dict | Parameter values for this execution | Required, merged with defaults |\n| trigger_type | str | How execution was initiated | MANUAL, SCHEDULED, API, WEBHOOK |\n| trigger_user_id | str | User who initiated execution | Optional, null for automated triggers |\n| execution_context | Dict | Environment and runtime metadata | Optional, execution environment details |\n| step_executions | List[str] | References to step execution records | Computed field |\n| artifacts | Dict[str, str] | Execution-level artifact URIs | Optional, summary outputs |\n| tags | Dict[str, str] | Execution-specific metadata | Optional, max 50 tags |\n\nThe execution follows a state machine that coordinates step-level progress:\n\n| Current Status | Valid Transitions | Trigger Events | Automated Actions |\n|---------------|------------------|----------------|-------------------|\n| PENDING | RUNNING, CANCELLED | Resource allocation, user cancellation | Initialize step queue |\n| RUNNING | COMPLETED, FAILED, CANCELLED | All steps complete, step failure, user action | Update step states |\n| COMPLETED | None | N/A | Archive artifacts, notify subscribers |\n| FAILED | PENDING (retry) | Retry command | Reset failed steps |\n| CANCELLED | PENDING (restart) | Restart command | Clean up resources |\n\n### Pipeline Step Execution Schema\n\nThe `PipelineStepExecution` entity tracks individual step executions within a pipeline run, providing detailed progress and resource usage information.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| step_execution_id | str | Unique identifier for step execution | Primary key, UUID format |\n| execution_id | str | Parent pipeline execution reference | Foreign key, immutable |\n| step_name | str | Step name from pipeline definition | Required, matches DAG definition |\n| status | str | Step execution status | PENDING, RUNNING, COMPLETED, FAILED, SKIPPED |\n| start_time | float | Unix timestamp when step began | Optional, set when resources allocated |\n| end_time | float | Unix timestamp when step finished | Optional, set on completion |\n| allocated_resources | Dict | Actual resources allocated to step | Optional, may differ from requested |\n| resource_usage | Dict | Measured resource consumption | Optional, collected during execution |\n| container_image | str | Docker image used for execution | Optional, for containerized steps |\n| worker_node_id | str | Compute node where step executed | Optional, for debugging |\n| exit_code | int | Process exit code | Optional, for script-type steps |\n| logs_uri | str | Location of execution logs | Optional, for debugging |\n| input_artifacts | Dict[str, str] | Input artifact URIs | Required, from upstream steps |\n| output_artifacts | Dict[str, str] | Generated artifact URIs | Optional, populated on completion |\n| error_message | str | Error details if step failed | Optional, for failure diagnosis |\n| retry_count | int | Number of retry attempts | Default: 0 |\n\nResource usage tracking captures detailed performance metrics for optimization and cost analysis:\n\n```json\n{\n  \"cpu_usage\": {\n    \"max_cores\": 7.8,\n    \"avg_cores\": 6.2, \n    \"duration_seconds\": 1800\n  },\n  \"memory_usage\": {\n    \"max_bytes\": 15728640000,\n    \"avg_bytes\": 12884901888\n  },\n  \"gpu_usage\": {\n    \"max_utilization\": 0.95,\n    \"avg_utilization\": 0.82,\n    \"memory_used_bytes\": 10737418240\n  },\n  \"io_stats\": {\n    \"bytes_read\": 5368709120,\n    \"bytes_written\": 2147483648,\n    \"read_ops\": 1024,\n    \"write_ops\": 512\n  }\n}\n```\n\n### Data Flow and Artifact Passing\n\nPipeline steps communicate through **artifact passing** where upstream steps produce outputs that become inputs for downstream steps. The pipeline orchestrator manages this data flow through a combination of object storage and metadata tracking.\n\n| Artifact Flow Stage | Components | Storage Location | Metadata Updates |\n|---------------------|------------|------------------|------------------|\n| Step Output Generation | Step execution environment | Temporary staging area | Register artifact URI and metadata |\n| Artifact Registration | Pipeline orchestrator | Permanent object storage | Update step execution output_artifacts |\n| Dependency Resolution | Pipeline orchestrator | Metadata store queries | Populate downstream input_artifacts |\n| Input Provisioning | Step execution environment | Local cache or mount | Download/mount artifacts for processing |\n\nThe orchestrator implements several optimization strategies for artifact management:\n\n1. **Lazy Loading**: Downloads artifacts only when steps are ready to execute\n2. **Caching**: Reuses artifacts from previous executions when inputs haven't changed\n3. **Parallel Transfers**: Downloads multiple input artifacts concurrently\n4. **Cleanup Policies**: Removes temporary artifacts based on retention rules\n\n### Conditional Execution and Dynamic Workflows\n\nPipeline definitions support **conditional execution** where steps run only when specific criteria are met, enabling dynamic workflows that adapt based on intermediate results.\n\n| Condition Type | Evaluation Context | Example | Use Case |\n|----------------|-------------------|---------|----------|\n| Upstream Artifact | Previous step outputs | `data_validation.row_count > 10000` | Skip training on insufficient data |\n| Upstream Metrics | Previous step metrics | `feature_engineering.feature_count > 50` | Conditional dimensionality reduction |\n| Pipeline Parameters | Execution parameters | `pipeline.mode == \"production\"` | Environment-specific behavior |\n| External State | API calls or database queries | `model_registry.current_accuracy < 0.9` | Trigger retraining workflows |\n\nConditions are evaluated by the pipeline orchestrator using a sandboxed expression engine that prevents arbitrary code execution while supporting complex logical expressions.\n\n## Deployment and Monitoring Entities\n\nThe deployment and monitoring components manage the transition from trained models to production services, tracking model serving infrastructure, traffic management, and real-time performance metrics. These entities capture both the **deployment topology** (how models are served) and **observability data** (how well they perform).\n\n### Mental Model: Restaurant Service Management\n\nConsider how a high-end restaurant manages its service operations. The **deployment system** acts like the kitchen management, coordinating which recipes (models) are prepared by which stations (serving infrastructure), how much of each dish to prepare (scaling policies), and how to roll out new menu items safely (canary deployments). The **monitoring system** functions like a combination of quality control and customer feedback analysis, tracking both kitchen performance (serving latency, resource usage) and diner satisfaction (prediction accuracy, data drift).\n\nJust as restaurants maintain detailed records of recipe versions, preparation techniques, customer preferences, and service quality, our platform tracks model versions, serving configurations, traffic patterns, and prediction performance with the same level of detail and operational rigor.\n\n### Deployment Entity Schema\n\nThe `Deployment` entity represents a specific model version serving configuration, capturing how the model is exposed as an HTTP endpoint with its scaling and traffic management policies.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| deployment_id | str | Unique identifier for deployment | Primary key, UUID format |\n| name | str | Human-readable deployment name | Required, max 255 chars |\n| model_id | str | Reference to deployed model | Foreign key, immutable |\n| model_version | str | Specific model version being served | Required, immutable |\n| environment | str | Deployment environment | DEVELOPMENT, STAGING, PRODUCTION |\n| status | str | Current deployment status | CREATING, HEALTHY, DEGRADED, FAILED, TERMINATING |\n| endpoint_url | str | HTTP endpoint for inference requests | Generated, unique per deployment |\n| serving_config | Dict | Model serving configuration | Required, serving framework specific |\n| scaling_config | Dict | Auto-scaling policy configuration | Required, min/max replicas and triggers |\n| traffic_config | Dict | Traffic routing and splitting rules | Optional, for A/B testing |\n| resource_allocation | Dict | Compute resources per serving replica | Required, CPU/memory/GPU specifications |\n| health_check_config | Dict | Health monitoring configuration | Required, readiness/liveness checks |\n| created_by_user_id | str | User who created deployment | Required, immutable |\n| created_at | float | Unix timestamp of creation | Required, immutable |\n| updated_at | float | Unix timestamp of last modification | Auto-updated |\n| last_health_check | float | Unix timestamp of latest health check | Auto-updated |\n| tags | Dict[str, str] | Deployment metadata for organization | Optional, max 20 tags |\n\nThe `serving_config` field captures framework-specific configuration for model serving platforms like TensorFlow Serving, TorchServe, or Triton Inference Server:\n\n```json\n{\n  \"framework\": \"tensorflow_serving\",\n  \"model_signature_name\": \"serving_default\", \n  \"batch_size\": 32,\n  \"max_batch_delay\": \"0.1s\",\n  \"enable_model_warmup\": true,\n  \"optimization\": {\n    \"enable_batching\": true,\n    \"enable_mixed_precision\": true,\n    \"tensorrt_optimization\": false\n  }\n}\n```\n\n### Deployment Revision Schema\n\nThe `DeploymentRevision` entity tracks changes to deployment configurations over time, enabling rollback capabilities and change auditing.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| revision_id | str | Unique identifier for this revision | Primary key, UUID format |\n| deployment_id | str | Parent deployment reference | Foreign key, immutable |\n| revision_number | int | Sequential revision number | Required, auto-increment per deployment |\n| change_description | str | Human-readable change summary | Required, max 512 chars |\n| config_diff | Dict | Changed configuration fields | Required, shows before/after values |\n| deployment_strategy | str | How changes were applied | BLUE_GREEN, CANARY, ROLLING, IMMEDIATE |\n| rollout_status | str | Rollout progress status | PENDING, IN_PROGRESS, COMPLETED, FAILED, ROLLED_BACK |\n| rollout_start_time | float | Unix timestamp when rollout began | Optional, set when rollout starts |\n| rollout_end_time | float | Unix timestamp when rollout finished | Optional, set on completion |\n| health_metrics_snapshot | Dict | Key metrics captured during rollout | Optional, for rollback decisions |\n| created_by_user_id | str | User who initiated the change | Required, immutable |\n| created_at | float | Unix timestamp of revision creation | Required, immutable |\n| is_active | bool | Whether this revision is currently deployed | Computed field |\n\nDeployment revisions support sophisticated rollout strategies with automatic rollback triggers:\n\n| Strategy | Traffic Pattern | Rollback Triggers | Completion Criteria |\n|----------|----------------|-------------------|-------------------|\n| BLUE_GREEN | Instant 100% switch | Error rate > 5%, latency > 2x baseline | New version stable for 10 minutes |\n| CANARY | Gradual 5% → 25% → 50% → 100% | Error rate > 2%, accuracy drop > 5% | All traffic shifted successfully |\n| ROLLING | Sequential replica replacement | Replica failure rate > 10% | All replicas updated and healthy |\n| IMMEDIATE | Instant replacement | Any serving failure | All replicas serving |\n\n### Endpoint Metrics Schema\n\nThe `EndpointMetrics` entity captures real-time serving performance measurements, supporting both operational monitoring and business analytics.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| metric_id | str | Unique identifier for metric record | Primary key, UUID format |\n| deployment_id | str | Reference to deployment | Foreign key, immutable |\n| metric_name | str | Specific metric being measured | Required, from predefined catalog |\n| metric_value | float | Measured value | Required, supports NaN for missing data |\n| metric_unit | str | Unit of measurement | Required, for proper aggregation |\n| aggregation_window | str | Time window for aggregated metrics | Required, 1m, 5m, 1h, 1d |\n| timestamp | float | Unix timestamp of measurement | Required, bucket-aligned for aggregation |\n| dimensions | Dict[str, str] | Metric dimensions for grouping | Optional, region, replica_id, etc. |\n| percentile | float | Percentile for latency metrics | Optional, 0.5, 0.95, 0.99 |\n| sample_count | int | Number of samples in aggregation | Required for rate calculations |\n| created_at | float | Unix timestamp when metric was recorded | Required, immutable |\n\nKey endpoint metrics include operational and business measurements:\n\n| Metric Category | Metric Names | Units | Aggregation | Purpose |\n|-----------------|-------------|-------|-------------|---------|\n| Latency | request_latency_p50, request_latency_p95, request_latency_p99 | milliseconds | percentile | SLA monitoring |\n| Throughput | requests_per_second, predictions_per_second | count/second | rate | Capacity planning |\n| Errors | error_rate, timeout_rate | percentage | ratio | Quality monitoring |\n| Resources | cpu_utilization, memory_utilization, gpu_utilization | percentage | average | Cost optimization |\n| Business | prediction_confidence, feature_coverage | various | distribution | Model quality |\n\n### Prediction Log Schema\n\nThe `PredictionLog` entity records individual inference requests and responses, enabling detailed analysis of model behavior and data drift detection.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| prediction_id | str | Unique identifier for this prediction | Primary key, UUID format |\n| deployment_id | str | Reference to serving deployment | Foreign key, immutable |\n| request_id | str | Client-provided request identifier | Optional, for client correlation |\n| timestamp | float | Unix timestamp of prediction request | Required, immutable |\n| model_version | str | Model version that generated prediction | Required, for lineage tracking |\n| input_features | Dict | Input feature values (anonymized) | Required, schema-validated |\n| prediction_output | Dict | Model prediction results | Required, includes confidence scores |\n| response_time_ms | float | Total request processing time | Required, includes queue time |\n| feature_hash | str | Hash of input features for drift detection | Required, for distribution tracking |\n| prediction_hash | str | Hash of output for distribution tracking | Required, for concept drift |\n| user_id | str | Anonymized user identifier | Optional, for personalization analysis |\n| session_id | str | Session identifier for request grouping | Optional, for multi-step interactions |\n| client_metadata | Dict[str, str] | Client-provided context | Optional, geography, device, etc. |\n\nPrediction logs support **privacy-preserving analytics** through selective field hashing and configurable retention policies:\n\n1. **Feature anonymization**: Raw feature values are hashed or tokenized to prevent PII exposure\n2. **Sampling policies**: High-traffic models log only a percentage of predictions to control storage costs\n3. **Retention schedules**: Different field categories have different retention periods (metrics forever, raw data 90 days)\n4. **Access controls**: Different roles can access different subsets of logged data\n\n### Data Drift Detection Schema\n\nThe `DriftAnalysis` entity captures statistical analysis of input data and prediction distributions to detect model degradation over time.\n\n| Field | Type | Description | Constraints |\n|-------|------|-------------|-------------|\n| analysis_id | str | Unique identifier for drift analysis | Primary key, UUID format |\n| deployment_id | str | Reference to monitored deployment | Foreign key, immutable |\n| analysis_type | str | Type of drift being measured | FEATURE_DRIFT, PREDICTION_DRIFT, CONCEPT_DRIFT |\n| analysis_window_start | float | Start time of analysis window | Required, defines comparison period |\n| analysis_window_end | float | End time of analysis window | Required, defines comparison period |\n| baseline_window_start | float | Start time of baseline comparison | Required, often training data period |\n| baseline_window_end | float | End time of baseline comparison | Required, often training data period |\n| drift_score | float | Statistical measure of distribution difference | Required, algorithm-specific |\n| drift_method | str | Algorithm used for drift calculation | PSI, KL_DIVERGENCE, WASSERSTEIN, KS_TEST |\n| significance_threshold | float | Threshold for significant drift detection | Required, configurable per deployment |\n| is_drift_detected | bool | Whether drift exceeds threshold | Computed field |\n| feature_drift_scores | Dict[str, float] | Per-feature drift measurements | Optional, for feature-level analysis |\n| affected_features | List[str] | Features showing significant drift | Computed field |\n| sample_sizes | Dict[str, int] | Sample counts for statistical validity | Required, baseline and current |\n| analysis_metadata | Dict | Algorithm-specific analysis details | Optional, confidence intervals, etc. |\n| created_at | float | Unix timestamp of analysis creation | Required, immutable |\n\nDrift detection supports multiple statistical methods optimized for different data types and drift patterns:\n\n| Drift Method | Best For | Sensitivity | Computational Cost | Interpretability |\n|-------------|----------|-------------|-------------------|------------------|\n| PSI (Population Stability Index) | Categorical features | Medium | Low | High |\n| KL Divergence | Continuous distributions | High | Medium | Medium |\n| Wasserstein Distance | Distribution shape changes | High | High | Medium |\n| Kolmogorov-Smirnov Test | Ordinal data | Medium | Low | High |\n| Jensen-Shannon Divergence | Probability distributions | High | Medium | Low |\n\n### Architecture Decision: Real-Time vs. Batch Analytics\n\n> **Decision: Hybrid Real-Time and Batch Analytics Architecture**\n> - **Context**: Model monitoring requires both immediate alerting for critical issues and comprehensive analysis for trend detection, creating tension between latency and analytical depth\n> - **Options Considered**:\n>   1. Pure real-time streaming analytics with immediate processing\n>   2. Pure batch analytics with periodic comprehensive analysis  \n>   3. Hybrid approach with real-time alerting and batch analysis\n> - **Decision**: Hybrid approach with real-time alerting and batch analysis\n> - **Rationale**: Critical serving issues need immediate detection (latency spikes, error rates), while statistical drift analysis requires larger sample sizes and complex computations better suited for batch processing\n> - **Consequences**: Increases system complexity but provides both operational responsiveness and analytical depth. Requires careful data flow coordination between streaming and batch systems\n\n| Analytics Type | Latency | Data Volume | Algorithms | Use Cases |\n|---------------|---------|-------------|------------|-----------|\n| Real-Time Streaming | < 1 minute | Individual predictions | Simple thresholds, sliding windows | Error rate alerts, latency spikes |\n| Batch Analytics | 15 minutes - 24 hours | Aggregated datasets | Statistical tests, ML algorithms | Drift detection, performance trends |\n| Interactive Queries | < 10 seconds | Sampled datasets | Aggregations, filters | Dashboard updates, ad-hoc analysis |\n\n⚠️ **Pitfall: Overlogging Prediction Details**\n\nA common mistake is logging every field of every prediction request, leading to explosive storage growth and privacy concerns. Instead, implement **selective logging policies** based on model criticality, traffic volume, and regulatory requirements. High-traffic models might log only 1% of predictions with full details, while critical models in regulated industries log everything with strong access controls.\n\n⚠️ **Pitfall: Static Drift Thresholds**\n\nSetting fixed drift detection thresholds often leads to false alarms during expected seasonal patterns or insufficient sensitivity during gradual degradation. Implement **adaptive baselines** that adjust to normal variance patterns and **time-aware comparisons** that account for cyclical data patterns.\n\n### Implementation Guidance\n\nThis implementation guidance provides practical code structures and technology recommendations for building the data layer of your MLOps platform.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Metadata Storage | PostgreSQL with SQLAlchemy | MongoDB with change streams |\n| Artifact Storage | Local filesystem with S3 API | AWS S3 with CloudFront CDN |\n| Time-Series Metrics | InfluxDB | Prometheus + Grafana |\n| Search and Analytics | Elasticsearch | Apache Druid with Superset |\n| Event Streaming | Redis Pub/Sub | Apache Kafka with Schema Registry |\n| Data Validation | Cerberus schema validation | Great Expectations with profiling |\n\n#### Recommended File Structure\n\n```\nmlops_platform/\n  core/\n    entities/\n      __init__.py                    ← Export all entity classes\n      base.py                        ← Base entity with common fields\n      experiment.py                  ← Experiment tracking entities\n      model.py                       ← Model registry entities\n      pipeline.py                    ← Pipeline orchestration entities\n      deployment.py                  ← Deployment and monitoring entities\n    storage/\n      __init__.py                    ← Storage interface definitions\n      metadata_store.py              ← MetadataStore implementation\n      artifact_store.py              ← ArtifactStore implementation\n      time_series_store.py           ← Metrics and monitoring data\n    validation/\n      __init__.py                    ← Schema validation utilities\n      schemas/                       ← JSON schemas for entity validation\n        experiment_schema.json\n        model_schema.json\n        pipeline_schema.json\n        deployment_schema.json\n  migrations/                        ← Database schema migrations\n  tests/\n    unit/\n      test_entities.py               ← Entity model tests\n      test_storage.py                ← Storage layer tests\n    integration/\n      test_end_to_end.py             ← Full workflow tests\n```\n\n#### Base Entity Infrastructure\n\n```python\n\"\"\"\nBase entity infrastructure providing common functionality for all MLOps entities.\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional, List\nimport uuid\nimport json\nfrom dataclasses import dataclass, field\nfrom enum import Enum\n\nclass HealthStatus(Enum):\n    \"\"\"Health status enumeration for system components.\"\"\"\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\" \n    UNHEALTHY = \"unhealthy\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass Event:\n    \"\"\"System event for cross-component coordination.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    type: str = \"\"\n    source: str = \"\"\n    timestamp: float = field(default_factory=lambda: datetime.now().timestamp())\n    payload: Dict[str, Any] = field(default_factory=dict)\n    \n    @classmethod\n    def create(cls, event_type: str, source: str, payload: Dict[str, Any]) -> 'Event':\n        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"\n        return cls(\n            type=event_type,\n            source=source, \n            payload=payload\n        )\n\n@dataclass\nclass HealthCheck:\n    \"\"\"Health check result for monitoring component status.\"\"\"\n    name: str\n    status: HealthStatus\n    message: str\n    timestamp: float = field(default_factory=lambda: datetime.now().timestamp())\n    details: Dict[str, Any] = field(default_factory=dict)\n\nclass MetadataStore(ABC):\n    \"\"\"Abstract interface for entity metadata storage.\"\"\"\n    \n    @abstractmethod\n    def create_table(self, table_name: str, schema: Dict[str, Any]) -> None:\n        \"\"\"Create table with specified schema.\"\"\"\n        pass\n    \n    @abstractmethod\n    def insert(self, table_name: str, data: Dict[str, Any]) -> str:\n        \"\"\"Insert data and return generated ID.\"\"\"\n        pass\n    \n    @abstractmethod\n    def update(self, table_name: str, entity_id: str, data: Dict[str, Any]) -> None:\n        \"\"\"Update existing entity.\"\"\"\n        pass\n    \n    @abstractmethod\n    def query(self, table_name: str, filters: Dict[str, Any], \n              limit: Optional[int] = None) -> List[Dict[str, Any]]:\n        \"\"\"Query entities with filters.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_by_id(self, table_name: str, entity_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get single entity by ID.\"\"\"\n        pass\n\nclass ArtifactStore(ABC):\n    \"\"\"Abstract interface for artifact storage.\"\"\"\n    \n    @abstractmethod\n    def put(self, key: str, data: bytes, metadata: Optional[Dict[str, str]] = None) -> str:\n        \"\"\"Store binary data with optional metadata.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get(self, key: str) -> bytes:\n        \"\"\"Retrieve binary data by key.\"\"\"\n        pass\n    \n    @abstractmethod\n    def delete(self, key: str) -> None:\n        \"\"\"Delete artifact by key.\"\"\"\n        pass\n    \n    @abstractmethod\n    def list_keys(self, prefix: str) -> List[str]:\n        \"\"\"List all keys with given prefix.\"\"\"\n        pass\n\nclass ComponentHealth:\n    \"\"\"Manages health checks for a component.\"\"\"\n    \n    def __init__(self):\n        self._checks: Dict[str, callable] = {}\n    \n    def add_check(self, check_name: str, check_func: callable) -> None:\n        \"\"\"Register periodic health check function.\"\"\"\n        self._checks[check_name] = check_func\n    \n    def run_checks(self) -> List[HealthCheck]:\n        \"\"\"Execute all health checks and return results.\"\"\"\n        results = []\n        for name, check_func in self._checks.items():\n            try:\n                # TODO: Execute check function with timeout\n                # TODO: Parse result into HealthCheck object\n                # TODO: Handle check function exceptions\n                # TODO: Add performance timing to details\n                pass\n            except Exception as e:\n                # TODO: Create failed HealthCheck with error details\n                pass\n        return results\n\nclass EventCoordinator:\n    \"\"\"Coordinates event publishing and subscription between components.\"\"\"\n    \n    def __init__(self):\n        self._subscribers: Dict[str, List[callable]] = {}\n    \n    def publish(self, event: Event, synchronous: bool = False) -> None:\n        \"\"\"Publish event to subscribers.\"\"\"\n        # TODO: Validate event object\n        # TODO: Find all subscribers for event.type  \n        # TODO: If synchronous, call handlers directly\n        # TODO: If asynchronous, queue event for background processing\n        # TODO: Handle handler exceptions gracefully\n        # TODO: Log event publishing for audit trail\n        pass\n    \n    def subscribe(self, event_type: str, handler: callable) -> None:\n        \"\"\"Register event handler for specific event type.\"\"\"\n        # TODO: Validate handler is callable\n        # TODO: Add handler to subscribers list for event_type\n        # TODO: Support handler deregistration\n        # TODO: Validate handler signature matches Event parameter\n        pass\n\n# Event type constants for cross-component coordination\nEXPERIMENT_COMPLETED = \"experiment.completed\"\nMODEL_PROMOTED = \"model.promoted\"  \nDEPLOYMENT_FAILED = \"deployment.failed\"\n```\n\n#### Experiment Tracking Entities\n\n```python\n\"\"\"\nEntity definitions for experiment tracking component.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any, Optional, List\nfrom enum import Enum\nimport uuid\nfrom datetime import datetime\n\nclass ExperimentLifecycleStage(Enum):\n    \"\"\"Experiment lifecycle stages.\"\"\"\n    ACTIVE = \"active\"\n    DELETED = \"deleted\" \n    ARCHIVED = \"archived\"\n\nclass RunStatus(Enum):\n    \"\"\"Training run execution status.\"\"\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    KILLED = \"killed\"\n\n@dataclass\nclass Experiment:\n    \"\"\"Represents a group of related ML training runs.\"\"\"\n    experiment_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    name: str = \"\"\n    description: str = \"\"\n    tags: Dict[str, str] = field(default_factory=dict)\n    creator_user_id: str = \"\"\n    created_at: float = field(default_factory=lambda: datetime.now().timestamp())\n    updated_at: float = field(default_factory=lambda: datetime.now().timestamp())\n    run_count: int = 0\n    lifecycle_stage: ExperimentLifecycleStage = ExperimentLifecycleStage.ACTIVE\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert experiment to dictionary for storage.\"\"\"\n        # TODO: Serialize all fields to JSON-compatible dictionary\n        # TODO: Handle enum conversion to string values\n        # TODO: Validate required fields are present\n        # TODO: Apply field length limits (name max 255 chars, etc.)\n        pass\n\n@dataclass  \nclass Run:\n    \"\"\"Represents a single ML training execution.\"\"\"\n    run_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    experiment_id: str = \"\"\n    run_name: str = \"\"\n    status: RunStatus = RunStatus.RUNNING\n    start_time: float = field(default_factory=lambda: datetime.now().timestamp())\n    end_time: Optional[float] = None\n    source_type: str = \"\"  # NOTEBOOK, SCRIPT, PIPELINE\n    source_name: str = \"\"\n    source_version: str = \"\"\n    user_id: str = \"\"\n    tags: Dict[str, str] = field(default_factory=dict)\n    lifecycle_stage: ExperimentLifecycleStage = ExperimentLifecycleStage.ACTIVE\n    \n    def complete(self, final_status: RunStatus) -> None:\n        \"\"\"Mark run as completed with final status.\"\"\"\n        # TODO: Set end_time to current timestamp\n        # TODO: Update status to final_status\n        # TODO: Validate final_status is terminal (COMPLETED, FAILED, KILLED)\n        # TODO: Publish EXPERIMENT_COMPLETED event\n        pass\n\n@dataclass\nclass Parameter:\n    \"\"\"Represents a training hyperparameter.\"\"\"\n    parameter_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    run_id: str = \"\"\n    key: str = \"\"\n    value: str = \"\"\n    value_type: str = \"\"  # INT, FLOAT, STRING, BOOL, JSON\n    created_at: float = field(default_factory=lambda: datetime.now().timestamp())\n\n@dataclass\nclass Metric:\n    \"\"\"Represents a training metric measurement.\"\"\" \n    metric_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    run_id: str = \"\"\n    key: str = \"\"\n    value: float = 0.0\n    step: Optional[int] = None\n    timestamp: float = field(default_factory=lambda: datetime.now().timestamp())\n    created_at: float = field(default_factory=lambda: datetime.now().timestamp())\n\n@dataclass\nclass Artifact:\n    \"\"\"Represents a file or object generated during training.\"\"\"\n    artifact_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    run_id: str = \"\"\n    path: str = \"\"  # Logical path within run\n    artifact_uri: str = \"\"  # Physical storage location\n    file_size: Optional[int] = None\n    checksum: Optional[str] = None\n    artifact_type: str = \"\"  # MODEL, DATASET, PLOT, CONFIG, LOG\n    mime_type: Optional[str] = None\n    created_at: float = field(default_factory=lambda: datetime.now().timestamp())\n\nclass ExperimentTracker:\n    \"\"\"Main interface for experiment tracking operations.\"\"\"\n    \n    def __init__(self, metadata_store: MetadataStore, artifact_store: ArtifactStore):\n        self.metadata_store = metadata_store\n        self.artifact_store = artifact_store\n    \n    def create_experiment(self, name: str, description: str = \"\", \n                         tags: Dict[str, str] = None) -> Experiment:\n        \"\"\"Create a new experiment.\"\"\"\n        # TODO: Validate experiment name is unique\n        # TODO: Create Experiment object with provided parameters  \n        # TODO: Store experiment in metadata_store\n        # TODO: Return created experiment object\n        pass\n    \n    def start_run(self, experiment_id: str, run_name: str = \"\", \n                  tags: Dict[str, str] = None) -> Run:\n        \"\"\"Start a new training run.\"\"\"\n        # TODO: Validate experiment_id exists\n        # TODO: Create Run object with RUNNING status\n        # TODO: Store run in metadata_store\n        # TODO: Increment experiment run_count\n        # TODO: Return created run object\n        pass\n    \n    def log_parameter(self, run_id: str, key: str, value: Any) -> None:\n        \"\"\"Log a hyperparameter for a run.\"\"\"\n        # TODO: Validate run_id exists and is active\n        # TODO: Convert value to string representation\n        # TODO: Detect value_type (int, float, string, etc.)\n        # TODO: Create Parameter object and store\n        # TODO: Support hierarchical keys with dot notation\n        pass\n```\n\n#### Model Registry Entities\n\n```python\n\"\"\"\nEntity definitions for model registry component.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any, Optional, List\nfrom enum import Enum\nimport uuid\nfrom datetime import datetime\n\nclass ModelStage(Enum):\n    \"\"\"Model version lifecycle stages.\"\"\"\n    DEVELOPMENT = \"development\"\n    STAGING = \"staging\" \n    PRODUCTION = \"production\"\n    ARCHIVED = \"archived\"\n\nclass ModelStatus(Enum):\n    \"\"\"Model version processing status.\"\"\"\n    CREATING = \"creating\"\n    READY = \"ready\"\n    FAILED = \"failed\"\n\n@dataclass\nclass Model:\n    \"\"\"Represents a named family of model versions.\"\"\"\n    model_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    name: str = \"\"\n    description: str = \"\"\n    tags: Dict[str, str] = field(default_factory=dict)\n    creation_source: str = \"\"  # EXPERIMENT, IMPORT, PIPELINE\n    creator_user_id: str = \"\"\n    created_at: float = field(default_factory=lambda: datetime.now().timestamp())\n    updated_at: float = field(default_factory=lambda: datetime.now().timestamp())\n    latest_version: str = \"\"\n    current_stage_version: Dict[str, str] = field(default_factory=dict)\n\n@dataclass\nclass ModelVersion:\n    \"\"\"Represents a specific iteration of a model.\"\"\"\n    version_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    model_id: str = \"\"\n    version: str = \"\"  # Semantic version\n    stage: ModelStage = ModelStage.DEVELOPMENT\n    status: ModelStatus = ModelStatus.CREATING\n    source_run_id: Optional[str] = None\n    model_uri: str = \"\"\n    model_format: str = \"\"  # PICKLE, ONNX, TENSORFLOW, PYTORCH\n    model_signature: Optional[Dict] = None\n    model_metrics: Dict[str, float] = field(default_factory=dict)\n    description: str = \"\"\n    tags: Dict[str, str] = field(default_factory=dict)\n    created_at: float = field(default_factory=lambda: datetime.now().timestamp())\n    updated_at: float = field(default_factory=lambda: datetime.now().timestamp())\n    creator_user_id: str = \"\"\n    \n    # Lineage tracking fields\n    data_version_hash: Optional[str] = None\n    code_commit_hash: Optional[str] = None\n    training_pipeline_id: Optional[str] = None\n    parent_model_ids: List[str] = field(default_factory=list)\n    derived_model_ids: List[str] = field(default_factory=list)\n\nclass ModelRegistry:\n    \"\"\"Main interface for model registry operations.\"\"\"\n    \n    def __init__(self, metadata_store: MetadataStore, artifact_store: ArtifactStore):\n        self.metadata_store = metadata_store\n        self.artifact_store = artifact_store\n    \n    def register_model(self, name: str, description: str = \"\") -> Model:\n        \"\"\"Register a new model family.\"\"\"\n        # TODO: Validate model name is unique\n        # TODO: Create Model object\n        # TODO: Store in metadata_store\n        # TODO: Return created model\n        pass\n    \n    def create_model_version(self, model_id: str, version: str, \n                           model_uri: str, source_run_id: str = None) -> ModelVersion:\n        \"\"\"Create a new model version.\"\"\"\n        # TODO: Validate model_id exists\n        # TODO: Validate version follows semantic versioning\n        # TODO: Validate model_uri points to valid artifact\n        # TODO: Create ModelVersion object\n        # TODO: Update parent model's latest_version\n        # TODO: Store version in metadata_store\n        # TODO: Publish MODEL_PROMOTED event if appropriate\n        pass\n    \n    def transition_stage(self, model_id: str, version: str, \n                        new_stage: ModelStage) -> None:\n        \"\"\"Transition model version to new stage.\"\"\"\n        # TODO: Validate version exists\n        # TODO: Check stage transition rules (dev->staging->prod)\n        # TODO: Update version stage\n        # TODO: Update model's current_stage_version mapping\n        # TODO: Publish MODEL_PROMOTED event\n        pass\n```\n\n#### Milestone Checkpoints\n\nAfter implementing the data model foundations, verify the following behavior:\n\n**Experiment Tracking Verification:**\n```bash\npython -c \"\nfrom core.entities.experiment import ExperimentTracker\ntracker = ExperimentTracker(metadata_store, artifact_store)\nexp = tracker.create_experiment('test-cnn', 'CNN experiments')\nrun = tracker.start_run(exp.experiment_id, 'baseline-run')\ntracker.log_parameter(run.run_id, 'learning_rate', 0.001)\nprint(f'Created run {run.run_id} in experiment {exp.name}')\n\"\n```\n\n**Model Registry Verification:**\n```bash\npython -c \"\nfrom core.entities.model import ModelRegistry\nregistry = ModelRegistry(metadata_store, artifact_store)\nmodel = registry.register_model('image-classifier', 'CNN for image classification')\nversion = registry.create_model_version(model.model_id, '1.0.0', 's3://models/v1.pkl')\nprint(f'Registered model {model.name} version {version.version}')\n\"\n```\n\n**Expected Database Tables:**\n- experiments: experiment_id, name, description, tags, creator_user_id, created_at\n- runs: run_id, experiment_id, status, start_time, end_time, tags\n- parameters: parameter_id, run_id, key, value, value_type\n- metrics: metric_id, run_id, key, value, step, timestamp\n- artifacts: artifact_id, run_id, path, artifact_uri, file_size\n- models: model_id, name, description, creation_source, created_at\n- model_versions: version_id, model_id, version, stage, model_uri, created_at\n\n**Common Issues and Debugging:**\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|-------------|------------------|-----|\n| UUID generation fails | Missing uuid import | Check import statements | Add `import uuid` |\n| Timestamp errors | Timezone issues | Check datetime.now() usage | Use UTC timestamps consistently |\n| Foreign key violations | Missing parent entities | Verify experiment exists before creating runs | Add existence validation |\n| JSON serialization fails | Non-serializable fields | Check Enum and datetime fields | Implement custom serializers |\n| Storage connection errors | Missing store initialization | Check MetadataStore setup | Initialize stores in application startup |\n\n\n## Experiment Tracking Component\n\n> **Milestone(s):** This section primarily corresponds to Milestone 1 (Experiment Tracking), which focuses on tracking experiments, parameters, metrics, and artifacts, while also establishing foundations used throughout Milestones 2-5 for lineage tracking and reproducibility.\n\n### Mental Model: Research Laboratory\n\nThink of experiment tracking as transforming a chaotic research laboratory into a meticulously organized scientific facility. In traditional ML development, data scientists run experiments like researchers working in isolation—they might scribble notes on napkins, save models with cryptic names like \"model_final_v3_ACTUALLY_FINAL.pkl\", and forget which hyperparameters produced their best results. This creates a digital equivalent of a messy lab where critical discoveries get lost, experiments can't be reproduced, and knowledge walks out the door with departing team members.\n\nThe experiment tracking component functions as a **digital laboratory notebook** combined with a **specimen archive**. Just as a proper research lab maintains detailed records of every experiment—the hypothesis, methodology, observations, and results—our experiment tracking system captures every detail of ML training runs. The laboratory notebook records the \"what\" and \"why\" (parameters and metadata), while the time-series observation log captures the \"how it unfolded\" (metrics over time), and the specimen archive preserves the \"what was produced\" (artifacts and models).\n\nThis mental model reveals why experiment tracking requires three distinct but connected storage systems: a **metadata store** for searchable experiment records (like a card catalog), a **time-series store** for metric evolution (like a monitoring chart), and an **artifact store** for binary outputs (like a specimen freezer). Each serves a different query pattern but must maintain referential integrity to preserve the complete experimental narrative.\n\nThe hierarchical organization mirrors how research labs group related studies. **Experiments** represent research programs (like \"customer churn prediction\" or \"image classification v2\"), while individual **runs** represent specific trials within that program. This hierarchy enables both focused analysis (\"which learning rate worked best in this experiment?\") and broad comparisons (\"how do our image models compare to our NLP models?\").\n\n![Experiment to Deployment Flow](./diagrams/experiment-flow.svg)\n\n### Logging APIs and Storage\n\nThe experiment tracking component exposes three primary logging interfaces that capture different aspects of ML training runs. These APIs must handle the diverse data types generated during ML experiments while providing consistent correlation mechanisms that link related information across storage systems.\n\n#### Parameter Logging Interface\n\nParameter logging captures the **configuration space** of ML experiments—the hyperparameters, data preprocessing settings, and model architecture choices that define how training was conducted. Unlike metrics that change during training, parameters represent static configuration that remains constant throughout a run.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `log_param` | run_id: str, key: str, value: Any | None | Log a single parameter key-value pair for the specified run |\n| `log_params` | run_id: str, params: Dict[str, Any] | None | Log multiple parameters in a single atomic operation |\n| `get_run_params` | run_id: str | Dict[str, Any] | Retrieve all parameters logged for a specific run |\n| `update_param` | run_id: str, key: str, value: Any | None | Update an existing parameter value (creates if not exists) |\n\nParameter storage must handle **nested configurations** common in modern ML frameworks. Training configurations often contain hierarchical structures like optimizer settings, data augmentation pipelines, and model architecture definitions. The storage layer flattens these hierarchies using dot notation (e.g., `optimizer.learning_rate`, `model.layers.0.units`) while maintaining the ability to reconstruct the original structure for display and comparison.\n\nThe parameter store implements **type-aware serialization** to preserve data types during storage and retrieval. String values, numeric types, boolean flags, and complex objects like lists require different handling to maintain semantic meaning. For example, a learning rate of 0.001 should remain a float, not convert to a string representation that breaks numerical comparisons.\n\n#### Metric Logging Interface\n\nMetric logging captures the **training dynamics** of ML experiments—how loss decreases, accuracy improves, and validation metrics evolve throughout the training process. This creates time-series data that reveals training behavior patterns and convergence characteristics.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `log_metric` | run_id: str, key: str, value: float, step: int, timestamp: float | None | Log a single metric value at a specific training step |\n| `log_metrics` | run_id: str, metrics: Dict[str, float], step: int, timestamp: float | None | Log multiple metrics for the same training step atomically |\n| `get_metric_history` | run_id: str, metric_key: str | List[MetricPoint] | Retrieve the complete time series for a specific metric |\n| `get_run_metrics` | run_id: str, step: int | Dict[str, float] | Get all metrics logged at a specific training step |\n\nEach metric point contains four essential components that enable both temporal analysis and cross-run comparison:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| run_id | str | Links the metric to the specific experiment run |\n| key | str | Metric name (e.g., \"train_loss\", \"val_accuracy\", \"learning_rate\") |\n| value | float | Numeric value of the metric at this point in training |\n| step | int | Training step number (epoch, batch, or iteration count) |\n| timestamp | float | Unix timestamp when the metric was recorded |\n\nThe metric storage system must handle **high-frequency logging** efficiently. Modern training runs can generate thousands of metric points per run, especially when logging at batch-level granularity. The storage layer uses **batch insertion** and **time-based partitioning** to maintain write performance while supporting both real-time monitoring and historical analysis queries.\n\n#### Artifact Storage Interface\n\nArtifact storage manages the **physical outputs** of ML experiments—trained models, evaluation plots, datasets, configuration files, and any other files generated during training. Unlike parameters and metrics, artifacts are binary objects that require object storage rather than relational databases.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `log_artifact` | run_id: str, local_path: str, artifact_path: str, metadata: Dict | str | Upload a local file as an experiment artifact |\n| `log_artifacts` | run_id: str, local_dir: str, artifact_path: str | List[str] | Upload entire directory structure as experiment artifacts |\n| `download_artifact` | run_id: str, artifact_path: str, local_path: str | None | Download an artifact to local filesystem |\n| `list_artifacts` | run_id: str, path: str | List[ArtifactInfo] | List all artifacts under a specific path for a run |\n| `get_artifact_uri` | run_id: str, artifact_path: str | str | Get downloadable URI for a specific artifact |\n\nEach artifact maintains metadata that enables discovery, validation, and efficient storage management:\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| run_id | str | Links the artifact to the specific experiment run |\n| path | str | Hierarchical path within the run's artifact space |\n| size_bytes | int | File size for storage cost tracking and transfer optimization |\n| checksum | str | SHA-256 hash for corruption detection and deduplication |\n| mime_type | str | Content type for proper handling and display |\n| created_at | float | Upload timestamp for version tracking |\n| metadata | Dict[str, str] | Custom key-value pairs for artifact classification |\n\nThe artifact store implements **content-addressed storage** using file checksums to eliminate duplicate storage of identical files across runs. When multiple experiments save the same dataset or model checkpoint, only one physical copy exists, dramatically reducing storage costs while maintaining logical separation between experiments.\n\n#### Storage Architecture Decisions\n\n> **Decision: Polyglot Persistence for Experiment Data**\n> - **Context**: Experiment tracking requires storing three distinct data types (parameters, metrics, artifacts) with different access patterns, performance requirements, and scalability characteristics.\n> - **Options Considered**: Single database for all data, separate specialized stores, hybrid approach\n> - **Decision**: Use specialized storage systems optimized for each data type\n> - **Rationale**: Parameters need flexible schema and complex queries (document store), metrics need time-series aggregation and fast writes (time-series DB), artifacts need blob storage with CDN capabilities (object store)\n> - **Consequences**: Enables optimal performance for each workload but requires cross-store consistency mechanisms and more complex deployment\n\n| Storage Type | Parameter Store | Metric Store | Artifact Store |\n|-------------|-----------------|--------------|----------------|\n| **Technology** | PostgreSQL with JSONB | InfluxDB or TimescaleDB | S3-compatible object storage |\n| **Strengths** | Complex queries, ACID guarantees, flexible schema | High write throughput, time-series aggregation, automatic retention | Unlimited scalability, CDN integration, cost-effective |\n| **Query Patterns** | Search, filter, compare across runs | Time-range queries, aggregation, downsampling | Get/put by key, list with prefixes |\n| **Consistency Model** | Strong consistency for metadata | Eventual consistency acceptable | Eventual consistency with versioning |\n\n> **Decision: Correlation ID Strategy**\n> - **Context**: Related data (parameters, metrics, artifacts) for a single experiment run must be reliably associated across multiple storage systems\n> - **Options Considered**: Run UUID as primary key, composite keys, foreign key relationships\n> - **Decision**: Use globally unique run ID (UUID) as correlation key across all storage systems\n> - **Rationale**: Simplifies cross-store queries, enables independent scaling of storage systems, provides clear data ownership boundaries\n> - **Consequences**: Requires careful run ID generation and validation, but eliminates complex join operations across heterogeneous stores\n\n### Querying and Comparison\n\nThe experiment tracking component must transform raw experimental data into actionable insights through sophisticated querying and comparison capabilities. Data scientists need to answer questions like \"which runs achieved accuracy above 0.95?\", \"how do learning rates affect convergence speed?\", and \"what changed between my best and worst performing experiments?\"\n\n#### Run Search and Filtering\n\nThe search interface provides **multi-dimensional filtering** across the parameter space, metric outcomes, and execution metadata. This enables data scientists to slice their experimental data along any combination of dimensions to identify patterns and outliers.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `search_runs` | experiment_id: str, filter_string: str, order_by: List[str], max_results: int | List[Run] | Search runs with SQL-like filter expressions |\n| `get_experiments` | view_type: str, max_results: int | List[Experiment] | List experiments with optional filtering by lifecycle stage |\n| `get_run` | run_id: str | Run | Retrieve complete run information including params, metrics, and metadata |\n\nThe filter string supports a **domain-specific query language** that combines SQL-like syntax with ML-specific operators:\n\n```\nmetrics.val_accuracy > 0.9 AND params.learning_rate <= 0.01 AND tags.model_type = 'transformer'\n```\n\nCommon query patterns include:\n\n| Query Pattern | Example Filter | Use Case |\n|---------------|----------------|----------|\n| **Metric Thresholds** | `metrics.accuracy >= 0.95` | Find high-performing runs |\n| **Parameter Ranges** | `params.learning_rate BETWEEN 0.001 AND 0.1` | Analyze hyperparameter sensitivity |\n| **Combination Filters** | `metrics.val_loss < 0.1 AND params.batch_size > 32` | Multi-criteria optimization |\n| **Tag-based Grouping** | `tags.experiment_type = 'baseline'` | Categorize experimental variants |\n| **Execution Metadata** | `status = 'FINISHED' AND duration_ms < 3600000` | Find fast, successful runs |\n\n#### Metric Comparison and Visualization\n\nThe comparison engine aggregates and analyzes metric time-series data to reveal training patterns and performance differences across runs. This goes beyond simple final metric values to examine **convergence behavior**, **training stability**, and **efficiency characteristics**.\n\n| Method Name | Parameters | Returns | Description |\n|-------------|------------|---------|-------------|\n| `compare_runs` | run_ids: List[str], metric_keys: List[str] | ComparisonResult | Generate statistical comparison across specified runs and metrics |\n| `get_metric_summary` | run_id: str, metric_key: str | MetricSummary | Compute aggregation statistics for a metric time-series |\n| `get_parallel_coordinates` | run_ids: List[str], param_keys: List[str], metric_keys: List[str] | ParallelCoordinatesData | Generate parallel coordinates plot data for parameter-outcome analysis |\n\nThe comparison system computes multiple statistical measures that capture different aspects of experimental performance:\n\n| Comparison Metric | Calculation | Insight Provided |\n|------------------|-------------|------------------|\n| **Final Value** | Last recorded metric value | Ultimate performance achieved |\n| **Best Value** | Maximum (for accuracy) or minimum (for loss) | Peak performance during training |\n| **Convergence Step** | Step where improvement stopped | Training efficiency and stability |\n| **Area Under Curve** | Integral of metric over training steps | Overall training quality |\n| **Stability Score** | Inverse of metric variance in final 10% of training | Model reliability and robustness |\n\n#### Parameter-Outcome Correlation Analysis\n\nUnderstanding how hyperparameters influence experimental outcomes requires sophisticated statistical analysis that goes beyond simple correlation coefficients. The tracking system implements **multidimensional analysis** that can identify complex parameter interactions and sensitivity patterns.\n\n| Analysis Type | Method | Output | Use Case |\n|---------------|---------|--------|----------|\n| **Sensitivity Analysis** | Partial correlation with other params held constant | Ranking of parameter importance | Hyperparameter prioritization |\n| **Interaction Detection** | Two-way ANOVA across parameter combinations | Significant parameter interactions | Complex optimization strategies |\n| **Pareto Frontier** | Multi-objective optimization analysis | Non-dominated parameter combinations | Trade-off analysis |\n| **Clustering Analysis** | K-means on parameter vectors weighted by outcomes | Groups of similar high-performing configurations | Configuration templates |\n\n#### Query Optimization Strategies\n\nExperiment tracking queries often involve complex joins across parameter, metric, and metadata stores, potentially scanning thousands of runs with millions of metric points. The system implements several optimization strategies to maintain sub-second response times even with large experimental datasets.\n\n> **Decision: Materialized Views for Common Queries**\n> - **Context**: Frequent queries like \"best run per experiment\" and \"parameter distribution analysis\" involve expensive aggregations across large datasets\n> - **Options Considered**: Real-time aggregation, pre-computed materialized views, cached query results\n> - **Decision**: Implement materialized views updated through event-driven triggers\n> - **Rationale**: Provides consistent sub-second response for common queries while maintaining data freshness through incremental updates\n> - **Consequences**: Increases storage requirements but dramatically improves user experience for exploratory data analysis\n\nThe query optimization layer implements several key strategies:\n\n**Indexed Parameter Search**: Parameters are indexed using **GIN indexes** on JSONB columns in PostgreSQL, enabling fast lookups even for nested parameter structures. The system maintains separate indexes for numeric and string parameters to optimize different query patterns.\n\n**Metric Aggregation Caching**: Frequently accessed metric aggregations (final values, maximums, convergence points) are pre-computed and cached in Redis with **time-based invalidation**. This transforms expensive time-series scans into simple key-value lookups.\n\n**Query Result Pagination**: Large result sets use **cursor-based pagination** to avoid memory exhaustion and provide consistent performance regardless of result set size. Each query returns a continuation token that enables stateless pagination.\n\n**Federated Query Planning**: Queries spanning multiple storage systems use a **cost-based query planner** that determines optimal execution strategies based on estimated data volumes and network costs.\n\n### Architecture Decisions\n\nThe experiment tracking component embodies several key architectural decisions that influence both its internal design and its integration with other MLOps platform components. These decisions reflect trade-offs between consistency, performance, scalability, and operational complexity.\n\n> **Decision: Event-Driven Architecture for Component Integration**\n> - **Context**: Experiment tracking must notify other components (model registry, pipeline orchestration) about experiment completion, model artifact availability, and performance milestones\n> - **Options Considered**: Synchronous API calls, database polling, event-driven messaging\n> - **Decision**: Implement asynchronous event publishing using the `EventCoordinator` pattern\n> - **Rationale**: Decouples components, enables independent scaling, supports complex workflows without tight coupling, provides audit trail of system interactions\n> - **Consequences**: Requires eventual consistency handling but enables more resilient and scalable system architecture\n\nThe event integration follows a structured pattern where significant experiment tracking events trigger notifications to interested components:\n\n| Event Type | Trigger Condition | Event Payload | Consuming Components |\n|------------|------------------|---------------|---------------------|\n| `EXPERIMENT_COMPLETED` | Run transitions to FINISHED status | run_id, experiment_id, final_metrics, artifact_paths | Model Registry, Pipeline Orchestration |\n| `run.started` | New run begins logging | run_id, experiment_id, parameters, start_time | Monitoring dashboards, Resource management |\n| `artifact.logged` | New artifact uploaded | run_id, artifact_path, artifact_type, checksum | Model Registry, Lineage tracking |\n| `metric.threshold_exceeded` | Metric crosses configured threshold | run_id, metric_name, threshold_value, current_value | Auto-promotion workflows, Alert systems |\n\n> **Decision: Immutable Event Log for Audit Trail**\n> - **Context**: MLOps platforms require complete audit trails for compliance, debugging, and reproducibility, especially in regulated industries\n> - **Options Considered**: Mutable records with timestamps, immutable append-only log, hybrid approach with versioning\n> - **Decision**: Implement append-only event log where all parameter/metric updates create new timestamped entries rather than modifying existing records\n> - **Rationale**: Provides complete audit trail, enables time-travel queries, supports compliance requirements, simplifies concurrent access patterns\n> - **Consequences**: Increases storage requirements but provides invaluable debugging and compliance capabilities\n\n> **Decision: Hierarchical Artifact Namespacing**\n> - **Context**: ML experiments generate diverse artifacts (models, plots, datasets, configs) that need logical organization and efficient access patterns\n> - **Options Considered**: Flat key-value storage, hierarchical paths with metadata, database-driven catalog\n> - **Decision**: Implement hierarchical path-based namespacing with conventional subdirectories\n> - **Rationale**: Mirrors familiar filesystem semantics, enables efficient prefix-based queries, supports nested organization patterns, integrates naturally with object storage systems\n> - **Consequences**: Provides intuitive organization but requires careful path validation and character encoding handling\n\nThe artifact namespace follows conventional ML workflow patterns:\n\n```\n/{run_id}/\n  ├── models/\n  │   ├── checkpoints/\n  │   │   ├── epoch_001.pt\n  │   │   └── epoch_010.pt\n  │   └── final_model.pkl\n  ├── data/\n  │   ├── train_features.parquet\n  │   └── validation_results.csv\n  ├── plots/\n  │   ├── learning_curves.png\n  │   └── confusion_matrix.png\n  └── configs/\n      ├── model_config.yaml\n      └── training_params.json\n```\n\n> **Decision: Pluggable Storage Backend Interface**\n> - **Context**: Different organizations have varying requirements for storage technology, compliance, cost optimization, and existing infrastructure integration\n> - **Options Considered**: Fixed storage implementation, configuration-driven backends, pluggable interface with adapters\n> - **Decision**: Define abstract storage interfaces (`MetadataStore`, `ArtifactStore`) with pluggable implementations\n> - **Rationale**: Enables deployment flexibility, supports cloud-agnostic deployments, allows optimization for specific workloads, facilitates testing with mock implementations\n> - **Consequences**: Requires careful interface design and adapter maintenance but provides crucial deployment flexibility\n\nThe storage abstraction defines minimal interfaces that capture essential operations while allowing implementation-specific optimizations:\n\n| Interface | Core Methods | Implementation Examples |\n|-----------|-------------|------------------------|\n| `MetadataStore` | create_table, insert, update, query, get_by_id | PostgreSQL, MongoDB, DynamoDB |\n| `ArtifactStore` | put, get, delete, list_keys | S3, Azure Blob, Google Cloud Storage, MinIO |\n| `EventStore` | append, read_stream, subscribe | Kafka, Redis Streams, AWS Kinesis |\n\n### Common Pitfalls\n\nExperiment tracking systems appear deceptively simple but contain numerous subtle complexities that frequently trip up implementers. Understanding these pitfalls helps avoid common mistakes that can compromise data integrity, performance, or usability.\n\n⚠️ **Pitfall: Inconsistent Metric Naming Across Experiments**\n\nData scientists often use slight variations in metric names (\"accuracy\", \"val_acc\", \"validation_accuracy\") that fragment the metric namespace and break cross-experiment comparisons. This happens because metric names are typically generated programmatically by training scripts without central validation.\n\nThe tracking system should implement **metric name normalization** and **alias resolution** to handle common variations. Maintain a registry of canonical metric names with known aliases, and warn users when logging metrics with new names that are similar to existing ones.\n\n⚠️ **Pitfall: Logging High-Frequency Metrics Without Batching**\n\nIndividual metric logging calls for each training step create enormous overhead when training models with thousands of iterations. This can slow training significantly and overwhelm the storage backend with small write operations.\n\nImplement **automatic batching** that accumulates metrics in memory and flushes to storage periodically or when batch size thresholds are reached. Provide explicit `flush()` methods for users who need immediate persistence at specific training milestones.\n\n⚠️ **Pitfall: Storing Large Artifacts Directly in Metadata Database**\n\nStoring model binaries or large datasets as BLOBs in relational databases causes severe performance degradation, backup failures, and storage cost explosions. This often happens when implementers take the \"store everything together\" approach for simplicity.\n\nAlways use object storage for artifacts larger than a few kilobytes. Store only artifact metadata (path, checksum, size) in the metadata database and maintain references to the actual object storage locations.\n\n⚠️ **Pitfall: Missing Pagination for Experiment Queries**\n\nExperiment queries without pagination can return thousands of runs, causing browser crashes, memory exhaustion, and poor user experience. This is especially problematic for long-running projects with extensive experiment histories.\n\nImplement **cursor-based pagination** with reasonable default page sizes (50-100 runs). Provide total count estimates without full result set computation to avoid expensive COUNT(*) queries on large tables.\n\n⚠️ **Pitfall: Inadequate Parameter Type Validation**\n\nStoring parameters as strings without type preservation breaks numerical comparisons and range queries. For example, storing learning rates as strings makes \"0.1\" > \"0.01\" evaluate to false in lexicographic ordering.\n\nImplement **type-aware parameter storage** that preserves numeric types, boolean values, and structured data. Use JSON schema validation to ensure parameter values match expected types and ranges before storage.\n\n⚠️ **Pitfall: Ignoring Concurrent Access Patterns**\n\nMultiple training processes logging to the same run simultaneously can cause race conditions, lost updates, and inconsistent experiment state. This is common in distributed training scenarios or when multiple team members accidentally use the same run ID.\n\nImplement **optimistic concurrency control** with version vectors or timestamps. Detect conflicting updates and provide clear error messages that help users understand and resolve concurrency issues.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| **Metadata Storage** | SQLite with JSON columns | PostgreSQL with JSONB indexing |\n| **Time-Series Storage** | PostgreSQL with TimescaleDB extension | InfluxDB or Prometheus |\n| **Object Storage** | Local filesystem with organization | AWS S3 or MinIO |\n| **Event Coordination** | Direct function calls | Redis Pub/Sub or Apache Kafka |\n| **API Framework** | Flask with SQLAlchemy | FastAPI with async database drivers |\n| **Serialization** | JSON with custom encoders | Protocol Buffers or MessagePack |\n\n#### Recommended File Structure\n\n```\nexperiment_tracking/\n├── __init__.py\n├── api/\n│   ├── __init__.py\n│   ├── runs.py              ← Run logging and retrieval endpoints\n│   ├── experiments.py       ← Experiment management endpoints\n│   └── search.py            ← Query and comparison endpoints\n├── storage/\n│   ├── __init__.py\n│   ├── interfaces.py        ← Abstract storage interfaces\n│   ├── metadata_store.py    ← MetadataStore implementations\n│   ├── artifact_store.py    ← ArtifactStore implementations\n│   └── migrations/          ← Database schema migrations\n├── models/\n│   ├── __init__.py\n│   ├── experiment.py        ← Experiment and Run data models\n│   ├── metric.py            ← Metric and parameter data models\n│   └── artifact.py          ← Artifact metadata models\n├── services/\n│   ├── __init__.py\n│   ├── tracking_service.py  ← Core experiment tracking logic\n│   ├── query_service.py     ← Search and comparison operations\n│   └── event_publisher.py   ← Event coordination logic\n└── utils/\n    ├── __init__.py\n    ├── validation.py        ← Input validation and normalization\n    └── serialization.py     ← Type-aware parameter serialization\n```\n\n#### Core Data Models (Complete Implementation)\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\nfrom enum import Enum\nimport uuid\n\nclass RunStatus(Enum):\n    RUNNING = \"RUNNING\"\n    FINISHED = \"FINISHED\"\n    FAILED = \"FAILED\"\n    KILLED = \"KILLED\"\n\n@dataclass\nclass Experiment:\n    experiment_id: str\n    name: str\n    lifecycle_stage: str = \"active\"\n    creation_time: float = field(default_factory=lambda: datetime.utcnow().timestamp())\n    last_update_time: float = field(default_factory=lambda: datetime.utcnow().timestamp())\n    tags: Dict[str, str] = field(default_factory=dict)\n    \n    @classmethod\n    def create(cls, name: str, tags: Dict[str, str] = None) -> 'Experiment':\n        return cls(\n            experiment_id=str(uuid.uuid4()),\n            name=name,\n            tags=tags or {}\n        )\n\n@dataclass\nclass Run:\n    run_id: str\n    experiment_id: str\n    status: RunStatus\n    start_time: float\n    end_time: Optional[float] = None\n    source_version: Optional[str] = None\n    entry_point: Optional[str] = None\n    user_id: Optional[str] = None\n    tags: Dict[str, str] = field(default_factory=dict)\n    \n    @classmethod\n    def create(cls, experiment_id: str, source_version: str = None) -> 'Run':\n        return cls(\n            run_id=str(uuid.uuid4()),\n            experiment_id=experiment_id,\n            status=RunStatus.RUNNING,\n            start_time=datetime.utcnow().timestamp(),\n            source_version=source_version\n        )\n\n@dataclass\nclass MetricPoint:\n    run_id: str\n    key: str\n    value: float\n    step: int\n    timestamp: float\n    \n@dataclass\nclass Parameter:\n    run_id: str\n    key: str\n    value: Any\n    value_type: str\n    \n@dataclass\nclass ArtifactInfo:\n    run_id: str\n    path: str\n    size_bytes: int\n    checksum: str\n    mime_type: str\n    created_at: float\n    metadata: Dict[str, str] = field(default_factory=dict)\n```\n\n#### Storage Interface Implementations\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Any, Optional\n\nclass MetadataStore(ABC):\n    \"\"\"Abstract interface for storing experiment metadata.\"\"\"\n    \n    @abstractmethod\n    def create_table(self, table_name: str, schema: Dict[str, str]) -> None:\n        \"\"\"Create a table with the specified schema.\"\"\"\n        pass\n    \n    @abstractmethod\n    def insert(self, table_name: str, data: Dict[str, Any]) -> str:\n        \"\"\"Insert data and return generated ID.\"\"\"\n        pass\n    \n    @abstractmethod\n    def update(self, table_name: str, record_id: str, data: Dict[str, Any]) -> None:\n        \"\"\"Update existing record with new data.\"\"\"\n        pass\n    \n    @abstractmethod\n    def query(self, table_name: str, filter_condition: str, order_by: List[str] = None, \n              limit: int = None, offset: int = None) -> List[Dict[str, Any]]:\n        \"\"\"Query records with filtering and pagination.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_by_id(self, table_name: str, record_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Retrieve a single record by its ID.\"\"\"\n        pass\n\nclass ArtifactStore(ABC):\n    \"\"\"Abstract interface for storing experiment artifacts.\"\"\"\n    \n    @abstractmethod\n    def put(self, key: str, data: bytes, metadata: Dict[str, str] = None) -> str:\n        \"\"\"Store binary data with optional metadata.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get(self, key: str) -> bytes:\n        \"\"\"Retrieve binary data by key.\"\"\"\n        pass\n    \n    @abstractmethod\n    def delete(self, key: str) -> None:\n        \"\"\"Delete data by key.\"\"\"\n        pass\n    \n    @abstractmethod\n    def list_keys(self, prefix: str = \"\", max_keys: int = 1000) -> List[str]:\n        \"\"\"List keys with optional prefix filtering.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_metadata(self, key: str) -> Dict[str, str]:\n        \"\"\"Retrieve metadata for a stored object.\"\"\"\n        pass\n\n# Simple PostgreSQL implementation for metadata storage\nimport psycopg2\nimport json\nfrom typing import Dict, List, Any, Optional\n\nclass PostgreSQLMetadataStore(MetadataStore):\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n    \n    def _get_connection(self):\n        return psycopg2.connect(self.connection_string)\n    \n    def create_table(self, table_name: str, schema: Dict[str, str]) -> None:\n        # TODO 1: Generate CREATE TABLE SQL from schema dictionary\n        # TODO 2: Execute DDL statement with proper error handling\n        # TODO 3: Add indexes for commonly queried columns (run_id, experiment_id)\n        pass\n    \n    def insert(self, table_name: str, data: Dict[str, Any]) -> str:\n        # TODO 1: Generate INSERT statement with RETURNING clause for ID\n        # TODO 2: Serialize complex data types (dicts, lists) to JSON\n        # TODO 3: Execute insert and return generated ID\n        # TODO 4: Handle constraint violations and provide meaningful errors\n        pass\n    \n    def update(self, table_name: str, record_id: str, data: Dict[str, Any]) -> None:\n        # TODO 1: Generate UPDATE statement with WHERE clause on ID\n        # TODO 2: Handle partial updates (only provided fields)\n        # TODO 3: Validate record exists before attempting update\n        pass\n    \n    def query(self, table_name: str, filter_condition: str, order_by: List[str] = None,\n              limit: int = None, offset: int = None) -> List[Dict[str, Any]]:\n        # TODO 1: Parse and validate filter_condition for SQL injection safety\n        # TODO 2: Build SELECT statement with WHERE, ORDER BY, LIMIT, OFFSET\n        # TODO 3: Execute query and convert rows to dictionaries\n        # TODO 4: Handle JSON deserialization for complex fields\n        pass\n    \n    def get_by_id(self, table_name: str, record_id: str) -> Optional[Dict[str, Any]]:\n        # TODO 1: Execute SELECT with WHERE id = %s\n        # TODO 2: Return None if no record found, dict if found\n        # TODO 3: Deserialize JSON fields back to Python objects\n        pass\n\n# Simple filesystem implementation for artifact storage\nimport os\nimport hashlib\nimport shutil\nfrom pathlib import Path\n\nclass FilesystemArtifactStore(ArtifactStore):\n    def __init__(self, base_path: str):\n        self.base_path = Path(base_path)\n        self.base_path.mkdir(parents=True, exist_ok=True)\n    \n    def put(self, key: str, data: bytes, metadata: Dict[str, str] = None) -> str:\n        # TODO 1: Validate key format and prevent directory traversal attacks\n        # TODO 2: Create directory structure for key path\n        # TODO 3: Write data to file atomically (write to temp, then rename)\n        # TODO 4: Compute and store checksum for data integrity\n        # TODO 5: Save metadata as separate .meta file alongside data\n        pass\n    \n    def get(self, key: str) -> bytes:\n        # TODO 1: Validate key exists and build full file path\n        # TODO 2: Read binary data from file\n        # TODO 3: Optionally verify checksum if metadata exists\n        # TODO 4: Handle file not found with appropriate exception\n        pass\n    \n    def delete(self, key: str) -> None:\n        # TODO 1: Remove both data file and metadata file\n        # TODO 2: Clean up empty parent directories\n        # TODO 3: Handle missing file gracefully (idempotent operation)\n        pass\n    \n    def list_keys(self, prefix: str = \"\", max_keys: int = 1000) -> List[str]:\n        # TODO 1: Walk directory tree starting from prefix path\n        # TODO 2: Convert file paths back to key format\n        # TODO 3: Apply max_keys limit and return sorted results\n        # TODO 4: Filter out metadata files from results\n        pass\n```\n\n#### Core Tracking Service (Skeleton)\n\n```python\nfrom typing import Dict, List, Any, Optional\nfrom .models import Experiment, Run, MetricPoint, Parameter, ArtifactInfo\nfrom .storage.interfaces import MetadataStore, ArtifactStore\n\nclass ExperimentTrackingService:\n    def __init__(self, metadata_store: MetadataStore, artifact_store: ArtifactStore):\n        self.metadata_store = metadata_store\n        self.artifact_store = artifact_store\n        self._initialize_tables()\n    \n    def _initialize_tables(self) -> None:\n        # TODO 1: Define schema for experiments table (id, name, lifecycle_stage, creation_time, tags)\n        # TODO 2: Define schema for runs table (id, experiment_id, status, start_time, end_time, user_id, tags)\n        # TODO 3: Define schema for metrics table (run_id, key, value, step, timestamp)\n        # TODO 4: Define schema for parameters table (run_id, key, value, value_type)\n        # TODO 5: Create all tables and indexes for optimal query performance\n        pass\n    \n    def create_experiment(self, name: str, tags: Dict[str, str] = None) -> Experiment:\n        # TODO 1: Validate experiment name is unique\n        # TODO 2: Create Experiment object with generated ID\n        # TODO 3: Insert experiment into metadata store\n        # TODO 4: Return created experiment object\n        pass\n    \n    def create_run(self, experiment_id: str, tags: Dict[str, str] = None, \n                   source_version: str = None) -> Run:\n        # TODO 1: Validate experiment exists\n        # TODO 2: Create Run object with generated ID and RUNNING status\n        # TODO 3: Insert run into metadata store\n        # TODO 4: Publish run.started event for monitoring\n        # TODO 5: Return created run object\n        pass\n    \n    def log_param(self, run_id: str, key: str, value: Any) -> None:\n        # TODO 1: Validate run exists and is in RUNNING status\n        # TODO 2: Determine value type for type-aware storage\n        # TODO 3: Check if parameter already exists (warn about overwrites)\n        # TODO 4: Insert parameter into metadata store\n        # TODO 5: Handle type serialization for complex objects\n        pass\n    \n    def log_metric(self, run_id: str, key: str, value: float, step: int, \n                   timestamp: Optional[float] = None) -> None:\n        # TODO 1: Validate run exists and metric value is numeric\n        # TODO 2: Use current timestamp if not provided\n        # TODO 3: Create MetricPoint object with all required fields\n        # TODO 4: Insert metric into time-series optimized storage\n        # TODO 5: Check for metric threshold events and publish if needed\n        pass\n    \n    def log_artifact(self, run_id: str, local_path: str, artifact_path: str) -> ArtifactInfo:\n        # TODO 1: Validate run exists and local file exists\n        # TODO 2: Read file data and compute checksum\n        # TODO 3: Determine MIME type from file extension\n        # TODO 4: Store file in artifact store using run_id/artifact_path as key\n        # TODO 5: Create ArtifactInfo record and store metadata\n        # TODO 6: Publish artifact.logged event with artifact details\n        pass\n    \n    def finish_run(self, run_id: str, status: str = \"FINISHED\") -> None:\n        # TODO 1: Validate run exists and is currently RUNNING\n        # TODO 2: Update run status and end_time in metadata store\n        # TODO 3: Compute final metrics summary for quick access\n        # TODO 4: Publish EXPERIMENT_COMPLETED event with run summary\n        # TODO 5: Trigger any auto-promotion workflows if configured\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the experiment tracking component, verify the following behavior:\n\n1. **Create and Run Experiment Test**:\n   ```bash\n   python -m pytest tests/test_experiment_tracking.py::test_create_experiment_and_run\n   ```\n   Expected: New experiment and run created with valid UUIDs and timestamps\n\n2. **Parameter Logging Test**:\n   ```python\n   # Should successfully log various parameter types\n   service.log_param(run_id, \"learning_rate\", 0.001)\n   service.log_param(run_id, \"batch_size\", 32)  \n   service.log_param(run_id, \"optimizer\", {\"type\": \"adam\", \"beta1\": 0.9})\n   ```\n   Expected: All parameters stored with correct types and retrievable\n\n3. **Metric Logging and Retrieval Test**:\n   Log 100 training steps with loss and accuracy metrics, then verify:\n   - All metric points stored with correct step numbers\n   - Time-series retrieval returns points in chronological order\n   - Metric comparison shows expected learning curves\n\n4. **Artifact Storage Test**:\n   Upload a test model file and configuration, then verify:\n   - Artifacts appear in list_artifacts output\n   - Download produces identical file content\n   - Metadata includes correct file size and checksum\n\n5. **Query and Search Test**:\n   Create multiple runs with different parameters, then verify:\n   - Search filters work correctly (e.g., `params.learning_rate > 0.001`)\n   - Run comparison shows parameter and metric differences\n   - Pagination handles large result sets properly\n\n**Signs of Successful Implementation**:\n- Sub-second response times for typical queries (< 100 runs)\n- No data corruption under concurrent logging from multiple processes  \n- Event publishing triggers can be verified in system logs\n- Memory usage remains stable during long-running experiments\n\n**Common Issues and Debugging**:\n- **Symptom**: \"Run not found\" errors during logging\n  - **Cause**: Race condition between run creation and first log call\n  - **Fix**: Add retry logic or ensure run creation completes before logging\n\n- **Symptom**: Query timeouts on large experiments\n  - **Cause**: Missing database indexes or inefficient filter conditions\n  - **Fix**: Add indexes on run_id, experiment_id, and commonly filtered columns\n\n- **Symptom**: Artifact upload failures with large files\n  - **Cause**: Memory exhaustion from loading entire file\n  - **Fix**: Implement streaming upload with chunked transfer\n\n\n## Model Registry Component\n\n> **Milestone(s):** This section primarily corresponds to Milestone 2 (Model Registry), which focuses on versioning and managing trained models with metadata, stage transitions, lineage tracking, and discovery capabilities.\n\n### Mental Model: Software Package Registry\n\nUnderstanding model versioning and lifecycle management is best approached through the familiar analogy of software package registries like npm, PyPI, or Docker Hub. Just as these registries manage software artifacts through their lifecycle, a model registry manages machine learning models as versioned, deployable assets.\n\nConsider how npm works: developers publish package versions (1.0.0, 1.1.0, 2.0.0) with metadata describing dependencies, compatibility, and usage. Users discover packages through search, examine version history, and install specific versions. Critical packages go through testing stages before promotion to \"latest\" or \"stable\" tags. The registry tracks who published what, when, and maintains immutable storage ensuring that version 1.2.3 always contains exactly the same code.\n\nA model registry operates on identical principles but with ML-specific concerns. Instead of JavaScript libraries, we're managing trained neural networks, decision trees, or ensemble models. Instead of semantic versioning based on API compatibility, we version based on training data, algorithm changes, or performance improvements. Instead of npm tags like \"latest\" or \"beta\", we have ML-specific stages like \"staging\", \"production\", or \"archived\". The registry tracks model lineage back to training experiments rather than git commits, but the fundamental versioning and lifecycle concepts remain the same.\n\nThis mental model is powerful because it immediately clarifies several design decisions. Just as package registries separate metadata (package.json) from artifacts (the actual code), model registries separate model metadata from the binary model files. Package registries enforce immutability—once published, a version never changes—and model registries must provide the same guarantee for reproducibility. Package registries support multiple simultaneous versions in production (different applications using different library versions), and model registries enable A/B testing by serving multiple model versions simultaneously.\n\nThe key insight is that models are not just files to be stored, but **versioned artifacts with rich metadata, lifecycle stages, and deployment semantics**. This perspective guides every design decision in the model registry component.\n\n### Version Management and Stages\n\nModel versioning requires a systematic approach to track changes, coordinate deployments, and maintain production stability. The version management system combines semantic versioning principles with ML-specific stage transitions to create a controlled path from experimental models to production deployments.\n\n**Version Numbering Strategy**\n\nModel versions follow a three-component semantic versioning scheme adapted for ML workflows: MAJOR.MINOR.PATCH. The major version increments when fundamental changes occur—new training data, different algorithms, or incompatible input/output schemas. The minor version increments for improvements that maintain compatibility—hyperparameter tuning, additional training epochs, or feature engineering changes. The patch version increments for metadata updates or bug fixes that don't affect model behavior.\n\nThis versioning strategy provides immediate insight into compatibility and risk. A change from version 2.1.3 to 2.2.0 suggests performance improvements with maintained compatibility. A jump to 3.0.0 signals potential breaking changes requiring careful testing. Unlike software versioning based on API contracts, ML versioning considers data schemas, performance characteristics, and prediction distributions.\n\n| Version Component | ML-Specific Meaning | Example Triggers |\n|------------------|-------------------|------------------|\n| MAJOR | Breaking changes to model interface or fundamental algorithm | New training dataset, schema changes, different model architecture |\n| MINOR | Performance improvements maintaining compatibility | Hyperparameter optimization, additional training data, feature engineering |\n| PATCH | Metadata updates without behavioral changes | Tag updates, description changes, ownership transfers |\n\n**Stage-Based Lifecycle Management**\n\nEach model version progresses through defined stages representing different levels of validation and approval. This stage-based approach prevents untested models from reaching production while enabling parallel development of multiple model variants.\n\nThe **Development** stage contains newly registered models undergoing initial validation. Models in this stage are accessible for experimentation but carry no production guarantees. The registry allows rapid iteration, frequent uploads, and experimental comparisons without formal approval processes.\n\nThe **Staging** stage represents models that have passed initial validation and are candidates for production deployment. Promotion from Development to Staging typically requires meeting accuracy thresholds, passing integration tests, and receiving approval from designated reviewers. Models in Staging undergo more rigorous testing including performance benchmarks, data compatibility checks, and shadow deployments.\n\nThe **Production** stage contains models actively serving real traffic. Promotion to Production requires formal approval workflows, often involving multiple stakeholders reviewing performance metrics, business impact analysis, and rollback procedures. Only one model version per model name typically holds Production status at any given time, though A/B testing scenarios may temporarily promote multiple versions.\n\nThe **Archived** stage stores models removed from active use but retained for historical analysis or regulatory compliance. Archived models remain immutable and queryable but are excluded from deployment workflows and discovery interfaces.\n\n| Stage | Purpose | Promotion Requirements | Access Control |\n|-------|---------|----------------------|----------------|\n| Development | Initial experimentation | Automatic on registration | Model owner and team |\n| Staging | Pre-production validation | Accuracy thresholds, reviewer approval | Extended team, QA personnel |\n| Production | Active serving | Formal approval workflow, performance validation | Production engineers, designated approvers |\n| Archived | Historical retention | Manual archival or automated policies | Read-only access for compliance |\n\n**Stage Transition Workflows**\n\nStage transitions implement approval gates ensuring models meet quality and safety requirements before promotion. Each transition type defines specific validation criteria and approval mechanisms.\n\nDevelopment to Staging transitions require automated validation checks: model artifact integrity, schema compatibility with existing pipelines, and baseline performance metrics. The system executes these checks automatically when promotion is requested, blocking the transition if any validation fails. Additional approvals from designated reviewers may be required based on organizational policies.\n\nStaging to Production transitions involve more rigorous validation including business stakeholder approval, performance benchmarking against current production models, and verification of rollback procedures. This transition often triggers automated deployment preparation, infrastructure provisioning, and monitoring configuration.\n\nEmergency rollback procedures enable rapid Production to Staging demotions when models exhibit unexpected behavior in production. These rollbacks bypass normal approval workflows but generate audit events and require post-incident review.\n\n> **Key Design Insight**: Stage transitions are operations on model versions, not model names. This allows multiple versions of the same model to exist in different stages simultaneously, enabling gradual migration strategies and emergency rollbacks.\n\n**Immutability Guarantees**\n\nOnce registered, model versions are immutable to ensure reproducibility and audit compliance. The registry enforces immutability at multiple levels: artifact content, metadata schemas, and version identifiers. This guarantee enables reliable rollbacks, regulatory compliance, and scientific reproducibility.\n\nArtifact immutability ensures that model version 2.1.3 always contains exactly the same trained weights, regardless of when it's accessed. The system computes cryptographic hashes of model artifacts during registration and validates these hashes during retrieval, detecting any corruption or tampering.\n\nMetadata immutability prevents unauthorized changes to model descriptions, performance metrics, or ownership information after registration. While some metadata fields like tags or descriptions might be updateable through controlled workflows, core metadata including training metrics, lineage information, and approval history remains frozen.\n\nVersion identifier immutability guarantees that version numbers are never reused. Once version 2.1.3 is registered, no future model can claim that identifier, even if the original is deleted. This prevents confusion and maintains clear audit trails.\n\n> **Architecture Decision: Content-Addressable Storage**\n> - **Context**: Need to guarantee model artifact immutability while supporting efficient storage and retrieval\n> - **Options Considered**: \n>   1. File-based storage with access controls\n>   2. Content-addressable storage with cryptographic hashes\n>   3. Database blob storage with versioning\n> - **Decision**: Content-addressable storage using SHA-256 hashes as keys\n> - **Rationale**: Provides automatic deduplication, tamper detection, and location-independent addressing. Hash-based keys make corruption immediately detectable and enable distributed caching.\n> - **Consequences**: Requires careful garbage collection to avoid orphaned artifacts, but provides strongest immutability guarantees with efficient storage utilization.\n\n### Model Lineage and Metadata\n\nModel lineage tracking creates an auditable chain of provenance linking deployed models back to their training experiments, data sources, and code versions. This traceability is essential for debugging production issues, ensuring regulatory compliance, and understanding model behavior changes over time.\n\n**Lineage Graph Construction**\n\nThe lineage graph represents dependencies between models, experiments, datasets, and code versions as a directed acyclic graph. Each model version serves as a root node with edges pointing to its dependencies: the experiment run that produced it, the training dataset version used, the code commit containing training logic, and any parent models in transfer learning scenarios.\n\nExperiment lineage links each model to its originating training run through the `source_run_id` field. This connection enables tracing model behavior back to specific hyperparameters, training metrics, and environmental conditions. The lineage system captures not just the final training run, but any preliminary experiments or hyperparameter sweeps that contributed to the final model configuration.\n\nData lineage tracks the training and validation datasets used to create each model version. This includes dataset versions, preprocessing pipelines, and feature engineering transformations. The system records dataset checksums, transformation code hashes, and schema versions to enable precise reproduction of training conditions.\n\nCode lineage connects models to specific git commits, Docker images, or training environment snapshots. This linkage enables reproducing the exact training environment, including framework versions, system dependencies, and configuration files. The lineage system stores enough information to recreate the training environment, not just identify it.\n\n| Lineage Type | Source | Destination | Information Captured |\n|--------------|--------|-------------|---------------------|\n| Experiment | Model Version | Training Run | Run ID, experiment parameters, training metrics |\n| Data | Model Version | Dataset Version | Dataset hash, schema version, preprocessing pipeline |\n| Code | Model Version | Code Version | Git commit, Docker image, dependency manifest |\n| Model | Model Version | Parent Model | Transfer learning base, fine-tuning checkpoint |\n\n**Metadata Schema Design**\n\nModel metadata encompasses both technical and business information required for model discovery, validation, and governance. The metadata schema balances completeness with flexibility, providing structured fields for common attributes while supporting extensible custom metadata.\n\nCore metadata includes model identification, versioning, and ownership information. Technical metadata captures model architecture, framework dependencies, input/output schemas, and performance characteristics. Business metadata includes model purpose, approved use cases, and regulatory classifications.\n\nPerformance metadata records accuracy metrics, latency benchmarks, and resource requirements captured during model training and validation. This information guides deployment decisions and capacity planning. The schema supports both standard metrics (accuracy, F1 score, AUC) and custom metrics specific to the problem domain.\n\nSchema metadata describes model input and output formats using JSON Schema or Protocol Buffer definitions. This enables automatic compatibility checking, client code generation, and runtime validation. Schema evolution tracking identifies when models introduce breaking changes requiring coordinated client updates.\n\n| Metadata Category | Fields | Purpose | Example Values |\n|-------------------|--------|---------|---------------|\n| Identification | name, version, id, created_at | Unique identification and discovery | \"sentiment-classifier\", \"2.1.3\", \"uuid-123\" |\n| Ownership | creator, team, maintainer | Responsibility and access control | \"data-science-team\", \"alice@company.com\" |\n| Technical | framework, architecture, size_mb | Deployment planning | \"tensorflow\", \"transformer\", 1250 |\n| Performance | accuracy, latency_p99, throughput | SLA planning and comparison | 0.94, \"15ms\", \"1000 req/s\" |\n| Schema | input_schema, output_schema | Compatibility validation | JSON Schema definitions |\n| Business | purpose, use_cases, compliance | Governance and approval | \"customer sentiment\", [\"marketing\", \"support\"] |\n\n**Lineage Query Capabilities**\n\nThe lineage system supports complex queries for impact analysis, compliance auditing, and debugging workflows. Query patterns include forward lineage (what models were derived from this dataset?), backward lineage (what data was used to train this model?), and impact analysis (if this dataset changes, which production models are affected?).\n\nForward lineage queries start from data sources or code versions and identify all downstream models that could be affected by changes. These queries are essential for data governance, enabling teams to understand the impact of dataset updates, schema changes, or data quality issues on deployed models.\n\nBackward lineage queries start from deployed models and trace back to all contributing data sources, experiments, and code versions. These queries support debugging production issues by identifying potential root causes in training data or configuration changes.\n\nCross-lineage queries combine multiple lineage types to answer complex questions like \"which models in production were trained on data from the compromised dataset collected between March 1-15?\" These queries require joining across experiment, data, and deployment records.\n\nTemporal lineage queries analyze how lineage relationships change over time, supporting questions like \"when did we start using the new feature engineering pipeline?\" or \"which models were affected by the data quality incident last month?\"\n\n> **Key Design Insight**: Lineage is not just about storage—it's about enabling queries that support critical operational workflows. The lineage schema must be optimized for the specific query patterns that model governance requires.\n\n**Automated Lineage Capture**\n\nManual lineage tracking is error-prone and incomplete, so the registry implements automated lineage capture integrated with training workflows. The system uses experiment tracking integration, environment introspection, and policy-based validation to build comprehensive lineage graphs without manual intervention.\n\nExperiment integration automatically captures lineage when models are registered from training runs. The registration API accepts the source run ID and automatically populates data lineage, code lineage, and experiment metadata. This integration eliminates manual lineage entry while ensuring completeness.\n\nEnvironment introspection captures code versions, dependency manifests, and system configurations from training environments. The system can extract git commit hashes, Docker image SHAs, and package version lists from running training jobs. This automated capture ensures lineage accuracy and completeness.\n\nPolicy enforcement validates lineage completeness before allowing model registration or promotion. Teams can define policies requiring specific lineage types (must include data version, code commit, and experiment run) and the system blocks registrations that don't meet these requirements.\n\n### Architecture Decisions\n\nThe model registry requires several critical architecture decisions around storage systems, consistency models, and API design. These decisions fundamentally shape the system's scalability, reliability, and operational characteristics.\n\n> **Decision: Polyglot Persistence for Metadata and Artifacts**\n> - **Context**: Model registry must store both structured metadata (for querying and discovery) and binary artifacts (model files, often gigabytes in size) with different access patterns and consistency requirements\n> - **Options Considered**:\n>   1. Single database storing everything (PostgreSQL with large object support)\n>   2. Metadata in relational database, artifacts in object storage (S3/GCS)\n>   3. Document database for everything (MongoDB GridFS)\n> - **Decision**: Metadata in PostgreSQL, artifacts in S3-compatible object storage\n> - **Rationale**: Relational databases excel at structured queries, joins, and transactions needed for metadata. Object storage provides scalability, durability, and cost-effectiveness for large binary files. Separation allows independent scaling and optimization.\n> - **Consequences**: Enables efficient metadata queries and artifact storage, but requires consistency management across two storage systems and adds complexity for atomic operations spanning both stores.\n\n| Storage Option | Metadata Performance | Artifact Scalability | Query Flexibility | Consistency Guarantees |\n|----------------|---------------------|---------------------|------------------|----------------------|\n| Single Database | Good | Poor (BLOB limits) | Excellent | Strong ACID |\n| Polyglot Persistence | Excellent | Excellent | Excellent | Eventual (cross-store) |\n| Document Database | Good | Good | Limited | Strong (single store) |\n\n> **Decision: Immutable Model Versions with Soft Deletion**\n> - **Context**: Need to support model lifecycle management while maintaining audit trails and enabling rollbacks to previously deployed models\n> - **Options Considered**:\n>   1. Mutable models with version history tracking\n>   2. Immutable versions with hard deletion capabilities\n>   3. Immutable versions with soft deletion only\n> - **Decision**: Immutable versions with soft deletion and configurable retention policies\n> - **Rationale**: Immutability provides strongest reproducibility guarantees. Soft deletion maintains audit trails while supporting cleanup. Retention policies balance compliance needs with storage costs.\n> - **Consequences**: Ensures reproducibility and supports compliance, but requires careful garbage collection and may increase storage costs. Prevents accidental data loss but requires explicit cleanup processes.\n\n> **Decision: Stage-Based Model Lifecycle with Approval Gates**\n> - **Context**: Need to balance rapid model iteration with production stability and quality control\n> - **Options Considered**:\n>   1. No formal stages - direct production deployment\n>   2. Simple staging/production stages\n>   3. Multi-stage lifecycle with approval workflows\n> - **Decision**: Four-stage lifecycle (Development, Staging, Production, Archived) with configurable approval gates\n> - **Rationale**: Provides structured quality gates while maintaining flexibility. Approval workflows enable governance without blocking experimentation. Multiple stages support diverse organizational policies.\n> - **Consequences**: Enables quality control and compliance, but adds complexity and potential bottlenecks. Requires workflow management but provides audit trails and risk mitigation.\n\n**API Design Strategy**\n\nThe model registry API design balances RESTful conventions with ML-specific workflows. The API provides both imperative operations (register model, promote version) and declarative state management (desired model state, automated promotion).\n\nResource hierarchy follows REST principles with models as top-level resources and versions as sub-resources: `/models/{model_name}/versions/{version}`. This structure naturally reflects the domain model and enables hierarchical permissions (model-level vs version-level access).\n\nState transition operations use POST verbs on sub-resources rather than PUT updates to the version resource. This design makes state changes explicit and auditable: `POST /models/{name}/versions/{version}/promote` rather than `PUT /models/{name}/versions/{version}` with a new stage field.\n\nBulk operations support common workflows like comparing multiple model versions or promoting models across environments. The API provides endpoints like `POST /models/compare` accepting multiple model references and returning comparative analytics.\n\n| API Pattern | Endpoint | Purpose | Request/Response |\n|-------------|----------|---------|------------------|\n| Resource Management | `GET /models` | List and search models | Query filters → model summaries |\n| Version Operations | `POST /models/{name}/versions` | Register new version | Model artifact + metadata → version ID |\n| State Transitions | `POST /models/{name}/versions/{ver}/promote` | Promote to next stage | Target stage → promotion status |\n| Lineage Queries | `GET /models/{name}/versions/{ver}/lineage` | Retrieve lineage graph | Lineage direction → dependency graph |\n| Bulk Operations | `POST /models/compare` | Compare multiple versions | Model version list → comparison matrix |\n\n**Consistency and Concurrency Model**\n\nThe model registry implements eventual consistency across storage systems with strong consistency guarantees for critical operations. Metadata operations within PostgreSQL maintain ACID properties, while cross-system operations (metadata + artifacts) use compensation patterns for failure recovery.\n\nModel registration implements two-phase commit across metadata and artifact stores. The system first uploads artifacts to object storage, then creates metadata records with artifact references. Failure at either stage triggers compensation: orphaned artifacts are garbage collected, and incomplete metadata records are cleaned up.\n\nConcurrent version registration for the same model uses optimistic locking on the model resource. Version numbers are assigned atomically during metadata insertion, preventing duplicate versions even under concurrent load. Stage transitions use pessimistic locking to prevent conflicting promotions.\n\nCache consistency maintains read performance while ensuring fresh data for critical operations. The system uses write-through caching for model metadata and lazy invalidation for artifact references. Time-sensitive operations like stage transitions bypass caches to ensure immediate consistency.\n\n> **Key Design Insight**: Model registries require different consistency guarantees for different operations. Artifact uploads can tolerate eventual consistency, but stage transitions affecting production deployments need immediate consistency across all system components.\n\n### Implementation Guidance\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Metadata Storage | SQLite with JSON columns | PostgreSQL with JSONB indexing |\n| Artifact Storage | Local filesystem with checksums | S3-compatible object storage (MinIO/AWS) |\n| API Framework | Flask-RESTful with SQLAlchemy | FastAPI with async PostgreSQL driver |\n| Schema Validation | JSON Schema with jsonschema library | Pydantic models with automatic OpenAPI |\n| Lineage Queries | Recursive SQL CTEs | Graph database (Neo4j) for complex traversals |\n| Caching Layer | In-memory Python dictionaries | Redis with TTL-based invalidation |\n\n**B. Recommended File/Module Structure**\n\n```\nmlops-platform/\n  model-registry/\n    src/\n      registry/\n        __init__.py\n        models/\n          __init__.py\n          model.py              ← Model and ModelVersion entities\n          lineage.py            ← Lineage tracking classes\n          metadata.py           ← Metadata schema definitions\n        storage/\n          __init__.py\n          metadata_store.py     ← PostgreSQL metadata operations\n          artifact_store.py     ← S3 artifact operations\n          lineage_store.py      ← Lineage graph storage\n        api/\n          __init__.py\n          models_api.py         ← Model CRUD endpoints\n          versions_api.py       ← Version management endpoints\n          lineage_api.py        ← Lineage query endpoints\n          schemas.py            ← API request/response schemas\n        services/\n          __init__.py\n          registry_service.py   ← Core business logic\n          promotion_service.py  ← Stage transition workflows\n          lineage_service.py    ← Lineage analysis logic\n        migrations/\n          001_initial_schema.sql\n          002_add_lineage_tables.sql\n    tests/\n      unit/\n        test_models.py\n        test_storage.py\n        test_services.py\n      integration/\n        test_api_endpoints.py\n        test_lineage_queries.py\n    requirements.txt\n    docker-compose.yml          ← PostgreSQL + MinIO for development\n```\n\n**C. Infrastructure Starter Code**\n\n```python\n# storage/metadata_store.py - Complete PostgreSQL metadata storage\nimport psycopg2\nfrom psycopg2.extras import RealDictCursor\nfrom typing import Dict, List, Optional, Any\nimport json\nfrom datetime import datetime\n\nclass ModelMetadataStore:\n    \"\"\"PostgreSQL-based metadata storage with JSONB support for flexible schemas.\"\"\"\n    \n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n        self._init_tables()\n    \n    def _init_tables(self):\n        \"\"\"Create tables with proper indexes for common query patterns.\"\"\"\n        with psycopg2.connect(self.connection_string) as conn:\n            with conn.cursor() as cur:\n                # Models table\n                cur.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS models (\n                        name VARCHAR(255) PRIMARY KEY,\n                        description TEXT,\n                        tags JSONB DEFAULT '{}',\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                    )\n                \"\"\")\n                \n                # Model versions table\n                cur.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS model_versions (\n                        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n                        model_name VARCHAR(255) REFERENCES models(name),\n                        version VARCHAR(50) NOT NULL,\n                        stage VARCHAR(20) DEFAULT 'Development',\n                        artifact_uri VARCHAR(500) NOT NULL,\n                        artifact_checksum VARCHAR(64) NOT NULL,\n                        metadata JSONB DEFAULT '{}',\n                        lineage JSONB DEFAULT '{}',\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        UNIQUE(model_name, version)\n                    )\n                \"\"\")\n                \n                # Create indexes for common queries\n                cur.execute(\"CREATE INDEX IF NOT EXISTS idx_versions_stage ON model_versions(stage)\")\n                cur.execute(\"CREATE INDEX IF NOT EXISTS idx_versions_metadata ON model_versions USING gin(metadata)\")\n                cur.execute(\"CREATE INDEX IF NOT EXISTS idx_models_tags ON models USING gin(tags)\")\n                \n                conn.commit()\n    \n    def create_model(self, name: str, description: str = \"\", tags: Dict[str, str] = None) -> Dict[str, Any]:\n        \"\"\"Create a new model entry.\"\"\"\n        tags = tags or {}\n        with psycopg2.connect(self.connection_string) as conn:\n            with conn.cursor(cursor_factory=RealDictCursor) as cur:\n                cur.execute(\"\"\"\n                    INSERT INTO models (name, description, tags)\n                    VALUES (%s, %s, %s)\n                    RETURNING *\n                \"\"\", (name, description, json.dumps(tags)))\n                return dict(cur.fetchone())\n    \n    def create_version(self, model_name: str, version: str, artifact_uri: str, \n                      artifact_checksum: str, metadata: Dict[str, Any] = None,\n                      lineage: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"Create a new model version.\"\"\"\n        metadata = metadata or {}\n        lineage = lineage or {}\n        with psycopg2.connect(self.connection_string) as conn:\n            with conn.cursor(cursor_factory=RealDictCursor) as cur:\n                cur.execute(\"\"\"\n                    INSERT INTO model_versions \n                    (model_name, version, artifact_uri, artifact_checksum, metadata, lineage)\n                    VALUES (%s, %s, %s, %s, %s, %s)\n                    RETURNING *\n                \"\"\", (model_name, version, artifact_uri, artifact_checksum, \n                     json.dumps(metadata), json.dumps(lineage)))\n                return dict(cur.fetchone())\n    \n    def update_version_stage(self, model_name: str, version: str, new_stage: str) -> bool:\n        \"\"\"Update model version stage with optimistic concurrency.\"\"\"\n        with psycopg2.connect(self.connection_string) as conn:\n            with conn.cursor() as cur:\n                cur.execute(\"\"\"\n                    UPDATE model_versions \n                    SET stage = %s \n                    WHERE model_name = %s AND version = %s\n                \"\"\", (new_stage, model_name, version))\n                return cur.rowcount > 0\n    \n    def search_models(self, name_filter: str = \"\", stage: str = \"\", \n                     tags: Dict[str, str] = None, limit: int = 100) -> List[Dict[str, Any]]:\n        \"\"\"Search models with flexible filtering.\"\"\"\n        conditions = []\n        params = []\n        \n        if name_filter:\n            conditions.append(\"m.name ILIKE %s\")\n            params.append(f\"%{name_filter}%\")\n        \n        if stage:\n            conditions.append(\"v.stage = %s\")\n            params.append(stage)\n        \n        if tags:\n            for key, value in tags.items():\n                conditions.append(\"m.tags->>%s = %s\")\n                params.extend([key, value])\n        \n        where_clause = \" AND \".join(conditions) if conditions else \"1=1\"\n        \n        with psycopg2.connect(self.connection_string) as conn:\n            with conn.cursor(cursor_factory=RealDictCursor) as cur:\n                cur.execute(f\"\"\"\n                    SELECT DISTINCT m.*, v.version, v.stage\n                    FROM models m\n                    LEFT JOIN model_versions v ON m.name = v.model_name\n                    WHERE {where_clause}\n                    ORDER BY m.created_at DESC\n                    LIMIT %s\n                \"\"\", params + [limit])\n                return [dict(row) for row in cur.fetchall()]\n```\n\n```python\n# storage/artifact_store.py - Complete S3-compatible artifact storage\nimport boto3\nfrom botocore.exceptions import ClientError\nimport hashlib\nfrom typing import Optional, Dict, Any, BinaryIO\nimport os\nfrom pathlib import Path\n\nclass ModelArtifactStore:\n    \"\"\"S3-compatible storage for model artifacts with checksum validation.\"\"\"\n    \n    def __init__(self, bucket_name: str, endpoint_url: Optional[str] = None,\n                 aws_access_key_id: Optional[str] = None, aws_secret_access_key: Optional[str] = None):\n        self.bucket_name = bucket_name\n        \n        # Support both AWS S3 and MinIO\n        session = boto3.Session()\n        self.s3_client = session.client(\n            's3',\n            endpoint_url=endpoint_url,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key\n        )\n        \n        self._ensure_bucket_exists()\n    \n    def _ensure_bucket_exists(self):\n        \"\"\"Create bucket if it doesn't exist.\"\"\"\n        try:\n            self.s3_client.head_bucket(Bucket=self.bucket_name)\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                self.s3_client.create_bucket(Bucket=self.bucket_name)\n            else:\n                raise\n    \n    def _compute_checksum(self, data: bytes) -> str:\n        \"\"\"Compute SHA-256 checksum for data integrity.\"\"\"\n        return hashlib.sha256(data).hexdigest()\n    \n    def put_artifact(self, key: str, data: bytes, metadata: Dict[str, str] = None) -> str:\n        \"\"\"Store artifact and return its checksum.\"\"\"\n        metadata = metadata or {}\n        checksum = self._compute_checksum(data)\n        \n        # Add checksum to metadata\n        metadata['checksum'] = checksum\n        metadata['size'] = str(len(data))\n        \n        try:\n            self.s3_client.put_object(\n                Bucket=self.bucket_name,\n                Key=key,\n                Body=data,\n                Metadata=metadata\n            )\n            return checksum\n        except ClientError as e:\n            raise RuntimeError(f\"Failed to store artifact {key}: {e}\")\n    \n    def put_file(self, key: str, file_path: Path, metadata: Dict[str, str] = None) -> str:\n        \"\"\"Store file artifact and return its checksum.\"\"\"\n        with open(file_path, 'rb') as f:\n            data = f.read()\n        \n        metadata = metadata or {}\n        metadata['original_filename'] = file_path.name\n        metadata['content_type'] = self._guess_content_type(file_path)\n        \n        return self.put_artifact(key, data, metadata)\n    \n    def get_artifact(self, key: str, validate_checksum: bool = True) -> bytes:\n        \"\"\"Retrieve artifact with optional checksum validation.\"\"\"\n        try:\n            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=key)\n            data = response['Body'].read()\n            \n            if validate_checksum and 'checksum' in response.get('Metadata', {}):\n                expected_checksum = response['Metadata']['checksum']\n                actual_checksum = self._compute_checksum(data)\n                if expected_checksum != actual_checksum:\n                    raise ValueError(f\"Checksum mismatch for {key}: expected {expected_checksum}, got {actual_checksum}\")\n            \n            return data\n        except ClientError as e:\n            if e.response['Error']['Code'] == 'NoSuchKey':\n                raise FileNotFoundError(f\"Artifact not found: {key}\")\n            raise RuntimeError(f\"Failed to retrieve artifact {key}: {e}\")\n    \n    def download_file(self, key: str, local_path: Path, validate_checksum: bool = True):\n        \"\"\"Download artifact to local file.\"\"\"\n        data = self.get_artifact(key, validate_checksum)\n        local_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(local_path, 'wb') as f:\n            f.write(data)\n    \n    def artifact_exists(self, key: str) -> bool:\n        \"\"\"Check if artifact exists.\"\"\"\n        try:\n            self.s3_client.head_object(Bucket=self.bucket_name, Key=key)\n            return True\n        except ClientError as e:\n            if e.response['Error']['Code'] == '404':\n                return False\n            raise\n    \n    def list_artifacts(self, prefix: str = \"\") -> List[Dict[str, Any]]:\n        \"\"\"List artifacts with metadata.\"\"\"\n        try:\n            response = self.s3_client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)\n            return [\n                {\n                    'key': obj['Key'],\n                    'size': obj['Size'],\n                    'last_modified': obj['LastModified'],\n                    'etag': obj['ETag'].strip('\"')\n                }\n                for obj in response.get('Contents', [])\n            ]\n        except ClientError as e:\n            raise RuntimeError(f\"Failed to list artifacts: {e}\")\n    \n    def _guess_content_type(self, file_path: Path) -> str:\n        \"\"\"Guess content type from file extension.\"\"\"\n        suffix = file_path.suffix.lower()\n        content_types = {\n            '.pkl': 'application/octet-stream',\n            '.joblib': 'application/octet-stream',\n            '.pt': 'application/octet-stream',\n            '.pth': 'application/octet-stream',\n            '.h5': 'application/octet-stream',\n            '.pb': 'application/octet-stream',\n            '.onnx': 'application/octet-stream',\n            '.json': 'application/json',\n            '.yaml': 'application/x-yaml',\n            '.yml': 'application/x-yaml'\n        }\n        return content_types.get(suffix, 'application/octet-stream')\n```\n\n**D. Core Logic Skeleton Code**\n\n```python\n# services/registry_service.py - Core model registry business logic\nfrom typing import Dict, List, Optional, Any\nfrom pathlib import Path\nimport uuid\nfrom datetime import datetime\n\nfrom ..models.model import Model, ModelVersion, ModelStage\nfrom ..storage.metadata_store import ModelMetadataStore\nfrom ..storage.artifact_store import ModelArtifactStore\n\nclass ModelRegistryService:\n    \"\"\"Core business logic for model registration and lifecycle management.\"\"\"\n    \n    def __init__(self, metadata_store: ModelMetadataStore, artifact_store: ModelArtifactStore):\n        self.metadata_store = metadata_store\n        self.artifact_store = artifact_store\n    \n    def register_model_version(self, model_name: str, version: str, \n                              artifact_path: Path, run_id: Optional[str] = None,\n                              metadata: Dict[str, Any] = None) -> ModelVersion:\n        \"\"\"\n        Register a new model version with artifact upload and lineage tracking.\n        \n        This implements the two-phase commit pattern for consistency across storage systems.\n        \"\"\"\n        # TODO 1: Validate inputs - check model_name format, version format, artifact_path exists\n        # TODO 2: Generate artifact key using model_name/version/filename pattern\n        # TODO 3: Upload artifact to storage and get checksum - handle upload failures\n        # TODO 4: Create metadata record with artifact reference - handle database failures\n        # TODO 5: If run_id provided, fetch lineage info from experiment tracking\n        # TODO 6: On any failure after artifact upload, implement cleanup (delete orphaned artifact)\n        # TODO 7: Return populated ModelVersion object\n        \n        # Hint: Use try/except with cleanup in except block\n        # Hint: Artifact key format: f\"models/{model_name}/versions/{version}/{artifact_path.name}\"\n        pass\n    \n    def promote_model_version(self, model_name: str, version: str, \n                             target_stage: ModelStage, \n                             approval_metadata: Dict[str, Any] = None) -> bool:\n        \"\"\"\n        Promote model version to target stage with validation and approval tracking.\n        \n        Implements stage transition validation and approval workflow.\n        \"\"\"\n        # TODO 1: Fetch current model version and validate it exists\n        # TODO 2: Validate stage transition is allowed (Development->Staging->Production)\n        # TODO 3: Check if approval is required for this transition\n        # TODO 4: If promoting to Production, demote current Production version to Archived\n        # TODO 5: Update version stage in metadata store\n        # TODO 6: Record approval metadata and transition timestamp\n        # TODO 7: Return success/failure boolean\n        \n        # Hint: Use database transactions for atomic stage updates\n        # Hint: Stage transition rules: Dev->Staging->Prod->Archived\n        # Hint: Only one version per model can be in Production simultaneously\n        pass\n    \n    def search_models(self, query: Optional[str] = None, \n                     stage: Optional[ModelStage] = None,\n                     tags: Dict[str, str] = None,\n                     include_versions: bool = True) -> List[Model]:\n        \"\"\"\n        Search models with flexible filtering and optional version inclusion.\n        \n        Supports text search, stage filtering, and tag-based queries.\n        \"\"\"\n        # TODO 1: Build search criteria from parameters - handle None values gracefully  \n        # TODO 2: Query metadata store with constructed filters\n        # TODO 3: If include_versions is True, fetch all versions for each model\n        # TODO 4: Convert database results to Model domain objects\n        # TODO 5: Apply any additional filtering that can't be done at database level\n        # TODO 6: Sort results by relevance (text match quality, creation date)\n        # TODO 7: Return list of Model objects with populated versions\n        \n        # Hint: Use database ILIKE for case-insensitive text search\n        # Hint: JSONB queries for tag filtering in PostgreSQL\n        pass\n    \n    def get_model_lineage(self, model_name: str, version: str, \n                         depth: int = 3) -> Dict[str, Any]:\n        \"\"\"\n        Build lineage graph showing model dependencies up to specified depth.\n        \n        Returns graph structure with nodes (experiments, datasets, models) and edges.\n        \"\"\"\n        # TODO 1: Fetch model version and validate it exists\n        # TODO 2: Extract lineage metadata from model version record\n        # TODO 3: Build graph structure with model version as root node\n        # TODO 4: For each dependency type (experiment, dataset, parent_model), add nodes and edges\n        # TODO 5: Recursively follow parent model references up to depth limit\n        # TODO 6: Query experiment tracking for experiment run details if run_id present\n        # TODO 7: Format as graph structure: {nodes: [], edges: [], metadata: {}}\n        \n        # Hint: Use breadth-first search to control depth\n        # Hint: Track visited nodes to prevent cycles\n        # Hint: Node format: {id, type, name, metadata}, Edge format: {source, target, relationship}\n        pass\n    \n    def compare_model_versions(self, version_refs: List[tuple[str, str]], \n                              metrics: List[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Generate comparison matrix for multiple model versions across specified metrics.\n        \n        Returns statistical comparison including performance deltas and significance tests.\n        \"\"\"\n        # TODO 1: Validate all version references exist and are accessible\n        # TODO 2: Fetch metadata for all specified versions\n        # TODO 3: If metrics not specified, find common metrics across all versions\n        # TODO 4: Extract metric values and organize into comparison matrix\n        # TODO 5: Calculate statistical comparisons (mean, std, relative differences)\n        # TODO 6: Identify best/worst performing versions per metric\n        # TODO 7: Return structured comparison with summary statistics\n        \n        # Hint: Handle missing metrics gracefully (some models may not have all metrics)\n        # Hint: Return format: {versions: [], metrics: [], matrix: [][], summary: {}}\n        pass\n```\n\n**E. Language-Specific Hints**\n\n- **Database Connections**: Use connection pooling with `psycopg2.pool` for production deployments to handle concurrent requests efficiently\n- **Object Storage**: The `boto3` library works with both AWS S3 and MinIO - use environment variables for configuration flexibility\n- **JSON Handling**: PostgreSQL JSONB columns support efficient querying - use `@>` operator for containment queries and GIN indexes for performance\n- **Error Handling**: Distinguish between retriable errors (network timeouts) and permanent failures (checksum mismatches) using specific exception types\n- **Async Operations**: Consider `asyncpg` and `aioboto3` for high-concurrency deployments, especially for artifact upload/download operations\n- **Schema Validation**: Use Pydantic models for API request/response validation and automatic OpenAPI documentation generation\n\n**F. Milestone Checkpoint**\n\nAfter implementing the Model Registry component, verify the following behavior:\n\n**Testing Commands:**\n```bash\n# Run unit tests for core services\npython -m pytest tests/unit/test_registry_service.py -v\n\n# Test metadata storage operations\npython -m pytest tests/unit/test_metadata_store.py -v\n\n# Test artifact storage with MinIO\ndocker-compose up -d  # Start PostgreSQL + MinIO\npython -m pytest tests/integration/test_model_registration.py -v\n```\n\n**Expected Behavior:**\n1. **Model Registration**: Upload a test model file and verify it appears in both metadata database and object storage with correct checksums\n2. **Version Management**: Register multiple versions of the same model and verify version numbering and immutability\n3. **Stage Transitions**: Promote a model through Development → Staging → Production stages and verify only one Production version exists\n4. **Lineage Tracking**: Register a model with run_id and verify lineage information is captured and queryable\n5. **Search Functionality**: Search models by name, stage, and tags - verify filtering works correctly\n\n**Manual Verification:**\n```bash\n# Check metadata in PostgreSQL\npsql -h localhost -U postgres -d mlops -c \"SELECT * FROM model_versions ORDER BY created_at DESC LIMIT 5\"\n\n# Check artifacts in MinIO (using mc client)\nmc ls local/models/\nmc cat local/models/test-model/versions/1.0.0/model.pkl | head -c 100\n```\n\n**Signs of Problems:**\n- **Checksum Mismatches**: Usually indicates file corruption during upload/download or storage system issues\n- **Orphaned Artifacts**: Artifacts in object storage without metadata records suggest transaction rollback failures\n- **Stage Transition Failures**: Check approval workflow configuration and database constraints\n- **Lineage Query Timeouts**: May need database indexes on lineage JSONB columns or query optimization\n\n\n## Training Pipeline Orchestration\n\n> **Milestone(s):** This section primarily corresponds to Milestone 3 (Training Pipeline), which focuses on orchestrating training workflows with DAG-based execution, resource management, and fault tolerance.\n\n### Mental Model: Assembly Line\n\nThink of training pipeline orchestration like a sophisticated manufacturing assembly line. In a traditional assembly line, raw materials flow through a sequence of workstations, where each station performs a specific operation and passes the result to the next station. Workers at each station need specific tools, workspace, and skills to perform their tasks. The assembly line manager ensures materials flow smoothly, workers have the resources they need, and if one station breaks down, the entire line doesn't grind to a halt.\n\nTraining pipeline orchestration operates on the same principles but for machine learning workflows. Instead of physical materials, we have datasets, model artifacts, and intermediate computations flowing through the pipeline. Instead of workstations, we have pipeline steps like data preprocessing, feature engineering, model training, and evaluation. Instead of workers needing tools, our steps need computational resources like CPU cores, memory, and GPUs. And just like an assembly line manager, our orchestrator ensures data flows between steps, resources are allocated efficiently, and failures are handled gracefully.\n\nThe key insight from the assembly line analogy is that orchestration is fundamentally about **dependency management** and **resource coordination**. A step cannot begin until its dependencies are satisfied (materials arrive from upstream), and it cannot proceed without adequate resources (workspace and tools). The orchestrator's job is to schedule work optimally while respecting these constraints.\n\nHowever, ML pipelines have additional complexities that manufacturing assembly lines don't face. Steps may need to process data in parallel across multiple machines, some steps may fail and need to be retried, and the computational requirements can vary dramatically between steps. This is where our orchestrator becomes more sophisticated than a simple assembly line manager.\n\n### DAG Definition and Execution\n\nAt the heart of pipeline orchestration lies the **directed acyclic graph (DAG)** representation of training workflows. A DAG captures the dependencies between pipeline steps while ensuring we never have circular dependencies that would create deadlocks. Each node in the DAG represents a computational step, and each edge represents a data dependency where the output of one step becomes the input to another.\n\nThe DAG definition starts with individual pipeline steps, which are the atomic units of computation in our system. Each step encapsulates a specific piece of ML logic like data validation, feature transformation, model training, or evaluation. Steps declare their input and output schemas, resource requirements, and the container image needed to execute their code.\n\n| Step Attribute | Type | Description |\n|---|---|---|\n| step_id | str | Unique identifier for this step within the pipeline |\n| name | str | Human-readable name for the step |\n| container_image | str | Docker image containing the step's execution environment |\n| command | List[str] | Command and arguments to execute within the container |\n| inputs | Dict[str, InputSpec] | Declared input parameters and their types |\n| outputs | Dict[str, OutputSpec] | Declared output artifacts and their types |\n| resource_requirements | ResourceSpec | CPU, memory, GPU, and storage requirements |\n| retry_policy | RetryPolicy | Configuration for handling step failures |\n| timeout_seconds | Optional[int] | Maximum execution time before step is killed |\n| environment_variables | Dict[str, str] | Environment variables passed to the container |\n\nThe `InputSpec` and `OutputSpec` types define the data contracts between steps. Input specifications declare what data a step expects to receive, including the data type, validation rules, and whether the input is required or optional. Output specifications declare what artifacts a step will produce upon successful completion.\n\n| InputSpec Field | Type | Description |\n|---|---|---|\n| input_type | str | Data type: 'dataset', 'model', 'parameter', 'artifact' |\n| validation_schema | Optional[Dict] | JSON schema for validating input data structure |\n| required | bool | Whether this input must be provided for step to execute |\n| default_value | Optional[Any] | Default value used when input is not required and not provided |\n\n| OutputSpec Field | Type | Description |\n|---|---|---|\n| output_type | str | Type of artifact produced: 'dataset', 'model', 'metrics', 'artifact' |\n| path_template | str | Template for where the output artifact will be stored |\n| metadata_schema | Optional[Dict] | Expected structure of output metadata |\n\nPipeline definitions combine individual steps into a workflow by specifying the data flow connections between them. The pipeline definition is essentially a blueprint that the orchestrator uses to construct the execution DAG at runtime.\n\n| Pipeline Attribute | Type | Description |\n|---|---|---|\n| pipeline_id | str | Unique identifier for this pipeline definition |\n| version | str | Semantic version of the pipeline definition |\n| name | str | Human-readable pipeline name |\n| description | str | Documentation describing the pipeline's purpose |\n| steps | Dict[str, Step] | All steps in the pipeline keyed by step_id |\n| step_dependencies | Dict[str, List[str]] | Maps each step to its dependency steps |\n| data_flow | Dict[str, Dict[str, str]] | Maps step outputs to downstream step inputs |\n| global_parameters | Dict[str, Any] | Pipeline-level parameters available to all steps |\n| default_resources | ResourceSpec | Default resource allocation for steps that don't specify requirements |\n\nThe `data_flow` mapping is crucial for understanding how information moves through the pipeline. Each entry specifies that a particular output from one step should be passed as input to another step. For example, `data_flow[\"preprocessing\"][\"dataset\"] = \"training.input_data\"` means the \"dataset\" output from the \"preprocessing\" step becomes the \"input_data\" input for the \"training\" step.\n\n> **Key Design Principle**: Data flow connections are explicit and declarative. Steps cannot access arbitrary outputs from other steps - they can only access data that is explicitly connected through the data flow specification. This ensures pipeline behavior is predictable and makes it easier to reason about data lineage.\n\nThe orchestrator executes the pipeline by constructing an execution plan from the DAG definition. This involves several phases: dependency analysis, topological sorting, resource planning, and step scheduling.\n\n**Dependency Analysis Algorithm:**\n\n1. The orchestrator parses the `step_dependencies` and `data_flow` mappings to build a complete dependency graph\n2. It validates that the graph is acyclic by performing a depth-first search and checking for back edges\n3. It verifies that all data flow connections are valid by checking that output specifications from upstream steps match input specifications from downstream steps\n4. It identifies pipeline inputs (steps with no dependencies) and outputs (step outputs not consumed by any other step)\n5. It calculates the transitive closure of dependencies to determine which steps can potentially run in parallel\n\n**Topological Sorting for Execution Order:**\n\n1. Initialize a queue with all steps that have no unfulfilled dependencies (pipeline inputs)\n2. While the queue is not empty, remove a step and add it to the execution plan\n3. For each step that depends on the completed step, decrement its dependency count\n4. If any step's dependency count reaches zero, add it to the queue\n5. If the execution plan contains fewer steps than the original DAG, report a circular dependency error\n\n**Parallel Execution Identification:**\n\nThe orchestrator identifies opportunities for parallel execution by analyzing the dependency structure. Steps that don't depend on each other (directly or transitively) can execute simultaneously, subject to resource constraints.\n\n| Execution Phase | Description | Steps Involved |\n|---|---|---|\n| Independent Parallel | Steps with no dependencies between them | Data ingestion, parameter validation, environment setup |\n| Sequential Dependencies | Steps that must run in order due to data flow | Preprocessing → Training → Evaluation |\n| Fan-out Parallel | Multiple steps consuming output from a single upstream step | Training multiple model variants from same preprocessed data |\n| Fan-in Dependencies | Single step consuming outputs from multiple upstream steps | Model ensemble that combines predictions from multiple models |\n\n**Data Passing Between Steps:**\n\nWhen a step completes successfully, the orchestrator handles transferring its outputs to the appropriate downstream steps. This process involves artifact storage, metadata tracking, and input validation.\n\n1. **Artifact Storage**: The orchestrator uploads step outputs to the configured artifact store using a standardized path structure: `pipelines/{pipeline_id}/runs/{run_id}/steps/{step_id}/outputs/{output_name}`\n\n2. **Metadata Registration**: Each output artifact is registered with metadata including checksum, size, creation timestamp, and the step that produced it\n\n3. **Input Preparation**: For downstream steps, the orchestrator downloads required input artifacts to a local staging area and validates them against the step's input specifications\n\n4. **Environment Variable Injection**: Input artifact paths and metadata are made available to the step through environment variables following a naming convention: `MLOPS_INPUT_{INPUT_NAME}_PATH` and `MLOPS_INPUT_{INPUT_NAME}_METADATA`\n\n> **Critical Implementation Detail**: The orchestrator never passes data directly between step containers. All data exchange happens through the persistent artifact store, which provides durability guarantees and enables recovery from failures. This design trades some performance for reliability and debuggability.\n\n### Resource Allocation and Scheduling\n\nEffective resource management is essential for running training pipelines efficiently and cost-effectively. The orchestrator must allocate computational resources (CPU, memory, GPU, storage) to pipeline steps while respecting cluster capacity constraints and optimizing for throughput and cost.\n\nThe foundation of resource management is the `ResourceSpec` type, which allows pipeline authors to declare the computational requirements for each step:\n\n| ResourceSpec Field | Type | Description |\n|---|---|---|\n| cpu_cores | float | Number of CPU cores (fractional values allowed) |\n| memory_gb | float | Amount of RAM in gigabytes |\n| gpu_count | int | Number of GPU devices required |\n| gpu_type | Optional[str] | Specific GPU model if required (e.g., 'V100', 'A100') |\n| storage_gb | float | Temporary storage space in gigabytes |\n| max_duration_hours | Optional[float] | Maximum execution time for resource reservation |\n| preemptible | bool | Whether this step can use preemptible/spot instances |\n| node_selector | Dict[str, str] | Key-value pairs for node selection (e.g., zone, instance type) |\n\nThe orchestrator implements a **multi-level resource scheduling** approach that considers both immediate availability and longer-term resource optimization:\n\n**Level 1: Admission Control**\n\nBefore starting pipeline execution, the orchestrator performs admission control to determine if the pipeline can be feasibly executed given current cluster state and resource reservations.\n\n1. Calculate the total resource requirements across all pipeline steps\n2. Check if peak resource usage (when independent steps run in parallel) exceeds cluster capacity\n3. Estimate execution cost based on resource requirements and current pricing\n4. If admission control fails, queue the pipeline execution with a priority score based on user quotas and historical usage\n\n**Level 2: Step-Level Scheduling**\n\nWhen a step becomes eligible for execution (all dependencies satisfied), the orchestrator schedules it on available cluster resources:\n\n1. **Resource Matching**: Find nodes that have sufficient CPU, memory, GPU, and storage capacity for the step\n2. **Affinity Scheduling**: Prefer nodes that already have the step's container image cached to reduce startup time\n3. **Data Locality**: Consider proximity to input artifacts stored in distributed storage systems\n4. **Cost Optimization**: For non-urgent steps, prefer cheaper preemptible instances when available\n\n**Level 3: Dynamic Resource Adjustment**\n\nDuring step execution, the orchestrator monitors resource usage and can make dynamic adjustments:\n\n| Adjustment Type | Trigger Condition | Action Taken |\n|---|---|---|\n| Vertical Scaling | Memory usage exceeds 80% of allocation | Increase memory allocation if node capacity allows |\n| Early Termination | Step exceeds maximum duration | Kill step and mark as failed with timeout reason |\n| Resource Reclamation | Step uses significantly less than allocated | Release unused resources for other waiting steps |\n| Preemption Handling | Spot instance receives preemption notice | Checkpoint step state and migrate to different node |\n\n**Containerization and Isolation:**\n\nEach pipeline step executes within a containerized environment that provides process isolation, dependency management, and resource enforcement. The orchestrator integrates with container runtimes (Docker, containerd) and orchestration platforms (Kubernetes, Docker Swarm) to manage step execution.\n\n| Container Configuration | Purpose | Implementation Details |\n|---|---|---|\n| Resource Limits | Enforce CPU, memory, and GPU allocation | Uses cgroups for CPU/memory, device plugins for GPU |\n| Network Isolation | Prevent steps from accessing external services | Custom network policies and firewall rules |\n| Filesystem Isolation | Separate temporary storage per step | Mounted volumes with per-step subdirectories |\n| Environment Variables | Pass input paths and metadata to steps | Standardized variable naming convention |\n| Security Context | Run with minimal privileges | Non-root user, read-only root filesystem where possible |\n\n**Distributed Training Support:**\n\nFor training steps that require multiple nodes (distributed training), the orchestrator provides specialized scheduling capabilities:\n\n1. **Gang Scheduling**: Ensures all nodes for a distributed training job are allocated simultaneously to prevent deadlocks\n2. **Communication Setup**: Configures inter-node networking and service discovery for distributed training frameworks\n3. **Failure Handling**: Implements all-or-nothing semantics where failure of any node causes the entire distributed job to be rescheduled\n4. **Resource Homogeneity**: Ensures all nodes in a distributed training job have identical hardware configurations\n\nThe orchestrator supports multiple distributed training patterns:\n\n| Pattern | Use Case | Resource Allocation Strategy |\n|---|---|---|\n| Data Parallel | Large datasets, model fits on single GPU | Multiple nodes with identical GPU configurations |\n| Model Parallel | Large models that don't fit on single GPU | Nodes with high-bandwidth interconnect |\n| Pipeline Parallel | Sequential model layers across nodes | Nodes with balanced compute and network capacity |\n| Hybrid Parallel | Combination of above approaches | Heterogeneous allocation based on layer requirements |\n\n### Architecture Decisions\n\nThe design of the training pipeline orchestration component involves several critical architecture decisions that impact scalability, reliability, and usability. Each decision represents a trade-off between different quality attributes and operational concerns.\n\n> **Decision: Orchestration Engine Selection**\n> - **Context**: We need to choose between building a custom orchestration engine versus adapting existing workflow engines like Argo Workflows, Apache Airflow, or Kubeflow Pipelines. Custom engines offer complete control but require significant development effort, while existing engines provide proven scalability but may not fit our ML-specific requirements.\n> - **Options Considered**: \n>   1. Custom orchestrator built on Kubernetes controllers\n>   2. Argo Workflows with custom ML extensions\n>   3. Apache Airflow with ML plugins\n> - **Decision**: Build a custom orchestrator using Kubernetes controllers with pluggable execution backends\n> - **Rationale**: ML pipelines have unique requirements like artifact lineage tracking, experiment correlation, and tight integration with model registry that are difficult to achieve with general-purpose workflow engines. A custom orchestrator allows us to optimize for ML workflows while still leveraging Kubernetes for resource management and scaling.\n> - **Consequences**: Higher initial development cost but better long-term maintainability and ML-specific features. We maintain full control over execution semantics and can optimize performance for our specific use cases.\n\n| Orchestration Option | Pros | Cons |\n|---|---|---|\n| Custom K8s Controller | Full control, ML-optimized, tight integration | High development cost, maintenance burden |\n| Argo Workflows | Proven scalability, active community | General-purpose design, complex ML integration |\n| Apache Airflow | Rich ecosystem, familiar to many teams | Python-centric, not optimized for containerized ML |\n\n> **Decision: Resource Scheduling Strategy**\n> - **Context**: Pipeline steps have diverse resource requirements from lightweight data validation (100m CPU) to intensive model training (8 GPUs). We need to decide between time-sharing resources across multiple steps versus dedicating resources to single steps for their entire duration.\n> - **Options Considered**: \n>   1. Time-sharing with preemption and checkpointing\n>   2. Dedicated resource allocation per step\n>   3. Hybrid approach with different strategies per step type\n> - **Decision**: Dedicated resource allocation with optional time-sharing for eligible steps\n> - **Rationale**: ML training workloads are often GPU-intensive and don't checkpoint well, making preemption expensive. Dedicated allocation provides predictable performance and simplified failure handling. We allow opt-in time-sharing for CPU-only steps that can handle interruption.\n> - **Consequences**: Higher resource efficiency for predictable workloads but potentially lower overall cluster utilization. Simplified scheduling logic and more predictable step execution times.\n\n| Scheduling Strategy | Pros | Cons |\n|---|---|---|\n| Time-sharing | Higher resource utilization, cost efficiency | Complex checkpointing, unpredictable performance |\n| Dedicated Allocation | Predictable performance, simple failure handling | Lower utilization, higher cost |\n| Hybrid Approach | Best of both worlds for different step types | Increased complexity, configuration overhead |\n\n> **Decision: Fault Tolerance Mechanism**\n> - **Context**: Pipeline steps can fail due to infrastructure issues (node failures, network partitions), resource exhaustion (OOM, timeout), or application errors (bad data, algorithm convergence issues). We need to decide how to handle failures while maintaining pipeline correctness and avoiding wasted computation.\n> - **Options Considered**: \n>   1. Automatic retry with exponential backoff\n>   2. Checkpoint-based resumption from partial progress\n>   3. Pipeline-level rollback to last known good state\n> - **Decision**: Automatic retry with configurable policies plus optional checkpointing for long-running steps\n> - **Rationale**: Most failures are transient infrastructure issues that resolve with retry. For expensive training steps, checkpointing allows resumption without losing hours of computation. Pipeline-level rollback is too coarse-grained and wastes too much work.\n> - **Consequences**: Good balance of automatic recovery and computational efficiency. Requires step authors to implement checkpointing for long-running operations but provides significant cost savings.\n\n| Fault Tolerance Option | Pros | Cons |\n|---|---|---|\n| Automatic Retry | Simple to implement, handles transient failures | Can waste computation on persistent failures |\n| Checkpoint Resumption | Preserves expensive computation | Requires step-level implementation, storage overhead |\n| Pipeline Rollback | Simple failure model | Wastes significant computation |\n\n> **Decision: Data Passing Implementation**\n> - **Context**: Pipeline steps need to exchange datasets, trained models, and intermediate artifacts. We must decide between in-memory passing (faster but limited by node memory), shared filesystem (requires distributed FS), or object storage (durable but higher latency).\n> - **Options Considered**: \n>   1. In-memory passing through shared volumes\n>   2. Distributed filesystem (NFS, GFS) with path-based sharing\n>   3. Object storage (S3, GCS) with explicit upload/download\n> - **Decision**: Object storage with local caching for frequently accessed artifacts\n> - **Rationale**: Object storage provides durability guarantees essential for reproducibility and debugging. Local caching mitigates latency concerns for artifacts accessed multiple times. Distributed filesystems add operational complexity and failure modes.\n> - **Consequences**: Higher latency for small artifacts but better durability and debuggability. Simplified cluster setup without requiring distributed filesystem deployment.\n\n| Data Passing Option | Pros | Cons |\n|---|---|---|\n| In-memory Volumes | Low latency, simple implementation | Memory limitations, no durability |\n| Distributed Filesystem | POSIX semantics, moderate latency | Operational complexity, additional failure modes |\n| Object Storage | Durability, scalability, vendor ecosystem | Higher latency, eventual consistency issues |\n\n**Step Isolation and Security:**\n\nThe orchestrator implements multiple layers of isolation to prevent steps from interfering with each other and to enforce security boundaries:\n\n| Isolation Layer | Mechanism | Purpose |\n|---|---|---|\n| Process Isolation | Container runtime (Docker/containerd) | Prevent resource conflicts and crashes |\n| Network Isolation | Kubernetes NetworkPolicies | Prevent unauthorized communication between steps |\n| Filesystem Isolation | Per-step mounted volumes | Prevent data corruption and unauthorized access |\n| Resource Isolation | cgroups and resource quotas | Enforce resource limits and prevent noisy neighbor issues |\n| Privilege Isolation | Non-root containers, seccomp profiles | Minimize attack surface and prevent privilege escalation |\n\n**Pipeline State Management:**\n\nThe orchestrator maintains comprehensive state information about pipeline executions to support monitoring, debugging, and recovery:\n\n| State Category | Information Tracked | Storage Location |\n|---|---|---|\n| Pipeline Execution | Status, start time, completion time, resource usage | PostgreSQL metadata store |\n| Step Execution | Individual step status, logs, resource consumption | Combination of metadata store and log aggregation |\n| Artifact Lineage | Input-output relationships, checksums, storage paths | Metadata store with references to object storage |\n| Error Information | Failure reasons, stack traces, retry attempts | Structured logs in log aggregation system |\n\nThe state management system ensures that pipeline executions can be resumed after orchestrator restarts and provides complete audit trails for compliance and debugging purposes.\n\n### Implementation Guidance\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Orchestration Backend | Kubernetes Jobs with custom controller | Argo Workflows with custom CRDs |\n| Resource Scheduling | Native Kubernetes scheduler | Volcano scheduler with gang scheduling |\n| Container Runtime | Docker with containerd | CRI-O with gVisor for enhanced security |\n| Artifact Storage | MinIO (S3-compatible) | Cloud object storage (S3, GCS, Azure Blob) |\n| Metadata Storage | PostgreSQL with JSONB | PostgreSQL with TimescaleDB for metrics |\n| Message Queue | Redis with pub/sub | Apache Kafka with persistent topics |\n| Monitoring | Prometheus with Grafana | Full observability stack with tracing |\n\n**Recommended File Structure:**\n\n```\ninternal/pipeline/\n  orchestrator/\n    orchestrator.go           ← Main orchestration engine\n    dag_executor.go          ← DAG parsing and execution logic\n    resource_scheduler.go    ← Resource allocation and scheduling\n    step_executor.go         ← Individual step execution management\n    state_manager.go         ← Pipeline state persistence\n    orchestrator_test.go     ← Comprehensive test suite\n  \n  models/\n    pipeline.go              ← Pipeline definition types\n    execution.go             ← Runtime execution types\n    resources.go             ← Resource specification types\n    \n  storage/\n    artifact_manager.go      ← Artifact upload/download handling\n    metadata_store.go        ← Pipeline metadata persistence\n    \n  executor/\n    kubernetes/\n      k8s_executor.go        ← Kubernetes-based step execution\n      job_manager.go         ← Kubernetes Job lifecycle management\n    local/\n      local_executor.go      ← Local execution for development/testing\n```\n\n**Infrastructure Starter Code:**\n\n```python\n# internal/pipeline/models/pipeline.py\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Any\nfrom enum import Enum\n\nclass StepStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\" \n    SUCCEEDED = \"succeeded\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n\n@dataclass\nclass ResourceSpec:\n    cpu_cores: float = 1.0\n    memory_gb: float = 4.0\n    gpu_count: int = 0\n    gpu_type: Optional[str] = None\n    storage_gb: float = 10.0\n    max_duration_hours: Optional[float] = None\n    preemptible: bool = False\n    node_selector: Dict[str, str] = field(default_factory=dict)\n\n@dataclass \nclass InputSpec:\n    input_type: str  # 'dataset', 'model', 'parameter', 'artifact'\n    validation_schema: Optional[Dict[str, Any]] = None\n    required: bool = True\n    default_value: Optional[Any] = None\n\n@dataclass\nclass OutputSpec:\n    output_type: str  # 'dataset', 'model', 'metrics', 'artifact'\n    path_template: str\n    metadata_schema: Optional[Dict[str, Any]] = None\n\n@dataclass\nclass RetryPolicy:\n    max_attempts: int = 3\n    backoff_multiplier: float = 2.0\n    initial_delay_seconds: int = 30\n    max_delay_seconds: int = 300\n\n@dataclass\nclass Step:\n    step_id: str\n    name: str\n    container_image: str\n    command: List[str]\n    inputs: Dict[str, InputSpec] = field(default_factory=dict)\n    outputs: Dict[str, OutputSpec] = field(default_factory=dict)\n    resource_requirements: ResourceSpec = field(default_factory=ResourceSpec)\n    retry_policy: RetryPolicy = field(default_factory=RetryPolicy)\n    timeout_seconds: Optional[int] = None\n    environment_variables: Dict[str, str] = field(default_factory=dict)\n\n@dataclass\nclass Pipeline:\n    pipeline_id: str\n    version: str\n    name: str\n    description: str\n    steps: Dict[str, Step] = field(default_factory=dict)\n    step_dependencies: Dict[str, List[str]] = field(default_factory=dict)\n    data_flow: Dict[str, Dict[str, str]] = field(default_factory=dict)\n    global_parameters: Dict[str, Any] = field(default_factory=dict)\n    default_resources: ResourceSpec = field(default_factory=ResourceSpec)\n\n@dataclass\nclass StepExecution:\n    step_id: str\n    pipeline_run_id: str\n    status: StepStatus\n    start_time: Optional[float] = None\n    end_time: Optional[float] = None\n    attempt_count: int = 0\n    error_message: Optional[str] = None\n    resource_usage: Dict[str, float] = field(default_factory=dict)\n    node_name: Optional[str] = None\n    pod_name: Optional[str] = None\n```\n\n**Core Logic Skeleton:**\n\n```python\n# internal/pipeline/orchestrator/dag_executor.py\nfrom typing import Dict, List, Set, Tuple\nfrom ..models.pipeline import Pipeline, Step, StepExecution\n\nclass DAGExecutor:\n    \"\"\"Analyzes pipeline DAGs and coordinates step execution.\"\"\"\n    \n    def __init__(self, artifact_store, metadata_store):\n        self.artifact_store = artifact_store\n        self.metadata_store = metadata_store\n    \n    def validate_pipeline_dag(self, pipeline: Pipeline) -> List[str]:\n        \"\"\"Validates pipeline DAG structure and returns list of validation errors.\n        \n        Returns empty list if pipeline is valid.\n        \"\"\"\n        # TODO 1: Build adjacency list from step_dependencies\n        # TODO 2: Check for circular dependencies using DFS with visit tracking\n        # TODO 3: Validate that all data_flow connections reference valid steps and outputs\n        # TODO 4: Verify input/output type compatibility for connected steps\n        # TODO 5: Check that all required inputs have either connections or default values\n        # Hint: Use three-color DFS (white/gray/black) to detect cycles\n        pass\n    \n    def compute_execution_order(self, pipeline: Pipeline) -> List[List[str]]:\n        \"\"\"Computes execution order as list of step groups that can run in parallel.\n        \n        Returns list where each inner list contains step_ids that can execute concurrently.\n        \"\"\"\n        # TODO 1: Create dependency count map for each step\n        # TODO 2: Initialize ready queue with steps that have zero dependencies\n        # TODO 3: While ready queue not empty, collect all ready steps as parallel group\n        # TODO 4: For each completed step, decrement dependency counts of downstream steps\n        # TODO 5: Add newly ready steps to queue for next iteration\n        # TODO 6: Return list of parallel execution groups\n        pass\n    \n    def prepare_step_inputs(self, step_id: str, pipeline: Pipeline, \n                          completed_steps: Dict[str, StepExecution]) -> Dict[str, str]:\n        \"\"\"Downloads required input artifacts and returns environment variables for step.\n        \n        Returns dict of environment variables to pass to step container.\n        \"\"\"\n        # TODO 1: Look up required inputs from step definition\n        # TODO 2: For each input, find the upstream step that produces it via data_flow\n        # TODO 3: Download artifact from artifact_store using upstream step's output path\n        # TODO 4: Validate downloaded artifact against input specification\n        # TODO 5: Create environment variables with input paths and metadata\n        # TODO 6: Handle default values for optional inputs that aren't connected\n        # Hint: Use naming convention MLOPS_INPUT_{INPUT_NAME}_PATH for env vars\n        pass\n    \n    def handle_step_completion(self, step_execution: StepExecution, \n                             pipeline: Pipeline) -> None:\n        \"\"\"Processes step completion by uploading outputs and updating metadata.\n        \n        Updates step execution record and handles artifact storage.\n        \"\"\"\n        # TODO 1: Read step outputs from container's output directory\n        # TODO 2: Validate outputs against step's output specifications  \n        # TODO 3: Upload artifacts to artifact_store with standardized paths\n        # TODO 4: Compute checksums and file metadata for each output\n        # TODO 5: Update step execution record with completion status and metadata\n        # TODO 6: Trigger downstream step eligibility check\n        pass\n```\n\n**Kubernetes Integration Starter Code:**\n\n```python\n# internal/pipeline/executor/kubernetes/k8s_executor.py\nfrom kubernetes import client, config\nimport yaml\nfrom typing import Dict, Optional\nfrom ...models.pipeline import Step, ResourceSpec, StepExecution\n\nclass KubernetesStepExecutor:\n    \"\"\"Executes pipeline steps as Kubernetes Jobs.\"\"\"\n    \n    def __init__(self):\n        try:\n            config.load_incluster_config()  # Running inside cluster\n        except:\n            config.load_kube_config()  # Development environment\n        \n        self.batch_v1 = client.BatchV1Api()\n        self.core_v1 = client.CoreV1Api()\n    \n    def create_job_spec(self, step: Step, step_execution: StepExecution,\n                       environment_vars: Dict[str, str]) -> Dict:\n        \"\"\"Creates Kubernetes Job specification for pipeline step.\"\"\"\n        \n        # Convert resource requirements to Kubernetes format\n        resources = {\n            \"requests\": {\n                \"cpu\": f\"{step.resource_requirements.cpu_cores}\",\n                \"memory\": f\"{step.resource_requirements.memory_gb}Gi\"\n            },\n            \"limits\": {\n                \"cpu\": f\"{step.resource_requirements.cpu_cores}\",\n                \"memory\": f\"{step.resource_requirements.memory_gb}Gi\"\n            }\n        }\n        \n        if step.resource_requirements.gpu_count > 0:\n            resources[\"requests\"][\"nvidia.com/gpu\"] = step.resource_requirements.gpu_count\n            resources[\"limits\"][\"nvidia.com/gpu\"] = step.resource_requirements.gpu_count\n        \n        # Build environment variable list\n        env_vars = []\n        for key, value in environment_vars.items():\n            env_vars.append({\"name\": key, \"value\": value})\n        for key, value in step.environment_variables.items():\n            env_vars.append({\"name\": key, \"value\": value})\n        \n        job_spec = {\n            \"apiVersion\": \"batch/v1\",\n            \"kind\": \"Job\", \n            \"metadata\": {\n                \"name\": f\"mlops-step-{step.step_id}-{step_execution.pipeline_run_id}\",\n                \"labels\": {\n                    \"app\": \"mlops-pipeline\",\n                    \"step-id\": step.step_id,\n                    \"pipeline-run-id\": step_execution.pipeline_run_id\n                }\n            },\n            \"spec\": {\n                \"backoffLimit\": step.retry_policy.max_attempts - 1,\n                \"template\": {\n                    \"spec\": {\n                        \"restartPolicy\": \"Never\",\n                        \"containers\": [{\n                            \"name\": \"step-container\",\n                            \"image\": step.container_image,\n                            \"command\": step.command,\n                            \"env\": env_vars,\n                            \"resources\": resources,\n                            \"volumeMounts\": [{\n                                \"name\": \"artifact-storage\",\n                                \"mountPath\": \"/mlops/artifacts\"\n                            }]\n                        }],\n                        \"volumes\": [{\n                            \"name\": \"artifact-storage\",\n                            \"emptyDir\": {\"sizeLimit\": f\"{step.resource_requirements.storage_gb}Gi\"}\n                        }]\n                    }\n                }\n            }\n        }\n        \n        # Add node selector if specified\n        if step.resource_requirements.node_selector:\n            job_spec[\"spec\"][\"template\"][\"spec\"][\"nodeSelector\"] = step.resource_requirements.node_selector\n        \n        # Add timeout if specified\n        if step.timeout_seconds:\n            job_spec[\"spec\"][\"activeDeadlineSeconds\"] = step.timeout_seconds\n            \n        return job_spec\n    \n    def submit_job(self, job_spec: Dict) -> str:\n        \"\"\"Submits job to Kubernetes and returns job name.\"\"\"\n        response = self.batch_v1.create_namespaced_job(\n            namespace=\"default\",\n            body=job_spec\n        )\n        return response.metadata.name\n    \n    def get_job_status(self, job_name: str) -> Dict:\n        \"\"\"Gets current status of Kubernetes job.\"\"\"\n        job = self.batch_v1.read_namespaced_job_status(\n            name=job_name,\n            namespace=\"default\"\n        )\n        return {\n            \"active\": job.status.active or 0,\n            \"succeeded\": job.status.succeeded or 0,\n            \"failed\": job.status.failed or 0,\n            \"conditions\": job.status.conditions or []\n        }\n```\n\n**Language-Specific Implementation Hints:**\n\n- Use `asyncio` for concurrent step monitoring and execution in Python\n- Implement exponential backoff using `tenacity` library for retry logic  \n- Use `kubernetes-python` client library for Kubernetes API interactions\n- Store pipeline state in PostgreSQL using `asyncpg` for async database access\n- Use `aiohttp` for non-blocking HTTP requests to artifact storage APIs\n- Implement circuit breakers using `aiobreaker` for external service calls\n- Use structured logging with `structlog` for correlation IDs and request tracing\n\n**Milestone Checkpoint:**\n\nAfter implementing the training pipeline orchestration:\n\n1. **Unit Tests**: Run `python -m pytest tests/pipeline/ -v` to verify core logic\n2. **Integration Test**: Create a simple 3-step pipeline (validate → preprocess → train) and verify it executes correctly with proper data flow\n3. **Expected Behavior**: \n   - Pipeline validates DAG structure and rejects circular dependencies\n   - Steps execute in correct dependency order with parallel execution where possible\n   - Artifacts flow correctly between steps through object storage\n   - Failed steps retry according to configured policies\n   - Resource limits are enforced at the Kubernetes level\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---|---|---|\n| Steps hang in pending state | Resource constraints or node selector mismatch | Check `kubectl get pods` and node capacity | Adjust resource requirements or node selectors |\n| Data flow failures between steps | Artifact path mismatch or corrupted upload | Examine artifact store logs and checksums | Verify output path templates and artifact validation |\n| Pipeline never completes | Circular dependency in DAG | Run DAG validation with detailed error logging | Fix step dependencies to create valid DAG |\n| High memory usage in orchestrator | Large pipeline state or insufficient garbage collection | Monitor orchestrator memory usage and state size | Implement state cleanup and optimize data structures |\n| Inconsistent pipeline execution | Race conditions in step scheduling | Add correlation IDs and trace execution order | Implement proper locking around shared state |\n\n![Training Pipeline Execution](./diagrams/pipeline-execution.svg)\n\n\n## Model Deployment Component\n\n> **Milestone(s):** This section primarily corresponds to Milestone 4 (Model Deployment), which focuses on deploying models to production as HTTP endpoints with traffic management, canary releases, and auto-scaling. This section also establishes the foundation for Milestone 5 (Model Monitoring) by implementing the prediction logging and traffic routing infrastructure.\n\nThe **model deployment component** transforms registered model versions from the Model Registry into scalable, production-ready inference endpoints. Think of this component as the bridge between the experimental world of model training and the demanding requirements of production systems, where millisecond latencies, high availability, and seamless updates determine business success.\n\n### Mental Model: Restaurant Service\n\nUnderstanding model deployment through restaurant service analogies helps build intuition for the complex orchestration required to serve models at scale. Consider how a successful restaurant must balance quality, speed, capacity, and customer satisfaction while introducing new menu items without disrupting ongoing service.\n\n**The Kitchen as Model Serving Infrastructure**: Just as a restaurant kitchen contains different stations (grill, sauté, pastry) optimized for specific dishes, model serving infrastructure contains specialized inference servers (TensorFlow Serving, TorchServe, Triton) optimized for different model frameworks and use cases. Each station has specific equipment, trained staff, and procedures - similarly, each inference server has optimized runtimes, memory management, and preprocessing pipelines tuned for its target model types.\n\n**Menu Items as Model Versions**: Restaurant menu items represent different model versions available for serving. Just as a restaurant might offer both the classic burger (stable, proven) and a seasonal special (new, experimental), production systems serve stable model versions alongside newer variants being evaluated. Each menu item has preparation instructions, ingredient requirements, and expected preparation time - model versions have inference code, resource requirements, and latency profiles.\n\n**Order Flow as Request Processing**: When customers place orders, the restaurant's order management system routes requests to appropriate kitchen stations, manages preparation queues, and coordinates delivery timing. Similarly, model deployment systems route inference requests to appropriate model instances, manage request batching for efficiency, and coordinate response aggregation when using ensemble approaches.\n\n**Quality Control as Prediction Validation**: Restaurants implement quality control checkpoints - temperature checks for food safety, taste testing for consistency, presentation review before serving. Model deployment systems implement analogous validation - input schema validation, prediction confidence thresholds, output format verification, and anomaly detection before returning results to clients.\n\n**Introducing New Dishes as Canary Deployments**: When restaurants introduce new menu items, they often start with limited availability - offering the new dish to select customers or during specific hours to gather feedback without risking the entire operation. This mirrors canary deployment strategies where new model versions serve a small percentage of production traffic while monitoring performance metrics. If the new dish receives positive feedback, it becomes a regular menu item; if customers complain, it's quickly withdrawn. Similarly, successful canary deployments gradually increase traffic allocation, while problematic deployments trigger automatic rollbacks.\n\n**Kitchen Capacity Management as Auto-Scaling**: Restaurants adjust staffing and station capacity based on expected demand - adding cooks during rush hours, opening additional grilling stations for burger-heavy periods, preparing mise en place during slow periods. Model deployment systems implement similar auto-scaling logic, monitoring request queues and response latencies to determine when additional model instances are needed, and scaling down during low-traffic periods to optimize resource costs.\n\n**Service Quality Monitoring as Performance Tracking**: Successful restaurants continuously monitor service quality - order fulfillment times, customer satisfaction scores, ingredient freshness, equipment performance. They establish alert systems for critical issues (kitchen fire, equipment breakdown, food safety violations) that require immediate intervention. Model deployment systems implement comprehensive monitoring for inference latency, prediction accuracy, error rates, and resource utilization, with alert systems for degraded performance or system failures that threaten service availability.\n\n### Model Serving and Scaling\n\nModel serving transforms static artifacts from the Model Registry into dynamic, responsive HTTP endpoints capable of handling production inference workloads. This process involves multiple sophisticated subsystems working together to optimize for latency, throughput, and resource efficiency while maintaining prediction quality and system reliability.\n\n**Inference Server Integration** provides the foundation for model serving by wrapping trained models in high-performance runtime environments optimized for production inference workloads. The deployment component integrates with specialized inference servers rather than implementing model execution directly, leveraging years of optimization work in frameworks like TensorFlow Serving, TorchServe, NVIDIA Triton, and MLflow's built-in serving capabilities.\n\nThe integration architecture uses a **serving endpoint abstraction** that encapsulates inference server specifics behind a common interface. When deploying a model version, the system examines the model's metadata to determine the appropriate inference server, generates server-specific configuration files, and manages the server lifecycle. For TensorFlow models, this involves creating SavedModel bundles and TensorFlow Serving configuration files specifying input/output tensor specifications. For PyTorch models, the system generates TorchServe model archives (MAR files) with custom preprocessing and postprocessing handlers when needed.\n\n| Inference Server | Model Types | Optimization Features | Integration Method |\n|------------------|-------------|----------------------|-------------------|\n| TensorFlow Serving | TensorFlow, Keras | Dynamic batching, GPU optimization, version management | REST API + gRPC, model repository mounting |\n| TorchServe | PyTorch, TorchScript | Multi-worker inference, custom handlers, metrics | REST management API, model store integration |\n| NVIDIA Triton | Multi-framework | Dynamic batching, model ensembles, backend optimization | HTTP/gRPC inference, model repository |\n| MLflow Serving | Scikit-learn, custom | Unified interface, environment management | REST API, conda environment packaging |\n| ONNX Runtime | ONNX models | Cross-platform optimization, hardware acceleration | Python API wrapper, optimized execution providers |\n\n**Model Loading and Initialization** represents a critical optimization point where the system balances startup time against memory efficiency. The deployment component implements **model warming strategies** that preload models into memory and execute initial inference requests to trigger JIT compilation and cache population. This prevents the \"cold start\" problem where the first production requests experience dramatically higher latency due to model loading overhead.\n\nThe model loading process follows a structured sequence: First, the deployment controller downloads the model artifact from the Model Registry's artifact store, verifying checksums to ensure integrity. Next, it extracts the model files into the inference server's expected directory structure, applying any framework-specific transformations. The inference server then loads the model into memory, allocating GPU resources if specified in the model's resource requirements. Finally, the warming process sends synthetic inference requests through the model to trigger any lazy initialization and populate caches.\n\n**Auto-Scaling Policies** enable model serving endpoints to adapt to changing demand patterns while optimizing for both performance and cost. The auto-scaling system monitors multiple metrics simultaneously and makes scaling decisions based on configurable thresholds and policies that account for the unique characteristics of ML inference workloads.\n\nThe auto-scaling controller tracks **request queue depth** as the primary indicator of insufficient capacity. Unlike traditional web services where CPU utilization is often the primary metric, ML inference endpoints can become bottlenecked on specialized resources like GPU memory or model-specific preprocessing pipelines. The queue depth metric captures these bottlenecks regardless of their underlying cause - if requests are waiting longer than acceptable thresholds, additional capacity is needed.\n\n**Latency percentile tracking** provides a quality-oriented scaling trigger that ensures user experience remains within acceptable bounds. The system monitors P95 and P99 response latencies, triggering scale-up when these percentiles exceed configured thresholds. This approach prevents the degraded user experience that can occur when average latency appears acceptable but tail latencies become problematic.\n\n| Scaling Metric | Scale-Up Threshold | Scale-Down Threshold | Evaluation Window | Rationale |\n|----------------|-------------------|---------------------|------------------|-----------|\n| Request Queue Depth | > 10 requests | < 2 requests | 1 minute | Direct capacity indicator regardless of bottleneck type |\n| P95 Latency | > 200ms | < 100ms | 2 minutes | User experience quality maintenance |\n| P99 Latency | > 500ms | < 250ms | 3 minutes | Tail latency protection for critical requests |\n| GPU Memory Usage | > 80% | < 40% | 30 seconds | Hardware resource constraint prevention |\n| Request Rate | > 100 RPS | < 20 RPS | 5 minutes | Predictive scaling based on traffic patterns |\n\n**Performance Optimization** within model serving encompasses multiple layers of the inference pipeline, from request preprocessing through model execution to response serialization. The deployment component implements several optimization strategies that can significantly improve throughput and reduce latency without requiring changes to the underlying models.\n\n**Dynamic request batching** groups multiple inference requests together to leverage vectorized operations and GPU parallelism. The batching system balances batch size against latency requirements - larger batches improve GPU utilization but increase waiting time for requests. The optimization algorithm considers the model's batch processing characteristics, available hardware resources, and latency SLAs to determine optimal batch sizes and timeout policies.\n\n**Result caching** stores inference results for repeated inputs, particularly valuable for models that frequently receive identical or similar requests. The caching system implements intelligent cache key generation that accounts for input features while handling floating-point precision issues. Cache eviction policies prioritize frequently accessed results while ensuring cache size remains within memory constraints.\n\n**Model compilation optimizations** leverage framework-specific compilation techniques to accelerate inference execution. For TensorFlow models, this includes TensorRT optimization for NVIDIA GPUs and XLA compilation for CPU and TPU workloads. For PyTorch models, the system applies TorchScript compilation and potentially ONNX conversion for cross-platform optimization.\n\n> **Key Insight**: Model serving optimization requires understanding the specific computational patterns of ML inference workloads, which differ significantly from traditional web services. While web services are often I/O bound and benefit from connection pooling and caching, ML inference is typically compute-bound with predictable processing patterns that benefit from batching and specialized hardware acceleration.\n\n### Traffic Management and Rollouts\n\nTraffic management enables safe, controlled deployment of new model versions while maintaining service availability and providing mechanisms for rapid rollback when issues arise. The traffic management subsystem implements sophisticated routing logic that supports multiple deployment strategies, each optimized for different risk profiles and operational requirements.\n\n![Deployment Traffic Management](./diagrams/deployment-traffic.svg)\n\n**Blue-Green Deployments** provide the safest approach for model updates by maintaining two complete, identical production environments and switching traffic atomically between them. In the context of model serving, blue-green deployments mean running both the current model version and the new model version on separate infrastructure, then redirecting all traffic from the blue environment to the green environment instantaneously.\n\nThe blue-green implementation maintains **environment isolation** to prevent any interference between model versions during the transition period. Each environment has dedicated inference server instances, separate monitoring dashboards, and independent resource allocations. This isolation ensures that performance testing of the green environment doesn't impact blue environment performance, and any issues in the green environment can't affect current production traffic.\n\nThe **traffic switch mechanism** uses a load balancer configuration update to redirect all incoming requests from the blue environment to the green environment. The deployment controller implements this switch as an atomic operation - it updates the load balancer's target group configuration and waits for the configuration propagation to complete before considering the deployment successful. During the brief propagation period (typically 10-30 seconds), some requests may still route to the blue environment, but no requests are lost.\n\n**Canary Release Strategy** provides a middle ground between the safety of blue-green deployments and the resource efficiency of in-place updates. Canary releases gradually shift traffic from the current model version to the new version while continuously monitoring performance metrics and automatically rolling back if degradation is detected.\n\nThe canary implementation defines **traffic splitting policies** that specify what percentage of requests should route to the new model version at each stage of the rollout. A typical canary progression might start with 5% traffic to the new version, then increase to 25%, 50%, 75%, and finally 100% as confidence in the new version grows. Each stage includes automatic validation gates that must pass before proceeding to the next stage.\n\n| Canary Stage | Traffic Percentage | Duration | Validation Criteria | Rollback Triggers |\n|--------------|-------------------|----------|--------------------|--------------------|\n| Initial | 5% | 10 minutes | Error rate < 0.1%, P95 latency increase < 10% | Error rate > 0.5%, latency increase > 50% |\n| Expansion | 25% | 30 minutes | Accuracy within 2% of baseline, no alerts fired | Error rate > 0.2%, accuracy drop > 5% |\n| Majority | 50% | 60 minutes | Full metric validation, A/B test significance | Any metric degrades beyond threshold |\n| Near-Complete | 75% | 30 minutes | Final validation before full rollout | Last chance for manual intervention |\n| Complete | 100% | Ongoing | Continuous monitoring for delayed issues | Standard production alerting |\n\n**A/B Testing Framework** extends traffic splitting to support controlled experiments where different model versions serve production traffic simultaneously for statistical comparison of business metrics. Unlike canary deployments that aim to replace the old version with the new version, A/B tests maintain traffic splits for extended periods to gather sufficient data for statistical significance.\n\nThe A/B testing system implements **experiment assignment logic** that ensures consistent user experiences by routing users to the same model version throughout their session or analysis period. This consistency prevents confusing user experiences where prediction behavior changes unexpectedly and ensures clean statistical analysis by avoiding cross-contamination between test groups.\n\n**Statistical significance monitoring** tracks the key metrics for each model version and calculates confidence intervals to determine when sufficient evidence exists to declare a winner. The system implements sequential testing procedures that can detect statistically significant differences as early as possible while controlling for multiple comparisons and ensuring adequate statistical power.\n\n**Traffic Routing Implementation** serves as the foundation for all traffic management strategies by implementing intelligent request routing logic that can direct requests to specific model versions based on various criteria including traffic split percentages, user attributes, request characteristics, and real-time performance metrics.\n\nThe routing system maintains **routing configuration state** that specifies how traffic should be distributed across available model versions. This configuration includes traffic split percentages, routing rules based on request headers or user attributes, geographic routing preferences, and fallback policies for handling failures. The configuration updates propagate to all load balancers and API gateways within seconds to ensure consistent routing behavior.\n\n**Session affinity and consistency** ensure that requests from the same user or session route to the same model version to prevent inconsistent prediction behavior. The system implements this through consistent hashing algorithms that map user identifiers to model versions, ensuring that adding or removing model versions doesn't disrupt existing user assignments more than necessary.\n\n> **Decision: Traffic Management Strategy**\n> - **Context**: Need to balance deployment safety, resource efficiency, and operational complexity when rolling out new model versions to production traffic\n> - **Options Considered**: \n>   - Blue-green deployments with atomic switching\n>   - Canary releases with gradual traffic shifting  \n>   - In-place updates with rolling restarts\n> - **Decision**: Implement canary releases as the primary deployment strategy with blue-green available for high-risk updates\n> - **Rationale**: Canary releases provide excellent safety through gradual rollout and automatic rollback while being more resource-efficient than maintaining full duplicate environments. Blue-green deployments are available as an option for critical updates where maximum safety is required.\n> - **Consequences**: Requires sophisticated traffic routing logic and comprehensive monitoring, but provides optimal balance of safety and efficiency for most model updates\n\n### Architecture Decisions\n\nThe architecture decisions for model deployment reflect the complex trade-offs between performance, reliability, cost, and operational complexity inherent in production ML systems. These decisions establish the foundation for scalable, maintainable deployment infrastructure that can evolve with changing requirements.\n\n> **Decision: Inference Server Selection Strategy**\n> - **Context**: Need to support multiple ML frameworks and model types while optimizing for performance and operational simplicity\n> - **Options Considered**:\n>   - Single inference server (e.g., only TensorFlow Serving) with framework conversion\n>   - Framework-specific servers (TensorFlow Serving, TorchServe, etc.) with routing logic\n>   - Universal serving platform (e.g., NVIDIA Triton) for all model types\n> - **Decision**: Framework-specific inference servers with intelligent routing based on model metadata\n> - **Rationale**: Framework-specific servers provide optimal performance for each model type and leverage extensive optimization work by framework teams. Conversion between frameworks often introduces performance penalties and compatibility issues. Universal platforms add complexity and may not optimize well for specific use cases.\n> - **Consequences**: Requires maintaining expertise in multiple inference servers and more complex deployment logic, but delivers superior performance and maintains framework-specific optimizations\n\n| Option | Performance | Operational Complexity | Framework Support | Chosen |\n|--------|-------------|----------------------|-------------------|--------|\n| Single inference server | Medium | Low | Limited (requires conversion) | No |\n| Framework-specific servers | High | Medium | Native for each framework | **Yes** |\n| Universal serving platform | Medium-High | Medium | Good but not native | No |\n\n> **Decision: Auto-Scaling Metrics and Policies**\n> - **Context**: ML inference workloads have different scaling characteristics than traditional web services due to GPU resource constraints and variable processing times\n> - **Options Considered**:\n>   - CPU/memory-based scaling like traditional web services\n>   - Request queue depth and latency percentiles\n>   - Predictive scaling based on historical patterns\n> - **Decision**: Multi-metric scaling using request queue depth, latency percentiles, and resource utilization with predictive elements\n> - **Rationale**: CPU/memory metrics don't capture GPU bottlenecks or model-specific performance characteristics. Queue depth provides immediate capacity indicators while latency percentiles ensure user experience quality. Predictive elements help handle traffic spikes proactively.\n> - **Consequences**: Requires more sophisticated monitoring infrastructure and tuning, but provides better scaling behavior for ML workloads\n\n> **Decision: Deployment Strategy Framework**\n> - **Context**: Different model updates have different risk profiles and require different deployment approaches\n> - **Options Considered**:\n>   - Single deployment strategy for all updates\n>   - Manual selection of deployment strategy per update\n>   - Automatic strategy selection based on model metadata and change analysis\n> - **Decision**: Configurable deployment strategies with intelligent defaults based on model risk assessment\n> - **Rationale**: Risk profiles vary significantly between model updates. Minor parameter updates may be safe for in-place deployment, while new model architectures require careful canary rollouts. Intelligent defaults reduce operational burden while allowing override for special cases.\n> - **Consequences**: Requires developing risk assessment heuristics and maintaining multiple deployment code paths, but provides optimal safety and efficiency trade-offs\n\n**Resource Allocation and Scheduling Architecture** determines how computational resources are assigned to model serving instances across the infrastructure. This decision impacts cost efficiency, performance predictability, and the ability to handle varying workloads.\n\nThe resource allocation system implements **multi-dimensional resource scheduling** that considers CPU cores, memory, GPU devices, and storage requirements when placing model serving instances. Unlike traditional web services that primarily consume CPU and memory, ML inference often requires specialized resources like GPUs with specific memory capacities or tensor processing units with particular performance characteristics.\n\n**Resource isolation mechanisms** prevent interference between different model serving instances running on shared infrastructure. The system uses containerization with resource limits to ensure that one model's resource consumption doesn't impact other models' performance. For GPU resources, the system implements GPU sharing strategies when appropriate or dedicates entire GPU devices when models require exclusive access.\n\n**Cost optimization strategies** balance performance requirements against infrastructure costs by implementing intelligent resource packing and scheduling policies. The system considers the cost implications of different resource allocation decisions, preferring to pack compatible workloads onto shared resources when performance requirements allow, while ensuring that performance-critical models receive dedicated resources when needed.\n\n| Resource Type | Allocation Strategy | Isolation Method | Sharing Policy | Cost Optimization |\n|---------------|-------------------|------------------|----------------|-------------------|\n| CPU Cores | Proportional share with limits | cgroups CPU limits | Multiple models per node | Pack by CPU efficiency |\n| Memory | Dedicated allocation with swap disabled | cgroups memory limits | Calculated based on model size | Minimize memory waste |\n| GPU Devices | Exclusive or shared based on model requirements | NVIDIA MPS or dedicated allocation | Single model for large models, shared for small | Maximize GPU utilization |\n| Storage | Shared model cache with dedicated scratch space | Volume mounts and quotas | Shared read-only, dedicated write | Deduplication and compression |\n\n**Monitoring and Observability Integration** establishes the foundation for understanding model serving performance and detecting issues before they impact users. The architecture integrates monitoring throughout the serving pipeline to provide comprehensive visibility into system behavior.\n\n**Metric collection strategies** gather performance data at multiple levels including infrastructure metrics (CPU, memory, GPU utilization), application metrics (request latency, throughput, error rates), and ML-specific metrics (prediction confidence, feature distribution statistics). The system implements structured logging with correlation IDs that enable tracing individual requests through the entire serving pipeline.\n\n**Alert escalation policies** define how different types of issues are handled, from automated responses for common problems to immediate human escalation for critical failures. The system implements intelligent alerting that considers context and severity when determining appropriate response actions.\n\n### Common Pitfalls in Model Deployment\n\n⚠️ **Pitfall: Cold Start Performance Issues**\nMany developers underestimate the model loading and initialization time required when scaling up model serving instances. They assume that spinning up new instances provides immediate capacity, but models often require significant time to load into memory, compile optimizations, and warm up caches. This leads to degraded performance during scaling events and poor user experience during traffic spikes. The fix involves implementing proper model warming strategies, maintaining warm spare instances during anticipated load increases, and using predictive scaling to start instance preparation before capacity is urgently needed.\n\n⚠️ **Pitfall: Inadequate Resource Specification**\nTeams frequently specify insufficient or inappropriate resource requirements for model serving instances, leading to out-of-memory errors, GPU resource contention, or poor performance. This happens because resource requirements often differ significantly between training and inference workloads, and developers may not account for framework overhead, concurrent request processing, or peak memory usage during batch processing. The solution involves thorough performance testing with realistic traffic patterns, monitoring resource utilization during normal and peak loads, and implementing resource request optimization based on observed usage patterns.\n\n⚠️ **Pitfall: Unsafe Traffic Routing During Deployments**\nDevelopers often implement traffic routing logic that can lose requests or route them to unavailable model versions during deployment transitions. This occurs when routing configuration updates aren't atomic, when health checks don't properly validate model readiness, or when rollback procedures don't account for in-flight requests. The fix requires implementing proper health check endpoints that verify model loading and readiness, using atomic configuration updates for traffic routing, implementing graceful shutdown procedures that allow in-flight requests to complete, and testing rollback scenarios under load.\n\n⚠️ **Pitfall: Missing Model Compatibility Validation**\nTeams frequently deploy model versions without validating that they're compatible with the existing serving infrastructure and client expectations. This leads to runtime errors when input schemas change, output formats differ from client expectations, or model versions require different preprocessing pipelines. The solution involves implementing comprehensive compatibility testing that validates input/output schemas, response format consistency, and integration with upstream and downstream systems before deployment.\n\n⚠️ **Pitfall: Inadequate Performance Testing**\nMany deployment implementations lack sufficient performance testing under realistic conditions, leading to surprising performance degradation or failures when models encounter production traffic patterns. This happens because development testing often uses synthetic data that doesn't match production characteristics, testing doesn't account for concurrent request processing, or testing environments don't match production infrastructure specifications. The fix involves implementing comprehensive performance testing with production-like data distributions, realistic concurrency levels, and infrastructure that matches production specifications.\n\n### Implementation Guidance\n\nThis implementation guidance provides concrete code examples and infrastructure templates for building a production-ready model deployment system. The focus is on creating maintainable, scalable code that implements the architectural decisions and strategies described above.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Load Balancer | HAProxy with configuration files | NGINX Plus with dynamic configuration API |\n| Container Orchestration | Docker Compose with health checks | Kubernetes with custom operators |\n| Inference Servers | MLflow serving for all models | Framework-specific (TF Serving, TorchServe, Triton) |\n| Monitoring | Prometheus + Grafana | Full observability stack (Jaeger, Prometheus, Grafana) |\n| Traffic Management | Simple round-robin routing | Istio service mesh with advanced traffic policies |\n| Auto-scaling | Basic threshold-based scaling | Kubernetes HPA with custom metrics |\n\n#### Recommended File Structure\n\n```\ndeployment/\n├── deployment_controller.py      ← Main deployment orchestration logic\n├── traffic_manager.py           ← Traffic routing and canary management\n├── serving_backend.py           ← Inference server integration\n├── auto_scaler.py              ← Auto-scaling policies and execution\n├── health_monitor.py           ← Health checking and readiness validation\n├── config/\n│   ├── serving_templates/      ← Server-specific config templates\n│   └── deployment_policies.yaml ← Default deployment strategies\n├── infrastructure/\n│   ├── kubernetes/             ← K8s manifests and operators\n│   ├── docker/                ← Container definitions and scripts\n│   └── monitoring/             ← Monitoring configuration\n└── tests/\n    ├── integration/            ← End-to-end deployment tests\n    └── performance/            ← Load testing and benchmarks\n```\n\n#### Infrastructure Starter Code\n\n**Complete Health Monitoring System:**\n\n```python\nimport time\nimport asyncio\nimport logging\nfrom typing import Dict, List, Optional, Callable\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport aiohttp\n\nclass HealthStatus(Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass HealthCheck:\n    name: str\n    status: HealthStatus\n    message: str\n    timestamp: float\n    details: Dict[str, any]\n\nclass ModelServingHealthMonitor:\n    \"\"\"Comprehensive health monitoring for model serving instances.\"\"\"\n    \n    def __init__(self, check_interval: int = 30):\n        self.check_interval = check_interval\n        self.health_checks: Dict[str, Callable] = {}\n        self.last_results: Dict[str, HealthCheck] = {}\n        self.running = False\n        \n    def register_check(self, name: str, check_func: Callable) -> None:\n        \"\"\"Register a health check function.\"\"\"\n        self.health_checks[name] = check_func\n        \n    async def check_model_endpoint(self, endpoint_url: str, model_name: str) -> HealthCheck:\n        \"\"\"Check if model serving endpoint is responding correctly.\"\"\"\n        try:\n            async with aiohttp.ClientSession() as session:\n                # Health check endpoint\n                health_url = f\"{endpoint_url}/health\"\n                async with session.get(health_url, timeout=5) as response:\n                    if response.status == 200:\n                        health_data = await response.json()\n                        return HealthCheck(\n                            name=f\"endpoint_{model_name}\",\n                            status=HealthStatus.HEALTHY,\n                            message=\"Endpoint responding\",\n                            timestamp=time.time(),\n                            details={\"response_time\": response.headers.get('X-Response-Time', 'unknown')}\n                        )\n                    else:\n                        return HealthCheck(\n                            name=f\"endpoint_{model_name}\",\n                            status=HealthStatus.UNHEALTHY,\n                            message=f\"HTTP {response.status}\",\n                            timestamp=time.time(),\n                            details={\"status_code\": response.status}\n                        )\n        except Exception as e:\n            return HealthCheck(\n                name=f\"endpoint_{model_name}\",\n                status=HealthStatus.UNHEALTHY,\n                message=f\"Connection failed: {str(e)}\",\n                timestamp=time.time(),\n                details={\"error\": str(e)}\n            )\n    \n    async def check_resource_usage(self, instance_id: str) -> HealthCheck:\n        \"\"\"Check resource usage for serving instance.\"\"\"\n        # Implementation would integrate with actual monitoring system\n        # This is a simplified example\n        try:\n            # Simulate resource check\n            cpu_usage = 45.2  # Would get from actual monitoring\n            memory_usage = 67.8\n            \n            if cpu_usage > 90 or memory_usage > 95:\n                status = HealthStatus.UNHEALTHY\n                message = \"Resource exhaustion\"\n            elif cpu_usage > 70 or memory_usage > 80:\n                status = HealthStatus.DEGRADED\n                message = \"High resource usage\"\n            else:\n                status = HealthStatus.HEALTHY\n                message = \"Resources normal\"\n                \n            return HealthCheck(\n                name=f\"resources_{instance_id}\",\n                status=status,\n                message=message,\n                timestamp=time.time(),\n                details={\"cpu_percent\": cpu_usage, \"memory_percent\": memory_usage}\n            )\n        except Exception as e:\n            return HealthCheck(\n                name=f\"resources_{instance_id}\",\n                status=HealthStatus.UNKNOWN,\n                message=f\"Failed to check resources: {str(e)}\",\n                timestamp=time.time(),\n                details={\"error\": str(e)}\n            )\n    \n    async def run_all_checks(self) -> List[HealthCheck]:\n        \"\"\"Execute all registered health checks.\"\"\"\n        results = []\n        for name, check_func in self.health_checks.items():\n            try:\n                result = await check_func()\n                self.last_results[name] = result\n                results.append(result)\n            except Exception as e:\n                error_result = HealthCheck(\n                    name=name,\n                    status=HealthStatus.UNKNOWN,\n                    message=f\"Check failed: {str(e)}\",\n                    timestamp=time.time(),\n                    details={\"error\": str(e)}\n                )\n                self.last_results[name] = error_result\n                results.append(error_result)\n        return results\n    \n    async def start_monitoring(self):\n        \"\"\"Start continuous health monitoring.\"\"\"\n        self.running = True\n        while self.running:\n            await self.run_all_checks()\n            await asyncio.sleep(self.check_interval)\n    \n    def stop_monitoring(self):\n        \"\"\"Stop health monitoring.\"\"\"\n        self.running = False\n    \n    def get_overall_health(self) -> HealthStatus:\n        \"\"\"Determine overall system health from individual checks.\"\"\"\n        if not self.last_results:\n            return HealthStatus.UNKNOWN\n            \n        statuses = [check.status for check in self.last_results.values()]\n        \n        if any(status == HealthStatus.UNHEALTHY for status in statuses):\n            return HealthStatus.UNHEALTHY\n        elif any(status == HealthStatus.DEGRADED for status in statuses):\n            return HealthStatus.DEGRADED\n        elif all(status == HealthStatus.HEALTHY for status in statuses):\n            return HealthStatus.HEALTHY\n        else:\n            return HealthStatus.UNKNOWN\n```\n\n**Complete Traffic Routing System:**\n\n```python\nimport random\nimport hashlib\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\n\n@dataclass\nclass ModelEndpoint:\n    model_name: str\n    version: str\n    endpoint_url: str\n    weight: float\n    health_status: HealthStatus\n    \n@dataclass\nclass RoutingRule:\n    name: str\n    traffic_percentage: float\n    target_version: str\n    conditions: Dict[str, str]  # Header-based routing conditions\n\nclass TrafficRouter(ABC):\n    \"\"\"Abstract base for traffic routing strategies.\"\"\"\n    \n    @abstractmethod\n    def route_request(self, request_context: Dict[str, str], \n                     available_endpoints: List[ModelEndpoint]) -> Optional[ModelEndpoint]:\n        pass\n\nclass CanaryTrafficRouter(TrafficRouter):\n    \"\"\"Implements canary deployment traffic routing.\"\"\"\n    \n    def __init__(self, canary_percentage: float = 10.0):\n        self.canary_percentage = canary_percentage\n    \n    def route_request(self, request_context: Dict[str, str], \n                     available_endpoints: List[ModelEndpoint]) -> Optional[ModelEndpoint]:\n        # Filter to healthy endpoints only\n        healthy_endpoints = [ep for ep in available_endpoints \n                           if ep.health_status == HealthStatus.HEALTHY]\n        \n        if not healthy_endpoints:\n            return None\n            \n        # Separate stable and canary versions\n        stable_endpoints = [ep for ep in healthy_endpoints if ep.weight >= 90.0]\n        canary_endpoints = [ep for ep in healthy_endpoints if ep.weight < 90.0]\n        \n        # Route based on traffic split\n        if canary_endpoints and random.random() * 100 < self.canary_percentage:\n            return random.choice(canary_endpoints)\n        elif stable_endpoints:\n            return random.choice(stable_endpoints)\n        else:\n            return random.choice(healthy_endpoints)\n\nclass ConsistentHashRouter(TrafficRouter):\n    \"\"\"Routes requests consistently based on user ID for A/B testing.\"\"\"\n    \n    def route_request(self, request_context: Dict[str, str], \n                     available_endpoints: List[ModelEndpoint]) -> Optional[ModelEndpoint]:\n        user_id = request_context.get('user_id', 'anonymous')\n        \n        # Filter to healthy endpoints\n        healthy_endpoints = [ep for ep in available_endpoints \n                           if ep.health_status == HealthStatus.HEALTHY]\n        \n        if not healthy_endpoints:\n            return None\n        \n        # Use consistent hashing to ensure same user gets same version\n        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n        endpoint_index = hash_value % len(healthy_endpoints)\n        \n        return healthy_endpoints[endpoint_index]\n\nclass TrafficManager:\n    \"\"\"Manages traffic routing and deployment strategies.\"\"\"\n    \n    def __init__(self):\n        self.endpoints: Dict[str, List[ModelEndpoint]] = {}\n        self.routing_rules: Dict[str, RoutingRule] = {}\n        self.routers: Dict[str, TrafficRouter] = {\n            'canary': CanaryTrafficRouter(),\n            'consistent_hash': ConsistentHashRouter()\n        }\n    \n    def register_endpoint(self, model_name: str, endpoint: ModelEndpoint):\n        \"\"\"Register a new model serving endpoint.\"\"\"\n        if model_name not in self.endpoints:\n            self.endpoints[model_name] = []\n        self.endpoints[model_name].append(endpoint)\n    \n    def update_endpoint_health(self, model_name: str, version: str, \n                              health_status: HealthStatus):\n        \"\"\"Update health status for specific endpoint.\"\"\"\n        if model_name in self.endpoints:\n            for endpoint in self.endpoints[model_name]:\n                if endpoint.version == version:\n                    endpoint.health_status = health_status\n    \n    def set_traffic_split(self, model_name: str, version_weights: Dict[str, float]):\n        \"\"\"Set traffic split percentages for model versions.\"\"\"\n        if model_name in self.endpoints:\n            for endpoint in self.endpoints[model_name]:\n                if endpoint.version in version_weights:\n                    endpoint.weight = version_weights[endpoint.version]\n    \n    def route_request(self, model_name: str, request_context: Dict[str, str],\n                     strategy: str = 'canary') -> Optional[ModelEndpoint]:\n        \"\"\"Route request to appropriate model endpoint.\"\"\"\n        if model_name not in self.endpoints:\n            return None\n            \n        if strategy not in self.routers:\n            strategy = 'canary'  # Default fallback\n            \n        router = self.routers[strategy]\n        return router.route_request(request_context, self.endpoints[model_name])\n```\n\n#### Core Logic Skeleton\n\n**Deployment Controller (Core implementation for learners):**\n\n```python\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass DeploymentStatus(Enum):\n    PENDING = \"pending\"\n    DEPLOYING = \"deploying\"\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    FAILED = \"failed\"\n    ROLLING_BACK = \"rolling_back\"\n\n@dataclass\nclass DeploymentSpec:\n    model_name: str\n    model_version: str\n    target_replicas: int\n    resource_requirements: Dict[str, str]\n    deployment_strategy: str\n    traffic_split: Dict[str, float]\n\nclass ModelDeploymentController:\n    \"\"\"Core deployment controller - implement the TODOs below.\"\"\"\n    \n    def __init__(self, traffic_manager: TrafficManager, health_monitor: ModelServingHealthMonitor):\n        self.traffic_manager = traffic_manager\n        self.health_monitor = health_monitor\n        self.active_deployments: Dict[str, DeploymentSpec] = {}\n        self.deployment_history: List[Dict] = []\n    \n    def deploy_model_version(self, deployment_spec: DeploymentSpec) -> str:\n        \"\"\"Deploy new model version using specified strategy.\n        \n        Returns deployment_id for tracking.\n        \"\"\"\n        # TODO 1: Validate deployment spec (check model exists in registry, validate resources)\n        # TODO 2: Generate unique deployment ID and store deployment spec\n        # TODO 3: Based on deployment_strategy, choose deployment method:\n        #         - 'blue_green': call _deploy_blue_green()\n        #         - 'canary': call _deploy_canary() \n        #         - 'rolling': call _deploy_rolling_update()\n        # TODO 4: Create serving instances with specified resource requirements\n        # TODO 5: Register health checks for new instances\n        # TODO 6: Wait for instances to pass health checks before proceeding\n        # TODO 7: Update traffic routing based on deployment strategy\n        # TODO 8: Return deployment ID\n        # Hint: Use self.traffic_manager.register_endpoint() for new instances\n        # Hint: Use self.health_monitor.register_check() for health monitoring\n        pass\n    \n    def _deploy_canary(self, deployment_spec: DeploymentSpec) -> bool:\n        \"\"\"Implement canary deployment strategy.\"\"\"\n        # TODO 1: Start new model version with 5% traffic allocation\n        # TODO 2: Monitor key metrics (latency, error rate, accuracy) for 10 minutes\n        # TODO 3: If metrics are acceptable, increase traffic to 25%\n        # TODO 4: Continue monitoring and gradually increase: 50% -> 75% -> 100%\n        # TODO 5: At each stage, validate metrics haven't degraded beyond thresholds\n        # TODO 6: If metrics degrade, immediately rollback to previous version\n        # TODO 7: Update deployment status throughout the process\n        # TODO 8: Return True if successful, False if rollback was needed\n        # Hint: Use time.sleep() or asyncio.sleep() between traffic increase stages\n        # Hint: Check self.health_monitor.get_overall_health() for validation\n        pass\n    \n    def _deploy_blue_green(self, deployment_spec: DeploymentSpec) -> bool:\n        \"\"\"Implement blue-green deployment strategy.\"\"\"\n        # TODO 1: Deploy new version to separate \"green\" environment (0% traffic)\n        # TODO 2: Run full validation suite against green environment\n        # TODO 3: If validation passes, switch 100% traffic to green environment atomically\n        # TODO 4: Monitor for 5 minutes to ensure no issues with traffic switch\n        # TODO 5: If successful, mark blue environment for termination\n        # TODO 6: If issues detected, immediately switch traffic back to blue\n        # TODO 7: Update deployment status and return success/failure\n        # Hint: Use self.traffic_manager.set_traffic_split() for atomic switch\n        # Hint: Keep blue environment running during monitoring period for fast rollback\n        pass\n    \n    def rollback_deployment(self, deployment_id: str) -> bool:\n        \"\"\"Rollback failed deployment to previous version.\"\"\"\n        # TODO 1: Look up deployment spec and current state using deployment_id\n        # TODO 2: Identify previous stable version from deployment history\n        # TODO 3: Immediately route 100% traffic to previous stable version\n        # TODO 4: Scale down failed version instances\n        # TODO 5: Update deployment status to indicate rollback\n        # TODO 6: Log rollback event with failure reason for analysis\n        # TODO 7: Return True if rollback successful, False if rollback failed\n        # Hint: Rollback should be as fast as possible - don't wait for graceful shutdown\n        # Hint: Keep detailed logs for post-incident analysis\n        pass\n    \n    def scale_deployment(self, model_name: str, target_replicas: int) -> bool:\n        \"\"\"Scale existing deployment to target replica count.\"\"\"\n        # TODO 1: Validate current deployment exists and is healthy\n        # TODO 2: Calculate scaling direction (up or down) and number of replicas to change\n        # TODO 3: For scale-up: create new instances and wait for health checks\n        # TODO 4: For scale-down: gracefully shutdown excess instances after draining\n        # TODO 5: Update traffic routing to include/exclude scaled instances\n        # TODO 6: Monitor overall deployment health during scaling operation\n        # TODO 7: Update deployment spec with new replica count\n        # TODO 8: Return success/failure status\n        # Hint: For scale-down, ensure in-flight requests complete before shutdown\n        # Hint: Scale gradually (e.g., change 25% of replicas at a time)\n        pass\n    \n    def get_deployment_status(self, deployment_id: str) -> Dict:\n        \"\"\"Get current status of deployment including health and metrics.\"\"\"\n        # TODO 1: Look up deployment using deployment_id\n        # TODO 2: Collect current health status from all instances\n        # TODO 3: Gather performance metrics (latency, throughput, error rate)\n        # TODO 4: Check traffic distribution across versions\n        # TODO 5: Determine overall deployment health status\n        # TODO 6: Format response with all relevant status information\n        # TODO 7: Return comprehensive status dictionary\n        # Hint: Include timestamps for all status information\n        # Hint: Return None if deployment_id not found\n        pass\n```\n\n#### Milestone Checkpoints\n\n**After implementing basic deployment:**\n- Run: `python -m pytest tests/test_deployment_controller.py::test_basic_deployment`\n- Expected: All tests pass, showing successful model deployment and health checking\n- Manual verification: Deploy a simple model and confirm it responds to inference requests\n- Check logs for proper health check execution and traffic routing updates\n\n**After implementing canary deployments:**\n- Run: `python test_canary_deployment.py` with a test that simulates traffic split\n- Expected: Traffic gradually shifts from 5% → 25% → 50% → 75% → 100% over time\n- Manual verification: Deploy new version and observe traffic split changes in monitoring dashboard\n- Verify rollback functionality by introducing a \"bad\" model version that triggers alerts\n\n**After implementing auto-scaling:**\n- Run: `python test_autoscaling.py` with load testing that generates traffic spikes  \n- Expected: System automatically scales up during load, scales down when traffic decreases\n- Manual verification: Generate load using `hey -n 10000 -c 50 http://model-endpoint/predict`\n- Check that P95 latency stays below configured thresholds during scaling events\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| New deployments stay in PENDING | Resource allocation failure | Check cluster capacity with `kubectl describe nodes` | Adjust resource requests or scale cluster |\n| Canary traffic not splitting correctly | Routing configuration error | Verify traffic split percentages in load balancer config | Update routing rules and restart load balancer |\n| Health checks failing for running instances | Model loading timeout | Check container logs for model loading errors | Increase health check timeout or optimize model loading |\n| Rollback takes too long | Graceful shutdown blocking | Monitor in-flight requests during rollback | Implement forced termination after timeout |\n| Auto-scaling oscillating | Thresholds too sensitive | Analyze scaling metrics over time | Adjust scaling thresholds and evaluation windows |\n| High latency during deployments | Cold start issues | Monitor instance startup times | Implement model warming and keep spare capacity |\n\n\n## Model Monitoring Component\n\n> **Milestone(s):** This section primarily corresponds to Milestone 5 (Model Monitoring), which focuses on monitoring model performance in production through prediction logging, drift detection, and automated alerting. This component integrates with the Model Deployment Component (Milestone 4) to observe deployed models and provides feedback that may influence future experiments (Milestone 1) and model promotions (Milestone 2).\n\n### Mental Model: Health Monitoring System\n\nThink of model monitoring as a comprehensive medical health monitoring system for deployed ML models. Just as a hospital continuously monitors a patient's vital signs—heart rate, blood pressure, temperature, oxygen levels—to detect early warning signs of health problems, model monitoring continuously tracks key \"vital signs\" of production ML models to detect performance degradation before it impacts business outcomes.\n\nIn this analogy, **prediction requests** are like individual heartbeats—each one provides a data point about the model's current state. The **prediction logging system** acts like an electrocardiogram (EKG) machine, continuously recording every heartbeat with precise timestamps and measurements. **Latency metrics** are like blood pressure readings—high values indicate the system is under stress and may not be functioning optimally. **Accuracy metrics** are like core body temperature—significant deviations from the baseline indicate something is seriously wrong and requires immediate attention.\n\n**Data drift detection** functions like blood chemistry analysis, comparing current blood samples (incoming feature distributions) against healthy baseline values (training data distributions). When the chemistry changes significantly—perhaps due to medication, diet changes, or illness—medical professionals investigate the root cause. Similarly, when input data distributions shift significantly from training distributions, data scientists investigate whether the model needs retraining or the data pipeline has issues.\n\n**Model drift detection** is analogous to monitoring cognitive function over time. Just as doctors track whether a patient's mental responses and decision-making abilities remain consistent, model drift detection tracks whether the model's prediction patterns remain stable. Sudden changes in prediction distributions might indicate concept drift—the underlying relationship between inputs and outputs has changed in the real world.\n\nThe **alerting system** acts like hospital alarm systems that trigger when vital signs move outside safe ranges. Different alert severities correspond to different urgency levels: a yellow alert for minor accuracy degradation is like a slightly elevated temperature (worth monitoring), while a red alert for severe data drift is like a cardiac emergency requiring immediate intervention.\n\n**Monitoring dashboards** serve as the patient monitoring displays that medical staff use to track trends over time. They show recent values, historical patterns, and highlight anomalies that require attention. Just as medical professionals look for patterns across multiple vital signs to diagnose complex conditions, data scientists use monitoring dashboards to correlate multiple model health metrics to understand system-wide issues.\n\nThis health monitoring metaphor is powerful because it emphasizes the **continuous, automated, and proactive** nature of effective model monitoring. You don't wait for a patient to collapse before checking their vital signs, and you shouldn't wait for business metrics to degrade before monitoring model performance. The monitoring system should detect problems early and escalate appropriately based on severity.\n\n### Prediction Logging and Metrics\n\nThe prediction logging system forms the foundation of model monitoring by capturing every inference request and response, along with contextual metadata necessary for performance analysis. This system must handle high-throughput production traffic while maintaining low latency overhead and ensuring data consistency for accurate metric computation.\n\n#### Request and Response Capture\n\nEvery prediction request flowing through deployed model endpoints gets intercepted and logged by the monitoring system. The **prediction logger** operates as middleware in the inference serving stack, positioned between the load balancer and model serving instances to capture complete request context.\n\nThe logging mechanism captures several categories of information for each prediction request. **Input features** are recorded exactly as they were sent to the model, preserving both feature names and values in their original data types. This enables downstream drift detection and feature importance analysis. **Model outputs** include both the final prediction and any intermediate outputs like confidence scores, class probabilities, or attention weights that provide insight into model decision-making.\n\n**Request metadata** captures contextual information about the inference environment. This includes the model version that processed the request, the serving instance identifier, the geographic region where inference occurred, and client identification when available. **Timing information** records request arrival time, inference start and completion times, and any queueing delays that contribute to overall latency.\n\n**Correlation identifiers** link prediction logs to broader request tracing systems, enabling end-to-end latency analysis across microservices. When a single user action triggers multiple model predictions—such as recommendation ranking followed by click-through prediction—correlation IDs help analyze the complete interaction sequence.\n\nThe prediction logging system implements **sampling strategies** to manage storage costs for high-volume services while maintaining statistical validity. Random sampling captures a representative subset of all predictions, while stratified sampling ensures adequate coverage across different user segments, feature ranges, or time periods. Critical predictions—such as those triggering high-value business decisions—may be logged with 100% capture rate regardless of sampling configuration.\n\n| Logged Data Category | Fields Captured | Purpose |\n|----------------------|-----------------|----------|\n| Input Features | feature_name, feature_value, feature_type, feature_schema_version | Drift detection, feature importance analysis, model debugging |\n| Model Outputs | prediction_value, confidence_score, class_probabilities, model_version | Performance analysis, A/B testing, model comparison |\n| Request Metadata | request_id, model_name, model_version, serving_instance_id, client_id | Request tracing, capacity planning, error attribution |\n| Timing Information | request_timestamp, inference_start_time, inference_end_time, queue_time | Latency analysis, performance optimization, SLA monitoring |\n| Context Data | geographical_region, user_segment, experiment_group, correlation_id | Segmented analysis, A/B testing, multi-model workflows |\n\n#### Latency and Throughput Measurement\n\n**Latency tracking** measures the time required to process prediction requests at multiple granularities. **End-to-end latency** captures the complete time from request arrival to response delivery, including network transmission, queueing delays, model inference, and response serialization. This metric directly impacts user experience and must stay below configured Service Level Objectives (SLOs).\n\n**Inference latency** isolates the time spent in actual model computation, excluding network and queueing overhead. This metric helps distinguish between model performance issues and infrastructure capacity problems. When end-to-end latency increases but inference latency remains stable, the issue likely stems from resource contention or networking problems rather than model complexity.\n\nThe system tracks latency using **percentile distributions** rather than simple averages, as averages can mask performance issues affecting a minority of requests. The P50 (median), P95, P99, and P99.9 percentiles provide insight into typical performance and tail latency behavior. A model with excellent average latency but poor P99 latency creates unpredictable user experiences that may impact business metrics.\n\n**Throughput measurement** tracks the number of prediction requests processed per unit time, typically measured in requests per second (RPS) or predictions per minute. Peak throughput helps determine infrastructure capacity requirements, while sustained throughput indicates normal operational load. Throughput analysis identifies traffic patterns—daily cycles, seasonal variations, and sudden spikes—that inform auto-scaling decisions.\n\nThe monitoring system correlates **latency and throughput** to identify performance characteristics under different load conditions. Some models maintain consistent latency until reaching a throughput threshold, then experience rapid latency degradation. Other models show gradual latency increases as throughput rises. Understanding these relationships helps set appropriate auto-scaling triggers and capacity planning decisions.\n\n| Latency Metric | Measurement Method | Alert Thresholds | Business Impact |\n|----------------|-------------------|------------------|-----------------|\n| P50 Latency | Median request processing time | > 100ms warning, > 200ms critical | User experience degradation |\n| P95 Latency | 95th percentile processing time | > 300ms warning, > 500ms critical | Poor experience for some users |\n| P99 Latency | 99th percentile processing time | > 1000ms warning, > 2000ms critical | Unacceptable delays for edge cases |\n| Inference Latency | Time in model computation only | > 50ms warning, > 100ms critical | Model complexity issues |\n| Queue Time | Time waiting for available resources | > 10ms warning, > 50ms critical | Insufficient serving capacity |\n\n#### Performance Metric Aggregation\n\nThe monitoring system computes **aggregated performance metrics** across multiple time windows to provide both real-time insights and historical trend analysis. **Real-time metrics** use sliding windows of recent predictions—typically the last 1, 5, and 15 minutes—to enable rapid detection of performance degradation. **Historical metrics** aggregate data over hourly, daily, and weekly periods to identify long-term trends and seasonal patterns.\n\n**Accuracy measurement** in production environments faces the challenge that ground truth labels are rarely available immediately after prediction. The system implements several strategies to estimate model accuracy. **Delayed feedback** incorporates ground truth labels when they become available—such as user clicks, conversion events, or manual annotations—and retroactively computes accuracy metrics. **Proxy metrics** use correlated signals available in real-time, such as user engagement rates or downstream system responses, as approximate indicators of model performance.\n\n**Business metric correlation** links model performance to measurable business outcomes. For recommendation models, this might include click-through rates, conversion rates, or revenue per impression. For fraud detection models, this could track false positive rates impacting customer experience and false negative rates impacting financial losses. These correlations help prioritize model improvements based on business impact rather than purely technical metrics.\n\nThe aggregation system implements **statistical significance testing** for metric comparisons across time periods or model versions. When comparing current performance to historical baselines, the system computes confidence intervals and p-values to distinguish meaningful changes from normal statistical variation. This prevents alert fatigue from triggering on insignificant metric fluctuations.\n\n**Segmented analysis** computes metrics across different user populations, geographic regions, or feature value ranges. A model may maintain overall accuracy while degrading for specific user segments, indicating dataset bias or insufficient training data coverage. Segmented metrics help identify these localized performance issues that might be masked in aggregate statistics.\n\n| Metric Category | Computation Method | Update Frequency | Retention Period |\n|-----------------|-------------------|------------------|------------------|\n| Real-time Accuracy | Sliding window with available labels | Every 60 seconds | 7 days |\n| Business KPIs | Correlation with downstream events | Every 5 minutes | 6 months |\n| Latency Percentiles | Quantile estimation over time windows | Every 30 seconds | 30 days |\n| Error Rates | Failure count / total request count | Every 60 seconds | 90 days |\n| Throughput | Request count per time window | Every 15 seconds | 1 year |\n\n### Data and Model Drift Detection\n\nDrift detection identifies when the statistical properties of model inputs or outputs change significantly compared to training data or historical baselines. These changes can indicate underlying shifts in user behavior, data collection processes, or real-world phenomena that affect model validity.\n\n#### Statistical Drift Detection Methods\n\n**Data drift detection** compares the distribution of incoming features against the distribution observed in training data. The system maintains **reference distributions** computed from the original training dataset, stored as statistical summaries rather than raw data to protect privacy and reduce storage requirements. For numerical features, reference distributions include mean, variance, quantiles, and histogram bins. For categorical features, they include class frequencies and unique value counts.\n\nThe **Kolmogorov-Smirnov (KS) test** evaluates whether incoming numerical features follow the same distribution as training data. The KS statistic measures the maximum difference between cumulative distribution functions, with larger values indicating greater distributional differences. The system computes KS statistics over rolling windows of recent predictions and triggers drift alerts when values exceed calibrated thresholds.\n\n**Population Stability Index (PSI)** provides a single metric summarizing drift across all features simultaneously. PSI compares the percentage of samples falling into each histogram bin between current and reference distributions. Values below 0.1 indicate minimal drift, values between 0.1 and 0.25 suggest moderate drift requiring investigation, and values above 0.25 indicate severe drift necessitating model retraining.\n\nFor categorical features, **chi-squared tests** evaluate whether the observed frequency distribution matches expected frequencies from training data. The test statistic follows a known distribution, enabling p-value computation and statistical significance assessment. High chi-squared values with low p-values indicate significant distributional changes.\n\n**Jensen-Shannon divergence** measures the difference between probability distributions in a symmetric, bounded metric. Unlike KL divergence, JS divergence remains finite even when distributions have non-overlapping support, making it robust for real-world data where new categorical values may appear in production. The system normalizes JS divergence scores to a 0-1 scale for consistent threshold setting across features.\n\n| Drift Detection Method | Applicable Feature Types | Computational Complexity | Sensitivity | Interpretability |\n|------------------------|--------------------------|-------------------------|-------------|------------------|\n| Kolmogorov-Smirnov | Numerical continuous | O(n log n) | High | Medium |\n| Population Stability Index | Numerical binned | O(b) where b = bins | Medium | High |\n| Chi-squared Test | Categorical | O(k) where k = categories | High | High |\n| Jensen-Shannon Divergence | Both numerical and categorical | O(b) or O(k) | Medium | Medium |\n| Two-sample t-test | Numerical with normal distribution | O(n) | Medium for mean shifts | High |\n\n#### Feature Distribution Monitoring\n\nThe monitoring system tracks **univariate distributions** for each input feature independently, computing drift metrics on a per-feature basis. This granular approach enables identification of specific features experiencing drift, facilitating targeted investigation and remediation. Feature-level drift scores aggregate into overall dataset drift scores using weighted averages based on feature importance or business relevance.\n\n**Multivariate drift detection** identifies changes in feature correlations and interactions that univariate methods might miss. **Principal Component Analysis (PCA)** projects features into lower-dimensional spaces where distributional changes become more apparent. Drift in the first few principal components often indicates systematic changes in data collection or user behavior patterns.\n\n**Covariate shift detection** specifically identifies changes in input feature distributions while assuming the underlying relationship between features and target variables remains constant. This type of drift is common when model deployment expands to new geographic markets, user segments, or time periods with different demographic characteristics but similar behavioral patterns.\n\nThe system implements **adaptive thresholds** that adjust drift detection sensitivity based on historical patterns. Models serving highly seasonal businesses—such as retail or travel—experience predictable feature distribution changes throughout the year. Adaptive thresholds learn these seasonal patterns and avoid triggering false alerts during expected distribution shifts.\n\n**Feature importance weighting** prioritizes drift detection for features that most strongly influence model predictions. Small drift in highly important features may be more concerning than large drift in features with minimal predictive power. The system computes feature importance scores using model-specific methods—such as SHAP values or permutation importance—and weights drift scores accordingly.\n\n#### Concept Drift Analysis\n\n**Concept drift** occurs when the relationship between input features and target variables changes over time, even if feature distributions remain stable. Unlike data drift, concept drift requires ground truth labels to detect, making it more challenging to identify in real-time production systems.\n\n**Prediction distribution monitoring** provides an early signal of potential concept drift by tracking changes in model output distributions. Sudden shifts in prediction confidence, class probability distributions, or regression value ranges may indicate underlying concept changes before ground truth feedback becomes available. The system monitors prediction distributions using the same statistical methods applied to input features.\n\n**Performance degradation tracking** identifies concept drift through declining model accuracy over time. The system maintains **rolling accuracy estimates** computed from available ground truth labels, accounting for label delay and coverage gaps. Significant accuracy decreases that cannot be explained by data drift suggest concept drift affecting model validity.\n\n**Temporal analysis** examines drift patterns over different time scales to distinguish between temporary fluctuations and sustained changes. **Short-term drift** might indicate transient events like marketing campaigns or news cycles that affect user behavior temporarily. **Long-term drift** suggests fundamental changes in the underlying domain requiring model updates or retraining.\n\nThe system implements **change point detection** algorithms that identify specific timestamps when drift began, helping correlate model performance changes with external events. **CUSUM (Cumulative Sum)** algorithms detect changes in statistical properties of time series data, while **Bayesian change point detection** provides probabilistic estimates of when distribution shifts occurred.\n\n| Concept Drift Type | Detection Method | Response Strategy | Example Scenarios |\n|--------------------|------------------|------------------|-------------------|\n| Sudden Drift | Change point detection on accuracy metrics | Immediate model rollback | Algorithm updates, policy changes |\n| Gradual Drift | Linear trend analysis over time windows | Scheduled retraining | Seasonal behavior changes |\n| Incremental Drift | Moving average convergence tracking | Continuous learning updates | User preference evolution |\n| Recurring Drift | Seasonal decomposition analysis | Scheduled model switching | Holiday shopping patterns |\n| Blip Drift | Outlier detection in performance metrics | Temporary monitoring increase | Marketing campaign effects |\n\n### Architecture Decisions\n\nThe model monitoring component requires several critical architecture decisions around data collection, storage, analysis, and alerting. These decisions significantly impact system performance, cost, and reliability.\n\n> **Decision: Real-time vs Batch Drift Detection**\n> - **Context**: Model monitoring must detect drift quickly enough to prevent business impact while managing computational costs for high-volume services processing millions of predictions daily.\n> - **Options Considered**: (1) Real-time drift computation on every prediction, (2) Micro-batch processing every few minutes, (3) Hourly batch processing\n> - **Decision**: Hybrid approach with micro-batch processing for drift detection and real-time aggregation for latency metrics\n> - **Rationale**: Real-time drift computation is computationally prohibitive at scale—computing KS statistics on millions of samples per second would require enormous compute resources. Hourly batches are too slow to catch rapid drift that could impact business outcomes. Micro-batches every 5-15 minutes provide good balance between detection speed and computational efficiency.\n> - **Consequences**: Enables drift detection within 15 minutes of occurrence while keeping compute costs manageable. However, extremely rapid drift might not be caught before affecting user experience. Requires careful batch size tuning to maintain statistical power.\n\n| Option | Detection Speed | Computational Cost | Statistical Power | Chosen? |\n|--------|----------------|-------------------|-------------------|---------|\n| Real-time (per request) | < 1 second | Very high | Low (small samples) | No |\n| Micro-batch (5-15 min) | 5-15 minutes | Medium | Medium | **Yes** |\n| Hourly batch | 60+ minutes | Low | High | No |\n\n> **Decision: Prediction Logging Storage Architecture**\n> - **Context**: Production models may generate millions of prediction logs daily, requiring storage that supports both high-throughput writes and complex analytical queries for drift detection and performance analysis.\n> - **Options Considered**: (1) Relational database with time-series optimization, (2) NoSQL document store, (3) Columnar analytics database, (4) Object storage with query engine\n> - **Decision**: Columnar analytics database (ClickHouse) with object storage backup\n> - **Rationale**: Columnar storage provides excellent compression for repetitive prediction data and fast analytical queries across time ranges. ClickHouse specifically handles time-series data well with automatic partitioning by timestamp. Object storage backup provides cost-effective long-term retention.\n> - **Consequences**: Enables fast drift detection queries and flexible analytics. However, requires specialized database expertise and careful schema design for optimal performance. Higher infrastructure complexity than simple document storage.\n\n| Option | Write Performance | Query Performance | Storage Cost | Operational Complexity | Chosen? |\n|--------|------------------|------------------|--------------|----------------------|---------|\n| PostgreSQL + TimescaleDB | Medium | Medium | Medium | Low | No |\n| MongoDB | High | Low for analytics | Medium | Medium | No |\n| ClickHouse | High | High for analytics | Low (compression) | High | **Yes** |\n| S3 + Athena | Medium | Medium | Very low | Medium | Backup only |\n\n> **Decision: Drift Detection Algorithm Selection**\n> - **Context**: Different drift detection algorithms have varying computational complexity, sensitivity, and interpretability characteristics. The system must detect meaningful drift while avoiding false positives from normal statistical variation.\n> - **Options Considered**: (1) Single algorithm (KS test) for simplicity, (2) Ensemble of multiple algorithms with voting, (3) Feature-type-specific algorithm selection\n> - **Decision**: Feature-type-specific algorithm selection with configurable sensitivity\n> - **Rationale**: Different feature types require different statistical tests—KS tests work well for continuous variables but are inappropriate for categorical data. PSI provides intuitive interpretability for business users. Algorithm selection based on feature characteristics maximizes detection accuracy.\n> - **Consequences**: Better drift detection accuracy and fewer false positives. However, requires more complex implementation and algorithm-specific parameter tuning. Need expertise in multiple statistical methods.\n\n| Approach | Implementation Complexity | Detection Accuracy | False Positive Rate | Interpretability | Chosen? |\n|----------|--------------------------|-------------------|-------------------|------------------|---------|\n| Single algorithm (KS only) | Low | Medium | Medium | High | No |\n| Algorithm ensemble | High | High | Low | Low | No |\n| Feature-type-specific | Medium | High | Low | High | **Yes** |\n\n> **Decision: Alerting Threshold Management**\n> - **Context**: Static drift thresholds generate excessive false positives for models with natural seasonal patterns or business cycles, while adaptive thresholds may miss genuine drift during expected variation periods.\n> - **Options Considered**: (1) Static thresholds set during model deployment, (2) Adaptive thresholds based on historical patterns, (3) Hierarchical thresholds with multiple severity levels\n> - **Decision**: Hierarchical thresholds with seasonal adjustment factors\n> - **Rationale**: Multiple threshold levels (warning/critical/emergency) enable appropriate response escalation. Seasonal adjustment factors accommodate predictable business patterns while maintaining sensitivity to unexpected changes. Provides balance between alerting precision and operational overhead.\n> - **Consequences**: Reduces alert fatigue while maintaining drift detection sensitivity. However, requires careful threshold calibration and seasonal pattern analysis during deployment. May miss drift that coincides with expected seasonal changes.\n\n#### Data Retention and Storage Optimization\n\nThe monitoring system must balance data retention requirements with storage costs and query performance. **Prediction logs** contain detailed request/response data needed for debugging and detailed analysis, but storing every prediction indefinitely becomes prohibitively expensive for high-volume services.\n\n**Tiered storage strategy** implements different retention policies based on data age and detail level. **Hot storage** keeps detailed prediction logs for the most recent 7-30 days in high-performance storage optimized for analytical queries. **Warm storage** retains aggregated metrics and sampled predictions for 3-12 months in cost-optimized storage. **Cold storage** preserves statistical summaries and drift detection results for multi-year retention in archive storage.\n\n**Data compression and aggregation** reduces storage requirements while preserving analytical capability. **Lossless compression** leverages the repetitive nature of prediction logs—model names, versions, and feature names repeat across millions of records. **Lossy aggregation** replaces detailed logs with statistical summaries when full precision is no longer needed for analysis.\n\n**Partitioning strategy** organizes data for efficient query performance and cost-effective deletion. **Time-based partitioning** allows efficient queries over date ranges and enables automatic partition dropping for data retention. **Model-based partitioning** enables model-specific analysis without scanning unrelated data.\n\nThe system implements **data lifecycle policies** that automatically transition data between storage tiers and delete expired data. These policies integrate with cost monitoring to maintain target storage budgets while ensuring adequate data availability for monitoring functions.\n\n| Storage Tier | Retention Period | Data Detail Level | Query Performance | Storage Cost | Use Cases |\n|--------------|------------------|------------------|------------------|--------------|-----------|\n| Hot | 7-30 days | Full prediction logs | Sub-second | High | Real-time monitoring, debugging |\n| Warm | 3-12 months | Aggregated metrics + samples | Few seconds | Medium | Trend analysis, drift investigation |\n| Cold | 1-5 years | Statistical summaries | Minutes | Low | Compliance, historical analysis |\n| Archive | 5+ years | Metadata only | N/A | Very low | Audit trails, legal requirements |\n\n#### Alert Configuration and Escalation\n\n**Multi-level alerting** implements different response procedures based on drift severity and business impact. **Warning alerts** notify data science teams about moderate drift that requires investigation but doesn't threaten immediate model performance. **Critical alerts** trigger automated responses like traffic reduction or model rollback when drift exceeds acceptable thresholds. **Emergency alerts** page on-call engineers for severe drift indicating potential data corruption or system compromise.\n\n**Alert correlation** prevents notification flooding when multiple related metrics trigger simultaneously. When both data drift and model performance degrade together, the system sends a single correlated alert rather than separate notifications for each metric. **Alert suppression** prevents repeated notifications for ongoing issues that haven't been resolved.\n\n**Escalation policies** ensure appropriate response timing based on drift severity. Warning alerts may wait for business hours, while critical alerts notify team members immediately regardless of time. **Automatic escalation** promotes unacknowledged alerts to higher severity levels after configured time delays.\n\nThe alerting system integrates with **external notification systems** including email, Slack, PagerDuty, and webhooks for custom integrations. **Alert routing** directs different alert types to appropriate teams—data drift alerts go to data science teams while infrastructure issues go to DevOps teams.\n\n### Common Pitfalls\n\n⚠️ **Pitfall: Logging Every Feature for Every Prediction**\nMany monitoring implementations capture complete feature vectors for every prediction request, leading to enormous storage costs and query performance problems. For models with hundreds of features serving millions of requests daily, this approach can generate terabytes of logs weekly. Instead, implement selective logging that captures all features for a statistical sample (1-10% of requests) and only key features for the remainder. Use feature importance scores to identify which features require continuous monitoring versus periodic validation.\n\n⚠️ **Pitfall: Using Fixed Drift Thresholds Across All Features**\nSetting the same drift threshold for all features ignores their varying importance and natural variability. A 10% distribution change in a critical feature may be alarming, while the same change in a low-importance feature may be meaningless. Additionally, some features naturally have higher variance than others—user age distributions change slowly while behavioral features may fluctuate rapidly. Use feature-specific thresholds calibrated based on historical variability and feature importance scores from the model.\n\n⚠️ **Pitfall: Ignoring Label Delay in Accuracy Monitoring**\nProduction model accuracy cannot be measured immediately because ground truth labels arrive with significant delays—sometimes hours, days, or weeks after prediction. Monitoring systems that assume immediate label availability will severely underestimate model performance or fail entirely. Implement delayed accuracy computation that accounts for realistic label arrival patterns, and use proxy metrics (user engagement, downstream conversion rates) for real-time performance estimation.\n\n⚠️ **Pitfall: Computing Drift on Insufficiently Large Sample Sizes**\nStatistical drift tests require adequate sample sizes to distinguish meaningful changes from random variation. Computing KS tests on 100 predictions may trigger false positives from normal statistical fluctuation, while computing on 10 million predictions may detect statistically significant but practically meaningless drift. Calibrate minimum sample sizes based on effect size you want to detect—typically 1,000-10,000 samples for meaningful business impact.\n\n⚠️ **Pitfall: Not Accounting for Seasonal Patterns in Drift Detection**\nBusiness applications often have predictable seasonal patterns—e-commerce shows different behavior during holidays, financial models vary by quarter, recommendation systems change with trends. Drift detection that doesn't account for these patterns will trigger false positives during every seasonal transition. Implement seasonal adjustment factors or separate baseline distributions for different time periods (weekday/weekend, monthly, quarterly patterns).\n\n⚠️ **Pitfall: Storing Prediction Logs Without Proper Data Governance**\nModel prediction logs often contain sensitive personal information and may be subject to privacy regulations like GDPR. Storing these logs indefinitely without proper anonymization, encryption, or retention policies creates compliance risks. Implement data governance policies that anonymize or pseudonymize personal identifiers, encrypt logs at rest and in transit, and automatically delete data according to retention policies. Consider differential privacy techniques for long-term analytical datasets.\n\n⚠️ **Pitfall: Not Implementing Monitoring for the Monitoring System**\nMonitoring systems themselves can fail—data ingestion may stop, drift computation may crash, or alert delivery may break. When monitoring fails silently, model degradation goes undetected until business impact becomes obvious. Implement meta-monitoring that tracks monitoring system health: data ingestion rates, computation job success rates, alert delivery confirmation, and end-to-end monitoring pipeline latency. Alert when the monitoring system itself shows signs of failure.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Prediction Logging | PostgreSQL with time-series extension + structured JSON | ClickHouse or Apache Druid with columnar compression |\n| Drift Computation | Python scikit-learn + pandas batch processing | Apache Kafka Streams + custom drift processors |\n| Metrics Storage | TimescaleDB with automatic aggregation | InfluxDB with downsampling policies |\n| Alerting | Simple email/Slack webhooks | Prometheus + AlertManager with PagerDuty integration |\n| Dashboard | Grafana with PostgreSQL datasource | Custom React dashboard with real-time updates |\n| Statistical Computing | scipy.stats for standard tests | Custom implementations optimized for streaming data |\n\n#### Recommended File Structure\n\n```\nmonitoring/\n├── __init__.py\n├── core/\n│   ├── __init__.py\n│   ├── prediction_logger.py     ← Request/response capture\n│   ├── metrics_calculator.py    ← Latency and accuracy computation\n│   └── drift_detector.py        ← Statistical drift analysis\n├── storage/\n│   ├── __init__.py\n│   ├── log_store.py            ← Prediction log storage interface\n│   ├── metrics_store.py        ← Aggregated metrics storage\n│   └── clickhouse_adapter.py   ← ClickHouse-specific implementation\n├── alerting/\n│   ├── __init__.py\n│   ├── alert_engine.py         ← Threshold evaluation and notification\n│   ├── escalation_manager.py   ← Alert routing and escalation policies\n│   └── notification_channels.py ← Email, Slack, webhook integrations\n├── api/\n│   ├── __init__.py\n│   ├── monitoring_service.py   ← HTTP API for monitoring queries\n│   └── health_checks.py       ← Monitoring system health validation\n└── scripts/\n    ├── drift_computation_job.py ← Batch drift analysis\n    └── metrics_aggregation_job.py ← Periodic metric summarization\n```\n\n#### Infrastructure Starter Code\n\n**Prediction Logger Interface:**\n\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\nimport time\nimport uuid\nfrom dataclasses import dataclass\n\n@dataclass\nclass PredictionLogEntry:\n    \"\"\"Single prediction log entry with all captured metadata.\"\"\"\n    log_id: str\n    model_name: str\n    model_version: str\n    timestamp: float\n    input_features: Dict[str, Any]\n    prediction_output: Any\n    confidence_score: Optional[float]\n    latency_ms: float\n    request_id: str\n    client_id: Optional[str]\n    \n    @classmethod\n    def create(cls, model_name: str, model_version: str, \n               input_features: Dict[str, Any], prediction_output: Any,\n               confidence_score: Optional[float] = None,\n               latency_ms: float = 0.0, request_id: Optional[str] = None,\n               client_id: Optional[str] = None) -> 'PredictionLogEntry':\n        return cls(\n            log_id=str(uuid.uuid4()),\n            model_name=model_name,\n            model_version=model_version,\n            timestamp=time.time(),\n            input_features=input_features,\n            prediction_output=prediction_output,\n            confidence_score=confidence_score,\n            latency_ms=latency_ms,\n            request_id=request_id or str(uuid.uuid4()),\n            client_id=client_id\n        )\n\nclass PredictionLogger(ABC):\n    \"\"\"Abstract interface for logging prediction requests and responses.\"\"\"\n    \n    @abstractmethod\n    def log_prediction(self, log_entry: PredictionLogEntry) -> bool:\n        \"\"\"Log a single prediction entry. Returns success status.\"\"\"\n        pass\n    \n    @abstractmethod\n    def log_batch(self, entries: List[PredictionLogEntry]) -> int:\n        \"\"\"Log multiple entries efficiently. Returns count of successful logs.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_recent_predictions(self, model_name: str, hours: int = 24) -> List[PredictionLogEntry]:\n        \"\"\"Retrieve recent predictions for drift analysis.\"\"\"\n        pass\n\nclass MemoryPredictionLogger(PredictionLogger):\n    \"\"\"In-memory prediction logger for development and testing.\"\"\"\n    \n    def __init__(self, max_entries: int = 10000):\n        self._logs: List[PredictionLogEntry] = []\n        self._max_entries = max_entries\n    \n    def log_prediction(self, log_entry: PredictionLogEntry) -> bool:\n        self._logs.append(log_entry)\n        if len(self._logs) > self._max_entries:\n            self._logs.pop(0)  # Remove oldest entry\n        return True\n    \n    def log_batch(self, entries: List[PredictionLogEntry]) -> int:\n        for entry in entries:\n            self.log_prediction(entry)\n        return len(entries)\n    \n    def get_recent_predictions(self, model_name: str, hours: int = 24) -> List[PredictionLogEntry]:\n        cutoff_time = time.time() - (hours * 3600)\n        return [log for log in self._logs \n                if log.model_name == model_name and log.timestamp >= cutoff_time]\n```\n\n**Metrics Calculation Framework:**\n\n```python\nfrom typing import List, Dict, Tuple\nimport numpy as np\nfrom scipy import stats\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass DriftSeverity(Enum):\n    NONE = \"none\"\n    LOW = \"low\"\n    MEDIUM = \"medium\"\n    HIGH = \"high\"\n    CRITICAL = \"critical\"\n\n@dataclass\nclass DriftResult:\n    \"\"\"Result from drift detection analysis.\"\"\"\n    feature_name: str\n    drift_score: float\n    severity: DriftSeverity\n    test_statistic: float\n    p_value: float\n    detection_method: str\n    sample_size: int\n    \nclass MetricsCalculator:\n    \"\"\"Calculate performance and drift metrics from prediction logs.\"\"\"\n    \n    def __init__(self, drift_thresholds: Dict[str, float] = None):\n        \"\"\"Initialize with configurable drift thresholds.\n        \n        Args:\n            drift_thresholds: Dict mapping severity levels to threshold values\n                             e.g., {\"low\": 0.1, \"medium\": 0.25, \"high\": 0.5}\n        \"\"\"\n        self.drift_thresholds = drift_thresholds or {\n            \"low\": 0.1, \"medium\": 0.25, \"high\": 0.5, \"critical\": 0.75\n        }\n    \n    def compute_latency_percentiles(self, predictions: List[PredictionLogEntry]) -> Dict[str, float]:\n        \"\"\"Compute latency percentiles from prediction logs.\"\"\"\n        latencies = [p.latency_ms for p in predictions]\n        if not latencies:\n            return {}\n        \n        return {\n            \"p50\": float(np.percentile(latencies, 50)),\n            \"p95\": float(np.percentile(latencies, 95)),\n            \"p99\": float(np.percentile(latencies, 99)),\n            \"mean\": float(np.mean(latencies)),\n            \"std\": float(np.std(latencies))\n        }\n    \n    def detect_numerical_drift(self, current_values: List[float], \n                             baseline_values: List[float],\n                             feature_name: str) -> DriftResult:\n        \"\"\"Detect drift in numerical features using Kolmogorov-Smirnov test.\"\"\"\n        # TODO: Implement sample size validation\n        # TODO: Compute KS statistic and p-value using scipy.stats.ks_2samp\n        # TODO: Classify drift severity based on KS statistic and thresholds\n        # TODO: Return DriftResult with all computed metrics\n        pass\n    \n    def detect_categorical_drift(self, current_values: List[str],\n                                baseline_values: List[str], \n                                feature_name: str) -> DriftResult:\n        \"\"\"Detect drift in categorical features using chi-squared test.\"\"\"\n        # TODO: Build frequency distributions for current and baseline\n        # TODO: Handle case where current data has categories not in baseline\n        # TODO: Compute chi-squared statistic using scipy.stats.chisquare\n        # TODO: Classify severity based on chi-squared value and degrees of freedom\n        # TODO: Return DriftResult with test statistics and interpretation\n        pass\n    \n    def compute_psi(self, current_values: List[float], baseline_values: List[float],\n                   bins: int = 10) -> float:\n        \"\"\"Compute Population Stability Index for numerical features.\"\"\"\n        # TODO: Create histogram bins based on baseline distribution\n        # TODO: Compute percentage of samples in each bin for both distributions\n        # TODO: Handle bins with zero samples (add small epsilon)\n        # TODO: Calculate PSI = sum((current_pct - baseline_pct) * ln(current_pct / baseline_pct))\n        # TODO: Return PSI value (0 = no drift, >0.25 = significant drift)\n        pass\n```\n\n#### Core Logic Skeleton Code\n\n**Drift Detection Engine:**\n\n```python\nclass DriftDetectionEngine:\n    \"\"\"Main engine for detecting data and model drift in production.\"\"\"\n    \n    def __init__(self, prediction_logger: PredictionLogger, \n                 baseline_store: BaselineStore, metrics_calculator: MetricsCalculator):\n        self.prediction_logger = prediction_logger\n        self.baseline_store = baseline_store\n        self.metrics_calculator = metrics_calculator\n    \n    def analyze_model_drift(self, model_name: str, analysis_window_hours: int = 24) -> List[DriftResult]:\n        \"\"\"Analyze all features for drift compared to training baseline.\n        \n        Returns list of DriftResult objects, one per feature analyzed.\n        \"\"\"\n        # TODO 1: Retrieve recent predictions from prediction_logger\n        # TODO 2: Extract feature values from predictions, organized by feature name\n        # TODO 3: Load baseline feature distributions from baseline_store\n        # TODO 4: For each feature, determine if numerical or categorical\n        # TODO 5: Call appropriate drift detection method from metrics_calculator\n        # TODO 6: Collect all drift results and return as list\n        # Hint: Group predictions by feature name for efficient batch processing\n        pass\n    \n    def detect_prediction_drift(self, model_name: str, \n                              comparison_window_hours: int = 24,\n                              baseline_window_hours: int = 168) -> DriftResult:\n        \"\"\"Detect concept drift by comparing prediction distributions over time.\"\"\"\n        # TODO 1: Get predictions from recent comparison window\n        # TODO 2: Get predictions from baseline window (e.g., last week)\n        # TODO 3: Extract prediction values from both time periods\n        # TODO 4: Use KS test to compare prediction distributions\n        # TODO 5: Compute drift severity based on test statistic\n        # TODO 6: Return DriftResult for prediction distribution\n        # Hint: Consider confidence scores as well as raw predictions\n        pass\n    \n    def compute_realtime_metrics(self, model_name: str, window_minutes: int = 15) -> Dict[str, Any]:\n        \"\"\"Compute real-time performance metrics for monitoring dashboard.\"\"\"\n        # TODO 1: Get predictions from the last window_minutes\n        # TODO 2: Compute latency percentiles using metrics_calculator\n        # TODO 3: Calculate throughput (requests per second)\n        # TODO 4: Compute error rate (failed predictions / total predictions)\n        # TODO 5: Calculate prediction confidence statistics\n        # TODO 6: Return all metrics as dictionary for dashboard consumption\n        # Hint: Handle case where no predictions exist in time window\n        pass\n\nclass AlertManager:\n    \"\"\"Manage drift alerts and escalation policies.\"\"\"\n    \n    def __init__(self, notification_channels: List[NotificationChannel]):\n        self.channels = notification_channels\n        self.active_alerts: Dict[str, Alert] = {}\n    \n    def evaluate_drift_alerts(self, drift_results: List[DriftResult], model_name: str) -> List[Alert]:\n        \"\"\"Evaluate drift results against alert thresholds and create alerts.\"\"\"\n        # TODO 1: Iterate through each drift result\n        # TODO 2: Check if drift severity exceeds alert thresholds\n        # TODO 3: Create Alert objects for threshold violations\n        # TODO 4: Correlate related alerts (avoid duplicate notifications)\n        # TODO 5: Check against active_alerts to avoid repeat notifications\n        # TODO 6: Return list of new alerts to send\n        # Hint: Different severity levels should route to different teams\n        pass\n    \n    def send_alert(self, alert: Alert) -> bool:\n        \"\"\"Send alert through configured notification channels.\"\"\"\n        # TODO 1: Determine appropriate notification channels based on alert severity\n        # TODO 2: Format alert message for each channel (email vs Slack formatting)\n        # TODO 3: Send notification through each selected channel\n        # TODO 4: Record alert in active_alerts for tracking\n        # TODO 5: Handle notification failures gracefully\n        # TODO 6: Return True if at least one notification succeeded\n        # Hint: Implement retry logic for failed notifications\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing the model monitoring component, verify correct operation with these checks:\n\n**1. Prediction Logging Verification:**\n```bash\n# Start monitoring service\npython -m monitoring.api.monitoring_service\n\n# Send test prediction and verify logging\ncurl -X POST http://localhost:8080/monitor/log_prediction \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"test_model\",\n    \"model_version\": \"v1.0\",\n    \"input_features\": {\"feature1\": 1.5, \"feature2\": \"category_a\"},\n    \"prediction\": 0.85,\n    \"latency_ms\": 12.5\n  }'\n\n# Query recent predictions\ncurl \"http://localhost:8080/monitor/predictions?model=test_model&hours=1\"\n```\n\n**Expected output:** JSON response containing the logged prediction with all metadata fields populated.\n\n**2. Drift Detection Testing:**\n```python\n# Generate synthetic drift data for testing\nimport numpy as np\nfrom monitoring.core.drift_detector import DriftDetectionEngine\n\n# Create baseline data (normal distribution)\nbaseline_data = np.random.normal(0, 1, 1000).tolist()\n\n# Create drifted data (shifted distribution) \ndrifted_data = np.random.normal(0.5, 1, 1000).tolist()  # Mean shift\n\n# Test drift detection\nengine = DriftDetectionEngine(logger, baseline_store, calculator)\nresult = engine.metrics_calculator.detect_numerical_drift(\n    drifted_data, baseline_data, \"test_feature\"\n)\n\nprint(f\"Drift detected: {result.severity}\")\nprint(f\"KS statistic: {result.test_statistic:.4f}\")\nprint(f\"P-value: {result.p_value:.4f}\")\n```\n\n**Expected behavior:** Drift severity should be \"MEDIUM\" or \"HIGH\" for the shifted distribution, with p-value < 0.05 indicating statistical significance.\n\n**3. Alert System Verification:**\n```bash\n# Configure test alert thresholds\nexport DRIFT_ALERT_THRESHOLD=0.2\nexport SLACK_WEBHOOK_URL=\"https://hooks.slack.com/test\"\n\n# Trigger alert with high drift\npython scripts/test_alert_system.py --drift-score=0.8 --model=test_model\n```\n\n**Expected behavior:** Alert notification sent to configured channels (Slack message, email) with drift details and recommended actions.\n\n**Signs of correct implementation:**\n- Prediction logs captured with sub-millisecond timestamp precision\n- Latency percentiles computed correctly across time windows  \n- Drift detection produces consistent results for identical input data\n- Alert thresholds properly calibrated to avoid false positives\n- Monitoring system health checks pass consistently\n\n**Signs of problems:**\n- Memory usage grows unbounded (indicates logging without retention policies)\n- Drift scores vary significantly between runs on same data (suggests statistical bugs)\n- Alerts fire repeatedly for same issue (missing alert correlation)\n- Query performance degrades over time (needs storage optimization)\n\n![Model Monitoring Data Flow](./diagrams/monitoring-architecture.svg)\n\n\n## Component Interactions and Data Flow\n\n> **Milestone(s):** This section integrates all milestones (1-5) by describing how experiment tracking, model registry, training pipeline orchestration, model deployment, and model monitoring communicate through APIs and events to create a cohesive MLOps workflow from experiment to production monitoring.\n\nThe MLOps platform components work together as a distributed system where each component maintains its own data and business logic while communicating with others through well-defined APIs and asynchronous events. Understanding these interactions is crucial because the platform's value comes not from individual components but from their seamless integration throughout the machine learning lifecycle.\n\n### Mental Model: Orchestra Coordination\n\nThink of the MLOps platform like a symphony orchestra where each component is a different instrument section. Just as musicians follow sheet music and respond to the conductor's signals to create harmonious music, our components follow API contracts and respond to events to orchestrate ML workflows. The conductor (event coordinator) doesn't control every note each musician plays, but ensures they start, stop, and transition together at the right moments. Each section (component) has specialized skills—strings handle melody, percussion provides rhythm—just as our components have specialized responsibilities for tracking, versioning, orchestration, deployment, and monitoring.\n\nThe sheet music represents our API specifications and data contracts, while the conductor's gestures represent the events that trigger coordinated actions across components. When the violins finish their solo (experiment completes), they signal the conductor, who then cues the brass section (model registry) to begin their part (model registration). This coordination happens without each section needing to know the internal details of how other sections operate—they just need to respond to the agreed-upon signals and timing.\n\n### Inter-Component APIs\n\nThe MLOps platform uses REST APIs as the primary communication mechanism between components, with each component exposing specific endpoints for cross-component operations while maintaining internal APIs for client interactions. This approach provides clear service boundaries, enables independent scaling, and supports polyglot implementation where different components can use different programming languages optimized for their specific requirements.\n\n#### Core API Design Principles\n\nEach component exposes two types of APIs: **external APIs** for client interactions (data scientists, ML engineers) and **internal APIs** for inter-component communication. The internal APIs focus on data exchange and lifecycle notifications, while external APIs provide rich query capabilities and user-friendly interfaces. All APIs follow REST principles with resource-based URLs, HTTP status codes for error handling, and JSON payloads with consistent field naming conventions.\n\nThe API design emphasizes **idempotency** for all mutation operations, meaning that repeating the same request multiple times produces the same result. This is crucial in distributed systems where network failures can cause request retries. For example, registering a model version with the same name, version, and checksum always returns the same model version ID, whether it's a new registration or a retry of a previous request.\n\n**Authentication and authorization** flow through all APIs using JWT tokens that contain user identity and permissions. Each component validates tokens independently and makes authorization decisions based on resource ownership and role-based access control. This distributed authorization model eliminates the need for a central authorization service while ensuring consistent security policies.\n\n#### Experiment Tracking APIs\n\nThe Experiment Tracking component exposes APIs for logging training data and querying experiment results, with specific endpoints designed for integration with other components that need to access experiment metadata and artifacts.\n\n| Method | Endpoint | Parameters | Returns | Description |\n|--------|----------|------------|---------|-------------|\n| `POST /api/v1/experiments` | name, description, tags | Experiment | Create new experiment |\n| `POST /api/v1/runs` | experiment_id, run_name, tags | Run | Start new training run |\n| `POST /api/v1/runs/{run_id}/params` | key, value, value_type | Parameter | Log parameter for run |\n| `POST /api/v1/runs/{run_id}/metrics` | key, value, step, timestamp | MetricPoint | Log metric point for run |\n| `POST /api/v1/runs/{run_id}/artifacts` | path, file_data, metadata | ArtifactInfo | Upload artifact for run |\n| `GET /api/v1/runs/{run_id}` | include_params, include_metrics | Run | Get run details with optional related data |\n| `GET /api/v1/runs/{run_id}/artifacts/{path}` | None | Binary data | Download artifact by path |\n| `POST /api/v1/runs/search` | experiment_ids, filter, order_by, limit | List[Run] | Search runs with filtering |\n| `POST /api/v1/runs/compare` | run_ids, metric_keys | Comparison | Compare metrics across runs |\n\nThe **inter-component integration points** focus on providing model registry and pipeline orchestration access to experiment data without exposing all experiment tracking functionality. The model registry calls the experiment tracking API to retrieve run metadata when registering models, ensuring complete lineage information. Pipeline orchestration queries experiment results to determine the best model versions for automated model selection workflows.\n\n#### Model Registry APIs\n\nThe Model Registry component provides APIs for model lifecycle management with emphasis on version control, stage transitions, and lineage tracking that other components need for deployment and monitoring workflows.\n\n| Method | Endpoint | Parameters | Returns | Description |\n|--------|----------|------------|---------|-------------|\n| `POST /api/v1/models` | name, description, tags | Model | Create new model entry |\n| `POST /api/v1/models/{name}/versions` | version, artifact_uri, run_id, metadata | ModelVersion | Register new model version |\n| `PUT /api/v1/models/{name}/versions/{version}/stage` | new_stage, approval_metadata | ModelVersion | Update version stage |\n| `GET /api/v1/models/{name}/versions/{version}` | include_lineage | ModelVersion | Get version details with optional lineage |\n| `GET /api/v1/models/{name}/versions/latest` | stage | ModelVersion | Get latest version for stage |\n| `GET /api/v1/models/{name}/lineage` | version, depth | Lineage graph | Build lineage graph for version |\n| `POST /api/v1/models/search` | stage, tags, metrics_filter, limit | List[ModelVersion] | Search models by criteria |\n| `GET /api/v1/models/{name}/versions/{version}/artifact` | path | Binary data | Download model artifact |\n\nThe **deployment integration** relies heavily on the \"get latest version for stage\" endpoint, which allows deployment components to automatically deploy the latest production model without hardcoding version numbers. The monitoring component uses the search API to discover all deployed model versions and establish monitoring for each one. This loose coupling means that promoting a model to production automatically triggers deployment workflows without explicit coordination.\n\n#### Pipeline Orchestration APIs\n\nThe Training Pipeline component exposes APIs for pipeline definition, execution control, and status monitoring, with integration points for automated model training and deployment workflows.\n\n| Method | Endpoint | Parameters | Returns | Description |\n|--------|----------|------------|---------|-------------|\n| `POST /api/v1/pipelines` | pipeline_definition | Pipeline | Create or update pipeline |\n| `POST /api/v1/pipelines/{pipeline_id}/runs` | parameters, trigger_type | PipelineRun | Start pipeline execution |\n| `GET /api/v1/pipelines/{pipeline_id}/runs/{run_id}` | include_steps | PipelineRun | Get run status with step details |\n| `POST /api/v1/pipelines/{pipeline_id}/runs/{run_id}/cancel` | reason | Success status | Cancel running pipeline |\n| `GET /api/v1/pipelines/{pipeline_id}/runs/{run_id}/logs` | step_id, follow | Log stream | Get execution logs for debugging |\n| `GET /api/v1/pipelines/{pipeline_id}/runs/{run_id}/artifacts` | step_id, path | List[ArtifactInfo] | List artifacts produced by step |\n| `POST /api/v1/pipelines/trigger` | event_type, payload | List[PipelineRun] | Trigger pipelines based on events |\n\nThe **event-driven integration** through the trigger endpoint allows other components to automatically start training pipelines when specific conditions are met. For example, when new training data becomes available or when model performance degrades below thresholds, monitoring components can trigger retraining pipelines without manual intervention.\n\n#### Model Deployment APIs\n\nThe Model Deployment component provides APIs for deploying models as serving endpoints with traffic management, health monitoring, and rollback capabilities that integrate with model registry and monitoring components.\n\n| Method | Endpoint | Parameters | Returns | Description |\n|--------|----------|------------|---------|-------------|\n| `POST /api/v1/deployments` | deployment_spec | Deployment | Deploy model version to endpoint |\n| `PUT /api/v1/deployments/{deployment_id}/traffic` | version_weights | Success status | Update traffic split between versions |\n| `POST /api/v1/deployments/{deployment_id}/rollback` | target_version | Success status | Rollback to previous version |\n| `GET /api/v1/deployments/{deployment_id}/health` | None | HealthCheck | Get deployment health status |\n| `PUT /api/v1/deployments/{deployment_id}/scale` | target_replicas | Success status | Scale deployment replicas |\n| `GET /api/v1/deployments/endpoints` | model_name, stage | List[ModelEndpoint] | List active endpoints for model |\n| `POST /api/v1/deployments/predict` | model_name, input_data, version | Prediction | Make prediction request |\n\nThe **monitoring integration** happens through the health and endpoints APIs, which allow monitoring components to discover all deployed models and track their serving status. The prediction API includes metadata that enables request logging for drift detection and performance analysis.\n\n#### Model Monitoring APIs\n\nThe Model Monitoring component exposes APIs for prediction logging, drift analysis, and alert management that close the loop by providing feedback to trigger retraining or rollback decisions.\n\n| Method | Endpoint | Parameters | Returns | Description |\n|--------|----------|------------|---------|-------------|\n| `POST /api/v1/monitoring/predictions` | log_entry | Success status | Log prediction for analysis |\n| `POST /api/v1/monitoring/predictions/batch` | entries | Batch status | Log multiple predictions efficiently |\n| `GET /api/v1/monitoring/models/{name}/metrics` | time_range, aggregation | Metrics summary | Get model performance metrics |\n| `POST /api/v1/monitoring/models/{name}/drift/analyze` | analysis_config | List[DriftResult] | Run drift analysis on demand |\n| `GET /api/v1/monitoring/models/{name}/alerts` | severity, status | List[Alert] | Get active alerts for model |\n| `PUT /api/v1/monitoring/models/{name}/thresholds` | metric_thresholds | Success status | Update alert thresholds |\n| `GET /api/v1/monitoring/models/{name}/baseline` | feature_names | Baseline data | Get baseline distributions for comparison |\n\nThe **feedback loop integration** enables monitoring to trigger actions in other components when issues are detected. High drift scores can automatically trigger retraining pipelines, while severe performance degradation can trigger deployment rollbacks through the respective component APIs.\n\n### End-to-End Workflows\n\nUnderstanding how components coordinate during complete ML workflows reveals the platform's true capabilities. These workflows demonstrate the API interactions and event flows that transform isolated training experiments into production-ready ML systems with continuous monitoring and improvement.\n\n#### Experiment to Deployment Workflow\n\nThis workflow represents the core path from initial model development to production deployment, showcasing how experiment tracking, model registry, and deployment components work together to maintain lineage and enable reproducible deployments.\n\n![Experiment to Deployment Flow](./diagrams/experiment-flow.svg)\n\nThe workflow begins when a data scientist starts a training experiment and concludes with an automatically deployed and monitored production model. This end-to-end process typically takes several API calls across multiple components, but the platform coordinates these interactions to make the experience seamless for users while maintaining complete audit trails.\n\n**Phase 1: Experiment Execution and Tracking**\n\nThe workflow starts when a data scientist initiates a training run through the experiment tracking component. The training code makes API calls to log parameters, metrics, and artifacts throughout the training process, building a complete record of the experiment.\n\n1. **Experiment Creation**: Data scientist calls `POST /api/v1/experiments` with experiment name \"credit-fraud-detection-v2\" and description \"Testing new feature engineering approach with SMOTE balancing\"\n2. **Run Initialization**: Training script calls `POST /api/v1/runs` with experiment ID and run name \"gradient-boosting-run-1\", receiving run ID \"run_abc123\"\n3. **Parameter Logging**: Script logs hyperparameters via `POST /api/v1/runs/run_abc123/params` including learning_rate=0.1, max_depth=6, n_estimators=100\n4. **Metric Tracking**: During training, script logs metrics via `POST /api/v1/runs/run_abc123/metrics` including accuracy, precision, recall at each epoch\n5. **Artifact Upload**: After training completes, script uploads model file via `POST /api/v1/runs/run_abc123/artifacts` with path \"model/fraud_detector.pkl\" and metadata including framework=\"scikit-learn\"\n6. **Run Completion**: Script calls `PUT /api/v1/runs/run_abc123/status` to mark run as FINISHED with final metrics summary\n\nThis phase creates a complete experiment record that includes all information needed for model reproduction and regulatory compliance. The artifact upload generates a content hash that ensures model integrity throughout the lifecycle.\n\n**Phase 2: Model Registration and Promotion**\n\nOnce the experiment produces a satisfactory model, the data scientist registers it in the model registry, beginning the formal model lifecycle management process.\n\n1. **Model Registration**: Data scientist calls `POST /api/v1/models/credit-fraud-detector/versions` with version=\"1.2.0\", run_id=\"run_abc123\", artifact_uri pointing to uploaded model\n2. **Lineage Establishment**: Model registry calls `GET /api/v1/runs/run_abc123` to retrieve complete experiment metadata and establish lineage linkage\n3. **Validation**: Model registry validates model artifact integrity by verifying checksum matches upload record from experiment tracking\n4. **Development Stage**: New model version starts in Development stage, allowing testing and validation before production promotion\n5. **Stage Promotion**: After validation, ML engineer calls `PUT /api/v1/models/credit-fraud-detector/versions/1.2.0/stage` with new_stage=\"Production\" and approval metadata\n6. **Approval Workflow**: Model registry executes approval workflow, potentially requiring additional approvals based on organization policies\n\nThis phase transforms an experiment artifact into a managed model version with formal lifecycle controls. The lineage linking ensures that production models can always be traced back to their training experiments for debugging and compliance.\n\n**Phase 3: Automated Deployment**\n\nModel promotion to Production stage triggers automated deployment workflows that create serving endpoints without manual intervention, ensuring consistent deployment practices.\n\n1. **Promotion Event**: Model registry publishes `MODEL_PROMOTED` event with payload including model_name=\"credit-fraud-detector\", version=\"1.2.0\", stage=\"Production\"\n2. **Deployment Trigger**: Deployment component receives event and calls `GET /api/v1/models/credit-fraud-detector/versions/1.2.0` to retrieve model metadata\n3. **Artifact Download**: Deployment component calls `GET /api/v1/models/credit-fraud-detector/versions/1.2.0/artifact` to download model file for deployment\n4. **Deployment Creation**: Deployment component creates deployment spec with auto-scaling configuration and calls `POST /api/v1/deployments` to start deployment\n5. **Health Verification**: Deployment component monitors deployment health via `GET /api/v1/deployments/{deployment_id}/health` until all replicas are healthy\n6. **Traffic Routing**: Once healthy, deployment component calls `PUT /api/v1/deployments/{deployment_id}/traffic` to route production traffic to new version\n\nThis phase demonstrates how event-driven architecture enables automated deployment while maintaining safety through health checks and gradual traffic shifting.\n\n**Phase 4: Monitoring Setup and Feedback**\n\nThe final phase establishes production monitoring for the newly deployed model and sets up feedback loops that can trigger retraining or rollback when issues are detected.\n\n1. **Endpoint Discovery**: Monitoring component calls `GET /api/v1/deployments/endpoints` with model_name=\"credit-fraud-detector\" to discover new production endpoint\n2. **Baseline Establishment**: Monitoring component retrieves training data distributions from experiment artifacts to establish drift detection baseline\n3. **Prediction Logging**: Production inference requests automatically log prediction data via `POST /api/v1/monitoring/predictions` including input features and model outputs\n4. **Drift Analysis**: Monitoring component periodically calls `POST /api/v1/monitoring/models/credit-fraud-detector/drift/analyze` to detect data or concept drift\n5. **Performance Tracking**: Monitoring component tracks model accuracy, latency, and throughput via `GET /api/v1/monitoring/models/credit-fraud-detector/metrics`\n6. **Alert Configuration**: ML operations team configures alert thresholds via `PUT /api/v1/monitoring/models/credit-fraud-detector/thresholds` for automated issue detection\n\nThis phase closes the loop by establishing continuous monitoring that provides feedback about model performance and can trigger new experiment cycles when retraining becomes necessary.\n\n#### Automated Retraining Workflow\n\nThis workflow demonstrates how the platform supports continuous learning by automatically detecting when model performance degrades and triggering retraining pipelines to maintain model quality without manual intervention.\n\n**Trigger Detection and Pipeline Initialization**\n\nThe retraining workflow begins when monitoring components detect performance degradation or data drift beyond acceptable thresholds. This automated detection prevents model quality degradation from impacting business operations.\n\n1. **Drift Detection**: Monitoring component analyzes recent predictions and detects significant data drift with PSI score 0.35 (above threshold 0.25)\n2. **Alert Generation**: Monitoring component creates alert with severity=HIGH and calls alert manager to evaluate escalation policies\n3. **Retraining Decision**: Alert manager determines that drift severity requires automated retraining based on configured policies\n4. **Pipeline Trigger**: Alert manager calls `POST /api/v1/pipelines/trigger` with event_type=\"model_drift_detected\" and payload containing model_name and drift_metrics\n5. **Pipeline Selection**: Pipeline orchestration component matches event to registered retraining pipeline \"credit-fraud-detector-retrain-v1\"\n6. **Pipeline Execution**: Pipeline component calls `POST /api/v1/pipelines/credit-fraud-detector-retrain-v1/runs` with parameters including drift_context and target_model_version\n\nThis trigger mechanism demonstrates how monitoring provides actionable feedback that drives automated improvement without requiring manual intervention to detect and respond to model quality issues.\n\n**Data Pipeline and Model Training**\n\nThe retraining pipeline executes a series of coordinated steps that fetch fresh training data, preprocess features, train new models, and evaluate their performance against current production baselines.\n\n1. **Data Collection**: Pipeline step calls external data sources to fetch training data updated since last training run, including new fraud patterns\n2. **Feature Engineering**: Pipeline step applies same feature transformations used in original training, ensuring consistency with production serving\n3. **Data Validation**: Pipeline step compares new training data distribution against baseline to detect training data quality issues\n4. **Model Training**: Pipeline step executes training with hyperparameters from best previous run plus automated hyperparameter optimization\n5. **Evaluation**: Pipeline step evaluates new model against held-out test set and compares performance metrics to current production model\n6. **Experiment Logging**: Each pipeline step logs parameters, metrics, and artifacts via experiment tracking APIs to maintain complete lineage\n\nThis phase ensures that retraining maintains the same quality standards as manual model development while incorporating the latest available data and potentially improved hyperparameters.\n\n**Model Selection and Deployment**\n\nThe pipeline concludes by automatically selecting the best model variant and deploying it to production if it meets quality criteria, or alerting human operators if manual review is required.\n\n1. **Performance Comparison**: Pipeline step retrieves current production model metrics and compares against newly trained models\n2. **Model Registration**: If new model shows improvement, pipeline step calls model registry to register new version with lineage pointing to retraining run\n3. **Automated Testing**: Pipeline step deploys new model to staging environment and runs automated test suite against known fraud patterns\n4. **Canary Deployment**: If tests pass, pipeline step calls deployment component to start canary deployment routing 5% traffic to new model version\n5. **Performance Monitoring**: Pipeline step monitors canary performance for specified duration, comparing key metrics against main production version\n6. **Full Rollout**: If canary shows improved performance with statistical significance, pipeline step completes rollout by shifting 100% traffic to new version\n\nThis automated deployment phase maintains safety through staged rollouts while enabling continuous model improvement without manual oversight for routine quality improvements.\n\n#### Model Rollback Workflow\n\nThis critical workflow demonstrates how the platform handles production incidents by rapidly reverting to previous model versions when performance degradation or system issues are detected.\n\n**Incident Detection and Response Initiation**\n\nModel rollback workflows typically begin with automated detection of severe performance degradation or system alerts indicating serving failures that require immediate remediation.\n\n1. **Performance Degradation**: Monitoring component detects accuracy drop from 94% to 78% over 30-minute window, exceeding emergency threshold\n2. **Alert Escalation**: Monitoring component generates CRITICAL severity alert and immediately notifies on-call engineering team\n3. **Incident Response**: On-call engineer receives alert and reviews monitoring dashboard showing performance timeline and potential causes\n4. **Rollback Decision**: Engineer determines that issue requires immediate rollback to previous stable model version to restore service quality\n5. **Version Identification**: Engineer calls `GET /api/v1/models/credit-fraud-detector/versions` to identify last known good version in production\n6. **Rollback Initiation**: Engineer calls `POST /api/v1/deployments/{deployment_id}/rollback` with target_version=\"1.1.0\" to begin rollback process\n\nThis rapid response process minimizes the time between incident detection and remediation, reducing business impact from model quality issues.\n\n**Traffic Shifting and Validation**\n\nThe rollback process carefully manages traffic shifting to ensure service continuity while validating that the rollback resolves the detected issues.\n\n1. **Traffic Diversion**: Deployment component immediately shifts 100% traffic from problematic version 1.2.0 to stable version 1.1.0\n2. **Health Verification**: Deployment component monitors rolled-back deployment health via continuous health checks ensuring all replicas serve correctly\n3. **Performance Validation**: Monitoring component tracks post-rollback metrics to verify that accuracy returns to expected baseline levels\n4. **Service Continuity**: Deployment component ensures zero-downtime rollback by maintaining sufficient capacity in previous version before traffic shift\n5. **Confirmation Monitoring**: Engineering team monitors service for 30 minutes post-rollback to confirm that issue resolution is stable\n6. **Incident Documentation**: System automatically logs rollback event with timestamps, reasons, and performance impact metrics for post-incident analysis\n\nThis controlled rollback process prioritizes service restoration while collecting information needed for root cause analysis and process improvement.\n\n### Event-Driven Coordination\n\nThe MLOps platform uses asynchronous events to coordinate complex workflows across components without tight coupling, enabling scalable and resilient operations. Events decouple components by allowing them to react to state changes without direct API dependencies, supporting scenarios where multiple components need to respond to single actions or where response timing varies significantly.\n\n#### Event Architecture and Patterns\n\n**Event-Driven Coordination Mental Model: Newspaper Publishing**\n\nThink of the event system like a newspaper publishing operation. Each component is like a different department—newsroom (experiment tracking), editorial (model registry), printing press (pipeline orchestration), distribution (deployment), and circulation analytics (monitoring). When a major story breaks (experiment completes), the newsroom doesn't call each department individually. Instead, they publish the story to the central editorial system (event coordinator), which then distributes it to all departments that need to know. The editorial department decides which stories get promoted to front page (production stage), the printing press schedules special editions (deployment pipelines), and circulation tracks reader response (monitoring metrics). Each department operates independently but stays coordinated through shared information flow.\n\nThe event coordinator serves as the central newsroom editorial desk, ensuring that important information reaches all relevant parties without requiring each department to know about all others. When the sports department (training pipeline) finishes a major feature story (model training), they publish it once, and the editorial desk handles distributing it to layout (model registry), printing (deployment), and marketing (monitoring) automatically.\n\nThe platform implements **event sourcing** patterns where important state changes are captured as immutable events that can be replayed to reconstruct system state or debug workflow issues. Each event includes correlation IDs that link related activities across components, enabling end-to-end tracing of complex workflows.\n\n**Event Schema and Metadata**\n\nAll platform events follow a consistent schema that includes sufficient metadata for routing, filtering, and audit trail reconstruction. The standardized event structure enables generic event processing infrastructure while supporting component-specific payload formats.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `id` | str | Unique identifier for event deduplication and tracking |\n| `type` | str | Hierarchical event type for filtering (e.g., \"model.promoted\", \"pipeline.completed\") |\n| `source` | str | Component that generated event for audit and debugging |\n| `timestamp` | float | Unix timestamp when event occurred for ordering and correlation |\n| `payload` | Dict[str, Any] | Event-specific data containing relevant state information |\n| `correlation_id` | str | Links related events across workflow execution |\n| `version` | str | Event schema version for backward compatibility |\n| `metadata` | Dict[str, str] | Additional context like user_id, tenant_id for multi-tenant deployments |\n\nThe event payload structure varies by event type but follows consistent naming conventions and includes sufficient information for downstream components to take appropriate actions without additional API calls.\n\n#### Core Platform Events\n\n**Model Lifecycle Events**\n\nModel registry events coordinate promotion workflows and trigger deployment automations when model stages change or new versions become available.\n\n| Event Type | Payload Fields | Description |\n|------------|----------------|-------------|\n| `MODEL_PROMOTED` | model_name, version, old_stage, new_stage, approval_metadata | Model version promoted to new stage |\n| `model.registered` | model_name, version, artifact_uri, run_id, metadata | New model version registered |\n| `model.deprecated` | model_name, version, reason, replacement_version | Model version marked as deprecated |\n| `model.deleted` | model_name, version, deletion_reason | Model version removed from registry |\n\nThe `MODEL_PROMOTED` event is particularly important as it triggers automated deployment workflows when models reach production readiness. The payload includes approval metadata that deployment components can use to apply appropriate deployment strategies based on risk assessment.\n\n**Pipeline Execution Events**\n\nPipeline orchestration events coordinate training workflows and provide status updates that other components use for scheduling and resource management decisions.\n\n| Event Type | Payload Fields | Description |\n|------------|----------------|-------------|\n| `PIPELINE_COMPLETED` | pipeline_id, run_id, status, artifacts, metrics, duration | Pipeline execution finished |\n| `STEP_FAILED` | pipeline_id, run_id, step_id, error_message, retry_count | Individual step failed with error details |\n| `pipeline.started` | pipeline_id, run_id, trigger_type, parameters | Pipeline execution initiated |\n| `RESOURCE_EXHAUSTED` | pipeline_id, run_id, step_id, resource_type, requested, available | Insufficient resources for step |\n\nPipeline completion events often trigger model evaluation and registration workflows, while failure events may trigger automated retry policies or alert escalation depending on the failure type and pipeline criticality.\n\n**Deployment Lifecycle Events**\n\nDeployment events coordinate model serving operations and provide status updates that monitoring components use to establish baseline measurements and alert configurations.\n\n| Event Type | Payload Fields | Description |\n|------------|----------------|-------------|\n| `deployment.started` | deployment_id, model_name, version, endpoint_url, strategy | Model deployment initiated |\n| `deployment.healthy` | deployment_id, model_name, version, replica_count, health_metrics | Deployment reached healthy state |\n| `DEPLOYMENT_FAILED` | deployment_id, model_name, version, error_message, rollback_version | Deployment failed with rollback info |\n| `traffic.shifted` | deployment_id, model_name, old_weights, new_weights, reason | Traffic routing updated |\n\nDeployment health events trigger monitoring setup, while failure events may trigger automatic rollback workflows or escalation to on-call teams depending on the deployment strategy and business criticality.\n\n**Monitoring and Alert Events**\n\nMonitoring events provide feedback that drives retraining decisions and alert escalation, closing the loop between production performance and model improvement workflows.\n\n| Event Type | Payload Fields | Description |\n|------------|----------------|-------------|\n| `drift.detected` | model_name, drift_type, severity, features, metrics, threshold | Data or concept drift detected |\n| `performance.degraded` | model_name, metric_name, current_value, baseline_value, severity | Model performance below threshold |\n| `alert.triggered` | alert_id, model_name, severity, message, escalation_level | Alert condition met |\n| `baseline.updated` | model_name, feature_names, distribution_metrics, version | Baseline distributions updated |\n\nDrift detection events often trigger automated retraining pipelines, while performance degradation events may trigger deployment rollbacks or manual investigation workflows depending on the severity and configured response policies.\n\n#### Event Coordination Workflows\n\n**Experiment-to-Production Coordination**\n\nThis event sequence demonstrates how asynchronous events coordinate the complete workflow from experiment completion to production monitoring without requiring components to directly orchestrate each other.\n\n1. **Experiment Completion**: Experiment tracking publishes `experiment.completed` event with run_id, metrics, and artifact locations\n2. **Model Evaluation**: Model registry subscribes to experiment completion and evaluates whether results meet promotion criteria\n3. **Automatic Registration**: If criteria are met, model registry automatically registers new version and publishes `model.registered` event\n4. **Deployment Trigger**: Deployment component receives `model.registered` event and checks if model should be deployed based on stage and policies\n5. **Monitoring Setup**: Monitoring component receives `deployment.healthy` event and automatically establishes baseline monitoring for new model\n6. **Feedback Loop**: Monitoring publishes `drift.detected` or `performance.degraded` events that can trigger new experiment cycles\n\nThis event-driven coordination means that data scientists only need to run experiments and mark them as successful—the platform handles model registration, deployment, and monitoring setup automatically based on configured policies.\n\n**Incident Response Coordination**\n\nEvent-driven incident response demonstrates how the platform can automatically respond to production issues while maintaining audit trails and escalation policies.\n\n1. **Performance Detection**: Monitoring component detects accuracy degradation and publishes `performance.degraded` event with severity=CRITICAL\n2. **Automated Response**: Deployment component receives performance degradation event and evaluates rollback policies for the affected model\n3. **Rollback Execution**: If policies indicate automatic rollback, deployment component executes rollback and publishes `deployment.rollback` event\n4. **Alert Escalation**: Alert manager receives rollback event and escalates to on-call team with context about automatic actions taken\n5. **Retraining Trigger**: Pipeline orchestration receives performance degradation event and schedules emergency retraining pipeline\n6. **Resolution Tracking**: All components log their responses to the original correlation_id, enabling complete incident timeline reconstruction\n\nThis automated incident response reduces mean time to recovery while ensuring that human operators receive complete context about automatic actions taken during the incident.\n\n> **Decision: Event-Driven Architecture vs Direct API Orchestration**\n> - **Context**: Components need to coordinate complex workflows involving multiple stages and potential retry/rollback scenarios\n> - **Options Considered**: Direct API calls between components, centralized workflow orchestrator, event-driven coordination\n> - **Decision**: Event-driven coordination with centralized event coordinator\n> - **Rationale**: Events provide loose coupling that enables independent component scaling and development, support complex workflow patterns like fan-out/fan-in, and create natural audit trails for compliance and debugging\n> - **Consequences**: Adds complexity in event ordering and delivery guarantees, but enables more resilient and scalable coordination patterns\n\n#### Event Delivery and Reliability\n\n**Event Coordinator Implementation**\n\nThe EventCoordinator provides reliable event delivery with ordering guarantees and failure handling that ensures workflow coordination remains consistent even during component failures or network partitions.\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `publish` | event, synchronous | bool | Publish event to registered subscribers |\n| `subscribe` | event_type, handler | subscription_id | Register event handler function |\n| `unsubscribe` | subscription_id | bool | Remove event subscription |\n| `replay_events` | start_time, end_time, event_types | List[Event] | Replay events for debugging or recovery |\n| `get_event_history` | correlation_id | List[Event] | Get all events for workflow tracing |\n\nThe event coordinator implements **at-least-once delivery** guarantees by persisting events to durable storage before notifying subscribers. Subscribers must implement idempotent event handling to manage potential duplicate deliveries during failure scenarios.\n\n**Event Ordering and Causality**\n\nThe platform maintains **causal ordering** for events that affect the same resources (model, pipeline, deployment) while allowing concurrent processing of independent events. This ensures that model promotion events are processed before deployment events for the same model version, preventing race conditions that could deploy outdated versions.\n\n| Ordering Guarantee | Scope | Implementation |\n|-------------------|-------|----------------|\n| Total order | Events affecting same model version | Sequential processing per model partition |\n| Causal order | Events in same workflow correlation | Vector clock timestamps |\n| No ordering | Independent model workflows | Parallel processing across partitions |\n\n**Event Storage and Replay**\n\nAll events are persisted to an append-only event log that enables replay for debugging, audit compliance, and disaster recovery scenarios. The event log includes event metadata and payload data with retention policies based on regulatory requirements and operational needs.\n\n| Retention Policy | Duration | Purpose |\n|------------------|----------|---------|\n| Real-time processing | 7 days | Active workflow coordination |\n| Audit compliance | 7 years | Regulatory audit trails |\n| Debugging replay | 90 days | Incident investigation and troubleshooting |\n| Performance analysis | 1 year | Workflow optimization and capacity planning |\n\n### Common Pitfalls\n\n⚠️ **Pitfall: API Versioning Inconsistency**\nComponent APIs evolve independently, leading to compatibility issues when different components expect different API versions. This manifests as cryptic errors when new model registry versions return additional fields that older deployment components don't expect, causing deployment failures. Fix this by implementing explicit API version headers in all requests and maintaining backward compatibility for at least two major versions. Use content negotiation to allow clients to specify which response format they support.\n\n⚠️ **Pitfall: Event Ordering Race Conditions**\nPublishing model promotion and deployment events simultaneously can cause deployments to start before model artifacts are fully replicated across storage systems. This results in deployment failures with \"artifact not found\" errors that resolve when retried later. Prevent this by implementing event dependencies where deployment events include artifact readiness checks, or use event sequencing where model registry confirms artifact replication before publishing promotion events.\n\n⚠️ **Pitfall: Missing Correlation ID Propagation**\nEvents related to the same workflow use different correlation IDs, making it impossible to trace complete workflow execution during debugging or compliance audits. This happens when components generate new correlation IDs instead of propagating existing ones from upstream events. Fix this by requiring all API calls to accept optional correlation IDs and propagating them through all downstream events and API calls.\n\n⚠️ **Pitfall: Event Payload Size Explosion**\nIncluding complete model metadata or large artifact lists in event payloads causes event delivery timeouts and memory issues in event processing systems. Events should contain only essential identifiers and lightweight metadata, with consumers making additional API calls to fetch detailed information when needed. Use event payload size limits (e.g., 1MB) and design events to reference resources rather than embedding them.\n\n⚠️ **Pitfall: Synchronous Event Processing Blocking**\nComponents that process events synchronously can block event delivery when processing takes significant time, such as downloading large model artifacts or running complex validation checks. This causes event backlogs and workflow delays. Implement asynchronous event processing where event handlers quickly acknowledge receipt and perform actual work in background tasks.\n\n⚠️ **Pitfall: Missing Event Deduplication**\nNetwork retries and component restarts can cause duplicate event delivery, leading to duplicate model registrations, multiple deployment attempts, or redundant monitoring setup. Implement idempotent event handlers that check if the requested action has already been completed before proceeding, using resource checksums or unique request identifiers to detect duplicates.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| API Framework | Flask/FastAPI with OpenAPI specs | Django REST Framework with auto-generated clients |\n| Event Broker | Redis Streams with consumer groups | Apache Kafka with Schema Registry |\n| API Gateway | nginx reverse proxy with basic routing | Kong or Envoy with rate limiting and authentication |\n| Service Discovery | Static configuration files | Consul or etcd with health checking |\n| Event Schema | JSON with manual validation | Protocol Buffers with automatic validation |\n| API Authentication | JWT tokens with shared secrets | OAuth2 with PKCE and token introspection |\n\n#### Recommended File Structure\n\n```\nmlops-platform/\n├── services/\n│   ├── experiment-tracking/\n│   │   ├── src/api/\n│   │   │   ├── external_api.py      # Client-facing REST endpoints\n│   │   │   ├── internal_api.py      # Inter-component endpoints\n│   │   │   └── schemas.py           # Request/response models\n│   │   ├── src/events/\n│   │   │   ├── handlers.py          # Event processing logic\n│   │   │   └── publishers.py        # Event publishing utilities\n│   │   └── tests/integration/\n│   │       └── api_integration_test.py\n│   ├── model-registry/\n│   │   ├── src/api/\n│   │   ├── src/events/\n│   │   └── tests/\n│   └── shared/\n│       ├── event_coordinator/\n│       │   ├── coordinator.py       # Event broker abstraction\n│       │   ├── event_schemas.py     # Common event definitions\n│       │   └── delivery.py          # Reliability and ordering\n│       ├── api_client/\n│       │   ├── base_client.py       # Common HTTP client functionality\n│       │   ├── auth.py              # JWT handling and refresh\n│       │   └── retry.py             # Retry policies and circuit breakers\n│       └── monitoring/\n│           ├── health_checks.py     # Component health monitoring\n│           ├── metrics.py           # Prometheus metrics collection\n│           └── tracing.py           # Request correlation and tracing\n```\n\n#### Event Coordinator Infrastructure Code\n\n```python\n\"\"\"\nEvent coordination infrastructure providing reliable delivery and ordering guarantees.\nThis module implements the central event distribution system that enables loose coupling\nbetween MLOps platform components while maintaining workflow consistency.\n\"\"\"\n\nimport json\nimport time\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Any, Optional, Callable\nimport threading\nimport queue\nimport logging\n\n@dataclass\nclass Event:\n    \"\"\"Immutable event object with standardized metadata for platform coordination.\"\"\"\n    id: str\n    type: str\n    source: str\n    timestamp: float\n    payload: Dict[str, Any]\n    correlation_id: Optional[str] = None\n    version: str = \"1.0\"\n    metadata: Optional[Dict[str, str]] = None\n    \n    @classmethod\n    def create(cls, event_type: str, source: str, payload: Dict[str, Any]) -> 'Event':\n        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"\n        return cls(\n            id=str(uuid.uuid4()),\n            type=event_type,\n            source=source,\n            timestamp=time.time(),\n            payload=payload,\n            correlation_id=payload.get('correlation_id'),\n            metadata={}\n        )\n    \n    def to_json(self) -> str:\n        \"\"\"Serialize event for storage and transmission.\"\"\"\n        return json.dumps(asdict(self))\n    \n    @classmethod\n    def from_json(cls, json_str: str) -> 'Event':\n        \"\"\"Deserialize event from JSON string.\"\"\"\n        data = json.loads(json_str)\n        return cls(**data)\n\nclass EventHandler:\n    \"\"\"Wrapper for event handler functions with error handling and retry logic.\"\"\"\n    \n    def __init__(self, handler_func: Callable[[Event], None], max_retries: int = 3):\n        self.handler_func = handler_func\n        self.max_retries = max_retries\n        self.logger = logging.getLogger(f\"event_handler.{handler_func.__name__}\")\n    \n    def handle(self, event: Event) -> bool:\n        \"\"\"Execute handler with retry logic, returns True if successful.\"\"\"\n        for attempt in range(self.max_retries + 1):\n            try:\n                self.handler_func(event)\n                return True\n            except Exception as e:\n                self.logger.warning(f\"Handler attempt {attempt + 1} failed: {e}\")\n                if attempt == self.max_retries:\n                    self.logger.error(f\"Handler failed after {self.max_retries + 1} attempts: {e}\")\n                    return False\n                time.sleep(2 ** attempt)  # Exponential backoff\n        return False\n\nclass EventStorage(ABC):\n    \"\"\"Abstract interface for event persistence supporting audit and replay capabilities.\"\"\"\n    \n    @abstractmethod\n    def store_event(self, event: Event) -> bool:\n        \"\"\"Persist event to durable storage.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_events(self, start_time: float, end_time: float, \n                   event_types: Optional[List[str]] = None) -> List[Event]:\n        \"\"\"Retrieve events for replay or audit purposes.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_events_by_correlation(self, correlation_id: str) -> List[Event]:\n        \"\"\"Get all events for a specific workflow or operation.\"\"\"\n        pass\n\nclass InMemoryEventStorage(EventStorage):\n    \"\"\"Simple in-memory event storage for development and testing.\"\"\"\n    \n    def __init__(self):\n        self.events: List[Event] = []\n        self.lock = threading.Lock()\n    \n    def store_event(self, event: Event) -> bool:\n        \"\"\"Store event in memory with thread safety.\"\"\"\n        with self.lock:\n            self.events.append(event)\n            return True\n    \n    def get_events(self, start_time: float, end_time: float, \n                   event_types: Optional[List[str]] = None) -> List[Event]:\n        \"\"\"Filter events by time range and optional type filter.\"\"\"\n        with self.lock:\n            filtered = [e for e in self.events \n                       if start_time <= e.timestamp <= end_time]\n            if event_types:\n                filtered = [e for e in filtered if e.type in event_types]\n            return filtered\n    \n    def get_events_by_correlation(self, correlation_id: str) -> List[Event]:\n        \"\"\"Get all events with matching correlation ID.\"\"\"\n        with self.lock:\n            return [e for e in self.events if e.correlation_id == correlation_id]\n\nclass EventCoordinator:\n    \"\"\"Central event coordination system managing subscription and delivery.\"\"\"\n    \n    def __init__(self, storage: EventStorage):\n        self.storage = storage\n        self.subscriptions: Dict[str, List[EventHandler]] = {}\n        self.event_queue = queue.Queue()\n        self.processing_thread = None\n        self.running = False\n        self.logger = logging.getLogger(\"event_coordinator\")\n        self.lock = threading.Lock()\n    \n    def start(self):\n        \"\"\"Start event processing thread.\"\"\"\n        if not self.running:\n            self.running = True\n            self.processing_thread = threading.Thread(target=self._process_events)\n            self.processing_thread.daemon = True\n            self.processing_thread.start()\n            self.logger.info(\"Event coordinator started\")\n    \n    def stop(self):\n        \"\"\"Stop event processing and wait for thread completion.\"\"\"\n        if self.running:\n            self.running = False\n            if self.processing_thread:\n                self.processing_thread.join(timeout=5.0)\n            self.logger.info(\"Event coordinator stopped\")\n    \n    def publish(self, event: Event, synchronous: bool = False) -> bool:\n        \"\"\"Publish event to subscribers with optional synchronous processing.\"\"\"\n        # Store event for audit trail\n        if not self.storage.store_event(event):\n            self.logger.error(f\"Failed to store event {event.id}\")\n            return False\n        \n        if synchronous:\n            return self._deliver_event(event)\n        else:\n            self.event_queue.put(event)\n            return True\n    \n    def subscribe(self, event_type: str, handler: Callable[[Event], None]) -> str:\n        \"\"\"Register event handler for specific event type.\"\"\"\n        subscription_id = str(uuid.uuid4())\n        event_handler = EventHandler(handler)\n        \n        with self.lock:\n            if event_type not in self.subscriptions:\n                self.subscriptions[event_type] = []\n            self.subscriptions[event_type].append(event_handler)\n        \n        self.logger.info(f\"Registered handler for {event_type}, subscription {subscription_id}\")\n        return subscription_id\n    \n    def _process_events(self):\n        \"\"\"Background thread that processes events from queue.\"\"\"\n        while self.running:\n            try:\n                event = self.event_queue.get(timeout=1.0)\n                self._deliver_event(event)\n                self.event_queue.task_done()\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.logger.error(f\"Error processing event: {e}\")\n    \n    def _deliver_event(self, event: Event) -> bool:\n        \"\"\"Deliver event to all registered handlers.\"\"\"\n        handlers = self.subscriptions.get(event.type, [])\n        if not handlers:\n            self.logger.debug(f\"No handlers registered for event type {event.type}\")\n            return True\n        \n        success_count = 0\n        for handler in handlers:\n            if handler.handle(event):\n                success_count += 1\n        \n        self.logger.info(f\"Delivered event {event.id} to {success_count}/{len(handlers)} handlers\")\n        return success_count == len(handlers)\n    \n    def replay_events(self, start_time: float, end_time: float, \n                     event_types: Optional[List[str]] = None) -> int:\n        \"\"\"Replay stored events for debugging or recovery.\"\"\"\n        events = self.storage.get_events(start_time, end_time, event_types)\n        replayed = 0\n        \n        for event in events:\n            if self._deliver_event(event):\n                replayed += 1\n        \n        self.logger.info(f\"Replayed {replayed}/{len(events)} events\")\n        return replayed\n    \n    def get_event_history(self, correlation_id: str) -> List[Event]:\n        \"\"\"Get complete event history for workflow tracing.\"\"\"\n        return self.storage.get_events_by_correlation(correlation_id)\n\n# Event type constants for type safety and consistency\nMODEL_PROMOTED = \"model.promoted\"\nPIPELINE_COMPLETED = \"pipeline.completed\"  \nDEPLOYMENT_FAILED = \"deployment.failed\"\nSTEP_FAILED = \"pipeline.step.failed\"\nRESOURCE_EXHAUSTED = \"pipeline.resource.exhausted\"\nEXPERIMENT_COMPLETED = \"experiment.completed\"\n\n# Global event coordinator instance (initialized by main application)\n_event_coordinator: Optional[EventCoordinator] = None\n\ndef get_event_coordinator() -> EventCoordinator:\n    \"\"\"Get global event coordinator instance.\"\"\"\n    global _event_coordinator\n    if _event_coordinator is None:\n        raise RuntimeError(\"Event coordinator not initialized\")\n    return _event_coordinator\n\ndef initialize_event_coordinator(storage: EventStorage) -> EventCoordinator:\n    \"\"\"Initialize global event coordinator with storage backend.\"\"\"\n    global _event_coordinator\n    _event_coordinator = EventCoordinator(storage)\n    _event_coordinator.start()\n    return _event_coordinator\n```\n\n#### API Client Infrastructure Code\n\n```python\n\"\"\"\nCommon API client infrastructure for inter-component communication.\nProvides authentication, retry logic, and circuit breakers for reliable\ndistributed system communication between MLOps platform components.\n\"\"\"\n\nimport requests\nimport time\nimport json\nfrom typing import Dict, Any, Optional, List\nfrom dataclasses import dataclass\nimport logging\nfrom enum import Enum\n\nclass CircuitState(Enum):\n    \"\"\"Circuit breaker states for handling downstream service failures.\"\"\"\n    CLOSED = \"closed\"      # Normal operation, requests pass through\n    OPEN = \"open\"          # Failing fast, requests rejected immediately\n    HALF_OPEN = \"half_open\"  # Testing if downstream service recovered\n\n@dataclass\nclass RetryConfig:\n    \"\"\"Configuration for exponential backoff retry policies.\"\"\"\n    max_attempts: int = 3\n    initial_delay: float = 1.0\n    max_delay: float = 60.0\n    backoff_multiplier: float = 2.0\n    retryable_status_codes: List[int] = None\n    \n    def __post_init__(self):\n        if self.retryable_status_codes is None:\n            self.retryable_status_codes = [500, 502, 503, 504, 429]\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker implementation preventing cascade failures.\"\"\"\n    \n    def __init__(self, failure_threshold: int = 5, reset_timeout: float = 60.0):\n        self.failure_threshold = failure_threshold\n        self.reset_timeout = reset_timeout\n        self.failure_count = 0\n        self.last_failure_time = 0\n        self.state = CircuitState.CLOSED\n        self.logger = logging.getLogger(\"circuit_breaker\")\n    \n    def can_execute(self) -> bool:\n        \"\"\"Check if request should be allowed through circuit breaker.\"\"\"\n        if self.state == CircuitState.CLOSED:\n            return True\n        elif self.state == CircuitState.OPEN:\n            if time.time() - self.last_failure_time > self.reset_timeout:\n                self.state = CircuitState.HALF_OPEN\n                self.logger.info(\"Circuit breaker entering half-open state\")\n                return True\n            return False\n        elif self.state == CircuitState.HALF_OPEN:\n            return True\n        return False\n    \n    def record_success(self):\n        \"\"\"Record successful request, potentially closing circuit.\"\"\"\n        if self.state == CircuitState.HALF_OPEN:\n            self.state = CircuitState.CLOSED\n            self.failure_count = 0\n            self.logger.info(\"Circuit breaker closed after successful request\")\n    \n    def record_failure(self):\n        \"\"\"Record failed request, potentially opening circuit.\"\"\"\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        \n        if self.failure_count >= self.failure_threshold:\n            self.state = CircuitState.OPEN\n            self.logger.warning(f\"Circuit breaker opened after {self.failure_count} failures\")\n\nclass APIClient:\n    \"\"\"Base HTTP client with authentication, retries, and circuit breaking.\"\"\"\n    \n    def __init__(self, base_url: str, auth_token: Optional[str] = None,\n                 retry_config: Optional[RetryConfig] = None):\n        self.base_url = base_url.rstrip('/')\n        self.auth_token = auth_token\n        self.retry_config = retry_config or RetryConfig()\n        self.circuit_breaker = CircuitBreaker()\n        self.session = requests.Session()\n        self.logger = logging.getLogger(\"api_client\")\n        \n        # Set default headers\n        self.session.headers.update({\n            'Content-Type': 'application/json',\n            'Accept': 'application/json',\n            'User-Agent': 'MLOps-Platform/1.0'\n        })\n        \n        if self.auth_token:\n            self.session.headers['Authorization'] = f'Bearer {self.auth_token}'\n    \n    def _should_retry(self, response: requests.Response, attempt: int) -> bool:\n        \"\"\"Determine if request should be retried based on response and attempt count.\"\"\"\n        if attempt >= self.retry_config.max_attempts:\n            return False\n        \n        return (response.status_code in self.retry_config.retryable_status_codes or\n                response.status_code == requests.codes.request_timeout)\n    \n    def _calculate_delay(self, attempt: int) -> float:\n        \"\"\"Calculate exponential backoff delay for retry attempt.\"\"\"\n        delay = self.retry_config.initial_delay * (self.retry_config.backoff_multiplier ** attempt)\n        return min(delay, self.retry_config.max_delay)\n    \n    def request(self, method: str, endpoint: str, **kwargs) -> requests.Response:\n        \"\"\"Make HTTP request with retry logic and circuit breaker protection.\"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        \n        # Check circuit breaker\n        if not self.circuit_breaker.can_execute():\n            raise requests.exceptions.HTTPError(\"Circuit breaker is open\")\n        \n        last_exception = None\n        \n        for attempt in range(self.retry_config.max_attempts):\n            try:\n                response = self.session.request(method, url, **kwargs)\n                \n                # Record success for circuit breaker\n                if response.status_code < 500:\n                    self.circuit_breaker.record_success()\n                \n                # Check if we should retry\n                if not self._should_retry(response, attempt):\n                    return response\n                \n                self.logger.warning(f\"Request failed with status {response.status_code}, \"\n                                  f\"attempt {attempt + 1}/{self.retry_config.max_attempts}\")\n                \n            except requests.exceptions.RequestException as e:\n                last_exception = e\n                self.circuit_breaker.record_failure()\n                self.logger.warning(f\"Request exception on attempt {attempt + 1}: {e}\")\n            \n            # Wait before retry (except on last attempt)\n            if attempt < self.retry_config.max_attempts - 1:\n                delay = self._calculate_delay(attempt)\n                time.sleep(delay)\n        \n        # All retries exhausted\n        if last_exception:\n            raise last_exception\n        else:\n            raise requests.exceptions.HTTPError(f\"Request failed after {self.retry_config.max_attempts} attempts\")\n    \n    def get(self, endpoint: str, params: Optional[Dict] = None) -> requests.Response:\n        \"\"\"Make GET request with error handling.\"\"\"\n        return self.request('GET', endpoint, params=params)\n    \n    def post(self, endpoint: str, data: Optional[Dict] = None) -> requests.Response:\n        \"\"\"Make POST request with JSON payload.\"\"\"\n        json_data = json.dumps(data) if data else None\n        return self.request('POST', endpoint, data=json_data)\n    \n    def put(self, endpoint: str, data: Optional[Dict] = None) -> requests.Response:\n        \"\"\"Make PUT request with JSON payload.\"\"\"\n        json_data = json.dumps(data) if data else None\n        return self.request('PUT', endpoint, data=json_data)\n    \n    def delete(self, endpoint: str) -> requests.Response:\n        \"\"\"Make DELETE request.\"\"\"\n        return self.request('DELETE', endpoint)\n```\n\n#### Component Integration Skeleton\n\n```python\n\"\"\"\nExample integration patterns for MLOps platform components.\nDemonstrates how components should interact through APIs and events\nwhile maintaining loose coupling and error resilience.\n\"\"\"\n\nfrom typing import Dict, Any, Optional, List\nimport logging\nfrom dataclasses import dataclass\n\n# Import shared infrastructure\nfrom shared.event_coordinator import Event, get_event_coordinator, MODEL_PROMOTED, PIPELINE_COMPLETED\nfrom shared.api_client import APIClient\n\n@dataclass\nclass ComponentConfig:\n    \"\"\"Configuration for component integration.\"\"\"\n    component_name: str\n    base_url: str\n    auth_token: str\n    event_subscriptions: List[str]\n\nclass MLOpsComponent:\n    \"\"\"Base class for MLOps platform components with common integration patterns.\"\"\"\n    \n    def __init__(self, config: ComponentConfig):\n        self.config = config\n        self.logger = logging.getLogger(f\"component.{config.component_name}\")\n        self.api_clients: Dict[str, APIClient] = {}\n        self.event_coordinator = get_event_coordinator()\n        self._setup_event_subscriptions()\n    \n    def _setup_event_subscriptions(self):\n        \"\"\"Register event handlers for component-specific events.\"\"\"\n        for event_type in self.config.event_subscriptions:\n            self.event_coordinator.subscribe(event_type, self._handle_event)\n            self.logger.info(f\"Subscribed to event type: {event_type}\")\n    \n    def _handle_event(self, event: Event):\n        \"\"\"Route events to specific handler methods based on event type.\"\"\"\n        handler_name = f\"_handle_{event.type.replace('.', '_')}\"\n        handler = getattr(self, handler_name, None)\n        \n        if handler:\n            try:\n                handler(event)\n            except Exception as e:\n                self.logger.error(f\"Error handling event {event.type}: {e}\")\n        else:\n            self.logger.warning(f\"No handler found for event type: {event.type}\")\n    \n    def get_api_client(self, service_name: str, base_url: str) -> APIClient:\n        \"\"\"Get or create API client for external service.\"\"\"\n        if service_name not in self.api_clients:\n            self.api_clients[service_name] = APIClient(\n                base_url=base_url,\n                auth_token=self.config.auth_token\n            )\n        return self.api_clients[service_name]\n    \n    def publish_event(self, event_type: str, payload: Dict[str, Any]):\n        \"\"\"Publish event with component source information.\"\"\"\n        event = Event.create(\n            event_type=event_type,\n            source=self.config.component_name,\n            payload=payload\n        )\n        self.event_coordinator.publish(event)\n        self.logger.info(f\"Published event: {event_type}\")\n\nclass ModelRegistryIntegration(MLOpsComponent):\n    \"\"\"Example integration for Model Registry component.\"\"\"\n    \n    def __init__(self, config: ComponentConfig):\n        super().__init__(config)\n        self.experiment_client = self.get_api_client(\n            'experiment_tracking', \n            'http://experiment-service:8080'\n        )\n    \n    def register_model_from_experiment(self, model_name: str, version: str, \n                                     run_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Register model version with lineage to experiment run.\n        TODO: Implement complete model registration workflow\n        TODO: Validate run_id exists in experiment tracking\n        TODO: Download and validate model artifact\n        TODO: Create model version entry with metadata\n        TODO: Publish model.registered event for downstream components\n        \"\"\"\n        # Get experiment run metadata for lineage\n        response = self.experiment_client.get(f'/api/v1/runs/{run_id}')\n        # TODO: Handle response errors and missing runs\n        \n        run_data = response.json()\n        # TODO: Extract relevant metadata (parameters, metrics, artifacts)\n        \n        # TODO: Register model version with extracted metadata\n        # TODO: Return model version details\n        pass\n    \n    def _handle_experiment_completed(self, event: Event):\n        \"\"\"\n        Handle experiment completion events for automatic model registration.\n        TODO: Evaluate if experiment results meet registration criteria\n        TODO: Check if automatic registration is configured for experiment\n        TODO: Call register_model_from_experiment if criteria are met\n        TODO: Handle registration failures gracefully\n        \"\"\"\n        pass\n\nclass DeploymentIntegration(MLOpsComponent):\n    \"\"\"Example integration for Model Deployment component.\"\"\"\n    \n    def __init__(self, config: ComponentConfig):\n        super().__init__(config)\n        self.model_registry_client = self.get_api_client(\n            'model_registry',\n            'http://model-registry:8080'\n        )\n        self.monitoring_client = self.get_api_client(\n            'monitoring',\n            'http://monitoring:8080'\n        )\n    \n    def _handle_model_promoted(self, event: Event):\n        \"\"\"\n        Handle model promotion events for automatic deployment.\n        TODO: Check if promoted stage requires automatic deployment\n        TODO: Retrieve model version details from registry\n        TODO: Create deployment specification based on model metadata\n        TODO: Execute deployment using configured strategy (blue-green, canary)\n        TODO: Monitor deployment health and publish deployment.healthy event\n        TODO: Setup monitoring baseline for newly deployed model\n        \"\"\"\n        model_name = event.payload['model_name']\n        version = event.payload['version']\n        new_stage = event.payload['new_stage']\n        \n        if new_stage == 'Production':\n            # TODO: Implement automatic production deployment\n            pass\n    \n    def deploy_model_version(self, model_name: str, version: str, \n                           deployment_spec: Dict[str, Any]) -> str:\n        \"\"\"\n        Deploy specific model version with given specification.\n        TODO: Validate deployment specification\n        TODO: Download model artifacts from registry\n        TODO: Create serving container with model\n        TODO: Configure auto-scaling and health checks\n        TODO: Set up traffic routing (canary or blue-green)\n        TODO: Return deployment ID for tracking\n        \"\"\"\n        pass\n\n# Example usage and testing patterns\ndef setup_component_integration():\n    \"\"\"\n    Example setup for component integration in main application.\n    TODO: Load configuration from environment variables\n    TODO: Initialize event coordinator with production storage\n    TODO: Create and start all component instances\n    TODO: Setup health check endpoints for each component\n    \"\"\"\n    pass\n\n# Milestone checkpoint: After implementing component integration\ndef test_integration_workflow():\n    \"\"\"\n    Integration test demonstrating complete workflow coordination.\n    Expected behavior:\n    1. Start experiment tracking and model registry components\n    2. Create experiment and log training run with model artifact\n    3. Verify model.registered event is published automatically\n    4. Verify deployment component receives and processes event\n    5. Check that model endpoint becomes available and healthy\n    \n    Run this test to verify: python -m pytest tests/integration/test_component_coordination.py\n    Expected output: All workflow steps complete successfully with events logged\n    \"\"\"\n    pass\n```\n\n#### Debugging Tips for Component Interactions\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| Events published but not received | Subscription registration failed or wrong event type | Check event coordinator logs for subscription confirmations | Verify event type strings match exactly between publishers and subscribers |\n| API calls timeout during high load | Circuit breaker opening due to downstream failures | Check API client circuit breaker state and failure counts | Increase circuit breaker thresholds or improve downstream service performance |\n| Workflow steps execute out of order | Event processing happening concurrently without ordering | Review event timestamps and correlation IDs | Implement event sequencing for same-resource operations |\n| Duplicate actions on workflow retry | Event handlers not idempotent | Check for duplicate model registrations or deployments | Add idempotency checks using resource checksums or unique identifiers |\n| Missing correlation between workflow steps | Correlation IDs not propagated | Search event logs for missing correlation ID fields | Ensure all API calls and events include correlation_id from upstream requests |\n| Component integration failures | Authentication tokens expired or invalid | Check API response status codes and authentication headers | Implement token refresh logic or verify token scope permissions |\n\n#### Milestone Checkpoint\n\nAfter implementing component interactions and data flow:\n\n**Verification Command**: `python -m pytest tests/integration/test_end_to_end_workflow.py -v`\n\n**Expected Behavior**:\n1. **Event Coordination**: Events published by one component are received and processed by subscribed components within 5 seconds\n2. **API Integration**: Components can successfully call each other's APIs with proper authentication and retry handling\n3. **Workflow Completion**: Complete experiment-to-deployment workflow completes successfully with all intermediate events logged\n4. **Error Recovery**: Failed API calls are retried according to configured policies, and circuit breakers prevent cascade failures\n5. **Audit Trail**: All workflow steps can be traced using correlation IDs through event and API logs\n\n**Manual Testing**:\n1. Start all components: `docker-compose up -d`\n2. Create experiment and run training: `curl -X POST http://localhost:8080/api/v1/experiments -d '{\"name\": \"test-workflow\"}'`\n3. Verify model registration: Check model registry UI shows new model version\n4. Verify deployment: Check that model endpoint responds to prediction requests\n5. Verify monitoring: Check that prediction requests are logged and baseline is established\n\n**Troubleshooting**: If workflow steps fail, check event coordinator logs for delivery failures and component logs for API integration errors. Use correlation IDs to trace specific workflow execution through all components.\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** This section applies to all milestones (1-5) by providing comprehensive failure mode analysis and recovery mechanisms that ensure the platform remains operational despite component failures, data corruption, and edge cases.\n\nBuilding a robust MLOps platform requires anticipating and gracefully handling the myriad ways distributed systems can fail. Think of error handling in an MLOps platform like designing a hospital's emergency response system - you need to identify every possible crisis, establish detection procedures, and create recovery protocols that minimize harm while restoring normal operations. Unlike simple applications that might crash and restart, an MLOps platform manages long-running training jobs, production model endpoints serving live traffic, and valuable experiment data that cannot be lost.\n\nThe complexity of error handling in MLOps stems from the platform's distributed nature and the diverse types of failures that can occur. Training pipelines might fail due to resource constraints, model deployments might encounter version incompatibilities, and monitoring systems might detect data drift requiring immediate intervention. Each component must handle both internal failures and cascading failures from dependent components, while maintaining data consistency and enabling automated recovery wherever possible.\n\n### System Failure Modes\n\nUnderstanding failure modes requires examining each component's critical dependencies and the ways they can break. Like a medical diagnosis guide, we categorize failures by symptoms, root causes, and affected systems to enable rapid identification and response.\n\n#### Experiment Tracking Failures\n\nThe experiment tracking component faces several critical failure modes that can disrupt the research workflow and cause data loss. These failures typically manifest as inability to log new experiments, missing historical data, or corrupted artifact storage.\n\n**Mental Model: Research Laboratory Breakdown**\nThink of experiment tracking failures like equipment failures in a research laboratory. A broken scale means you can't measure new samples, but existing measurements remain valid. A fire in the storage room destroys historical samples but doesn't prevent new experiments. A power outage stops all work until restored. Each type of failure requires different emergency procedures and recovery strategies.\n\n**Database Connection Failures** occur when the metadata store becomes unreachable due to network partitions, database server crashes, or connection pool exhaustion. These failures prevent logging new experiments while leaving existing data intact.\n\n| Failure Symptom | Root Cause | Immediate Impact | Downstream Effects |\n|------------------|------------|------------------|-------------------|\n| Connection timeout on log_param calls | Database server overload | Cannot log new parameters | Training scripts hang waiting for logging |\n| \"Too many connections\" errors | Connection pool exhaustion | New experiment creation fails | Researchers cannot start new runs |\n| Intermittent query failures | Network partition to database | Inconsistent data retrieval | Run comparison views show incomplete results |\n| Database lock timeouts | Concurrent write conflicts | Metric logging operations fail | Training progress not tracked |\n\n**Artifact Storage Failures** manifest when the object storage system experiences outages, quota exhaustion, or corruption. Unlike metadata failures, artifact failures can cause permanent data loss if not handled properly.\n\n| Failure Type | Detection Method | Data at Risk | Recovery Approach |\n|--------------|------------------|---------------|-------------------|\n| S3 bucket unreachable | HTTP 503 responses | New artifacts only | Retry with exponential backoff |\n| Storage quota exceeded | HTTP 413 responses | All new artifacts | Trigger artifact cleanup policies |\n| Corrupted artifact downloads | Checksum validation failure | Specific artifacts | Re-upload from training environment |\n| Permission denied errors | HTTP 403 responses | Component-specific artifacts | Update IAM policies and rotate credentials |\n\n**Metadata Corruption** represents the most serious failure mode, where stored experiment data becomes inconsistent or unreadable. This typically occurs during partial writes, concurrent modifications, or storage system bugs.\n\nThe experiment tracking system must detect corruption early through consistency checks and provide recovery mechanisms that minimize data loss. Corruption scenarios include orphaned artifacts (metadata points to non-existent files), missing foreign keys (runs reference non-existent experiments), and inconsistent timestamps (end time before start time).\n\n#### Model Registry Failures\n\nThe model registry's failure modes center around version consistency, artifact integrity, and stage transition workflows. Since the registry serves as the authoritative source for production model deployments, failures can directly impact live systems.\n\n**Mental Model: Bank Vault Security Breach**\nThink of model registry failures like security breaches in a bank vault. A broken lock means you can't access your assets temporarily. Corrupted records mean you can't prove ownership. A compromised vault means the integrity of all assets is questionable. Each scenario requires different containment and recovery procedures.\n\n**Version Consistency Failures** occur when model metadata becomes disconnected from actual model artifacts, or when the version history becomes corrupted. These failures threaten the fundamental guarantees that model versions are immutable and traceable.\n\n| Consistency Violation | Detection Method | Risk Level | Mitigation Strategy |\n|----------------------|------------------|------------|-------------------|\n| Model artifact missing for registered version | Checksum validation during retrieval | High - deployment failure | Maintain redundant artifact storage |\n| Multiple versions claiming same artifact hash | Content-addressable storage verification | Medium - lineage confusion | Implement atomic registration transactions |\n| Stage transition without approval workflow | Audit log verification | High - unauthorized production deployment | Enforce approval gates in API layer |\n| Orphaned model versions after experiment deletion | Foreign key constraint violations | Low - storage waste | Cascade deletion policies with grace periods |\n\n**Stage Transition Failures** disrupt the model promotion workflow, potentially blocking production deployments or allowing unauthorized model releases. These failures require immediate intervention to maintain deployment governance.\n\nThe registry must handle scenarios where approval workflows fail mid-transition, multiple users attempt simultaneous promotions, and external validation systems become unavailable during promotion checks. Recovery procedures must ensure that partially completed transitions are either completed or cleanly rolled back.\n\n**Lineage Tracking Corruption** breaks the traceability links between models and their source experiments, training data, and code versions. While not immediately fatal, lineage corruption undermines reproducibility and compliance requirements.\n\n| Lineage Break | Impact | Detection | Recovery |\n|---------------|---------|-----------|----------|\n| Missing experiment run reference | Cannot reproduce model training | Periodic lineage validation | Manual reconstruction from logs |\n| Broken training data hash links | Cannot verify data provenance | Data integrity checks | Re-compute hashes from source data |\n| Invalid code commit references | Cannot access training code | Git repository validation | Update references or mark as unrecoverable |\n| Circular dependency in lineage graph | Infinite loops in dependency traversal | Graph cycle detection | Break cycles at newest dependency |\n\n#### Training Pipeline Failures\n\nTraining pipelines face the most complex failure scenarios due to their distributed nature, resource dependencies, and long execution times. Failures can occur at the orchestration level, individual step level, or resource management level.\n\n**Mental Model: Assembly Line Disruption**\nThink of pipeline failures like disruptions in a manufacturing assembly line. A broken machine stops one station but shouldn't shut down the entire line. A power outage affects everything temporarily. A defective component early in the line creates waste downstream. Each disruption type requires different containment and recovery strategies.\n\n**Orchestration Engine Failures** occur when the pipeline scheduler becomes unavailable, loses track of running jobs, or encounters resource allocation conflicts. These failures can leave jobs running without supervision or prevent new pipelines from starting.\n\n| Orchestration Issue | Manifestation | Immediate Action | Long-term Impact |\n|-------------------|---------------|------------------|------------------|\n| Scheduler pod crash | New pipelines stuck in PENDING | Restart scheduler with state recovery | Delayed pipeline starts |\n| Lost job tracking state | Running jobs become \"orphaned\" | Reconcile actual vs. recorded job state | Resource leaks from untracked jobs |\n| Resource quota exhaustion | Steps fail with insufficient resources | Trigger resource cleanup and queuing | Cascading delays across pipelines |\n| Dead node with running steps | Steps marked RUNNING but not progressing | Detect node failure and reschedule | Partial work loss requiring restart |\n\n**Step Execution Failures** happen when individual pipeline steps crash, encounter data validation errors, or exceed resource limits. The pipeline orchestrator must decide whether to retry, skip, or abort the entire pipeline based on the failure type and configured policies.\n\nStep failures require sophisticated error classification to determine appropriate recovery actions. Transient errors (network timeouts, temporary resource unavailability) warrant automatic retry with exponential backoff. Data errors (schema validation failures, corrupt input files) require human intervention to fix upstream issues. Resource errors (out-of-memory, disk full) need resource reconfiguration before retry.\n\n**Data Dependency Violations** occur when pipeline steps receive invalid or missing input data, breaking the expected data flow between steps. These violations can cascade through the pipeline, corrupting downstream processing.\n\n| Dependency Violation | Root Cause | Detection Point | Recovery Strategy |\n|---------------------|------------|----------------|-------------------|\n| Missing input artifact | Upstream step failure or cleanup | Step startup validation | Re-run upstream dependencies |\n| Schema validation failure | Data format change or corruption | Input processing stage | Fail fast with clear error message |\n| Stale data dependencies | Clock skew or caching issues | Timestamp validation | Force refresh of cached dependencies |\n| Cross-pipeline data conflicts | Concurrent modifications to shared data | File lock conflicts | Implement data versioning and isolation |\n\n#### Model Deployment Failures\n\nModel deployment failures can directly impact production traffic and user-facing applications. These failures require immediate detection and automated recovery to minimize service disruption.\n\n**Mental Model: Restaurant Service Breakdown**\nThink of deployment failures like service breakdowns in a restaurant. A chef getting sick means one station slows down but others continue. A power outage stops all cooking until restored. A food safety issue requires immediate shutdown and cleanup. Each scenario has different urgency levels and recovery procedures.\n\n**Health Check Failures** indicate that deployed model endpoints are not responding correctly to requests, either due to model loading issues, resource constraints, or infrastructure problems.\n\n| Health Check Failure | Probable Cause | Service Impact | Auto-Recovery Action |\n|---------------------|---------------|----------------|---------------------|\n| HTTP 503 responses | Model loading timeout or memory pressure | Partial traffic loss | Scale up replicas and retry |\n| High latency responses | CPU throttling or model complexity | Degraded user experience | Implement request queuing |\n| Prediction accuracy drop | Model serving infrastructure bug | Incorrect results to users | Rollback to previous version |\n| Memory leak detection | Model or serving framework bug | Gradually degrading performance | Rolling restart of serving pods |\n\n**Traffic Routing Failures** disrupt the careful traffic management required for canary deployments and A/B testing. These failures can route traffic to wrong model versions or fail to balance load appropriately.\n\nRouting failures often manifest as traffic imbalances (all traffic to one version), routing loops (requests bounce between endpoints), or version confusion (requests served by wrong model version). The deployment system must detect these issues quickly and implement safeguards to restore proper traffic flow.\n\n**Rollback Failures** represent the worst-case scenario where both the new model version and the rollback mechanism fail simultaneously. This leaves the deployment in an inconsistent state with no clear recovery path.\n\n| Rollback Scenario | Failure Point | Remaining Options | Prevention Strategy |\n|------------------|---------------|-------------------|-------------------|\n| Previous version artifacts deleted | Artifact cleanup policy too aggressive | Deploy older known-good version | Implement version retention policies |\n| Configuration drift during rollback | Infrastructure changes since last deployment | Manual infrastructure reconciliation | Configuration drift detection |\n| Database migration incompatibility | Schema changes not backward compatible | Emergency maintenance mode | Backward compatibility testing |\n| Cascading dependency failures | Related services expect new model schema | Service mesh circuit breakers | Dependency impact analysis |\n\n#### Model Monitoring Failures\n\nMonitoring system failures create blind spots that hide model performance degradation and drift, potentially allowing serious issues to persist undetected.\n\n**Mental Model: Medical Monitoring Equipment Failure**\nThink of monitoring failures like vital sign monitors failing in an intensive care unit. A broken heart rate monitor doesn't stop the heart, but doctors can't detect dangerous changes. Multiple monitor failures create dangerous blind spots. Backup monitoring systems and manual checks become critical for patient safety.\n\n**Prediction Logging Failures** prevent the collection of inference data needed for drift detection and performance analysis. These failures can occur due to storage system issues, high request volumes, or logging pipeline bugs.\n\n| Logging Issue | Data Loss Risk | Detection Method | Mitigation Approach |\n|---------------|---------------|------------------|-------------------|\n| Log ingestion backpressure | Recent predictions dropped | Queue depth monitoring | Scale logging infrastructure |\n| Storage quota exhaustion | All new logs rejected | Storage utilization alerts | Implement data retention policies |\n| Schema evolution conflicts | Logs with new fields rejected | Schema validation errors | Deploy backward-compatible schemas |\n| Batch processing failures | Delayed availability of metrics | Processing job status monitoring | Implement streaming analytics backup |\n\n**Drift Detection Algorithm Failures** occur when statistical analysis components encounter edge cases, insufficient data, or numerical instability. These failures can produce false alerts or miss genuine drift events.\n\nDrift detection failures often result from assumptions violated by real-world data: non-normal distributions breaking statistical tests, seasonal patterns triggering false alarms, or insufficient historical data preventing baseline establishment. The monitoring system must validate its own assumptions and gracefully degrade when conditions don't meet requirements.\n\n**Alert Escalation Failures** prevent critical notifications from reaching the appropriate teams, allowing serious issues to persist without intervention. These failures can occur in notification systems, communication channels, or alert routing logic.\n\n### Detection and Recovery Strategies\n\nEffective detection and recovery requires a layered approach that combines proactive health monitoring, automated recovery procedures, and human escalation pathways. Think of this like a tiered emergency response system where automated systems handle routine issues, escalate complex problems to specialists, and always maintain situational awareness through comprehensive monitoring.\n\n#### Health Check Framework\n\nThe health check framework provides the foundation for failure detection across all components. Each component implements standardized health checks that assess both its own functionality and its dependencies.\n\n**Health Check Categories** organize monitoring into distinct areas with different urgency levels and escalation procedures. Critical health checks indicate immediate service impact requiring automatic recovery actions. Warning-level checks indicate degraded performance that may require scaling or attention. Informational checks provide operational insights without triggering alerts.\n\n| Health Check Type | Check Frequency | Failure Threshold | Auto-Recovery Action |\n|------------------|----------------|-------------------|---------------------|\n| Liveness probe | 10 seconds | 3 consecutive failures | Container restart |\n| Readiness probe | 5 seconds | 1 failure | Remove from load balancer |\n| Deep dependency check | 60 seconds | 5 failures in 5 minutes | Escalate to operations team |\n| Performance baseline | 300 seconds | 20% degradation sustained | Trigger auto-scaling |\n\n**Component-Specific Health Checks** verify the unique functionality and dependencies of each platform component. The experiment tracking component checks database connectivity and artifact storage availability. The model registry validates artifact integrity and stage transition workflows. Training pipelines monitor resource availability and job scheduling capability.\n\n```\nExperiment Tracking Health Checks:\n1. Database connection test with simple query execution\n2. Artifact storage write/read/delete cycle test\n3. Metadata consistency validation for recent experiments\n4. Query performance benchmark against baseline latency\n5. Storage quota verification with buffer thresholds\n\nModel Registry Health Checks:\n1. Model artifact checksum validation for recent versions\n2. Stage transition workflow simulation\n3. Lineage graph traversal performance test\n4. Approval workflow integration connectivity\n5. Version immutability constraint verification\n\nTraining Pipeline Health Checks:\n1. Kubernetes cluster resource availability check\n2. Container image registry accessibility test\n3. Persistent volume claim creation and mounting test\n4. Inter-node network connectivity validation\n5. GPU resource detection and allocation test\n\nModel Deployment Health Checks:\n1. Model endpoint response time and accuracy test\n2. Traffic routing configuration validation\n3. Auto-scaling trigger and response verification\n4. Load balancer health and configuration check\n5. Canary deployment traffic split accuracy\n\nModel Monitoring Health Checks:\n1. Prediction log ingestion rate and latency check\n2. Drift detection algorithm execution and accuracy\n3. Alert routing and escalation pathway test\n4. Dashboard data freshness and query performance\n5. Storage retention policy execution validation\n```\n\n#### Circuit Breaker Implementation\n\nCircuit breakers prevent cascading failures by isolating failing components and providing fallback behavior during outages. The circuit breaker pattern monitors failure rates and response times, automatically opening to prevent further damage when thresholds are exceeded.\n\n**Mental Model: Electrical Circuit Protection**\nThink of software circuit breakers like electrical circuit breakers in your home. When a device draws too much current, the breaker trips to prevent fire damage. The breaker can be manually reset once the problem is fixed. Software circuit breakers work similarly - they \"trip\" when error rates exceed thresholds, preventing cascading failures until the underlying issue is resolved.\n\n**Circuit Breaker States** define the operational behavior and transition conditions. The closed state allows normal operation while monitoring failure rates. The open state blocks requests and returns immediate failures. The half-open state allows limited testing to determine if the underlying issue has been resolved.\n\n| State | Request Behavior | Monitoring Actions | Transition Conditions |\n|-------|------------------|-------------------|----------------------|\n| Closed | Forward all requests to backend | Track success/failure rates | Failure rate > threshold → Open |\n| Open | Immediately return circuit breaker error | Monitor for timeout expiration | Timeout elapsed → Half-Open |\n| Half-Open | Forward limited test requests | Evaluate test request results | All tests succeed → Closed, Any test fails → Open |\n\n**Component Integration Points** identify where circuit breakers provide maximum protection against cascading failures. Critical integration points include database connections, external service calls, and inter-component API communications.\n\nThe experiment tracking component uses circuit breakers around database queries and artifact storage operations. When the database becomes unavailable, the circuit breaker prevents connection pool exhaustion by immediately failing requests with clear error messages. Similarly, artifact upload operations fail fast when storage systems experience outages, allowing training scripts to save artifacts locally for later upload.\n\n**Fallback Strategies** define alternative behavior when circuit breakers open. Effective fallback strategies maintain essential functionality while clearly communicating degraded service state. Read operations might serve stale cached data with appropriate warnings. Write operations might queue requests for later processing or store data locally.\n\n| Component | Circuit Breaker Location | Fallback Strategy | Degraded Service Impact |\n|-----------|-------------------------|-------------------|------------------------|\n| Experiment Tracking | Database connections | Cache recent experiment data | Read-only access to recent experiments |\n| Model Registry | Artifact storage | Return metadata only, defer downloads | Model information available, artifacts delayed |\n| Training Pipeline | Kubernetes API | Queue pipeline submissions | Delayed pipeline execution |\n| Model Deployment | Model serving endpoints | Route to previous version | Gradual traffic shift to stable version |\n| Model Monitoring | Prediction logging | Local buffering with delayed upload | Temporary gap in real-time monitoring |\n\n#### Automated Recovery Procedures\n\nAutomated recovery procedures handle common failure scenarios without human intervention, reducing mean time to recovery and operational burden. These procedures must be carefully designed to avoid making failures worse through inappropriate automated actions.\n\n**Recovery Procedure Categories** organize automated responses by failure type and required intervention complexity. Immediate recovery procedures activate within seconds to handle transient issues. Scheduled recovery procedures run periodically to address accumulated issues. Escalation procedures engage human operators when automated recovery fails.\n\n**Database Recovery Procedures** handle common database connectivity and performance issues that affect the metadata storage layer. These procedures include connection pool reset, query optimization, and failover coordination.\n\n```\nDatabase Connection Recovery Procedure:\n1. Detect connection failure through health check or operation timeout\n2. Verify network connectivity to database host using ping and port checks\n3. Attempt connection pool refresh with exponential backoff\n4. If pool refresh fails, check for connection limit exhaustion\n5. Implement circuit breaker to prevent further connection attempts\n6. Switch to read-only replica if available for degraded service\n7. Alert operations team if primary database remains unavailable\n8. Monitor recovery and gradually increase connection attempts\n```\n\n**Storage Recovery Procedures** address artifact storage failures that can prevent model versioning and experiment artifact management. Recovery includes retry logic, alternative storage backends, and cleanup procedures.\n\nStorage failures often resolve automatically through retry with exponential backoff, particularly for network-related timeouts. However, quota exhaustion requires active cleanup of old artifacts based on retention policies. The recovery system maintains multiple storage backends when possible, automatically failing over to secondary storage during outages.\n\n**Resource Allocation Recovery** handles training pipeline failures related to insufficient compute resources, node failures, and scheduling conflicts. These procedures coordinate with cluster management systems to restore service capability.\n\n| Recovery Scenario | Detection Signal | Automated Actions | Escalation Criteria |\n|------------------|------------------|-------------------|-------------------|\n| Node failure with running jobs | Kubernetes node NotReady event | Reschedule affected jobs to available nodes | Job rescheduling fails repeatedly |\n| GPU resource exhaustion | Job pending with unschedulable reason | Trigger cluster auto-scaling, queue jobs | Auto-scaling limit reached |\n| Persistent volume failures | Pod stuck in ContainerCreating | Attempt PV repair, schedule on different node | PV remains unrecoverable |\n| Container image pull failures | Pod ImagePullBackOff status | Clear image cache, retry pull from backup registry | Image not available in any registry |\n\n#### Event-Driven Coordination\n\nEvent-driven coordination enables components to respond to failures and recovery actions throughout the platform without tight coupling. Components publish events about their state changes and subscribe to events that require their attention.\n\n**Mental Model: Hospital Emergency Communication System**\nThink of event-driven coordination like a hospital's emergency communication system. When a patient codes, the alert goes to all relevant departments simultaneously. The cardiac team responds immediately, pharmacy prepares emergency medications, and the lab prioritizes stat tests. Each department knows their role and acts based on the alert type without requiring central coordination.\n\n**Event Types for Error Handling** define the categories of failure and recovery events that components must publish and handle. These events carry sufficient context for subscribers to determine appropriate responses.\n\n| Event Type | Publishing Component | Event Payload | Typical Subscribers |\n|------------|---------------------|---------------|-------------------|\n| `COMPONENT_HEALTH_DEGRADED` | Any component health check | Component name, health status, error details | Monitoring dashboard, alert manager |\n| `STORAGE_QUOTA_WARNING` | Experiment tracking, model registry | Storage backend, usage percentage, projection | Cleanup services, capacity planning |\n| `DEPLOYMENT_FAILED` | Model deployment | Model name, version, error details, rollback needed | Model registry, monitoring, alerting |\n| `PIPELINE_STEP_RETRY_EXHAUSTED` | Training pipeline orchestrator | Pipeline ID, step name, error summary | Pipeline monitoring, error analysis |\n| `DRIFT_ALERT_CRITICAL` | Model monitoring | Model name, drift metric, severity level | Model registry, deployment service |\n\n**Event Ordering and Consistency** ensures that components process related events in the correct sequence and maintain consistent state despite asynchronous delivery. Critical events use causal ordering to prevent race conditions between related state changes.\n\nEvent processing implements idempotent handlers that produce the same result regardless of how many times they execute. This prevents duplicate processing when events are redelivered due to network issues or processing failures. Each event includes a correlation ID that links related events and enables end-to-end tracing of failure and recovery workflows.\n\n**Recovery Workflow Coordination** orchestrates complex recovery procedures that require coordination between multiple components. For example, rolling back a failed model deployment involves the deployment service, model registry, monitoring system, and traffic routing components.\n\n```\nModel Deployment Rollback Coordination:\n1. Deployment service publishes DEPLOYMENT_FAILED event with rollback request\n2. Model registry subscribes to event and prepares previous version metadata\n3. Traffic routing service receives event and prepares traffic shifting plan\n4. Monitoring service pauses drift detection during rollback window\n5. Deployment service coordinates rollback execution across subscribers\n6. Each component publishes completion events for overall workflow tracking\n7. Final DEPLOYMENT_ROLLBACK_COMPLETE event signals successful recovery\n```\n\n### Data Consistency Guarantees\n\nMaintaining data consistency across distributed MLOps components requires careful transaction design, conflict resolution strategies, and consistency level management. Unlike traditional applications with single-database transactions, MLOps platforms must coordinate state across metadata stores, artifact storage, container registries, and external services.\n\n**Mental Model: Bank Transaction Processing**\nThink of MLOps data consistency like bank transaction processing. When you transfer money between accounts, the system must ensure both accounts are updated or neither is changed - you can't have money disappear or appear from nowhere. Similarly, when registering a model version, the metadata and artifacts must remain synchronized, even if storage systems experience failures during the process.\n\n#### Transaction Boundaries and ACID Properties\n\nTransaction boundaries define the scope of operations that must complete atomically to maintain platform consistency. Each component establishes transaction boundaries around operations that modify multiple related pieces of state.\n\n**Experiment Tracking Transactions** encompass parameter logging, metric recording, and artifact upload operations that belong to a single experiment run. These transactions ensure that experiment state remains consistent even during concurrent updates from distributed training jobs.\n\n| Transaction Scope | ACID Property Implementation | Consistency Invariant | Failure Handling |\n|------------------|----------------------------|----------------------|------------------|\n| Single run parameter batch | Atomicity through database transaction | All parameters logged or none | Rollback on any parameter validation failure |\n| Metric time series update | Consistency through monotonic timestamps | Metrics never decrease in step number | Reject out-of-order metric updates |\n| Artifact upload with metadata | Isolation through file staging | Metadata references only uploaded artifacts | Clean up staged files on metadata failure |\n| Run completion marking | Durability through WAL flushing | Run status reflects actual completion | Mark failed if artifacts missing |\n\n**Model Registry Transactions** coordinate model version registration with artifact storage and lineage tracking. These transactions implement the immutability guarantees that production deployments depend on.\n\nModel registration transactions use a two-phase approach: first validate and stage all artifacts, then atomically update registry metadata. If artifact validation fails during staging, the transaction aborts without creating registry entries. If metadata updates fail after successful staging, the system retries the metadata operation using staged artifacts.\n\n**Cross-Component Transactions** handle operations that span multiple platform components, such as promoting a model from experiment tracking through registry to deployment. These transactions use event sourcing and compensation patterns since traditional ACID transactions cannot span independent services.\n\n| Cross-Component Operation | Transaction Pattern | Consistency Mechanism | Compensation Strategy |\n|--------------------------|-------------------|----------------------|----------------------|\n| Model promotion from experiment to registry | Saga pattern with events | Event log ordering guarantees | Reverse compensation events |\n| Pipeline completion with model registration | Two-phase commit across services | Coordinator service with participant votes | Automated retry with timeout |\n| Deployment rollback with monitoring pause | Event-driven coordination | Causal event ordering | Forward compensation to final state |\n\n#### Conflict Resolution Strategies\n\nConflict resolution handles situations where concurrent operations attempt to modify the same resources in incompatible ways. MLOps platforms face unique conflicts around model versioning, experiment naming, and resource allocation.\n\n**Model Version Conflicts** occur when multiple processes attempt to register the same model version simultaneously, or when stage transitions conflict with ongoing operations. The registry implements optimistic concurrency control with version vectors to detect and resolve these conflicts.\n\nVersion conflicts use a deterministic resolution strategy based on timestamps and content hashes. When two processes register the same model version with different artifacts, the system compares creation timestamps and artifact checksums. If the artifacts are identical (same checksum), the later registration succeeds but references the existing artifact. If artifacts differ, the registration fails with a clear error message requiring manual resolution.\n\n**Experiment Naming Conflicts** arise when researchers create experiments with duplicate names or when automated systems generate conflicting experiment identifiers. The experiment tracking system resolves these conflicts through hierarchical namespacing and automatic disambiguation.\n\n| Conflict Type | Detection Method | Resolution Strategy | User Experience |\n|---------------|------------------|-------------------|-----------------|\n| Duplicate experiment name | Unique constraint violation | Append timestamp suffix | Experiment created as \"model-tuning-2023-10-15-143022\" |\n| Concurrent run creation | Run ID collision | Regenerate ID with retry | Transparent to user, automatic retry |\n| Parameter key conflicts within run | Duplicate key insertion | Last write wins with warning | Parameter overwritten, warning logged |\n| Artifact path conflicts | Path already exists check | Generate unique suffix | Artifact stored with disambiguation suffix |\n\n**Resource Allocation Conflicts** happen when multiple training pipelines compete for limited cluster resources, or when deployment scaling conflicts with resource quotas. The platform implements fair scheduling and resource reservation to minimize conflicts.\n\nResource conflicts use a combination of preemption and queuing strategies. High-priority jobs can preempt lower-priority jobs with sufficient notice for checkpoint saving. Jobs that cannot be scheduled immediately enter a priority queue with estimated wait times. The scheduler periodically rebalances allocations to ensure fair resource distribution across users and teams.\n\n#### Eventual Consistency and Convergence\n\nSome MLOps operations can tolerate eventual consistency in exchange for higher availability and performance. The platform implements eventual consistency for operations where immediate consistency is not critical for correctness.\n\n**Mental Model: News Distribution Network**\nThink of eventual consistency like news distribution in a global network. A breaking news story published in New York doesn't instantly appear in Tokyo newspapers, but the information eventually propagates everywhere. Readers might see slightly different versions temporarily, but the final story converges to the same content once distribution completes.\n\n**Metrics Aggregation Consistency** allows experiment metrics to propagate through caching layers and materialized views with eventual convergence. Real-time dashboards might show slightly stale data during high write loads, but views eventually converge to consistent state.\n\nMetrics aggregation implements conflict-free replicated data types (CRDTs) for operations like counting experiment runs and computing performance statistics. These data types guarantee convergence without requiring coordination, enabling high write throughput during intensive training periods.\n\n**Artifact Replication Consistency** manages the propagation of model artifacts across multiple storage regions and caching layers. Downloads might occasionally receive stale versions during propagation, but checksums ensure detection of inconsistencies.\n\n| Consistency Level | Use Case | Convergence Time | Detection Method |\n|------------------|----------|------------------|------------------|\n| Strong consistency | Model version registration | Immediate | Synchronous validation |\n| Sequential consistency | Experiment run ordering | < 1 second | Vector clocks |\n| Eventual consistency | Dashboard metrics | < 30 seconds | Background reconciliation |\n| Weak consistency | Usage statistics | < 5 minutes | Periodic aggregation |\n\n**Monitoring Data Consistency** handles the high-volume prediction logging and drift detection data that can tolerate some inconsistency for performance. The monitoring system implements lambda architecture with real-time and batch processing layers that eventually converge.\n\nReal-time monitoring provides approximate metrics with low latency for immediate alerting. Batch processing computes authoritative metrics periodically and corrects any inconsistencies detected in the real-time layer. This approach enables responsive alerting while maintaining data accuracy for compliance and auditing requirements.\n\n![Failure Recovery Flows](./diagrams/error-handling-flows.svg)\n\n### Common Recovery Scenarios\n\nUnderstanding how the platform handles common failure scenarios helps operators troubleshoot issues and validates the robustness of recovery procedures. Each scenario includes the failure sequence, detection methods, automated recovery actions, and manual intervention requirements.\n\n#### Training Pipeline Catastrophic Failure\n\nA catastrophic training pipeline failure occurs when the orchestration engine crashes during active job execution, potentially leaving running containers without supervision and consuming resources without progress tracking.\n\n```\nFailure Sequence:\n1. Training pipeline orchestrator pod crashes due to memory pressure\n2. Kubernetes reschedules orchestrator to new node with state loss\n3. Previously running training jobs continue executing but become \"orphaned\"\n4. New job submissions fail due to missing orchestrator state\n5. Resource quotas fill up with untracked jobs preventing new work\n6. Monitoring alerts fire due to job submission failures\n```\n\n**Detection and Recovery Process:**\nThe platform detects this failure through health check timeouts and job submission error rates. Automated recovery includes state reconciliation, orphaned job cleanup, and orchestrator restart with recovered state.\n\n```\nRecovery Procedure:\n1. Detect orchestrator failure through health check timeout\n2. Query Kubernetes API for all running jobs matching orchestrator labels\n3. Cross-reference running jobs against expected pipeline executions\n4. For orphaned jobs: attempt graceful termination with artifact preservation\n5. Rebuild orchestrator state from persisted pipeline definitions and job history\n6. Resume monitoring of recovered jobs and accept new job submissions\n7. Send notification summarizing recovery actions and any data loss\n```\n\n**Manual Intervention Requirements:**\nOperators must validate that recovered state correctly reflects actual cluster state and manually resolve any pipelines that cannot be automatically recovered. Long-running training jobs may need manual checkpoint restoration if automatic state recovery fails.\n\n#### Model Deployment Rollback Cascade\n\nA deployment rollback cascade occurs when rolling back a failed model deployment triggers failures in the previous version, creating a situation where no stable model version is available for production traffic.\n\n```\nFailure Sequence:\n1. New model version deployed successfully but produces incorrect predictions\n2. Automated monitoring detects accuracy degradation and triggers rollback\n3. Rollback to previous version fails due to artifact corruption\n4. Traffic routing attempts to find stable version but all recent versions problematic\n5. Model serving endpoints become unavailable causing customer impact\n6. Manual intervention required to deploy known-good version from older backup\n```\n\nThis scenario requires sophisticated rollback strategies that maintain multiple stable versions and validate rollback targets before traffic switching. The deployment system must implement health validation for rollback targets and maintain emergency deployment procedures for crisis scenarios.\n\n#### Data Corruption During Experiment Migration\n\nData corruption during experiment migration represents a complex scenario where database schema changes or data migration scripts corrupt historical experiment data, affecting research reproducibility.\n\n**Corruption Detection:**\nThe system detects corruption through periodic consistency checks that validate foreign key relationships, timestamp ordering, and artifact checksums. Corruption manifests as missing experiment runs, unreachable artifacts, or inconsistent metric time series.\n\n**Recovery Strategy:**\nRecovery requires restoring from validated backups while preserving recent uncorrupted data. The process involves identifying the corruption scope, isolating affected data, and merging clean historical data with recent additions.\n\n```\nMigration Recovery Process:\n1. Identify corruption scope through consistency validation queries\n2. Stop all write operations to prevent further corruption spread\n3. Restore database from last known-good backup to isolated environment\n4. Extract uncorrupted recent data from production database\n5. Merge clean historical data with validated recent data\n6. Perform full consistency validation on merged dataset\n7. Replace production database with merged data after validation\n8. Resume operations with enhanced validation during recovery period\n```\n\n### Implementation Guidance\n\nBuilding robust error handling requires implementing the health check framework, circuit breaker patterns, and recovery procedures described above. The following guidance provides concrete implementation strategies for each component.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Health Checks | HTTP endpoints with JSON responses | Kubernetes liveness/readiness probes |\n| Circuit Breakers | Simple threshold-based implementation | Netflix Hystrix or similar library |\n| Event Coordination | Redis pub/sub with message queues | Apache Kafka with event sourcing |\n| Monitoring | Prometheus metrics with Grafana dashboards | Full observability stack with distributed tracing |\n| Recovery Automation | Shell scripts with cron scheduling | Kubernetes operators with custom resource definitions |\n\n#### File Structure\n\n```\nplatform/\n├── internal/\n│   ├── health/\n│   │   ├── checker.go              # ComponentHealth implementation\n│   │   ├── checks.go               # Standard health check functions\n│   │   └── registry.go             # Health check registration\n│   ├── circuit/\n│   │   ├── breaker.go              # CircuitBreaker implementation\n│   │   ├── config.go               # Configuration structures\n│   │   └── metrics.go              # Circuit breaker metrics\n│   ├── events/\n│   │   ├── coordinator.go          # EventCoordinator implementation\n│   │   ├── handlers.go             # Event handler utilities\n│   │   └── storage.go              # Event persistence layer\n│   ├── recovery/\n│   │   ├── procedures.go           # Automated recovery procedures\n│   │   ├── detection.go            # Failure detection algorithms\n│   │   └── coordination.go         # Multi-component recovery coordination\n│   └── consistency/\n│       ├── transactions.go         # Cross-component transaction patterns\n│       ├── conflicts.go            # Conflict resolution strategies\n│       └── convergence.go          # Eventual consistency mechanisms\n└── pkg/\n    ├── errors/\n    │   ├── types.go                # Error type definitions\n    │   ├── classification.go       # Error classification utilities\n    │   └── context.go              # Error context enrichment\n    └── monitoring/\n        ├── alerts.go               # Alert management\n        └── dashboards.go           # Health dashboard utilities\n```\n\n#### Health Check Infrastructure\n\n```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict, Any, List, Callable, Optional\nimport time\nimport asyncio\n\nclass HealthStatus(Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass HealthCheck:\n    name: str\n    status: HealthStatus\n    message: str\n    timestamp: float\n    details: Dict[str, Any]\n\nclass ComponentHealth:\n    \"\"\"Manages health checks for a platform component.\"\"\"\n    \n    def __init__(self, component_name: str):\n        self.component_name = component_name\n        self.checks: Dict[str, Callable[[], HealthCheck]] = {}\n        self.check_intervals: Dict[str, float] = {}\n        self.last_results: Dict[str, HealthCheck] = {}\n    \n    def add_check(self, check_name: str, check_func: Callable[[], HealthCheck], \n                  interval_seconds: float = 60.0):\n        \"\"\"Register a periodic health check function.\"\"\"\n        # TODO 1: Store check function in self.checks dictionary\n        # TODO 2: Store check interval in self.check_intervals\n        # TODO 3: Initialize last_results entry with UNKNOWN status\n        pass\n    \n    def run_checks(self) -> List[HealthCheck]:\n        \"\"\"Execute all health checks and return results.\"\"\"\n        results = []\n        # TODO 1: Iterate through all registered checks\n        # TODO 2: For each check, call the check function safely with exception handling\n        # TODO 3: Update last_results with new results\n        # TODO 4: Append result to results list\n        # TODO 5: Return complete results list\n        return results\n    \n    def get_overall_status(self) -> HealthStatus:\n        \"\"\"Compute overall component health from individual checks.\"\"\"\n        # TODO 1: If no checks registered, return UNKNOWN\n        # TODO 2: If any check is UNHEALTHY, return UNHEALTHY\n        # TODO 3: If any check is DEGRADED, return DEGRADED\n        # TODO 4: If all checks are HEALTHY, return HEALTHY\n        # TODO 5: Otherwise return UNKNOWN\n        pass\n\n# Example health check implementations\ndef database_connectivity_check() -> HealthCheck:\n    \"\"\"Check database connectivity and response time.\"\"\"\n    # TODO 1: Attempt simple database query with timeout\n    # TODO 2: Measure query execution time\n    # TODO 3: Return HEALTHY if query succeeds within threshold\n    # TODO 4: Return DEGRADED if query slow but successful\n    # TODO 5: Return UNHEALTHY if query fails or times out\n    pass\n\ndef artifact_storage_check() -> HealthCheck:\n    \"\"\"Check artifact storage availability through write/read cycle.\"\"\"\n    # TODO 1: Generate test artifact with unique key\n    # TODO 2: Attempt to upload test artifact\n    # TODO 3: Attempt to download and verify test artifact\n    # TODO 4: Clean up test artifact\n    # TODO 5: Return appropriate status based on operation success\n    pass\n```\n\n#### Circuit Breaker Implementation\n\n```python\nimport time\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Optional, Any, Callable\nimport threading\n\nclass CircuitBreakerState(Enum):\n    CLOSED = \"closed\"\n    OPEN = \"open\"\n    HALF_OPEN = \"half_open\"\n\n@dataclass\nclass CircuitBreakerConfig:\n    failure_threshold: int = 5\n    timeout_seconds: float = 60.0\n    success_threshold: int = 3\n    call_timeout: float = 30.0\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker preventing cascade failures.\"\"\"\n    \n    def __init__(self, config: CircuitBreakerConfig):\n        self.config = config\n        self.state = CircuitBreakerState.CLOSED\n        self.failure_count = 0\n        self.success_count = 0\n        self.last_failure_time = 0.0\n        self.lock = threading.Lock()\n    \n    def can_execute(self) -> bool:\n        \"\"\"Check if request should be allowed through circuit breaker.\"\"\"\n        with self.lock:\n            # TODO 1: If state is CLOSED, return True\n            # TODO 2: If state is OPEN, check if timeout period has elapsed\n            # TODO 3: If timeout elapsed, transition to HALF_OPEN and return True\n            # TODO 4: If state is HALF_OPEN, return True (allow test requests)\n            # TODO 5: Otherwise return False\n            pass\n    \n    def record_success(self):\n        \"\"\"Record successful operation result.\"\"\"\n        with self.lock:\n            # TODO 1: Reset failure_count to 0\n            # TODO 2: If state is HALF_OPEN, increment success_count\n            # TODO 3: If success_count >= success_threshold, transition to CLOSED\n            # TODO 4: If state is CLOSED, ensure it remains CLOSED\n            pass\n    \n    def record_failure(self):\n        \"\"\"Record failed operation result.\"\"\"\n        with self.lock:\n            # TODO 1: Increment failure_count\n            # TODO 2: Record current timestamp as last_failure_time\n            # TODO 3: If failure_count >= failure_threshold, transition to OPEN\n            # TODO 4: If state is HALF_OPEN, transition back to OPEN and reset success_count\n            pass\n\ndef circuit_breaker_wrapper(breaker: CircuitBreaker, func: Callable, *args, **kwargs):\n    \"\"\"Wrapper function that applies circuit breaker to function calls.\"\"\"\n    # TODO 1: Check if breaker.can_execute() returns True\n    # TODO 2: If not, raise CircuitBreakerOpenError immediately\n    # TODO 3: Try executing func(*args, **kwargs) with timeout\n    # TODO 4: If successful, call breaker.record_success() and return result\n    # TODO 5: If failed, call breaker.record_failure() and re-raise exception\n    pass\n```\n\n#### Event-Driven Coordination System\n\n```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any, Optional, Callable, List\nimport uuid\nimport time\nimport json\nimport asyncio\nfrom enum import Enum\n\n@dataclass\nclass Event:\n    id: str\n    type: str\n    source: str\n    timestamp: float\n    payload: Dict[str, Any]\n    correlation_id: Optional[str] = None\n    version: str = \"1.0\"\n    metadata: Optional[Dict[str, str]] = None\n    \n    @classmethod\n    def create(cls, event_type: str, source: str, payload: Dict[str, Any]) -> 'Event':\n        \"\"\"Create new event with auto-generated ID and timestamp.\"\"\"\n        # TODO 1: Generate unique event ID using uuid.uuid4()\n        # TODO 2: Set timestamp to current time using time.time()\n        # TODO 3: Create and return Event instance with provided parameters\n        pass\n\nclass EventHandler:\n    \"\"\"Wrapper for event handler functions with retry logic.\"\"\"\n    \n    def __init__(self, handler_func: Callable[[Event], None], \n                 max_retries: int = 3, retry_delay: float = 1.0):\n        self.handler_func = handler_func\n        self.max_retries = max_retries\n        self.retry_delay = retry_delay\n    \n    async def handle_event(self, event: Event) -> bool:\n        \"\"\"Execute event handler with retry logic.\"\"\"\n        # TODO 1: Attempt to call handler_func(event) with try/except\n        # TODO 2: If successful, return True\n        # TODO 3: If exception occurs, implement exponential backoff retry\n        # TODO 4: Log retry attempts and final success/failure\n        # TODO 5: Return False if all retries exhausted\n        pass\n\nclass EventCoordinator:\n    \"\"\"Central event coordination system.\"\"\"\n    \n    def __init__(self):\n        self.subscribers: Dict[str, List[EventHandler]] = {}\n        self.event_storage: Optional[EventStorage] = None\n    \n    def subscribe(self, event_type: str, handler: Callable[[Event], None]) -> str:\n        \"\"\"Register event handler for specific event type.\"\"\"\n        # TODO 1: Create EventHandler wrapper around handler function\n        # TODO 2: Add handler to subscribers[event_type] list\n        # TODO 3: Generate and return subscription ID for later unsubscription\n        pass\n    \n    async def publish(self, event: Event, synchronous: bool = False) -> bool:\n        \"\"\"Publish event to subscribers.\"\"\"\n        # TODO 1: Store event in event_storage if configured\n        # TODO 2: Get list of subscribers for event.type\n        # TODO 3: If synchronous=True, await all handler executions\n        # TODO 4: If synchronous=False, schedule handlers as background tasks\n        # TODO 5: Return True if all handlers succeeded (synchronous) or scheduled (async)\n        pass\n\n# Example event handlers for MLOps platform\nasync def handle_deployment_failed(event: Event):\n    \"\"\"Handle deployment failure by triggering rollback.\"\"\"\n    # TODO 1: Extract model name and version from event payload\n    # TODO 2: Look up previous stable version from model registry\n    # TODO 3: Initiate rollback deployment to previous version\n    # TODO 4: Update deployment status and send notifications\n    pass\n\nasync def handle_drift_alert(event: Event):\n    \"\"\"Handle drift detection alert by triggering retraining.\"\"\"\n    # TODO 1: Extract drift metrics and model information from event\n    # TODO 2: Evaluate drift severity against configured thresholds\n    # TODO 3: If severe drift, trigger retraining pipeline\n    # TODO 4: Update model status and alert appropriate teams\n    pass\n```\n\n#### Recovery Procedure Framework\n\n```python\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, List, Optional\nimport asyncio\nimport logging\n\nclass RecoveryResult(Enum):\n    SUCCESS = \"success\"\n    PARTIAL = \"partial\"\n    FAILED = \"failed\"\n    MANUAL_REQUIRED = \"manual_required\"\n\n@dataclass\nclass RecoveryProcedure:\n    name: str\n    description: str\n    detection_criteria: Dict[str, Any]\n    max_attempts: int = 3\n    timeout_seconds: float = 300.0\n\nclass AutomatedRecovery:\n    \"\"\"Framework for automated failure recovery procedures.\"\"\"\n    \n    def __init__(self):\n        self.procedures: Dict[str, Callable] = {}\n        self.recovery_history: List[Dict[str, Any]] = []\n        self.logger = logging.getLogger(__name__)\n    \n    def register_procedure(self, failure_type: str, procedure: Callable):\n        \"\"\"Register recovery procedure for specific failure type.\"\"\"\n        # TODO 1: Store procedure function in procedures dictionary\n        # TODO 2: Validate procedure signature matches expected interface\n        # TODO 3: Log registration of new recovery procedure\n        pass\n    \n    async def attempt_recovery(self, failure_type: str, context: Dict[str, Any]) -> RecoveryResult:\n        \"\"\"Attempt automated recovery for detected failure.\"\"\"\n        # TODO 1: Look up registered procedure for failure_type\n        # TODO 2: If no procedure found, return MANUAL_REQUIRED\n        # TODO 3: Execute procedure with retry logic and timeout\n        # TODO 4: Record recovery attempt in recovery_history\n        # TODO 5: Return appropriate RecoveryResult based on procedure outcome\n        pass\n\n# Example recovery procedures\nasync def recover_database_connection(context: Dict[str, Any]) -> RecoveryResult:\n    \"\"\"Recover from database connectivity issues.\"\"\"\n    # TODO 1: Test current database connectivity\n    # TODO 2: If connection works, return SUCCESS\n    # TODO 3: Attempt connection pool reset\n    # TODO 4: Try connecting to read replica if available\n    # TODO 5: Return appropriate result based on recovery success\n    pass\n\nasync def recover_storage_quota_exhaustion(context: Dict[str, Any]) -> RecoveryResult:\n    \"\"\"Recover from storage quota exhaustion.\"\"\"\n    # TODO 1: Calculate current storage usage\n    # TODO 2: Identify oldest artifacts eligible for cleanup\n    # TODO 3: Execute retention policy cleanup\n    # TODO 4: Verify sufficient space available after cleanup\n    # TODO 5: Return SUCCESS if space recovered, MANUAL_REQUIRED if not\n    pass\n\nasync def recover_orphaned_training_jobs(context: Dict[str, Any]) -> RecoveryResult:\n    \"\"\"Recover from orchestrator failure with orphaned jobs.\"\"\"\n    # TODO 1: Query Kubernetes for all jobs with orchestrator labels\n    # TODO 2: Cross-reference with expected pipeline executions\n    # TODO 3: Identify truly orphaned jobs\n    # TODO 4: Attempt graceful termination with checkpoint preservation\n    # TODO 5: Rebuild orchestrator state from remaining valid jobs\n    pass\n```\n\n#### Milestone Checkpoints\n\nAfter implementing the error handling framework, verify the following behavior:\n\n**Health Check Validation:**\n- Run `python -m platform.health.checker` to execute all component health checks\n- Verify that healthy components return status \"healthy\" with appropriate details\n- Simulate database disconnection and confirm health checks detect degraded state\n- Check that health check results are properly cached and timestamped\n\n**Circuit Breaker Testing:**\n- Implement a test service that fails after N requests\n- Verify circuit breaker opens after threshold failures reached\n- Confirm circuit breaker prevents further requests when open\n- Test that circuit breaker transitions to half-open after timeout\n- Validate successful requests close circuit breaker from half-open state\n\n**Event Coordination Verification:**\n- Publish test events and verify subscribers receive them\n- Test both synchronous and asynchronous event delivery\n- Simulate handler failures and confirm retry logic works\n- Verify event ordering is preserved for related events\n\n**Recovery Procedure Testing:**\n- Trigger controlled failures (disconnect database, fill storage)\n- Verify automated recovery procedures detect and respond appropriately\n- Test that recovery procedures respect timeout and retry limits\n- Confirm manual escalation occurs when automated recovery fails\n\n#### Debugging Guide\n\n| Symptom | Likely Cause | Diagnostic Steps | Fix |\n|---------|--------------|------------------|-----|\n| Health checks always return UNKNOWN | Health check functions not registered | Check ComponentHealth.add_check() calls | Register health checks in component initialization |\n| Circuit breaker stuck in OPEN state | Underlying service not recovered or timeout too short | Check service health and circuit breaker timeout configuration | Fix underlying service or increase timeout |\n| Events not reaching subscribers | Event type mismatch or subscriber registration failure | Check event.type matches subscription and verify subscriber list | Ensure exact string match and re-register subscribers |\n| Recovery procedures not triggering | Detection criteria not matching actual failures | Review failure detection logic and criteria | Update detection criteria or add missing failure patterns |\n| Inconsistent data after recovery | Recovery procedures not atomic or concurrent modifications | Check transaction boundaries and locking | Implement proper transaction isolation |\n\n\n## Testing Strategy\n\n> **Milestone(s):** This section applies to all milestones (1-5) by providing comprehensive testing approaches that validate correct implementation and integration across experiment tracking, model registry, training pipeline orchestration, model deployment, and model monitoring components.\n\nTesting an MLOps platform requires a multi-layered approach that validates both individual component correctness and end-to-end workflow integration. Think of testing like a **quality assurance factory** - you need inspection checkpoints at every stage of the assembly line (unit tests), integration testing where components connect (API tests), and final quality validation of the complete product (end-to-end scenarios). Each milestone introduces new complexity layers that must be thoroughly validated before proceeding to the next phase.\n\nThe testing strategy addresses three critical dimensions: **functional correctness** (does each component do what it's supposed to do), **integration reliability** (do components work together correctly), and **operational resilience** (does the system handle failures gracefully). Unlike traditional web applications, MLOps platforms must also validate data science workflows, model artifacts, distributed training coordination, and production monitoring - each with unique testing challenges.\n\n### Component Testing\n\nComponent testing forms the foundation of our testing pyramid, validating individual component logic and their interactions through well-defined interfaces. Think of this like **testing individual instruments in an orchestra** before bringing them together for a full symphony performance. Each component must prove it can handle its responsibilities correctly in isolation before we test how they harmonize together.\n\n#### Unit Testing Strategy\n\nUnit tests focus on the core business logic within each component, testing pure functions and isolated behaviors without external dependencies. These tests should run quickly (under 100ms each) and provide immediate feedback during development.\n\n**Experiment Tracking Unit Tests:**\n\n| Test Category | Test Cases | Key Assertions |\n|---------------|------------|----------------|\n| Parameter Logging | Valid parameter types, nested parameters, parameter overwriting | Parameter correctly stored with run correlation, type validation works, overwrites generate warnings |\n| Metric Validation | Numeric metrics, step ordering, timestamp handling | Metrics stored with correct precision, steps are monotonic, timestamps are realistic |\n| Artifact Management | File upload, checksum validation, metadata attachment | Checksums match uploaded content, metadata correctly associated, duplicate detection works |\n| Query Engine | Filter parsing, sorting logic, pagination | SQL-like filters compile correctly, sorting handles null values, pagination maintains consistency |\n| Run Comparison | Statistical comparison, missing metrics handling | Statistical tests use correct formulas, missing values handled gracefully, confidence intervals computed |\n\n**Model Registry Unit Tests:**\n\n| Test Category | Test Cases | Key Assertions |\n|---------------|------------|----------------|\n| Version Management | Semantic versioning, version conflicts, automatic incrementing | Versions follow semantic rules, conflicts detected and resolved, auto-increment preserves ordering |\n| Stage Transitions | Valid transitions, approval gates, rollback scenarios | Only valid transitions allowed, approval metadata captured, rollbacks restore previous state |\n| Lineage Tracking | Parent-child relationships, circular dependency detection, depth limits | Lineage graphs are acyclic, circular references rejected, depth limits prevent infinite recursion |\n| Artifact Storage | Content addressing, deduplication, integrity verification | Identical artifacts share storage, checksums prevent corruption, retrieval validates integrity |\n| Metadata Validation | Schema enforcement, tag normalization, search indexing | Invalid metadata rejected, tags normalized consistently, search indexes updated correctly |\n\n**Pipeline Orchestration Unit Tests:**\n\n| Test Category | Test Cases | Key Assertions |\n|---------------|------------|----------------|\n| DAG Validation | Cycle detection, unreachable nodes, dependency resolution | Cycles detected and rejected, all nodes reachable, dependencies form valid execution order |\n| Resource Allocation | CPU/memory/GPU requests, constraint satisfaction, resource conflicts | Resource requests validated, constraints satisfied, conflicts detected before execution |\n| Step Execution | Input preparation, output processing, error handling | Inputs correctly mapped, outputs properly captured, errors propagated with context |\n| Retry Logic | Exponential backoff, maximum attempts, transient vs permanent failures | Backoff calculations correct, attempt limits enforced, failure types classified correctly |\n| Data Flow | Artifact passing, schema validation, data lineage | Artifacts flow between steps correctly, schemas validated at boundaries, lineage tracked accurately |\n\n**Model Deployment Unit Tests:**\n\n| Test Category | Test Cases | Key Assertions |\n|---------------|------------|----------------|\n| Traffic Management | Weight calculation, routing logic, canary progression | Traffic weights sum to 100%, routing respects weights, canary progression follows schedule |\n| Health Monitoring | Endpoint validation, failure detection, recovery triggers | Health checks validate correctly, failures trigger appropriate responses, recovery procedures execute |\n| Scaling Logic | Replica calculation, resource limits, scaling policies | Replica counts calculated correctly, resource limits respected, scaling policies enforced |\n| Rollback Mechanisms | Trigger conditions, state preservation, recovery validation | Rollbacks trigger on correct conditions, previous state restored, recovery validated before completion |\n| Model Loading | Artifact download, model initialization, warming procedures | Artifacts downloaded correctly, models initialize successfully, warming procedures complete |\n\n**Model Monitoring Unit Tests:**\n\n| Test Category | Test Cases | Key Assertions |\n|---------------|------------|----------------|\n| Drift Detection | Statistical test calculations, threshold evaluation, severity classification | Statistical tests use correct formulas, thresholds evaluated properly, severity levels assigned correctly |\n| Prediction Logging | Request capturing, response correlation, metadata extraction | Requests captured completely, responses correlated correctly, metadata extracted accurately |\n| Metrics Aggregation | Time window calculations, percentile computation, trend analysis | Time windows aligned correctly, percentiles computed accurately, trends calculated properly |\n| Alert Generation | Threshold evaluation, escalation logic, notification formatting | Thresholds evaluated correctly, escalation follows policy, notifications formatted properly |\n| Performance Tracking | Latency measurement, throughput calculation, resource utilization | Latency measured accurately, throughput calculated correctly, resource utilization tracked |\n\n#### Integration Testing Approach\n\nIntegration tests validate how components communicate through their APIs and event interfaces. These tests use real network communication but mock external dependencies like databases and cloud services.\n\n**API Integration Tests:**\n\n| Component Pair | Test Scenarios | Success Criteria |\n|----------------|----------------|------------------|\n| Experiment Tracking ↔ Model Registry | Run completion triggers model registration, artifact linking, metadata transfer | Model created with correct run reference, artifacts accessible from registry, metadata synchronized |\n| Model Registry ↔ Training Pipeline | Model download for fine-tuning, version updates from training, lineage tracking | Models downloaded successfully, versions updated with training results, lineage preserved through fine-tuning |\n| Training Pipeline ↔ Model Deployment | Model artifact handoff, deployment triggering, resource coordination | Artifacts transferred correctly, deployments triggered automatically, resources allocated without conflicts |\n| Model Deployment ↔ Model Monitoring | Prediction logging setup, health status reporting, performance feedback | Logging configured correctly, health status synchronized, performance metrics flow back to deployment |\n\n**Event-Driven Integration Tests:**\n\n| Event Flow | Trigger | Expected Propagation | Validation Points |\n|------------|---------|---------------------|-------------------|\n| `EXPERIMENT_COMPLETED` | Experiment finishes with acceptable metrics | Model registration triggered, pipeline deployment initiated | Model appears in registry, deployment starts automatically, monitoring configured |\n| `MODEL_PROMOTED` | Model version promoted to production stage | Deployment updated, monitoring activated, alerts configured | Production deployment reflects new version, monitoring active, alert rules applied |\n| `DEPLOYMENT_FAILED` | Model deployment encounters errors | Rollback initiated, alerts fired, incident recorded | Previous version restored, alerts sent to correct channels, incident logged with context |\n| `DRIFT_ALERT_CRITICAL` | Critical drift detected in production model | Deployment scaling paused, escalation triggered, investigation initiated | Scaling policies updated, escalation follows defined workflow, investigation tools activated |\n\n**Data Consistency Integration Tests:**\n\nThese tests verify that data remains consistent across component boundaries, especially during concurrent operations and failure scenarios.\n\n| Consistency Scenario | Test Setup | Validation Method |\n|---------------------|------------|-------------------|\n| Concurrent Model Registration | Multiple training runs register models simultaneously | All models registered correctly, no version conflicts, lineage preserved |\n| Pipeline Interruption Recovery | Training pipeline interrupted mid-execution | Partial results preserved, restart from checkpoint, no data corruption |\n| Deployment Traffic Split Consistency | Traffic weights updated during active requests | Request routing remains consistent, no dropped requests, weights eventually consistent |\n| Monitoring Data Pipeline Integrity | High-volume prediction logging with processing lag | All predictions processed exactly once, metrics aggregated correctly, no data loss |\n\n> **Design Insight**: Integration tests should focus on the **contract boundaries** between components rather than internal implementation details. Test what each component promises to deliver to its collaborators, not how it delivers it internally.\n\n#### Database and Storage Testing\n\nData persistence testing validates that each component correctly stores and retrieves data through its storage abstractions, ensuring data integrity across application restarts.\n\n**Metadata Storage Tests:**\n\n| Storage Operation | Test Cases | Integrity Checks |\n|------------------|------------|------------------|\n| Experiment Run Storage | Parameter insertion, metric updates, concurrent writes | Parameters stored with correct types, metrics maintain temporal ordering, concurrent writes don't corrupt data |\n| Model Version Storage | Version creation, stage updates, lineage recording | Versions are immutable after creation, stage transitions follow rules, lineage relationships preserved |\n| Pipeline Execution Storage | Step status updates, resource tracking, failure recording | Step statuses reflect actual execution, resource usage accurately recorded, failures captured with full context |\n| Deployment Configuration Storage | Traffic split updates, health status changes, configuration versioning | Traffic splits are atomic updates, health status changes timestamped correctly, configuration history preserved |\n\n**Artifact Storage Tests:**\n\n| Artifact Operation | Test Cases | Validation Criteria |\n|-------------------|------------|-------------------|\n| Model Artifact Upload | Large model files, concurrent uploads, checksum validation | Files uploaded completely, concurrent uploads don't interfere, checksums prevent corruption |\n| Pipeline Artifact Passing | Inter-step data transfer, schema validation, cleanup procedures | Data transfers completely between steps, schemas validated at boundaries, temporary artifacts cleaned up |\n| Deployment Artifact Download | Model loading, caching behavior, update detection | Models download correctly for serving, caching reduces redundant downloads, updates detected reliably |\n| Monitoring Data Archival | Log rotation, compression, retention policies | Logs rotated without data loss, compression maintains data integrity, retention policies enforced correctly |\n\n> **Architecture Decision: Testing Database Strategy**\n> - **Context**: Components need database testing but setting up full databases for every test is slow and complex\n> - **Options Considered**: \n>   1. In-memory SQLite for all tests\n>   2. Docker containers with real databases\n>   3. Database mocking with interface validation\n> - **Decision**: Use in-memory SQLite for unit tests, Docker containers for integration tests\n> - **Rationale**: SQLite provides real SQL semantics without setup overhead for fast unit tests, while Docker containers test against production database types for integration scenarios\n> - **Consequences**: Unit tests run quickly in CI/CD, integration tests catch database-specific issues, but requires maintaining test data setup scripts\n\n### End-to-End Scenarios\n\nEnd-to-end scenarios validate complete workflows from data ingestion through model deployment and monitoring, testing the platform as data scientists would actually use it. Think of these tests like **full dress rehearsals** before opening night - everything must work together seamlessly under realistic conditions.\n\n#### Complete ML Workflow Scenarios\n\n**Scenario 1: New Model Development and Deployment**\n\nThis scenario tests the complete journey from initial experiment to production deployment, validating that all components work together to support a typical data science workflow.\n\n*Setup Requirements:*\n- Sample training dataset (100MB CSV with realistic ML features)\n- Training script that logs parameters, metrics, and artifacts\n- Model serving container image with health check endpoint\n- Monitoring configuration with drift detection rules\n\n*Workflow Steps:*\n\n1. **Experiment Tracking Phase**\n   - Data scientist starts new experiment with tagged dataset version\n   - Training script logs hyperparameters (learning_rate, batch_size, model_architecture)\n   - Training loop logs metrics every 100 steps (loss, accuracy, validation_score)\n   - Model artifacts (weights, config, visualization plots) uploaded on completion\n   - Experiment marked as completed with final model accuracy above threshold\n\n2. **Model Registration Phase**\n   - Training run automatically triggers model registration\n   - Model version created with semantic version increment (1.2.3 → 1.3.0)\n   - Lineage links model to training run, dataset version, and code commit\n   - Model stage initialized to Development with registration metadata\n   - Artifact integrity validated through checksum verification\n\n3. **Pipeline Orchestration Phase**\n   - Model validation pipeline triggered for registered model\n   - Validation steps include: accuracy testing, bias analysis, performance benchmarking\n   - Pipeline executes on dedicated validation cluster with GPU resources\n   - Validation results stored as model annotations and pipeline artifacts\n   - Successful validation automatically promotes model to Staging stage\n\n4. **Deployment Phase**\n   - Staging deployment created with single replica for internal testing\n   - Model loaded into inference server with health check validation\n   - Internal testing generates sample predictions with acceptable latency\n   - A/B testing configuration prepared for production canary deployment\n   - Production deployment initiated with 5% traffic allocation\n\n5. **Monitoring Phase**\n   - Prediction logging activated for both staging and production endpoints\n   - Baseline feature distributions captured from training data\n   - Real-time monitoring dashboard configured with key metrics\n   - Drift detection rules configured with warning and critical thresholds\n   - Performance alerts configured for latency, error rate, and throughput\n\n*Expected Outcomes:*\n\n| Phase | Success Criteria | Validation Method |\n|-------|------------------|-------------------|\n| Experiment Tracking | All parameters/metrics logged correctly, artifacts downloadable | Query API returns complete run data, artifacts download without corruption |\n| Model Registration | Model registered with correct version and lineage | Registry API returns model with complete metadata and valid artifact links |\n| Pipeline Orchestration | Validation pipeline completes successfully | Pipeline status shows all steps succeeded, validation metrics meet thresholds |\n| Deployment | Model serving responds correctly to test requests | Health checks pass, test predictions return expected format with acceptable latency |\n| Monitoring | Monitoring captures predictions and computes metrics | Dashboard shows real-time metrics, drift detection processes sample data correctly |\n\n**Scenario 2: Model Update with Canary Deployment**\n\nThis scenario tests updating an existing production model through a controlled canary deployment process, validating traffic management and rollback capabilities.\n\n*Workflow Steps:*\n\n1. **Model Improvement Cycle**\n   - Existing production model (v1.3.0) serves 100% of traffic\n   - New training experiment produces improved model (v1.4.0) with better accuracy\n   - Model registry shows clear lineage from previous version\n   - Staging deployment validates new model performance\n\n2. **Canary Deployment Initiation**\n   - Canary deployment configured: 95% traffic to v1.3.0, 5% to v1.4.0\n   - Traffic routing rules deployed to load balancer\n   - Both model versions receive real production traffic\n   - Monitoring tracks per-version metrics separately\n\n3. **Performance Validation**\n   - Canary version shows improved accuracy on validation metrics\n   - Latency remains within acceptable bounds for both versions\n   - Error rates comparable between versions\n   - No significant drift detected in prediction distributions\n\n4. **Gradual Traffic Increase**\n   - Traffic shifted incrementally: 80%/20%, then 60%/40%, then 20%/80%\n   - Each traffic shift monitored for 2 hours before next increase\n   - Performance metrics remain stable throughout progression\n   - A/B testing shows statistical significance in favor of new version\n\n5. **Full Deployment**\n   - Traffic shifted to 100% new version after successful validation\n   - Old version kept running for 24 hours as rollback safety net\n   - Monitoring continues to validate production performance\n   - Old version finally decommissioned after confirmation\n\n*Validation Points:*\n\n| Stage | Metrics Tracked | Success Thresholds |\n|-------|----------------|-------------------|\n| Canary 5% | Latency p99, error rate, prediction accuracy | p99 < 100ms, error rate < 0.1%, accuracy improvement > 2% |\n| Traffic 20% | Business metrics, user satisfaction, system load | Conversion rate stable, complaints < baseline, CPU utilization < 80% |\n| Traffic 50% | Statistical significance, drift detection, capacity | A/B test p-value < 0.05, drift score < 0.3, no capacity bottlenecks |\n| Full Deployment | Production stability, rollback readiness | Error rate < production baseline, rollback tested and ready |\n\n**Scenario 3: Drift Detection and Model Retraining**\n\nThis scenario tests the platform's ability to detect performance degradation and coordinate automated retraining workflows.\n\n*Workflow Steps:*\n\n1. **Baseline Establishment**\n   - Production model deployed with comprehensive monitoring\n   - Baseline feature distributions captured from training data\n   - Performance thresholds configured for accuracy and drift scores\n   - Automated retraining pipeline prepared but not activated\n\n2. **Gradual Data Drift Introduction**\n   - Simulated drift through gradually shifting input distributions\n   - Feature drift scores increase over two-week period\n   - Prediction accuracy begins declining from baseline levels\n   - Monitoring dashboard shows increasing drift severity\n\n3. **Alert Escalation**\n   - Warning alerts triggered when drift scores exceed low thresholds\n   - Critical alerts triggered when multiple features show significant drift\n   - Escalation procedures notify data science team\n   - Automated retraining pipeline activation considered\n\n4. **Automated Retraining Decision**\n   - Platform evaluates retraining triggers: drift severity, accuracy decline, data availability\n   - Sufficient recent data available for retraining (last 30 days)\n   - Retraining pipeline automatically initiated with updated dataset\n   - Original model continues serving while retraining proceeds\n\n5. **Model Update Cycle**\n   - Retraining completes with improved model adapted to recent data\n   - New model registered with lineage showing drift-triggered retraining\n   - Automated validation confirms improved performance on recent data\n   - Canary deployment initiated with drift-adapted model\n\n*Monitoring Validation:*\n\n| Drift Type | Detection Method | Threshold | Alert Action |\n|------------|------------------|-----------|--------------|\n| Feature Drift | Population Stability Index (PSI) | PSI > 0.2 warning, PSI > 0.4 critical | Warning: dashboard notification, Critical: team alert + retraining evaluation |\n| Concept Drift | Prediction distribution change | KL divergence > 0.3 | Accuracy validation triggered, retraining pipeline evaluated |\n| Performance Drift | Accuracy decline on validation set | >5% decline from baseline | Immediate investigation, expedited retraining if cause unclear |\n| Input Data Quality | Missing features, invalid ranges | >1% invalid requests | Data pipeline investigation, input validation rule updates |\n\n#### Integration Testing with External Systems\n\nMLOps platforms integrate with numerous external systems that must be tested through realistic interfaces rather than simple mocks.\n\n**Cloud Storage Integration:**\n\n| Integration Point | Test Scenarios | Validation Criteria |\n|------------------|----------------|-------------------|\n| Artifact Storage (S3/GCS) | Large model upload/download, concurrent access, network failures | Files transferred completely, concurrent operations don't corrupt data, failures handled gracefully with retries |\n| Data Lake Access | Training data ingestion, schema evolution, access control | Data loaded correctly with schema validation, schema changes handled appropriately, unauthorized access denied |\n| Backup and Disaster Recovery | Metadata backup, artifact replication, cross-region restore | Backups complete and restorable, artifacts replicated correctly, cross-region restore preserves all functionality |\n\n**Container Orchestration Integration:**\n\n| Integration Point | Test Scenarios | Validation Criteria |\n|------------------|----------------|-------------------|\n| Kubernetes Training Jobs | GPU scheduling, distributed training, resource limits | GPUs allocated correctly, distributed training coordinates properly, resource limits enforced |\n| Model Serving Deployment | Auto-scaling, rolling updates, health checks | Auto-scaling responds to load changes, rolling updates maintain availability, health checks prevent bad deployments |\n| Pipeline Step Execution | Container lifecycle, secret management, persistent volumes | Containers start/stop cleanly, secrets mounted securely, data persists across container restarts |\n\n**Monitoring System Integration:**\n\n| Integration Point | Test Scenarios | Validation Criteria |\n|------------------|----------------|-------------------|\n| Metrics Collection (Prometheus) | High-frequency metrics, metric cardinality, retention policies | Metrics collected at required frequency, cardinality stays within limits, retention policies applied correctly |\n| Alerting (AlertManager) | Alert routing, escalation policies, notification delivery | Alerts routed to correct teams, escalation follows defined policies, notifications delivered reliably |\n| Observability (Jaeger/Zipkin) | Distributed tracing, performance profiling, error correlation | Traces capture complete request flows, performance bottlenecks identified, errors correlated across services |\n\n> **Common Pitfalls in End-to-End Testing:**\n> \n> ⚠️ **Pitfall: Test Data Pollution**: Using the same test data repeatedly leads to unrealistic scenarios where models memorize test patterns rather than learning generalizable behaviors.\n> **Fix**: Generate fresh test data for each scenario or use data versioning to ensure tests use appropriate datasets for their validation goals.\n>\n> ⚠️ **Pitfall: Timing Dependencies**: Tests that depend on specific timing (e.g., expecting metrics to update within exactly 5 seconds) become flaky in different environments or under load.\n> **Fix**: Use event-driven validation where possible, and implement exponential backoff polling for time-sensitive assertions with reasonable timeout bounds.\n>\n> ⚠️ **Pitfall: Resource Cleanup**: Failed tests leave behind cloud resources, test models, or storage artifacts that interfere with subsequent test runs and accumulate costs.\n> **Fix**: Implement comprehensive cleanup in test teardown with unique resource naming (test-run-id prefixes) and scheduled cleanup jobs that remove orphaned test resources.\n\n### Milestone Verification\n\nAfter each milestone implementation, specific verification procedures ensure the platform meets acceptance criteria and integrates correctly with existing components. Think of milestone verification like **quality gates in a manufacturing process** - each stage must pass inspection before proceeding to the next phase of development.\n\n#### Milestone 1: Experiment Tracking Verification\n\n**Functional Verification Checklist:**\n\n| Feature | Verification Method | Expected Behavior | Pass Criteria |\n|---------|-------------------|-------------------|---------------|\n| Parameter Logging | Log nested parameters with mixed types | Parameters stored with correct types, nested structure preserved | Query API returns parameters exactly as logged, type information maintained |\n| Metric Tracking | Log metrics at irregular intervals with duplicate steps | Metrics stored with step correlation, duplicates handled correctly | Time series queries return metrics in step order, duplicate handling documented behavior |\n| Artifact Upload | Upload 100MB model file with metadata | File stored with integrity verification, metadata searchable | Download matches upload exactly, metadata query returns correct results |\n| Run Comparison | Compare 5 runs with different parameter sets | Statistical comparison computed correctly, missing data handled | Comparison view shows parameter differences, statistical tests use appropriate methods |\n\n**Performance Verification:**\n\n| Load Scenario | Test Configuration | Performance Target | Measurement Method |\n|---------------|-------------------|-------------------|-------------------|\n| High-Frequency Logging | 1000 metrics/second for 10 minutes | <100ms p99 latency for metric logging | Load testing with concurrent clients, measure response times |\n| Large Artifact Upload | 1GB model file upload | <10 minutes upload time on 100Mbps connection | Time upload operation, verify integrity after completion |\n| Query Performance | Search across 10,000 runs with complex filters | <2 seconds for complex queries | Execute representative queries, measure database response times |\n| Concurrent Access | 50 simultaneous users logging to different runs | No data corruption or lost updates | Verify all logged data appears correctly in final state |\n\n**Integration Verification:**\n\nVerify that experiment tracking correctly publishes events and provides APIs that downstream components can consume.\n\n```bash\n# Event Publication Test\ncurl -X POST http://localhost:8080/api/v1/runs/complete \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"run_id\": \"test-run-123\", \"final_metrics\": {\"accuracy\": 0.95}}'\n\n# Verify EXPERIMENT_COMPLETED event published\ncurl -X GET http://localhost:8080/api/v1/events?type=experiment.completed&since=2024-01-01\n\n# API Integration Test  \ncurl -X GET http://localhost:8080/api/v1/runs/test-run-123/artifacts \\\n  -H \"Authorization: Bearer test-token\"\n```\n\n**Verification Outputs:**\n\nAfter successful milestone 1 verification, the following artifacts should be available:\n\n- **Experiment Dashboard**: Web interface showing experiment list with sortable columns for key metrics\n- **API Documentation**: Complete OpenAPI specification with example requests/responses  \n- **Performance Report**: Load testing results demonstrating throughput and latency targets\n- **Event Integration**: Documented event schemas with example payloads for downstream consumers\n\n#### Milestone 2: Model Registry Verification\n\n**Functional Verification Checklist:**\n\n| Feature | Verification Method | Expected Behavior | Pass Criteria |\n|---------|-------------------|-------------------|---------------|\n| Model Registration | Register model from experiment run with artifacts | Model created with version 1.0.0, linked to run | Registry API returns model with correct lineage, artifacts downloadable |\n| Stage Transitions | Promote model through Development → Staging → Production | Each transition recorded with timestamp and metadata | Stage history shows complete transition log, current stage accurate |\n| Version Management | Register multiple versions of same model | Automatic version incrementing, parallel development branches | Semantic versioning rules followed, version conflicts prevented |\n| Lineage Tracking | Trace model back to training data and code commit | Complete lineage graph from model to source data | Lineage API returns full dependency chain, graph visualization possible |\n\n**Registry Operations Verification:**\n\n| Operation | Test Scenario | Validation Method |\n|-----------|---------------|-------------------|\n| Concurrent Registration | 10 simultaneous model registrations | All models registered successfully, no version conflicts |\n| Large Model Handling | 5GB model artifact registration | Registration completes successfully, checksums validated |\n| Stage Approval Workflow | Model promotion requiring manual approval | Approval metadata captured, unauthorized promotions blocked |\n| Registry Migration | Export/import registry to new deployment | All models, versions, and lineage preserved exactly |\n\n**Model Registry API Verification:**\n\nTest the model registry API endpoints comprehensively:\n\n```bash\n# Model Registration\ncurl -X POST http://localhost:8080/api/v1/models \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"fraud-detection\", \"description\": \"Credit card fraud detection model\"}'\n\n# Version Registration  \ncurl -X POST http://localhost:8080/api/v1/models/fraud-detection/versions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"version\": \"1.0.0\", \"run_id\": \"test-run-123\", \"description\": \"Initial production model\"}'\n\n# Stage Promotion\ncurl -X PUT http://localhost:8080/api/v1/models/fraud-detection/versions/1.0.0/stage \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"stage\": \"Staging\", \"approval_metadata\": {\"approver\": \"data-science-lead\"}}'\n\n# Lineage Query\ncurl -X GET \"http://localhost:8080/api/v1/models/fraud-detection/versions/1.0.0/lineage?depth=3\"\n```\n\n#### Milestone 3: Training Pipeline Verification\n\n**Pipeline Execution Verification:**\n\n| Pipeline Type | Test Configuration | Success Criteria | Verification Method |\n|---------------|-------------------|------------------|-------------------|\n| Linear Pipeline | 5 sequential steps with data dependencies | All steps complete successfully in order | Check step execution logs, verify data flow between steps |\n| Parallel Pipeline | 3 parallel training branches merging to evaluation | Parallel steps execute concurrently, merge step waits for all | Monitor resource utilization, verify merge step input timing |\n| Distributed Training | Multi-GPU training across 4 nodes | Training completes with gradient synchronization | Verify model convergence, check distributed training logs |\n| Failure Recovery | Pipeline with intentional step failure and retry | Failed step retries with exponential backoff | Monitor retry attempts, verify eventual success or failure after max attempts |\n\n**Resource Management Verification:**\n\n| Resource Scenario | Test Configuration | Expected Behavior | Validation Method |\n|-------------------|-------------------|-------------------|-------------------|\n| GPU Allocation | Request 2 GPUs for training step | GPUs allocated exclusively to step | Check GPU utilization, verify no sharing with other processes |\n| Memory Limits | Step requiring 16GB RAM on 8GB node | Step fails with clear resource error | Verify error message indicates insufficient memory |\n| Storage Requirements | Step generating 50GB temporary data | Sufficient storage allocated and cleaned up | Monitor disk usage during and after step execution |\n| Preemptible Instances | Pipeline on spot instances with interruption | Graceful handling of node preemption | Verify checkpoint/resume behavior on node replacement |\n\n**Pipeline Orchestration API Verification:**\n\n```bash\n# Pipeline Definition Upload\ncurl -X POST http://localhost:8080/api/v1/pipelines \\\n  -H \"Content-Type: application/json\" \\\n  -F \"pipeline=@fraud-detection-pipeline.yaml\"\n\n# Pipeline Execution\ncurl -X POST http://localhost:8080/api/v1/pipelines/fraud-detection/runs \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"parameters\": {\"dataset_version\": \"v2.1\", \"learning_rate\": 0.001}}'\n\n# Execution Monitoring\ncurl -X GET http://localhost:8080/api/v1/pipelines/fraud-detection/runs/run-456/status\n\n# Step Log Retrieval  \ncurl -X GET http://localhost:8080/api/v1/pipelines/runs/run-456/steps/training/logs\n```\n\n#### Milestone 4: Model Deployment Verification\n\n**Deployment Strategy Verification:**\n\n| Deployment Type | Test Configuration | Success Criteria | Verification Method |\n|-----------------|-------------------|------------------|-------------------|\n| Blue-Green Deployment | Switch traffic between two model versions | Zero-downtime traffic switch, rollback capability | Monitor request success rate during switch, verify rollback restores previous version |\n| Canary Deployment | Gradual 5%→25%→50%→100% traffic shift | Traffic split accurately according to configuration | Measure actual traffic distribution, verify gradual progression |\n| A/B Testing | Split traffic 50/50 between two models for comparison | Statistical significance calculation, performance comparison | Collect sufficient samples for statistical validity, compare conversion rates |\n\n**Auto-scaling Verification:**\n\n| Load Pattern | Configuration | Expected Behavior | Measurement |\n|--------------|---------------|-------------------|-------------|\n| Gradual Load Increase | Scale up when p99 latency > 100ms | Replicas increase before latency degrades | Monitor latency and replica count during load ramp |\n| Traffic Spike | 10x traffic increase in 1 minute | Rapid scale-up maintains service availability | Verify no request failures during scaling event |\n| Load Decrease | Traffic drops to 10% of peak | Scale down to minimum replicas after cooldown | Confirm scale-down occurs after appropriate delay |\n\n**Model Serving Integration Verification:**\n\nTest integration with various model serving frameworks:\n\n```bash\n# TensorFlow Serving Integration\ncurl -X POST http://localhost:8080/api/v1/deploy \\\n  -H \"Content-Type: application/json\" \\  \n  -d '{\"model_name\": \"fraud-detection\", \"version\": \"1.0.0\", \"serving_framework\": \"tensorflow-serving\"}'\n\n# Model Inference Test\ncurl -X POST http://localhost:8080/v1/models/fraud-detection:predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"instances\": [{\"transaction_amount\": 1500.0, \"merchant_category\": \"restaurant\"}]}'\n\n# Health Check Verification\ncurl -X GET http://localhost:8080/v1/models/fraud-detection/metadata\n```\n\n#### Milestone 5: Model Monitoring Verification\n\n**Drift Detection Verification:**\n\n| Drift Type | Simulation Method | Detection Threshold | Validation Criteria |\n|------------|------------------|-------------------|-------------------|\n| Feature Drift | Gradually shift input feature distributions | PSI > 0.2 warning, PSI > 0.4 critical | Alerts triggered at correct thresholds, drift scores calculated accurately |\n| Concept Drift | Change relationship between features and predictions | Prediction distribution divergence > 0.3 | Concept drift detected before significant accuracy loss |\n| Data Quality Issues | Introduce missing values and outliers | >1% invalid requests | Data quality alerts trigger investigation workflows |\n\n**Performance Monitoring Verification:**\n\n| Metric Category | Measurement | Target | Validation Method |\n|-----------------|-------------|---------|-------------------|\n| Prediction Latency | p50, p90, p99 latency percentiles | p99 < 100ms | Load test with realistic request patterns |\n| Throughput | Requests per second capacity | >1000 RPS sustained | Sustained load test for 30 minutes |\n| Accuracy Tracking | Online accuracy vs batch validation | <5% difference | Compare online predictions with known labels |\n\n**Monitoring Dashboard Verification:**\n\nThe monitoring dashboard should display real-time and historical metrics:\n\n```bash\n# Dashboard API Test\ncurl -X GET \"http://localhost:8080/api/v1/monitoring/fraud-detection/metrics?start_time=2024-01-01&end_time=2024-01-02\"\n\n# Real-time Metrics\ncurl -X GET \"http://localhost:8080/api/v1/monitoring/fraud-detection/realtime\"\n\n# Drift Analysis  \ncurl -X GET \"http://localhost:8080/api/v1/monitoring/fraud-detection/drift?analysis_window=24h\"\n\n# Alert History\ncurl -X GET \"http://localhost:8080/api/v1/monitoring/fraud-detection/alerts?severity=critical&since=7d\"\n```\n\n> **Design Insight**: Milestone verification should be **cumulative** - each milestone should continue to validate all previous functionality while adding new capabilities. This ensures that integration work doesn't break existing features and that the platform maintains coherent functionality as complexity increases.\n\n### Implementation Guidance\n\nThe testing strategy requires careful coordination between fast-running unit tests and comprehensive integration scenarios. Focus on building a **testing pyramid** where fast unit tests provide immediate feedback during development, while slower integration tests validate realistic workflows.\n\n#### Technology Recommendations\n\n| Testing Layer | Simple Option | Advanced Option |\n|---------------|---------------|-----------------|\n| Unit Testing | pytest with fixtures | pytest + hypothesis for property-based testing |\n| API Testing | requests library with custom helpers | tavern for API contract testing |\n| Database Testing | SQLite in-memory for fast tests | testcontainers for realistic database testing |\n| Load Testing | Simple threading with concurrent.futures | locust for distributed load testing |\n| End-to-End Testing | selenium for web UI testing | playwright for modern web automation |\n| Test Data Management | JSON fixtures in test files | factory-boy for realistic data generation |\n\n#### Recommended File Structure\n\nOrganize test files to mirror the application structure while providing clear separation between test types:\n\n```\nmlops-platform/\n├── tests/\n│   ├── unit/                          # Fast unit tests\n│   │   ├── experiment_tracking/       # Component-specific unit tests\n│   │   │   ├── test_parameter_logging.py\n│   │   │   ├── test_metric_tracking.py\n│   │   │   └── test_artifact_storage.py\n│   │   ├── model_registry/\n│   │   │   ├── test_version_management.py\n│   │   │   ├── test_stage_transitions.py\n│   │   │   └── test_lineage_tracking.py\n│   │   └── shared/                     # Shared test utilities\n│   │       ├── fixtures.py\n│   │       └── test_helpers.py\n│   ├── integration/                    # Component integration tests\n│   │   ├── test_api_integration.py\n│   │   ├── test_event_coordination.py\n│   │   └── test_database_integration.py\n│   ├── end_to_end/                     # Complete workflow tests\n│   │   ├── test_model_development_flow.py\n│   │   ├── test_canary_deployment.py\n│   │   └── test_drift_detection.py\n│   ├── performance/                    # Load and performance tests\n│   │   ├── test_experiment_tracking_load.py\n│   │   ├── test_serving_throughput.py\n│   │   └── test_monitoring_scalability.py\n│   └── fixtures/                       # Test data and configuration\n│       ├── sample_datasets/\n│       ├── model_artifacts/\n│       └── pipeline_definitions/\n├── src/\n│   └── mlops_platform/                # Application code\n└── docker-compose.test.yml            # Test infrastructure\n```\n\n#### Test Infrastructure Setup\n\n**Docker Compose Test Environment:**\n\n```yaml\n# docker-compose.test.yml - Complete test infrastructure\nversion: '3.8'\nservices:\n  postgres-test:\n    image: postgres:14\n    environment:\n      POSTGRES_DB: mlops_test\n      POSTGRES_USER: test_user\n      POSTGRES_PASSWORD: test_pass\n    ports:\n      - \"5433:5432\"\n    \n  minio-test:\n    image: minio/minio\n    command: server /data --console-address \":9001\"\n    environment:\n      MINIO_ROOT_USER: test_access_key\n      MINIO_ROOT_PASSWORD: test_secret_key\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    \n  redis-test:\n    image: redis:7-alpine\n    ports:\n      - \"6380:6379\"\n```\n\n**Test Configuration Management:**\n\n```python\n# tests/shared/config.py - Centralized test configuration\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass TestConfig:\n    \"\"\"Centralized configuration for all test environments.\"\"\"\n    \n    # Database configuration\n    postgres_url: str = \"postgresql://test_user:test_pass@localhost:5433/mlops_test\"\n    \n    # Object storage configuration  \n    minio_endpoint: str = \"localhost:9000\"\n    minio_access_key: str = \"test_access_key\"\n    minio_secret_key: str = \"test_secret_key\"\n    \n    # Redis configuration\n    redis_url: str = \"redis://localhost:6380/0\"\n    \n    # API base URLs\n    experiment_api_base: str = \"http://localhost:8080/api/v1/experiments\"\n    registry_api_base: str = \"http://localhost:8080/api/v1/models\"\n    \n    # Test data paths\n    fixtures_path: str = \"tests/fixtures\"\n    sample_model_path: str = \"tests/fixtures/model_artifacts/sample_model.pkl\"\n    \n    @classmethod\n    def from_environment(cls) -> 'TestConfig':\n        \"\"\"Load configuration from environment variables.\"\"\"\n        return cls(\n            postgres_url=os.getenv('TEST_POSTGRES_URL', cls.postgres_url),\n            minio_endpoint=os.getenv('TEST_MINIO_ENDPOINT', cls.minio_endpoint),\n            # ... other environment variable mappings\n        )\n```\n\n#### Test Utilities and Fixtures\n\n**Database Test Utilities:**\n\n```python\n# tests/shared/database.py - Database testing utilities\nimport pytest\nimport asyncpg\nfrom typing import Generator\nfrom contextlib import asynccontextmanager\n\nclass DatabaseTestHelper:\n    \"\"\"Helper class for database testing operations.\"\"\"\n    \n    def __init__(self, postgres_url: str):\n        self.postgres_url = postgres_url\n        \n    @asynccontextmanager\n    async def temporary_database(self) -> Generator[str, None, None]:\n        \"\"\"Create a temporary database for testing.\"\"\"\n        # TODO 1: Generate unique database name with test prefix\n        # TODO 2: Create database using asyncpg connection\n        # TODO 3: Run schema migrations on new database  \n        # TODO 4: Yield database URL for test use\n        # TODO 5: Clean up database after test completion\n        pass\n    \n    async def reset_database(self, database_url: str) -> None:\n        \"\"\"Reset database to clean state between tests.\"\"\"\n        # TODO 1: Connect to database\n        # TODO 2: Truncate all tables in correct dependency order\n        # TODO 3: Reset auto-increment sequences\n        # TODO 4: Verify database is in clean state\n        pass\n\n@pytest.fixture\nasync def clean_database():\n    \"\"\"Provides a clean database for each test.\"\"\"\n    helper = DatabaseTestHelper(TestConfig().postgres_url)\n    async with helper.temporary_database() as db_url:\n        yield db_url\n        # Database automatically cleaned up by context manager\n```\n\n**API Testing Utilities:**\n\n```python\n# tests/shared/api_client.py - API testing client\nimport requests\nimport json\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass APIResponse:\n    \"\"\"Structured API response for testing assertions.\"\"\"\n    status_code: int\n    json_data: Optional[Dict[str, Any]]\n    headers: Dict[str, str]\n    elapsed_seconds: float\n\nclass MLOpsAPIClient:\n    \"\"\"Test client for MLOps platform APIs.\"\"\"\n    \n    def __init__(self, base_url: str, auth_token: Optional[str] = None):\n        self.base_url = base_url.rstrip('/')\n        self.session = requests.Session()\n        if auth_token:\n            self.session.headers['Authorization'] = f'Bearer {auth_token}'\n    \n    def create_experiment(self, name: str, tags: Dict[str, str] = None) -> APIResponse:\n        \"\"\"Create a new experiment.\"\"\"\n        # TODO 1: Prepare request payload with experiment data\n        # TODO 2: Make POST request to experiments endpoint\n        # TODO 3: Parse response and measure response time\n        # TODO 4: Return structured APIResponse object\n        # TODO 5: Handle request failures with descriptive errors\n        pass\n    \n    def log_metrics(self, run_id: str, metrics: Dict[str, float], step: int) -> APIResponse:\n        \"\"\"Log metrics for a specific run and step.\"\"\"\n        # TODO 1: Validate metric names and values\n        # TODO 2: Format metrics according to API schema\n        # TODO 3: Make POST request with batch metric logging\n        # TODO 4: Verify response indicates successful logging\n        # TODO 5: Return response with timing information\n        pass\n    \n    def wait_for_experiment_completion(self, run_id: str, timeout_seconds: int = 300) -> bool:\n        \"\"\"Wait for experiment run to complete with polling.\"\"\"\n        # TODO 1: Start polling timer for timeout handling\n        # TODO 2: Poll run status every 5 seconds\n        # TODO 3: Return True when run status becomes FINISHED\n        # TODO 4: Return False if timeout exceeded or run FAILED\n        # TODO 5: Log polling progress for debugging\n        pass\n```\n\n#### Test Data Generation\n\n**Realistic ML Data Factories:**\n\n```python\n# tests/shared/data_factories.py - Test data generation\nimport factory\nimport random\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Any, List\n\nclass ExperimentFactory(factory.Factory):\n    \"\"\"Factory for creating realistic experiment test data.\"\"\"\n    \n    class Meta:\n        model = dict\n    \n    experiment_id = factory.LazyFunction(lambda: f\"exp_{random.randint(1000, 9999)}\")\n    name = factory.Sequence(lambda n: f\"fraud_detection_experiment_{n}\")\n    creation_time = factory.LazyFunction(lambda: datetime.utcnow().timestamp())\n    tags = factory.LazyFunction(lambda: {\n        \"team\": random.choice([\"data-science\", \"ml-engineering\"]),\n        \"priority\": random.choice([\"high\", \"medium\", \"low\"]),\n        \"dataset_version\": f\"v{random.randint(1, 5)}.{random.randint(0, 9)}\"\n    })\n\nclass RunFactory(factory.Factory):\n    \"\"\"Factory for creating ML training run data.\"\"\"\n    \n    class Meta:\n        model = dict\n        \n    run_id = factory.LazyFunction(lambda: f\"run_{random.randint(10000, 99999)}\")\n    experiment_id = factory.SubFactory(ExperimentFactory)['experiment_id']\n    parameters = factory.LazyFunction(lambda: {\n        \"learning_rate\": round(random.uniform(0.0001, 0.01), 6),\n        \"batch_size\": random.choice([16, 32, 64, 128]),\n        \"epochs\": random.randint(10, 100),\n        \"model_type\": random.choice([\"random_forest\", \"xgboost\", \"neural_network\"])\n    })\n    \n    @factory.LazyAttribute\n    def metrics(obj):\n        \"\"\"Generate realistic training metrics time series.\"\"\"\n        epochs = obj.parameters[\"epochs\"]\n        return {\n            \"accuracy\": [round(0.5 + 0.4 * (1 - np.exp(-i/10)), 4) for i in range(epochs)],\n            \"loss\": [round(2.0 * np.exp(-i/15) + 0.1, 4) for i in range(epochs)],\n            \"val_accuracy\": [round(0.45 + 0.35 * (1 - np.exp(-i/12)), 4) for i in range(epochs)]\n        }\n\ndef generate_realistic_training_data(num_samples: int = 1000) -> Dict[str, Any]:\n    \"\"\"Generate realistic training dataset for testing.\"\"\"\n    # TODO 1: Create feature columns with realistic distributions\n    # TODO 2: Add correlations between features that ML models would learn\n    # TODO 3: Generate target variable with realistic class imbalance\n    # TODO 4: Add some missing values and outliers for robustness testing\n    # TODO 5: Package data in format expected by training pipelines\n    pass\n```\n\n#### Milestone Checkpoint Implementation\n\n**Automated Milestone Validation:**\n\n```python\n# tests/milestones/milestone_validator.py - Automated milestone validation\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass ValidationResult:\n    \"\"\"Result of a milestone validation check.\"\"\"\n    check_name: str\n    passed: bool\n    details: Dict[str, Any]\n    error_message: str = \"\"\n\nclass MilestoneValidator(ABC):\n    \"\"\"Base class for milestone validation.\"\"\"\n    \n    @abstractmethod\n    def validate(self) -> List[ValidationResult]:\n        \"\"\"Run all validation checks for this milestone.\"\"\"\n        pass\n\nclass Milestone1Validator(MilestoneValidator):\n    \"\"\"Validator for Experiment Tracking milestone.\"\"\"\n    \n    def __init__(self, api_client: MLOpsAPIClient):\n        self.api = api_client\n    \n    def validate(self) -> List[ValidationResult]:\n        \"\"\"Validate experiment tracking functionality.\"\"\"\n        results = []\n        \n        # TODO 1: Test parameter logging with various data types\n        results.append(self._test_parameter_logging())\n        \n        # TODO 2: Test metric tracking with time series data\n        results.append(self._test_metric_tracking())\n        \n        # TODO 3: Test artifact upload and download integrity\n        results.append(self._test_artifact_handling())\n        \n        # TODO 4: Test experiment querying and filtering\n        results.append(self._test_experiment_queries())\n        \n        # TODO 5: Test run comparison functionality\n        results.append(self._test_run_comparison())\n        \n        return results\n    \n    def _test_parameter_logging(self) -> ValidationResult:\n        \"\"\"Test parameter logging with various data types.\"\"\"\n        # TODO 1: Create test experiment and run\n        # TODO 2: Log parameters with different types (int, float, str, dict, list)\n        # TODO 3: Retrieve parameters and verify exact match\n        # TODO 4: Test parameter overwriting behavior\n        # TODO 5: Return validation result with details\n        pass\n\ndef run_milestone_validation(milestone: int) -> bool:\n    \"\"\"Run validation for specified milestone.\"\"\"\n    validators = {\n        1: Milestone1Validator,\n        2: Milestone2Validator,\n        3: Milestone3Validator,\n        4: Milestone4Validator,\n        5: Milestone5Validator\n    }\n    \n    if milestone not in validators:\n        print(f\"Unknown milestone: {milestone}\")\n        return False\n    \n    # TODO 1: Initialize validator with appropriate configuration\n    # TODO 2: Run all validation checks\n    # TODO 3: Print detailed results for failed checks\n    # TODO 4: Return overall pass/fail status\n    # TODO 5: Generate validation report for documentation\n    pass\n\n# CLI entry point for milestone validation\nif __name__ == \"__main__\":\n    import sys\n    milestone = int(sys.argv[1]) if len(sys.argv) > 1 else 1\n    success = run_milestone_validation(milestone)\n    sys.exit(0 if success else 1)\n```\n\n#### Performance Testing Framework\n\n**Load Testing Infrastructure:**\n\n```python\n# tests/performance/load_test_framework.py - Load testing framework\nimport asyncio\nimport time\nimport statistics\nfrom typing import List, Dict, Callable, Any\nfrom dataclasses import dataclass, field\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n@dataclass\nclass LoadTestResult:\n    \"\"\"Results from a load testing scenario.\"\"\"\n    total_requests: int\n    successful_requests: int\n    failed_requests: int\n    response_times: List[float]\n    throughput_rps: float\n    p50_latency: float\n    p95_latency: float\n    p99_latency: float\n    error_rate: float\n    \n    @classmethod\n    def from_measurements(cls, response_times: List[float], errors: List[str], duration: float):\n        \"\"\"Calculate load test metrics from raw measurements.\"\"\"\n        # TODO 1: Calculate basic counts and rates\n        # TODO 2: Compute latency percentiles using statistics module\n        # TODO 3: Calculate throughput as requests per second\n        # TODO 4: Determine error rate from failed requests\n        # TODO 5: Return structured result object\n        pass\n\nclass LoadTestRunner:\n    \"\"\"Framework for running load tests against MLOps APIs.\"\"\"\n    \n    def __init__(self, max_workers: int = 50):\n        self.max_workers = max_workers\n    \n    def run_load_test(\n        self, \n        test_function: Callable[[], Any],\n        target_rps: int,\n        duration_seconds: int,\n        warmup_seconds: int = 30\n    ) -> LoadTestResult:\n        \"\"\"Run load test with specified parameters.\"\"\"\n        # TODO 1: Execute warmup period to stabilize system\n        # TODO 2: Calculate request timing to achieve target RPS\n        # TODO 3: Use ThreadPoolExecutor for concurrent request execution\n        # TODO 4: Collect response times and errors from all requests\n        # TODO 5: Calculate and return performance metrics\n        pass\n\ndef test_experiment_tracking_load():\n    \"\"\"Load test for experiment tracking API.\"\"\"\n    \n    def single_request():\n        \"\"\"Execute single API request for load testing.\"\"\"\n        # TODO 1: Generate unique test data for this request\n        # TODO 2: Make API call to log parameters and metrics\n        # TODO 3: Measure response time and capture any errors\n        # TODO 4: Return timing and success information\n        # TODO 5: Include resource usage if available\n        pass\n    \n    runner = LoadTestRunner(max_workers=100)\n    \n    # Test scenarios with different load patterns\n    scenarios = [\n        {\"target_rps\": 100, \"duration\": 300, \"description\": \"Sustained load\"},\n        {\"target_rps\": 500, \"duration\": 60, \"description\": \"Peak load burst\"},\n        {\"target_rps\": 1000, \"duration\": 30, \"description\": \"Stress test\"}\n    ]\n    \n    for scenario in scenarios:\n        print(f\"Running {scenario['description']} scenario...\")\n        result = runner.run_load_test(\n            single_request,\n            scenario[\"target_rps\"],\n            scenario[\"duration\"]\n        )\n        \n        # Validate performance targets\n        assert result.p99_latency < 100.0, f\"p99 latency too high: {result.p99_latency}ms\"\n        assert result.error_rate < 0.01, f\"Error rate too high: {result.error_rate}\"\n        \n        print(f\"Results: {result.throughput_rps:.1f} RPS, \"\n              f\"p99: {result.p99_latency:.1f}ms, \"\n              f\"errors: {result.error_rate:.2%}\")\n\n```\n\n\n## Debugging Guide\n\n> **Milestone(s):** This section applies to all milestones (1-5) by providing comprehensive debugging strategies that help developers diagnose and fix issues across experiment tracking, model registry, training pipeline orchestration, model deployment, and model monitoring components.\n\nBuilding an MLOps platform involves coordinating multiple distributed components, each with its own failure modes and debugging challenges. Think of debugging a distributed MLOps system like being a detective investigating a crime scene where the evidence is scattered across multiple locations, timestamps don't always align, and witnesses (logs) might be unreliable or missing. Unlike debugging a single-threaded application where you can step through code linearly, MLOps debugging requires understanding how components interact asynchronously, how data flows between systems, and how failures cascade across service boundaries.\n\nThe complexity of MLOps debugging stems from the inherent distributed nature of the system. An experiment might fail due to a parameter logging issue in the tracking component, but the symptoms appear as missing metrics in the dashboard. A model deployment might succeed technically but fail functionally due to data drift that wasn't detected by the monitoring component. A training pipeline might hang indefinitely due to resource contention in the orchestration layer, but the user only sees a \"pending\" status in the UI.\n\nThis section provides structured approaches for diagnosing and fixing these complex, multi-component issues. We'll start with symptom-based diagnosis tables that help identify root causes, then cover debugging tools and techniques for gaining visibility into system behavior, and finally address performance troubleshooting for identifying and eliminating bottlenecks.\n\n### Symptom-Based Diagnosis\n\nThe key to effective MLOps debugging is recognizing that symptoms often appear far from their root causes. A deployment failure might be caused by an experiment tracking issue, or a monitoring alert might indicate a problem with the model registry. This subsection provides comprehensive symptom-to-cause mappings organized by the component where symptoms typically appear.\n\n**Experiment Tracking Issues**\n\n| Symptom | Likely Causes | Diagnostic Steps | Resolution Strategy |\n|---------|---------------|------------------|-------------------|\n| Experiment run stuck in RUNNING status | Database connection timeout, missing end_time update, process crash during logging | Check database connectivity with `ComponentHealth.run_checks()`, query recent `Run` records for missing `end_time` values, examine application logs for crash stack traces | Implement automated cleanup job that sets abandoned runs to FAILED after timeout, add database connection pooling, ensure `log_artifact` calls include proper exception handling |\n| Parameters not appearing in experiment comparison | Parameter value serialization failure, metadata store write timeout, incorrect run_id correlation | Validate parameter values with `Parameter` type constraints, check `MetadataStore.insert` return codes, verify run_id exists in experiments table | Add parameter validation before storage, implement retry logic for metadata writes, create foreign key constraints for referential integrity |\n| Artifact upload fails intermittently | Object storage network timeouts, insufficient storage permissions, concurrent write conflicts | Test `ArtifactStore.put` with sample data, verify IAM permissions for storage bucket, check for duplicate artifact paths | Implement exponential backoff retry for storage operations, add artifact path uniqueness validation, create separate storage prefixes per run |\n| Metric comparison shows inconsistent results | Clock skew between training nodes, metric aggregation errors, floating-point precision issues | Compare timestamps across `MetricPoint` entries, validate metric calculations with known test data, check for NaN or infinity values | Synchronize clocks using NTP, implement consistent metric aggregation with stable sorting, add numerical validation for metric values |\n| Search queries return incomplete results | Database query timeouts, missing indexes on search columns, pagination implementation bugs | Execute search query directly against database, check query execution plan for full table scans, validate `search_runs` pagination logic | Add composite indexes on commonly searched columns, implement query result caching, optimize pagination with cursor-based navigation |\n\n**Model Registry Issues**\n\n| Symptom | Likely Causes | Diagnostic Steps | Resolution Strategy |\n|---------|---------------|------------------|-------------------|\n| Model version promotion fails silently | Stage transition validation errors, approval workflow misconfiguration, database constraint violations | Check `ModelStage` enum constraints, validate approval metadata format, examine database transaction logs | Implement explicit validation for stage transitions, add approval workflow status tracking, create audit log for all stage changes |\n| Model lineage graph shows broken connections | Missing foreign key relationships, experiment run cleanup affecting lineage, metadata corruption | Trace lineage path using `get_model_lineage`, verify experiment run still exists, check for orphaned model versions | Implement soft deletion for experiment runs, add referential integrity constraints, create lineage validation procedures |\n| Model artifact download corruption | Network transmission errors, storage checksum mismatches, concurrent modification during download | Verify artifact checksum with `get_artifact` validation, test download with different network conditions, check for write operations during download | Implement artifact immutability guarantees, add download retry with integrity verification, use atomic storage operations |\n| Version registration succeeds but model missing | Asynchronous processing delays, metadata store inconsistency, artifact storage failures after metadata commit | Check `ModelVersion` creation timestamp vs artifact upload time, verify artifact exists in storage, examine event processing logs | Implement two-phase commit for version registration, add artifact existence validation before metadata commit, create reconciliation jobs |\n| Model search returns stale results | Metadata caching inconsistencies, database replica lag, index update delays | Compare direct database query with search API results, check cache invalidation logs, verify database replication status | Implement cache invalidation on model updates, add read-after-write consistency checks, create cache warming procedures |\n\n**Training Pipeline Issues**\n\n| Symptom | Likely Causes | Diagnostic Steps | Resolution Strategy |\n|---------|---------------|------------------|-------------------|\n| Pipeline execution hangs indefinitely | Resource allocation deadlock, step dependency cycles, Kubernetes job scheduling failures | Check `StepExecution` status for all pipeline steps, validate DAG structure with `validate_pipeline_dag`, examine Kubernetes events | Implement pipeline execution timeouts, add dependency cycle detection, create resource allocation monitoring |\n| Step fails with \"resource exhausted\" error | Insufficient cluster capacity, resource quota exceeded, memory leak in step container | Check cluster resource utilization, verify resource quotas with `kubectl describe quota`, examine container memory usage patterns | Implement dynamic resource scaling, add resource usage monitoring per step, optimize container resource requests |\n| Data passing between steps corrupted | Network failures during artifact transfer, concurrent write/read operations, serialization format changes | Verify artifact integrity with checksums, check network connectivity between nodes, validate data schema consistency | Implement atomic data transfer operations, add data validation at step boundaries, create backup data paths |\n| Parallel step execution produces inconsistent results | Race conditions in shared resource access, non-deterministic processing order, inadequate step isolation | Execute steps sequentially to verify correctness, check for shared file system access patterns, validate container isolation settings | Implement proper step isolation, add shared resource locking mechanisms, create deterministic execution ordering |\n| Pipeline restart from checkpoint fails | Checkpoint data corruption, missing intermediate artifacts, version incompatibility between runs | Verify checkpoint file integrity, check artifact availability for restart point, compare pipeline version with checkpoint metadata | Implement checkpoint validation before restart, add backward compatibility for pipeline versions, create checkpoint repair procedures |\n\n**Model Deployment Issues**\n\n| Symptom | Likely Causes | Diagnostic Steps | Resolution Strategy |\n|---------|---------------|------------------|-------------------|\n| Model endpoint returns 5xx errors | Model loading failures, insufficient memory allocation, inference server misconfiguration | Check container logs for model loading errors, verify memory usage against allocation, test inference server configuration locally | Implement model loading validation, add memory usage monitoring, create inference server health checks |\n| Traffic splitting not working correctly | Load balancer misconfiguration, endpoint weight calculation errors, routing rule conflicts | Verify traffic percentages sum to 100%, check load balancer configuration, examine routing rule precedence | Implement traffic split validation, add routing rule conflict detection, create traffic distribution monitoring |\n| Canary deployment stuck in progress | Health check failures for new version, automatic rollback trigger conditions, deployment orchestration bugs | Check `DeploymentStatus` for new version, verify health check results, examine deployment orchestration logs | Implement deployment timeout mechanisms, add health check debugging tools, create manual deployment control overrides |\n| Auto-scaling thrashing | Inappropriate scaling metrics, too aggressive scaling policies, resource allocation delays | Monitor scaling decision logs, check metric collection frequency, verify resource allocation timing | Implement scaling decision smoothing, add scaling policy validation, create resource allocation monitoring |\n| Model serving latency spikes | Cold start delays, resource contention, network connectivity issues | Measure model loading time, check resource utilization during spikes, test network connectivity to inference servers | Implement model warming procedures, add resource reservation for serving, create network connectivity monitoring |\n\n**Model Monitoring Issues**\n\n| Symptom | Likely Causes | Diagnostic Steps | Resolution Strategy |\n|---------|---------------|------------------|-------------------|\n| Drift detection shows false positives | Inappropriate statistical thresholds, seasonal data patterns, insufficient baseline data | Review drift detection parameters, analyze data patterns over longer time periods, verify baseline data quality | Implement adaptive thresholds, add seasonal pattern detection, create baseline data validation procedures |\n| Prediction logging missing entries | Network failures during logging, storage quota exceeded, logging buffer overflow | Check network connectivity to prediction logger, verify storage space availability, examine logging buffer configuration | Implement prediction logging retry mechanisms, add storage monitoring, create logging buffer scaling policies |\n| Performance metrics calculation errors | Timestamp synchronization issues, missing ground truth data, aggregation window configuration errors | Verify timestamp consistency across logs, check ground truth data availability, validate aggregation window settings | Implement timestamp normalization, add ground truth data validation, create aggregation window optimization |\n| Alerts not firing for known issues | Alert threshold misconfiguration, notification delivery failures, alert rule evaluation errors | Manually trigger alert conditions, check notification channel configuration, examine alert rule evaluation logs | Implement alert rule testing procedures, add notification delivery confirmation, create alert escalation mechanisms |\n| Dashboard shows stale monitoring data | Data pipeline processing delays, cache invalidation issues, metric aggregation job failures | Check data pipeline processing logs, verify cache invalidation timing, examine metric aggregation job status | Implement real-time data processing, add cache invalidation monitoring, create metric aggregation job recovery |\n\n**Cross-Component Integration Issues**\n\n| Symptom | Likely Causes | Diagnostic Steps | Resolution Strategy |\n|---------|---------------|------------------|-------------------|\n| Events not propagating between components | Message queue failures, event serialization errors, subscriber registration issues | Check event queue health, verify event payload format, examine subscriber registration logs | Implement event delivery confirmation, add event serialization validation, create subscriber health monitoring |\n| API calls timing out between components | Network connectivity issues, service overload, authentication token expiration | Test network connectivity between services, check service resource utilization, verify authentication token validity | Implement API call retry mechanisms, add service load monitoring, create token refresh automation |\n| Data inconsistency across components | Transaction boundary violations, eventual consistency delays, concurrent modification conflicts | Compare data state across components, check transaction isolation levels, examine conflict resolution logs | Implement proper transaction boundaries, add consistency validation procedures, create conflict resolution mechanisms |\n| Circuit breaker preventing valid requests | Overly aggressive failure detection, insufficient recovery time, health check implementation issues | Review circuit breaker configuration, check failure detection logs, verify health check accuracy | Implement circuit breaker tuning procedures, add failure pattern analysis, create health check debugging tools |\n\n### Debugging Tools and Techniques\n\nEffective MLOps debugging requires a comprehensive toolkit that provides visibility into system behavior across multiple components and abstraction layers. Think of debugging tools like a diagnostic laboratory for distributed systems - you need different instruments to examine different types of evidence, from microscopic code-level behavior to macroscopic system-wide patterns.\n\n**Structured Logging Strategy**\n\nThe foundation of MLOps debugging is structured logging that correlates activity across components using correlation IDs. Every operation should generate logs with consistent format, appropriate verbosity levels, and contextual information that helps reconstruct system behavior during incidents.\n\n| Log Level | Use Cases | Information Included | Retention Policy |\n|-----------|-----------|---------------------|-----------------|\n| DEBUG | Detailed execution flow, parameter values, intermediate calculations | Function entry/exit, variable values, algorithm steps | 7 days, high volume |\n| INFO | Normal operations, successful completions, state transitions | Operation results, timing information, resource usage | 30 days, moderate volume |\n| WARN | Recoverable errors, degraded performance, configuration issues | Error conditions, fallback mechanisms, performance metrics | 90 days, low volume |\n| ERROR | Operation failures, data corruption, service unavailability | Error messages, stack traces, recovery actions | 1 year, critical preservation |\n| FATAL | System-wide failures, data loss, security breaches | Complete context, system state, emergency procedures | Permanent, immediate escalation |\n\nThe logging implementation should include correlation IDs that trace operations across component boundaries. When an experiment run begins, generate a unique correlation ID that appears in all related logs across experiment tracking, model registry, training pipeline, deployment, and monitoring components. This enables reconstructing the complete flow of operations even when failures occur in different components at different times.\n\n| Log Field | Type | Purpose | Example Value |\n|-----------|------|---------|---------------|\n| timestamp | float | Precise timing for sequence reconstruction | 1640995200.123456 |\n| level | str | Log severity for filtering and alerting | \"ERROR\" |\n| component | str | Source component for distributed tracing | \"experiment-tracker\" |\n| correlation_id | str | Operation correlation across components | \"exp_20220101_abc123\" |\n| operation | str | Specific operation being performed | \"log_metric\" |\n| message | str | Human-readable description | \"Failed to store metric value\" |\n| context | dict | Structured context for debugging | {\"run_id\": \"run_123\", \"metric\": \"accuracy\"} |\n| stack_trace | str | Error stack trace when applicable | Full Python/Go stack trace |\n\n**Health Check and Monitoring Systems**\n\nComprehensive health checking provides real-time visibility into component status and enables proactive issue detection. The `ComponentHealth` system should implement both shallow and deep health checks that validate different aspects of system functionality.\n\n| Health Check Type | Validation Scope | Check Frequency | Failure Threshold |\n|-------------------|------------------|----------------|-------------------|\n| Shallow | Basic connectivity, process status | Every 30 seconds | 3 consecutive failures |\n| Deep | End-to-end functionality, data consistency | Every 5 minutes | 2 consecutive failures |\n| External | Dependent service availability | Every 2 minutes | 5 consecutive failures |\n| Resource | Memory, disk, CPU utilization | Every 1 minute | Threshold-based alerts |\n\nThe health check implementation should provide detailed diagnostic information when checks fail, not just binary pass/fail status. For example, a database health check should report connection pool status, query response times, and any constraint violations.\n\n| Component | Health Check Method | Success Criteria | Diagnostic Information |\n|-----------|-------------------|------------------|----------------------|\n| Experiment Tracker | `check_experiment_tracking()` | Can create run and log metric within 5 seconds | Database connection status, storage availability, recent operation latency |\n| Model Registry | `check_model_registry()` | Can retrieve model version and download artifact within 10 seconds | Metadata store responsiveness, artifact storage connectivity, lineage graph integrity |\n| Training Pipeline | `check_pipeline_orchestration()` | Can submit test job and receive status update within 30 seconds | Kubernetes cluster status, resource availability, job queue depth |\n| Model Deployment | `check_model_serving()` | Can route request and receive prediction within 2 seconds | Endpoint health, traffic routing accuracy, auto-scaler responsiveness |\n| Model Monitoring | `check_model_monitoring()` | Can log prediction and compute metrics within 5 seconds | Prediction logging pipeline status, drift detection job health, alerting system connectivity |\n\n**Distributed Tracing Implementation**\n\nDistributed tracing provides end-to-end visibility into request flows across multiple components. Unlike logs which provide point-in-time snapshots, traces show the complete journey of operations through the system, including timing, dependencies, and error propagation paths.\n\nThe tracing system should capture spans for all major operations, with parent-child relationships that reflect the actual call hierarchy. Each span should include operation metadata, timing information, and any errors encountered.\n\n| Trace Component | Span Operations | Metadata Captured | Error Information |\n|-----------------|----------------|-------------------|-------------------|\n| API Gateway | Request routing, authentication, rate limiting | HTTP method, endpoint, client ID, response code | Authentication failures, rate limit violations, routing errors |\n| Experiment Tracker | Parameter logging, metric storage, artifact upload | Run ID, parameter count, artifact size, storage location | Serialization errors, storage failures, validation errors |\n| Model Registry | Version creation, stage promotion, lineage tracking | Model name, version number, stage transition, lineage depth | Version conflicts, approval failures, lineage corruption |\n| Pipeline Orchestrator | Job submission, resource allocation, step execution | Pipeline ID, step count, resource requirements, execution time | Resource allocation failures, step execution errors, timeout conditions |\n| Model Deployment | Traffic routing, scaling decisions, health checks | Model version, traffic percentage, replica count, response time | Routing failures, scaling errors, health check failures |\n| Model Monitoring | Prediction logging, drift detection, alert evaluation | Model name, prediction count, drift score, alert status | Logging failures, calculation errors, alert delivery issues |\n\n**Performance Profiling Tools**\n\nPerformance profiling helps identify bottlenecks and resource utilization patterns across the MLOps platform. Profiling should cover both individual component performance and cross-component interaction patterns.\n\n| Profiling Type | Measurement Focus | Collection Method | Analysis Tools |\n|-----------------|-------------------|-------------------|----------------|\n| CPU Profiling | Function execution time, hot paths, blocking operations | Statistical sampling every 10ms | Flame graphs, call trees, execution histograms |\n| Memory Profiling | Allocation patterns, memory leaks, garbage collection | Heap snapshots every 5 minutes | Memory growth analysis, allocation tracking, leak detection |\n| I/O Profiling | Database queries, storage operations, network requests | Operation timing and volume | Query optimization, storage access patterns, network utilization |\n| Resource Profiling | Container resource usage, cluster utilization | System metrics every 30 seconds | Resource efficiency analysis, capacity planning, cost optimization |\n\nThe profiling system should automatically detect performance anomalies and generate actionable insights. For example, when database query latency increases significantly, the profiler should identify which specific queries are affected and suggest optimization strategies.\n\n**Debugging Dashboards and Visualization**\n\nEffective debugging requires visual representations of system behavior that help identify patterns, anomalies, and correlations across multiple metrics and timeframes. The debugging dashboard should provide both real-time monitoring and historical analysis capabilities.\n\n| Dashboard Section | Visualization Types | Data Sources | Interaction Features |\n|-------------------|-------------------|--------------|---------------------|\n| System Overview | Service topology, health status, traffic flow | Health checks, API metrics, trace data | Component drill-down, time range selection, alert correlation |\n| Component Deep Dive | Metric time series, error rates, resource usage | Component logs, performance metrics, profiling data | Metric correlation, anomaly detection, performance baseline comparison |\n| Operation Tracing | Request flow diagrams, timing waterfalls, dependency graphs | Distributed traces, span data, error propagation | Trace filtering, span inspection, error root cause analysis |\n| Trend Analysis | Historical patterns, capacity trends, performance degradation | Aggregated metrics, long-term storage, statistical analysis | Trend prediction, seasonal pattern detection, capacity planning alerts |\n\nThe dashboard implementation should support collaborative debugging by allowing users to save and share specific views, annotate incidents with debugging notes, and create custom alerts based on complex conditions.\n\n**Automated Incident Response**\n\nAutomated incident response reduces the mean time to recovery by implementing predefined procedures for common failure scenarios. The system should detect specific failure patterns and execute appropriate recovery actions without human intervention.\n\n| Incident Type | Detection Criteria | Automated Response | Escalation Conditions |\n|---------------|-------------------|-------------------|----------------------|\n| Database Connection Loss | Health check failures > 3 minutes | Restart database connection pool, switch to read replica | Connection restoration fails after 10 minutes |\n| Storage Space Exhaustion | Available space < 10% | Clean up temporary files, archive old artifacts | Space cannot be recovered to > 20% |\n| Memory Leak Detection | Memory usage growth > 50% in 1 hour | Restart affected service instances | Memory usage continues growing after restart |\n| Service Overload | Error rate > 10% for 5 minutes | Enable circuit breakers, scale up replicas | Error rate remains high after scaling |\n| Data Pipeline Failure | Processing lag > 2 hours | Restart pipeline jobs, switch to backup processing | Pipeline cannot catch up within 6 hours |\n\n### Performance Troubleshooting\n\nPerformance troubleshooting in MLOps platforms requires understanding how different components interact under load and identifying bottlenecks that may not be apparent during normal operation. Think of performance troubleshooting like optimizing a complex assembly line - you need to identify the slowest station, understand how delays propagate downstream, and optimize the entire flow rather than just individual components.\n\n**Experiment Tracking Performance Issues**\n\nExperiment tracking performance typically degrades due to high-frequency metric logging, large artifact uploads, or inefficient query patterns. The key insight is that most performance issues stem from treating the tracking system like a high-throughput streaming system rather than a structured data repository with different access patterns.\n\n| Performance Issue | Symptoms | Root Cause Analysis | Optimization Strategy |\n|-------------------|----------|-------------------|----------------------|\n| Metric logging latency spikes | `log_metric` calls taking > 5 seconds intermittently | Check database connection pool exhaustion, examine metric insertion batch sizes, analyze index usage patterns | Implement metric batching, increase connection pool size, add composite indexes on (run_id, step, timestamp) |\n| Artifact upload timeouts | `log_artifact` fails with network timeouts, incomplete uploads | Measure network bandwidth utilization, check object storage rate limits, examine concurrent upload patterns | Implement chunked upload with retry, add upload progress tracking, create artifact upload queuing |\n| Experiment comparison slow queries | `compare_runs` takes > 30 seconds for large experiments | Analyze query execution plan, check for full table scans, examine result set sizes | Add materialized views for common comparisons, implement query result caching, optimize metric aggregation queries |\n| Memory usage growth during long runs | Python process memory usage increases continuously | Profile memory allocation patterns, check for metric data accumulation, examine artifact reference retention | Implement periodic metric flushing, add memory usage monitoring, create garbage collection tuning |\n| Parameter logging bottlenecks | High parameter count runs causing storage delays | Check parameter serialization overhead, examine database transaction sizes, analyze concurrent run impact | Implement parameter compression, batch parameter storage operations, add parameter storage optimization |\n\nThe experiment tracking component should implement adaptive performance optimizations based on usage patterns. For example, when detecting high-frequency metric logging, automatically enable batching mode to reduce database transaction overhead.\n\n**Model Registry Performance Bottlenecks**\n\nModel registry performance issues typically manifest during model version queries, artifact downloads, or lineage graph construction. The challenge is balancing data consistency with query performance, especially when dealing with large model artifacts and complex lineage relationships.\n\n| Performance Bottleneck | Manifestation | Diagnostic Approach | Resolution Strategy |\n|------------------------|---------------|-------------------|-------------------|\n| Model search query slowness | `search_models` taking > 10 seconds with filters | Profile query execution against model metadata tables, examine index coverage, analyze filter selectivity | Create composite indexes on commonly filtered fields, implement search result pagination, add query optimization hints |\n| Large artifact download delays | Model artifact downloads failing or taking > 5 minutes | Measure network throughput to storage backend, check artifact compression effectiveness, examine concurrent download impact | Implement artifact compression, add download resumption capability, create content delivery network caching |\n| Lineage graph construction timeouts | `get_model_lineage` timing out for deep lineage chains | Analyze recursive query performance, check for circular references, examine graph traversal algorithm efficiency | Implement lineage graph caching, add graph depth limits, optimize recursive query structure |\n| Version creation transaction delays | `create_version` hanging during concurrent registrations | Check for database lock contention, examine transaction isolation levels, analyze version numbering conflicts | Implement optimistic locking, add version creation queuing, optimize database schema for concurrency |\n| Stage transition processing overhead | Model promotion operations taking > 2 minutes | Profile approval workflow execution, check for synchronous processing bottlenecks, examine notification delivery delays | Implement asynchronous stage transitions, add workflow step parallelization, optimize approval process structure |\n\nModel registry optimization should focus on separating metadata operations from artifact operations, allowing metadata queries to execute quickly while artifact operations happen asynchronously in the background.\n\n**Training Pipeline Orchestration Bottlenecks**\n\nPipeline orchestration performance issues typically occur during resource allocation, step scheduling, or data transfer between steps. The key challenge is optimizing resource utilization while maintaining step isolation and fault tolerance.\n\n| Orchestration Bottleneck | Symptoms | Analysis Methodology | Optimization Approach |\n|---------------------------|----------|---------------------|----------------------|\n| Step scheduling delays | Pipeline steps remaining in PENDING status for > 10 minutes | Examine Kubernetes cluster resource availability, check job queue depth, analyze resource request patterns | Implement resource pre-allocation, add cluster autoscaling, optimize resource request sizing |\n| Data transfer between steps | Inter-step data passing taking > 30 minutes | Profile network bandwidth usage, check storage backend performance, examine data serialization overhead | Implement data streaming between steps, add data compression, optimize storage backend configuration |\n| Resource allocation conflicts | Steps failing with \"resource exhausted\" errors | Monitor cluster resource utilization patterns, check resource quota enforcement, analyze concurrent job impact | Implement resource reservation system, add priority-based scheduling, create resource utilization forecasting |\n| Pipeline execution parallelization limits | Independent steps executing sequentially instead of parallel | Analyze DAG execution scheduling, check for artificial dependencies, examine resource allocation constraints | Optimize DAG execution algorithm, remove unnecessary dependencies, implement gang scheduling for distributed training |\n| Checkpoint and recovery overhead | Pipeline restart taking > 1 hour from checkpoints | Profile checkpoint size and complexity, check recovery process efficiency, examine state reconstruction time | Implement incremental checkpointing, add parallel recovery processing, optimize checkpoint data structure |\n\nPipeline performance optimization should focus on resource utilization efficiency and minimizing data movement overhead. The goal is to keep compute resources busy with actual training work rather than waiting for scheduling or data transfer operations.\n\n**Model Deployment Performance Issues**\n\nModel deployment performance issues typically involve inference latency, scaling responsiveness, or traffic routing overhead. The challenge is maintaining low latency and high throughput while supporting advanced deployment patterns like canary releases and A/B testing.\n\n| Deployment Performance Issue | Observable Symptoms | Investigation Steps | Performance Tuning |\n|-----------------------------|-------------------|-------------------|-------------------|\n| Inference latency degradation | P99 latency > 500ms for simple models | Profile model loading and inference pipeline, check for resource contention, examine batching effectiveness | Implement model warming procedures, optimize inference batching, add GPU utilization monitoring |\n| Auto-scaling responsiveness delays | Replica scaling taking > 5 minutes to respond to load | Analyze scaling metric collection frequency, check scaling decision logic, examine resource allocation timing | Implement predictive scaling, optimize scaling metric calculation, add scaling decision pre-warming |\n| Traffic routing overhead | Load balancer adding > 50ms latency | Profile routing rule evaluation, check for routing table size impact, examine health check frequency | Optimize routing rule structure, implement routing table caching, add health check result batching |\n| Model serving cold starts | New replica startup taking > 2 minutes | Profile container startup time, check model loading process, examine dependency initialization overhead | Implement container image optimization, add model preloading, create replica warm pool |\n| Canary deployment traffic splitting accuracy | Traffic not splitting according to configured percentages | Analyze load balancer session affinity, check routing algorithm implementation, examine request distribution patterns | Implement consistent hashing for traffic distribution, add traffic split monitoring, optimize routing algorithm |\n\nModel deployment optimization should prioritize inference latency while ensuring scaling operations don't disrupt ongoing request processing. The key is implementing proper request queuing and load balancing during scaling transitions.\n\n**Model Monitoring Performance Challenges**\n\nModel monitoring performance issues typically involve prediction logging throughput, drift detection processing time, or real-time metrics calculation overhead. The challenge is processing high-volume prediction streams while maintaining low-latency alerting for critical issues.\n\n| Monitoring Performance Challenge | Impact on System | Diagnosis Procedure | Performance Enhancement |\n|---------------------------------|------------------|-------------------|------------------------|\n| Prediction logging backlog | Predictions not appearing in monitoring dashboard for > 15 minutes | Check prediction log processing queue depth, examine batch processing efficiency, analyze storage write performance | Implement prediction log batching, add parallel processing pipelines, optimize storage backend for high throughput |\n| Drift detection computation delays | Drift analysis results delayed by > 1 hour | Profile statistical calculation performance, check for algorithm optimization opportunities, examine data preprocessing overhead | Implement incremental drift calculation, add computation result caching, optimize statistical algorithm implementation |\n| Real-time metrics aggregation overhead | Monitoring dashboard updates delayed by > 5 minutes | Analyze metric aggregation query performance, check for computation bottlenecks, examine data freshness requirements | Implement streaming metric aggregation, add pre-computed metric materialization, optimize aggregation time windows |\n| Alert evaluation processing delays | Critical alerts delayed by > 2 minutes | Profile alert rule evaluation performance, check for rule complexity overhead, examine notification delivery bottlenecks | Implement alert rule optimization, add alert evaluation parallelization, optimize notification delivery batching |\n| Monitoring data retention overhead | Historical data queries taking > 30 seconds | Analyze data storage schema efficiency, check for query optimization opportunities, examine data archival effectiveness | Implement data partitioning strategy, add query result caching, optimize data retention policies |\n\nModel monitoring optimization should focus on stream processing efficiency and ensuring critical alerts have priority over historical analysis queries. The system should maintain real-time responsiveness for active monitoring while handling batch processing for historical analysis.\n\n**Cross-Component Performance Integration**\n\nPerformance issues often span multiple components, requiring system-wide optimization rather than individual component tuning. Understanding these interactions is crucial for achieving overall platform performance goals.\n\n| Integration Performance Issue | Cross-Component Impact | Analysis Strategy | System-Wide Optimization |\n|------------------------------|----------------------|------------------|-------------------------|\n| Event propagation delays | Operations completing in one component not reflected in others for > 5 minutes | Trace event flow across components, check message queue performance, examine event processing bottlenecks | Implement event prioritization, add event processing parallelization, optimize message serialization |\n| API call cascading delays | Single user operation triggering multiple slow API calls | Profile API call chains, check for synchronous vs asynchronous processing, examine timeout and retry behavior | Implement asynchronous operation processing, add API call result caching, optimize inter-component protocols |\n| Database connection pool exhaustion | Multiple components competing for limited database connections | Monitor connection pool utilization across components, check for connection leak patterns, examine transaction duration distribution | Implement connection pool sharing, add connection usage monitoring, optimize database transaction boundaries |\n| Storage backend overload | Concurrent operations from multiple components overwhelming storage systems | Analyze storage operation patterns, check for hot spot identification, examine load distribution effectiveness | Implement storage operation queuing, add storage backend load balancing, optimize data access patterns |\n| Resource contention during peak loads | System performance degrading when multiple components under high load simultaneously | Profile system resource usage patterns, check for resource allocation conflicts, examine performance isolation effectiveness | Implement resource allocation prioritization, add component performance isolation, optimize resource sharing policies |\n\n### Implementation Guidance\n\nThis implementation guidance provides practical tools and code frameworks for building effective debugging capabilities into your MLOps platform. The focus is on creating debugging infrastructure that integrates seamlessly with your components while providing comprehensive visibility into system behavior.\n\n**Technology Recommendations**\n\n| Debugging Capability | Simple Option | Advanced Option | \n|----------------------|---------------|-----------------|\n| Structured Logging | Python logging module with JSON formatter | Structured logging with OpenTelemetry and log correlation |\n| Health Checks | Simple HTTP endpoints returning status | Comprehensive health check framework with dependency validation |\n| Distributed Tracing | Manual correlation ID propagation | OpenTelemetry distributed tracing with Jaeger backend |\n| Performance Profiling | Python cProfile with custom analysis | Continuous profiling with Pyroscope or similar tools |\n| Metrics Collection | Custom metrics with Prometheus client | Full observability stack with Grafana, Prometheus, and AlertManager |\n| Error Tracking | Log aggregation with ELK stack | Dedicated error tracking with Sentry or Rollbar integration |\n\n**Recommended File Structure**\n\n```\nproject-root/\n  mlops_platform/\n    debugging/\n      __init__.py\n      health_checks.py          ← Component health validation\n      structured_logging.py     ← Correlation ID logging framework  \n      performance_profiler.py   ← Performance monitoring utilities\n      tracing.py               ← Distributed tracing implementation\n      incident_response.py      ← Automated recovery procedures\n      debugging_dashboard.py    ← Debugging visualization endpoints\n    components/\n      experiment_tracker/\n        health_checks.py        ← Component-specific health validation\n        performance_monitor.py   ← Component performance monitoring\n      model_registry/\n        health_checks.py\n        performance_monitor.py\n      # ... other components\n    tests/\n      debugging/\n        test_health_checks.py\n        test_incident_response.py\n```\n\n**Infrastructure Starter Code**\n\nHere's complete, production-ready infrastructure for debugging capabilities:\n\n```python\n# mlops_platform/debugging/structured_logging.py\nimport json\nimport logging\nimport time\nimport uuid\nfrom typing import Any, Dict, Optional\nfrom contextvars import ContextVar\n\n# Context variable for correlation ID propagation\ncorrelation_id_var: ContextVar[Optional[str]] = ContextVar('correlation_id', default=None)\n\nclass CorrelationIDFilter(logging.Filter):\n    \"\"\"Add correlation ID to log records.\"\"\"\n    \n    def filter(self, record):\n        record.correlation_id = correlation_id_var.get() or \"unknown\"\n        return True\n\nclass StructuredFormatter(logging.Formatter):\n    \"\"\"Format logs as structured JSON with consistent fields.\"\"\"\n    \n    def format(self, record):\n        log_entry = {\n            \"timestamp\": record.created,\n            \"level\": record.levelname,\n            \"component\": getattr(record, 'component', 'unknown'),\n            \"correlation_id\": getattr(record, 'correlation_id', 'unknown'),\n            \"operation\": getattr(record, 'operation', 'unknown'),\n            \"message\": record.getMessage(),\n        }\n        \n        if hasattr(record, 'context'):\n            log_entry[\"context\"] = record.context\n            \n        if record.exc_info:\n            log_entry[\"stack_trace\"] = self.formatException(record.exc_info)\n            \n        return json.dumps(log_entry)\n\ndef setup_structured_logging(component_name: str, log_level: str = \"INFO\"):\n    \"\"\"Configure structured logging for a component.\"\"\"\n    logger = logging.getLogger(component_name)\n    logger.setLevel(getattr(logging, log_level))\n    \n    # Remove existing handlers\n    for handler in logger.handlers[:]:\n        logger.removeHandler(handler)\n    \n    # Add structured handler\n    handler = logging.StreamHandler()\n    handler.setFormatter(StructuredFormatter())\n    handler.addFilter(CorrelationIDFilter())\n    logger.addHandler(handler)\n    \n    return logger\n\ndef set_correlation_id(correlation_id: str):\n    \"\"\"Set correlation ID for current context.\"\"\"\n    correlation_id_var.set(correlation_id)\n\ndef generate_correlation_id(prefix: str = \"mlops\") -> str:\n    \"\"\"Generate unique correlation ID.\"\"\"\n    timestamp = int(time.time())\n    unique_id = str(uuid.uuid4())[:8]\n    return f\"{prefix}_{timestamp}_{unique_id}\"\n\nclass OperationLogger:\n    \"\"\"Context manager for logging operations with correlation.\"\"\"\n    \n    def __init__(self, logger: logging.Logger, operation: str, **context):\n        self.logger = logger\n        self.operation = operation\n        self.context = context\n        self.start_time = None\n        \n    def __enter__(self):\n        self.start_time = time.time()\n        self.logger.info(\n            f\"Starting {self.operation}\",\n            extra={\"operation\": self.operation, \"context\": self.context}\n        )\n        return self\n        \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        duration = time.time() - self.start_time\n        self.context[\"duration_seconds\"] = round(duration, 3)\n        \n        if exc_type:\n            self.logger.error(\n                f\"Failed {self.operation}: {str(exc_val)}\",\n                extra={\"operation\": self.operation, \"context\": self.context},\n                exc_info=(exc_type, exc_val, exc_tb)\n            )\n        else:\n            self.logger.info(\n                f\"Completed {self.operation}\",\n                extra={\"operation\": self.operation, \"context\": self.context}\n            )\n```\n\n```python\n# mlops_platform/debugging/health_checks.py\nimport asyncio\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional\n\nclass HealthStatus(Enum):\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\" \n    UNHEALTHY = \"unhealthy\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass HealthCheck:\n    name: str\n    status: HealthStatus\n    message: str\n    timestamp: float\n    details: Dict[str, Any]\n\nclass HealthCheckFunction:\n    \"\"\"Wrapper for health check functions with metadata.\"\"\"\n    \n    def __init__(self, name: str, check_func: Callable[[], HealthCheck], \n                 timeout_seconds: int = 30, critical: bool = True):\n        self.name = name\n        self.check_func = check_func\n        self.timeout_seconds = timeout_seconds\n        self.critical = critical\n\nclass ComponentHealth:\n    \"\"\"Manages health checks for a component.\"\"\"\n    \n    def __init__(self, component_name: str):\n        self.component_name = component_name\n        self.checks: Dict[str, HealthCheckFunction] = {}\n        self.last_results: Dict[str, HealthCheck] = {}\n        \n    def add_check(self, check_name: str, check_func: Callable[[], HealthCheck],\n                  timeout_seconds: int = 30, critical: bool = True):\n        \"\"\"Register a health check function.\"\"\"\n        self.checks[check_name] = HealthCheckFunction(\n            check_name, check_func, timeout_seconds, critical\n        )\n        \n    async def run_checks(self) -> List[HealthCheck]:\n        \"\"\"Execute all health checks and return results.\"\"\"\n        results = []\n        \n        for check_name, check_func in self.checks.items():\n            try:\n                # Run check with timeout\n                result = await asyncio.wait_for(\n                    asyncio.get_event_loop().run_in_executor(\n                        None, check_func.check_func\n                    ),\n                    timeout=check_func.timeout_seconds\n                )\n                results.append(result)\n                self.last_results[check_name] = result\n                \n            except asyncio.TimeoutError:\n                result = HealthCheck(\n                    name=check_name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=f\"Health check timed out after {check_func.timeout_seconds}s\",\n                    timestamp=time.time(),\n                    details={\"timeout_seconds\": check_func.timeout_seconds}\n                )\n                results.append(result)\n                self.last_results[check_name] = result\n                \n            except Exception as e:\n                result = HealthCheck(\n                    name=check_name,\n                    status=HealthStatus.UNHEALTHY,\n                    message=f\"Health check failed: {str(e)}\",\n                    timestamp=time.time(),\n                    details={\"error\": str(e), \"error_type\": type(e).__name__}\n                )\n                results.append(result)\n                self.last_results[check_name] = result\n                \n        return results\n    \n    def get_overall_status(self) -> HealthStatus:\n        \"\"\"Determine overall component health status.\"\"\"\n        if not self.last_results:\n            return HealthStatus.UNKNOWN\n            \n        critical_checks = [\n            result for name, result in self.last_results.items()\n            if self.checks[name].critical\n        ]\n        \n        if any(check.status == HealthStatus.UNHEALTHY for check in critical_checks):\n            return HealthStatus.UNHEALTHY\n        elif any(check.status == HealthStatus.DEGRADED for check in critical_checks):\n            return HealthStatus.DEGRADED\n        else:\n            return HealthStatus.HEALTHY\n\n# Example health check implementations\ndef create_database_health_check(db_connection):\n    \"\"\"Create health check for database connectivity.\"\"\"\n    \n    def check_database():\n        try:\n            start_time = time.time()\n            # Execute simple query to verify connectivity\n            cursor = db_connection.cursor()\n            cursor.execute(\"SELECT 1\")\n            result = cursor.fetchone()\n            response_time = time.time() - start_time\n            \n            if result and response_time < 5.0:\n                return HealthCheck(\n                    name=\"database_connectivity\",\n                    status=HealthStatus.HEALTHY,\n                    message=\"Database connection healthy\",\n                    timestamp=time.time(),\n                    details={\"response_time_seconds\": round(response_time, 3)}\n                )\n            else:\n                return HealthCheck(\n                    name=\"database_connectivity\", \n                    status=HealthStatus.DEGRADED,\n                    message=\"Database response slow\",\n                    timestamp=time.time(),\n                    details={\"response_time_seconds\": round(response_time, 3)}\n                )\n        except Exception as e:\n            return HealthCheck(\n                name=\"database_connectivity\",\n                status=HealthStatus.UNHEALTHY,\n                message=f\"Database connection failed: {str(e)}\",\n                timestamp=time.time(),\n                details={\"error\": str(e)}\n            )\n            \n    return check_database\n\ndef create_storage_health_check(storage_client):\n    \"\"\"Create health check for object storage connectivity.\"\"\"\n    \n    def check_storage():\n        try:\n            start_time = time.time()\n            # Test storage operations\n            test_key = f\"health_check_{int(time.time())}\"\n            test_data = b\"health_check_data\"\n            \n            # Test write operation\n            storage_client.put(test_key, test_data)\n            \n            # Test read operation\n            retrieved_data = storage_client.get(test_key)\n            \n            # Clean up test data\n            storage_client.delete(test_key)\n            \n            response_time = time.time() - start_time\n            \n            if retrieved_data == test_data and response_time < 10.0:\n                return HealthCheck(\n                    name=\"storage_connectivity\",\n                    status=HealthStatus.HEALTHY,\n                    message=\"Storage operations healthy\",\n                    timestamp=time.time(),\n                    details={\"response_time_seconds\": round(response_time, 3)}\n                )\n            else:\n                return HealthCheck(\n                    name=\"storage_connectivity\",\n                    status=HealthStatus.DEGRADED,\n                    message=\"Storage operations slow or data mismatch\",\n                    timestamp=time.time(),\n                    details={\"response_time_seconds\": round(response_time, 3)}\n                )\n        except Exception as e:\n            return HealthCheck(\n                name=\"storage_connectivity\",\n                status=HealthStatus.UNHEALTHY,\n                message=f\"Storage operations failed: {str(e)}\",\n                timestamp=time.time(),\n                details={\"error\": str(e)}\n            )\n            \n    return check_storage\n```\n\n**Core Logic Skeleton Code**\n\nHere are the skeleton implementations for the main debugging components:\n\n```python\n# mlops_platform/debugging/incident_response.py\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\nimport time\n\nclass RecoveryResult(Enum):\n    SUCCESS = \"success\"\n    PARTIAL = \"partial\"\n    FAILED = \"failed\"\n    MANUAL_REQUIRED = \"manual_required\"\n\nclass RecoveryProcedure(ABC):\n    \"\"\"Base class for automated recovery procedures.\"\"\"\n    \n    @abstractmethod\n    def can_handle(self, failure_type: str, context: Dict[str, Any]) -> bool:\n        \"\"\"Check if this procedure can handle the failure type.\"\"\"\n        pass\n    \n    @abstractmethod\n    def execute_recovery(self, failure_type: str, context: Dict[str, Any]) -> RecoveryResult:\n        \"\"\"Execute the recovery procedure.\"\"\"\n        pass\n\nclass AutomatedRecovery:\n    \"\"\"Framework for automated failure recovery procedures.\"\"\"\n    \n    def __init__(self):\n        self.procedures: List[RecoveryProcedure] = []\n        self.recovery_history: List[Dict[str, Any]] = []\n    \n    def register_procedure(self, procedure: RecoveryProcedure):\n        \"\"\"Register a recovery procedure.\"\"\"\n        # TODO: Add procedure to the procedures list\n        # TODO: Validate procedure implements required methods\n        # TODO: Log procedure registration for debugging\n        pass\n    \n    def attempt_recovery(self, failure_type: str, context: Dict[str, Any]) -> RecoveryResult:\n        \"\"\"Attempt automated recovery for detected failure.\"\"\"\n        # TODO: Find procedures that can handle this failure type using can_handle()\n        # TODO: Execute procedures in priority order (implement priority system)\n        # TODO: Record recovery attempt in history with timestamp and context\n        # TODO: Return SUCCESS if any procedure succeeds, FAILED if all fail\n        # TODO: Return MANUAL_REQUIRED for complex failures requiring human intervention\n        pass\n\nclass DatabaseRecoveryProcedure(RecoveryProcedure):\n    \"\"\"Recovery procedure for database connection issues.\"\"\"\n    \n    def can_handle(self, failure_type: str, context: Dict[str, Any]) -> bool:\n        # TODO: Return True for database-related failure types\n        # TODO: Check context contains required database connection information\n        pass\n    \n    def execute_recovery(self, failure_type: str, context: Dict[str, Any]) -> RecoveryResult:\n        \"\"\"Execute database recovery steps.\"\"\"\n        # TODO: Attempt to recreate database connection pool\n        # TODO: Test connection with simple query\n        # TODO: If primary database fails, attempt connection to read replica\n        # TODO: Return SUCCESS if connection restored, FAILED otherwise\n        # TODO: Log all recovery steps for debugging\n        pass\n```\n\n```python\n# mlops_platform/debugging/performance_profiler.py\nimport time\nimport psutil\nimport threading\nfrom collections import defaultdict, deque\nfrom typing import Any, Dict, List, Optional\nimport json\n\nclass PerformanceMetrics:\n    \"\"\"Container for performance measurement data.\"\"\"\n    \n    def __init__(self):\n        self.operation_times: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))\n        self.resource_usage: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))\n        self.error_counts: Dict[str, int] = defaultdict(int)\n        self.lock = threading.Lock()\n\n    def record_operation_time(self, operation: str, duration_seconds: float):\n        \"\"\"Record operation execution time.\"\"\"\n        # TODO: Add duration to operation_times[operation] deque with thread safety\n        # TODO: Update operation statistics (avg, p95, p99 percentiles)\n        # TODO: Detect performance anomalies (duration > 3x average)\n        # TODO: Log performance warnings for slow operations\n        pass\n\n    def record_resource_usage(self, cpu_percent: float, memory_mb: float, disk_io_mb: float):\n        \"\"\"Record system resource utilization.\"\"\"\n        # TODO: Add resource measurements to appropriate deques with timestamps\n        # TODO: Calculate resource usage trends over time windows\n        # TODO: Detect resource exhaustion conditions (CPU > 90%, memory > 85%)\n        # TODO: Generate resource utilization alerts when thresholds exceeded\n        pass\n\nclass PerformanceProfiler:\n    \"\"\"Comprehensive performance monitoring and profiling.\"\"\"\n    \n    def __init__(self, component_name: str):\n        self.component_name = component_name\n        self.metrics = PerformanceMetrics()\n        self.monitoring_active = False\n        self.monitoring_thread = None\n\n    def start_monitoring(self, interval_seconds: int = 30):\n        \"\"\"Start continuous performance monitoring.\"\"\"\n        # TODO: Set monitoring_active to True and create monitoring thread\n        # TODO: In monitoring loop, collect CPU, memory, disk I/O every interval_seconds\n        # TODO: Use psutil to gather system resource information\n        # TODO: Record metrics using record_resource_usage method\n        # TODO: Handle thread lifecycle and graceful shutdown\n        pass\n\n    def profile_operation(self, operation_name: str):\n        \"\"\"Context manager for profiling individual operations.\"\"\"\n        # TODO: Return context manager that measures operation execution time\n        # TODO: Record start time on enter, calculate duration on exit\n        # TODO: Handle exceptions during profiled operations\n        # TODO: Record operation metrics including success/failure status\n        # TODO: Add operation context information (parameters, result size, etc.)\n        pass\n\n    def generate_performance_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive performance analysis report.\"\"\"\n        # TODO: Calculate operation statistics (min, max, avg, percentiles)\n        # TODO: Analyze resource usage patterns and trends\n        # TODO: Identify performance bottlenecks and anomalies\n        # TODO: Generate actionable performance optimization recommendations\n        # TODO: Format report as structured data for dashboard consumption\n        pass\n```\n\n**Milestone Checkpoints**\n\nAfter implementing debugging infrastructure for each milestone:\n\n**Milestone 1 (Experiment Tracking) - Debugging Verification:**\n```bash\n# Test structured logging\npython -c \"\nfrom mlops_platform.debugging.structured_logging import setup_structured_logging, set_correlation_id\nlogger = setup_structured_logging('test-component')\nset_correlation_id('test-123')\nlogger.info('Test message', extra={'operation': 'test', 'context': {'key': 'value'}})\n\"\n# Expected: JSON log entry with correlation_id, timestamp, and structured fields\n\n# Test health checks\npython -c \"\nfrom mlops_platform.debugging.health_checks import ComponentHealth\nimport asyncio\nhealth = ComponentHealth('experiment-tracker')\nhealth.add_check('test', lambda: HealthCheck('test', HealthStatus.HEALTHY, 'OK', time.time(), {}))\nresults = asyncio.run(health.run_checks())\nprint([r.status.value for r in results])\n\"\n# Expected: ['healthy']\n```\n\n**Load Testing Checkpoint:**\n```python\n# Test experiment tracking under load\nimport concurrent.futures\nimport time\n\ndef load_test_experiment_tracking():\n    # TODO: Create multiple concurrent experiment runs\n    # TODO: Log parameters and metrics at high frequency\n    # TODO: Measure logging latency and system resource usage\n    # TODO: Verify all operations complete successfully under load\n    # TODO: Check for memory leaks or resource exhaustion\n    pass\n\n# Expected: All operations complete within latency targets, no resource exhaustion\n```\n\n**Debugging Tips for Common Issues:**\n\n| Symptom | Likely Cause | Diagnosis Command | Fix |\n|---------|--------------|------------------|-----|\n| Logs missing correlation IDs | Context variable not propagated across async boundaries | `grep \"correlation_id.*unknown\" /var/log/mlops.log` | Ensure `set_correlation_id()` called before async operations |\n| Health checks timing out | Database/storage connectivity issues | `python -m mlops_platform.debugging.health_checks --component experiment-tracker` | Check network connectivity, increase timeout values |\n| Performance metrics missing | Monitoring thread not started | `ps aux \\| grep performance_monitor` | Call `start_monitoring()` during component initialization |\n| Recovery procedures not executing | Failure detection not triggering procedures | `tail -f /var/log/mlops.log \\| grep \"recovery_attempt\"` | Verify failure detection thresholds and procedure registration |\n\nThe debugging infrastructure should integrate seamlessly with your MLOps components, providing comprehensive visibility into system behavior while maintaining performance under production loads.\n\n\n## Future Extensions\n\n> **Milestone(s):** This section applies to all milestones (1-5) by identifying potential enhancements and scale-out scenarios that the current architecture supports. Understanding these extensions helps validate the architectural decisions and guides future development planning.\n\nThe MLOps platform architecture we've designed provides a solid foundation for enterprise machine learning operations, but the ML landscape continues to evolve rapidly. This section explores how the platform can be extended to support advanced MLOps features, scale to enterprise requirements, and integrate with the broader ML ecosystem. Think of this as a **growth roadmap** — just as a well-designed building can support additional floors and renovations, our modular architecture can accommodate new capabilities without fundamental restructuring.\n\nEach extension leverages the existing architectural patterns: the event-driven coordination system enables loose coupling between new and existing components, the polyglot persistence approach allows optimal data stores for new use cases, and the hexagonal architecture ensures clean integration boundaries. Understanding these potential extensions validates our design decisions and provides guidance for prioritizing future development efforts.\n\n### Advanced MLOps Features\n\nThe platform's current feature set addresses core MLOps workflows, but production ML systems often require additional capabilities for data management, automated optimization, and multi-tenant operations. These advanced features build upon the existing components while introducing new architectural patterns and data flows.\n\n#### Feature Store Integration\n\nModern ML systems require consistent feature engineering and serving across training and inference workloads. A **feature store** acts like a **data warehouse specifically designed for ML features** — it provides a centralized repository for feature definitions, transformations, and both batch and real-time feature serving. Think of it as a **feature cafeteria** where data scientists can discover, reuse, and serve high-quality features without rebuilding the same transformations repeatedly.\n\nThe feature store extends our architecture by introducing new entities and data flows that integrate with existing experiment tracking and model serving components. Features become first-class citizens with their own versioning, lineage tracking, and monitoring capabilities.\n\n**Feature Store Entity Model:**\n\n| Entity | Fields | Purpose |\n|--------|--------|---------|\n| `FeatureGroup` | group_id str, name str, description str, source_table str, entity_key str, timestamp_key str, features List[FeatureDefinition], owner str, tags Dict[str, str] | Groups related features from same data source |\n| `FeatureDefinition` | feature_name str, data_type str, transformation str, validation_rules List[Rule], description str, creation_time float, last_update_time float | Defines individual feature with transformation logic |\n| `FeatureView` | view_id str, name str, feature_groups List[str], join_keys List[str], filters Dict[str, Any], ttl_hours int, description str | Defines logical view joining features for specific use case |\n| `FeatureValue` | feature_name str, entity_id str, timestamp float, value Any, feature_group_id str | Individual feature value for specific entity and time |\n| `FeatureLineage` | feature_name str, source_tables List[str], transformation_code str, dependencies List[str], created_by str | Tracks feature provenance and dependencies |\n\nThe feature store integrates with existing components through event-driven coordination. When a training pipeline requests features, the feature store publishes `FEATURES_REQUESTED` events that trigger batch feature computation. During model serving, real-time feature requests generate `FEATURE_SERVED` events that feed into monitoring for feature drift detection.\n\n**Feature Store API Integration:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `create_feature_group(name, source_config, features)` | name str, source_config Dict, features List[FeatureDefinition] | str | Register new feature group with data source |\n| `get_training_features(feature_view, entity_ids, timestamp_range)` | feature_view str, entity_ids List[str], timestamp_range Tuple[float, float] | DataFrame | Retrieve point-in-time correct features for training |\n| `get_online_features(feature_view, entity_ids)` | feature_view str, entity_ids List[str] | Dict[str, Any] | Retrieve latest feature values for inference |\n| `compute_feature_statistics(feature_group, time_range)` | feature_group str, time_range Tuple[float, float] | Dict[str, Any] | Compute feature distribution statistics |\n| `validate_feature_schema(feature_group, data)` | feature_group str, data Any | List[ValidationError] | Validate data against feature schema |\n\n> **Design Insight**: The feature store leverages the same event-driven architecture and polyglot persistence patterns as core components. Feature computation triggers use the pipeline orchestration engine, while online feature serving uses low-latency key-value stores. This architectural consistency simplifies operation and reduces cognitive load for developers.\n\n**Feature Store Architecture Decision:**\n\n> **Decision: Hybrid Online/Offline Feature Architecture**\n> - **Context**: ML systems need both batch features for training and real-time features for serving, with consistency requirements between the two modes\n> - **Options Considered**: \n>   - Separate online and offline stores with manual synchronization\n>   - Single unified store serving both batch and online workloads\n>   - Hybrid architecture with dual writes and consistency validation\n> - **Decision**: Implement hybrid architecture with materialization pipelines keeping online and offline stores synchronized\n> - **Rationale**: Provides optimal performance for each use case while maintaining consistency through automated synchronization and drift detection\n> - **Consequences**: Requires additional infrastructure complexity but ensures feature consistency and enables point-in-time correctness\n\n#### Automated Model Selection and Hyperparameter Optimization\n\nAdvanced ML teams often train hundreds of model variants to find optimal configurations. **Automated Machine Learning (AutoML)** capabilities transform the platform from a passive tracking system into an **active optimization engine** that systematically explores the model space. Think of this as evolving from a **manual laboratory** where scientists conduct individual experiments to an **automated research facility** that runs systematic optimization protocols.\n\nAutoML extends the experiment tracking and training pipeline components by introducing optimization algorithms, search space definitions, and automated resource allocation. The system becomes capable of generating experiment configurations, launching training runs, and converging on optimal model architectures.\n\n**AutoML Entity Extensions:**\n\n| Entity | Fields | Purpose |\n|--------|--------|---------|\n| `SearchSpace` | search_id str, parameter_ranges Dict[str, ParameterRange], constraints List[Constraint], optimization_metric str, direction str | Defines hyperparameter search boundaries |\n| `ParameterRange` | name str, type str, min_value Optional[float], max_value Optional[float], choices Optional[List[Any]], distribution str | Individual parameter search range |\n| `OptimizationRun` | optimization_id str, search_space_id str, algorithm str, budget_hours float, best_score float, completed_trials int, status OptimizationStatus | Tracks overall optimization process |\n| `Trial` | trial_id str, optimization_id str, parameters Dict[str, Any], score Optional[float], status TrialStatus, start_time float, end_time Optional[float] | Individual model training attempt |\n| `OptimizationStatus` | enum: RUNNING, COMPLETED, FAILED, STOPPED | Status of optimization run |\n\nThe automated optimization system integrates with existing pipeline orchestration by generating `Pipeline` definitions dynamically based on optimization algorithm recommendations. Each trial becomes a standard training pipeline execution with additional metadata linking it to the optimization run.\n\n**AutoML Integration APIs:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `create_search_space(name, parameter_ranges, constraints)` | name str, parameter_ranges Dict, constraints List | str | Define hyperparameter search space |\n| `start_optimization(search_space_id, algorithm, budget)` | search_space_id str, algorithm str, budget ResourceBudget | str | Launch automated optimization run |\n| `suggest_trial_parameters(optimization_id)` | optimization_id str | Dict[str, Any] | Get next parameter configuration to try |\n| `report_trial_result(trial_id, score, metadata)` | trial_id str, score float, metadata Dict | bool | Report trial completion and performance |\n| `get_optimization_status(optimization_id)` | optimization_id str | OptimizationRun | Check optimization progress and best results |\n\nThe optimization algorithms leverage Bayesian optimization, evolutionary strategies, or neural architecture search depending on the problem characteristics. The system maintains a **surrogate model** of the parameter-performance relationship that guides exploration toward promising regions of the search space.\n\n> **Design Insight**: AutoML capabilities transform the platform from reactive to proactive. Instead of just tracking what users do, the system actively suggests improvements and automates tedious hyperparameter tuning. This shift requires careful resource management to prevent optimization runs from consuming all cluster capacity.\n\n#### Multi-Tenant Support and Resource Isolation\n\nEnterprise MLOps platforms serve multiple teams with varying security, compliance, and resource requirements. **Multi-tenancy** transforms the platform from a single-team tool into a **shared service bureau** that provides isolated environments while maximizing resource utilization. Think of this evolution like moving from a **private workshop** to a **co-working space** with private offices, shared facilities, and usage-based billing.\n\nMulti-tenancy introduces hierarchical resource management, fine-grained access controls, and tenant-specific configuration while maintaining the unified operational model that makes the platform valuable.\n\n**Multi-Tenancy Entity Model:**\n\n| Entity | Fields | Purpose |\n|--------|--------|---------|\n| `Tenant` | tenant_id str, name str, subscription_tier str, resource_quotas Dict[str, float], billing_account str, created_at float, status TenantStatus | Top-level tenant organization |\n| `Workspace` | workspace_id str, tenant_id str, name str, description str, members List[WorkspaceMember], resource_allocation Dict[str, float] | Project workspace within tenant |\n| `WorkspaceMember` | user_id str, workspace_id str, role WorkspaceRole, permissions List[Permission], added_at float | User access to workspace |\n| `ResourceQuota` | quota_id str, tenant_id str, resource_type str, limit_value float, current_usage float, enforcement_policy str | Resource usage limits and tracking |\n| `TenantStatus` | enum: ACTIVE, SUSPENDED, TRIAL, DEACTIVATED | Tenant account status |\n| `WorkspaceRole` | enum: OWNER, ADMIN, CONTRIBUTOR, VIEWER | User role in workspace |\n\nMulti-tenant isolation operates at multiple architectural layers. Data isolation ensures tenants cannot access each other's experiments, models, or pipelines. Resource isolation prevents one tenant from monopolizing compute resources. Network isolation restricts inter-tenant communication in shared infrastructure.\n\n**Multi-Tenancy Access Control:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `create_tenant(name, subscription_config, quotas)` | name str, subscription_config Dict, quotas Dict[str, float] | str | Create new tenant with resource limits |\n| `create_workspace(tenant_id, name, initial_members)` | tenant_id str, name str, initial_members List[WorkspaceMember] | str | Create workspace within tenant |\n| `check_access(user_id, resource_type, resource_id, action)` | user_id str, resource_type str, resource_id str, action str | bool | Verify user permissions for resource action |\n| `allocate_resources(workspace_id, resource_request)` | workspace_id str, resource_request ResourceSpec | ResourceAllocation | Reserve resources with quota enforcement |\n| `track_usage(tenant_id, resource_type, usage_amount)` | tenant_id str, resource_type str, usage_amount float | bool | Record resource consumption for billing |\n\nThe multi-tenant architecture introduces **hierarchical namespacing** where all resources include tenant and workspace identifiers in their paths. For example, model artifacts are stored as `tenants/{tenant_id}/workspaces/{workspace_id}/models/{model_name}/versions/{version}/artifacts/` ensuring complete isolation while maintaining the familiar model registry interface.\n\n> **Architecture Decision: Shared Infrastructure with Logical Isolation**\n> - **Context**: Need to support multiple tenants while controlling infrastructure costs and operational complexity\n> - **Options Considered**:\n>   - Physical isolation with dedicated infrastructure per tenant\n>   - Logical isolation with shared infrastructure and access controls\n>   - Hybrid approach with dedicated resources for sensitive workloads\n> - **Decision**: Implement logical isolation with namespace-based separation and resource quotas\n> - **Rationale**: Maximizes resource utilization while providing adequate security through access controls and audit logging\n> - **Consequences**: Requires sophisticated access control implementation but enables cost-effective multi-tenancy with flexibility for dedicated resources when needed\n\n**Common Pitfalls in Advanced Features:**\n\n⚠️ **Pitfall: Feature Store Data Consistency**\nAdding a feature store without proper consistency guarantees can lead to training-serving skew where models see different feature values during training versus inference. This happens when online and offline feature stores drift apart due to failed synchronization or timing differences. Prevent this by implementing feature store drift monitoring that compares online and offline feature distributions, and use feature lineage tracking to ensure the same transformation code runs in both batch and streaming contexts.\n\n⚠️ **Pitfall: AutoML Resource Exhaustion**\nAutomated hyperparameter optimization can consume unlimited cluster resources if not properly constrained, starving other users of compute capacity. This occurs when optimization algorithms launch too many parallel trials or don't respect resource quotas. Prevent this by implementing dynamic resource budgets that adjust based on cluster utilization, setting maximum parallel trial limits per optimization run, and integrating with the multi-tenant resource quota system.\n\n⚠️ **Pitfall: Multi-Tenant Data Leakage**\nImproper tenant isolation can leak sensitive data between organizations through shared caches, logging systems, or artifact storage. This happens when tenant identifiers aren't properly validated at every access point or when aggregated metrics inadvertently reveal tenant-specific information. Prevent this by implementing defense-in-depth with tenant validation at API boundaries, database row-level security, and regular security audits of cross-tenant data flows.\n\n### Scale and Performance Extensions\n\nAs ML teams grow and model complexity increases, the platform must scale beyond single-datacenter deployments to support global operations, edge computing, and massive training workloads. These extensions stress-test the architectural decisions and often require fundamental changes to data distribution and coordination patterns.\n\n#### Multi-Region Deployments and Global Model Serving\n\nGlobal organizations need ML models deployed close to users for optimal latency while maintaining consistency across regions. **Multi-region deployments** transform the platform from a **centralized service** to a **distributed federation** of regional clusters with sophisticated coordination mechanisms. Think of this like evolving from a **single headquarters** to a **multinational corporation** with regional offices that operate independently while maintaining global coordination.\n\nMulti-region architecture introduces challenges around data replication, eventual consistency, conflict resolution, and cross-region network partitions that don't exist in single-region deployments.\n\n**Multi-Region Architecture Components:**\n\n| Component | Regional Scope | Global Scope | Coordination Mechanism |\n|-----------|----------------|--------------|-------------------------|\n| Model Registry | Regional cache with async sync | Global authoritative store | Event-driven replication with conflict resolution |\n| Experiment Tracking | Regional storage | Global aggregation | Eventual consistency with merge strategies |\n| Pipeline Orchestration | Regional execution clusters | Global scheduling coordination | Leader election with regional failover |\n| Model Serving | Regional endpoints | Global traffic routing | DNS-based routing with health monitoring |\n| Monitoring | Regional data collection | Global alerting and dashboards | Cross-region metric aggregation |\n\nThe multi-region coordination system extends the existing `EventCoordinator` to handle cross-region message delivery with retries, ordering guarantees, and partition tolerance. Regional failures cannot block other regions, but eventual consistency ensures global coherence when connectivity is restored.\n\n**Global Coordination APIs:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `replicate_model_version(model_name, version, target_regions)` | model_name str, version str, target_regions List[str] | ReplicationStatus | Replicate model to specified regions |\n| `get_nearest_endpoint(model_name, client_location)` | model_name str, client_location Location | ModelEndpoint | Return closest healthy model endpoint |\n| `sync_experiment_metadata(experiment_id, source_region)` | experiment_id str, source_region str | SyncResult | Synchronize experiment data across regions |\n| `resolve_version_conflict(model_name, conflicting_versions)` | model_name str, conflicting_versions List[ModelVersion] | ModelVersion | Resolve concurrent model updates |\n| `check_global_consistency(resource_type, resource_id)` | resource_type str, resource_id str | ConsistencyReport | Verify consistency across regions |\n\nMulti-region deployments require **split-brain protection** to handle network partitions where regions cannot communicate. The system uses consensus protocols for critical operations like model promotion while allowing regions to operate independently for read-heavy workloads like experiment tracking and model serving.\n\n> **Design Insight**: Multi-region deployments expose the tension between consistency and availability. The platform must gracefully degrade during network partitions while ensuring critical safety properties like preventing conflicting model versions from serving simultaneously in different regions.\n\n#### Edge Computing and Model Deployment\n\nIoT devices, mobile applications, and low-latency scenarios require ML models deployed at the **network edge** rather than centralized cloud infrastructure. **Edge deployment** transforms the platform from a **cloud-centric service** to a **hierarchical distribution network** that pushes intelligence closer to data sources. Think of this like evolving from **centralized broadcasting** to a **content delivery network** with local caching and adaptive streaming.\n\nEdge deployments introduce constraints around limited compute resources, intermittent connectivity, model size restrictions, and autonomous operation when disconnected from the central platform.\n\n**Edge Deployment Architecture:**\n\n| Component | Edge Capability | Sync Requirements | Offline Operation |\n|-----------|-----------------|-------------------|-------------------|\n| Model Serving | Optimized inference engines | Model updates via sync protocol | Full autonomy with cached models |\n| Prediction Logging | Local buffering with batch upload | Upload when connectivity available | Store-and-forward with compression |\n| Monitoring | Local health checks and basic metrics | Aggregate metrics upload | Alert on local thresholds only |\n| Model Updates | Incremental model patching | Delta synchronization | Version rollback capability |\n\nEdge model serving requires **model optimization** techniques that reduce memory footprint and inference latency while maintaining acceptable accuracy. The platform automatically generates optimized model variants using quantization, pruning, and knowledge distillation based on edge device capabilities.\n\n**Edge Optimization Pipeline:**\n\n| Optimization | Input Requirements | Output Characteristics | Quality Impact |\n|--------------|-------------------|------------------------|----------------|\n| Quantization | Float32 model weights | Int8 weights with 4x size reduction | 1-3% accuracy loss typical |\n| Pruning | Dense neural network | Sparse network with 50-90% weight reduction | 2-5% accuracy loss with careful tuning |\n| Knowledge Distillation | Large teacher model | Small student model with similar behavior | 5-15% accuracy loss for 10x size reduction |\n| Model Compilation | Framework-specific model | Optimized binary for target hardware | No accuracy loss, 2-5x speed improvement |\n\nThe edge synchronization protocol handles intermittent connectivity by batching updates, compressing data transfers, and implementing conflict-free replicated data types (CRDTs) for prediction logs and monitoring metrics.\n\n**Edge Deployment APIs:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `create_edge_deployment(model_name, edge_config, optimization_spec)` | model_name str, edge_config EdgeConfig, optimization_spec OptimizationSpec | str | Deploy optimized model to edge devices |\n| `sync_edge_data(edge_device_id, data_batch)` | edge_device_id str, data_batch CompressedData | SyncAcknowledgment | Upload batched data from edge device |\n| `push_model_update(device_group, model_delta)` | device_group str, model_delta ModelDelta | PushStatus | Send incremental model update to device group |\n| `check_edge_health(device_id)` | device_id str | EdgeHealthStatus | Monitor edge device operational status |\n| `rollback_edge_model(device_id, target_version)` | device_id str, target_version str | RollbackResult | Revert edge device to previous model version |\n\n#### Large-Scale Training Orchestration\n\nAdvanced ML models require training across hundreds or thousands of GPUs with sophisticated parallelization strategies. **Large-scale training** transforms the platform from supporting **individual researchers** to enabling **industrial-scale model development** comparable to training foundation models. Think of this evolution like moving from a **university chemistry lab** to a **pharmaceutical manufacturing plant** with automated processes and quality controls.\n\nLarge-scale training introduces challenges around gang scheduling, gradient synchronization, fault tolerance at scale, and dynamic resource allocation that stress-test the pipeline orchestration component.\n\n**Large-Scale Training Components:**\n\n| Component | Single-Node | Multi-Node | Large-Scale (100+ nodes) |\n|-----------|-------------|------------|--------------------------|\n| Resource Scheduling | Simple container allocation | Gang scheduling for distributed jobs | Hierarchical scheduling with priority queues |\n| Gradient Synchronization | In-memory parameter updates | AllReduce communication patterns | Hierarchical AllReduce with compression |\n| Fault Tolerance | Checkpoint to persistent storage | Coordinated checkpointing | Automatic failure detection and recovery |\n| Data Pipeline | Local data loading | Distributed data sharding | Parallel data preprocessing with caching |\n| Monitoring | Basic GPU utilization | Per-node communication metrics | System-wide bottleneck detection |\n\nThe large-scale training orchestrator extends the existing `Pipeline` and `Step` abstractions with distributed execution primitives and collective communication operations. Training steps become **distributed operations** with explicit parallelization strategies rather than single-container executions.\n\n**Distributed Training Extensions:**\n\n| Entity | Fields | Purpose |\n|--------|--------|---------|\n| `DistributedStep` | step_id str, parallelism_strategy str, node_count int, processes_per_node int, communication_backend str, synchronization_mode str | Training step with distributed execution |\n| `CollectiveOperation` | operation_type str, participants List[str], data_size_bytes int, compression str, timeout_seconds int | Coordinated multi-node operation |\n| `TrainingTopology` | topology_type str, node_assignments Dict[str, List[str]], bandwidth_matrix Dict[str, Dict[str, float]] | Physical layout of training cluster |\n| `CheckpointStrategy` | frequency_steps int, storage_location str, compression bool, async_upload bool, retention_policy str | Fault tolerance configuration |\n\nLarge-scale training requires **hierarchical fault tolerance** where node failures don't restart the entire job. The system implements elastic training that can continue with reduced parallelism when nodes fail and scale back up when replacement resources become available.\n\n**Large-Scale Training APIs:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `schedule_distributed_training(training_spec, resource_requirements)` | training_spec DistributedTrainingSpec, resource_requirements ResourceSpec | str | Schedule multi-node training job |\n| `create_checkpoint(job_id, checkpoint_metadata)` | job_id str, checkpoint_metadata Dict | CheckpointInfo | Save distributed training state |\n| `resume_from_checkpoint(checkpoint_id, new_resource_spec)` | checkpoint_id str, new_resource_spec ResourceSpec | str | Restart training from saved checkpoint |\n| `monitor_collective_operations(job_id)` | job_id str | CollectiveMetrics | Track communication performance |\n| `handle_node_failure(job_id, failed_nodes)` | job_id str, failed_nodes List[str] | RecoveryPlan | Respond to node failures during training |\n\n> **Architecture Decision: Hierarchical Training Coordination**\n> - **Context**: Large-scale training requires coordination across hundreds of nodes while maintaining fault tolerance and performance\n> - **Options Considered**:\n>   - Centralized parameter server architecture with bottleneck risks\n>   - Fully decentralized peer-to-peer coordination with complex failure handling\n>   - Hierarchical coordination with regional aggregators and global coordination\n> - **Decision**: Implement hierarchical coordination with tree-based aggregation and elastic scaling\n> - **Rationale**: Balances coordination efficiency with fault tolerance while supporting elastic scaling based on resource availability\n> - **Consequences**: Requires sophisticated topology-aware scheduling but provides optimal performance and resilience for large-scale workloads\n\n**Common Pitfalls in Scale Extensions:**\n\n⚠️ **Pitfall: Cross-Region Consistency Violations**\nMulti-region deployments can serve inconsistent model versions if replication delays cause some regions to lag behind during model updates. This creates subtle bugs where the same input produces different outputs depending on which region serves the request. Prevent this by implementing global model version coordination with rollout controls that prevent serving until all target regions confirm successful deployment.\n\n⚠️ **Pitfall: Edge Device Resource Exhaustion**\nEdge deployments often fail when optimized models still exceed device memory or compute capabilities, especially when multiple models run simultaneously on the same device. This leads to out-of-memory crashes or unacceptably slow inference times. Prevent this by implementing device capability profiling that measures actual resource consumption and automatically selects appropriate optimization levels based on measured device performance.\n\n⚠️ **Pitfall: Large-Scale Training Communication Bottlenecks**\nLarge-scale training can become communication-bound when gradient synchronization dominates training time, especially with high-dimensional models or slow network connections. This manifests as poor GPU utilization despite abundant compute resources. Prevent this by implementing gradient compression, overlapping communication with computation, and adaptive batching based on measured network bandwidth.\n\n### Ecosystem Integrations\n\nThe MLOps platform operates within a broader ecosystem of ML frameworks, cloud services, and third-party tools. **Deep ecosystem integration** transforms the platform from an **isolated solution** to a **central hub** that orchestrates the entire ML toolchain. Think of this evolution like moving from a **standalone application** to an **operating system** that provides infrastructure for diverse applications while maintaining compatibility and interoperability.\n\nSuccessful ecosystem integration requires understanding the interaction patterns, data formats, and operational models of popular ML tools while maintaining the platform's architectural integrity and avoiding tight coupling to specific vendor solutions.\n\n#### ML Framework Integration\n\nModern ML teams use diverse frameworks like TensorFlow, PyTorch, Scikit-learn, XGBoost, and emerging frameworks for specific domains. **Framework integration** ensures the platform provides value regardless of the underlying ML technology choices. This requires **framework-agnostic abstractions** while supporting framework-specific optimizations.\n\nThe integration strategy uses **adapter patterns** that translate between the platform's common interfaces and framework-specific APIs. Each framework adapter handles model serialization, metadata extraction, and execution environment setup while exposing a consistent interface to the core platform components.\n\n**Framework Integration Architecture:**\n\n| Framework | Model Format | Metadata Extraction | Runtime Requirements | Serving Integration |\n|-----------|--------------|-------------------|---------------------|-------------------|\n| TensorFlow | SavedModel format | TensorFlow metadata API | TensorFlow Serving | Native TF Serving support |\n| PyTorch | TorchScript or pickle | Manual metadata registration | TorchServe container | TorchServe integration |\n| Scikit-learn | Pickle with joblib | Scikit-learn introspection | Python runtime | Custom serving wrapper |\n| XGBoost | XGB binary format | Booster introspection | XGBoost library | Native XGBoost prediction |\n| ONNX | ONNX model format | ONNX metadata | ONNX Runtime | Universal ONNX serving |\n\nFramework adapters implement the standard `ModelArtifactStore` interface while handling framework-specific serialization and validation. This allows the model registry to support any framework through a pluggable adapter system without modifying core platform code.\n\n**Framework Adapter Interface:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `serialize_model(model_object, metadata)` | model_object Any, metadata Dict | SerializedModel | Convert framework model to storage format |\n| `deserialize_model(model_data, target_format)` | model_data bytes, target_format str | Any | Load model from storage in requested format |\n| `extract_model_signature(model_object)` | model_object Any | ModelSignature | Extract input/output schema from model |\n| `validate_model_compatibility(model_data, runtime_env)` | model_data bytes, runtime_env Dict | ValidationResult | Check if model can run in target environment |\n| `optimize_for_serving(model_data, optimization_config)` | model_data bytes, optimization_config Dict | bytes | Apply serving optimizations like quantization |\n\nThe framework integration system automatically detects model types and selects appropriate adapters based on file extensions, metadata markers, or explicit framework specifications. This enables seamless workflows where data scientists can register models without worrying about platform-specific conversion requirements.\n\n> **Design Insight**: Framework adapters provide a translation layer that preserves framework-specific optimizations while exposing platform-standard interfaces. This pattern enables supporting new frameworks through plugin development without modifying core platform components.\n\n#### Cloud Platform Integration\n\nEnterprise ML teams often use cloud-managed services for specific capabilities like data warehouses, managed Kubernetes clusters, or specialized ML services. **Cloud integration** extends the platform's reach by **federating with external services** rather than rebuilding equivalent capabilities. Think of this like **diplomatic relations** where the platform maintains sovereignty while establishing treaties for specific collaborations.\n\nCloud integrations follow the **adapter pattern** similar to framework integrations, but focus on operational concerns like authentication, resource provisioning, and service lifecycle management rather than data format conversion.\n\n**Cloud Service Integration Patterns:**\n\n| Integration Type | Authentication | Resource Management | Data Flow | Service Discovery |\n|------------------|----------------|-------------------|-----------|------------------|\n| Data Sources | Cloud IAM roles | Query-based access | Pull data for training | Service endpoint configuration |\n| Compute Clusters | Kubernetes RBAC | Node pool management | Push workloads to cluster | Cluster API integration |\n| Storage Services | Cloud credentials | Bucket lifecycle policies | Stream artifacts bidirectionally | SDK-based discovery |\n| ML Services | API key management | Usage quota monitoring | REST API integration | Service catalog lookup |\n| Monitoring | Service account tokens | Dashboard provisioning | Push metrics and logs | Metrics endpoint registration |\n\nThe cloud integration framework provides **credential management**, **service discovery**, and **lifecycle coordination** capabilities that individual adapters can leverage. This prevents each cloud adapter from implementing its own authentication and configuration management.\n\n**Cloud Integration APIs:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `register_cloud_service(service_type, credentials, config)` | service_type str, credentials CloudCredentials, config Dict | str | Register cloud service for platform use |\n| `provision_compute_cluster(cloud_provider, cluster_spec)` | cloud_provider str, cluster_spec ClusterSpec | ClusterInfo | Create managed compute cluster |\n| `sync_data_from_warehouse(warehouse_config, query, destination)` | warehouse_config Dict, query str, destination str | SyncJob | Pull training data from cloud warehouse |\n| `push_metrics_to_service(service_name, metrics_batch)` | service_name str, metrics_batch List[Metric] | bool | Send metrics to cloud monitoring |\n| `backup_artifacts_to_cloud(artifact_paths, cloud_storage_config)` | artifact_paths List[str], cloud_storage_config Dict | BackupJob | Replicate artifacts to cloud storage |\n\nCloud integrations handle **credential rotation**, **service health monitoring**, and **cost optimization** through automated policies. For example, the compute cluster integration can automatically scale down expensive GPU nodes during periods of low utilization while maintaining rapid scale-up capability.\n\n> **Architecture Decision: Federation Over Replication**\n> - **Context**: Cloud providers offer specialized ML services that would be expensive and time-consuming to replicate within the platform\n> - **Options Considered**:\n>   - Build equivalent capabilities within the platform for complete independence\n>   - Integrate with cloud services through APIs while maintaining platform control\n>   - Use cloud services as the primary platform with custom extensions\n> - **Decision**: Implement federation through standardized adapters that preserve platform workflows while leveraging cloud capabilities\n> - **Rationale**: Maximizes value from cloud investments while maintaining operational consistency and avoiding vendor lock-in\n> - **Consequences**: Requires sophisticated adapter development but provides flexibility and cost optimization opportunities\n\n#### Third-Party Tool Integration\n\nML teams use diverse tools for data preparation, model development, deployment automation, and operational monitoring. **Tool ecosystem integration** creates **workflows that span multiple tools** while maintaining the platform as the **system of record** for ML artifacts and metadata. Think of this like **API orchestration** where the platform conducts a symphony of specialized tools rather than replacing every instrument.\n\nThird-party integrations focus on **data synchronization**, **workflow triggering**, and **metadata federation** to ensure information flows seamlessly between tools while avoiding duplicate work or inconsistent states.\n\n**Common Integration Categories:**\n\n| Tool Category | Integration Pattern | Data Synchronization | Event Coordination | Examples |\n|---------------|-------------------|---------------------|-------------------|----------|\n| Data Preparation | Pipeline triggers | Export feature engineering | Trigger on data updates | dbt, Apache Airflow, Prefect |\n| Model Development | Artifact sync | Import notebooks and models | Sync on experiment completion | Jupyter, Databricks, SageMaker |\n| Deployment Automation | CD pipeline triggers | Export deployment specs | Trigger on model promotion | GitLab CI, GitHub Actions, Jenkins |\n| Security Scanning | Validation hooks | Send models for analysis | Block on security failures | Twistlock, Aqua Security |\n| Business Intelligence | Metrics federation | Export model performance | Schedule report updates | Tableau, PowerBI, Looker |\n\nThe integration framework provides **webhook infrastructure**, **event transformation**, and **credential management** that individual tool integrations can leverage. This enables rapid integration development for new tools without rebuilding common infrastructure.\n\n**Third-Party Integration APIs:**\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `register_webhook_endpoint(tool_name, event_types, endpoint_config)` | tool_name str, event_types List[str], endpoint_config WebhookConfig | str | Register webhook for tool notifications |\n| `trigger_external_workflow(tool_name, workflow_id, parameters)` | tool_name str, workflow_id str, parameters Dict | TriggerResult | Start workflow in external tool |\n| `sync_metadata_to_tool(tool_name, metadata_type, data)` | tool_name str, metadata_type str, data Any | SyncResult | Export metadata to external tool |\n| `import_artifacts_from_tool(tool_name, import_spec)` | tool_name str, import_spec ImportSpec | ImportResult | Import artifacts from external tool |\n| `federate_metrics(tool_name, metric_mapping)` | tool_name str, metric_mapping Dict | FederationSetup | Set up bidirectional metric sharing |\n\nThird-party integrations implement **idempotent synchronization** to handle network failures, **conflict resolution** for concurrent updates, and **audit logging** to track data provenance across tool boundaries.\n\n> **Design Insight**: Successful third-party integrations preserve each tool's strengths while ensuring the MLOps platform remains the authoritative source for model lineage and deployment decisions. This requires careful interface design that respects tool boundaries while enabling seamless workflows.\n\n**Common Pitfalls in Ecosystem Integrations:**\n\n⚠️ **Pitfall: Framework Lock-in Through Tight Coupling**\nIntegrating too deeply with specific ML frameworks can create hidden dependencies that make it difficult to support new frameworks or upgrade existing ones. This happens when platform code directly imports framework libraries or relies on framework-specific data structures. Prevent this by using adapter patterns with well-defined interfaces, serializing models to framework-agnostic formats when possible, and testing framework upgrades in isolated environments.\n\n⚠️ **Pitfall: Cloud Credential Sprawl and Security Risks**\nCloud integrations often accumulate credentials and permissions over time, creating security risks and operational complexity. This occurs when each integration manages its own credentials without centralized policies or rotation procedures. Prevent this by implementing centralized credential management with automatic rotation, principle of least privilege access controls, and regular security audits of cloud service permissions.\n\n⚠️ **Pitfall: Third-Party Integration Cascade Failures**\nExternal tool failures can cascade into platform failures when integrations don't handle service unavailability gracefully. This manifests as platform operations blocking on external API calls or failing when third-party webhooks are unreachable. Prevent this by implementing circuit breakers for external service calls, asynchronous integration patterns with retry queues, and graceful degradation when external tools are unavailable.\n\n### Implementation Guidance\n\nThe future extensions outlined in this section demonstrate the platform's architectural flexibility while providing concrete guidance for prioritizing and implementing advanced capabilities. This implementation guidance focuses on the foundational patterns that enable extension development rather than complete implementations of specific features.\n\n#### Technology Recommendations for Extensions\n\n| Extension Category | Simple Option | Advanced Option | \n|-------------------|---------------|-----------------|\n| Feature Store | SQLite with Pandas integration | Apache Feast with Redis online store |\n| AutoML Optimization | Grid search with multiprocessing | Optuna with distributed trials |\n| Multi-Tenant Storage | PostgreSQL schemas with RLS | Dedicated databases with federation |\n| Edge Deployment | Docker containers with sync scripts | Kubernetes edge clusters with GitOps |\n| Large-Scale Training | Horovod with MPI backend | Ray Train with elastic scaling |\n| Cloud Integration | Direct SDK calls with retry logic | Terraform providers with state management |\n\n#### Extension Development Framework\n\nThe platform provides a standardized framework for developing extensions that maintains architectural consistency while supporting diverse integration requirements. Extensions should follow these patterns:\n\n**Extension Base Classes:**\n\n```python\n# MLOps platform extension framework\nclass MLOpsExtension:\n    \"\"\"Base class for platform extensions with standard lifecycle.\"\"\"\n    \n    def __init__(self, config: ExtensionConfig, event_coordinator: EventCoordinator):\n        # TODO 1: Initialize extension with configuration and event system access\n        # TODO 2: Register extension-specific health checks\n        # TODO 3: Set up extension-specific metrics collection\n        # TODO 4: Initialize any required external service connections\n        pass\n    \n    def start(self) -> bool:\n        \"\"\"Start extension services and begin processing.\"\"\"\n        # TODO 1: Validate configuration and external dependencies\n        # TODO 2: Subscribe to relevant platform events\n        # TODO 3: Start any background processing threads or tasks\n        # TODO 4: Register extension APIs with the platform router\n        # TODO 5: Publish extension ready event\n        pass\n    \n    def health_check(self) -> HealthCheck:\n        \"\"\"Report extension health status.\"\"\"\n        # TODO 1: Check external service connectivity\n        # TODO 2: Validate critical configuration is still valid\n        # TODO 3: Verify background processes are running correctly\n        # TODO 4: Return detailed health status with failure reasons\n        pass\n\nclass FeatureStoreExtension(MLOpsExtension):\n    \"\"\"Feature store extension providing feature management capabilities.\"\"\"\n    \n    def create_feature_group(self, name: str, source_config: Dict, features: List[FeatureDefinition]) -> str:\n        # TODO 1: Validate feature definitions and source configuration\n        # TODO 2: Create feature group metadata in extension storage\n        # TODO 3: Set up data synchronization from source to feature store\n        # TODO 4: Register feature group with platform model registry for lineage\n        # TODO 5: Publish feature group created event\n        pass\n```\n\n**Extension Integration Helpers:**\n\n```python\n# Standard patterns for extension development\nclass ExtensionEventHandler:\n    \"\"\"Helper for handling platform events in extensions.\"\"\"\n    \n    def __init__(self, extension_name: str, event_coordinator: EventCoordinator):\n        # TODO 1: Register extension as event source\n        # TODO 2: Set up structured logging with extension context\n        # TODO 3: Initialize event processing metrics\n        pass\n    \n    def subscribe_to_events(self, event_mappings: Dict[str, callable]):\n        \"\"\"Subscribe to platform events with extension-specific handlers.\"\"\"\n        # TODO 1: Register event handlers with error handling and retries\n        # TODO 2: Set up event processing metrics and monitoring\n        # TODO 3: Implement graceful shutdown for event processing\n        pass\n\nclass ExtensionAPIRouter:\n    \"\"\"Helper for exposing extension APIs through platform routing.\"\"\"\n    \n    def register_endpoints(self, extension_name: str, endpoints: Dict[str, callable]):\n        \"\"\"Register extension HTTP endpoints with platform API gateway.\"\"\"\n        # TODO 1: Add authentication and authorization middleware\n        # TODO 2: Set up request logging and metrics collection\n        # TODO 3: Add input validation and error handling\n        # TODO 4: Register endpoints with API documentation system\n        pass\n```\n\n#### Extension Development Guidelines\n\n**Milestone Checkpoint for Extension Development:**\n\nAfter implementing an extension using the framework, verify the following behavior:\n\n1. **Extension Lifecycle**: Start the extension and confirm it publishes a ready event and responds to health checks\n2. **Event Integration**: Trigger a relevant platform event and verify the extension receives and processes it correctly\n3. **API Exposure**: Make HTTP requests to extension endpoints through the platform API gateway\n4. **Error Handling**: Simulate external service failures and confirm graceful degradation\n5. **Monitoring**: Check that extension metrics appear in platform monitoring dashboards\n\n**Extension Testing Pattern:**\n\n```python\n# Test framework for extensions\nclass ExtensionTestHelper:\n    \"\"\"Helper for testing extension integrations.\"\"\"\n    \n    def create_test_environment(self, extension_config: Dict) -> TestEnvironment:\n        \"\"\"Set up isolated test environment for extension development.\"\"\"\n        # TODO 1: Create temporary database and storage for extension\n        # TODO 2: Set up mock external services with configurable responses\n        # TODO 3: Initialize extension with test configuration\n        # TODO 4: Provide access to platform test utilities\n        pass\n    \n    def simulate_platform_events(self, events: List[Event]) -> List[EventResult]:\n        \"\"\"Send test events to extension and collect responses.\"\"\"\n        # TODO 1: Publish events through test event coordinator\n        # TODO 2: Wait for extension processing with timeout\n        # TODO 3: Collect any events published by extension\n        # TODO 4: Return processing results and timing information\n        pass\n```\n\nThe extension development framework ensures new capabilities integrate cleanly with existing platform components while maintaining operational consistency and debuggability across the entire system.\n\n\n## Glossary\n\n> **Milestone(s):** This section provides essential definitions and terminology that apply to all milestones (1-5), ensuring consistent understanding of technical terms, MLOps concepts, and domain-specific vocabulary used throughout the platform architecture.\n\nThe MLOps platform introduces numerous technical concepts, architectural patterns, and domain-specific terminology that span machine learning, distributed systems, and software engineering. Understanding these terms is crucial for implementing and maintaining the platform effectively. This glossary provides comprehensive definitions organized by functional areas, with cross-references to related concepts and concrete examples from the platform's implementation.\n\n### Core MLOps Concepts\n\n**Experiment tracking** refers to the systematic logging and organization of ML training runs, capturing parameters, metrics, and artifacts to enable reproducibility and comparison across different training attempts. The experiment tracking component maintains a hierarchical relationship where experiments group related runs, and each run captures the complete context of a training session including hyperparameters, performance metrics at each step, and generated artifacts like model files and visualizations.\n\n**Model registry** provides versioned storage and lifecycle management for trained ML models, implementing semantic versioning with stage transitions through development, staging, and production environments. The registry enforces immutability guarantees ensuring that once a model version is registered, its artifacts and metadata cannot be modified, while maintaining complete model lineage that traces each version back to its source training run, data dependencies, and code commit.\n\n**Pipeline orchestration** coordinates multi-step ML workflows using directed acyclic graphs (DAGs) that define data dependencies between steps. The orchestration engine handles resource allocation, step isolation through containerization, and fault tolerance through retry policies and checkpointing. Each pipeline execution creates a complete audit trail of step executions, resource usage, and data flow that enables debugging and performance optimization.\n\n**Model deployment** encompasses the process of serving ML models as scalable HTTP endpoints in production environments, supporting advanced deployment patterns like blue-green deployments for zero-downtime updates and canary deployments for gradual traffic shifting. The deployment component integrates with specialized inference servers and implements auto-scaling policies that adjust replica counts based on request load and latency requirements.\n\n**Model monitoring** tracks ML model performance and data characteristics in production through comprehensive prediction logging and statistical analysis. The monitoring system detects data drift by comparing incoming feature distributions against training baselines, identifies concept drift through prediction distribution analysis, and maintains real-time performance metrics including latency percentiles and throughput measurements.\n\n### Architecture and Design Patterns\n\n**Microservices approach** structures the platform as independent services that communicate through well-defined APIs, enabling independent scaling, deployment, and technology choices for each component. Each service maintains its own data store and implements clear boundaries that prevent tight coupling while supporting platform-wide coordination through event-driven patterns.\n\n**Hexagonal architecture** separates business logic from external concerns by defining explicit interfaces for all dependencies, allowing components to swap implementations without affecting core functionality. This pattern enables the platform to support multiple storage backends, inference servers, and orchestration engines while maintaining consistent internal APIs.\n\n**Polyglot persistence** employs different data storage technologies optimized for specific access patterns, using PostgreSQL for structured metadata with complex queries, object storage for large binary artifacts, and time-series databases for metrics and monitoring data. This approach maximizes performance while ensuring data consistency across the platform.\n\n**Event-driven coordination** implements asynchronous communication between components using immutable events that capture state changes and trigger downstream processing. The event system supports at-least-once delivery guarantees and maintains causal ordering for events affecting the same resources, enabling reliable workflow coordination without tight coupling.\n\n### Data Management and Storage\n\n**Correlation ID** provides a unique identifier that links related data and operations across multiple components, enabling distributed tracing and debugging by following the flow of requests and events through the entire system. Each API request, pipeline execution, and model deployment receives a correlation ID that appears in all related logs and database records.\n\n**Artifact** refers to binary files produced during ML workflows, including trained model files, evaluation plots, configuration files, and dataset snapshots. The platform implements content-addressable storage for artifacts using cryptographic hashes as keys, enabling deduplication and ensuring data integrity through checksum validation.\n\n**Model lineage** creates a directed acyclic graph showing the complete provenance of a trained model, linking it to the specific training dataset version, code commit, hyperparameters, and experiment run that produced it. This lineage graph enables impact analysis when data or code changes and supports regulatory compliance requirements for model traceability.\n\n**Hierarchical namespacing** organizes platform resources using path-based structures that separate tenants, workspaces, and individual resources, enabling fine-grained access control and resource quotas. The namespace hierarchy supports multi-tenant deployments while maintaining strict isolation between different organizations or teams.\n\n### Model Lifecycle Management\n\n**Semantic versioning** adapts the MAJOR.MINOR.PATCH version scheme for ML workflows, where major versions indicate breaking changes to model inputs or outputs, minor versions represent significant algorithmic improvements, and patch versions capture bug fixes or retraining with additional data.\n\n**Stage transitions** implement a promotion workflow where model versions progress through predefined stages (Development, Staging, Production, Archived) with approval gates and validation requirements at each transition. The system maintains a complete audit trail of stage changes including approval metadata and rollback capabilities.\n\n**Immutability guarantees** ensure that once a model version is registered in the model registry, its artifacts and core metadata cannot be modified, preventing accidental corruption of production models while allowing non-breaking metadata updates like tags and descriptions.\n\n**Approval workflows** define structured processes requiring validation and authorization before model versions can be promoted to higher stages, supporting both automated checks (performance thresholds, integration tests) and manual approvals from designated reviewers.\n\n### Pipeline and Training Concepts\n\n**DAG (Directed Acyclic Graph)** represents pipeline step dependencies as a mathematical graph structure where nodes are computational steps and edges represent data dependencies, ensuring that upstream steps complete before downstream steps begin execution. The pipeline orchestrator computes execution order by performing topological sorting on the DAG structure.\n\n**Resource allocation** assigns computational resources (CPU cores, memory, GPU units, storage) to pipeline steps based on declared requirements and cluster availability, supporting both guaranteed resource reservations and best-effort scheduling for cost optimization.\n\n**Step isolation** executes each pipeline step within a separate container environment with dedicated resource limits, preventing interference between steps and enabling precise resource accounting. Containerization also ensures reproducible execution environments across different cluster nodes.\n\n**Fault tolerance** handles various failure scenarios through retry policies with exponential backoff, checkpoint-restart mechanisms for long-running steps, and graceful degradation when non-critical steps fail. The system maintains detailed failure logs and supports both automatic recovery and manual intervention.\n\n**Distributed training** coordinates model training across multiple GPUs or compute nodes using parameter server architectures or all-reduce communication patterns, requiring careful gang scheduling to ensure all resources are allocated simultaneously and handling node failures through checkpointing and restart mechanisms.\n\n**Gang scheduling** allocates all required resources for a distributed training job simultaneously rather than incrementally, preventing deadlock situations where partially allocated jobs block resources needed by other jobs waiting in the queue.\n\n**Artifact lineage** tracks the flow of data artifacts through pipeline steps, recording which outputs were generated from which inputs and maintaining a complete dependency graph that enables impact analysis and debugging of data quality issues.\n\n### Deployment and Serving\n\n**Blue-green deployments** maintain two complete production environments (blue and green) and switch traffic atomically between them during model updates, enabling zero-downtime deployments with instant rollback capabilities if issues are detected with the new version.\n\n**Canary deployments** gradually shift traffic from the current model version to a new version by routing a small percentage of requests to the new version initially and increasing the percentage based on performance metrics and error rates, providing risk mitigation for production deployments.\n\n**Auto-scaling** automatically adjusts the number of model serving replicas based on observed metrics like request rate, latency percentiles, and resource utilization, ensuring adequate capacity to handle traffic spikes while minimizing costs during low-traffic periods.\n\n**Traffic management** controls the routing of inference requests between different model versions using configurable rules based on request headers, client properties, or random sampling, enabling A/B testing and gradual rollouts with precise control over traffic distribution.\n\n**Inference servers** are specialized systems optimized for serving ML models in production, including TensorFlow Serving, TorchServe, and NVIDIA Triton, providing features like dynamic batching, model optimization, and GPU memory management that maximize serving performance.\n\n**Model warming** involves preloading models into memory and executing initial inference requests to trigger just-in-time compilation and optimization before routing production traffic to new serving instances, reducing cold start latency and ensuring consistent performance.\n\n**Health checks** implement validation endpoints that verify model serving instances are ready to handle requests, checking model loading status, dependency availability, and basic inference functionality to support load balancer configuration and auto-scaling decisions.\n\n**Traffic splitting** distributes incoming requests across multiple model versions according to configured percentages, enabling controlled experiments and gradual rollouts while maintaining detailed metrics for each version to support decision-making.\n\n**Rollback** provides mechanisms to revert to a previous model version when issues are detected, including automated rollback based on error rate thresholds and manual rollback procedures that preserve traffic routing configurations and monitoring baselines.\n\n### Monitoring and Observability\n\n**Data drift** represents statistical changes in input data distribution compared to the training dataset, detected using techniques like the Kolmogorov-Smirnov test for continuous features and chi-squared tests for categorical features, indicating potential degradation in model performance.\n\n**Concept drift** refers to changes in the underlying relationship between input features and target variables, typically detected by monitoring changes in prediction distributions or performance metrics over time, requiring model retraining to maintain accuracy.\n\n**Prediction logging** captures comprehensive information about each inference request including input features, model outputs, confidence scores, latency measurements, and request metadata, providing the foundation for performance analysis and drift detection.\n\n**Population Stability Index (PSI)** measures the stability of feature distributions between two time periods by comparing the proportion of samples in different bins, with values above 0.2 typically indicating significant distribution shifts that may affect model performance.\n\n**Kolmogorov-Smirnov test** compares two continuous distributions by measuring the maximum difference between their cumulative distribution functions, providing a statistical test for detecting changes in feature distributions with quantified confidence levels.\n\n**Chi-squared test** evaluates whether categorical feature distributions differ significantly between two samples by comparing observed versus expected frequencies across categories, supporting drift detection for discrete and ordinal features.\n\n**Alert escalation** implements tiered notification systems that route alerts to appropriate teams based on severity levels and response times, ensuring critical issues receive immediate attention while preventing alert fatigue through intelligent filtering and grouping.\n\n**Statistical significance** quantifies the probability that observed differences in model performance or data distributions are not due to random variation, supporting data-driven decisions about model updates and drift response actions.\n\n### System Integration and Communication\n\n**Inter-component APIs** define RESTful interfaces for communication between platform components, specifying request/response formats, authentication requirements, error handling patterns, and versioning strategies that enable independent component evolution while maintaining compatibility.\n\n**Circuit breaker** implements a pattern that prevents cascade failures in distributed systems by monitoring error rates and response times, automatically routing requests away from failing services and periodically testing recovery to restore normal operation when services become healthy again.\n\n**Event sourcing** captures all state changes as immutable events stored in an append-only log, enabling complete system state reconstruction, audit trails, and support for complex queries about historical system behavior and data lineage.\n\n**Idempotent event handlers** process events in a way that produces the same result regardless of how many times the event is delivered, supporting at-least-once delivery guarantees while preventing duplicate processing and maintaining system consistency.\n\n**Causal ordering** ensures that events affecting the same resources are processed in dependency order, preventing race conditions and maintaining data consistency in distributed systems where events may arrive out of order due to network delays or system failures.\n\n**At-least-once delivery** guarantees that published events will be delivered to all registered subscribers, implementing retry mechanisms and persistent event storage to handle temporary system failures while requiring subscribers to implement idempotent processing.\n\n**Workflow coordination** orchestrates complex multi-component operations through a combination of synchronous API calls for immediate feedback and asynchronous events for long-running processes, maintaining clear transaction boundaries and consistent error handling.\n\n### Error Handling and Recovery\n\n**Automated recovery** implements procedures that detect and resolve common failure scenarios without human intervention, including service restarts, data inconsistency repairs, and resource cleanup, with clear escalation paths when automated approaches are insufficient.\n\n**Failure detection** monitors system health through comprehensive health checks, metric thresholds, and log analysis to identify component failures, performance degradation, and data quality issues as quickly as possible to minimize impact.\n\n**Data consistency** maintains synchronized state across distributed components through careful transaction boundary design, conflict resolution mechanisms, and eventual consistency guarantees that ensure the system converges to a correct state even after failures.\n\n**Transaction boundaries** define the scope of operations that must complete atomically, using database transactions, compensating actions, and saga patterns to maintain data integrity across multiple components and external systems.\n\n**Conflict resolution** handles concurrent operations that modify the same resources through optimistic locking, version vectors, and merge strategies that preserve user intent while maintaining system consistency and preventing data corruption.\n\n**Eventual consistency** provides guarantees that the distributed system will converge to a consistent state within a bounded time period, even in the presence of network partitions and component failures, enabling high availability while ensuring data integrity.\n\n**Recovery procedures** define systematic approaches to restoring normal system operation after failures, including data restoration from backups, service restart sequences, and validation steps to confirm successful recovery.\n\n**Escalation** routes complex issues to human operators when automated recovery procedures are insufficient, providing comprehensive context, suggested actions, and clear procedures for manual intervention while maintaining detailed audit trails.\n\n### Testing and Quality Assurance\n\n**Testing pyramid** structures the testing strategy with many fast, focused unit tests at the base, fewer integration tests in the middle, and minimal slow end-to-end tests at the top, optimizing for quick feedback during development while ensuring comprehensive coverage.\n\n**Milestone verification** implements validation procedures that ensure the platform meets acceptance criteria after each development phase, including automated test suites, performance benchmarks, and functional validation scenarios that demonstrate correct behavior.\n\n**Load testing** evaluates system performance under realistic traffic patterns by simulating concurrent users, varying request rates, and peak load scenarios to identify bottlenecks, validate auto-scaling behavior, and establish performance baselines.\n\n**Integration testing** validates component interactions through their public APIs and event interfaces, using test doubles and contract testing to ensure components work correctly together while maintaining independent development and deployment.\n\n**End-to-end testing** verifies complete workflows from experiment tracking through model deployment and monitoring using realistic datasets and scenarios, ensuring the platform delivers value to users while catching integration issues that unit tests might miss.\n\n**Test fixtures** provide reusable test data, configuration, and infrastructure setup that enables consistent testing environments and reduces test maintenance overhead while supporting both local development and continuous integration pipelines.\n\n**Performance benchmarks** establish quantitative targets for system performance including request latency, throughput, resource utilization, and scalability limits that guide development priorities and validate optimization efforts.\n\n### Observability and Operations\n\n**Structured logging** implements consistent log formats with correlation IDs, contextual metadata, and standardized severity levels that enable effective log aggregation, searching, and analysis across all platform components.\n\n**Distributed tracing** provides end-to-end visibility into request flows across multiple components by propagating trace contexts and recording timing, dependencies, and errors to support performance optimization and debugging of complex workflows.\n\n**Performance profiling** identifies bottlenecks and resource utilization patterns through systematic measurement of CPU usage, memory allocation, I/O patterns, and database query performance to guide optimization efforts and capacity planning.\n\n**Incident response** establishes systematic procedures for detecting, investigating, and resolving system failures through runbooks, escalation procedures, and post-incident reviews that capture lessons learned and prevent recurrence.\n\n### Advanced MLOps Features\n\n**Feature store** provides a centralized repository for feature definitions, transformations, and serving infrastructure that enables feature reuse across different models while maintaining consistency between training and inference data processing pipelines.\n\n**Automated machine learning (AutoML)** systematically explores model architectures, hyperparameter configurations, and feature engineering approaches to optimize model performance with minimal human intervention, requiring sophisticated search algorithms and resource management.\n\n**Multi-tenancy** enables a shared platform to serve multiple isolated organizations or teams through hierarchical resource organization, access control policies, and resource quotas that ensure security and fair resource allocation.\n\n**Edge deployment** optimizes and deploys ML models at network edge locations to minimize latency for end users, requiring model compression, specialized runtime environments, and synchronization mechanisms for model updates and telemetry collection.\n\n**Large-scale training** coordinates model training across hundreds or thousands of compute nodes using advanced parallelization strategies, fault-tolerant communication protocols, and sophisticated scheduling algorithms that maximize resource utilization while handling node failures.\n\n### Integration and Ecosystem\n\n**Framework integration** supports multiple ML frameworks (TensorFlow, PyTorch, scikit-learn, XGBoost) through adapter patterns and standardized interfaces that abstract framework-specific details while preserving access to advanced features and optimizations.\n\n**Cloud integration** federates with external cloud services like managed training platforms, serving infrastructure, and storage systems through standardized APIs and credential management that enables hybrid deployment scenarios and vendor flexibility.\n\n**Model optimization** applies techniques like quantization, pruning, knowledge distillation, and specialized compilation to reduce model size and improve inference performance while maintaining acceptable accuracy levels for production deployment constraints.\n\n**Federation over replication** integrates with existing external services rather than rebuilding equivalent functionality, using APIs, webhooks, and data synchronization to leverage specialized tools while maintaining platform coherence and user experience.\n\n### Implementation Guidance\n\nThe platform's comprehensive terminology reflects the complexity of building production-grade MLOps systems that must handle the full lifecycle of machine learning applications. Understanding these concepts is essential for several reasons: they provide precise vocabulary for discussing system behavior and requirements, they establish clear boundaries between different concerns and components, and they enable effective communication between team members working on different aspects of the platform.\n\nWhen implementing the platform, developers should internalize these concepts progressively, starting with core MLOps terminology (experiments, models, pipelines, deployments, monitoring) before moving to advanced distributed systems concepts (consistency, fault tolerance, observability). Each milestone introduces terminology relevant to its specific domain while building on concepts from previous milestones.\n\nThe glossary serves as both a reference during implementation and a validation tool to ensure consistent understanding across the development team. When design discussions arise, referring to these standardized definitions helps maintain clarity and prevents misunderstandings that could lead to architectural inconsistencies.\n\n**Technology Integration Reference:**\n\n| Concept Category | Core Technologies | Integration Pattern |\n|------------------|-------------------|-------------------|\n| Experiment Tracking | PostgreSQL, MLflow, S3 | Database storage with object storage for artifacts |\n| Model Registry | Docker Registry patterns, Git semantics | Immutable storage with semantic versioning |\n| Pipeline Orchestration | Kubernetes, Apache Airflow | Container orchestration with DAG execution |\n| Model Deployment | Kubernetes, NGINX, Triton | Service mesh with specialized inference servers |\n| Model Monitoring | Prometheus, Grafana, Kafka | Time-series metrics with stream processing |\n| System Integration | REST APIs, CloudEvents, OpenAPI | Event-driven architecture with standard protocols |\n\n**Common Terminology Pitfalls:**\n\n⚠️ **Pitfall: Confusing experiments and runs**\nMany developers initially treat experiments and runs as the same concept, leading to flat organizational structures that become unwieldy at scale. Remember that experiments are logical groupings (like \"hyperparameter tuning for ResNet model\") while runs are individual training executions within those experiments.\n\n⚠️ **Pitfall: Mixing deployment patterns**\nUsing inconsistent terminology for deployment strategies (calling canary deployments \"rolling updates\" or blue-green deployments \"staged rollouts\") creates confusion during incident response. Stick to standard industry terminology to ensure clear communication.\n\n⚠️ **Pitfall: Overloading \"pipeline\" terminology**\nThe term \"pipeline\" appears in multiple contexts (training pipelines, data pipelines, CI/CD pipelines, inference pipelines). Always specify the context or use more precise terms like \"training workflow\" or \"feature pipeline\" to avoid ambiguity.\n\n⚠️ **Pitfall: Inconsistent drift terminology**\nDistinguish clearly between data drift (input distribution changes) and concept drift (relationship changes between inputs and outputs). Using \"drift\" generically makes it difficult to implement appropriate detection and response strategies.\n\nThis comprehensive glossary ensures that all stakeholders in the MLOps platform development and operation share a common understanding of critical concepts, enabling more effective collaboration and reducing miscommunication that could lead to implementation errors or architectural inconsistencies.\n"}