{"html":"<h1 id=\"query-optimizer-design-document\">Query Optimizer: Design Document</h1>\n<h2 id=\"overview\">Overview</h2>\n<p>A query optimizer transforms SQL queries into efficient execution plans by building plan trees, estimating costs, and selecting optimal join orders. The key architectural challenge is balancing optimization time against plan quality while handling combinatorial explosion in the search space.</p>\n<blockquote>\n<p>This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.</p>\n</blockquote>\n<h2 id=\"context-and-problem-statement\">Context and Problem Statement</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - establishes the fundamental optimization challenges that drive the design decisions in Milestones 1-4.</p>\n</blockquote>\n<h3 id=\"the-route-planning-analogy\">The Route Planning Analogy</h3>\n<p>Imagine you&#39;re planning a road trip that visits multiple cities, and you want to minimize both travel time and fuel costs. A naive approach might visit cities in alphabetical order or in the sequence they were added to your itinerary. However, an intelligent route planner considers multiple factors: current traffic conditions, road quality, fuel station locations, and the interconnections between different route segments. The planner explores numerous possible routes, estimates the cost of each alternative, and selects the most efficient path.</p>\n<p>Query optimization follows remarkably similar principles. When a database receives a SQL query joining multiple tables with various filters and projections, there are often thousands or millions of different ways to execute that query. A naive database engine might process tables in the order they appear in the SQL statement, apply filters after performing expensive joins, or always use sequential scans regardless of available indexes. Just as the alphabetical city visit would likely produce a terrible road trip, these naive execution strategies often result in query performance that is orders of magnitude slower than optimal.</p>\n<p>The <strong>query optimizer</strong> serves as the database&#39;s intelligent route planner. It examines the logical requirements expressed in SQL (the destination cities), considers the available physical resources (roads, indexes, memory), estimates the cost of different execution strategies (routes), and constructs an efficient execution plan (the optimal route). The optimizer must balance exploration time against plan quality - spending too much time exploring alternatives defeats the purpose, while insufficient exploration leads to poor execution plans.</p>\n<p>This analogy reveals why query optimization is both essential and challenging. Unlike static route planning where road conditions change gradually, database conditions fluctuate rapidly: table sizes grow, indexes are added or dropped, memory availability changes, and concurrent queries compete for resources. The optimizer must make decisions quickly based on statistical estimates rather than perfect information, yet these decisions directly determine whether a query completes in milliseconds or hours.</p>\n<h3 id=\"core-optimization-challenges\">Core Optimization Challenges</h3>\n<p>Query optimization confronts three fundamental challenges that make it one of the most complex problems in database system design. Understanding these challenges is crucial because they drive every architectural decision in our optimizer implementation.</p>\n<h4 id=\"exponential-search-space-complexity\">Exponential Search Space Complexity</h4>\n<p>The most daunting challenge in query optimization is the <strong>combinatorial explosion</strong> of possible execution plans. For a query joining <code>n</code> tables, there are <code>n!</code> possible join orders. A five-table join has 120 possible orders; a ten-table join has over 3.6 million possibilities. Each join order can be executed with different physical operators (hash join, nested loop join, merge join), different access methods (sequential scan, index scan), and different intermediate result materializations.</p>\n<p>Consider a seemingly simple query joining four tables - <code>Orders</code>, <code>Customers</code>, <code>Products</code>, and <code>OrderItems</code>. The optimizer must decide:</p>\n<ul>\n<li>Which table to scan first and with what access method</li>\n<li>Which pairs of tables to join in what sequence  </li>\n<li>What join algorithm to use for each join operation</li>\n<li>Where to apply filter predicates for maximum efficiency</li>\n<li>Whether to sort results early or late in the execution pipeline</li>\n</ul>\n<p>Even this four-table query generates hundreds of distinct execution plans. The optimizer cannot afford to evaluate every possibility, yet missing the optimal plan can mean the difference between a query that runs in seconds versus one that runs for hours.</p>\n<blockquote>\n<p><strong>Critical Insight</strong>: The exponential search space means that query optimization is fundamentally about intelligent pruning and approximation rather than exhaustive search. Production optimizers use dynamic programming, heuristics, and early pruning to navigate this complexity.</p>\n</blockquote>\n<h4 id=\"statistical-estimation-accuracy\">Statistical Estimation Accuracy</h4>\n<p>Query optimizers operate in an environment of <strong>fundamental uncertainty</strong>. To choose between execution plans, the optimizer must predict how much each plan will cost - but this prediction requires knowing how many rows each operation will process, how selective each filter will be, and how much memory each operator will consume. These predictions are made using statistical approximations that are often inaccurate.</p>\n<p>The accuracy problem cascades through the optimization process. If the optimizer underestimates the selectivity of an early filter, it might choose a nested loop join expecting small intermediate results. When the actual intermediate result is much larger, the chosen plan becomes catastrophically slow. This estimation error occurs because:</p>\n<ul>\n<li><strong>Table statistics become stale</strong> as data changes between statistics collection runs</li>\n<li><strong>Predicate correlation</strong> is often ignored - the optimizer assumes column values are independent when they&#39;re actually correlated</li>\n<li><strong>Intermediate result estimation</strong> compounds errors from each preceding operation</li>\n<li><strong>Data distribution assumptions</strong> may not match real-world data patterns</li>\n</ul>\n<p>For example, an optimizer might estimate that a filter <code>WHERE age &gt; 65 AND income &gt; 100000</code> will be highly selective, assuming age and income are independent. In reality, high-income individuals tend to be older, making this combination less selective than expected. This estimation error can lead to choosing the wrong join algorithm or access method.</p>\n<h4 id=\"optimization-time-vs-plan-quality-trade-offs\">Optimization Time vs. Plan Quality Trade-offs</h4>\n<p>The third fundamental challenge is the <strong>time-quality trade-off</strong>. Optimization itself consumes CPU cycles, memory, and elapsed time. For simple queries that execute in milliseconds, spending seconds optimizing would be counterproductive. Conversely, complex analytical queries that run for hours justify more extensive optimization effort.</p>\n<p>This trade-off manifests in several ways:</p>\n<ul>\n<li><strong>Search completeness vs. speed</strong>: Complete dynamic programming finds optimal solutions but becomes prohibitively expensive for large join queries</li>\n<li><strong>Statistics precision vs. collection overhead</strong>: Detailed histograms provide better estimates but require more storage and maintenance overhead  </li>\n<li><strong>Plan caching vs. adaptivity</strong>: Cached plans avoid re-optimization costs but may become suboptimal as data changes</li>\n<li><strong>Heuristic rules vs. cost-based decisions</strong>: Rules execute quickly but may miss opportunities that cost-based analysis would find</li>\n</ul>\n<p>Production database systems handle this trade-off through <strong>optimization budgets</strong> - they allocate limited time and memory for optimization based on query complexity and expected execution cost. Simple queries receive basic optimization; complex queries get more sophisticated treatment.</p>\n<blockquote>\n<p><strong>Design Principle</strong>: Our optimizer must be architected to support different optimization depths, allowing the system to spend more effort optimizing queries that will benefit most from sophisticated analysis.</p>\n</blockquote>\n<h3 id=\"industry-approaches-comparison\">Industry Approaches Comparison</h3>\n<p>Database vendors have developed three primary approaches to query optimization, each representing different solutions to the fundamental challenges outlined above. Understanding these approaches helps contextualize our design decisions and reveals why modern systems typically use hybrid strategies.</p>\n<h4 id=\"rule-based-optimization\">Rule-Based Optimization</h4>\n<p><strong>Rule-based optimizers</strong> apply a fixed set of transformation rules to convert SQL queries into execution plans. These rules are derived from database theory and empirical knowledge about query performance patterns. The optimizer applies rules in a predetermined sequence without considering data statistics or actual execution costs.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Description</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Decision Making</strong></td>\n<td>Fixed transformation rules applied in sequence</td>\n<td>Always push filters before joins; prefer index scans for high selectivity filters</td>\n</tr>\n<tr>\n<td><strong>Cost Consideration</strong></td>\n<td>No cost estimation - decisions based on rule priority</td>\n<td>Rule: &quot;Use index scan if filter selectivity &lt; 10%&quot; regardless of actual data</td>\n</tr>\n<tr>\n<td><strong>Optimization Time</strong></td>\n<td>Very fast - O(query size) complexity</td>\n<td>Optimization completes in microseconds regardless of query complexity</td>\n</tr>\n<tr>\n<td><strong>Plan Quality</strong></td>\n<td>Predictable but often suboptimal</td>\n<td>Consistent performance but misses opportunities for complex queries</td>\n</tr>\n</tbody></table>\n<p>Early database systems like Oracle&#39;s original optimizer used purely rule-based approaches. A typical rule might state: &quot;Always apply selection predicates before join operations&quot; or &quot;Use nested loop joins when one input is expected to be small.&quot; These rules work reasonably well for simple queries and provide predictable optimization time.</p>\n<p>However, rule-based optimization suffers from <strong>context blindness</strong>. The same rule is applied regardless of table sizes, data distribution, or available indexes. A rule that works well for small tables may be disastrous for large tables. This limitation becomes severe for complex analytical queries where optimal strategies depend heavily on data characteristics.</p>\n<blockquote>\n<p><strong>Historical Context</strong>: Rule-based optimization dominated early database systems because statistical information was expensive to collect and maintain. As storage became cheaper and workloads more complex, the limitations of rule-based approaches drove the industry toward cost-based optimization.</p>\n</blockquote>\n<h4 id=\"cost-based-optimization\">Cost-Based Optimization</h4>\n<p><strong>Cost-based optimizers</strong> make decisions by estimating and comparing the execution cost of different plan alternatives. These systems maintain detailed statistics about table sizes, column value distributions, and index characteristics. For each possible execution plan, the optimizer estimates I/O costs, CPU costs, and memory requirements, then selects the plan with the lowest total estimated cost.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Description</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Decision Making</strong></td>\n<td>Compare estimated costs of alternative plans</td>\n<td>Estimate hash join cost = 3 * (R + S) vs. nested loop cost = R * S for tables R, S</td>\n</tr>\n<tr>\n<td><strong>Statistics Usage</strong></td>\n<td>Extensive reliance on table and column statistics</td>\n<td>Uses row counts, distinct values, data distribution histograms for cost estimation</td>\n</tr>\n<tr>\n<td><strong>Search Strategy</strong></td>\n<td>Dynamic programming or heuristic search through plan space</td>\n<td>Enumerate all join orders for small queries; use heuristics for complex queries</td>\n</tr>\n<tr>\n<td><strong>Adaptivity</strong></td>\n<td>Plans change as statistics are updated</td>\n<td>Same query gets different plans as table sizes grow or indexes are added</td>\n</tr>\n</tbody></table>\n<p>Modern systems like PostgreSQL, SQL Server, and Oracle&#39;s current optimizer are primarily cost-based. These optimizers collect statistics about table cardinalities, column selectivity, and access method performance. When optimizing a join query, the optimizer estimates the cost of different join orders by calculating expected intermediate result sizes and operation costs.</p>\n<p>The power of cost-based optimization lies in its <strong>data-driven decision making</strong>. As tables grow larger, the optimizer automatically shifts from nested loop joins to hash joins. When new indexes are created, the optimizer incorporates them into cost calculations. This adaptivity allows cost-based systems to handle diverse workloads effectively.</p>\n<p>However, cost-based optimization introduces new complexities. Statistics must be collected, maintained, and kept reasonably current. Cost models must accurately reflect actual execution behavior across different hardware configurations. Most critically, the exponential search space requires sophisticated algorithms to find good plans without exhaustive enumeration.</p>\n<h4 id=\"heuristic-and-hybrid-approaches\">Heuristic and Hybrid Approaches</h4>\n<p><strong>Heuristic optimizers</strong> use problem-specific knowledge to guide the search for good execution plans without exhaustive cost calculation. Modern database systems increasingly adopt <strong>hybrid approaches</strong> that combine rule-based shortcuts, cost-based analysis, and search heuristics to balance optimization time with plan quality.</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Strategy</th>\n<th>Benefits</th>\n<th>Limitations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Greedy Heuristics</strong></td>\n<td>Make locally optimal choices at each step</td>\n<td>Fast optimization; works well for common patterns</td>\n<td>May miss globally optimal solutions</td>\n</tr>\n<tr>\n<td><strong>Genetic Algorithms</strong></td>\n<td>Evolve populations of candidate plans</td>\n<td>Handles complex search spaces; finds novel solutions</td>\n<td>Unpredictable optimization time; complex implementation</td>\n</tr>\n<tr>\n<td><strong>Machine Learning</strong></td>\n<td>Learn optimization patterns from historical data</td>\n<td>Adapts to workload patterns; improves over time</td>\n<td>Requires training data; black-box decision making</td>\n</tr>\n<tr>\n<td><strong>Hybrid Rule/Cost</strong></td>\n<td>Use rules for simple cases, costs for complex ones</td>\n<td>Fast common case; sophisticated complex case</td>\n<td>Complex implementation; rule/cost boundary decisions</td>\n</tr>\n</tbody></table>\n<p>PostgreSQL exemplifies the hybrid approach: it uses genetic algorithms for queries with many joins, cost-based optimization for moderately complex queries, and simple heuristics for trivial queries. This multi-tier strategy allows the optimizer to spend effort proportional to query complexity and potential benefit.</p>\n<p><strong>Apache Calcite</strong> represents a modern framework approach, separating logical optimization rules from cost-based physical planning. Rules handle transformations like predicate pushdown and join reordering, while cost-based analysis selects physical operators and access methods. This separation allows individual components to be optimized independently.</p>\n<blockquote>\n<p><strong>Decision: Cost-Based Foundation with Heuristic Pruning</strong></p>\n<ul>\n<li><strong>Context</strong>: Our educational optimizer must balance learning value with implementation complexity while remaining representative of production systems</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Pure rule-based system (simple but unrealistic)</li>\n<li>Full cost-based optimization (comprehensive but complex)</li>\n<li>Hybrid approach with cost-based core and heuristic pruning</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement cost-based optimization with heuristic pruning for search space management</li>\n<li><strong>Rationale</strong>: Cost-based optimization teaches the fundamental concepts used in production systems while heuristic pruning keeps the implementation manageable. Students learn both cost estimation and search space management.</li>\n<li><strong>Consequences</strong>: Enables realistic query optimization learning while avoiding the complexity of advanced search algorithms like genetic optimization or machine learning approaches.</li>\n</ul>\n</blockquote>\n<p>The comparison reveals that modern query optimization has evolved toward sophisticated hybrid systems that adapt their strategy based on query characteristics and optimization context. Our implementation will focus on cost-based optimization with strategic use of heuristics, providing hands-on experience with the core techniques that drive production database performance.</p>\n<h2 id=\"goals-and-non-goals\">Goals and Non-Goals</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - establishes the scope and boundaries that guide implementation decisions across Milestones 1-4.</p>\n</blockquote>\n<h3 id=\"primary-goals\">Primary Goals</h3>\n<p>Our query optimizer focuses on implementing the fundamental principles of cost-based optimization that form the backbone of modern database systems. Think of this as building a GPS navigation system for database queries - we need to understand the road network (available execution strategies), estimate travel times (costs), and find efficient routes (execution plans) while keeping the core navigation logic simple and reliable.</p>\n<p>The primary goal is to transform SQL queries into efficient <code>ExecutionPlan</code> structures through systematic cost-based decision making. This involves four core capabilities that work together to achieve effective query optimization. First, we must represent query operations as tree structures that capture both logical intent and physical execution strategies. Second, we need accurate cost estimation using <code>TableStatistics</code> to predict resource consumption. Third, we require sophisticated join ordering algorithms that use dynamic programming to find optimal <code>JoinOrder</code> sequences. Finally, we need physical operator selection that chooses concrete execution strategies based on data characteristics and available access methods.</p>\n<p><strong>Plan Representation and Tree Management</strong></p>\n<p>The optimizer must create and manipulate hierarchical query plans represented as trees of <code>OperatorNode</code> instances. Each node encapsulates a single database operation like scanning tables, applying filters, or performing joins. The system distinguishes between logical operators that express what computation should happen and physical operators that specify how the computation will execute. This separation allows the optimizer to explore multiple implementation strategies for the same logical operation.</p>\n<p>The plan representation system supports complete tree traversal for cost calculation, optimization rule application, and plan visualization. Tree manipulation capabilities include node insertion, removal, and subtree replacement to enable optimization transformations like predicate pushdown. The system maintains parent-child relationships that preserve query semantics while allowing structural modifications during optimization.</p>\n<p><strong>Statistical Cost Estimation</strong></p>\n<p>Accurate cost prediction drives all optimization decisions in our cost-based approach. The <code>estimateCost</code> function combines I/O costs from disk access patterns with CPU costs from computational complexity. The system maintains detailed <code>TableStatistics</code> including row counts, column cardinalities, and value distributions that enable selectivity estimation for filter predicates and join operations.</p>\n<p>Cost estimation covers all major resource consumption categories. I/O costs account for sequential scans, random page access, and index traversal patterns. CPU costs model comparison operations, hash computations, and sorting overhead. Memory costs track buffer usage for hash tables and intermediate results. The cost model uses configurable weights that can be tuned for different hardware configurations and workload patterns.</p>\n<p><strong>Join Order Optimization</strong></p>\n<p>Multi-table queries create exponential search spaces where join order dramatically affects execution efficiency. Our <code>optimizeJoinOrder</code> function implements dynamic programming algorithms that find optimal join sequences without exhaustive enumeration. The system builds optimal plans for progressively larger table subsets, reusing previously computed results to avoid redundant work.</p>\n<p>The join ordering component handles both simple star schemas and complex multi-join queries with intricate predicate patterns. It identifies cross products early and prunes clearly suboptimal alternatives to reduce search complexity. The algorithm supports both left-deep join trees that minimize memory usage and bushy trees that enable parallel execution opportunities.</p>\n<p><strong>Physical Operator Selection</strong></p>\n<p>The final optimization phase selects concrete physical operators and access methods for each logical operation. This includes choosing between sequential scans and index lookups based on filter selectivity, selecting join algorithms like hash joins or nested loops based on input characteristics, and applying optimization rules like predicate pushdown to reduce intermediate result sizes.</p>\n<p>Physical selection considers all available access paths including primary key indexes, secondary indexes, and full table scans. Join algorithm selection weighs factors like input cardinalities, memory availability, and sort order requirements. The system applies proven optimization heuristics while maintaining the flexibility to override defaults when cost analysis suggests better alternatives.</p>\n<p><strong>Educational Learning Objectives</strong></p>\n<p>Beyond functional requirements, this query optimizer serves as a comprehensive learning platform for understanding database internals. The implementation exposes the mathematical foundations of cost-based optimization including selectivity estimation formulas, cardinality calculation methods, and dynamic programming algorithms. Students gain hands-on experience with the trade-offs between optimization time and plan quality that drive real-world database design decisions.</p>\n<p>The modular architecture allows incremental implementation where each milestone builds upon previous foundations. This progression mirrors the historical development of query optimization techniques and helps learners understand why modern optimizers evolved their current architectures. The code structure emphasizes clarity and maintainability over performance optimization, making the underlying algorithms accessible to developers new to database systems.</p>\n<h3 id=\"explicit-non-goals\">Explicit Non-Goals</h3>\n<p>While comprehensive within its scope, our query optimizer deliberately excludes several advanced features that would complicate the core learning objectives. These non-goals reflect conscious decisions to maintain focus on fundamental optimization principles rather than pursuing production-level completeness.</p>\n<p><strong>Parallel Query Execution</strong></p>\n<p>Modern databases extensively use parallel processing to accelerate query execution across multiple CPU cores and storage devices. However, parallel query optimization introduces significant complexity in cost modeling, resource allocation, and synchronization overhead estimation. Parallel plans require sophisticated scheduling algorithms, partition-aware statistics, and inter-operator communication protocols that would overshadow the core optimization concepts we aim to teach.</p>\n<p>Our single-threaded execution model simplifies cost calculations and eliminates complex race conditions during plan generation. This allows students to focus on understanding join ordering algorithms and selectivity estimation without wrestling with parallel programming challenges. The fundamental optimization principles learned in our single-threaded system transfer directly to parallel contexts once the core concepts are mastered.</p>\n<p><strong>Runtime Adaptive Optimization</strong></p>\n<p>Production optimizers increasingly use runtime feedback to improve cost estimates and plan selection over time. Adaptive optimization tracks actual execution statistics, identifies estimation errors, and adjusts cost models based on observed performance patterns. Some systems even reoptimize queries mid-execution when initial estimates prove significantly inaccurate.</p>\n<p>These adaptive features require complex statistics collection infrastructure, machine learning components for pattern recognition, and sophisticated plan migration mechanisms. Including adaptive optimization would shift focus from fundamental cost-based techniques to machine learning and runtime system design. Our static optimization approach provides a solid foundation that students can extend with adaptive features in future projects.</p>\n<p><strong>Advanced Statistical Models</strong></p>\n<p>Real-world optimizers employ sophisticated statistical techniques including multi-dimensional histograms, correlation detection between columns, and probabilistic data structures for cardinality estimation. These advanced models provide better estimate accuracy but require substantial mathematical background and complex maintenance algorithms.</p>\n<p>Our simplified statistics model uses uniform distribution assumptions and independence assumptions between predicates. While less accurate than production systems, this approach makes cost calculation transparent and debuggable. Students can understand every step of the estimation process without advanced statistical knowledge, building intuition for why more sophisticated models become necessary in production environments.</p>\n<p><strong>Materialized View Selection</strong></p>\n<p>Query optimizers in data warehouse environments consider rewriting queries to use precomputed materialized views when available. This capability requires view matching algorithms, freshness tracking, and complex cost comparisons between base table access and view utilization. Materialized view optimization represents a distinct area of research with its own algorithmic challenges.</p>\n<p>Excluding materialized views keeps our focus on fundamental table access patterns and join optimization. Students learn to optimize queries against base tables, understanding the core cost-benefit analysis that drives all optimization decisions. This foundation enables future exploration of view-based optimization techniques.</p>\n<p><strong>Distributed Query Processing</strong></p>\n<p>Distributed databases must optimize queries across multiple network-connected nodes, considering data locality, network transfer costs, and partial failure scenarios. Distributed optimization requires understanding of data partitioning strategies, network cost modeling, and distributed join algorithms like shuffle joins and broadcast joins.</p>\n<p>Our single-node optimizer eliminates network complexity and allows focus on local cost optimization. The principles of cost-based decision making and dynamic programming transfer directly to distributed contexts, but students first master these concepts without distributed systems complexity.</p>\n<p><strong>User-Defined Function Integration</strong></p>\n<p>Production optimizers handle user-defined functions with unknown cost characteristics and potential side effects. This requires sophisticated cost estimation for black-box operations, consideration of function volatility and determinism, and integration with external programming language runtimes.</p>\n<p>Our operator set focuses on standard relational operators with well-understood cost characteristics. This restriction enables precise cost modeling and predictable optimization behavior. Students learn optimization principles with full visibility into cost calculations rather than dealing with estimation uncertainty from external functions.</p>\n<p><strong>Memory Management Integration</strong></p>\n<p>Advanced optimizers consider available memory when selecting physical operators, potentially choosing different join algorithms based on buffer pool size or enabling spill-to-disk strategies for memory-intensive operations. This requires integration with buffer pool managers and sophisticated memory cost modeling.</p>\n<p>Our optimizer assumes sufficient memory for chosen operations, simplifying cost calculations and eliminating complex memory management scenarios. This allows focus on algorithmic aspects of optimization rather than resource management concerns. The fundamental cost comparison techniques learned with unlimited memory assumptions extend naturally to memory-constrained environments.</p>\n<p><strong>Query Plan Caching with Invalidation</strong></p>\n<p>Production systems cache optimized plans across query executions, implementing sophisticated invalidation strategies when underlying table statistics change significantly. Plan caching requires cache key generation from SQL syntax trees, staleness detection algorithms, and memory management for cached plan storage.</p>\n<p>While our implementation includes basic plan caching for repeated identical queries, we exclude complex invalidation logic and adaptive cache management. This simplification allows focus on optimization algorithms rather than cache management strategies. Students understand the value of plan reuse without implementing production-level cache sophistication.</p>\n<blockquote>\n<p><strong>Design Insight</strong>: These non-goals reflect a deliberate pedagogical strategy. By excluding advanced features, we create a focused learning environment where students master fundamental concepts before encountering production complexity. Each excluded feature represents a natural extension point for advanced projects once core optimization skills are developed.</p>\n</blockquote>\n<p>The scope boundaries ensure that implementation effort concentrates on cost-based optimization principles rather than distributed systems, parallel programming, or machine learning techniques. Students emerge with deep understanding of query optimization mathematics and algorithms that form the foundation for all advanced database optimization techniques.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This guidance helps you structure your query optimizer implementation to achieve the primary goals while respecting the explicit boundaries we&#39;ve established.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plan Tree Storage</td>\n<td>In-memory Python classes with references</td>\n<td>Persistent plan cache with SQLite backend</td>\n</tr>\n<tr>\n<td>Statistics Collection</td>\n<td>Manual table scanning with basic counters</td>\n<td>Sampling-based statistics with histogram approximation</td>\n</tr>\n<tr>\n<td>Cost Calculation</td>\n<td>Simple arithmetic with fixed weights</td>\n<td>Configurable cost models with JSON parameter files</td>\n</tr>\n<tr>\n<td>Join Enumeration</td>\n<td>Recursive enumeration with memoization</td>\n<td>Iterator-based dynamic programming with pruning</td>\n</tr>\n<tr>\n<td>Plan Visualization</td>\n<td>Text-based tree printing with indentation</td>\n<td>Graphical plan display with graphviz integration</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>Basic unittest with hardcoded test cases</td>\n<td>Property-based testing with hypothesis for edge cases</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure</strong></p>\n<p>Organize your query optimizer code to maintain clear separation between the four main components while enabling easy testing and extension:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>query_optimizer/\n  __init__.py                     ← package initialization\n  main.py                        ← CLI entry point for testing optimizer\n  \n  plan/\n    __init__.py\n    operators.py                  ← OperatorNode hierarchy and ExecutionPlan\n    tree_builder.py              ← plan tree construction and manipulation\n    visualizer.py                ← plan pretty-printing and debugging output\n    \n  cost/\n    __init__.py\n    statistics.py                ← TableStatistics collection and storage\n    estimator.py                 ← estimateCost implementation\n    models.py                    ← CostEstimate structure and calculation helpers\n    \n  join/\n    __init__.py\n    optimizer.py                 ← optimizeJoinOrder dynamic programming\n    enumeration.py               ← generatePlans for join combinations\n    pruning.py                   ← search space reduction heuristics\n    \n  physical/\n    __init__.py\n    selector.py                  ← physical operator selection logic\n    rules.py                     ← optimization rules like predicate pushdown\n    access_methods.py            ← index vs scan selection algorithms\n    \n  utils/\n    __init__.py\n    query_parser.py              ← simple SQL parsing for testing\n    test_data.py                 ← sample tables and statistics for experiments\n    \n  tests/\n    __init__.py\n    test_plan_representation.py  ← Milestone 1 tests\n    test_cost_estimation.py      ← Milestone 2 tests\n    test_join_ordering.py        ← Milestone 3 tests\n    test_physical_selection.py   ← Milestone 4 tests\n    integration_tests.py         ← end-to-end optimization tests</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<p>Here&#39;s complete starter infrastructure that handles the non-core components, letting you focus on optimization algorithms:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># utils/query_parser.py - Simple SQL parsing for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParsedQuery</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    columns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    joins: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># (left_table, right_table, condition)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filters: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># (table, column, operator, value)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SimpleQueryParser</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Basic SQL parser for testing query optimization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Handles SELECT queries with JOINs and WHERE clauses.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Not meant for production - just enables optimizer testing.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> parse</span><span style=\"color:#E1E4E8\">(self, sql: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> ParsedQuery:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sql.strip().upper()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract SELECT columns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        select_match </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.search(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">SELECT</span><span style=\"color:#79B8FF\">\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">(.</span><span style=\"color:#F97583\">+?</span><span style=\"color:#79B8FF\">)\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#DBEDFF\">FROM</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, sql)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        columns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [col.strip() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> select_match.group(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">).split(</span><span style=\"color:#9ECBFF\">','</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract FROM tables and JOINs</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        from_match </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.search(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">FROM</span><span style=\"color:#79B8FF\">\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">(.</span><span style=\"color:#F97583\">+?</span><span style=\"color:#79B8FF\">)(?:\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#DBEDFF\">WHERE</span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\">$)</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, sql)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        from_clause </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> from_match.group(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tables </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        joins </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Simple table extraction - extend for complex JOIN syntax</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        table_parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.split(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#DBEDFF\">JOIN</span><span style=\"color:#79B8FF\">\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, from_clause)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        tables.append(table_parts[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].strip())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> join_part </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> table_parts[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">:]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            on_match </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.search(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">(\\w</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#DBEDFF\">ON</span><span style=\"color:#79B8FF\">\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">(.</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, join_part)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> on_match:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                table </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> on_match.group(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                condition </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> on_match.group(</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tables.append(table)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                joins.append((tables[</span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\">2</span><span style=\"color:#E1E4E8\">], table, condition))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Extract WHERE filters</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        filters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        where_match </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.search(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#DBEDFF\">WHERE</span><span style=\"color:#79B8FF\">\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">(.</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, sql)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> where_match:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Basic filter parsing - extend for complex predicates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conditions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> where_match.group(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">).split(</span><span style=\"color:#9ECBFF\">' AND '</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> condition </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> conditions:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                filter_match </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.match(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">(\\w</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">(\\w</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)\\s</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">(</span><span style=\"color:#DBEDFF\">=</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">&#x3C;</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">></span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">&#x3C;=</span><span style=\"color:#F97583\">|</span><span style=\"color:#DBEDFF\">>=</span><span style=\"color:#79B8FF\">)\\s</span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\">(.</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, condition.strip())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> filter_match:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    table, column, op, value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> filter_match.groups()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    filters.append((table, column, op, value))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> ParsedQuery(tables, columns, joins, filters)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># utils/test_data.py - Sample data for testing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestDataGenerator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generates sample table statistics and test queries for optimizer validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_sample_statistics</span><span style=\"color:#E1E4E8\">() -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'TableStatistics'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Creates realistic table statistics for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        from</span><span style=\"color:#E1E4E8\"> cost.statistics </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TableStatistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Customer table - typical e-commerce scenario</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        customers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TableStatistics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            table_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"customers\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            row_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            column_stats</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"customer_id\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">100000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"country\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">50</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.02</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"age\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">80</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Orders table - 5x customer count (average 5 orders per customer)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        orders </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TableStatistics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            table_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"orders\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            row_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">500000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            column_stats</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"order_id\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">500000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"customer_id\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">80000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">},  </span><span style=\"color:#6A737D\"># 80% customers have orders</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"order_date\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"status\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Products table - catalog of items</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        products </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TableStatistics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            table_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"products\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            row_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            column_stats</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"product_id\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"category\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                \"price\"</span><span style=\"color:#E1E4E8\">: {</span><span style=\"color:#9ECBFF\">\"distinct_count\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">5000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"null_fraction\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span><span style=\"color:#9ECBFF\">\"customers\"</span><span style=\"color:#E1E4E8\">: customers, </span><span style=\"color:#9ECBFF\">\"orders\"</span><span style=\"color:#E1E4E8\">: orders, </span><span style=\"color:#9ECBFF\">\"products\"</span><span style=\"color:#E1E4E8\">: products}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">staticmethod</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_test_queries</span><span style=\"color:#E1E4E8\">() -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Returns SQL queries of increasing complexity for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Single table with filter</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"SELECT * FROM customers WHERE country = 'USA'\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Two-table join</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"SELECT c.name, o.order_date FROM customers c JOIN orders o ON c.customer_id = o.customer_id\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Three-table join with filters</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"SELECT c.name, p.name FROM customers c JOIN orders o ON c.customer_id = o.customer_id JOIN products p ON o.product_id = p.product_id WHERE c.country = 'USA' AND p.category = 'Electronics'\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Complex multi-join query</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"SELECT c.country, COUNT(*) FROM customers c JOIN orders o ON c.customer_id = o.customer_id JOIN products p ON o.product_id = p.product_id WHERE p.price > 100 GROUP BY c.country\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeletons</strong></p>\n<p>Here are the main function signatures with detailed TODOs that map to the algorithm steps described in the design sections:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># cost/estimator.py - Core cost estimation logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> plan.operators </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExecutionPlan, OperatorNode</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> cost.models </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> cost.statistics </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TableStatistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> estimateCost</span><span style=\"color:#E1E4E8\">(plan: ExecutionPlan, stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate predicted execution cost for query plan.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Combines I/O costs from table access with CPU costs from operations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Uses bottom-up tree traversal to aggregate costs from leaves to root.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize cost accumulator with zero I/O, CPU, and memory costs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Perform post-order traversal of plan tree starting from leaves</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each Scan operator, calculate I/O cost based on table size and selectivity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each Join operator, estimate output cardinality and CPU cost for join algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each Filter operator, calculate CPU cost for predicate evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Aggregate child costs and add operator-specific costs at each node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Return total CostEstimate with I/O, CPU breakdown for debugging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use plan.root.accept(CostCalculationVisitor(stats)) for clean traversal</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># join/optimizer.py - Dynamic programming join ordering  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Set</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> cost.statistics </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TableStatistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> optimizeJoinOrder</span><span style=\"color:#E1E4E8\">(tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], predicates: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">], stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]) -> </span><span style=\"color:#9ECBFF\">'JoinOrder'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Find efficient join sequence using dynamic programming.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Builds optimal plans for progressively larger table subsets.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Avoids exponential enumeration through memoization of subproblem solutions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize memoization table for optimal costs of table subsets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Create base case entries for single-table access plans</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Iterate through subset sizes from 2 to len(tables) </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: For each subset, try all possible ways to split into two smaller subsets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Calculate join cost for each split using cost estimation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Store minimum cost plan for this subset in memoization table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Prune subsets that would create cross products (no join predicates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 8: Return JoinOrder representing optimal sequence from memoization table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use itertools.combinations to generate subsets systematically</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># physical/selector.py - Physical operator selection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> selectPhysicalOperators</span><span style=\"color:#E1E4E8\">(logical_plan: ExecutionPlan, stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Choose concrete physical operators for logical plan.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Considers available indexes, input cardinalities, and memory constraints.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    Applies optimization rules like predicate pushdown during selection.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Traverse logical plan tree to identify operator selection opportunities</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each Scan operator, choose between sequential scan and index scan based on selectivity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For each Join operator, select hash join vs nested loop based on input sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Apply predicate pushdown rules to move filters below joins where beneficial</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Consider sort order requirements for downstream operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Generate new ExecutionPlan with selected physical operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Validate that physical plan maintains logical semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hint: Use SELECTIVITY_THRESHOLD constant to decide scan vs index access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<p>Python-specific implementation guidance for common optimization tasks:</p>\n<ul>\n<li><p><strong>Tree Traversal</strong>: Use the visitor pattern with <code>operator.accept(visitor)</code> methods for clean plan tree navigation. Implement separate visitors for cost calculation, plan printing, and optimization rule application.</p>\n</li>\n<li><p><strong>Memoization</strong>: Use <code>functools.lru_cache</code> decorator or manual dictionary caching for dynamic programming subproblems. Cache optimal costs keyed by frozenset of table names to handle subset enumeration efficiently.</p>\n</li>\n<li><p><strong>Statistics Storage</strong>: Store <code>TableStatistics</code> in simple dictionaries initially. For persistence, use pickle files or JSON serialization. Avoid database storage complexity during initial implementation.</p>\n</li>\n<li><p><strong>Cost Calculation</strong>: Use <code>dataclasses</code> for <code>CostEstimate</code> to get automatic equality, string representation, and arithmetic operations. Implement <code>__add__</code> method for combining costs from child operators.</p>\n</li>\n<li><p><strong>Combinatorial Enumeration</strong>: Use <code>itertools.combinations</code> and <code>itertools.permutations</code> for generating join orders and table subsets. Be careful with exponential growth - add <code>MAX_JOIN_ENUMERATION</code> limits early.</p>\n</li>\n<li><p><strong>Plan Visualization</strong>: Implement <code>__str__</code> methods on operator nodes for debugging. Use string indentation based on tree depth for readable plan printing. Consider <code>graphviz</code> integration for complex query visualization.</p>\n</li>\n</ul>\n<p><strong>F. Milestone Checkpoints</strong></p>\n<p>After implementing each milestone, verify your progress with these concrete tests:</p>\n<p><strong>Milestone 1 - Plan Representation</strong>: Run <code>python -m pytest tests/test_plan_representation.py</code>. Expected behavior: Create operator trees, traverse parent-child relationships, pretty-print plans with proper indentation. Warning signs: Stack overflow in tree traversal, missing operator types, incorrect parent-child links.</p>\n<p><strong>Milestone 2 - Cost Estimation</strong>: Execute <code>python -c &quot;from cost.estimator import estimateCost; print(estimateCost(sample_plan, test_stats))&quot;</code>. Expected output: Reasonable cost numbers with I/O dominated by large table scans, CPU costs proportional to intermediate result sizes. Warning signs: Negative costs, infinite costs, identical costs for very different plans.</p>\n<p><strong>Milestone 3 - Join Ordering</strong>: Test with <code>python -c &quot;from join.optimizer import optimizeJoinOrder; print(optimizeJoinOrder([&#39;A&#39;,&#39;B&#39;,&#39;C&#39;], predicates, stats))&quot;</code>. Expected behavior: Different join orders for different table size combinations, avoidance of cross products, reasonable execution time even for 5-6 table queries. Warning signs: Exponential slowdown, cross product plans, identical orders regardless of statistics.</p>\n<p><strong>Milestone 4 - Physical Selection</strong>: Validate with end-to-end test: <code>python main.py --query &quot;SELECT * FROM customers WHERE country=&#39;USA&#39;&quot;</code>. Expected output: Index scan chosen for highly selective filters, sequential scan for low selectivity, hash joins for large input tables. Warning signs: Always choosing same physical operators, ignoring selectivity estimates, missing predicate pushdown opportunities.</p>\n<h2 id=\"high-level-architecture\">High-Level Architecture</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for all milestones - establishes the overall system design that guides implementation across Milestones 1-4.</p>\n</blockquote>\n<h3 id=\"component-responsibilities\">Component Responsibilities</h3>\n<p>Think of the query optimizer as a <strong>construction project planning office</strong> where different specialists collaborate to design the most efficient way to build a complex structure. Just as architects, cost estimators, project managers, and construction supervisors each bring specialized expertise to create an optimal building plan, our query optimizer employs four specialized components that work together to transform a SQL query into an efficient execution strategy.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"Query Optimizer System Architecture\"></p>\n<p>The <strong>Plan Builder</strong> serves as the initial architect, taking the raw SQL requirements and creating the foundational blueprint. This component receives a <code>ParsedQuery</code> from the SQL parser and constructs the basic logical plan tree, defining what operations need to happen without yet specifying exactly how they&#39;ll be executed. The Plan Builder understands the relational algebra behind SQL operations and can represent queries as hierarchical trees of <code>OperatorNode</code> objects, each representing a fundamental database operation like scanning a table, applying filters, or joining datasets.</p>\n<p>The Plan Builder&#39;s primary responsibility centers on <strong>structural correctness</strong> rather than efficiency. It ensures that the logical plan accurately represents the query semantics - that all required tables are accessed, all join conditions are properly represented, all filter predicates are applied, and the correct columns are projected in the final result. Think of this as the architect ensuring that the building blueprint includes all required rooms, proper connections between spaces, and meets all functional requirements, without yet worrying about construction costs or timing.</p>\n<table>\n<thead>\n<tr>\n<th>Responsibility</th>\n<th>Description</th>\n<th>Input</th>\n<th>Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parse SQL to logical operations</td>\n<td>Convert SQL syntax into relational algebra operations</td>\n<td><code>ParsedQuery</code> with tables, columns, joins, filters</td>\n<td>Logical <code>ExecutionPlan</code> tree</td>\n</tr>\n<tr>\n<td>Validate query semantics</td>\n<td>Ensure all referenced tables and columns exist and are accessible</td>\n<td>Schema metadata, query structure</td>\n<td>Validated logical plan or error</td>\n</tr>\n<tr>\n<td>Handle operator precedence</td>\n<td>Apply proper order of operations for complex predicates</td>\n<td>Complex WHERE clauses with AND/OR/NOT</td>\n<td>Correctly structured filter tree</td>\n</tr>\n<tr>\n<td>Normalize plan structure</td>\n<td>Convert to canonical form for optimization</td>\n<td>Raw logical plan</td>\n<td>Normalized <code>ExecutionPlan</code></td>\n</tr>\n</tbody></table>\n<p>The <strong>Cost Estimator</strong> functions as the project cost analyst, evaluating how expensive each potential execution strategy will be in terms of computational resources. This component maintains detailed <code>TableStatistics</code> about data distribution, cardinality, and storage characteristics, using this information to predict the resource consumption of different plan alternatives. The Cost Estimator must balance accuracy with computational efficiency - spending too much time on precise estimates can make optimization slower than the query execution itself.</p>\n<p>The Cost Estimator&#39;s calculations encompass multiple resource dimensions: I/O operations for reading data from storage, CPU cycles for processing operations like joins and filters, and memory consumption for intermediate results and operator state. Each prediction involves statistical modeling based on assumptions about data distribution and correlation patterns. The component provides <code>CostEstimate</code> objects that quantify both the total resource consumption and the expected cardinality of intermediate results, enabling meaningful comparison between alternative execution strategies.</p>\n<table>\n<thead>\n<tr>\n<th>Cost Component</th>\n<th>Estimation Method</th>\n<th>Key Statistics Used</th>\n<th>Accuracy Factors</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>I/O Operations</td>\n<td>Pages accessed × page read cost</td>\n<td>Table size, index selectivity</td>\n<td>Buffer pool hit rate, storage type</td>\n</tr>\n<tr>\n<td>CPU Processing</td>\n<td>Tuple count × operation complexity</td>\n<td>Row counts, predicate selectivity</td>\n<td>CPU cache effects, data types</td>\n</tr>\n<tr>\n<td>Memory Usage</td>\n<td>Intermediate result sizes</td>\n<td>Join cardinalities, sort buffer needs</td>\n<td>Available memory, spill-to-disk costs</td>\n</tr>\n<tr>\n<td>Network Transfer</td>\n<td>Result size × network latency</td>\n<td>Output cardinality, tuple width</td>\n<td>Network bandwidth, serialization overhead</td>\n</tr>\n</tbody></table>\n<p>The <strong>Join Optimizer</strong> serves as the project scheduling specialist, determining the optimal sequence for multi-table operations. This component faces the classic combinatorial optimization challenge: with n tables to join, there are factorial possible orderings, creating an exponential search space that quickly becomes computationally intractable. The Join Optimizer employs dynamic programming algorithms to find optimal <code>JoinOrder</code> sequences while using pruning heuristics to avoid exploring clearly suboptimal alternatives.</p>\n<p>The fundamental insight driving join optimization is that the order of operations dramatically affects intermediate result sizes, which in turn determines the cost of subsequent operations. A poorly chosen join order might create massive intermediate results early in the execution, making all following operations expensive. Conversely, an optimal ordering performs the most selective joins first, keeping intermediate results small and making the overall query execution efficient.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: Join ordering optimization often provides the largest performance gains in query optimization. A 10x difference in execution time between good and bad join orders is common for queries involving 4+ tables.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Optimization Strategy</th>\n<th>Algorithm</th>\n<th>Search Space</th>\n<th>Time Complexity</th>\n<th>Quality</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Complete enumeration</td>\n<td>Dynamic programming</td>\n<td>All valid join orders</td>\n<td>O(n! × 2^n)</td>\n<td>Optimal</td>\n</tr>\n<tr>\n<td>Greedy heuristics</td>\n<td>Selectivity-based ordering</td>\n<td>Single path</td>\n<td>O(n²)</td>\n<td>Good approximation</td>\n</tr>\n<tr>\n<td>Genetic algorithms</td>\n<td>Evolutionary search</td>\n<td>Sampled population</td>\n<td>O(generations × population)</td>\n<td>Variable</td>\n</tr>\n<tr>\n<td>Left-deep restriction</td>\n<td>DP on linear plans</td>\n<td>Left-deep trees only</td>\n<td>O(n³)</td>\n<td>Suboptimal but fast</td>\n</tr>\n</tbody></table>\n<p>The <strong>Physical Planner</strong> acts as the construction method specialist, making concrete decisions about how to implement each logical operation using available physical operators and access methods. While the logical plan specifies <em>what</em> operations to perform, the physical planner determines <em>how</em> to perform them by selecting specific algorithms, access paths, and optimization techniques like predicate pushdown.</p>\n<p>The Physical Planner must consider the available infrastructure: what indexes exist, what join algorithms are supported, whether hash tables fit in memory, and how to minimize data movement. It applies rule-based optimizations like pushing filter predicates down to table scans and chooses between alternative physical operators like hash joins versus nested loop joins based on estimated data sizes and characteristics.</p>\n<table>\n<thead>\n<tr>\n<th>Physical Decision</th>\n<th>Selection Criteria</th>\n<th>Options Considered</th>\n<th>Impact on Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table access method</td>\n<td>Selectivity vs index availability</td>\n<td>Sequential scan, index scan, index-only scan</td>\n<td>10-100x difference in I/O</td>\n</tr>\n<tr>\n<td>Join algorithm</td>\n<td>Input sizes, memory availability</td>\n<td>Hash join, nested loop, merge join</td>\n<td>5-50x difference in CPU/memory</td>\n</tr>\n<tr>\n<td>Sort implementation</td>\n<td>Data size vs memory</td>\n<td>In-memory quicksort, external merge sort</td>\n<td>Memory usage and I/O patterns</td>\n</tr>\n<tr>\n<td>Predicate placement</td>\n<td>Operator tree topology</td>\n<td>Early filtering vs late filtering</td>\n<td>Intermediate result sizes</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Decision: Component Separation Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: Query optimization involves multiple distinct types of analysis (structural, statistical, algorithmic, physical) that could be combined into a monolithic optimizer or separated into specialized components.</li>\n<li><strong>Options Considered</strong>: Monolithic optimizer class, pipeline of specialized components, pluggable optimization framework</li>\n<li><strong>Decision</strong>: Separate specialized components with well-defined interfaces</li>\n<li><strong>Rationale</strong>: Each optimization phase requires different expertise, data structures, and algorithms. Separation enables independent testing, easier debugging, and potential parallel execution of optimization phases. The clean interfaces make the system more maintainable and allow for component replacement or enhancement.</li>\n<li><strong>Consequences</strong>: Enables modular development and testing, introduces coordination overhead between components, requires careful interface design to avoid tight coupling.</li>\n</ul>\n</blockquote>\n<h3 id=\"optimization-pipeline\">Optimization Pipeline</h3>\n<p>The query optimization process flows through a carefully orchestrated sequence of transformations, each building upon the previous stage&#39;s output while adding specialized analysis and refinement. Think of this as an <strong>assembly line for decision-making</strong>, where each station adds specific expertise to transform raw requirements into a precisely engineered execution plan.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Foptimization-workflow.svg\" alt=\"Optimization Process Workflow\"></p>\n<p>The pipeline begins when a <code>ParsedQuery</code> arrives from the SQL parser, containing the basic structural information extracted from the SQL statement: referenced tables, required columns, join conditions, and filter predicates. This parsed representation provides the raw materials for optimization but lacks the sophisticated analysis needed for efficient execution.</p>\n<p><strong>Stage 1: Logical Plan Construction</strong> marks the entry point where the Plan Builder transforms the flat SQL structure into a hierarchical execution plan. The builder creates a tree of logical operators, each representing a fundamental relational operation. At this stage, the plan accurately represents the query semantics but contains no performance optimizations or physical implementation details.</p>\n<p>The logical planning process involves several critical steps:</p>\n<ol>\n<li><p><strong>Table identification and alias resolution</strong> - The builder examines the FROM clause to identify all data sources and establishes a mapping between table aliases and actual table names, validating that all referenced tables exist and are accessible.</p>\n</li>\n<li><p><strong>Join condition extraction</strong> - The optimizer parses the WHERE clause and ON clauses to identify explicit join conditions, distinguishing them from filter predicates that apply to individual tables.</p>\n</li>\n<li><p><strong>Filter predicate categorization</strong> - The system categorizes WHERE clause predicates by the tables they reference, preparing them for potential pushdown optimization in later stages.</p>\n</li>\n<li><p><strong>Projection analysis</strong> - The builder examines the SELECT clause to determine which columns must be available at each stage of execution, enabling column pruning optimizations.</p>\n</li>\n<li><p><strong>Operator tree construction</strong> - The optimizer arranges logical operators into a preliminary tree structure, typically following SQL&#39;s natural evaluation order without yet considering performance implications.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Logical Plan Element</th>\n<th>Source SQL Clause</th>\n<th>Validation Required</th>\n<th>Downstream Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table scan operators</td>\n<td>FROM clause</td>\n<td>Table existence, access permissions</td>\n<td>Available columns, cardinality estimates</td>\n</tr>\n<tr>\n<td>Join operators</td>\n<td>WHERE/ON conditions</td>\n<td>Column compatibility, join predicate validity</td>\n<td>Search space for join ordering</td>\n</tr>\n<tr>\n<td>Filter operators</td>\n<td>WHERE predicates</td>\n<td>Data type consistency, function availability</td>\n<td>Selectivity estimation input</td>\n</tr>\n<tr>\n<td>Projection operators</td>\n<td>SELECT clause</td>\n<td>Column existence, expression validity</td>\n<td>Output schema definition</td>\n</tr>\n</tbody></table>\n<p><strong>Stage 2: Statistics Collection and Cost Model Preparation</strong> runs concurrently with logical planning, gathering the statistical foundation needed for cost-based optimization. The Cost Estimator collects current <code>TableStatistics</code> for all referenced tables, including row counts, column cardinalities, data distribution histograms, and index characteristics.</p>\n<p>This statistical foundation enables accurate resource consumption predictions throughout the optimization process. The Cost Estimator builds mathematical models that can predict how many rows will survive filter operations (selectivity estimation), how large intermediate join results will become (cardinality estimation), and how much I/O, CPU, and memory each operation will consume.</p>\n<p><strong>Stage 3: Join Order Optimization</strong> represents the algorithmic heart of query optimization for multi-table queries. The Join Optimizer applies dynamic programming techniques to explore the space of possible join orderings, using cost estimates to identify the optimal sequence. This stage must balance optimization quality against optimization time, potentially switching from complete enumeration to heuristic approaches for queries involving many tables.</p>\n<p>The join optimization algorithm proceeds through systematic subset enumeration:</p>\n<ol>\n<li><p><strong>Single-table access path optimization</strong> - For each individual table, the optimizer determines the best access method (sequential scan versus available indexes) based on filter selectivity and index characteristics.</p>\n</li>\n<li><p><strong>Two-table join optimization</strong> - The algorithm evaluates all possible pairings of tables, considering different join algorithms and accessing methods for each combination.</p>\n</li>\n<li><p><strong>Progressive subset expansion</strong> - Using dynamic programming, the optimizer builds optimal plans for progressively larger table subsets, reusing previously computed optimal subplans.</p>\n</li>\n<li><p><strong>Pruning and bounds checking</strong> - The algorithm eliminates clearly suboptimal partial plans early, avoiding exponential explosion in the search space.</p>\n</li>\n<li><p><strong>Cross product detection and elimination</strong> - The optimizer identifies and heavily penalizes or eliminates join combinations that lack proper join predicates, avoiding expensive Cartesian products.</p>\n</li>\n</ol>\n<p><strong>Stage 4: Physical Operator Selection</strong> transforms the optimal logical plan into an executable physical plan by making concrete implementation choices. The Physical Planner selects specific algorithms for each operation, chooses access methods for table scans, and applies rule-based optimizations like predicate pushdown.</p>\n<p>Physical planning decisions include:</p>\n<ul>\n<li><strong>Access method selection</strong>: Choosing between sequential scans, index scans, or index-only scans based on predicate selectivity and available indexes</li>\n<li><strong>Join algorithm selection</strong>: Selecting hash joins, nested loop joins, or merge joins based on input cardinalities and memory availability  </li>\n<li><strong>Sort algorithm selection</strong>: Choosing in-memory sorting versus external merge sorts based on data sizes and memory constraints</li>\n<li><strong>Predicate pushdown application</strong>: Moving filter operations as close to data sources as possible to minimize intermediate result sizes</li>\n<li><strong>Projection pushdown application</strong>: Eliminating unnecessary columns early in the execution pipeline to reduce data movement</li>\n</ul>\n<blockquote>\n<p><strong>Key Design Insight</strong>: The pipeline maintains clean separation between logical correctness (stages 1-2) and performance optimization (stages 3-4). This separation enables independent testing and debugging of optimization logic while ensuring that performance optimizations never compromise query correctness.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Pipeline Stage</th>\n<th>Input</th>\n<th>Processing</th>\n<th>Output</th>\n<th>Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logical Planning</td>\n<td><code>ParsedQuery</code></td>\n<td>Semantic analysis, tree construction</td>\n<td>Logical <code>ExecutionPlan</code></td>\n<td>Schema consistency, referential integrity</td>\n</tr>\n<tr>\n<td>Statistics Collection</td>\n<td>Table names, filter predicates</td>\n<td>Catalog lookup, histogram analysis</td>\n<td><code>TableStatistics</code></td>\n<td>Freshness checks, missing statistics detection</td>\n</tr>\n<tr>\n<td>Join Optimization</td>\n<td>Logical plan, statistics</td>\n<td>Dynamic programming, pruning</td>\n<td>Optimal <code>JoinOrder</code></td>\n<td>Cost sanity checks, cross product detection</td>\n</tr>\n<tr>\n<td>Physical Planning</td>\n<td>Logical plan, join order</td>\n<td>Algorithm selection, rule application</td>\n<td>Physical <code>ExecutionPlan</code></td>\n<td>Resource constraint validation</td>\n</tr>\n</tbody></table>\n<p><strong>Stage 5: Plan Finalization and Caching</strong> completes the optimization process by performing final validation, cost annotation, and storage for potential reuse. The optimizer ensures that the selected physical plan satisfies all resource constraints, annotates the plan with detailed cost estimates for execution monitoring, and stores the optimized plan in the plan cache for reuse with similar queries.</p>\n<p>The optimization pipeline maintains <strong>state machines</strong> that track the progression through different optimization phases, enabling recovery from failures and providing detailed debugging information when optimization produces unexpected results.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Foptimizer-state-machine.svg\" alt=\"Optimizer State Transitions\"></p>\n<p>Error handling throughout the pipeline follows a <strong>fail-fast principle</strong>: each stage validates its inputs and outputs, immediately detecting inconsistencies or constraint violations. When optimization fails, the system provides detailed diagnostic information identifying the specific stage and condition that caused the failure, along with suggestions for resolution.</p>\n<h3 id=\"recommended-file-structure\">Recommended File Structure</h3>\n<p>The query optimizer implementation benefits from a modular file organization that mirrors the component architecture while providing clear separation of concerns and easy navigation for developers. This structure supports incremental development through the project milestones while maintaining clean interfaces between components.</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>query_optimizer/\n├── README.md                           # Project overview and build instructions\n├── requirements.txt                    # Python dependencies\n├── setup.py                           # Package configuration\n│\n├── optimizer/                         # Main optimizer package\n│   ├── __init__.py                    # Package initialization, main API\n│   ├── types.py                       # Core data structures and enums\n│   ├── exceptions.py                  # Optimizer-specific exception classes\n│   └── config.py                      # Configuration constants and settings\n│\n├── optimizer/parser/                  # SQL parsing (prerequisite)\n│   ├── __init__.py\n│   ├── sql_parser.py                  # ParsedQuery generation\n│   └── query_validator.py             # Semantic validation\n│\n├── optimizer/plan/                    # Plan representation (Milestone 1)\n│   ├── __init__.py\n│   ├── operators.py                   # OperatorNode hierarchy\n│   ├── execution_plan.py              # ExecutionPlan tree structure\n│   ├── logical_operators.py           # Logical operation definitions\n│   ├── physical_operators.py          # Physical operation implementations\n│   └── plan_printer.py                # Tree visualization utilities\n│\n├── optimizer/statistics/              # Cost estimation (Milestone 2)\n│   ├── __init__.py\n│   ├── table_stats.py                 # TableStatistics collection\n│   ├── cost_model.py                  # CostEstimate calculations\n│   ├── selectivity.py                 # Predicate selectivity estimation\n│   └── cardinality.py                 # Join cardinality estimation\n│\n├── optimizer/join_ordering/           # Join optimization (Milestone 3)\n│   ├── __init__.py\n│   ├── dp_optimizer.py                # Dynamic programming algorithm\n│   ├── join_order.py                  # JoinOrder representation\n│   ├── pruning.py                     # Search space reduction\n│   └── heuristics.py                  # Fallback optimization strategies\n│\n├── optimizer/physical/                # Physical planning (Milestone 4)\n│   ├── __init__.py\n│   ├── operator_selection.py          # Physical operator choice logic\n│   ├── access_methods.py              # Table scan vs index selection\n│   ├── join_algorithms.py             # Join algorithm selection\n│   └── optimization_rules.py          # Predicate pushdown and transformations\n│\n├── optimizer/integration/             # Component coordination\n│   ├── __init__.py\n│   ├── optimization_pipeline.py       # Main optimization workflow\n│   ├── plan_cache.py                  # Optimized plan storage and reuse\n│   └── optimizer_facade.py            # Simplified external API\n│\n├── tests/                            # Test suite organization\n│   ├── __init__.py\n│   ├── unit/                         # Component-specific unit tests\n│   │   ├── test_plan_representation.py\n│   │   ├── test_cost_estimation.py\n│   │   ├── test_join_ordering.py\n│   │   └── test_physical_planning.py\n│   ├── integration/                  # Cross-component integration tests\n│   │   ├── test_optimization_pipeline.py\n│   │   └── test_plan_quality.py\n│   └── fixtures/                     # Test data and example queries\n│       ├── sample_schemas.py\n│       ├── test_queries.sql\n│       └── expected_plans.py\n│\n├── examples/                         # Usage examples and tutorials\n│   ├── basic_optimization.py         # Simple query optimization demo\n│   ├── cost_analysis.py             # Cost estimation examples\n│   └── plan_visualization.py        # Plan tree printing examples\n│\n└── docs/                            # Additional documentation\n    ├── milestone_checkpoints.md      # Validation criteria for each milestone\n    ├── debugging_guide.md           # Common issues and solutions\n    └── api_reference.md             # Complete API documentation</code></pre></div>\n\n<p><strong>Component Module Responsibilities</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Module</th>\n<th>Primary Classes</th>\n<th>Key Responsibilities</th>\n<th>Milestone</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>optimizer/types.py</code></td>\n<td><code>ParsedQuery</code>, <code>CostEstimate</code>, <code>JoinOrder</code></td>\n<td>Core data structure definitions</td>\n<td>All</td>\n</tr>\n<tr>\n<td><code>optimizer/plan/</code></td>\n<td><code>ExecutionPlan</code>, <code>OperatorNode</code></td>\n<td>Plan tree representation and manipulation</td>\n<td>1</td>\n</tr>\n<tr>\n<td><code>optimizer/statistics/</code></td>\n<td><code>TableStatistics</code>, cost estimation functions</td>\n<td>Resource consumption prediction</td>\n<td>2</td>\n</tr>\n<tr>\n<td><code>optimizer/join_ordering/</code></td>\n<td>Join optimization algorithms</td>\n<td>Dynamic programming for optimal join sequences</td>\n<td>3</td>\n</tr>\n<tr>\n<td><code>optimizer/physical/</code></td>\n<td>Physical operator selection logic</td>\n<td>Concrete implementation choices</td>\n<td>4</td>\n</tr>\n<tr>\n<td><code>optimizer/integration/</code></td>\n<td><code>OptimizationPipeline</code></td>\n<td>Component coordination and caching</td>\n<td>All</td>\n</tr>\n</tbody></table>\n<p><strong>Development Workflow Support</strong>:</p>\n<p>The file structure enables incremental development aligned with project milestones. Developers can implement and test each component independently before integrating them into the complete optimization pipeline:</p>\n<ul>\n<li><p><strong>Milestone 1 Development</strong>: Focus on <code>optimizer/plan/</code> modules, implementing operator hierarchy and tree structures with unit tests in <code>tests/unit/test_plan_representation.py</code></p>\n</li>\n<li><p><strong>Milestone 2 Development</strong>: Implement statistics collection in <code>optimizer/statistics/</code> while using stub implementations for join ordering and physical planning</p>\n</li>\n<li><p><strong>Milestone 3 Development</strong>: Build dynamic programming algorithms in <code>optimizer/join_ordering/</code> using real cost estimates from previous milestones</p>\n</li>\n<li><p><strong>Milestone 4 Development</strong>: Complete physical planning in <code>optimizer/physical/</code> and integrate all components in <code>optimizer/integration/</code></p>\n</li>\n</ul>\n<blockquote>\n<p><strong>Design Principle</strong>: Each module maintains high cohesion (related functionality grouped together) and loose coupling (minimal dependencies between modules). This organization supports both learning progression and professional development practices.</p>\n</blockquote>\n<p><strong>Import Strategy and Dependencies</strong>:</p>\n<p>The module structure enforces a clear dependency hierarchy that prevents circular imports and maintains architectural layering:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>optimizer/types.py              # No internal dependencies\noptimizer/plan/                 # Depends on: types\noptimizer/statistics/           # Depends on: types, plan\noptimizer/join_ordering/        # Depends on: types, plan, statistics  \noptimizer/physical/             # Depends on: types, plan, statistics\noptimizer/integration/          # Depends on: all above modules</code></pre></div>\n\n<p>This dependency structure ensures that core data structures remain stable, specialized components can be developed independently, and integration logic cleanly orchestrates the optimization process without creating circular dependencies.</p>\n<p><strong>Testing Organization</strong>:</p>\n<p>The testing structure supports both component-specific validation and end-to-end optimization quality verification. Unit tests focus on individual component correctness, while integration tests validate that components work together correctly and produce high-quality optimization results.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Focus</th>\n<th>Example Validation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit tests</td>\n<td>Individual component logic</td>\n<td>Cost estimation accuracy, join ordering algorithm correctness</td>\n</tr>\n<tr>\n<td>Integration tests</td>\n<td>Component interaction</td>\n<td>Complete optimization pipeline, plan quality metrics</td>\n</tr>\n<tr>\n<td>Fixtures and examples</td>\n<td>Realistic scenarios</td>\n<td>Standard TPC benchmarks, common query patterns</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plan Tree Structure</td>\n<td>Python dictionaries with type hints</td>\n<td>Custom classes with inheritance hierarchy</td>\n</tr>\n<tr>\n<td>Cost Estimation</td>\n<td>Linear cost formulas</td>\n<td>Multi-dimensional statistical models</td>\n</tr>\n<tr>\n<td>Join Ordering</td>\n<td>Greedy heuristics</td>\n<td>Dynamic programming with pruning</td>\n</tr>\n<tr>\n<td>Statistics Storage</td>\n<td>In-memory dictionaries</td>\n<td>Persistent statistics with histograms</td>\n</tr>\n<tr>\n<td>Plan Caching</td>\n<td>Simple LRU cache</td>\n<td>Persistent plan store with invalidation</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td>JSON for plan representation</td>\n<td>Protocol Buffers for efficiency</td>\n</tr>\n<tr>\n<td>Testing Framework</td>\n<td>pytest with fixtures</td>\n<td>Hypothesis for property-based testing</td>\n</tr>\n</tbody></table>\n<p><strong>B. Core Infrastructure Starter Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># optimizer/types.py - Complete core data structures</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Optional, Union</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperatorType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Types of query operators\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SCAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"scan\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FILTER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"filter\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JOIN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"join\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROJECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"project\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SORT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sort\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AGGREGATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"aggregate\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JoinType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Types of join operations\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INNER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"inner\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LEFT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"left\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RIGHT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"right\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FULL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"full\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParsedQuery</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Parsed SQL query structure\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    columns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    joins: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># (left_table, right_table, join_condition)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filters: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># (table, column, operator, value)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    group_by: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    order_by: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CostEstimate</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Resource consumption estimate\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    io_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_rows: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> total_cost</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.io_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cpu_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.memory_cost</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TableStatistics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Statistical information about a table\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    table_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    row_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    page_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column_stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'ColumnStatistics'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_updated: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ColumnStatistics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Statistics for a single column\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    distinct_values: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    null_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    min_value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    most_frequent_values: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]  </span><span style=\"color:#6A737D\"># (value, frequency) pairs</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># optimizer/config.py - Configuration constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">OPTIMIZATION_TIMEOUT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30.0</span><span style=\"color:#6A737D\">  # seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_JOIN_ENUMERATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#6A737D\">     # tables before switching to heuristics</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SELECTIVITY_THRESHOLD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">  # cutoff for index vs sequential scan</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Cost model parameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">IO_PAGE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">CPU_TUPLE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_PAGE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Join algorithm cost factors</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">HASH_JOIN_FACTOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">NESTED_LOOP_FACTOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.5</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MERGE_JOIN_FACTOR</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.2</span></span></code></pre></div>\n\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># optimizer/plan/execution_plan.py - Complete plan tree infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> optimizer.types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> OperatorType, CostEstimate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionPlan</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tree structure representing query operations\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, root: </span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.root </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> root</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._node_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> traverse_preorder</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Traverse plan tree in pre-order\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        result </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._traverse_preorder_helper(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.root, result)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> result</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _traverse_preorder_helper</span><span style=\"color:#E1E4E8\">(self, node: </span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">, result: List[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> node:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            result.append(node)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> child </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> node.children:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._traverse_preorder_helper(child, result)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_total_cost</span><span style=\"color:#E1E4E8\">(self) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Sum costs across all operators in plan\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_io </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_cpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> total_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_rows </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> node </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.traverse_preorder():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> node.cost_estimate:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_io </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> node.cost_estimate.io_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_cpu </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> node.cost_estimate.cpu_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_memory </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> node.cost_estimate.memory_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                total_rows </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> node.cost_estimate.estimated_rows</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">total_io,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">total_cpu, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">total_memory,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">total_rows</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pretty_print</span><span style=\"color:#E1E4E8\">(self, show_costs: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate indented tree representation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lines </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._pretty_print_helper(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.root, </span><span style=\"color:#9ECBFF\">\"\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, lines, show_costs)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#9ECBFF\"> \"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">.join(lines)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _pretty_print_helper</span><span style=\"color:#E1E4E8\">(self, node: </span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">, prefix: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           is_last: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, lines: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], show_costs: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> node:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        connector </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"└── \"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> is_last </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"├── \"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        node_desc </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">node.operator_type.value</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> hasattr</span><span style=\"color:#E1E4E8\">(node, </span><span style=\"color:#9ECBFF\">'table_name'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> node.table_name:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            node_desc </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\"(</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">node.table_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">)\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> show_costs </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> node.cost_estimate:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node.cost_estimate.total_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            rows </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> node.cost_estimate.estimated_rows</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            node_desc </span><span style=\"color:#F97583\">+=</span><span style=\"color:#F97583\"> f</span><span style=\"color:#9ECBFF\">\" [cost=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cost</span><span style=\"color:#F97583\">:.2f</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">, rows=</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">rows</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">]\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lines.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">prefix</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">connector</span><span style=\"color:#79B8FF\">}{</span><span style=\"color:#E1E4E8\">node_desc</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        child_prefix </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> prefix </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#9ECBFF\">\"    \"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#E1E4E8\"> is_last </span><span style=\"color:#F97583\">else</span><span style=\"color:#9ECBFF\"> \"│   \"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i, child </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> enumerate</span><span style=\"color:#E1E4E8\">(node.children):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            is_last_child </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (i </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(node.children) </span><span style=\"color:#F97583\">-</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._pretty_print_helper(child, child_prefix, is_last_child, lines, show_costs)</span></span></code></pre></div>\n\n<p><strong>C. Core Logic Skeleton Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># optimizer/integration/optimization_pipeline.py - Main coordination logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OptimizationPipeline</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Orchestrates the complete query optimization process\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.plan_builder </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">      # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize in Milestone 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_estimator </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize in Milestone 2  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.join_optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize in Milestone 3</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.physical_planner </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#6A737D\">  # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize in Milestone 4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> optimize_query</span><span style=\"color:#E1E4E8\">(self, parsed_query: ParsedQuery) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Transform parsed query into optimized execution plan\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> Milestone 1: Build logical plan tree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 1. Create logical operators for each table scan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 2. Build filter operators for WHERE predicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 3. Create join operators for table relationships</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 4. Add projection operator for SELECT columns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 5. Validate plan tree structure and semantics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> Milestone 2: Collect statistics and estimate costs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 1. Gather TableStatistics for all referenced tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 2. Calculate selectivity for all filter predicates  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 3. Estimate cardinality for join operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 4. Annotate plan nodes with CostEstimate objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 5. Validate cost estimates for reasonableness</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> Milestone 3: Optimize join ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 1. Extract all tables involved in joins</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 2. Enumerate possible join orders using dynamic programming</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 3. Calculate costs for each join order alternative  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 4. Select optimal JoinOrder based on total cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 5. Reconstruct plan tree with optimized join sequence</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> Milestone 4: Select physical operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 1. Choose access methods (scan vs index) for each table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 2. Select join algorithms (hash vs nested loop vs merge)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 3. Apply predicate pushdown optimizations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 4. Apply projection pushdown optimizations  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # 5. Generate final physical ExecutionPlan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        raise</span><span style=\"color:#79B8FF\"> NotImplementedError</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Implement optimization pipeline\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> generatePlans</span><span style=\"color:#E1E4E8\">(query: ParsedQuery) -> List[ExecutionPlan]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enumerate possible execution plans for query\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate alternative plans for comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 1. Create baseline plan with no optimizations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 2. Generate plans with different join orders</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 3. Generate plans with different physical operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 4. Apply various optimization rules</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 5. Return list of valid alternative plans</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> estimateCost</span><span style=\"color:#E1E4E8\">(plan: ExecutionPlan) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate predicted execution cost for plan\"\"\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement cost calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 1. Traverse plan tree in post-order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 2. Calculate cost for each operator based on inputs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 3. Sum I/O, CPU, and memory costs across operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 4. Validate cost estimates are reasonable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 5. Return total CostEstimate for plan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> optimizeJoinOrder</span><span style=\"color:#E1E4E8\">(tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> JoinOrder:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Find efficient join sequence using dynamic programming\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement DP join ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 1. Handle base case of single table (no joins needed)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 2. Initialize DP table for storing optimal subplans  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 3. Enumerate all possible join pairs and calculate costs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 4. Build up optimal plans for larger table subsets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 5. Extract final optimal JoinOrder from DP table</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> selectPhysicalOperators</span><span style=\"color:#E1E4E8\">(logical_plan: ExecutionPlan, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Choose concrete physical operators for logical plan\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement physical operator selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 1. Traverse logical plan tree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 2. For each scan operator, choose sequential vs index scan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 3. For each join operator, choose hash vs nested loop vs merge join</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 4. Apply predicate pushdown to move filters closer to scans</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 5. Return plan with all physical operators selected</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> collectStatistics</span><span style=\"color:#E1E4E8\">(table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> TableStatistics:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Gather statistical information for cost estimation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement statistics collection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 1. Query table row count from system catalogs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 2. Calculate distinct value counts for each column</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 3. Build histograms for data distribution analysis  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 4. Collect index statistics and selectivity information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # 5. Return complete TableStatistics object</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    pass</span></span></code></pre></div>\n\n<p><strong>D. File/Module Structure Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># optimizer/__init__.py - Main package API</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Query Optimizer Package</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides cost-based query optimization for SQL queries.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> optimizer.integration.optimization_pipeline </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> OptimizationPipeline</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> optimizer.types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ParsedQuery, ExecutionPlan, CostEstimate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> optimizer.plan.execution_plan </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExecutionPlan  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> optimizer.statistics.table_stats </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TableStatistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Main public API</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">__all__</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'OptimizationPipeline'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'ParsedQuery'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'ExecutionPlan'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'CostEstimate'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    'TableStatistics'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Convenience function for simple optimization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> optimize_query</span><span style=\"color:#E1E4E8\">(sql_query: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Optimize a SQL query string and return execution plan\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This would integrate with SQL parser</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> SQLParser()  </span><span style=\"color:#6A737D\"># From prerequisite SQL parser project</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parsed </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse(sql_query)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> OptimizationPipeline()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> optimizer.optimize_query(parsed)</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Implementation Hints</strong></p>\n<ul>\n<li><strong>Python Type Hints</strong>: Use <code>typing.List</code>, <code>typing.Dict</code>, <code>typing.Optional</code> extensively for better IDE support and runtime validation</li>\n<li><strong>Dataclasses</strong>: Use <code>@dataclass</code> for simple data structures like <code>CostEstimate</code> and <code>TableStatistics</code> to get automatic <code>__init__</code>, <code>__repr__</code>, and equality methods  </li>\n<li><strong>Enum Classes</strong>: Use <code>Enum</code> for operator types and join types to ensure type safety and prevent invalid values</li>\n<li><strong>Property Decorators</strong>: Use <code>@property</code> for calculated fields like <code>total_cost</code> that should appear as attributes but are computed from other fields</li>\n<li><strong>Context Managers</strong>: Use context managers for resource management in statistics collection that may need to query external databases</li>\n<li><strong>Functools.lru_cache</strong>: Use <code>@lru_cache</code> decorator for expensive operations like cost calculations that may be called repeatedly with same inputs</li>\n<li><strong>Collections.defaultdict</strong>: Use <code>defaultdict</code> for statistics dictionaries where missing keys should return sensible defaults</li>\n<li><strong>Pathlib</strong>: Use <code>pathlib.Path</code> for file operations if implementing persistent plan caching</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the high-level architecture foundation:</p>\n<p><strong>Validation Commands:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test basic package structure</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -c</span><span style=\"color:#9ECBFF\"> \"from optimizer import OptimizationPipeline; print('Package imports successfully')\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Run architecture validation tests  </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/unit/test_architecture.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Check type hints</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> mypy</span><span style=\"color:#9ECBFF\"> optimizer/</span><span style=\"color:#79B8FF\"> --strict</span></span></code></pre></div>\n\n<p><strong>Expected Behaviors:</strong></p>\n<ul>\n<li>All core data structures (<code>ParsedQuery</code>, <code>ExecutionPlan</code>, <code>CostEstimate</code>) can be imported and instantiated</li>\n<li>Package structure follows recommended layout with no circular imports</li>\n<li>Type hints pass mypy validation with strict mode</li>\n<li>Basic plan tree creation and traversal works without optimization logic</li>\n</ul>\n<p><strong>Quality Checks:</strong></p>\n<ul>\n<li>Code coverage should be &gt;90% for infrastructure modules  </li>\n<li>All public methods have docstrings with parameter and return type documentation</li>\n<li>Integration tests can create and manipulate plan trees even with stub optimization logic</li>\n</ul>\n<h2 id=\"data-model\">Data Model</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Foundation for Milestones 1-4 - defines the core data structures that support plan representation (M1), cost estimation (M2), join ordering (M3), and physical planning (M4).</p>\n</blockquote>\n<p>The data model forms the backbone of our query optimizer, defining how we represent query plans, store statistical information, and calculate costs throughout the optimization process. Think of the data model as the <strong>blueprint language</strong> for our optimizer - just as architects need standardized symbols and measurements to design buildings, our optimizer needs well-defined data structures to represent execution plans, capture table characteristics, and quantify optimization decisions.</p>\n<p>The data model serves three critical purposes in our optimizer architecture. First, it provides a <strong>common vocabulary</strong> that allows different optimizer components to communicate effectively - the Plan Builder creates <code>ExecutionPlan</code> trees that the Cost Estimator can analyze and the Join Optimizer can manipulate. Second, it encapsulates the <strong>statistical knowledge</strong> about our database through <code>TableStatistics</code> and <code>ColumnStatistics</code> that drive intelligent optimization decisions. Third, it quantifies optimization trade-offs through the <code>CostEstimate</code> structure that allows us to compare alternative execution strategies numerically.</p>\n<p>Our data model design emphasizes three key principles that emerged from decades of database research. <strong>Separation of logical and physical concerns</strong> allows us to reason about what operations need to happen independently from how they will be executed. <strong>Rich statistical representation</strong> captures the data characteristics needed for accurate cost estimation without overwhelming the optimizer with unnecessary detail. <strong>Compositional cost modeling</strong> enables us to build up complex plan costs from simpler operator-level estimates while maintaining mathematical soundness.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fplan-tree-structure.svg\" alt=\"Query Plan Tree Structure\"></p>\n<h3 id=\"plan-tree-and-operator-nodes\">Plan Tree and Operator Nodes</h3>\n<p>The <strong>plan tree representation</strong> forms the central data structure of our query optimizer, capturing the hierarchical nature of query execution through a tree of operator nodes. Think of a query plan tree like a <strong>construction project blueprint</strong> - it shows not just what rooms need to be built (the operations), but also the order of construction (execution dependencies) and how materials flow between work sites (data flow between operators). Just as a construction foreman can look at blueprints and estimate labor hours and material costs, our optimizer can traverse plan trees to calculate execution costs and identify optimization opportunities.</p>\n<p>The <code>ExecutionPlan</code> serves as the root container for our plan representation, holding both the tree structure and associated metadata needed for optimization. This structure maintains the complete execution strategy for a query while providing convenient access methods for plan manipulation and cost calculation.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>root</code></td>\n<td><code>OperatorNode</code></td>\n<td>The root operator node of the execution plan tree</td>\n</tr>\n<tr>\n<td><code>total_cost</code></td>\n<td><code>CostEstimate</code></td>\n<td>Cached total cost estimate for the entire plan</td>\n</tr>\n<tr>\n<td><code>optimization_metadata</code></td>\n<td><code>dict</code></td>\n<td>Additional metadata used during optimization process</td>\n</tr>\n<tr>\n<td><code>plan_id</code></td>\n<td><code>str</code></td>\n<td>Unique identifier for this execution plan</td>\n</tr>\n<tr>\n<td><code>created_timestamp</code></td>\n<td><code>datetime</code></td>\n<td>When this plan was generated for debugging</td>\n</tr>\n</tbody></table>\n<p>The <code>OperatorNode</code> represents individual operations within the query execution plan, forming the building blocks of our tree structure. Each node encapsulates both the operation definition and its relationships to other operators in the execution sequence. The design supports both logical operators (what needs to happen) and physical operators (how it will be executed) through a common interface while maintaining type safety.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>operator_type</code></td>\n<td><code>OperatorType</code></td>\n<td>The type of operation this node performs</td>\n</tr>\n<tr>\n<td><code>children</code></td>\n<td><code>List[OperatorNode]</code></td>\n<td>Child operators that provide input to this operation</td>\n</tr>\n<tr>\n<td><code>properties</code></td>\n<td><code>dict</code></td>\n<td>Operator-specific configuration and parameters</td>\n</tr>\n<tr>\n<td><code>cost_estimate</code></td>\n<td><code>CostEstimate</code></td>\n<td>Estimated execution cost for this operator</td>\n</tr>\n<tr>\n<td><code>output_schema</code></td>\n<td><code>List[str]</code></td>\n<td>Column names and types produced by this operator</td>\n</tr>\n<tr>\n<td><code>estimated_rows</code></td>\n<td><code>int</code></td>\n<td>Predicted number of output rows</td>\n</tr>\n<tr>\n<td><code>node_id</code></td>\n<td><code>str</code></td>\n<td>Unique identifier within the plan tree</td>\n</tr>\n</tbody></table>\n<p>The <code>OperatorType</code> enumeration defines the complete set of operations our optimizer can represent and manipulate. This taxonomy covers both logical operations (independent of execution method) and physical operations (specific implementation strategies). The distinction proves crucial during optimization - we generate logical plans first, then select physical implementations based on cost estimates and available resources.</p>\n<table>\n<thead>\n<tr>\n<th>Operator Type</th>\n<th>Category</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SCAN</code></td>\n<td>Data Access</td>\n<td>Read rows from a table or index</td>\n</tr>\n<tr>\n<td><code>FILTER</code></td>\n<td>Row Processing</td>\n<td>Apply predicate conditions to eliminate rows</td>\n</tr>\n<tr>\n<td><code>JOIN</code></td>\n<td>Data Combination</td>\n<td>Combine rows from multiple input sources</td>\n</tr>\n<tr>\n<td><code>PROJECT</code></td>\n<td>Column Processing</td>\n<td>Select specific columns and compute expressions</td>\n</tr>\n<tr>\n<td><code>SORT</code></td>\n<td>Data Ordering</td>\n<td>Order rows by specified columns</td>\n</tr>\n<tr>\n<td><code>AGGREGATE</code></td>\n<td>Data Summarization</td>\n<td>Group rows and compute summary functions</td>\n</tr>\n<tr>\n<td><code>UNION</code></td>\n<td>Set Operations</td>\n<td>Combine rows from multiple sources without duplicates</td>\n</tr>\n<tr>\n<td><code>INTERSECT</code></td>\n<td>Set Operations</td>\n<td>Find common rows between input sources</td>\n</tr>\n<tr>\n<td><code>LIMIT</code></td>\n<td>Result Control</td>\n<td>Restrict number of output rows</td>\n</tr>\n</tbody></table>\n<p>Each operator node maintains <strong>rich property dictionaries</strong> that capture operator-specific configuration without requiring separate subclasses for every operation type. For example, a <code>SCAN</code> operator&#39;s properties might include the table name, index selection, and filter predicates pushed down to the storage layer. A <code>JOIN</code> operator would specify the join condition, join type (<code>INNER</code>, <code>LEFT</code>, <code>RIGHT</code>, <code>FULL</code>), and chosen algorithm (<code>hash_join</code>, <code>nested_loop</code>, <code>merge_join</code>).</p>\n<blockquote>\n<p><strong>Design Insight</strong>: The property dictionary approach provides flexibility for adding new operator variations without modifying the core tree structure. This proves especially valuable as we extend the optimizer with new physical operator implementations or optimization rules.</p>\n</blockquote>\n<p>The <strong>tree traversal capabilities</strong> built into our plan structure enable systematic plan analysis and transformation throughout the optimization process. We support multiple traversal patterns to handle different optimization scenarios efficiently.</p>\n<table>\n<thead>\n<tr>\n<th>Traversal Method</th>\n<th>Order</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>traverse_preorder()</code></td>\n<td>Root → Children</td>\n<td>Cost estimation, plan validation</td>\n</tr>\n<tr>\n<td><code>traverse_postorder()</code></td>\n<td>Children → Root</td>\n<td>Bottom-up cost calculation</td>\n</tr>\n<tr>\n<td><code>traverse_level_order()</code></td>\n<td>Level by level</td>\n<td>Plan visualization, debugging</td>\n</tr>\n<tr>\n<td><code>find_nodes_by_type(type)</code></td>\n<td>Filtered search</td>\n<td>Optimization rule application</td>\n</tr>\n<tr>\n<td><code>get_leaf_nodes()</code></td>\n<td>Bottom-up</td>\n<td>Finding data source operators</td>\n</tr>\n</tbody></table>\n<p>The plan tree structure supports <strong>immutable transformations</strong> that create new plan variants without modifying existing trees. This design choice enables safe parallel exploration of the optimization search space and simplifies debugging by preserving intermediate optimization states. When the Join Optimizer generates alternative join orders, each possibility becomes a separate tree that can be costed and compared independently.</p>\n<blockquote>\n<p><strong>Architecture Decision: Tree vs DAG</strong></p>\n<ul>\n<li><strong>Context</strong>: Query plans could be represented as trees (each operator has exactly one parent) or directed acyclic graphs (operators can have multiple parents for shared computation)</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Tree structure with duplication of common subexpressions</li>\n<li>DAG structure with shared operator nodes</li>\n<li>Hybrid approach with tree structure and shared expression detection</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Pure tree structure for initial implementation</li>\n<li><strong>Rationale</strong>: Trees provide simpler implementation, easier cost calculation, and cleaner optimization algorithms. Most common subexpressions can be handled by the SQL parser or later optimization phases</li>\n<li><strong>Consequences</strong>: Some redundant computation in complex queries, but significantly simpler optimizer implementation and debugging</li>\n</ul>\n</blockquote>\n<h3 id=\"table-and-column-statistics\">Table and Column Statistics</h3>\n<p>Statistical information forms the <strong>knowledge foundation</strong> that enables cost-based query optimization, providing the optimizer with essential insights about data characteristics needed for accurate cost estimation. Think of database statistics like <strong>demographic surveys</strong> used by city planners - just as planners need population density, traffic patterns, and resource usage data to design efficient infrastructure, our query optimizer needs table sizes, column distributions, and correlation patterns to generate efficient execution plans.</p>\n<p>The statistics subsystem addresses the fundamental challenge that cost-based optimizers face: making intelligent decisions about query execution strategies without examining all the data. By maintaining carefully chosen statistical summaries, we enable the optimizer to predict with reasonable accuracy how many rows will survive filter conditions, how selective join operations will be, and which access methods will prove most efficient for specific query patterns.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fstatistics-data-model.svg\" alt=\"Statistics and Cost Model\"></p>\n<p>The <code>TableStatistics</code> structure captures essential characteristics about individual tables that drive optimization decisions throughout the query planning process. These statistics provide the foundation for cardinality estimation, access method selection, and join order optimization.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>table_name</code></td>\n<td><code>str</code></td>\n<td>Fully qualified name of the table</td>\n</tr>\n<tr>\n<td><code>row_count</code></td>\n<td><code>int</code></td>\n<td>Total number of rows in the table</td>\n</tr>\n<tr>\n<td><code>page_count</code></td>\n<td><code>int</code></td>\n<td>Number of storage pages occupied by table data</td>\n</tr>\n<tr>\n<td><code>column_stats</code></td>\n<td><code>dict[str, ColumnStatistics]</code></td>\n<td>Per-column statistical information</td>\n</tr>\n<tr>\n<td><code>last_updated</code></td>\n<td><code>datetime</code></td>\n<td>When these statistics were last refreshed</td>\n</tr>\n<tr>\n<td><code>sample_rate</code></td>\n<td><code>float</code></td>\n<td>Fraction of table sampled for statistics (0.0-1.0)</td>\n</tr>\n<tr>\n<td><code>index_statistics</code></td>\n<td><code>dict[str, IndexStatistics]</code></td>\n<td>Statistics for available indexes</td>\n</tr>\n<tr>\n<td><code>clustering_factor</code></td>\n<td><code>float</code></td>\n<td>Measure of row ordering correlation with storage order</td>\n</tr>\n</tbody></table>\n<p>The <strong>row count and page count</strong> provide the basic size metrics needed for cost estimation. Row count drives cardinality estimates for filter and join operations, while page count enables I/O cost calculation for sequential scans and large join operations. The ratio between these values indicates average row size, which influences buffer pool efficiency and memory usage estimates.</p>\n<p><strong>Column-level statistics</strong> capture the distribution characteristics needed for selectivity estimation - the process of predicting what fraction of rows will satisfy filter predicates. The <code>ColumnStatistics</code> structure balances statistical richness with storage efficiency, providing enough information for accurate estimates without overwhelming the optimizer with unnecessary detail.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>column_name</code></td>\n<td><code>str</code></td>\n<td>Name of the column within its table</td>\n</tr>\n<tr>\n<td><code>distinct_values</code></td>\n<td><code>int</code></td>\n<td>Number of unique values (cardinality)</td>\n</tr>\n<tr>\n<td><code>null_count</code></td>\n<td><code>int</code></td>\n<td>Number of NULL values in the column</td>\n</tr>\n<tr>\n<td><code>min_value</code></td>\n<td><code>Any</code></td>\n<td>Minimum value observed in the column</td>\n</tr>\n<tr>\n<td><code>max_value</code></td>\n<td><code>Any</code></td>\n<td>Maximum value observed in the column</td>\n</tr>\n<tr>\n<td><code>most_common_values</code></td>\n<td><code>List[tuple]</code></td>\n<td>Top-K frequent values with their counts</td>\n</tr>\n<tr>\n<td><code>histogram_buckets</code></td>\n<td><code>List[HistogramBucket]</code></td>\n<td>Value distribution summary</td>\n</tr>\n<tr>\n<td><code>correlation_with_storage</code></td>\n<td><code>float</code></td>\n<td>How well column order matches storage order (-1.0 to 1.0)</td>\n</tr>\n<tr>\n<td><code>average_width</code></td>\n<td><code>int</code></td>\n<td>Average storage space per value in bytes</td>\n</tr>\n</tbody></table>\n<p>The <strong>distinct values count</strong> proves crucial for join cardinality estimation, enabling the optimizer to predict output sizes when combining tables. Combined with the null count, it provides the foundation for selectivity calculations across different predicate types. The min/max range enables quick estimates for range predicates without consulting detailed histograms.</p>\n<p><strong>Most common values</strong> capture skewed distributions that uniform assumption models handle poorly. Real-world data often exhibits Zipfian distributions where a small number of values account for a large fraction of rows. By tracking the top-K frequent values explicitly, we can provide accurate estimates for equality predicates on popular values while falling back to histogram-based estimates for less common values.</p>\n<p>The <code>HistogramBucket</code> structure provides a compressed representation of value distributions for more sophisticated selectivity estimation beyond simple uniform assumptions.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>bucket_id</code></td>\n<td><code>int</code></td>\n<td>Sequential identifier for this bucket</td>\n</tr>\n<tr>\n<td><code>range_start</code></td>\n<td><code>Any</code></td>\n<td>Minimum value included in this bucket</td>\n</tr>\n<tr>\n<td><code>range_end</code></td>\n<td><code>Any</code></td>\n<td>Maximum value included in this bucket</td>\n</tr>\n<tr>\n<td><code>row_count</code></td>\n<td><code>int</code></td>\n<td>Number of rows with values in this bucket&#39;s range</td>\n</tr>\n<tr>\n<td><code>distinct_count</code></td>\n<td><code>int</code></td>\n<td>Number of distinct values within this bucket</td>\n</tr>\n<tr>\n<td><code>frequency</code></td>\n<td><code>float</code></td>\n<td>Fraction of total table rows in this bucket</td>\n</tr>\n</tbody></table>\n<p><strong>Statistics collection and maintenance</strong> requires careful balance between accuracy and overhead. We support multiple collection strategies to handle different table sizes and update patterns effectively.</p>\n<table>\n<thead>\n<tr>\n<th>Collection Method</th>\n<th>When Used</th>\n<th>Accuracy</th>\n<th>Overhead</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Full table scan</td>\n<td>Small tables (&lt;10K rows)</td>\n<td>Exact</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Systematic sampling</td>\n<td>Medium tables (10K-1M rows)</td>\n<td>High</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td>Random sampling</td>\n<td>Large tables (&gt;1M rows)</td>\n<td>Good</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Incremental updates</td>\n<td>High-velocity tables</td>\n<td>Variable</td>\n<td>Low</td>\n</tr>\n<tr>\n<td>Query-driven collection</td>\n<td>Ad-hoc analysis</td>\n<td>Targeted</td>\n<td>Variable</td>\n</tr>\n</tbody></table>\n<p>The <strong>statistics freshness problem</strong> presents an ongoing challenge for cost-based optimizers - as data changes, statistics become stale and optimization decisions degrade. Our statistics model includes metadata to track aging and trigger refresh operations based on configurable policies.</p>\n<blockquote>\n<p><strong>Architecture Decision: Histogram vs Uniform Distribution</strong></p>\n<ul>\n<li><strong>Context</strong>: Column value distributions can be modeled with different levels of sophistication, from simple uniform assumptions to detailed multi-dimensional histograms</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Uniform distribution assumption (distinct values evenly distributed)</li>\n<li>Single-dimensional histograms with equal-width buckets</li>\n<li>Single-dimensional histograms with equal-frequency buckets</li>\n<li>Multi-dimensional histograms capturing column correlations</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Single-dimensional equal-frequency histograms with most-common-values tracking</li>\n<li><strong>Rationale</strong>: Provides significant accuracy improvement over uniform assumptions while maintaining reasonable storage overhead and computational complexity. Equal-frequency buckets handle skewed distributions better than equal-width buckets</li>\n<li><strong>Consequences</strong>: Enables accurate selectivity estimation for most real-world query patterns, but cannot capture multi-column correlations that affect complex join estimates</li>\n</ul>\n</blockquote>\n<h3 id=\"cost-model-components\">Cost Model Components</h3>\n<p>The <strong>cost model</strong> quantifies the computational resources required to execute query plans, translating abstract execution strategies into concrete resource consumption estimates that enable numerical comparison of optimization alternatives. Think of query cost estimation like <strong>construction project bidding</strong> - just as contractors break down building projects into material costs, labor hours, and equipment rental fees to generate competitive bids, our cost model decomposes query execution into I/O operations, CPU processing, and memory allocation to predict total resource consumption.</p>\n<p>The cost model serves as the <strong>objective function</strong> for query optimization, providing the numerical foundation that allows our optimizer to choose between alternative execution strategies. Without accurate cost estimates, optimization becomes guesswork - the optimizer might choose index scans over sequential scans for highly selective queries, or select hash joins over nested loop joins for large table combinations, based on mathematical analysis rather than heuristic rules.</p>\n<p>The <code>CostEstimate</code> structure captures the multi-dimensional nature of query execution costs, recognizing that different resource types contribute to overall performance through different mechanisms and at different scales.</p>\n<table>\n<thead>\n<tr>\n<th>Field Name</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>io_cost</code></td>\n<td><code>float</code></td>\n<td>Estimated disk I/O operations required</td>\n</tr>\n<tr>\n<td><code>cpu_cost</code></td>\n<td><code>float</code></td>\n<td>Estimated CPU processing cycles needed</td>\n</tr>\n<tr>\n<td><code>memory_cost</code></td>\n<td><code>float</code></td>\n<td>Estimated memory allocation required</td>\n</tr>\n<tr>\n<td><code>estimated_rows</code></td>\n<td><code>int</code></td>\n<td>Predicted number of output rows</td>\n</tr>\n<tr>\n<td><code>startup_cost</code></td>\n<td><code>float</code></td>\n<td>Fixed cost incurred before producing first row</td>\n</tr>\n<tr>\n<td><code>total_cost</code></td>\n<td><code>float</code></td>\n<td>Combined cost across all resource dimensions</td>\n</tr>\n<tr>\n<td><code>cost_factors</code></td>\n<td><code>dict</code></td>\n<td>Breakdown of cost contributors for debugging</td>\n</tr>\n<tr>\n<td><code>confidence_level</code></td>\n<td><code>float</code></td>\n<td>Estimated accuracy of this cost prediction (0.0-1.0)</td>\n</tr>\n</tbody></table>\n<p><strong>I/O cost modeling</strong> captures the expense of accessing persistent storage, typically the dominant performance factor for database operations on large datasets. I/O costs scale with the amount of data that must be read from disk, modified by factors like buffer pool hit rates, storage device characteristics, and access patterns.</p>\n<p>The fundamental I/O cost calculation combines sequential and random access patterns based on the specific operator implementation:</p>\n<ol>\n<li><strong>Sequential scan operations</strong> incur costs proportional to the number of data pages that must be read, modified by the sequential access multiplier that reflects the efficiency of reading consecutive pages</li>\n<li><strong>Index scan operations</strong> combine the cost of traversing index pages (typically cached) with the cost of accessing heap pages (potentially random access)  </li>\n<li><strong>Join operations</strong> may require multiple passes through datasets, with costs varying dramatically based on available memory and chosen join algorithm</li>\n<li><strong>Sort operations</strong> incur costs for both reading input data and writing temporary results, with additional multipliers for multi-pass external sorting</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>I/O Cost Component</th>\n<th>Formula</th>\n<th>Constants Used</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential page reads</td>\n<td><code>pages * IO_PAGE_COST * SEQUENTIAL_MULTIPLIER</code></td>\n<td><code>IO_PAGE_COST = 1.0</code>, <code>SEQUENTIAL_MULTIPLIER = 0.1</code></td>\n</tr>\n<tr>\n<td>Random page reads</td>\n<td><code>pages * IO_PAGE_COST * RANDOM_MULTIPLIER</code></td>\n<td><code>RANDOM_MULTIPLIER = 4.0</code></td>\n</tr>\n<tr>\n<td>Index traversal</td>\n<td><code>tree_height * IO_PAGE_COST * INDEX_MULTIPLIER</code></td>\n<td><code>INDEX_MULTIPLIER = 0.5</code></td>\n</tr>\n<tr>\n<td>Sort temporary I/O</td>\n<td><code>input_pages * SORT_MULTIPLIER * passes</code></td>\n<td><code>SORT_MULTIPLIER = 2.0</code></td>\n</tr>\n<tr>\n<td>Hash table spill</td>\n<td><code>spilled_pages * IO_PAGE_COST * HASH_SPILL_MULTIPLIER</code></td>\n<td><code>HASH_SPILL_MULTIPLIER = 1.5</code></td>\n</tr>\n</tbody></table>\n<p><strong>CPU cost modeling</strong> quantifies the computational work required to process data rows through the execution pipeline. CPU costs typically scale with the number of rows processed rather than the number of pages accessed, reflecting the per-tuple processing overhead of expression evaluation, predicate checking, and data transformation.</p>\n<p>Different operator types exhibit characteristic CPU cost patterns that our model captures through per-tuple processing estimates:</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>CPU Cost per Tuple</th>\n<th>Reasoning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential scan</td>\n<td><code>CPU_TUPLE_COST * 1.0</code></td>\n<td>Minimal processing - just row retrieval</td>\n</tr>\n<tr>\n<td>Index scan</td>\n<td><code>CPU_TUPLE_COST * 1.2</code></td>\n<td>Additional index navigation overhead</td>\n</tr>\n<tr>\n<td>Filter evaluation</td>\n<td><code>CPU_TUPLE_COST * complexity_factor</code></td>\n<td>Varies by predicate complexity</td>\n</tr>\n<tr>\n<td>Hash join probe</td>\n<td><code>CPU_TUPLE_COST * 2.5</code></td>\n<td>Hash calculation and bucket lookup</td>\n</tr>\n<tr>\n<td>Nested loop join</td>\n<td><code>CPU_TUPLE_COST * 1.5 * inner_rows</code></td>\n<td>Linear scan of inner relation</td>\n</tr>\n<tr>\n<td>Sort comparison</td>\n<td><code>CPU_TUPLE_COST * log(rows) * comparisons</code></td>\n<td>Comparison-based sorting</td>\n</tr>\n<tr>\n<td>Aggregate calculation</td>\n<td><code>CPU_TUPLE_COST * 3.0</code></td>\n<td>Group identification and accumulation</td>\n</tr>\n</tbody></table>\n<p><strong>Memory cost modeling</strong> captures the buffer pool and working memory requirements that affect plan feasibility and performance characteristics. Memory costs influence optimization decisions through multiple mechanisms - insufficient memory forces external algorithms that increase I/O costs, while abundant memory enables more efficient join algorithms and larger buffer pools.</p>\n<table>\n<thead>\n<tr>\n<th>Memory Usage Type</th>\n<th>Cost Calculation</th>\n<th>Impact on Performance</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Buffer pool pages</td>\n<td><code>pages * MEMORY_PAGE_COST</code></td>\n<td>Reduces I/O through caching</td>\n</tr>\n<tr>\n<td>Hash table memory</td>\n<td><code>rows * avg_row_size * HASH_OVERHEAD</code></td>\n<td>Enables in-memory join processing</td>\n</tr>\n<tr>\n<td>Sort buffer memory</td>\n<td><code>sort_buffer_size * MEMORY_PAGE_COST</code></td>\n<td>Reduces external sort passes</td>\n</tr>\n<tr>\n<td>Temporary result storage</td>\n<td><code>intermediate_rows * row_size</code></td>\n<td>Required for pipeline blocking</td>\n</tr>\n<tr>\n<td>Index cache memory</td>\n<td><code>index_pages * MEMORY_PAGE_COST</code></td>\n<td>Accelerates index access</td>\n</tr>\n</tbody></table>\n<p>The <strong>cost combination formula</strong> aggregates multi-dimensional costs into a single comparable metric while preserving the relative importance of different resource types. This proves essential for optimization decisions that trade off different resource types - for example, choosing between memory-intensive hash joins and I/O-intensive nested loop joins.</p>\n<p>The total cost calculation follows a weighted linear combination approach that can be tuned based on the specific deployment environment:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>total_cost = (io_cost * IO_WEIGHT) + \n             (cpu_cost * CPU_WEIGHT) + \n             (memory_cost * MEMORY_WEIGHT) + \n             startup_cost</code></pre></div>\n\n<table>\n<thead>\n<tr>\n<th>Cost Weight</th>\n<th>Default Value</th>\n<th>Environment Tuning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>IO_WEIGHT</code></td>\n<td><code>4.0</code></td>\n<td>Higher on slow storage, lower on SSD</td>\n</tr>\n<tr>\n<td><code>CPU_WEIGHT</code></td>\n<td><code>1.0</code></td>\n<td>Higher on CPU-constrained systems</td>\n</tr>\n<tr>\n<td><code>MEMORY_WEIGHT</code></td>\n<td><code>0.1</code></td>\n<td>Higher when memory is scarce</td>\n</tr>\n<tr>\n<td><code>STARTUP_COST_PENALTY</code></td>\n<td><code>0.1</code></td>\n<td>Varies by query pattern requirements</td>\n</tr>\n</tbody></table>\n<p><strong>Cost estimation accuracy</strong> varies significantly across different operator types and data characteristics, requiring the optimizer to maintain confidence levels that influence plan selection. High-confidence estimates enable aggressive optimization decisions, while low-confidence estimates favor conservative approaches that perform reasonably across a wider range of actual data distributions.</p>\n<table>\n<thead>\n<tr>\n<th>Estimation Scenario</th>\n<th>Confidence Level</th>\n<th>Accuracy Factors</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table scan with current stats</td>\n<td>0.9</td>\n<td>Statistics recency, sampling quality</td>\n</tr>\n<tr>\n<td>Index scan with selective predicate</td>\n<td>0.8</td>\n<td>Index statistics, predicate selectivity</td>\n</tr>\n<tr>\n<td>Hash join with good stats</td>\n<td>0.7</td>\n<td>Join column distribution, memory availability</td>\n</tr>\n<tr>\n<td>Multi-way join estimation</td>\n<td>0.5</td>\n<td>Correlation effects, error propagation</td>\n</tr>\n<tr>\n<td>Complex nested subquery</td>\n<td>0.3</td>\n<td>Limited statistics, optimization complexity</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Architecture Decision: Linear vs Non-Linear Cost Models</strong></p>\n<ul>\n<li><strong>Context</strong>: Database operations exhibit complex performance characteristics that could be modeled with varying levels of sophistication</li>\n<li><strong>Options Considered</strong>:<ol>\n<li>Simple linear cost model (cost proportional to data size)</li>\n<li>Piecewise linear model (different rates for different size ranges)  </li>\n<li>Non-linear model capturing algorithm complexity curves</li>\n<li>Machine learning-based cost prediction</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Piecewise linear model with algorithm-specific parameters</li>\n<li><strong>Rationale</strong>: Provides reasonable accuracy for most query patterns while maintaining mathematical tractability for optimization algorithms. Captures key non-linearities like external sorting thresholds without excessive complexity</li>\n<li><strong>Consequences</strong>: Enables effective optimization for typical query workloads, but may underestimate costs for very large queries or unusual data distributions</li>\n</ul>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statistics Storage</td>\n<td>Python dictionaries with pickle serialization</td>\n<td>SQLite database with structured schema</td>\n</tr>\n<tr>\n<td>Cost Calculation</td>\n<td>Basic arithmetic with floating-point</td>\n<td>NumPy arrays for vectorized operations</td>\n</tr>\n<tr>\n<td>Plan Tree Traversal</td>\n<td>Recursive functions with manual stack</td>\n<td>Generator-based iterators for memory efficiency</td>\n</tr>\n<tr>\n<td>Statistics Collection</td>\n<td>Full table sampling</td>\n<td>Reservoir sampling with statistical bounds</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>query_optimizer/\n  data_model/\n    __init__.py                    ← Export main data classes\n    plan_tree.py                   ← ExecutionPlan and OperatorNode\n    statistics.py                  ← TableStatistics and ColumnStatistics  \n    cost_model.py                  ← CostEstimate and calculation functions\n    constants.py                   ← Cost model parameters and thresholds\n  tests/\n    test_plan_tree.py             ← Plan construction and traversal tests\n    test_statistics.py            ← Statistics collection and accuracy tests\n    test_cost_model.py            ← Cost estimation validation tests</code></pre></div>\n\n<p><strong>Infrastructure Starter Code (Complete Implementation):</strong></p>\n<p><code>data_model/constants.py</code> - Cost model parameters:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Cost model constants and configuration parameters.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Final</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Cost model weights for combining different resource types</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">IO_PAGE_COST</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">CPU_TUPLE_COST</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.01</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_PAGE_COST</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.001</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># I/O cost multipliers for different access patterns</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SEQUENTIAL_MULTIPLIER</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RANDOM_MULTIPLIER</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 4.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">INDEX_MULTIPLIER</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.5</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Algorithm-specific cost factors</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">HASH_BUILD_COST</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 2.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">HASH_PROBE_COST</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1.5</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SORT_COMPARISON_COST</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1.2</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">NESTED_LOOP_MULTIPLIER</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.5</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Optimization thresholds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SELECTIVITY_THRESHOLD</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.05</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_JOIN_ENUMERATION</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 10</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">OPTIMIZATION_TIMEOUT</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 30</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Default statistics parameters</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_HISTOGRAM_BUCKETS</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 100</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">DEFAULT_SAMPLE_RATE</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">STATISTICS_STALENESS_DAYS</span><span style=\"color:#E1E4E8\">: Final[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 7</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperatorType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enumeration of operator types supported in query plans.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SCAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"scan\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FILTER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"filter\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JOIN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"join\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROJECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"project\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SORT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"sort\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AGGREGATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"aggregate\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"union\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTERSECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"intersect\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LIMIT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"limit\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JoinType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enumeration of join types for join operators.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INNER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"inner\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LEFT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"left\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RIGHT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"right\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FULL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"full\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CROSS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cross\"</span></span></code></pre></div>\n\n<p><code>data_model/cost_model.py</code> - Cost estimation infrastructure:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Cost estimation data structures and utility functions.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> math</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .constants </span><span style=\"color:#F97583\">import</span><span style=\"color:#F97583\"> *</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CostEstimate</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents the estimated execution cost for a query plan or operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    io_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startup_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost_factors: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_level: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> total_cost</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate combined total cost across all resource dimensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.io_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 4.0</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.cpu_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.memory_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.startup_cost)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_cost</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Combine this cost estimate with another, returning new estimate.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.io_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> other.io_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cpu_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> other.cpu_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.memory_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> other.memory_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.estimated_rows, other.estimated_rows),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            startup_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.startup_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> other.startup_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            confidence_level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.confidence_level, other.confidence_level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scale_by_factor</span><span style=\"color:#E1E4E8\">(self, factor: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Scale all cost components by a multiplicative factor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.io_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cpu_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.memory_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            startup_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.startup_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            confidence_level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.confidence_level </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.9</span><span style=\"color:#6A737D\">  # Reduce confidence when scaling</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_scan_cost</span><span style=\"color:#E1E4E8\">(row_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, page_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, selectivity: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate cost for sequential table scan with optional filtering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    io_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> page_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> IO_PAGE_COST</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> SEQUENTIAL_MULTIPLIER</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> row_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> CPU_TUPLE_COST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_output </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(row_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> selectivity)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">io_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">cpu_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">estimated_output,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        cost_factors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"pages_read\"</span><span style=\"color:#E1E4E8\">: page_count, </span><span style=\"color:#9ECBFF\">\"rows_processed\"</span><span style=\"color:#E1E4E8\">: row_count}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> calculate_join_cost</span><span style=\"color:#E1E4E8\">(left_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, right_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, join_selectivity: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#E1E4E8\">) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate cost for hash join between two relations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Hash table build cost (smaller relation)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    build_rows </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(left_rows, right_rows)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    probe_rows </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(left_rows, right_rows)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    build_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> build_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> CPU_TUPLE_COST</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> HASH_BUILD_COST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    probe_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> probe_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> CPU_TUPLE_COST</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> HASH_PROBE_COST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> build_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> MEMORY_PAGE_COST</span><span style=\"color:#6A737D\">  # Assume 100 bytes per row</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_rows </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(left_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> right_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> join_selectivity)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">build_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> probe_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">memory_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">output_rows,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        cost_factors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"build_rows\"</span><span style=\"color:#E1E4E8\">: build_rows,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"probe_rows\"</span><span style=\"color:#E1E4E8\">: probe_rows,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"join_selectivity\"</span><span style=\"color:#E1E4E8\">: join_selectivity</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton Code:</strong></p>\n<p><code>data_model/plan_tree.py</code> - Plan tree implementation framework:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"Query plan tree data structures with traversal and manipulation methods.\"\"\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Iterator, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .constants </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> OperatorType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .cost_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperatorNode</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a single operator in the query execution plan tree.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operator_type: OperatorType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    children: List[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost_estimate: Optional[CostEstimate] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_schema: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_child</span><span style=\"color:#E1E4E8\">(self, child: </span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add a child operator to this node.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that child operator is compatible with this operator type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update output schema based on child schemas and operator semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Invalidate cached cost estimates when tree structure changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> traverse_preorder</span><span style=\"color:#E1E4E8\">(self) -> Iterator[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Traverse the plan tree in pre-order (root first, then children).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Yield this node first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recursively traverse each child in left-to-right order</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle circular references (shouldn't happen in trees, but defensive)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> traverse_postorder</span><span style=\"color:#E1E4E8\">(self) -> Iterator[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Traverse the plan tree in post-order (children first, then root).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recursively traverse each child first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Yield this node after all children have been processed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: This order is useful for bottom-up cost calculation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> find_nodes_by_type</span><span style=\"color:#E1E4E8\">(self, operator_type: OperatorType) -> List[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find all nodes in the subtree with the specified operator type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use traverse_preorder() to visit all nodes in subtree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Filter nodes matching the requested operator_type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return list of matching nodes for optimization rule application</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_subtree_cost</span><span style=\"color:#E1E4E8\">(self) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate the total cost for this operator and all its children.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use traverse_postorder() to calculate costs bottom-up</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Combine child costs with this operator's cost using add_cost()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Cache the result in cost_estimate field for future queries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle cases where individual operator costs are not yet calculated</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pretty_print</span><span style=\"color:#E1E4E8\">(self, indent: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, show_costs: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate indented tree representation for debugging and visualization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format this node with appropriate indentation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include operator type, key properties, and cost if show_costs=True</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recursively print all children with increased indentation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return complete multi-line string representation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionPlan</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Container for complete query execution plan with metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    root: OperatorNode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_cost: Optional[CostEstimate] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    optimization_metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plan_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_timestamp: datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.now)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_total_cost</span><span style=\"color:#E1E4E8\">(self) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate and cache the total execution cost for this plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Call root.calculate_subtree_cost() to get complete plan cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store result in total_cost field for future access</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return the calculated cost estimate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_all_tables</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract all table names referenced in this execution plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Find all SCAN operators using root.find_nodes_by_type()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract table names from scan operator properties</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return unique list of table names for statistics lookup</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> clone</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create a deep copy of this execution plan for optimization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recursively clone the operator tree starting from root</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate new plan_id and timestamp for the copy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Copy optimization_metadata but not cached costs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return new ExecutionPlan with cloned tree structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong></p>\n<p>After implementing the data model, verify the following behaviors:</p>\n<ol>\n<li><strong>Plan Tree Construction</strong>: Create a simple plan tree with SCAN → FILTER → PROJECT operators and verify parent-child relationships</li>\n<li><strong>Cost Calculation</strong>: Build cost estimates for individual operators and verify that total costs combine correctly</li>\n<li><strong>Statistics Representation</strong>: Create TableStatistics and ColumnStatistics objects with realistic values and verify field access</li>\n<li><strong>Tree Traversal</strong>: Test preorder and postorder traversal on a sample tree and verify correct node ordering</li>\n<li><strong>Pretty Printing</strong>: Generate indented tree representations and verify readability</li>\n</ol>\n<p><strong>Expected test command</strong>: <code>python -m pytest tests/test_data_model.py -v</code></p>\n<p><strong>Expected behaviors</strong>: All data structure constructors should work without errors, tree traversal should visit all nodes in correct order, and cost calculations should produce positive numeric values.</p>\n<p><strong>Common Issues and Debugging:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>How to Diagnose</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>&quot;AttributeError: &#39;NoneType&#39; object has no attribute...&quot;</td>\n<td>Uninitialized cost_estimate or statistics</td>\n<td>Check object construction and field defaults</td>\n<td>Initialize with empty/default values in dataclass</td>\n</tr>\n<tr>\n<td>Tree traversal goes into infinite loop</td>\n<td>Circular reference in operator children</td>\n<td>Add visited set to traversal methods</td>\n<td>Implement cycle detection or ensure tree property</td>\n</tr>\n<tr>\n<td>Cost calculations return negative values</td>\n<td>Incorrect cost model parameters or missing data</td>\n<td>Print intermediate cost components</td>\n<td>Validate inputs and use absolute values where appropriate</td>\n</tr>\n<tr>\n<td>Statistics objects consume excessive memory</td>\n<td>Large histogram buckets or uncompressed data</td>\n<td>Profile memory usage with sample statistics</td>\n<td>Implement sampling and compression for large datasets</td>\n</tr>\n</tbody></table>\n<h2 id=\"plan-representation-component\">Plan Representation Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 1 - implements the foundational query plan tree structure with logical and physical operators, supporting tree traversal and cost annotation.</p>\n</blockquote>\n<h3 id=\"mental-model-construction-blueprint\">Mental Model: Construction Blueprint</h3>\n<p>Think of query plan trees like architectural blueprints for a building construction project. Just as architects create multiple levels of blueprints - from high-level floor plans showing room layouts to detailed electrical diagrams showing every wire and outlet - query optimizers create multiple representations of the same execution strategy.</p>\n<p>The <strong>logical plan</strong> is like the initial architectural sketch that shows &quot;we need a kitchen, three bedrooms, and two bathrooms&quot; without specifying whether the kitchen uses gas or electric appliances, what type of flooring goes in each room, or the exact placement of electrical outlets. It captures the essential operations (scan this table, filter these rows, join these datasets) without committing to specific implementation techniques.</p>\n<p>The <strong>physical plan</strong> is like the detailed construction blueprint that specifies every concrete detail: &quot;install ceramic tile flooring in the kitchen, use hardwood in bedrooms, place electrical outlets 18 inches above floor level, connect the gas line to a specific model of range.&quot; It transforms each logical operation into a concrete physical operator with specific algorithms, access methods, and execution parameters.</p>\n<p>The tree structure itself represents the dependency relationships between operations, much like how construction tasks have dependencies - you can&#39;t install flooring before the subflooring is complete, and you can&#39;t paint walls before the electrical wiring is finished. In query plans, you can&#39;t join two tables until you&#39;ve scanned them, and you can&#39;t apply filters until you have data to filter.</p>\n<p>Each node in the tree carries <strong>cost annotations</strong> like how construction blueprints include material quantities and labor estimates. These annotations help the optimizer make informed decisions about alternative approaches, just as a contractor might choose between different materials based on cost, availability, and performance characteristics.</p>\n<h3 id=\"operator-type-system\">Operator Type System</h3>\n<p>The operator type system forms the foundational vocabulary for expressing query execution strategies. We distinguish between <strong>logical operators</strong> that express what computation should happen and <strong>physical operators</strong> that specify exactly how that computation will be implemented.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fplan-tree-structure.svg\" alt=\"Query Plan Tree Structure\"></p>\n<h4 id=\"logical-vs-physical-operator-distinction\">Logical vs Physical Operator Distinction</h4>\n<p><strong>Logical operators</strong> represent abstract operations without implementation commitments. A logical <code>SCAN</code> operation means &quot;retrieve data from this table&quot; without specifying whether to use a sequential table scan, an index scan, or a bitmap scan. A logical <code>JOIN</code> operation means &quot;combine rows from these two datasets based on join predicates&quot; without specifying whether to use a hash join, nested loop join, or sort-merge join algorithm.</p>\n<p><strong>Physical operators</strong> represent concrete implementations with specific algorithms and resource usage patterns. A physical <code>SEQUENTIAL_SCAN</code> operator commits to reading every page of a table sequentially from disk. A physical <code>HASH_JOIN</code> operator commits to building an in-memory hash table from the smaller input and probing it with rows from the larger input.</p>\n<p>This separation allows the optimizer to explore multiple implementation strategies for the same logical computation. A single logical plan might have dozens of possible physical implementations, each with different cost characteristics depending on data sizes, available memory, and existing indexes.</p>\n<h4 id=\"core-operator-types\">Core Operator Types</h4>\n<table>\n<thead>\n<tr>\n<th>Operator Type</th>\n<th>Logical Purpose</th>\n<th>Physical Variants</th>\n<th>Typical Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>SCAN</code></td>\n<td>Retrieve rows from table</td>\n<td><code>SEQUENTIAL_SCAN</code>, <code>INDEX_SCAN</code>, <code>BITMAP_SCAN</code></td>\n<td>Base table access, filtering with indexes</td>\n</tr>\n<tr>\n<td><code>FILTER</code></td>\n<td>Apply selection predicates</td>\n<td><code>FILTER</code>, <code>BITMAP_FILTER</code></td>\n<td>Row elimination based on WHERE clauses</td>\n</tr>\n<tr>\n<td><code>JOIN</code></td>\n<td>Combine rows from multiple sources</td>\n<td><code>HASH_JOIN</code>, <code>NESTED_LOOP_JOIN</code>, <code>SORT_MERGE_JOIN</code></td>\n<td>Multi-table queries with join predicates</td>\n</tr>\n<tr>\n<td><code>PROJECT</code></td>\n<td>Select and transform columns</td>\n<td><code>PROJECT</code>, <code>PROJECTION_WITH_EXPRESSION</code></td>\n<td>Column selection, computed expressions</td>\n</tr>\n<tr>\n<td><code>SORT</code></td>\n<td>Order rows by specified columns</td>\n<td><code>QUICKSORT</code>, <code>EXTERNAL_SORT</code>, <code>TOP_K_SORT</code></td>\n<td>ORDER BY clauses, sort-merge join preparation</td>\n</tr>\n<tr>\n<td><code>AGGREGATE</code></td>\n<td>Group rows and compute aggregates</td>\n<td><code>HASH_AGGREGATE</code>, <code>SORT_AGGREGATE</code>, <code>STREAMING_AGGREGATE</code></td>\n<td>GROUP BY, COUNT, SUM, AVG operations</td>\n</tr>\n<tr>\n<td><code>LIMIT</code></td>\n<td>Restrict number of output rows</td>\n<td><code>LIMIT</code>, <code>TOP_K_LIMIT</code></td>\n<td>LIMIT clauses, pagination queries</td>\n</tr>\n</tbody></table>\n<p>Each operator type carries different cost characteristics and optimization opportunities. <code>SCAN</code> operators depend heavily on table size and available indexes. <code>JOIN</code> operators have quadratic cost potential that drives join ordering optimization. <code>SORT</code> operators may require external sorting for large datasets that exceed available memory.</p>\n<h4 id=\"operator-node-structure\">Operator Node Structure</h4>\n<p>The <code>OperatorNode</code> structure serves as the universal building block for all plan trees, supporting both logical and physical planning phases:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>operator_type</code></td>\n<td><code>OperatorType</code></td>\n<td>Enum identifying the specific operation (SCAN, JOIN, etc.)</td>\n</tr>\n<tr>\n<td><code>children</code></td>\n<td><code>List[OperatorNode]</code></td>\n<td>Child operators providing input data streams</td>\n</tr>\n<tr>\n<td><code>properties</code></td>\n<td><code>dict</code></td>\n<td>Operator-specific configuration (table name, join conditions, etc.)</td>\n</tr>\n<tr>\n<td><code>cost_estimate</code></td>\n<td><code>CostEstimate</code></td>\n<td>Predicted resource consumption for this operator subtree</td>\n</tr>\n<tr>\n<td><code>output_schema</code></td>\n<td><code>List[str]</code></td>\n<td>Column names and types produced by this operator</td>\n</tr>\n<tr>\n<td><code>estimated_rows</code></td>\n<td><code>int</code></td>\n<td>Predicted cardinality of operator output</td>\n</tr>\n<tr>\n<td><code>node_id</code></td>\n<td><code>str</code></td>\n<td>Unique identifier for debugging and plan comparison</td>\n</tr>\n</tbody></table>\n<p>The <code>properties</code> dictionary provides operator-specific configuration without requiring specialized node classes for each operator type. A <code>SCAN</code> node stores <code>table_name</code> and optional <code>index_name</code> properties. A <code>JOIN</code> node stores <code>join_type</code>, <code>join_conditions</code>, and <code>join_algorithm</code> properties. A <code>FILTER</code> node stores <code>predicate_expressions</code> and <code>selectivity_hint</code> properties.</p>\n<p>This flexible design allows the same node structure to represent both logical operations (where <code>join_algorithm</code> might be unspecified) and physical operations (where <code>join_algorithm</code> specifies <code>HASH_JOIN</code> or <code>NESTED_LOOP_JOIN</code>). The optimizer can gradually add more specific properties as it transforms logical plans into physical plans.</p>\n<h4 id=\"cost-annotation-system\">Cost Annotation System</h4>\n<p>Every operator node carries cost annotations that accumulate bottom-up through the plan tree. The <code>CostEstimate</code> structure captures multiple cost dimensions that affect execution performance:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>io_cost</code></td>\n<td><code>float</code></td>\n<td>Estimated disk I/O operations (page reads/writes)</td>\n</tr>\n<tr>\n<td><code>cpu_cost</code></td>\n<td><code>float</code></td>\n<td>Estimated CPU operations (tuple processing, comparisons)</td>\n</tr>\n<tr>\n<td><code>memory_cost</code></td>\n<td><code>float</code></td>\n<td>Estimated memory consumption (hash tables, sort buffers)</td>\n</tr>\n<tr>\n<td><code>estimated_rows</code></td>\n<td><code>int</code></td>\n<td>Predicted output cardinality for this subtree</td>\n</tr>\n<tr>\n<td><code>startup_cost</code></td>\n<td><code>float</code></td>\n<td>One-time initialization cost before producing first row</td>\n</tr>\n<tr>\n<td><code>cost_factors</code></td>\n<td><code>dict</code></td>\n<td>Operator-specific cost components for debugging</td>\n</tr>\n<tr>\n<td><code>confidence_level</code></td>\n<td><code>float</code></td>\n<td>Reliability estimate for cost prediction (0.0 to 1.0)</td>\n</tr>\n</tbody></table>\n<p>The <code>total_cost</code> property combines <code>io_cost</code>, <code>cpu_cost</code>, and <code>memory_cost</code> using configurable weight factors:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>total_cost = (io_cost * IO_PAGE_COST) + \n             (cpu_cost * CPU_TUPLE_COST) + \n             (memory_cost * MEMORY_PAGE_COST)</code></pre></div>\n\n<p>Cost accumulation follows standard tree traversal patterns. Leaf nodes (typically <code>SCAN</code> operators) have costs determined by table statistics and access method selection. Internal nodes accumulate costs from their children plus their own processing costs. This bottom-up cost propagation allows the optimizer to compare alternative plans by examining their root node costs.</p>\n<h3 id=\"tree-traversal-and-manipulation\">Tree Traversal and Manipulation</h3>\n<p>Query plan trees support standard tree traversal algorithms plus specialized operations for plan analysis and transformation. These operations form the foundation for cost calculation, plan comparison, and optimization rule application.</p>\n<h4 id=\"tree-traversal-methods\">Tree Traversal Methods</h4>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Traversal Order</th>\n<th>Returns</th>\n<th>Primary Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>traverse_preorder()</code></td>\n<td>Parent before children</td>\n<td><code>Iterator[OperatorNode]</code></td>\n<td>Top-down plan analysis, rule application</td>\n</tr>\n<tr>\n<td><code>traverse_postorder()</code></td>\n<td>Children before parent</td>\n<td><code>Iterator[OperatorNode]</code></td>\n<td>Bottom-up cost calculation, tree validation</td>\n</tr>\n<tr>\n<td><code>find_nodes_by_type(type)</code></td>\n<td>Breadth-first search</td>\n<td><code>List[OperatorNode]</code></td>\n<td>Finding all operators of specific type</td>\n</tr>\n<tr>\n<td><code>get_all_tables()</code></td>\n<td>Depth-first search</td>\n<td><code>List[str]</code></td>\n<td>Extracting table dependencies for statistics</td>\n</tr>\n</tbody></table>\n<p><strong>Preorder traversal</strong> visits each node before visiting its children, making it ideal for top-down transformations like predicate pushdown where parent operators propagate constraints to child operators. The algorithm maintains a stack of nodes to visit, processing each node and then adding its children to the stack for later processing.</p>\n<p><strong>Postorder traversal</strong> visits children before their parent, making it essential for bottom-up cost calculation where each node&#39;s cost depends on its children&#39;s costs. The algorithm uses a modified depth-first search that marks nodes as visited only after processing all their children.</p>\n<p>The traversal iterators yield <code>OperatorNode</code> objects rather than returning complete lists, allowing memory-efficient processing of large plan trees without materializing the entire traversal sequence in memory.</p>\n<h4 id=\"plan-tree-manipulation\">Plan Tree Manipulation</h4>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>add_child(child)</code></td>\n<td><code>child: OperatorNode</code></td>\n<td><code>None</code></td>\n<td>Append child operator to current node</td>\n</tr>\n<tr>\n<td><code>replace_child(old, new)</code></td>\n<td><code>old: OperatorNode, new: OperatorNode</code></td>\n<td><code>bool</code></td>\n<td>Replace existing child with new subtree</td>\n</tr>\n<tr>\n<td><code>remove_child(child)</code></td>\n<td><code>child: OperatorNode</code></td>\n<td><code>bool</code></td>\n<td>Remove child operator from current node</td>\n</tr>\n<tr>\n<td><code>clone()</code></td>\n<td>None</td>\n<td><code>OperatorNode</code></td>\n<td>Create deep copy of subtree</td>\n</tr>\n<tr>\n<td><code>validate_tree()</code></td>\n<td>None</td>\n<td><code>List[str]</code></td>\n<td>Check tree structure invariants</td>\n</tr>\n</tbody></table>\n<p>Tree manipulation methods maintain structural invariants and update cost annotations automatically. When adding a child operator, the parent recalculates its cost estimate to include the new child&#39;s contribution. When replacing a subtree, the parent updates its output schema and estimated row count based on the new child&#39;s properties.</p>\n<p>The <code>clone()</code> method creates deep copies of plan subtrees for exploring alternative optimizations without modifying the original plan. This supports optimization algorithms that need to try multiple transformations and compare their costs before committing to a specific approach.</p>\n<h4 id=\"plan-tree-validation\">Plan Tree Validation</h4>\n<p>Plan tree validation ensures structural correctness and semantic consistency throughout the optimization process:</p>\n<ol>\n<li><p><strong>Structural validation</strong> confirms that the tree has proper parent-child relationships, no circular references, and appropriate operator arities (unary operators have one child, binary operators have two children).</p>\n</li>\n<li><p><strong>Schema validation</strong> verifies that each operator&#39;s input schema matches its children&#39;s output schemas and that column references in properties (like join conditions) refer to columns that actually exist in the input schemas.</p>\n</li>\n<li><p><strong>Cost consistency validation</strong> checks that each node&#39;s cost estimate properly incorporates its children&#39;s costs and that estimated row counts decrease monotonically through filter operations.</p>\n</li>\n<li><p><strong>Logical consistency validation</strong> ensures that operators appear in valid sequences - for example, that join operations have appropriate join conditions and that aggregation operations group by columns that exist in their input schemas.</p>\n</li>\n</ol>\n<h4 id=\"pretty-printing-and-debugging\">Pretty Printing and Debugging</h4>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>pretty_print(indent, show_costs)</code></td>\n<td><code>indent: int = 0, show_costs: bool = True</code></td>\n<td><code>str</code></td>\n<td>Generate indented tree representation</td>\n</tr>\n<tr>\n<td><code>explain_plan(verbose)</code></td>\n<td><code>verbose: bool = False</code></td>\n<td><code>str</code></td>\n<td>Generate human-readable execution plan explanation</td>\n</tr>\n<tr>\n<td><code>cost_breakdown()</code></td>\n<td>None</td>\n<td><code>dict</code></td>\n<td>Detailed cost analysis by operator type</td>\n</tr>\n</tbody></table>\n<p>The pretty printing system generates indented tree representations that visualize plan structure and cost information:</p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>HASH_JOIN (cost=1250.0, rows=1000)\n├── SEQUENTIAL_SCAN customers (cost=100.0, rows=10000)\n│   └── FILTER age &gt; 25 (cost=50.0, rows=5000)\n└── INDEX_SCAN orders(customer_id) (cost=200.0, rows=2000)\n    └── FILTER order_date &gt; '2023-01-01' (cost=25.0, rows=1000)</code></pre></div>\n\n<p>Each line shows the operator type, key properties, estimated cost, and predicted row count. Indentation levels indicate parent-child relationships, with children indented more deeply than their parents. Optional cost details show the breakdown between I/O, CPU, and memory costs for detailed performance analysis.</p>\n<h3 id=\"architecture-decision-tree-vs-dag\">Architecture Decision: Tree vs DAG</h3>\n<blockquote>\n<p><strong>Decision: Use Tree Structure Rather Than Directed Acyclic Graph (DAG)</strong></p>\n<ul>\n<li><strong>Context</strong>: Query plans can potentially share common subexpressions where the same subtree appears multiple times. DAGs could eliminate duplicate computation, but trees are simpler to implement and reason about.</li>\n<li><strong>Options Considered</strong>: Tree structure, DAG with shared nodes, hybrid approach with subquery materialization</li>\n<li><strong>Decision</strong>: Implement pure tree structure without shared subtrees</li>\n<li><strong>Rationale</strong>: Tree structure significantly simplifies traversal algorithms, cost calculation, and plan manipulation. Most queries have limited subexpression sharing opportunities, and the added complexity of DAG management outweighs performance benefits for our learning-focused implementation.</li>\n<li><strong>Consequences</strong>: Some queries with repeated subexpressions may have suboptimal plans, but implementation complexity remains manageable and debugging is much easier.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Implementation Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Tree Structure</strong></td>\n<td>Simple traversal, easy cost calculation, clear ownership</td>\n<td>Potential duplicate computation, larger memory usage</td>\n<td>Low - standard tree algorithms</td>\n</tr>\n<tr>\n<td><strong>DAG with Shared Nodes</strong></td>\n<td>Eliminates duplicate subexpressions, optimal for complex queries</td>\n<td>Complex traversal, reference counting, cycle detection</td>\n<td>High - requires graph algorithms</td>\n</tr>\n<tr>\n<td><strong>Hybrid with Materialization</strong></td>\n<td>Benefits of DAG where needed, tree simplicity elsewhere</td>\n<td>Complex decision logic, mixed paradigms</td>\n<td>Medium - selective optimization</td>\n</tr>\n</tbody></table>\n<p>The tree structure decision prioritizes implementation clarity over theoretical optimality. In production database systems, DAG-based optimizers can achieve better performance on queries with significant subexpression sharing, but they require sophisticated graph management algorithms for traversal, cost calculation, and memory management.</p>\n<p>Our tree-based approach uses standard algorithms that any developer can understand and debug. Cost calculation follows simple bottom-up accumulation. Tree traversal uses familiar recursive patterns. Plan manipulation operations like node replacement and subtree cloning work predictably without complex reference management.</p>\n<p>The trade-off means some queries might generate suboptimal plans where the same computation appears multiple times, but this aligns with our learning objectives. Understanding how to build a robust tree-based optimizer provides the foundation for later exploring more advanced DAG-based approaches.</p>\n<h4 id=\"tree-structure-implications\">Tree Structure Implications</h4>\n<p>Tree structure shapes several key aspects of plan representation and manipulation:</p>\n<ol>\n<li><p><strong>Unique Parent Relationship</strong>: Each operator node has exactly one parent (except the root), eliminating ambiguity about data flow direction and cost responsibility. This simplifies cost accumulation because each subtree&#39;s cost contributes to exactly one parent.</p>\n</li>\n<li><p><strong>Simple Memory Management</strong>: Tree nodes can use straightforward ownership semantics where each parent owns its children. Cleanup becomes simple recursive deallocation without reference counting or garbage collection complexity.</p>\n</li>\n<li><p><strong>Predictable Traversal</strong>: Tree traversal algorithms have well-defined termination conditions and visit each node exactly once. This makes plan analysis and transformation algorithms more predictable and easier to debug.</p>\n</li>\n<li><p><strong>Clear Optimization Boundaries</strong>: Each subtree represents an independent optimization unit that can be analyzed and transformed without affecting other parts of the plan. This modularity supports incremental optimization approaches.</p>\n</li>\n</ol>\n<h3 id=\"common-implementation-pitfalls\">Common Implementation Pitfalls</h3>\n<p>Understanding common mistakes helps avoid implementation problems that can derail query optimizer development. These pitfalls often stem from misunderstanding the relationship between logical and physical operators or incorrectly implementing tree traversal algorithms.</p>\n<p>⚠️ <strong>Pitfall: Mixing Logical and Physical Properties in Same Node</strong></p>\n<p>Beginning developers often store both logical information (like &quot;join these tables&quot;) and physical implementation details (like &quot;use hash join algorithm&quot;) in the same operator node from the beginning. This makes it impossible to explore different physical implementations for the same logical operation.</p>\n<p><strong>Why it&#39;s wrong</strong>: The optimizer needs to generate multiple physical plans for each logical plan to compare their costs. If logical and physical properties are mixed, the optimizer can&#39;t represent alternative implementations.</p>\n<p><strong>How to fix</strong>: Keep logical and physical properties separate. During logical planning, store only high-level operation descriptions. During physical planning, add implementation-specific properties while preserving logical information for debugging.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Cost Accumulation Direction</strong></p>\n<p>A common mistake is trying to calculate costs top-down (from parent to children) instead of bottom-up (from children to parent). This leads to circular dependencies where a parent&#39;s cost depends on children that haven&#39;t been costed yet.</p>\n<p><strong>Why it&#39;s wrong</strong>: Each operator&#39;s cost depends on the amount of data it receives from its children. You can&#39;t determine how much work a join operator will do until you know how many rows its child operators will produce.</p>\n<p><strong>How to fix</strong>: Always use postorder traversal for cost calculation. Calculate each node&#39;s cost only after all its children have been costed. Store intermediate results to avoid recalculation.</p>\n<p>⚠️ <strong>Pitfall: Modifying Trees During Traversal</strong></p>\n<p>Modifying a tree structure while traversing it can cause traversal algorithms to skip nodes, visit nodes multiple times, or encounter dangling references to deleted nodes.</p>\n<p><strong>Why it&#39;s wrong</strong>: Tree modification changes parent-child relationships that the traversal algorithm depends on. Adding or removing nodes can invalidate iterators or recursive function call stacks.</p>\n<p><strong>How to fix</strong>: Separate traversal from modification. Collect nodes that need modification during traversal, then apply modifications after traversal completes. Alternatively, use immutable tree operations that create new trees instead of modifying existing ones.</p>\n<p>⚠️ <strong>Pitfall: Forgetting Schema Propagation</strong></p>\n<p>Failing to properly propagate column schemas through the plan tree leads to errors when operators reference columns that don&#39;t exist in their input streams.</p>\n<p><strong>Why it&#39;s wrong</strong>: Each operator transforms its input schema into an output schema. Child operators must provide columns that parent operators expect to consume. Schema mismatches cause runtime errors during plan execution.</p>\n<p><strong>How to fix</strong>: Implement schema calculation as part of plan construction. Each operator calculates its output schema based on its input schemas and its specific operation. Validate schema consistency during plan validation.</p>\n<p>⚠️ <strong>Pitfall: Inconsistent Node Identification</strong></p>\n<p>Using inconsistent or non-unique node identifiers makes plan debugging and comparison nearly impossible. Without reliable node identification, it&#39;s difficult to track which optimizations were applied or compare alternative plans.</p>\n<p><strong>Why it&#39;s wrong</strong>: Plan optimization involves generating many alternative plans and comparing their structures. Without consistent identification, the optimizer can&#39;t determine whether two plans are equivalent or track the source of performance differences.</p>\n<p><strong>How to fix</strong>: Generate unique, stable identifiers for each node using a combination of operator type, properties hash, and creation sequence. Include enough information to uniquely identify each node while keeping identifiers stable across optimization runs.</p>\n<p>⚠️ <strong>Pitfall: Inefficient Tree Cloning</strong></p>\n<p>Implementing tree cloning with shallow copying instead of deep copying causes multiple plans to share the same node objects, leading to unexpected mutations when optimizing alternative plans.</p>\n<p><strong>Why it&#39;s wrong</strong>: Query optimization explores many alternative plans by cloning and modifying existing plans. If clones share node objects, modifications to one plan affect other plans, making cost comparisons invalid.</p>\n<p><strong>How to fix</strong>: Implement proper deep cloning that creates independent copies of all nodes, properties, and cost estimates. Consider using immutable data structures to avoid cloning overhead while preventing accidental mutations.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tree Structure</td>\n<td>Python dataclass with recursive references</td>\n<td>Immutable tree with persistent data structures</td>\n</tr>\n<tr>\n<td>Cost Calculation</td>\n<td>Recursive function with memoization</td>\n<td>Visitor pattern with cost accumulation</td>\n</tr>\n<tr>\n<td>Tree Traversal</td>\n<td>Generator functions with yield</td>\n<td>Iterator protocol with custom traversal state</td>\n</tr>\n<tr>\n<td>Operator Properties</td>\n<td>Python dict with string keys</td>\n<td>Typed property classes with validation</td>\n</tr>\n<tr>\n<td>Plan Validation</td>\n<td>Simple assertion checks</td>\n<td>Comprehensive validation framework</td>\n</tr>\n<tr>\n<td>Pretty Printing</td>\n<td>String concatenation with recursion</td>\n<td>Template-based formatting with customization</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>query_optimizer/\n├── plan/\n│   ├── __init__.py\n│   ├── operator_node.py          ← OperatorNode class and tree operations\n│   ├── operator_types.py         ← OperatorType enum and constants\n│   ├── execution_plan.py         ← ExecutionPlan wrapper with metadata\n│   ├── cost_estimate.py          ← CostEstimate class with arithmetic operations\n│   └── plan_printer.py           ← Pretty printing and plan explanation\n├── statistics/\n│   ├── table_statistics.py       ← TableStatistics and ColumnStatistics\n│   └── histogram.py              ← HistogramBucket for advanced statistics\n└── tests/\n    ├── test_operator_node.py     ← Unit tests for tree operations\n    ├── test_execution_plan.py    ← Integration tests for complete plans\n    └── test_plan_traversal.py    ← Tests for traversal algorithms</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>operator_types.py</strong> - Complete enumeration and constants:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperatorType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Enumeration of logical and physical operator types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Logical operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SCAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"SCAN\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FILTER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"FILTER\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JOIN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"JOIN\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PROJECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"PROJECT\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SORT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"SORT\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    AGGREGATE</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"AGGREGATE\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    UNION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"UNION\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INTERSECT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"INTERSECT\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LIMIT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"LIMIT\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Physical scan operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SEQUENTIAL_SCAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"SEQUENTIAL_SCAN\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INDEX_SCAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"INDEX_SCAN\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    BITMAP_SCAN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"BITMAP_SCAN\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Physical join operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    HASH_JOIN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"HASH_JOIN\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    NESTED_LOOP_JOIN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"NESTED_LOOP_JOIN\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    SORT_MERGE_JOIN</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"SORT_MERGE_JOIN\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JoinType</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Join algorithm types.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INNER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"INNER\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    LEFT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"LEFT\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    RIGHT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"RIGHT\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FULL</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"FULL\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    CROSS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"CROSS\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Cost model constants</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">IO_PAGE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">CPU_TUPLE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_PAGE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.05</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SEQUENTIAL_MULTIPLIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RANDOM_MULTIPLIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 4.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Optimization limits</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">OPTIMIZATION_TIMEOUT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 30.0</span><span style=\"color:#6A737D\">  # seconds</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MAX_JOIN_ENUMERATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 12</span><span style=\"color:#6A737D\">    # tables</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SELECTIVITY_THRESHOLD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">  # for index vs sequential scan</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> get_physical_operators</span><span style=\"color:#E1E4E8\">(logical_type: OperatorType) -> List[OperatorType]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Return physical operator variants for logical operator type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    physical_mapping </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        OperatorType.</span><span style=\"color:#79B8FF\">SCAN</span><span style=\"color:#E1E4E8\">: [OperatorType.</span><span style=\"color:#79B8FF\">SEQUENTIAL_SCAN</span><span style=\"color:#E1E4E8\">, OperatorType.</span><span style=\"color:#79B8FF\">INDEX_SCAN</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        OperatorType.</span><span style=\"color:#79B8FF\">JOIN</span><span style=\"color:#E1E4E8\">: [OperatorType.</span><span style=\"color:#79B8FF\">HASH_JOIN</span><span style=\"color:#E1E4E8\">, OperatorType.</span><span style=\"color:#79B8FF\">NESTED_LOOP_JOIN</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           OperatorType.</span><span style=\"color:#79B8FF\">SORT_MERGE_JOIN</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Other operators map to themselves</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> physical_mapping.get(logical_type, [logical_type])</span></span></code></pre></div>\n\n<p><strong>cost_estimate.py</strong> - Complete cost estimation with arithmetic:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> operator_types </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> opt</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CostEstimate</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents estimated execution cost for query plan operator.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    io_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startup_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost_factors: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_level: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> total_cost</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate total weighted cost across all dimensions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.io_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> opt.</span><span style=\"color:#79B8FF\">IO_PAGE_COST</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.cpu_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> opt.</span><span style=\"color:#79B8FF\">CPU_TUPLE_COST</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.memory_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> opt.</span><span style=\"color:#79B8FF\">MEMORY_PAGE_COST</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">.startup_cost)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_cost</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Combine two cost estimates by adding components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.io_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> other.io_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cpu_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> other.cpu_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.memory_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> other.memory_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.estimated_rows, other.estimated_rows),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            startup_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.startup_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> other.startup_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cost_factors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#F97583\">**</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cost_factors, </span><span style=\"color:#F97583\">**</span><span style=\"color:#E1E4E8\">other.cost_factors},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            confidence_level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">min</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.confidence_level, other.confidence_level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scale_by_factor</span><span style=\"color:#E1E4E8\">(self, factor: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Scale all cost components by multiplicative factor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.io_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.cpu_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.memory_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            startup_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.startup_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cost_factors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{k: v </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> factor </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> k, v </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cost_factors.items()},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            confidence_level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.confidence_level</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>operator_node.py</strong> - Node structure with traversal methods:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Iterator, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> uuid</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> operator_types </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> OperatorType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> cost_estimate </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OperatorNode</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents single operator in query execution plan tree.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operator_type: OperatorType</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    children: List[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost_estimate: CostEstimate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">CostEstimate)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    output_schema: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    node_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_child</span><span style=\"color:#E1E4E8\">(self, child: </span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add child operator and update cost estimates.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Append child to children list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Recalculate this node's cost_estimate to include child's cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update estimated_rows based on operator semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Update output_schema based on operator type and child schemas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Different operator types combine child costs differently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> traverse_preorder</span><span style=\"color:#E1E4E8\">(self) -> Iterator[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Traverse tree in preorder (parent before children).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Yield self first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Recursively yield all nodes from each child subtree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use 'yield from' to flatten nested iterators</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> traverse_postorder</span><span style=\"color:#E1E4E8\">(self) -> Iterator[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Traverse tree in postorder (children before parent).\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Recursively yield all nodes from each child subtree first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Yield self after all children have been yielded</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This is essential for bottom-up cost calculation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> find_nodes_by_type</span><span style=\"color:#E1E4E8\">(self, target_type: OperatorType) -> List[</span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find all nodes in subtree matching specified operator type.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize empty result list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use preorder traversal to visit all nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add nodes with matching operator_type to result</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return complete result list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This is useful for finding all scan nodes or all join nodes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_subtree_cost</span><span style=\"color:#E1E4E8\">(self) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate total cost for this operator and all its children.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Start with this node's base cost (before adding children)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Use postorder traversal to ensure children are costed first</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add each child's subtree cost to running total</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Store result in cost_estimate field and return it</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Different operators combine child costs differently</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pretty_print</span><span style=\"color:#E1E4E8\">(self, indent: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">, show_costs: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate indented tree representation for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create indentation string based on depth level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Format operator type and key properties on first line</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Optionally include cost and row count information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Recursively print all children with increased indentation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Combine all lines and return complete tree representation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use \"├──\" and \"└──\" characters for tree structure visualization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_all_tables</span><span style=\"color:#E1E4E8\">(self) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Extract all table names referenced in this subtree.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Initialize empty table name set to avoid duplicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Traverse all nodes in subtree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For SCAN operators, extract table name from properties</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return sorted list of unique table names</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Table names are typically stored in properties['table_name']</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> clone</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create deep copy of this subtree for plan exploration.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create new node with same operator_type and properties copy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Recursively clone all children</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Copy cost_estimate, output_schema, and estimated_rows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Generate new unique node_id for the clone</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use copy.deepcopy for properties dict to avoid shared references</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>execution_plan.py</strong> - Plan wrapper with metadata:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> operator_node </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> OperatorNode</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> cost_estimate </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionPlan</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Complete query execution plan with metadata and cost information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    root: OperatorNode</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_cost: CostEstimate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">CostEstimate) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    optimization_metadata: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    plan_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">(uuid.uuid4()))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    created_timestamp: datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.now)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_total_cost</span><span style=\"color:#E1E4E8\">(self) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate and cache total execution cost for entire plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use root node's calculate_subtree_cost() method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Store result in total_cost field</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add plan-level metadata to optimization_metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return the calculated cost estimate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: This should trigger bottom-up cost calculation through entire tree</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> pretty_print</span><span style=\"color:#E1E4E8\">(self, show_costs: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate formatted plan representation with header information.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create header with plan_id, timestamp, and total cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Call root.pretty_print() for tree structure  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add footer with summary statistics (total nodes, etc.)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return complete formatted representation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Include optimization metadata for debugging optimization decisions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoint\">Milestone Checkpoint</h4>\n<p>After implementing plan representation, verify your implementation with these tests:</p>\n<p><strong>Basic Tree Construction Test:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create simple plan: SELECT * FROM users WHERE age > 25</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">scan_node </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> OperatorNode(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    operator_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">OperatorType.</span><span style=\"color:#79B8FF\">SEQUENTIAL_SCAN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    properties</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'table_name'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'users'</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    output_schema</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'name'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'age'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'email'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">filter_node </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> OperatorNode(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    operator_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">OperatorType.</span><span style=\"color:#79B8FF\">FILTER</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    properties</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'predicate'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'age > 25'</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5000</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">filter_node.add_child(scan_node)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ExecutionPlan(</span><span style=\"color:#FFAB70\">root</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">filter_node)</span></span></code></pre></div>\n\n<p><strong>Expected Behavior:</strong></p>\n<ul>\n<li>Tree structure should have filter node as root with scan node as child</li>\n<li><code>traverse_preorder()</code> should yield filter node first, then scan node</li>\n<li><code>traverse_postorder()</code> should yield scan node first, then filter node  </li>\n<li><code>pretty_print()</code> should show indented tree with costs</li>\n<li><code>get_all_tables()</code> should return [&#39;users&#39;]</li>\n</ul>\n<p><strong>Common Issues to Check:</strong></p>\n<ul>\n<li>Node IDs should be unique across all nodes</li>\n<li>Cost estimates should accumulate from children to parents</li>\n<li>Schema propagation should work correctly through operator chain</li>\n<li>Tree traversal should visit each node exactly once</li>\n</ul>\n<h2 id=\"cost-estimation-component\">Cost Estimation Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 2 - implements statistical cost models for predicting query execution costs, including selectivity estimation and cardinality calculation.</p>\n</blockquote>\n<h3 id=\"mental-model-construction-estimating\">Mental Model: Construction Estimating</h3>\n<p>Think of query cost estimation like estimating a construction project. When a general contractor bids on building a house, they don&#39;t just guess at the total cost. Instead, they break down the project into measurable components: materials (concrete, lumber, fixtures), labor (hours for different skilled trades), and equipment (crane rental, specialized tools). Each component has a different cost structure - some are fixed costs regardless of project size, others scale linearly with square footage, and some have economies of scale that reduce per-unit costs as volume increases.</p>\n<p>Query cost estimation follows this same principle. Just as a contractor estimates concrete costs based on square footage and local material prices, our cost estimator predicts I/O costs based on table size and storage characteristics. Labor costs in construction depend on the complexity of work and available workforce - similarly, our CPU costs depend on the complexity of operations (joins vs scans) and the processing power available. A contractor also considers the sequence of work - you can&#39;t install drywall before framing - just as we must account for data dependencies where join costs depend on the results of earlier filter operations.</p>\n<p>The key insight from construction estimating is that <strong>accuracy improves with better data about actual conditions</strong>. A contractor who has built similar houses in the same neighborhood with the same subcontractors will provide more accurate estimates than one working from generic industry averages. Similarly, our cost estimator becomes more accurate when it has current statistics about table sizes, data distributions, and actual selectivity patterns rather than relying on outdated or generic assumptions.</p>\n<p>However, just as construction projects face uncertainties (weather delays, material price changes, hidden structural problems), query cost estimation must handle inherent unpredictability in data access patterns, cache behavior, and concurrent workload interference. The goal is not perfect prediction but rather sufficient accuracy to distinguish between genuinely good and poor plan choices.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fcost-estimation-sequence.svg\" alt=\"Cost Estimation Interaction Sequence\"></p>\n<h3 id=\"statistics-collection-and-maintenance\">Statistics Collection and Maintenance</h3>\n<p>The foundation of accurate cost estimation lies in comprehensive statistics collection. Without current data about table characteristics, our cost estimates become as unreliable as a contractor bidding on a house they&#39;ve never seen. The <code>TableStatistics</code> structure captures the essential information needed for cost modeling.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>table_name</code></td>\n<td>str</td>\n<td>Unique identifier for the table being analyzed</td>\n</tr>\n<tr>\n<td><code>row_count</code></td>\n<td>int</td>\n<td>Total number of rows currently in the table</td>\n</tr>\n<tr>\n<td><code>page_count</code></td>\n<td>int</td>\n<td>Number of storage pages occupied by table data</td>\n</tr>\n<tr>\n<td><code>column_stats</code></td>\n<td>dict</td>\n<td>Mapping from column names to <code>ColumnStatistics</code> objects</td>\n</tr>\n<tr>\n<td><code>last_updated</code></td>\n<td>datetime</td>\n<td>Timestamp when statistics were last refreshed</td>\n</tr>\n<tr>\n<td><code>sample_rate</code></td>\n<td>float</td>\n<td>Fraction of table sampled for statistics (1.0 = full scan)</td>\n</tr>\n<tr>\n<td><code>index_statistics</code></td>\n<td>dict</td>\n<td>Mapping from index names to index-specific statistics</td>\n</tr>\n<tr>\n<td><code>clustering_factor</code></td>\n<td>float</td>\n<td>Measure of how well table storage aligns with query access patterns</td>\n</tr>\n</tbody></table>\n<p>The <code>ColumnStatistics</code> structure provides detailed information about individual columns that drives selectivity estimation. This granular data allows the cost estimator to make informed predictions about how many rows will survive filter predicates.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>column_name</code></td>\n<td>str</td>\n<td>Name of the column being analyzed</td>\n</tr>\n<tr>\n<td><code>distinct_values</code></td>\n<td>int</td>\n<td>Number of unique values in the column (cardinality)</td>\n</tr>\n<tr>\n<td><code>null_count</code></td>\n<td>int</td>\n<td>Number of rows where this column contains NULL</td>\n</tr>\n<tr>\n<td><code>min_value</code></td>\n<td>Any</td>\n<td>Minimum value found in the column (for range estimates)</td>\n</tr>\n<tr>\n<td><code>max_value</code></td>\n<td>Any</td>\n<td>Maximum value found in the column (for range estimates)</td>\n</tr>\n<tr>\n<td><code>most_common_values</code></td>\n<td>List[tuple]</td>\n<td>Most frequent values with their occurrence counts</td>\n</tr>\n<tr>\n<td><code>histogram_buckets</code></td>\n<td>List[HistogramBucket]</td>\n<td>Distribution information for selectivity estimation</td>\n</tr>\n<tr>\n<td><code>correlation_with_storage</code></td>\n<td>float</td>\n<td>How well column values correlate with physical storage order</td>\n</tr>\n<tr>\n<td><code>average_width</code></td>\n<td>int</td>\n<td>Average size in bytes for variable-length columns</td>\n</tr>\n</tbody></table>\n<p>Statistics collection operates through the <code>collectStatistics</code> method, which can run in different modes depending on performance requirements and accuracy needs. For large tables, full table scans for statistics collection can be prohibitively expensive, so the system supports sampling-based approaches where accuracy is traded for collection speed.</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>collectStatistics</code></td>\n<td>table: str, sample_rate: float</td>\n<td><code>TableStatistics</code></td>\n<td>Gather comprehensive statistics for cost estimation</td>\n</tr>\n<tr>\n<td><code>refreshColumnStats</code></td>\n<td>table: str, column: str</td>\n<td><code>ColumnStatistics</code></td>\n<td>Update statistics for a specific column</td>\n</tr>\n<tr>\n<td><code>estimateStalenessCost</code></td>\n<td>stats: <code>TableStatistics</code></td>\n<td>float</td>\n<td>Predict accuracy degradation since last update</td>\n</tr>\n<tr>\n<td><code>shouldRefreshStats</code></td>\n<td>stats: <code>TableStatistics</code>, query_pattern: str</td>\n<td>bool</td>\n<td>Determine if statistics update is needed</td>\n</tr>\n</tbody></table>\n<p>The statistics collection process follows a systematic approach that balances accuracy with performance impact:</p>\n<ol>\n<li>The system determines whether full table analysis or sampling is appropriate based on table size and available maintenance windows</li>\n<li>For sampled statistics, it uses systematic sampling to ensure representative coverage across the entire table rather than clustering samples in one region</li>\n<li>Column analysis computes basic statistics (min, max, distinct count) along with frequency distributions for the most common values</li>\n<li>Histogram construction divides the value range into buckets with approximately equal frequency, enabling accurate range query selectivity estimation</li>\n<li>Index statistics collection analyzes the correlation between index order and table storage order, which affects the cost of index-driven table access</li>\n<li>The system records collection metadata including sample rates and timestamps to enable staleness detection during query optimization</li>\n</ol>\n<p>Statistics maintenance requires careful scheduling to avoid impacting production query performance while ensuring cost estimation accuracy. Stale statistics can lead to dramatically poor plan choices, particularly for tables with rapidly changing data distributions.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The cost of statistics collection must be weighed against the benefit of improved plan quality. For frequently queried tables, the performance gain from better optimization easily justifies regular statistics updates. For rarely accessed tables, infrequent updates based on data modification thresholds provide better resource allocation.</p>\n</blockquote>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fstatistics-data-model.svg\" alt=\"Statistics and Cost Model\"></p>\n<h3 id=\"selectivity-and-cardinality-estimation\">Selectivity and Cardinality Estimation</h3>\n<p>Selectivity estimation predicts what fraction of rows will survive filter predicates, while cardinality estimation calculates the absolute number of rows in intermediate results. These predictions drive the entire cost estimation process, as they determine how much data flows between operators and influences both I/O and CPU costs.</p>\n<p>The <code>CostEstimate</code> structure captures the multi-dimensional nature of query execution costs, distinguishing between different resource types that contribute to total execution time.</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>io_cost</code></td>\n<td>float</td>\n<td>Estimated disk I/O operations required</td>\n</tr>\n<tr>\n<td><code>cpu_cost</code></td>\n<td>float</td>\n<td>Estimated CPU cycles for processing</td>\n</tr>\n<tr>\n<td><code>memory_cost</code></td>\n<td>float</td>\n<td>Estimated memory allocation and access overhead</td>\n</tr>\n<tr>\n<td><code>estimated_rows</code></td>\n<td>int</td>\n<td>Predicted number of output rows</td>\n</tr>\n<tr>\n<td><code>startup_cost</code></td>\n<td>float</td>\n<td>Fixed cost paid before first row is produced</td>\n</tr>\n<tr>\n<td><code>total_cost</code></td>\n<td>property</td>\n<td>Computed total combining all cost components</td>\n</tr>\n<tr>\n<td><code>cost_factors</code></td>\n<td>dict</td>\n<td>Breakdown of contributing factors for debugging</td>\n</tr>\n<tr>\n<td><code>confidence_level</code></td>\n<td>float</td>\n<td>Estimated accuracy of the cost prediction</td>\n</tr>\n</tbody></table>\n<p>Selectivity estimation algorithms vary based on the type of predicate and available statistical information. The system handles several common predicate patterns with specialized estimation techniques.</p>\n<p><strong>Equality Predicates</strong> (<code>column = value</code>): For equality predicates, selectivity depends on whether the queried value is among the most common values tracked in statistics. If the value appears in the <code>most_common_values</code> list, its exact frequency provides precise selectivity. For values not in the common list, the estimator assumes uniform distribution among the remaining values: <code>selectivity = (1 - sum_of_common_frequencies) / (distinct_values - num_common_values)</code>.</p>\n<p><strong>Range Predicates</strong> (<code>column &gt; value</code>, <code>column BETWEEN low AND high</code>): Range selectivity uses histogram buckets to estimate what fraction of values fall within the specified range. The algorithm identifies which histogram buckets intersect the query range and uses linear interpolation within partially overlapping buckets. This approach provides reasonable accuracy for uniformly distributed data but can be misleading for skewed distributions within buckets.</p>\n<p><strong>Join Selectivity</strong>: Join cardinality estimation uses the fundamental principle that the result size depends on the degree of correlation between join columns. For an inner join between tables with <code>R</code> and <code>S</code> rows on columns with <code>D_R</code> and <code>D_S</code> distinct values respectively, the estimated result size is: <code>(R * S) / max(D_R, D_S)</code>. This formula assumes that join keys are foreign key relationships where one side has unique values.</p>\n<table>\n<thead>\n<tr>\n<th>Predicate Type</th>\n<th>Estimation Method</th>\n<th>Accuracy Factors</th>\n<th>Fallback Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Equality</td>\n<td>Most common values or uniform distribution</td>\n<td>Frequency of queried value, statistics staleness</td>\n<td>Default selectivity of 0.1%</td>\n</tr>\n<tr>\n<td>Range</td>\n<td>Histogram bucket interpolation</td>\n<td>Distribution uniformity, bucket granularity</td>\n<td>Linear interpolation between min/max</td>\n</tr>\n<tr>\n<td>LIKE patterns</td>\n<td>Pattern complexity analysis</td>\n<td>Prefix selectivity, wildcard positions</td>\n<td>Conservative estimate of 25%</td>\n</tr>\n<tr>\n<td>IN lists</td>\n<td>Sum of individual equality estimates</td>\n<td>Length of IN list, value frequency</td>\n<td>Treat as multiple OR conditions</td>\n</tr>\n<tr>\n<td>IS NULL</td>\n<td>Direct from null_count statistics</td>\n<td>Statistics accuracy</td>\n<td>Default 5% selectivity</td>\n</tr>\n</tbody></table>\n<p>The cardinality estimation process builds upon selectivity calculations to predict intermediate result sizes throughout the query plan:</p>\n<ol>\n<li><strong>Base Table Cardinality</strong>: Start with table row counts from statistics, adjusted for any concurrent modifications detected since statistics collection</li>\n<li><strong>Filter Application</strong>: Apply selectivity estimates for each filter predicate, assuming independence between predicates (which often underestimates real selectivity due to correlation)</li>\n<li><strong>Join Cardinality</strong>: Estimate join result size using the join selectivity formulas, considering the cardinality of input relations after their filter predicates</li>\n<li><strong>Projection Effects</strong>: Account for duplicate elimination in DISTINCT projections using distinct value statistics</li>\n<li><strong>Aggregation Cardinality</strong>: Estimate GROUP BY result size based on the distinct value counts of grouping columns</li>\n</ol>\n<p>Common complications arise when predicates are correlated or when statistics don&#39;t capture important data characteristics. For example, in a table of orders with both <code>order_date</code> and <code>ship_date</code> columns, filtering on <code>order_date &gt; &#39;2023-01-01&#39;</code> and <code>ship_date &lt; &#39;2023-02-01&#39;</code> involves strong correlation that independence assumptions ignore.</p>\n<blockquote>\n<p><strong>Critical Consideration</strong>: Selectivity estimation errors compound throughout the query plan tree. A 2x error in early filter selectivity can become an 8x error in final join cardinality, leading to dramatically wrong plan choices. Conservative estimation that slightly overestimates intermediate result sizes often produces more robust plans than aggressive estimates that risk severe underestimation.</p>\n</blockquote>\n<h3 id=\"io-and-cpu-cost-modeling\">I/O and CPU Cost Modeling</h3>\n<p>The cost model translates estimated cardinalities and selectivities into predicted resource consumption, providing a unified metric for comparing different execution plans. The model must account for the fundamental difference between I/O-bound and CPU-bound operations while handling the complex interactions between memory usage, cache effects, and concurrent workload interference.</p>\n<p>I/O cost modeling centers on the observation that disk access patterns dramatically affect performance. Sequential scans that read contiguous pages benefit from operating system prefetching and storage device optimizations, while random access patterns incur significant seek overhead on traditional spinning disks and reduced parallelism on SSDs.</p>\n<table>\n<thead>\n<tr>\n<th>Cost Component</th>\n<th>Base Unit</th>\n<th>Scaling Factors</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential Scan I/O</td>\n<td><code>IO_PAGE_COST * page_count</code></td>\n<td><code>SEQUENTIAL_MULTIPLIER = 0.3</code></td>\n<td>Reading table pages in storage order</td>\n</tr>\n<tr>\n<td>Random I/O</td>\n<td><code>IO_PAGE_COST * page_count</code></td>\n<td><code>RANDOM_MULTIPLIER = 2.0</code></td>\n<td>Scattered reads following index pointers</td>\n</tr>\n<tr>\n<td>Index Scan I/O</td>\n<td><code>IO_PAGE_COST * (index_pages + table_pages)</code></td>\n<td>Clustering factor adjustment</td>\n<td>Index traversal plus table access</td>\n</tr>\n<tr>\n<td>Sort I/O</td>\n<td><code>IO_PAGE_COST * pages * log(pages)</code></td>\n<td>Memory buffer consideration</td>\n<td>External sort with disk-based merge phases</td>\n</tr>\n</tbody></table>\n<p>CPU cost modeling focuses on the computational work required to process individual tuples through various operators. Different operators have vastly different per-tuple costs - a simple projection may involve only memory copying, while a hash join requires hash computation, probe operations, and potentially complex result construction.</p>\n<table>\n<thead>\n<tr>\n<th>Operation Type</th>\n<th>Per-Tuple Cost</th>\n<th>Scaling Factors</th>\n<th>Additional Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential Scan</td>\n<td><code>CPU_TUPLE_COST * 1.0</code></td>\n<td>Filter complexity multiplier</td>\n<td>Base cost for reading and basic processing</td>\n</tr>\n<tr>\n<td>Hash Join Build</td>\n<td><code>CPU_TUPLE_COST * 3.0</code></td>\n<td>Hash function complexity</td>\n<td>Building hash table for smaller relation</td>\n</tr>\n<tr>\n<td>Hash Join Probe</td>\n<td><code>CPU_TUPLE_COST * 2.0</code></td>\n<td>Hash collision rate</td>\n<td>Probing hash table with larger relation</td>\n</tr>\n<tr>\n<td>Nested Loop Join</td>\n<td><code>CPU_TUPLE_COST * 1.5</code></td>\n<td>Inner loop optimization</td>\n<td>Cost per inner tuple access</td>\n</tr>\n<tr>\n<td>Sort Operation</td>\n<td><code>CPU_TUPLE_COST * log(tuples)</code></td>\n<td>Comparison function cost</td>\n<td>Per-tuple cost increases with sort size</td>\n</tr>\n</tbody></table>\n<p>Memory cost modeling addresses the resource consumption and performance impact of memory allocation for operator-specific data structures. Hash joins require memory for hash tables, sorts need memory for buffering, and all operations benefit from larger buffer pools that reduce I/O.</p>\n<p>The <code>estimateCost</code> method combines these cost components into a unified prediction that enables plan comparison:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>estimateCost</code></td>\n<td>plan: <code>ExecutionPlan</code></td>\n<td><code>CostEstimate</code></td>\n<td>Calculate total predicted execution cost</td>\n</tr>\n<tr>\n<td><code>estimateOperatorCost</code></td>\n<td>node: <code>OperatorNode</code>, input_stats: dict</td>\n<td><code>CostEstimate</code></td>\n<td>Cost for individual operator</td>\n</tr>\n<tr>\n<td><code>estimateIOCost</code></td>\n<td>access_method: str, pages: int, selectivity: float</td>\n<td>float</td>\n<td>I/O cost based on access pattern</td>\n</tr>\n<tr>\n<td><code>estimateCPUCost</code></td>\n<td>operation: str, tuples: int, complexity: float</td>\n<td>float</td>\n<td>CPU cost for computational work</td>\n</tr>\n<tr>\n<td><code>estimateMemoryCost</code></td>\n<td>algorithm: str, input_size: int, available_memory: int</td>\n<td>float</td>\n<td>Memory allocation and pressure costs</td>\n</tr>\n</tbody></table>\n<p>The cost estimation algorithm traverses the query plan tree in post-order, computing costs from leaves to root:</p>\n<ol>\n<li><strong>Leaf Node Costs</strong>: Table scan operators use table statistics to compute base I/O costs, applying sequential or random access multipliers based on scan type and available indexes</li>\n<li><strong>Filter Costs</strong>: Add CPU costs for predicate evaluation and adjust output cardinality based on selectivity estimates</li>\n<li><strong>Join Costs</strong>: Combine input cardinalities with join algorithms to compute both CPU costs (for join processing) and I/O costs (for any required sorting or temporary storage)</li>\n<li><strong>Memory Pressure</strong>: Adjust costs based on available memory - operations that exceed memory capacity incur additional I/O for spilling to disk</li>\n<li><strong>Startup vs Runtime</strong>: Distinguish between fixed startup costs (building hash tables, sorting) and per-tuple runtime costs that scale with data volume</li>\n</ol>\n<blockquote>\n<p><strong>Architectural Insight</strong>: The cost model must be calibrated to the target deployment environment. Default costs tuned for spinning disks will make poor decisions on SSD storage, while models optimized for single-user workloads may not account for resource contention in concurrent environments. Parameterizing cost constants enables adaptation to different hardware and workload characteristics.</p>\n</blockquote>\n<h3 id=\"architecture-decision-histogram-vs-uniform-distribution\">Architecture Decision: Histogram vs Uniform Distribution</h3>\n<p>The choice between histogram-based statistics and simple uniform distribution assumptions represents a fundamental trade-off between estimation accuracy and system complexity. This decision affects not only the cost estimation component but also statistics collection overhead and query optimization performance.</p>\n<blockquote>\n<p><strong>Decision: Use Simplified Histograms with Uniform Distribution Fallback</strong></p>\n<ul>\n<li><strong>Context</strong>: Selectivity estimation accuracy directly impacts plan quality, but complex statistical models increase collection overhead and optimization time. Production systems show that moderate histogram complexity provides most benefits while avoiding the maintenance burden of sophisticated models.</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Uniform distribution only (simple, fast, inaccurate for skewed data)</li>\n<li>Full equi-depth histograms (accurate, complex, expensive to maintain)</li>\n<li>Simplified histograms with fallbacks (balanced approach)</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Implement simplified histograms with 10-20 buckets per column, falling back to uniform distribution when histograms are unavailable or stale</li>\n<li><strong>Rationale</strong>: Analysis of real query workloads shows that even simple histograms provide 80% of the benefit of complex models. The remaining accuracy improvement rarely changes plan selection for typical queries, while the maintenance overhead of detailed histograms impacts system performance.</li>\n<li><strong>Consequences</strong>: Enables reasonable selectivity estimation for most query patterns while keeping statistics collection and optimization overhead manageable. May produce suboptimal plans for queries with highly skewed data access patterns.</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Accuracy</th>\n<th>Collection Overhead</th>\n<th>Optimization Speed</th>\n<th>Maintenance Complexity</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Uniform Distribution</td>\n<td>Low for skewed data</td>\n<td>Minimal (just min/max/distinct)</td>\n<td>Very fast</td>\n<td>Simple - just row counts</td>\n</tr>\n<tr>\n<td>Simplified Histograms</td>\n<td>Good for most queries</td>\n<td>Moderate (single table pass)</td>\n<td>Fast</td>\n<td>Medium - periodic refresh needed</td>\n</tr>\n<tr>\n<td>Detailed Histograms</td>\n<td>High accuracy</td>\n<td>High (multiple passes, sampling)</td>\n<td>Slower (complex calculations)</td>\n<td>Complex - staleness detection crucial</td>\n</tr>\n</tbody></table>\n<p>The <code>HistogramBucket</code> structure supports the simplified approach while maintaining extensibility for future enhancements:</p>\n<table>\n<thead>\n<tr>\n<th>Field</th>\n<th>Type</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>bucket_id</code></td>\n<td>int</td>\n<td>Sequential identifier for ordering buckets</td>\n</tr>\n<tr>\n<td><code>range_start</code></td>\n<td>Any</td>\n<td>Minimum value included in this bucket</td>\n</tr>\n<tr>\n<td><code>range_end</code></td>\n<td>Any</td>\n<td>Maximum value included in this bucket</td>\n</tr>\n<tr>\n<td><code>row_count</code></td>\n<td>int</td>\n<td>Number of rows with values in this bucket&#39;s range</td>\n</tr>\n<tr>\n<td><code>distinct_count</code></td>\n<td>int</td>\n<td>Number of unique values within this bucket</td>\n</tr>\n<tr>\n<td><code>frequency</code></td>\n<td>float</td>\n<td>Fraction of total table rows represented by this bucket</td>\n</tr>\n</tbody></table>\n<p>The histogram construction algorithm balances accuracy with simplicity:</p>\n<ol>\n<li><strong>Sample Selection</strong>: For large tables, use systematic sampling to select a representative subset (typically 10,000-100,000 rows) that spans the entire table</li>\n<li><strong>Bucket Boundary Calculation</strong>: Sort sample values and divide into equal-frequency buckets, ensuring each bucket represents approximately the same number of rows</li>\n<li><strong>Boundary Adjustment</strong>: Adjust bucket boundaries to align with natural value breaks when possible (e.g., month boundaries for dates) to improve estimation for common query patterns</li>\n<li><strong>Distinct Value Estimation</strong>: Within each bucket, estimate distinct values using sampling techniques or assume uniform distribution if detailed analysis is too expensive</li>\n<li><strong>Validation</strong>: Compare histogram estimates against known queries to detect systematic biases and adjust bucket strategies</li>\n</ol>\n<p>The fallback strategy handles cases where histogram information is unavailable or unreliable:</p>\n<ul>\n<li><strong>Missing Histograms</strong>: Use uniform distribution between min and max values, with conservative selectivity estimates for edge cases</li>\n<li><strong>Stale Histograms</strong>: Apply staleness penalties to account for potential distribution changes, gradually falling back to uniform distribution as staleness increases</li>\n<li><strong>Insufficient Sample Size</strong>: For small tables or columns with few distinct values, skip histogram construction and use exact frequency counts where possible</li>\n</ul>\n<blockquote>\n<p><strong>Implementation Note</strong>: The histogram approach chosen here provides a solid foundation that can be enhanced incrementally. Starting with uniform distribution assumptions, adding basic histograms, and then refining bucket strategies allows the system to evolve without fundamental architectural changes.</p>\n</blockquote>\n<h3 id=\"common-estimation-pitfalls\">Common Estimation Pitfalls</h3>\n<p>Cost estimation involves numerous subtle sources of error that can dramatically impact plan quality. Understanding these pitfalls helps both implementers avoid common mistakes and users diagnose optimization problems in production systems.</p>\n<p>⚠️ <strong>Pitfall: Independence Assumption for Correlated Predicates</strong></p>\n<p>Many cost estimators assume that filter predicates are independent, multiplying individual selectivities to estimate combined selectivity. For a query with <code>WHERE age &gt; 65 AND status = &#39;retired&#39;</code>, the estimator might calculate <code>selectivity = 0.15 * 0.12 = 0.018</code>. However, these predicates are strongly correlated - nearly all retired people are over 65. The actual selectivity might be closer to 0.12, making the estimate off by a factor of 6.</p>\n<p>This error compounds in complex queries with many correlated predicates. The fix involves detecting common correlation patterns (age/status, date/season, geographic coordinates) and maintaining correlation statistics for the most important column combinations. When correlation statistics aren&#39;t available, using the most selective predicate alone often produces better estimates than independence assumptions.</p>\n<p>⚠️ <strong>Pitfall: Stale Statistics Causing Plan Regression</strong></p>\n<p>Statistics become stale as data changes, but the degradation isn&#39;t uniform across all table characteristics. Row counts may remain accurate while data distributions shift dramatically. Consider a table that grows from 1 million to 1.5 million rows (50% increase) while a new application feature causes 90% of queries to target recently added data. Statistics showing uniform distribution will dramatically underestimate filter selectivity for these queries.</p>\n<p>The solution requires staleness detection based on both time and data modification patterns. Track not just when statistics were last updated, but also the volume of insertions, deletions, and updates since collection. Implement automatic re-optimization triggers when plan performance deviates significantly from cost estimates.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Memory Constraints in Cost Models</strong></p>\n<p>Many cost models assume unlimited memory availability, leading to severe underestimation of costs for operations that exceed available memory. A hash join estimated to cost 1000 units with unlimited memory might actually cost 10,000 units when hash table construction forces frequent disk spills.</p>\n<p>Address this by incorporating memory pressure into cost calculations. Monitor actual memory usage during query execution and adjust cost constants based on observed spill behavior. For operations with variable memory requirements, estimate costs for both in-memory and disk-spill scenarios, weighting by the probability of each outcome.</p>\n<p>⚠️ <strong>Pitfall: Linear Cost Scaling Assumptions</strong></p>\n<p>Assuming that costs scale linearly with data size ignores important threshold effects and algorithmic complexity changes. Sorting 1,000 rows might cost 10 units, but sorting 100,000 rows doesn&#39;t cost 1,000 units - it might cost 1,500 units due to cache misses and external sort algorithms.</p>\n<p>Model these effects by using algorithmic complexity formulas rather than simple linear scaling. For sorting, use <code>O(n log n)</code> scaling. For hash operations, account for increased collision rates and memory pressure as hash tables grow. Calibrate these models against actual execution data to ensure accuracy across different data sizes.</p>\n<p>⚠️ <strong>Pitfall: Ignoring Concurrent Workload Impact</strong></p>\n<p>Cost models developed for single-user systems often fail in production environments with concurrent queries competing for resources. I/O costs increase when multiple queries access storage simultaneously, and CPU costs vary with system load.</p>\n<p>Incorporate workload-aware cost adjustments that modify base costs based on current system utilization. This requires monitoring concurrent query activity and adjusting cost models dynamically. Simple approaches include multiplying base costs by load factors, while more sophisticated systems maintain separate cost models for different concurrency levels.</p>\n<p>⚠️ <strong>Pitfall: Inadequate Error Propagation Through Plan Trees</strong></p>\n<p>Small estimation errors at leaf nodes can become large errors at the root of complex query plans. A 20% error in base table cardinality can become a 300% error after multiple joins, completely changing the optimal plan choice.</p>\n<p>Implement confidence tracking that propagates estimation uncertainty through the plan tree. Maintain confidence bounds on cardinality estimates and use these bounds to evaluate plan robustness. When confidence is low, prefer plans that perform reasonably across a wide range of actual cardinalities rather than plans that are optimal only for specific estimates.</p>\n<table>\n<thead>\n<tr>\n<th>Pitfall</th>\n<th>Detection Method</th>\n<th>Prevention Strategy</th>\n<th>Recovery Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Correlated Predicates</td>\n<td>Monitor queries with multiple filters on same table</td>\n<td>Collect correlation statistics for common column pairs</td>\n<td>Use most selective predicate when correlation unknown</td>\n</tr>\n<tr>\n<td>Stale Statistics</td>\n<td>Compare actual vs predicted cardinalities</td>\n<td>Automatic re-collection based on data change volume</td>\n<td>Gradual fallback to conservative estimates</td>\n</tr>\n<tr>\n<td>Memory Pressure</td>\n<td>Monitor query memory usage and spill events</td>\n<td>Include memory constraints in cost model</td>\n<td>Dynamic re-optimization when spills detected</td>\n</tr>\n<tr>\n<td>Nonlinear Scaling</td>\n<td>Profile actual costs across different data sizes</td>\n<td>Use algorithmic complexity formulas instead of linear</td>\n<td>Calibrate cost constants from execution feedback</td>\n</tr>\n<tr>\n<td>Concurrency Impact</td>\n<td>Track query performance during peak load periods</td>\n<td>Load-aware cost model adjustments</td>\n<td>Separate cost models for different load levels</td>\n</tr>\n<tr>\n<td>Error Propagation</td>\n<td>Confidence interval tracking through plan tree</td>\n<td>Prefer robust plans when uncertainty is high</td>\n<td>Plan re-evaluation when estimates prove inaccurate</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The cost estimation component bridges statistical analysis with practical resource modeling. This implementation provides the foundation for effective cost-based optimization while maintaining the flexibility to evolve as understanding of workload patterns improves.</p>\n<p><strong>Technology Recommendations:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statistics Storage</td>\n<td>JSON files with periodic updates</td>\n<td>Embedded SQLite database for statistics</td>\n</tr>\n<tr>\n<td>Histogram Implementation</td>\n<td>Fixed-size arrays with linear search</td>\n<td>B-tree structures for efficient range queries</td>\n</tr>\n<tr>\n<td>Cost Model Calibration</td>\n<td>Hard-coded constants from literature</td>\n<td>Machine learning-based cost model training</td>\n</tr>\n<tr>\n<td>Staleness Detection</td>\n<td>Time-based refresh schedules</td>\n<td>Trigger-based statistics invalidation</td>\n</tr>\n</tbody></table>\n<p><strong>Recommended File Structure:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>optimizer/\n  cost_estimation/\n    __init__.py                    ← Component interface\n    statistics_collector.py        ← Statistics gathering and maintenance\n    selectivity_estimator.py       ← Predicate selectivity calculation\n    cost_model.py                  ← I/O and CPU cost modeling\n    histogram.py                   ← Distribution modeling utilities\n    cost_estimation_test.py        ← Component test suite\n  data/\n    table_statistics.json          ← Cached statistics storage\n    cost_calibration.json          ← Environment-specific cost constants</code></pre></div>\n\n<p><strong>Statistics Collection Infrastructure:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Complete implementation for statistics gathering</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Any, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass, field</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> HistogramBucket</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    range_start: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    range_end: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    row_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    distinct_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    frequency: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ColumnStatistics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    distinct_values: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    null_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    min_value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    max_value: Any</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    most_common_values: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    histogram_buckets: List[HistogramBucket] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlation_with_storage: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    average_width: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TableStatistics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    table_name: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    row_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    page_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column_stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ColumnStatistics] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    last_updated: datetime </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.now)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sample_rate: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index_statistics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    clustering_factor: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StatisticsCollector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, storage_path: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"data/table_statistics.json\"</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.storage_path </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> storage_path</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cached_stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.load_cached_statistics()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> load_cached_statistics</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Load previously collected statistics from storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            with</span><span style=\"color:#79B8FF\"> open</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.storage_path, </span><span style=\"color:#9ECBFF\">'r'</span><span style=\"color:#E1E4E8\">) </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> f:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                data </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> json.load(f)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Deserialize JSON data into TableStatistics objects</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle version compatibility for statistics format changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> FileNotFoundError</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.cached_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> save_statistics</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Persist current statistics to storage.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Serialize TableStatistics objects to JSON format</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include metadata about collection time and sample rates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement atomic write to prevent corruption during updates</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Cost Model Core Logic:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CostEstimate</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    io_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_rows: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    startup_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cost_factors: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> field(</span><span style=\"color:#FFAB70\">default_factory</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_level: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">    @</span><span style=\"color:#79B8FF\">property</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> total_cost</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.io_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cpu_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.memory_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.startup_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> add_cost</span><span style=\"color:#E1E4E8\">(self, other: </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Combine two cost estimates for sequential operations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add all cost components together</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle estimated_rows appropriately based on operation type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Combine confidence levels using uncertainty propagation formulas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Merge cost_factors dictionaries for detailed cost breakdown</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> scale_by_factor</span><span style=\"color:#E1E4E8\">(self, factor: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'CostEstimate'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Scale cost estimate by a multiplier factor.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Scale all cost components by the factor</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Keep estimated_rows unchanged (scaling doesn't change output size)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Adjust confidence_level based on scaling magnitude</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Cost model constants - calibrate these for your environment</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">IO_PAGE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#6A737D\">          # Base cost per I/O page read</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">CPU_TUPLE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span><span style=\"color:#6A737D\">       # Base cost per tuple processed</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_PAGE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.001</span><span style=\"color:#6A737D\">    # Cost per memory page allocated</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SEQUENTIAL_MULTIPLIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.3</span><span style=\"color:#6A737D\">  # Efficiency factor for sequential I/O</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RANDOM_MULTIPLIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2.0</span><span style=\"color:#6A737D\">     # Penalty factor for random I/O</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CostModel</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.io_page_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> IO_PAGE_COST</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cpu_tuple_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> CPU_TUPLE_COST</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_page_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> MEMORY_PAGE_COST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimateCost</span><span style=\"color:#E1E4E8\">(self, plan: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate predicted execution cost for complete plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Traverse plan tree in post-order to calculate costs bottom-up</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Start with leaf nodes (table scans) and work toward root</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: For each operator, combine input costs with operator-specific costs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Accumulate startup costs and per-tuple costs separately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return total cost estimate with confidence bounds</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_scan_cost</span><span style=\"color:#E1E4E8\">(self, table_stats: TableStatistics, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          selectivity: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#E1E4E8\">) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate cost for table scan with optional filtering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate I/O cost based on page_count and access pattern</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply sequential multiplier for full table scans  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate CPU cost for tuple processing and filter evaluation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Estimate output rows using table row_count and selectivity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Set startup_cost to account for query setup overhead</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_join_cost</span><span style=\"color:#E1E4E8\">(self, left_input: CostEstimate, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          right_input: CostEstimate, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                          join_selectivity: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate cost for join operation between two inputs.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Choose join algorithm (hash vs nested loop) based on input sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: For hash join: build cost + probe cost + memory allocation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: For nested loop: outer loop cost * inner loop iterations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate output cardinality using join selectivity formula</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add I/O costs if join exceeds available memory (spill to disk)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Selectivity Estimation Skeleton:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SelectivityEstimator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, stats_collector: StatisticsCollector):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> stats_collector</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_filter_selectivity</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   column_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   operator: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   value: Any) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate fraction of rows surviving a filter predicate.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Get column statistics from stats collector</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle case where statistics are missing (return conservative default)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: For equality: check most_common_values or use uniform distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: For range operators: use histogram buckets for estimation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: For LIKE patterns: analyze pattern complexity and estimate</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return selectivity between 0.0 and 1.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_join_cardinality</span><span style=\"color:#E1E4E8\">(self, left_table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, left_column: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 right_table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, right_column: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate number of rows produced by join operation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Get statistics for both tables and join columns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply standard join cardinality formula: (R * S) / max(D_R, D_S)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Adjust for foreign key relationships if detected</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle case where one side has much higher cardinality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply conservative bounds to prevent extreme over/underestimation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> detect_predicate_correlation</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   predicates: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Detect correlation between multiple predicates on same table.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if predicates involve known correlated column pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: For uncorrelated predicates: multiply individual selectivities  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: For correlated predicates: use most selective predicate only</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return combined selectivity estimate accounting for correlation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Milestone Checkpoint:</strong>\nAfter implementing the cost estimation component, verify correct behavior:</p>\n<ol>\n<li><p><strong>Statistics Collection Test</strong>: Run <code>python -m pytest cost_estimation/statistics_collector_test.py -v</code></p>\n<ul>\n<li>Should collect row counts, distinct values, and basic histograms</li>\n<li>Verify statistics serialization and loading from storage</li>\n<li>Test sampling-based collection for large tables</li>\n</ul>\n</li>\n<li><p><strong>Selectivity Estimation Test</strong>: Create test tables with known data distributions</p>\n<ul>\n<li>Execute <code>collectStatistics(&quot;test_table&quot;)</code> and verify histogram accuracy  </li>\n<li>Test selectivity estimation: <code>estimate_filter_selectivity(&quot;test_table&quot;, &quot;status&quot;, &quot;=&quot;, &quot;active&quot;)</code> should return reasonable values</li>\n<li>Verify join cardinality estimates match expected results for known foreign key relationships</li>\n</ul>\n</li>\n<li><p><strong>Cost Model Validation</strong>: Compare cost estimates with actual execution measurements</p>\n<ul>\n<li>Run identical queries with different plans and measure actual execution time</li>\n<li>Verify that cost model ranks plans in same order as actual performance</li>\n<li>Check that cost estimates are within 2-3x of actual costs (closer is better, but exact accuracy isn&#39;t required)</li>\n</ul>\n</li>\n</ol>\n<p><strong>Expected Output:</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>Statistics Collection: PASS\n- Collected stats for 3 tables in 1.2 seconds\n- Histogram accuracy: 95% for uniform data, 78% for skewed data\n- Storage size: 15KB for typical table statistics\n\nCost Estimation: PASS  \n- Selectivity estimates within 50% of actual for 85% of test cases\n- Join cardinality estimates within 2x of actual for 92% of test cases\n- Plan ranking correlation with actual performance: 88%</code></pre></div>\n\n<p><strong>Debugging Tips:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>All selectivity estimates return 0.1</td>\n<td>Missing or corrupt statistics</td>\n<td>Check statistics loading, verify table exists</td>\n<td>Recollect statistics with <code>collectStatistics()</code></td>\n</tr>\n<tr>\n<td>Join cardinality estimates are wildly high</td>\n<td>Missing join predicates detected as cross product</td>\n<td>Verify join conditions in parsed query</td>\n<td>Add cross-product detection and warnings</td>\n</tr>\n<tr>\n<td>Cost estimates don&#39;t correlate with performance</td>\n<td>Cost constants not calibrated for environment</td>\n<td>Run benchmark queries and measure actual costs</td>\n<td>Adjust <code>IO_PAGE_COST</code> and <code>CPU_TUPLE_COST</code> constants</td>\n</tr>\n<tr>\n<td>Statistics collection takes too long</td>\n<td>Full table scan on large tables</td>\n<td>Check sampling rate configuration</td>\n<td>Set <code>sample_rate=0.1</code> for tables over 1M rows</td>\n</tr>\n<tr>\n<td>Histogram-based estimates worse than uniform</td>\n<td>Insufficient bucket count or poor boundaries</td>\n<td>Examine histogram bucket distribution</td>\n<td>Increase bucket count or improve boundary selection</td>\n</tr>\n</tbody></table>\n<h2 id=\"join-optimization-component\">Join Optimization Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 3 - implements dynamic programming algorithms for finding optimal join orders in multi-table queries while managing exponential search complexity.</p>\n</blockquote>\n<h3 id=\"mental-model-assembly-line-optimization\">Mental Model: Assembly Line Optimization</h3>\n<p>Think of join ordering as optimizing a manufacturing assembly line that builds complex products from multiple components. Just as a factory must decide the sequence for assembling parts into a final product, the query optimizer must determine the order for combining tables into the final result set.</p>\n<p>In a car manufacturing plant, you might have engines, transmissions, chassis, and electronics arriving from different suppliers. The assembly sequence matters enormously - you can&#39;t install the engine after the hood is welded shut, and certain combinations are more efficient than others. If you attach the transmission first, you might need specialized lifting equipment. If you install the electronics early, they might get damaged during heavy welding operations.</p>\n<p>Similarly, in query optimization, each table join is like an assembly operation that combines two intermediate results. The order determines both the cost and feasibility of subsequent operations. Joining two large tables early might create a massive intermediate result that makes all subsequent joins expensive. Conversely, applying selective joins first can dramatically reduce the working set size, making later operations much cheaper.</p>\n<p>The assembly line analogy extends to resource constraints and dependencies. Just as some manufacturing steps require specific equipment or create bottlenecks, different join orders have varying memory requirements and I/O patterns. A hash join needs sufficient memory to build hash tables, while a nested loop join might perform better with smaller inner relations. The optimizer must consider these physical constraints when determining the optimal assembly sequence.</p>\n<h3 id=\"dynamic-programming-algorithm\">Dynamic Programming Algorithm</h3>\n<p>The core challenge in join ordering is the combinatorial explosion of possibilities. For n tables, there are (2n-2)!/(n-1)! possible join orders when considering all tree shapes. Dynamic programming solves this by building optimal solutions incrementally, reusing previously computed results for subproblems.</p>\n<p><strong>Bottom-Up Subset Enumeration</strong></p>\n<p>The dynamic programming approach works by considering all possible subsets of tables and computing the cheapest way to join them. We start with individual tables (subsets of size 1), then consider all ways to join pairs of tables (subsets of size 2), and progressively build larger subsets until we have the complete query.</p>\n<p>The algorithm maintains a memoization table where each entry represents the optimal plan for joining a specific subset of tables. The key insight is that if we know the optimal way to join tables {A, B} and the optimal way to join tables {C, D}, we can evaluate the cost of combining these subproblems to form the larger subset {A, B, C, D}.</p>\n<p>Here&#39;s the step-by-step algorithm:</p>\n<ol>\n<li><p><strong>Initialize base cases</strong>: Create single-table access plans for each table in the query, including the choice between sequential scans and index scans based on any applicable predicates.</p>\n</li>\n<li><p><strong>Enumerate subsets by size</strong>: For each subset size from 2 to n tables, consider all possible combinations of tables of that size.</p>\n</li>\n<li><p><strong>Evaluate partition options</strong>: For each subset S, consider all possible ways to partition S into two non-empty subsets S1 and S2 such that S1 ∪ S2 = S and S1 ∩ S2 = ∅.</p>\n</li>\n<li><p><strong>Check join feasibility</strong>: Verify that tables in S1 and S2 can be joined directly (there exists at least one join predicate connecting the two subsets) or that cross products are acceptable.</p>\n</li>\n<li><p><strong>Calculate join cost</strong>: For each valid partition, compute the cost of joining the optimal plan for S1 with the optimal plan for S2, considering different join algorithms.</p>\n</li>\n<li><p><strong>Store optimal solution</strong>: Keep only the lowest-cost plan for subset S, storing both the plan structure and its estimated cost.</p>\n</li>\n<li><p><strong>Propagate to larger subsets</strong>: Use the optimal solutions for smaller subsets as building blocks for evaluating larger subset combinations.</p>\n</li>\n</ol>\n<p>The memoization structure stores not just costs but complete plan fragments, enabling reconstruction of the full optimal plan tree once all subsets have been evaluated.</p>\n<p><strong>Join Predicate Analysis</strong></p>\n<p>A critical aspect of the dynamic programming algorithm is determining which table combinations can be joined meaningfully. The algorithm must analyze the query&#39;s join predicates to understand the connectivity graph between tables.</p>\n<p>For each potential join between subsets S1 and S2, the algorithm searches for applicable join predicates. These might be explicit equality conditions (table1.id = table2.foreign_id), range conditions, or even complex expressions. Join predicates affect both feasibility and cost estimation.</p>\n<p>When no direct join predicate exists between two subsets, the algorithm faces a choice: either defer the combination to a later stage (when transitive join paths exist) or accept a cross product if necessary for query correctness. Cross products are generally avoided due to their explosive cardinality, but they may be unavoidable in certain query patterns.</p>\n<p><strong>Cost Accumulation Strategy</strong></p>\n<p>The dynamic programming algorithm accumulates costs from multiple sources as it builds larger join combinations. Each join operation contributes I/O costs for reading input relations, CPU costs for comparison operations, and memory costs for maintaining join state.</p>\n<p>The cost calculation must account for the fact that intermediate results from subquery joins become inputs to larger joins. This creates a dependency chain where the cardinality estimates and cost calculations compound through the join tree. Errors in early estimates can cascade through the optimization process.</p>\n<p>The algorithm also considers the different physical join algorithms available (hash join, nested loop, merge join) and their varying cost characteristics. A hash join might have high startup costs for building hash tables but low per-tuple processing costs, while nested loops have low startup costs but high per-tuple costs for large relations.</p>\n<h3 id=\"search-space-pruning-strategies\">Search Space Pruning Strategies</h3>\n<p>Without pruning techniques, dynamic programming for join ordering becomes computationally prohibitive as the number of tables grows. Several heuristic pruning strategies help manage the exponential search space while preserving solution quality.</p>\n<p><strong>Cross Product Elimination</strong></p>\n<p>One of the most effective pruning techniques is eliminating plans that create unnecessary cross products. When evaluating potential joins between subsets S1 and S2, the algorithm first checks whether any join predicates connect tables in S1 with tables in S2.</p>\n<p>If no connecting predicates exist, the algorithm can often prune this combination entirely, deferring the join until a path through other tables creates connectivity. This pruning is safe when alternative join orders exist that avoid cross products, but requires careful analysis to ensure the query remains solvable.</p>\n<p>The cross product elimination logic maintains a connectivity graph representing which tables can be joined directly. It then uses graph algorithms to determine whether joining S1 and S2 immediately is necessary or if alternative paths exist through other table combinations.</p>\n<p><strong>Cost-Based Pruning</strong></p>\n<p>The algorithm employs several cost-based pruning techniques to eliminate clearly suboptimal plans early in the search. These techniques compare partial costs and intermediate result sizes to detect when a plan branch cannot possibly lead to an optimal solution.</p>\n<p><em>Intermediate Result Size Bounds</em>: Plans that generate extremely large intermediate results are often suboptimal because they increase the cost of all subsequent operations. The algorithm can establish upper bounds on reasonable intermediate result sizes and prune plans that exceed these thresholds.</p>\n<p><em>Incremental Cost Comparison</em>: When multiple plans exist for the same subset of tables, the algorithm keeps only the best few candidates rather than just the single optimum. This allows for different plans that might be optimal depending on how they combine with other subsets, while still limiting the search space.</p>\n<p><em>Resource Constraint Checking</em>: Plans that would exceed available memory for hash joins or create temp files larger than available disk space can be pruned early, focusing the search on feasible execution strategies.</p>\n<p><strong>Heuristic Ordering Preferences</strong></p>\n<p>The algorithm incorporates several heuristic preferences that guide the search toward promising plan regions while avoiding exhaustive enumeration in less promising areas.</p>\n<p><em>Selectivity-Based Ordering</em>: Joins involving highly selective predicates are preferred early in the join sequence because they reduce intermediate result sizes. The algorithm can prioritize exploring plans that apply selective joins before less selective ones.</p>\n<p><em>Size-Based Preferences</em>: Smaller tables are often preferred as the inner relations in nested loop joins, and this preference can guide the search toward more promising plan regions without eliminating alternatives entirely.</p>\n<p><em>Index Utilization</em>: When indexes are available to support specific join predicates, the algorithm can prefer exploring plans that utilize these indexes, as they often provide significant performance benefits.</p>\n<p><strong>Search Space Partitioning</strong></p>\n<p>For queries with many tables, the algorithm can partition the search space into independent subproblems when the query structure permits. This partitioning reduces the overall complexity from exponential in the total number of tables to exponential in the size of the largest connected component.</p>\n<p>The partitioning analysis examines the join predicate graph to identify disconnected components or weakly connected regions. Each component can be optimized independently, with the results combined at a higher level. This technique is particularly effective for queries that join multiple independent fact tables with shared dimension tables.</p>\n<h3 id=\"left-deep-vs-bushy-join-trees\">Left-Deep vs Bushy Join Trees</h3>\n<p>The choice between left-deep and bushy join tree topologies represents a fundamental trade-off between optimization complexity and execution efficiency. This decision affects both the search space size during optimization and the runtime characteristics of the resulting execution plans.</p>\n<p><strong>Left-Deep Tree Characteristics</strong></p>\n<p>Left-deep trees restrict the join structure so that the right input to every join operation is a base table (or an index scan of a base table). This creates a linear chain structure where each join operation takes the result of all previous joins on the left side and adds one new table on the right side.</p>\n<p>This restriction dramatically reduces the optimization search space. For n tables, left-deep trees have only n! possible orderings compared to the much larger space of all possible tree shapes. This reduction makes optimization tractable for larger numbers of tables without requiring aggressive pruning techniques.</p>\n<p>Left-deep trees also have favorable memory usage patterns for certain join algorithms. Hash joins can build hash tables for each right-side table and probe them with the growing left-side result. This approach allows for predictable memory allocation and can take advantage of indexes on the right-side tables.</p>\n<p>However, left-deep trees can create execution inefficiencies. The left side of each join grows progressively larger as more tables are added, potentially creating large intermediate results that must be carried through the entire remaining join sequence. This growth pattern can lead to suboptimal execution when smaller intermediate results would be possible with different tree shapes.</p>\n<p><strong>Bushy Tree Advantages</strong></p>\n<p>Bushy trees allow arbitrary tree structures where both inputs to a join operation can be intermediate results from other join operations. This flexibility can enable more efficient execution plans by allowing independent subqueries to be computed in parallel and then combined.</p>\n<p>The key advantage of bushy trees is their ability to minimize intermediate result sizes by allowing optimal subproblem combinations. If tables A and B join very selectively, and tables C and D also join selectively, a bushy tree can compute (A⋈B) and (C⋈D) independently before joining these smaller results. A left-deep tree might be forced to create a larger intermediate result by joining A⋈B⋈C before adding D.</p>\n<p>Bushy trees also enable better parallelization opportunities. Independent subtrees can be computed on separate processors or threads, with synchronization only required when combining the results. This parallelism can provide significant performance benefits on multi-core systems, even if the total amount of work is similar.</p>\n<p>The optimization challenge with bushy trees is the dramatically larger search space. The number of possible bushy trees grows much more rapidly than left-deep alternatives, requiring more sophisticated pruning techniques and potentially longer optimization times.</p>\n<p><strong>Hybrid Optimization Approaches</strong></p>\n<p>Many practical query optimizers use hybrid approaches that consider both left-deep and bushy alternatives while managing the computational complexity. These approaches might start with left-deep optimization and then selectively explore bushy alternatives for promising subproblems.</p>\n<p><em>Complexity-Based Selection</em>: For queries with few tables (typically 6 or fewer), the optimizer can afford to explore bushy tree alternatives. For larger queries, it defaults to left-deep trees to maintain reasonable optimization times.</p>\n<p><em>Selectivity-Based Bushy Exploration</em>: The optimizer identifies highly selective join pairs and considers bushy trees that compute these subproblems independently. This approach focuses the additional optimization effort on cases where bushy trees are most likely to provide benefits.</p>\n<p><em>Cost Threshold Analysis</em>: When left-deep optimization produces plans with very high costs, the optimizer can invest additional time exploring bushy alternatives. This adaptive approach balances optimization time against the potential for finding significantly better plans.</p>\n<h3 id=\"architecture-decision-full-enumeration-vs-heuristics\">Architecture Decision: Full Enumeration vs Heuristics</h3>\n<p>The choice between complete dynamic programming enumeration and heuristic-based join ordering represents a critical trade-off between optimization quality and optimization time. This decision fundamentally shapes the optimizer&#39;s behavior and performance characteristics.</p>\n<blockquote>\n<p><strong>Decision: Adaptive Enumeration with Complexity Threshold</strong></p>\n<ul>\n<li><strong>Context</strong>: Join ordering optimization faces exponential search space growth, making complete enumeration impractical for large queries while heuristics may miss optimal plans for smaller queries</li>\n<li><strong>Options Considered</strong>: Always use full enumeration, always use heuristics, adaptive approach based on query complexity</li>\n<li><strong>Decision</strong>: Use full dynamic programming enumeration for queries with 8 or fewer tables, switch to heuristic ordering for larger queries</li>\n<li><strong>Rationale</strong>: Full enumeration provides guaranteed optimal solutions for small-to-medium queries where optimization time remains reasonable, while heuristics prevent optimization timeout on large queries</li>\n<li><strong>Consequences</strong>: Enables optimal plans for common query sizes while maintaining reasonable optimization latency for complex queries</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Pros</th>\n<th>Cons</th>\n<th>Query Size Limit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Full Enumeration</td>\n<td>Guaranteed optimal solution, systematic exploration, reproducible results</td>\n<td>Exponential time complexity, memory usage growth, optimization timeouts</td>\n<td>~8 tables</td>\n</tr>\n<tr>\n<td>Heuristic Ordering</td>\n<td>Linear or polynomial complexity, predictable optimization time, scales to large queries</td>\n<td>Potentially suboptimal plans, hard to validate quality, heuristic maintenance</td>\n<td>Unlimited</td>\n</tr>\n<tr>\n<td>Adaptive Threshold</td>\n<td>Best of both approaches, complexity-aware adaptation, tunable parameters</td>\n<td>Implementation complexity, threshold determination, mixed behavior</td>\n<td>Configurable</td>\n</tr>\n</tbody></table>\n<p><strong>Full Enumeration Benefits and Limitations</strong></p>\n<p>Complete dynamic programming enumeration provides theoretical optimality guarantees by systematically exploring all valid join orders and selecting the minimum-cost alternative. This approach ensures that the optimizer finds the globally optimal solution within the constraints of its cost model and search space restrictions.</p>\n<p>The systematic nature of full enumeration makes it easier to reason about optimizer behavior and debug optimization decisions. Each step of the algorithm follows predictable rules, and the final plan selection can be traced back through the dynamic programming table to understand why specific choices were made.</p>\n<p>However, full enumeration becomes computationally prohibitive as query complexity grows. The combination of exponential search space growth and the need for accurate cost estimation at each step creates optimization times that can exceed query execution times for large queries. This limitation makes full enumeration impractical for data warehouse workloads or queries involving many tables.</p>\n<p>Memory usage also becomes problematic with full enumeration. The dynamic programming table stores optimal plans for all possible table subsets, and the memory requirement grows exponentially with the number of tables. For queries with 12+ tables, the optimization process might exceed available memory before completing.</p>\n<p><strong>Heuristic Approaches</strong></p>\n<p>Heuristic join ordering uses rules and approximations to quickly identify good (though not necessarily optimal) join orders without exhaustive search. These approaches typically run in polynomial time and can handle queries with dozens or hundreds of tables.</p>\n<p><em>Greedy Selection Heuristics</em>: These approaches iteratively select the next table to join based on local cost metrics, such as choosing the join that produces the smallest intermediate result or has the highest selectivity. While fast, greedy approaches can make early decisions that preclude globally optimal solutions.</p>\n<p><em>Graph-Based Heuristics</em>: These techniques analyze the join predicate graph structure to identify promising join orders. They might prioritize joining tables that form cliques in the predicate graph or use graph traversal algorithms to determine join sequences.</p>\n<p><em>Statistical Heuristics</em>: These approaches use table and column statistics to estimate join costs without exhaustive plan enumeration. They might sort tables by size and apply rules about joining small tables before large ones, or prioritize joins with high selectivity predicates.</p>\n<p>The main challenge with heuristic approaches is validating their quality. Unlike full enumeration, there&#39;s no guarantee that heuristic-selected plans are optimal or even close to optimal. Poorly chosen heuristics can produce plans that are orders of magnitude slower than optimal plans.</p>\n<p><strong>Adaptive Implementation Strategy</strong></p>\n<p>The adaptive approach monitors query complexity and optimization progress to decide between enumeration and heuristic strategies dynamically. This implementation provides flexibility while maintaining performance guarantees for different query patterns.</p>\n<p>The complexity assessment considers multiple factors beyond just table count: the number of join predicates, the presence of complex expressions, the availability of indexes, and the size of table statistics. Queries with many tables but simple star-schema join patterns might remain tractable for full enumeration, while queries with fewer tables but complex predicate structures might benefit from heuristic approaches.</p>\n<p>The adaptive implementation also includes timeout mechanisms that can switch from enumeration to heuristics if optimization takes too long. This approach ensures that optimization time remains bounded while still attempting to find optimal solutions when feasible.</p>\n<p><strong>Implementation Complexity Considerations</strong></p>\n<p>The adaptive approach requires implementing both full enumeration and heuristic algorithms, along with the decision logic for choosing between them. This implementation complexity must be weighed against the benefits of having both approaches available.</p>\n<p>The decision logic itself needs tuning and validation. The thresholds for switching between approaches affect both optimization time and plan quality, and finding good default values requires extensive testing with representative query workloads.</p>\n<p>Maintenance overhead increases with the adaptive approach because both optimization strategies require ongoing development and bug fixes. Changes to cost models or statistics collection affect both code paths, and testing must verify correct behavior across the full range of query complexities.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fjoin-ordering-algorithm.svg\" alt=\"Dynamic Programming Join Ordering\"></p>\n<h3 id=\"common-join-ordering-pitfalls\">Common Join Ordering Pitfalls</h3>\n<p>Understanding common mistakes in join ordering implementation helps avoid subtle bugs that can lead to suboptimal plans or incorrect results. These pitfalls often arise from the complexity of managing state across recursive optimization calls and handling edge cases in the dynamic programming algorithm.</p>\n<p>⚠️ <strong>Pitfall: Incorrect Subset Enumeration</strong></p>\n<p>A frequent implementation mistake is generating invalid or duplicate table subsets during the dynamic programming enumeration. This can happen when the subset generation logic doesn&#39;t properly handle bit manipulation for representing table sets, or when the enumeration algorithm visits the same subset multiple times with different internal representations.</p>\n<p>The error typically manifests as missing optimal plans for certain table combinations or performance degradation due to redundant cost calculations. In some cases, the optimizer might compute different costs for the same logical subset, leading to inconsistent plan selection.</p>\n<p>To avoid this pitfall, use a canonical representation for table subsets (such as sorted bit vectors) and implement explicit duplicate detection. Validate subset enumeration logic with small test cases where all combinations can be verified manually before testing with larger queries.</p>\n<p>⚠️ <strong>Pitfall: Cross Product Explosion</strong></p>\n<p>Failing to properly handle cross products can lead to catastrophic optimization performance and incorrect cost estimates. This happens when the algorithm doesn&#39;t recognize disconnected table groups or when it attempts to optimize queries that require cross products without appropriate safeguards.</p>\n<p>Cross products create intermediate results whose cardinality is the product of input cardinalities, leading to extremely large cost estimates that can overflow numeric types or consume excessive memory during optimization. The algorithm might spend most of its time exploring clearly suboptimal cross product plans instead of focusing on connected join alternatives.</p>\n<p>Implement explicit cross product detection by maintaining a connectivity graph of tables based on join predicates. Before evaluating any join combination, check whether the tables can be connected through existing predicates. For queries that legitimately require cross products, implement separate handling with appropriate cost model adjustments and resource limit checks.</p>\n<p>⚠️ <strong>Pitfall: Memoization Cache Corruption</strong></p>\n<p>The dynamic programming memoization table can become corrupted when multiple optimization threads access it concurrently or when the cache key generation doesn&#39;t account for all relevant plan properties. This leads to incorrect plan reuse where the cached plan doesn&#39;t match the actual requirements for the current optimization context.</p>\n<p>Cache corruption typically appears as inconsistent optimization results where running the same query multiple times produces different plans or costs. It might also manifest as assertion failures when cached plans reference tables that aren&#39;t part of the current optimization subset.</p>\n<p>Design the memoization key to include all properties that affect plan equivalence, including table subsets, join predicates, sort orders, and physical property requirements. If using concurrent optimization, implement proper synchronization around cache access or use thread-local caches with explicit merging strategies.</p>\n<p>⚠️ <strong>Pitfall: Cost Model Inconsistencies</strong></p>\n<p>Inconsistent cost calculations between different parts of the optimization algorithm can lead to incorrect plan comparisons and suboptimal selections. This often occurs when the cost estimation logic makes different assumptions about data distribution or physical properties during subset optimization versus final plan construction.</p>\n<p>Cost inconsistencies might cause the optimizer to select plans that appear optimal during dynamic programming but perform poorly during execution. The problem is particularly subtle because cost estimation errors often compound through the join tree, making it difficult to trace the root cause of poor plan selection.</p>\n<p>Implement cost estimation as a centralized service with well-defined interfaces and assumptions. Document all cost model parameters and ensure that the same statistical information and calculation methods are used consistently throughout the optimization process. Include validation checks that verify cost calculations remain consistent when plans are reconstructed from the memoization table.</p>\n<p>⚠️ <strong>Pitfall: Predicate Placement Errors</strong></p>\n<p>Incorrectly handling join predicates during plan construction can result in plans that apply predicates at the wrong join levels or fail to apply them at all. This happens when the optimization algorithm doesn&#39;t properly track which predicates are satisfied by each join operation or when it assumes predicates can be applied at arbitrary points in the join tree.</p>\n<p>Predicate placement errors can lead to incorrect query results if selective predicates aren&#39;t applied, or to inefficient execution if predicates are applied later than necessary. The errors are often difficult to detect because the final query results might still be correct even when predicates are applied suboptimally.</p>\n<p>Implement explicit predicate tracking that associates each join predicate with the specific table combinations that satisfy it. Validate during plan construction that all join predicates are applied exactly once at the appropriate join level. Use predicate pushdown analysis to move selection predicates as early as possible in the join tree while maintaining correctness.</p>\n<p>⚠️ <strong>Pitfall: Memory Management in Large Search Spaces</strong></p>\n<p>Poor memory management during dynamic programming can lead to excessive memory usage or memory leaks that crash the optimization process. This is particularly problematic for queries with many tables where the memoization table can grow very large and intermediate plan structures accumulate throughout the optimization process.</p>\n<p>Memory problems might appear as gradual performance degradation as optimization progresses, or as sudden crashes when available memory is exhausted. In some cases, the optimizer might succeed but consume so much memory that subsequent query execution fails due to resource constraints.</p>\n<p>Implement explicit memory bounds for the optimization process and monitor memory usage during dynamic programming. Use techniques like plan structure sharing to reduce memory overhead, and implement garbage collection for intermediate results that are no longer needed. Consider disk-based memoization for very large queries that exceed reasonable memory limits.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The join optimization component represents the most algorithmically complex part of the query optimizer, requiring careful implementation of dynamic programming algorithms and sophisticated data structures for managing the exponential search space.</p>\n<p><strong>Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Subset Representation</td>\n<td>Python sets with frozenset</td>\n<td>Bit vectors with numpy arrays</td>\n</tr>\n<tr>\n<td>Memoization Storage</td>\n<td>Dictionary with tuple keys</td>\n<td>Custom hash table with optimized keys</td>\n</tr>\n<tr>\n<td>Cost Calculation</td>\n<td>Direct arithmetic operations</td>\n<td>Vectorized operations with numpy</td>\n</tr>\n<tr>\n<td>Graph Analysis</td>\n<td>NetworkX for connectivity</td>\n<td>Custom adjacency lists for performance</td>\n</tr>\n</tbody></table>\n<p><strong>File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>query_optimizer/\n  join_optimizer/\n    __init__.py\n    dp_optimizer.py           ← dynamic programming implementation\n    cost_calculator.py        ← join cost estimation\n    plan_enumerator.py       ← subset enumeration and plan generation\n    pruning_strategies.py    ← search space reduction techniques\n    join_graph.py           ← predicate connectivity analysis\n    heuristic_optimizer.py  ← fallback for large queries\n    test_join_optimizer.py  ← comprehensive test suite</code></pre></div>\n\n<p><strong>Infrastructure Starter Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># join_graph.py - Complete predicate connectivity analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Set, List, Dict, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JoinPredicate</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a join condition between tables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    left_table: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    left_column: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    right_table: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    right_column: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operator: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # '=', '&#x3C;', '>', etc.</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    selectivity: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JoinGraph</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Analyzes table connectivity through join predicates.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], predicates: List[JoinPredicate]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.tables </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(tables)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.predicates </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> predicates</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.adjacency </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._build_adjacency_graph()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.components </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._find_connected_components()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _build_adjacency_graph</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Build adjacency list representation of table connectivity.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        graph </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> predicate </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.predicates:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            graph[predicate.left_table].add(predicate.right_table)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            graph[predicate.right_table].add(predicate.left_table)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> dict</span><span style=\"color:#E1E4E8\">(graph)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _find_connected_components</span><span style=\"color:#E1E4E8\">(self) -> List[Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find disconnected components using DFS.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        visited </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        components </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> table </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.tables:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> table </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> visited:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._dfs(table, visited, component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                components.append(component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> components</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _dfs</span><span style=\"color:#E1E4E8\">(self, table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, visited: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], component: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Depth-first search for component discovery.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        visited.add(table)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component.add(table)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> neighbor </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.adjacency.get(table, </span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">()):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> neighbor </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> visited:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._dfs(neighbor, visited, component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> can_join_directly</span><span style=\"color:#E1E4E8\">(self, subset1: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], subset2: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if two table subsets have connecting predicates.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> predicate </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.predicates:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> ((predicate.left_table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subset1 </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> predicate.right_table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subset2) </span><span style=\"color:#F97583\">or</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                (predicate.left_table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subset2 </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> predicate.right_table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subset1)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_connecting_predicates</span><span style=\"color:#E1E4E8\">(self, subset1: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], subset2: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> List[JoinPredicate]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return predicates that connect two table subsets.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        connecting </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> predicate </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.predicates:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> ((predicate.left_table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subset1 </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> predicate.right_table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subset2) </span><span style=\"color:#F97583\">or</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                (predicate.left_table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subset2 </span><span style=\"color:#F97583\">and</span><span style=\"color:#E1E4E8\"> predicate.right_table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> subset1)):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                connecting.append(predicate)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> connecting</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># cost_calculator.py - Complete join cost estimation utilities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimate, TableStatistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JoinCostCalculator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Estimates costs for different join algorithms and configurations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, table_stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.table_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> table_stats</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.io_page_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> IO_PAGE_COST</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cpu_tuple_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> CPU_TUPLE_COST</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_page_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> MEMORY_PAGE_COST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_hash_join_cost</span><span style=\"color:#E1E4E8\">(self, left_plan, right_plan, join_predicates) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate hash join cost with build and probe phases.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Build phase: scan right relation and build hash table</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        build_io </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> right_plan.cost_estimate.io_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        build_cpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> right_plan.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cpu_tuple_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#6A737D\">  # hash + store</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        build_memory </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> right_plan.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.1</span><span style=\"color:#6A737D\">  # hash table overhead</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Probe phase: scan left relation and probe hash table</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        probe_io </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> left_plan.cost_estimate.io_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        probe_cpu </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> left_plan.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cpu_tuple_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1.5</span><span style=\"color:#6A737D\">  # hash + lookup</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Output phase: write matching tuples</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_rows </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_join_cardinality(left_plan, right_plan, join_predicates)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> output_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cpu_tuple_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.5</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">build_io </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> probe_io,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">build_cpu </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> probe_cpu </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> output_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">build_memory,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">output_rows,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            startup_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">build_io </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> build_cpu,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cost_factors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'algorithm'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'hash_join'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'build_side'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'right'</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            confidence_level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.8</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_nested_loop_cost</span><span style=\"color:#E1E4E8\">(self, outer_plan, inner_plan, join_predicates) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate nested loop join cost.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Outer relation scan cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        outer_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> outer_plan.cost_estimate.total_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Inner relation scanned once per outer tuple</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inner_scans </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> outer_plan.estimated_rows</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        inner_cost_per_scan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> inner_plan.cost_estimate.total_cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_inner_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> inner_scans </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> inner_cost_per_scan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Join processing cost</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        comparisons </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> outer_plan.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> inner_plan.estimated_rows</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        join_cpu_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> comparisons </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cpu_tuple_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        output_rows </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_join_cardinality(outer_plan, inner_plan, join_predicates)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">outer_plan.cost_estimate.io_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> total_inner_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">outer_plan.cost_estimate.cpu_cost </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> join_cpu_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.01</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># minimal memory for nested loops</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">output_rows,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            startup_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">outer_plan.cost_estimate.startup_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cost_factors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'algorithm'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">'nested_loop'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'outer_relation'</span><span style=\"color:#E1E4E8\">: outer_plan.node_id},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            confidence_level</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.9</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_join_cardinality</span><span style=\"color:#E1E4E8\">(self, left_plan, right_plan, join_predicates) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate number of rows produced by join.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> join_predicates:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Cross product</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> left_plan.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> right_plan.estimated_rows</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Use most selective predicate for estimation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        min_selectivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(pred.selectivity </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> pred </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> join_predicates)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(left_plan.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> right_plan.estimated_rows </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> min_selectivity)</span></span></code></pre></div>\n\n<p><strong>Core Logic Skeleton</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># dp_optimizer.py - Dynamic programming join optimization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Set, List, Optional, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> itertools </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> combinations</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExecutionPlan, OperatorNode, ParsedQuery</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> DynamicProgrammingOptimizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Implements dynamic programming algorithm for optimal join ordering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, cost_calculator, max_tables</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">MAX_JOIN_ENUMERATION</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_calculator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cost_calculator</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_tables </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_tables</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memo_table: Dict[</span><span style=\"color:#79B8FF\">frozenset</span><span style=\"color:#E1E4E8\">, ExecutionPlan] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.join_graph </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> optimizeJoinOrder</span><span style=\"color:#E1E4E8\">(self, tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], join_predicates: List) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Find optimal join order using dynamic programming.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns the minimum cost plan for joining all specified tables.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if query size exceeds enumeration threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Initialize join graph for connectivity analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Create base case plans for single tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Enumerate subsets by size from 2 to len(tables)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: For each subset, find optimal partitioning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return optimal plan for complete table set</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use frozenset for subset representation in memo table</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _enumerate_subsets_by_size</span><span style=\"color:#E1E4E8\">(self, tables: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> List[Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Generate all possible subsets of specified size.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns list of table subsets for dynamic programming enumeration.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use itertools.combinations to generate subsets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Convert combinations to sets for easier manipulation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Filter out subsets that form disconnected components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return valid subsets for optimization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: combinations(tables, size) generates all size-k subsets</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _find_optimal_partition</span><span style=\"color:#E1E4E8\">(self, subset: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Tuple[ExecutionPlan, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Find the lowest-cost way to partition subset into two joined parts.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns optimal plan and its cost for the given table subset.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate all possible ways to split subset into two non-empty parts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each partition, check if tables can be joined (connectivity)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Look up optimal plans for both parts in memo table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate cost of joining the two parts with different algorithms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Track minimum cost partition and corresponding plan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return best plan and cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use subset.difference() to generate complementary partitions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _evaluate_join_cost</span><span style=\"color:#E1E4E8\">(self, left_plan: ExecutionPlan, right_plan: ExecutionPlan, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           join_predicates: List) -> List[Tuple[ExecutionPlan, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Evaluate cost of joining two plans with different physical join algorithms.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns list of (plan, cost) tuples for each viable join method.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check input sizes to determine viable join algorithms</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Estimate hash join cost if right relation fits in memory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Estimate nested loop cost (always viable)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Estimate merge join cost if inputs are sorted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Create ExecutionPlan for each algorithm with proper operator nodes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Return all viable options sorted by cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Hash join preferred when inner relation &#x3C; available_memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_base_case_plans</span><span style=\"color:#E1E4E8\">(self, tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Initialize memo table with single-table access plans.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Creates optimal plans for accessing each table individually.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: For each table, determine available access methods</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Compare sequential scan vs index scan costs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Apply any single-table filter predicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create ExecutionPlan with chosen access method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Store plan in memo table with frozenset([table]) as key</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use table statistics to choose between scan methods</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _check_connectivity</span><span style=\"color:#E1E4E8\">(self, subset1: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], subset2: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Check if two table subsets can be joined directly.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns True if join predicates connect the subsets.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Use join_graph to check for connecting predicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Return False if no direct connection exists (would create cross product)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Handle special case where cross products are explicitly allowed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: self.join_graph.can_join_directly(subset1, subset2)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _build_join_plan</span><span style=\"color:#E1E4E8\">(self, left_plan: ExecutionPlan, right_plan: ExecutionPlan,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        join_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, join_predicates: List) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Construct ExecutionPlan for joining two subplans.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns complete plan tree with proper cost annotations.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create OperatorNode for the join operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Set join_type and join_predicates in operator properties</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Add left_plan and right_plan as children</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate and set cost estimate for the join node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Determine output schema by combining input schemas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Create ExecutionPlan wrapping the join operator tree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Join output schema = left_schema + right_schema - duplicate keys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Language-Specific Hints</strong></p>\n<ul>\n<li>Use <code>frozenset()</code> for immutable table subset representations that can serve as dictionary keys in the memoization table</li>\n<li>Implement subset enumeration with <code>itertools.combinations()</code> for clean, readable code that handles all size-k combinations</li>\n<li>Use <code>dataclasses</code> with <code>@dataclass</code> decorator for clean cost estimation and plan representation structures</li>\n<li>Consider <code>lru_cache</code> decorator for memoizing frequently computed cost estimates, but be careful with memory usage</li>\n<li>Use <code>typing.Union</code> and <code>Optional</code> for clear function signatures that handle nullable plan references</li>\n<li>Implement custom <code>__hash__</code> and <code>__eq__</code> methods for plan structures if using them as dictionary keys</li>\n</ul>\n<p><strong>Milestone Checkpoints</strong></p>\n<p>After implementing the dynamic programming join optimizer:</p>\n<ol>\n<li><p><strong>Subset Enumeration Test</strong>: Run <code>python -m pytest test_join_optimizer.py::test_subset_enumeration</code> to verify that all valid table combinations are generated correctly for small queries.</p>\n</li>\n<li><p><strong>Cost Calculation Validation</strong>: Execute test queries with known optimal join orders and verify that the optimizer selects the expected plans. Use <code>EXPLAIN</code> output to compare with expected join sequences.</p>\n</li>\n<li><p><strong>Cross Product Handling</strong>: Test queries without join predicates to ensure cross products are handled appropriately - either rejected with errors or optimized with proper cost penalties.</p>\n</li>\n<li><p><strong>Memory Usage Monitoring</strong>: Profile optimization of queries with 6-8 tables to ensure memory usage remains reasonable (under 100MB for typical table counts).</p>\n</li>\n<li><p><strong>Performance Benchmarks</strong>: Measure optimization times for queries of different complexity levels:</p>\n<ul>\n<li>3-4 tables: &lt; 10ms optimization time</li>\n<li>5-6 tables: &lt; 100ms optimization time  </li>\n<li>7-8 tables: &lt; 1 second optimization time</li>\n</ul>\n</li>\n</ol>\n<p><strong>Debugging Tips</strong></p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Optimization never completes</td>\n<td>Infinite recursion in subset enumeration</td>\n<td>Add logging to track subset generation progress</td>\n<td>Add base case checks and maximum depth limits</td>\n</tr>\n<tr>\n<td>Wrong join order selected</td>\n<td>Cost estimation errors or missing predicates</td>\n<td>Compare actual vs estimated costs for different join pairs</td>\n<td>Verify selectivity calculations and predicate connectivity</td>\n</tr>\n<tr>\n<td>Memory usage grows without bound</td>\n<td>Memo table not being cleaned up</td>\n<td>Monitor memo table size during optimization</td>\n<td>Implement LRU eviction or periodic cleanup</td>\n</tr>\n<tr>\n<td>Cross product plans generated</td>\n<td>Missing join predicate detection</td>\n<td>Check join graph connectivity analysis</td>\n<td>Verify predicate parsing and graph construction</td>\n</tr>\n<tr>\n<td>Inconsistent optimization results</td>\n<td>Race conditions in concurrent optimization</td>\n<td>Run single-threaded to isolate threading issues</td>\n<td>Add proper synchronization or use thread-local storage</td>\n</tr>\n</tbody></table>\n<h2 id=\"physical-planning-component\">Physical Planning Component</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Milestone 4 - selects concrete physical operators and access methods, applies optimization rules like predicate pushdown, and generates final executable plans.</p>\n</blockquote>\n<h3 id=\"mental-model-tool-selection\">Mental Model: Tool Selection</h3>\n<p>Think of physical planning as the process a master craftsman goes through when translating architectural blueprints into a concrete construction plan. The architect&#39;s drawings show what needs to be built - a foundation, walls, roof, electrical systems - but they don&#39;t specify which specific tools and techniques to use. The craftsman must decide: Should I use a pneumatic nail gun or a traditional hammer? Is this job better suited for a circular saw or a miter saw? Should I frame this wall with metal studs or wooden ones?</p>\n<p>In query optimization, we face the same decisions. The logical plan tells us what operations need to happen - scan tables, filter rows, join datasets, project columns - but it doesn&#39;t specify how to perform these operations. Physical planning is where we make the concrete tool choices: Should we use a sequential scan or leverage an index? Is this join better served by a hash join algorithm or nested loops? Should we apply this filter before or after the join?</p>\n<p>Just as a master craftsman considers factors like material properties, tool availability, workspace constraints, and time requirements, our physical planner considers data characteristics, available indexes, memory constraints, and performance requirements. The goal is the same: transform an abstract plan into an executable sequence of concrete actions that efficiently accomplishes the desired outcome.</p>\n<p>The physical planner serves as the final decision maker in our optimization pipeline, taking the logically correct but implementation-agnostic plan from the join optimizer and transforming it into a detailed execution strategy with specific operators, access methods, and optimization transformations applied.</p>\n<h3 id=\"index-vs-sequential-scan-selection\">Index vs Sequential Scan Selection</h3>\n<p>The choice between index access and sequential scanning represents one of the most fundamental decisions in physical planning. This decision dramatically impacts query performance, often determining whether a query completes in milliseconds or minutes. The decision logic must balance multiple factors including data selectivity, index availability, data clustering, and system resources.</p>\n<p><strong>Selectivity-Based Decision Framework</strong></p>\n<p>The primary factor driving scan selection is <strong>predicate selectivity</strong> - the fraction of rows that survive the filter conditions. When selectivity is low (few rows match), indexes provide significant benefit by avoiding the need to examine irrelevant data. When selectivity is high (most rows match), sequential scanning often proves more efficient due to reduced overhead and better cache utilization.</p>\n<p>Our decision framework uses the <code>SELECTIVITY_THRESHOLD</code> constant as the primary decision boundary. For predicates with estimated selectivity below this threshold, we prefer index access when suitable indexes exist. Above the threshold, sequential scanning typically provides better performance unless other factors override this preference.</p>\n<p>The selectivity calculation process integrates with our cost estimation component through the <code>estimate_filter_selectivity</code> method, which combines column statistics with predicate analysis to predict the fraction of rows surviving each filter condition. This estimation accounts for data distribution patterns captured in our <code>ColumnStatistics</code> histograms and handles common predicate types including equality, range, and pattern matching conditions.</p>\n<p><strong>Access Method Cost Comparison</strong></p>\n<p>The <code>selectPhysicalOperators</code> method implements a comprehensive cost comparison between available access methods. For each table access in the logical plan, we evaluate the following options:</p>\n<table>\n<thead>\n<tr>\n<th>Access Method</th>\n<th>Cost Components</th>\n<th>Best When</th>\n<th>Avoided When</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential Scan</td>\n<td><code>IO_PAGE_COST * page_count + CPU_TUPLE_COST * row_count</code></td>\n<td>High selectivity, no suitable indexes</td>\n<td>Time-critical queries with low selectivity</td>\n</tr>\n<tr>\n<td>Index Scan</td>\n<td><code>IO_PAGE_COST * index_pages + RANDOM_MULTIPLIER * data_pages + CPU_TUPLE_COST * matching_rows</code></td>\n<td>Low selectivity, covering index available</td>\n<td>High selectivity, outdated statistics</td>\n</tr>\n<tr>\n<td>Index-Only Scan</td>\n<td><code>IO_PAGE_COST * index_pages + CPU_TUPLE_COST * matching_rows</code></td>\n<td>Low selectivity, all columns in index</td>\n<td>Index doesn&#39;t cover required columns</td>\n</tr>\n<tr>\n<td>Bitmap Index Scan</td>\n<td><code>IO_PAGE_COST * (index_pages + unique_data_pages) + CPU_TUPLE_COST * matching_rows</code></td>\n<td>Multiple indexes, moderate selectivity</td>\n<td>Very high or very low selectivity</td>\n</tr>\n</tbody></table>\n<p>The cost calculation incorporates the <code>RANDOM_MULTIPLIER</code> penalty for index scans because they typically generate random I/O patterns when accessing the underlying table data. Sequential scans benefit from the <code>SEQUENTIAL_MULTIPLIER</code> discount because they read data in storage order, maximizing cache efficiency and minimizing seek time.</p>\n<p><strong>Index Selection Algorithm</strong></p>\n<p>When multiple indexes exist for a table, our selection algorithm evaluates each candidate index based on several criteria:</p>\n<ol>\n<li><strong>Predicate Coverage</strong>: Indexes covering more filter predicates receive higher priority scores</li>\n<li><strong>Column Prefix Matching</strong>: B-tree indexes are most effective when predicates match the leftmost columns in the index key</li>\n<li><strong>Index Selectivity</strong>: Indexes with high cardinality (many distinct values) provide better filtering for equality predicates</li>\n<li><strong>Clustering Factor</strong>: Indexes with low clustering factors (data stored in index order) minimize random I/O overhead</li>\n</ol>\n<p>The algorithm maintains a candidate scoring system where each index receives points based on these factors. The highest-scoring index becomes the primary access method, with additional indexes potentially used for bitmap combining when multiple predicates exist.</p>\n<p><strong>Storage Layout Considerations</strong></p>\n<p>Physical planning must account for how data is physically organized on storage devices. Tables with good <strong>clustering</strong> on frequently queried columns benefit more from sequential scans because related data remains physically co-located. Conversely, tables with random data distribution see greater benefits from index access because the clustering advantage of sequential scanning is already lost.</p>\n<p>Our <code>TableStatistics</code> structure captures the <code>clustering_factor</code> metric, which measures how well the table&#39;s physical storage order matches the logical ordering of frequently accessed data. High clustering factors indicate random data distribution, making index access relatively more attractive even at moderate selectivity levels.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: The break-even point between index and sequential access is not fixed - it shifts based on data characteristics, available memory, and concurrent workload. Adaptive systems monitor actual execution performance to refine these thresholds over time.</p>\n</blockquote>\n<h3 id=\"join-algorithm-selection\">Join Algorithm Selection</h3>\n<p>Join algorithm selection represents one of the most performance-critical decisions in physical planning. Different join algorithms excel under different conditions, and the wrong choice can result in orders-of-magnitude performance differences. Our selection framework evaluates input characteristics, available memory, and data distribution patterns to choose the most appropriate join implementation.</p>\n<p><strong>Join Algorithm Comparison Matrix</strong></p>\n<table>\n<thead>\n<tr>\n<th>Algorithm</th>\n<th>Memory Requirement</th>\n<th>Best Input Sizes</th>\n<th>I/O Pattern</th>\n<th>CPU Overhead</th>\n<th>Preferred When</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Nested Loop</td>\n<td>Minimal</td>\n<td>Small × Any</td>\n<td>Sequential + Random</td>\n<td>Low</td>\n<td>Small outer, good indexes on inner</td>\n</tr>\n<tr>\n<td>Hash Join</td>\n<td>Build side fits in memory</td>\n<td>Large × Large</td>\n<td>Sequential both sides</td>\n<td>Moderate</td>\n<td>Similar sizes, sufficient memory</td>\n</tr>\n<tr>\n<td>Merge Join</td>\n<td>Minimal</td>\n<td>Any × Any</td>\n<td>Sequential both sides</td>\n<td>Low</td>\n<td>Both inputs pre-sorted</td>\n</tr>\n<tr>\n<td>Index Nested Loop</td>\n<td>Minimal</td>\n<td>Any × Any</td>\n<td>Random on inner</td>\n<td>Low</td>\n<td>Selective join predicates, fast indexes</td>\n</tr>\n</tbody></table>\n<p>The selection algorithm implemented in the <code>_evaluate_join_cost</code> method compares the estimated costs of applicable join algorithms and selects the option with the lowest total cost. This comparison accounts for I/O costs, CPU processing costs, and memory allocation costs using our unified <code>CostEstimate</code> framework.</p>\n<p><strong>Hash Join Selection Logic</strong></p>\n<p>Hash joins excel when one input (the <strong>build side</strong>) fits comfortably in available memory while the other input (the <strong>probe side</strong>) can be any size. Our selection logic estimates the memory footprint of the smaller input relation and compares it against available buffer pool space.</p>\n<p>The cost calculation for hash joins includes several components:</p>\n<ul>\n<li><strong>Build Phase</strong>: Sequential scan of build relation plus hash table construction</li>\n<li><strong>Probe Phase</strong>: Sequential scan of probe relation plus hash lookups</li>\n<li><strong>Memory Overhead</strong>: Hash table space allocation and maintenance</li>\n</ul>\n<p>When the estimated build side exceeds available memory, the algorithm must consider <strong>hash partitioning</strong> strategies that divide the inputs into smaller chunks that fit in memory. The cost model accounts for the additional I/O overhead of writing and re-reading partitioned data.</p>\n<p><strong>Nested Loop Optimization</strong></p>\n<p>While nested loop joins have poor theoretical complexity (O(n×m)), they prove highly effective in several practical scenarios. Our selection logic identifies these favorable conditions:</p>\n<ol>\n<li><strong>Small Outer Relation</strong>: When the outer loop iterates over few rows, the inner relation access cost dominates</li>\n<li><strong>Index-Backed Inner Access</strong>: Fast index lookups on the inner relation can make nested loops competitive with hash joins</li>\n<li><strong>Early Termination</strong>: LIMIT clauses or EXISTS predicates allow nested loops to terminate early</li>\n</ol>\n<p>The <code>Index Nested Loop</code> variant leverages indexes on the inner relation to avoid full table scans for each outer row. This approach works particularly well when join predicates have high selectivity and suitable indexes exist on the inner table&#39;s join columns.</p>\n<p><strong>Merge Join Considerations</strong></p>\n<p>Merge joins require both inputs to be sorted on the join columns, but they provide excellent performance characteristics when this prerequisite is satisfied. Our selection logic considers merge joins in these scenarios:</p>\n<ul>\n<li>Both inputs are already sorted (from previous operations or indexes)</li>\n<li>The sort cost plus merge cost is lower than hash join alternatives</li>\n<li>Memory constraints make hash joins impractical</li>\n</ul>\n<p>The cost estimation for merge joins must account for potential sorting overhead when inputs are not pre-sorted. This calculation uses the cost model for sort operations, which depends on input size and available memory for sort buffers.</p>\n<p><strong>Multi-Table Join Algorithm Selection</strong></p>\n<p>For queries involving multiple tables, join algorithm selection becomes more complex because different join pairs within the same query may benefit from different algorithms. Our optimization framework evaluates algorithm choices independently for each join operation in the optimized join order.</p>\n<p>This independent evaluation allows for <strong>heterogeneous join plans</strong> where early joins might use hash algorithms (when memory is available) while later joins switch to nested loops (when intermediate results are small) or merge joins (when data is already sorted from previous operations).</p>\n<h3 id=\"optimization-rules-and-predicate-pushdown\">Optimization Rules and Predicate Pushdown</h3>\n<p>Optimization rules transform query plans to improve execution efficiency without changing semantic correctness. These rule-based transformations complement our cost-based optimization by applying proven techniques that consistently improve performance across diverse workloads. The most impactful rule is <strong>predicate pushdown</strong>, but our framework includes several categories of optimization rules.</p>\n<p><strong>Predicate Pushdown Implementation</strong></p>\n<p>Predicate pushdown moves filter operations as close to data sources as possible, reducing the volume of data flowing through the query execution pipeline. This optimization can dramatically improve performance by eliminating irrelevant rows early in the execution process.</p>\n<p>Our predicate pushdown implementation analyzes the dependency relationships between filter predicates and data sources. The algorithm works through the following steps:</p>\n<ol>\n<li><strong>Predicate Classification</strong>: Categorize each filter predicate based on the tables and columns it references</li>\n<li><strong>Dependency Analysis</strong>: Determine which predicates can be evaluated at each point in the execution tree</li>\n<li><strong>Safety Verification</strong>: Ensure that moving predicates doesn&#39;t change query semantics (particularly important for outer joins)</li>\n<li><strong>Cost Benefit Analysis</strong>: Verify that pushdown actually improves performance (some predicates are expensive to evaluate)</li>\n</ol>\n<p>The pushdown algorithm distinguishes between different types of predicates:</p>\n<table>\n<thead>\n<tr>\n<th>Predicate Type</th>\n<th>Pushdown Strategy</th>\n<th>Example</th>\n<th>Semantic Considerations</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Single-table filters</td>\n<td>Push to table scan</td>\n<td><code>WHERE customer.age &gt; 25</code></td>\n<td>Always safe</td>\n</tr>\n<tr>\n<td>Join predicates</td>\n<td>Keep at join level</td>\n<td><code>WHERE orders.customer_id = customer.id</code></td>\n<td>Don&#39;t push past the join</td>\n</tr>\n<tr>\n<td>Cross-table filters</td>\n<td>Push past inner joins only</td>\n<td><code>WHERE orders.amount &gt; customer.credit_limit</code></td>\n<td>Unsafe for outer joins</td>\n</tr>\n<tr>\n<td>Computed predicates</td>\n<td>Evaluate pushdown cost</td>\n<td><code>WHERE UPPER(name) LIKE &#39;JOHN%&#39;</code></td>\n<td>May be expensive to evaluate early</td>\n</tr>\n</tbody></table>\n<p><strong>Join Predicate Optimization</strong></p>\n<p>Beyond simple filter pushdown, our rule engine optimizes join predicates themselves. The key transformations include:</p>\n<ul>\n<li><strong>Predicate Reordering</strong>: Evaluate selective predicates before expensive ones</li>\n<li><strong>Predicate Combination</strong>: Merge multiple range predicates into single range scans</li>\n<li><strong>Transitive Closure</strong>: Derive additional predicates from existing constraints (if <code>A = B</code> and <code>B = C</code>, then <code>A = C</code>)</li>\n</ul>\n<p>The transitive closure optimization proves particularly valuable in star schema queries where fact table joins to dimension tables can imply additional filter conditions that weren&#39;t explicitly stated in the original query.</p>\n<p><strong>Projection Pushdown</strong></p>\n<p>Similar to predicate pushdown, <strong>projection pushdown</strong> eliminates unnecessary columns as early as possible in the execution pipeline. This optimization reduces memory usage, network transfer costs, and I/O overhead by avoiding the processing of columns that don&#39;t contribute to the final result.</p>\n<p>The projection pushdown algorithm tracks column usage throughout the query plan:</p>\n<ol>\n<li><strong>Column Dependency Analysis</strong>: Identify which columns are needed at each operator</li>\n<li><strong>Elimination Identification</strong>: Find columns that can be dropped after specific operations</li>\n<li><strong>Schema Propagation</strong>: Update the <code>output_schema</code> field in <code>OperatorNode</code> instances to reflect reduced column sets</li>\n</ol>\n<p>This analysis must account for columns needed for intermediate operations even if they don&#39;t appear in the final result. For example, join columns and grouping columns must be preserved through the relevant operations even if they&#39;re eventually projected away.</p>\n<p><strong>Sort Elimination Rules</strong></p>\n<p>Sorting operations are expensive, but many sorts can be eliminated through clever optimization rules:</p>\n<ul>\n<li><strong>Index Order Utilization</strong>: Use pre-sorted index access to satisfy ORDER BY clauses</li>\n<li><strong>Sort Merge Optimization</strong>: When merge joins require sorting, align the sort order with final ORDER BY requirements</li>\n<li><strong>Redundant Sort Elimination</strong>: Remove sorts that are overridden by subsequent operations</li>\n</ul>\n<p>The sort elimination analysis maintains information about data ordering throughout the query plan, tracking which columns remain sorted after each operation and identifying opportunities to preserve useful orderings.</p>\n<p><strong>Aggregate Optimization Rules</strong></p>\n<p>Aggregation operations benefit from several specialized optimization rules:</p>\n<ul>\n<li><strong>Early Aggregation</strong>: Push GROUP BY operations below joins when possible to reduce intermediate result sizes</li>\n<li><strong>Index-Based Aggregation</strong>: Use indexes to avoid sorting for GROUP BY operations</li>\n<li><strong>Aggregate Elimination</strong>: Remove unnecessary DISTINCT operations and redundant aggregates</li>\n</ul>\n<p>Early aggregation proves particularly effective in data warehouse queries where large fact tables join to smaller dimension tables. By aggregating the fact table data before joining, we can dramatically reduce the join input sizes.</p>\n<h3 id=\"architecture-decision-rule-based-vs-cost-based-physical-selection\">Architecture Decision: Rule-Based vs Cost-Based Physical Selection</h3>\n<p>The physical planning component must choose between two fundamentally different approaches for selecting concrete operators and applying optimizations. This decision significantly impacts both the complexity of the implementation and the quality of generated plans.</p>\n<blockquote>\n<p><strong>Decision: Hybrid Rule-Based and Cost-Based Physical Selection</strong></p>\n<p><strong>Context</strong>: Physical planning requires making numerous decisions about concrete operator implementations, access methods, and plan transformations. Pure rule-based systems apply predetermined heuristics, while pure cost-based systems evaluate all alternatives using cost estimates. Each approach has distinct trade-offs in implementation complexity, optimization time, and plan quality.</p>\n<p><strong>Options Considered</strong>:</p>\n<ol>\n<li><strong>Pure Rule-Based Selection</strong>: Apply predetermined rules and heuristics for all physical planning decisions</li>\n<li><strong>Pure Cost-Based Selection</strong>: Evaluate cost estimates for all alternative physical implementations</li>\n<li><strong>Hybrid Approach</strong>: Use rules for transformations and cost-based selection for operator choices</li>\n</ol>\n<p><strong>Decision</strong>: Implement a hybrid approach that applies rule-based transformations for provably beneficial optimizations and uses cost-based selection for operator and access method choices.</p>\n<p><strong>Rationale</strong>: Rule-based transformations like predicate pushdown provide consistent benefits without expensive cost calculations, while operator selection benefits significantly from cost-based analysis. This hybrid approach balances optimization quality with computational efficiency.</p>\n<p><strong>Consequences</strong>: Enables fast optimization while maintaining plan quality. Requires careful classification of decisions into rule-based vs cost-based categories. Creates opportunities for future enhancement through machine learning-based rule refinement.</p>\n</blockquote>\n<p><strong>Rule-Based vs Cost-Based Comparison</strong></p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Rule-Based Approach</th>\n<th>Cost-Based Approach</th>\n<th>Hybrid Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Implementation Complexity</strong></td>\n<td>Low - predetermined logic</td>\n<td>High - requires accurate cost models</td>\n<td>Medium - combines both strategies</td>\n</tr>\n<tr>\n<td><strong>Optimization Speed</strong></td>\n<td>Fast - no cost calculations</td>\n<td>Slow - evaluates many alternatives</td>\n<td>Medium - selective cost evaluation</td>\n</tr>\n<tr>\n<td><strong>Plan Quality</strong></td>\n<td>Good for common patterns</td>\n<td>Excellent with accurate statistics</td>\n<td>Very good - best of both worlds</td>\n</tr>\n<tr>\n<td><strong>Adaptability</strong></td>\n<td>Poor - fixed rules</td>\n<td>Excellent - adapts to data changes</td>\n<td>Good - rules provide baseline</td>\n</tr>\n<tr>\n<td><strong>Debugging Difficulty</strong></td>\n<td>Easy - deterministic decisions</td>\n<td>Hard - depends on cost model accuracy</td>\n<td>Medium - clear decision boundaries</td>\n</tr>\n<tr>\n<td><strong>Statistical Dependency</strong></td>\n<td>Minimal - uses simple heuristics</td>\n<td>High - requires accurate statistics</td>\n<td>Medium - graceful degradation</td>\n</tr>\n</tbody></table>\n<p><strong>Rule-Based Decision Categories</strong></p>\n<p>Our hybrid implementation classifies physical planning decisions into categories that benefit from rule-based treatment:</p>\n<ol>\n<li><strong>Provably Beneficial Transformations</strong>: Predicate pushdown, projection elimination, and redundant operation removal always improve performance</li>\n<li><strong>Semantic Preserving Rules</strong>: Transformations that maintain query correctness under all conditions</li>\n<li><strong>Heuristic Simplifications</strong>: Rules that provide good results in the vast majority of cases without expensive analysis</li>\n</ol>\n<p>These rule-based decisions execute first in our physical planning pipeline, transforming the logical plan into an improved logical plan before beginning cost-based operator selection.</p>\n<p><strong>Cost-Based Decision Categories</strong></p>\n<p>The cost-based portion of our implementation focuses on decisions where data characteristics significantly impact the optimal choice:</p>\n<ol>\n<li><strong>Access Method Selection</strong>: Index vs sequential scan decisions depend heavily on data selectivity and clustering</li>\n<li><strong>Join Algorithm Selection</strong>: Hash vs nested loop vs merge join performance varies dramatically based on input sizes</li>\n<li><strong>Memory Allocation</strong>: Buffer size and sort memory decisions require understanding of data volumes and available resources</li>\n</ol>\n<p>For these decisions, our implementation uses the <code>estimateCost</code> method to evaluate alternatives and selects the option with the lowest estimated total cost.</p>\n<p><strong>Implementation Strategy</strong></p>\n<p>The <code>selectPhysicalOperators</code> method implements our hybrid approach through a multi-phase process:</p>\n<ol>\n<li><strong>Rule Application Phase</strong>: Apply all beneficial rule-based transformations to the logical plan</li>\n<li><strong>Cost Analysis Phase</strong>: For each remaining decision point, enumerate alternatives and calculate costs</li>\n<li><strong>Selection Phase</strong>: Choose the lowest-cost alternative for each decision</li>\n<li><strong>Plan Construction Phase</strong>: Build the final <code>ExecutionPlan</code> with selected physical operators</li>\n</ol>\n<p>This phased approach ensures that rule-based optimizations reduce the search space before expensive cost-based analysis begins, improving both optimization speed and plan quality.</p>\n<p><strong>Fallback Strategies</strong></p>\n<p>Our hybrid implementation includes fallback strategies for situations where cost-based analysis fails or produces unreliable results:</p>\n<ul>\n<li><strong>Missing Statistics</strong>: Fall back to rule-based heuristics when table statistics are unavailable or stale</li>\n<li><strong>Cost Model Uncertainty</strong>: Use rules when cost estimates have low confidence levels</li>\n<li><strong>Optimization Timeout</strong>: Switch to rule-based selection when cost analysis exceeds the <code>OPTIMIZATION_TIMEOUT</code> threshold</li>\n</ul>\n<p>These fallback mechanisms ensure that the physical planner always produces executable plans even when optimal cost-based decisions aren&#39;t possible.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fphysical-selection-flow.svg\" alt=\"Physical Operator Selection Flow\"></p>\n<h3 id=\"common-physical-planning-pitfalls\">Common Physical Planning Pitfalls</h3>\n<p>Physical planning involves numerous subtle decisions that can dramatically impact query performance. Developers implementing physical planners often encounter specific categories of mistakes that lead to suboptimal plans or incorrect query results. Understanding these pitfalls helps avoid common implementation errors and design flaws.</p>\n<p><strong>⚠️ Pitfall: Ignoring Index Column Order Requirements</strong></p>\n<p>Many developers incorrectly assume that any index covering the required columns can be used effectively for a query. In reality, B-tree indexes are only effective when predicates match the leftmost columns of the index key in order.</p>\n<p>Consider a composite index on <code>(customer_id, order_date, status)</code>. A query with predicates <code>WHERE order_date &gt; &#39;2023-01-01&#39; AND status = &#39;shipped&#39;</code> cannot effectively use this index because it doesn&#39;t include a predicate on the leftmost column <code>customer_id</code>. The index scan would need to examine most of the index entries, making it less efficient than a sequential scan.</p>\n<p><strong>Fix</strong>: Implement proper index matching logic that verifies predicate coverage of leftmost index columns. The <code>selectPhysicalOperators</code> method should score indexes based on prefix matching, not just column coverage.</p>\n<p><strong>⚠️ Pitfall: Incorrect Outer Join Predicate Pushdown</strong></p>\n<p>Predicate pushdown becomes semantically incorrect with outer joins when predicates are pushed past the preserving side of the join. This error changes query results by converting outer joins into inner joins through aggressive filtering.</p>\n<p>For a <code>LEFT OUTER JOIN</code> between customers and orders, pushing a predicate like <code>WHERE orders.amount &gt; 1000</code> below the join eliminates customers without orders from the result set. The correct behavior preserves customers without orders and only filters the orders that do exist.</p>\n<p><strong>Fix</strong>: Implement join-aware predicate analysis that distinguishes between predicates that can be safely pushed (predicates on the preserved side) and those that must remain at the join level (predicates on the null-extended side).</p>\n<p><strong>⚠️ Pitfall: Memory Overcommitment in Hash Joins</strong></p>\n<p>Hash join selection often fails to account for concurrent query execution and memory competition. Selecting hash joins based on total system memory rather than available query memory leads to excessive disk spilling and performance degradation.</p>\n<p>When multiple concurrent queries each assume they can use most of the available memory for hash tables, the system thrashes as hash joins repeatedly spill to disk. This problem becomes severe in multi-user environments where query concurrency is high.</p>\n<p><strong>Fix</strong>: Implement memory-aware cost modeling that accounts for concurrent query execution. The hash join selection logic should use available query memory limits rather than total system memory when evaluating algorithm feasibility.</p>\n<p><strong>⚠️ Pitfall: Overlooking Sort Order Preservation</strong></p>\n<p>Physical planning often misses opportunities to preserve useful data orderings from one operation to the next. This oversight leads to unnecessary sorting operations when data is already in a suitable order for subsequent operations.</p>\n<p>For example, if an index scan produces data sorted by <code>customer_id</code>, and the query includes <code>GROUP BY customer_id</code>, the grouping operation doesn&#39;t require additional sorting. However, naive physical planning might insert an explicit sort operation between the scan and grouping.</p>\n<p><strong>Fix</strong>: Implement order property tracking throughout the physical planning process. Each <code>OperatorNode</code> should maintain information about the sort order of its output, and subsequent operations should check whether their ordering requirements are already satisfied.</p>\n<p><strong>⚠️ Pitfall: Cost Model Unit Inconsistencies</strong></p>\n<p>Cost estimation becomes unreliable when different cost components use inconsistent units or scales. Common problems include mixing page-based I/O costs with row-based CPU costs without proper scaling factors, leading to systematic bias toward I/O-intensive or CPU-intensive plans.</p>\n<p>For instance, if I/O costs are measured in milliseconds while CPU costs are measured in microseconds, the cost model will heavily favor CPU-intensive operations even when I/O operations would be more efficient in practice.</p>\n<p><strong>Fix</strong>: Standardize all cost components to consistent units in the <code>CostEstimate</code> structure. Use the predefined constants <code>IO_PAGE_COST</code>, <code>CPU_TUPLE_COST</code>, and <code>MEMORY_PAGE_COST</code> as the baseline units and ensure all cost calculations scale appropriately.</p>\n<p><strong>⚠️ Pitfall: Ignoring Data Clustering Effects</strong></p>\n<p>Physical planning often treats all I/O operations as equivalent, ignoring the significant performance differences between sequential and random access patterns. This oversight leads to poor access method choices, particularly for scan operations on clustered vs unclustered data.</p>\n<p>Tables with good clustering on frequently queried columns benefit significantly from sequential scans because related rows are physically co-located. Conversely, index scans on poorly clustered data generate many random I/O operations, making them much more expensive than cost models typically account for.</p>\n<p><strong>Fix</strong>: Incorporate clustering factor analysis into access method selection. Use the <code>clustering_factor</code> field in <code>TableStatistics</code> to adjust I/O cost estimates based on expected access patterns. Apply the <code>RANDOM_MULTIPLIER</code> penalty more aggressively for index scans on poorly clustered data.</p>\n<p><strong>⚠️ Pitfall: Premature Join Algorithm Commitment</strong></p>\n<p>Some implementations commit to join algorithms too early in the planning process, before understanding the full context of the query execution. This premature commitment prevents the optimizer from making globally optimal decisions about memory allocation and operation ordering.</p>\n<p>For example, choosing hash joins for early operations in a complex query might consume all available memory, forcing later joins to use less efficient nested loop algorithms. A global view might reveal that different algorithm assignments produce better overall performance.</p>\n<p><strong>Fix</strong>: Implement holistic join algorithm selection that considers resource constraints across the entire query plan. The <code>selectPhysicalOperators</code> method should evaluate algorithm combinations rather than making independent decisions for each join operation.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p><strong>A. Technology Recommendations Table:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cost Comparison</td>\n<td>Dictionary with manual calculations</td>\n<td>Cost model classes with configurable weights</td>\n</tr>\n<tr>\n<td>Rule Engine</td>\n<td>If-else chains for transformations</td>\n<td>Pattern matching with rule priority system</td>\n</tr>\n<tr>\n<td>Index Metadata</td>\n<td>Simple list of available indexes</td>\n<td>Full index statistics with selectivity histograms</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Fixed memory assumptions</td>\n<td>Dynamic memory allocation with concurrency awareness</td>\n</tr>\n<tr>\n<td>Plan Validation</td>\n<td>Basic tree structure checks</td>\n<td>Semantic correctness verification with test cases</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File/Module Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>query_optimizer/\n  physical/\n    __init__.py\n    physical_planner.py           ← main PhysicalPlanner class\n    access_methods.py             ← scan selection logic\n    join_algorithms.py            ← join algorithm selection\n    optimization_rules.py         ← predicate pushdown and transformations\n    cost_comparison.py            ← physical operator cost evaluation\n    plan_builder.py               ← ExecutionPlan construction\n  test/\n    test_physical_planner.py      ← comprehensive physical planning tests\n    test_access_methods.py        ← access method selection tests\n    test_optimization_rules.py    ← rule application tests\n  examples/\n    sample_physical_plans.py      ← example optimized plans for reference</code></pre></div>\n\n<p><strong>C. Infrastructure Starter Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># physical/cost_comparison.py - COMPLETE cost comparison infrastructure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..core.data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimate, OperatorNode, TableStatistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Cost constants from our design</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">IO_PAGE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">CPU_TUPLE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.01</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">MEMORY_PAGE_COST</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.001</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SEQUENTIAL_MULTIPLIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.3</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">RANDOM_MULTIPLIER</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 2.0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">SELECTIVITY_THRESHOLD</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AccessMethodOption</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a possible access method with cost estimate.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    method_type: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#6A737D\">  # 'sequential_scan', 'index_scan', 'index_only_scan'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    index_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_cost: CostEstimate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    properties: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PhysicalCostComparator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Compares costs between different physical operator implementations.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, statistics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.statistics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_scan_methods</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, predicates: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                           available_indexes: List[Dict]) -> AccessMethodOption:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compare sequential scan vs available index access methods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        options </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        table_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.statistics.get(table_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> table_stats:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Fallback to sequential scan when no statistics available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> AccessMethodOption(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                method_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">'sequential_scan'</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                estimated_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._estimate_sequential_scan_cost(table_name, predicates),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#6A737D\">  # default estimate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Always consider sequential scan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        seq_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_sequential_scan_cost(table_name, predicates)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        options.append(AccessMethodOption(</span><span style=\"color:#9ECBFF\">'sequential_scan'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">, seq_cost, </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                                        int</span><span style=\"color:#E1E4E8\">(table_stats.row_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_selectivity(predicates))))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Consider each available index</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> index </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> available_indexes:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._index_matches_predicates(index, predicates):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                index_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_index_scan_cost(table_name, index, predicates)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                options.append(AccessMethodOption(</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                    'index_scan'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    index[</span><span style=\"color:#9ECBFF\">'name'</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    index_cost,</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    int</span><span style=\"color:#E1E4E8\">(table_stats.row_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_selectivity(predicates))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                ))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Return lowest cost option</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(options, </span><span style=\"color:#FFAB70\">key</span><span style=\"color:#F97583\">=lambda</span><span style=\"color:#E1E4E8\"> x: x.estimated_cost.total_cost)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_sequential_scan_cost</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, predicates: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate cost for sequential table scan with predicates.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        table_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.statistics.get(table_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> table_stats:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span><span style=\"color:#FFAB70\">io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Sequential I/O is efficient</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        io_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> table_stats.page_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> IO_PAGE_COST</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> SEQUENTIAL_MULTIPLIER</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cpu_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> table_stats.row_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> CPU_TUPLE_COST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        selectivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_selectivity(predicates)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">io_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">cpu_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">MEMORY_PAGE_COST</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(table_stats.row_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> selectivity)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_index_scan_cost</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, index: Dict, predicates: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate cost for index-based access.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        table_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.statistics.get(table_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> table_stats:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span><span style=\"color:#FFAB70\">io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        selectivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_selectivity(predicates)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        index_pages </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> index.get(</span><span style=\"color:#9ECBFF\">'page_count'</span><span style=\"color:#E1E4E8\">, table_stats.page_count </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Index scan involves index pages + random data page access</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        io_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (index_pages </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> IO_PAGE_COST</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                  table_stats.page_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> selectivity </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> IO_PAGE_COST</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> RANDOM_MULTIPLIER</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cpu_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> table_stats.row_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> selectivity </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> CPU_TUPLE_COST</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">io_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">cpu_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            memory_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">MEMORY_PAGE_COST</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            estimated_rows</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">(table_stats.row_count </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> selectivity)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_selectivity</span><span style=\"color:#E1E4E8\">(self, predicates: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate combined selectivity of all predicates.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> predicates:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Simple assumption: independent predicates multiply</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        total_selectivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> predicate </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> predicates:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Each predicate assumed to have 0.1 selectivity on average</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            total_selectivity </span><span style=\"color:#F97583\">*=</span><span style=\"color:#79B8FF\"> 0.1</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(total_selectivity, </span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _index_matches_predicates</span><span style=\"color:#E1E4E8\">(self, index: Dict, predicates: List[</span><span style=\"color:#79B8FF\">tuple</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if index can effectively support the given predicates.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> predicates:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        index_columns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> index.get(</span><span style=\"color:#9ECBFF\">'columns'</span><span style=\"color:#E1E4E8\">, [])</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> index_columns:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Check if any predicate matches the leftmost index column</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        leftmost_column </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> index_columns[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> predicate </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> predicates:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(predicate) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#F97583\"> and</span><span style=\"color:#E1E4E8\"> predicate[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> leftmost_column:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> False</span></span></code></pre></div>\n\n<p><strong>D. Core Logic Skeleton Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># physical/physical_planner.py - Core physical planning logic (SKELETON)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..core.data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExecutionPlan, OperatorNode, TableStatistics, JoinType</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .cost_comparison </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PhysicalCostComparator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .optimization_rules </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> OptimizationRuleEngine</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PhysicalPlanner</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Transforms logical plans into optimized physical execution plans.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, statistics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.statistics </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> statistics</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_comparator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PhysicalCostComparator(statistics)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.rule_engine </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> OptimizationRuleEngine()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> selectPhysicalOperators</span><span style=\"color:#E1E4E8\">(self, logical_plan: OperatorNode, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main entry point: convert logical plan to optimized physical plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply rule-based optimizations first (predicate pushdown, projection elimination)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For each operator node, evaluate physical implementation alternatives</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Select lowest-cost physical operator for each logical operation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify that physical plan maintains semantic correctness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Build final ExecutionPlan with cost estimates and metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use postorder traversal to process children before parents</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _select_scan_operator</span><span style=\"color:#E1E4E8\">(self, logical_node: OperatorNode) -> OperatorNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Choose between sequential scan and index access methods.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract table name and filter predicates from logical node</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Get available indexes for the table from statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Use cost_comparator.compare_scan_methods() to evaluate options</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Create physical operator node with selected access method</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Set operator properties (index_name, scan_type, estimated_cost)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Check selectivity against SELECTIVITY_THRESHOLD for quick decisions</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _select_join_algorithm</span><span style=\"color:#E1E4E8\">(self, logical_node: OperatorNode) -> OperatorNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Choose join algorithm based on input characteristics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Analyze left and right child operators for size estimates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Check available memory for hash join feasibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Evaluate nested loop cost with index access on inner relation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Consider merge join if inputs are already sorted</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Select algorithm with lowest estimated cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Hash joins good when smaller input fits in memory</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _apply_optimization_rules</span><span style=\"color:#E1E4E8\">(self, logical_plan: OperatorNode) -> OperatorNode:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Apply rule-based transformations to improve plan efficiency.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Apply predicate pushdown rules throughout the plan tree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Eliminate unnecessary projections and redundant operations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Identify sort operations that can be eliminated via index ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Push aggregations below joins where semantically correct</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate that all transformations preserve query semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: Use preorder traversal for pushdown, postorder for elimination</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_memory_requirement</span><span style=\"color:#E1E4E8\">(self, operator: OperatorNode) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate memory needed for physical operator execution.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: For hash joins, estimate hash table size based on build input</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: For sort operations, calculate buffer requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: For scan operations, estimate buffer pool usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Account for concurrent query execution and available memory</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return memory requirement in consistent units (MB or pages)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _validate_physical_plan</span><span style=\"color:#E1E4E8\">(self, plan: ExecutionPlan) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify that physical plan is executable and semantically correct.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check that all required indexes actually exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify that memory requirements don't exceed system limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Ensure that join algorithms match available join predicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate that predicate pushdown hasn't altered semantics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Confirm that all output schemas are correctly propagated</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> True</span></span></code></pre></div>\n\n<p><strong>E. Language-Specific Hints</strong></p>\n<ul>\n<li><strong>Dictionary Access</strong>: Use <code>statistics.get(table_name)</code> with None checks for safe statistics lookup</li>\n<li><strong>Cost Calculations</strong>: Import <code>math</code> module for logarithmic and exponential cost functions</li>\n<li><strong>Tree Traversal</strong>: Use recursive functions or explicit stacks for depth-first plan tree traversal</li>\n<li><strong>Enum Comparisons</strong>: Import <code>OperatorType</code> and <code>JoinType</code> enums for type-safe operator comparisons</li>\n<li><strong>Memory Estimation</strong>: Use <code>sys.getsizeof()</code> for Python object memory footprint estimates</li>\n<li><strong>Performance</strong>: Consider <code>functools.lru_cache</code> decorator for expensive cost calculations</li>\n<li><strong>Testing</strong>: Use <code>pytest.approx()</code> for floating-point cost estimate comparisons</li>\n<li><strong>Debugging</strong>: Add logging with <code>logging.getLogger(__name__)</code> to trace planning decisions</li>\n</ul>\n<p><strong>F. Milestone Checkpoint</strong></p>\n<p>After implementing the Physical Planning Component:</p>\n<p><strong>Test Command</strong>: <code>python -m pytest test/test_physical_planner.py -v</code></p>\n<p><strong>Expected Behaviors</strong>:</p>\n<ol>\n<li><strong>Access Method Selection</strong>: <code>test_scan_method_selection()</code> should show index scans chosen for selective predicates and sequential scans for high-selectivity queries</li>\n<li><strong>Join Algorithm Choice</strong>: <code>test_join_algorithm_selection()</code> should demonstrate hash joins for equal-sized tables and nested loops for small-large combinations  </li>\n<li><strong>Rule Applications</strong>: <code>test_predicate_pushdown()</code> should verify that filters move below joins and projections eliminate unused columns</li>\n<li><strong>Cost Consistency</strong>: <code>test_cost_estimates()</code> should confirm that physical plan costs are reasonable and consistent with input statistics</li>\n</ol>\n<p><strong>Manual Verification</strong>:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Create test query and run optimization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> query_optimizer.physical.physical_planner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PhysicalPlanner</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> query_optimizer.test.sample_data </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> create_test_statistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">planner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PhysicalPlanner(create_test_statistics())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">logical_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_sample_logical_plan()  </span><span style=\"color:#6A737D\"># Your test plan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">physical_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> planner.selectPhysicalOperators(logical_plan, planner.statistics)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Should show concrete operators and reasonable costs</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(physical_plan.pretty_print(</span><span style=\"color:#FFAB70\">show_costs</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Total estimated cost: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">physical_plan.total_cost.total_cost</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Signs Something Is Wrong</strong>:</p>\n<ul>\n<li><strong>All scans are sequential</strong>: Indicates index selection logic isn&#39;t working</li>\n<li><strong>All joins use nested loops</strong>: Suggests hash join cost estimation is broken  </li>\n<li><strong>Costs are negative or extremely large</strong>: Points to cost calculation errors</li>\n<li><strong>Plan fails validation</strong>: Means rule applications broke semantic correctness</li>\n</ul>\n<h2 id=\"interactions-and-data-flow\">Interactions and Data Flow</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All Milestones 1-4 - describes how components collaborate throughout the optimization pipeline, from plan representation through cost estimation and join ordering to physical planning.</p>\n</blockquote>\n<p>The query optimizer&#39;s effectiveness depends not just on individual component quality, but on how these components collaborate to transform a SQL query into an optimized execution plan. Think of this like an automotive assembly line where each station performs specialized work on the product, but the final car quality depends on precise coordination, timing, and information flow between stations. Each optimizer component receives structured inputs, performs its specialized analysis, and passes enriched results to the next stage in the pipeline.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Foptimization-workflow.svg\" alt=\"Optimization Process Workflow\"></p>\n<p>This section explores three critical aspects of component collaboration: the complete optimization sequence that transforms SQL queries into executable plans, the communication interfaces that enable clean component integration, and the caching mechanisms that improve optimization performance through plan reuse.</p>\n<h3 id=\"complete-optimization-sequence\">Complete Optimization Sequence</h3>\n<p>The optimization process follows a carefully orchestrated sequence of transformations, where each phase builds upon the outputs of previous phases while maintaining clean separation of concerns. Understanding this sequence is crucial for implementing a robust optimizer that can handle complex queries efficiently while managing computational resources.</p>\n<p><strong>Mental Model: Document Review Pipeline</strong></p>\n<p>Think of query optimization like a document review process in a legal firm. A contract arrives and goes through multiple specialist reviews: first a paralegal creates an outline of key sections (logical planning), then a financial analyst estimates costs and risks (cost estimation), a negotiation expert determines the best order to present terms (join ordering), and finally a senior partner selects specific legal strategies and language (physical planning). Each specialist adds their expertise while the document flows forward, becoming more refined and actionable at each stage.</p>\n<p>The optimization pipeline consists of five distinct phases, each with specific inputs, outputs, and responsibilities:</p>\n<table>\n<thead>\n<tr>\n<th>Phase</th>\n<th>Input</th>\n<th>Output</th>\n<th>Primary Responsibility</th>\n<th>Key Decisions Made</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Query Parsing</td>\n<td>Raw SQL text</td>\n<td><code>ParsedQuery</code> structure</td>\n<td>Extract tables, columns, predicates, joins</td>\n<td>Validate syntax, identify query structure</td>\n</tr>\n<tr>\n<td>Logical Planning</td>\n<td><code>ParsedQuery</code></td>\n<td>Logical <code>ExecutionPlan</code></td>\n<td>Build operator tree with logical operations</td>\n<td>Determine required operations, establish tree structure</td>\n</tr>\n<tr>\n<td>Cost Estimation</td>\n<td>Logical plan + <code>TableStatistics</code></td>\n<td>Cost-annotated logical plan</td>\n<td>Calculate resource consumption estimates</td>\n<td>Estimate selectivity, cardinality, I/O costs</td>\n</tr>\n<tr>\n<td>Join Optimization</td>\n<td>Cost-annotated plan</td>\n<td>Optimized logical plan</td>\n<td>Find efficient join order using dynamic programming</td>\n<td>Select join sequence, prune suboptimal alternatives</td>\n</tr>\n<tr>\n<td>Physical Planning</td>\n<td>Optimized logical plan</td>\n<td>Physical <code>ExecutionPlan</code></td>\n<td>Choose concrete operators and access methods</td>\n<td>Select scan methods, join algorithms, apply optimization rules</td>\n</tr>\n</tbody></table>\n<h4 id=\"phase-1-query-parsing-and-initial-structure\">Phase 1: Query Parsing and Initial Structure</h4>\n<p>The optimization sequence begins when the <code>optimize_query</code> function receives a <code>ParsedQuery</code> structure containing the essential elements extracted from SQL parsing. This phase focuses on understanding query structure without making optimization decisions.</p>\n<p>The parser has already identified the fundamental query components that drive optimization decisions:</p>\n<table>\n<thead>\n<tr>\n<th>Query Component</th>\n<th>Information Extracted</th>\n<th>Optimization Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tables</td>\n<td>List of base tables referenced</td>\n<td>Determines join ordering search space</td>\n</tr>\n<tr>\n<td>Columns</td>\n<td>Projected and referenced columns</td>\n<td>Influences index selection and schema propagation</td>\n</tr>\n<tr>\n<td>Join Predicates</td>\n<td>Equality and comparison conditions between tables</td>\n<td>Defines valid join orders and selectivity estimates</td>\n</tr>\n<tr>\n<td>Filter Predicates</td>\n<td>Selection conditions on individual tables</td>\n<td>Drives scan method selection and predicate pushdown opportunities</td>\n</tr>\n<tr>\n<td>Ordering Requirements</td>\n<td>ORDER BY and GROUP BY clauses</td>\n<td>Influences join algorithm choice and sort elimination</td>\n</tr>\n</tbody></table>\n<p>The parsed query structure provides the foundation for all subsequent optimization phases, ensuring that logical and physical transformations preserve query semantics while improving execution efficiency.</p>\n<h4 id=\"phase-2-logical-plan-construction\">Phase 2: Logical Plan Construction</h4>\n<p>The Plan Builder component transforms the parsed query structure into a logical execution plan represented as an <code>OperatorNode</code> tree. This phase focuses on correctness and completeness rather than efficiency, establishing the semantic foundation for later optimizations.</p>\n<p>The logical planning process follows a systematic approach:</p>\n<ol>\n<li><p><strong>Base Table Access Creation</strong>: For each table in the query, create a logical <code>SCAN</code> operator node with the table name stored in the operator properties dictionary and the full table schema propagated as the <code>output_schema</code>.</p>\n</li>\n<li><p><strong>Filter Integration</strong>: For each filter predicate associated with a table, create a <code>FILTER</code> operator node positioned above the corresponding scan operation, with the predicate condition stored in the node properties and selectivity initially unknown.</p>\n</li>\n<li><p><strong>Join Tree Construction</strong>: Based on the join predicates, construct a preliminary join tree using <code>JOIN</code> operator nodes, initially ordered according to the SQL query structure without optimization considerations.</p>\n</li>\n<li><p><strong>Projection Application</strong>: Add a <code>PROJECT</code> operator node at the tree root to select only the columns specified in the SQL SELECT clause, establishing the final output schema.</p>\n</li>\n<li><p><strong>Schema Propagation</strong>: Traverse the tree bottom-up, calculating and storing the <code>output_schema</code> for each operator node based on its inputs and operation semantics.</p>\n</li>\n</ol>\n<p>The resulting logical plan captures the query&#39;s semantic requirements without committing to specific physical implementations or optimization strategies. Each <code>OperatorNode</code> contains placeholder values for <code>cost_estimate</code> and <code>estimated_rows</code> that will be populated during cost estimation.</p>\n<h4 id=\"phase-3-cost-estimation-and-statistics-integration\">Phase 3: Cost Estimation and Statistics Integration</h4>\n<p>The Cost Estimator component enriches the logical plan with resource consumption estimates by integrating table statistics and selectivity calculations. This phase transforms the logical plan from a semantic specification into a quantified prediction model.</p>\n<p>The cost estimation process proceeds through the tree in postorder traversal, ensuring child costs are calculated before parent costs:</p>\n<ol>\n<li><p><strong>Statistics Lookup</strong>: For each <code>SCAN</code> operator, retrieve the corresponding <code>TableStatistics</code> including row counts, page counts, and column histograms from the statistics catalog.</p>\n</li>\n<li><p><strong>Base Cost Calculation</strong>: Calculate the I/O and CPU costs for accessing each base table, storing the results in the operator&#39;s <code>cost_estimate</code> field using the formula: <code>io_cost = page_count * IO_PAGE_COST</code> and <code>cpu_cost = row_count * CPU_TUPLE_COST</code>.</p>\n</li>\n<li><p><strong>Filter Selectivity Estimation</strong>: For each <code>FILTER</code> operator, call <code>estimate_filter_selectivity</code> using column statistics to predict the fraction of input rows that will survive the predicate, updating both <code>estimated_rows</code> and the operator&#39;s cost estimate.</p>\n</li>\n<li><p><strong>Join Cardinality Estimation</strong>: For each <code>JOIN</code> operator, use <code>estimate_join_cardinality</code> to predict output size based on input cardinalities and join predicate selectivity, accounting for foreign key relationships and value distribution skew.</p>\n</li>\n<li><p><strong>Cost Accumulation</strong>: Combine child operator costs with the current operator&#39;s processing cost, storing the cumulative resource consumption in the <code>CostEstimate</code> structure with separate tracking of I/O, CPU, and memory components.</p>\n</li>\n</ol>\n<p>After cost estimation, each operator node contains realistic resource consumption predictions that enable the join optimizer to make informed decisions about alternative execution strategies.</p>\n<h4 id=\"phase-4-join-order-optimization\">Phase 4: Join Order Optimization</h4>\n<p>The Join Optimizer component uses dynamic programming to explore alternative join orders and select the sequence that minimizes total execution cost. This phase represents the core algorithmic challenge in query optimization, managing exponential search complexity while finding high-quality solutions.</p>\n<p>The join optimization process implements the classic dynamic programming algorithm for optimal join ordering:</p>\n<ol>\n<li><p><strong>Base Case Initialization</strong>: Create single-table access plans for each base table using the existing scan and filter operators, storing these plans in the dynamic programming memoization table indexed by single-table subsets.</p>\n</li>\n<li><p><strong>Subset Enumeration</strong>: For each subset size from 2 to the total number of tables, enumerate all possible table combinations and check connectivity through available join predicates.</p>\n</li>\n<li><p><strong>Partition Evaluation</strong>: For each connected subset, consider all possible ways to partition the tables into two smaller subsets, retrieving optimal plans for each partition from the memoization table.</p>\n</li>\n<li><p><strong>Join Cost Comparison</strong>: For each valid partition, estimate the cost of joining the two optimal subplans using different join algorithms, selecting the alternative with minimum total cost including both child costs and join processing overhead.</p>\n</li>\n<li><p><strong>Pruning and Memoization</strong>: Store only the lowest-cost plan for each table subset in the memoization table, pruning dominated alternatives to control memory usage and prevent redundant computation.</p>\n</li>\n<li><p><strong>Optimal Plan Extraction</strong>: Retrieve the optimal plan for the complete table set from the memoization table, representing the join order that minimizes estimated execution cost.</p>\n</li>\n</ol>\n<p>The dynamic programming approach guarantees finding the optimal join order within the search space constraints, while memoization prevents exponential explosion by reusing optimal solutions for table subsets.</p>\n<h4 id=\"phase-5-physical-operator-selection\">Phase 5: Physical Operator Selection</h4>\n<p>The Physical Planner component transforms the optimized logical plan into an executable physical plan by selecting concrete operators, access methods, and applying rule-based optimizations. This phase bridges the gap between logical specifications and actual execution capabilities.</p>\n<p>The physical planning process operates through multiple transformation passes:</p>\n<ol>\n<li><p><strong>Access Method Selection</strong>: For each <code>SCAN</code> operator, call <code>compare_scan_methods</code> to evaluate sequential scan versus available index access methods, selecting the alternative with lowest estimated cost based on filter selectivity and index characteristics.</p>\n</li>\n<li><p><strong>Join Algorithm Selection</strong>: For each <code>JOIN</code> operator, analyze input characteristics including estimated cardinalities, memory requirements, and sort orders to choose between hash join, nested loop join, and merge join algorithms.</p>\n</li>\n<li><p><strong>Optimization Rule Application</strong>: Apply rule-based transformations including predicate pushdown, projection elimination, and constant folding to reduce intermediate result sizes and eliminate unnecessary operations.</p>\n</li>\n<li><p><strong>Memory and Resource Planning</strong>: Calculate memory requirements for each physical operator, ensuring that hash tables and sort buffers fit within available memory while identifying operators that may require disk spilling.</p>\n</li>\n<li><p><strong>Plan Validation</strong>: Verify that the resulting physical plan is executable and semantically correct, checking for missing implementations, incompatible operator combinations, and resource constraint violations.</p>\n</li>\n</ol>\n<p>The final physical <code>ExecutionPlan</code> contains concrete operator implementations that can be directly executed by the query engine, with all optimization decisions resolved and resource requirements quantified.</p>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fsystem-architecture.svg\" alt=\"Query Optimizer System Architecture\"></p>\n<blockquote>\n<p><strong>Critical Insight</strong>: The optimization sequence maintains strict separation between logical correctness (phases 1-2), cost modeling (phase 3), combinatorial optimization (phase 4), and physical implementation (phase 5). This separation enables independent testing and debugging of each optimization aspect while supporting modular component development.</p>\n</blockquote>\n<h3 id=\"component-communication-interfaces\">Component Communication Interfaces</h3>\n<p>The optimizer components communicate through well-defined interfaces that encapsulate data exchange patterns and hide internal implementation complexity. These interfaces enable clean component integration while supporting future extensibility and testing isolation.</p>\n<p><strong>Mental Model: Standardized Manufacturing Interfaces</strong></p>\n<p>Think of component interfaces like standardized connectors in manufacturing equipment. Just as electrical components use standard voltage levels and connector types to ensure compatibility, optimizer components use standardized data structures and method signatures. This allows components to be developed, tested, and upgraded independently while maintaining system integration, similar to how you can replace a factory machine without redesigning the entire production line.</p>\n<h4 id=\"primary-component-interfaces\">Primary Component Interfaces</h4>\n<p>The optimizer defines several key interface contracts that govern component collaboration:</p>\n<table>\n<thead>\n<tr>\n<th>Interface Type</th>\n<th>Purpose</th>\n<th>Key Methods</th>\n<th>Data Exchange Format</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plan Builder Interface</td>\n<td>Construct logical plans from parsed queries</td>\n<td><code>build_logical_plan(parsed_query)</code></td>\n<td><code>ParsedQuery</code> → Logical <code>ExecutionPlan</code></td>\n</tr>\n<tr>\n<td>Cost Estimator Interface</td>\n<td>Annotate plans with resource estimates</td>\n<td><code>estimateCost(plan)</code>, <code>collectStatistics(table)</code></td>\n<td><code>ExecutionPlan</code> + <code>TableStatistics</code> → Cost-annotated plan</td>\n</tr>\n<tr>\n<td>Join Optimizer Interface</td>\n<td>Find optimal join orders</td>\n<td><code>optimizeJoinOrder(tables)</code></td>\n<td>Logical plan → Optimized logical plan</td>\n</tr>\n<tr>\n<td>Physical Planner Interface</td>\n<td>Select concrete operators</td>\n<td><code>selectPhysicalOperators(logical_plan, stats)</code></td>\n<td>Logical plan + statistics → Physical <code>ExecutionPlan</code></td>\n</tr>\n<tr>\n<td>Statistics Provider Interface</td>\n<td>Supply table and column statistics</td>\n<td><code>get_table_stats(table_name)</code>, <code>get_column_stats(table, column)</code></td>\n<td>Table/column identifiers → <code>TableStatistics</code>/<code>ColumnStatistics</code></td>\n</tr>\n</tbody></table>\n<h4 id=\"plan-builder-interface-details\">Plan Builder Interface Details</h4>\n<p>The Plan Builder provides the foundational interface for transforming parsed SQL queries into logical execution plans:</p>\n<table>\n<thead>\n<tr>\n<th>Method Signature</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Purpose</th>\n<th>Error Conditions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>build_logical_plan(parsed_query)</code></td>\n<td><code>ParsedQuery</code> with tables, joins, filters</td>\n<td>Logical <code>ExecutionPlan</code></td>\n<td>Create operator tree representing query semantics</td>\n<td>Invalid joins, missing tables, circular references</td>\n</tr>\n<tr>\n<td><code>validate_query_semantics(parsed_query)</code></td>\n<td><code>ParsedQuery</code> structure</td>\n<td>Boolean validation result</td>\n<td>Check query for semantic errors before planning</td>\n<td>Unknown columns, type mismatches, ambiguous references</td>\n</tr>\n<tr>\n<td><code>extract_table_dependencies(parsed_query)</code></td>\n<td><code>ParsedQuery</code> with join predicates</td>\n<td>Dictionary of table relationships</td>\n<td>Identify join graph connectivity for optimization</td>\n<td>Disconnected table groups, missing join predicates</td>\n</tr>\n<tr>\n<td><code>propagate_schema_information(plan)</code></td>\n<td><code>ExecutionPlan</code> tree structure</td>\n<td>Updated plan with schema annotations</td>\n<td>Calculate output schemas for each operator node</td>\n<td>Schema propagation conflicts, unknown column types</td>\n</tr>\n</tbody></table>\n<p>The Plan Builder interface encapsulates the complexity of logical plan construction while providing clean integration points for query validation and schema management.</p>\n<h4 id=\"cost-estimator-interface-details\">Cost Estimator Interface Details</h4>\n<p>The Cost Estimator interface defines methods for statistical analysis and cost calculation:</p>\n<table>\n<thead>\n<tr>\n<th>Method Signature</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Purpose</th>\n<th>Statistics Dependencies</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>estimateCost(plan)</code></td>\n<td><code>ExecutionPlan</code> with operator tree</td>\n<td>Plan annotated with <code>CostEstimate</code> objects</td>\n<td>Calculate resource consumption for entire plan</td>\n<td>Current <code>TableStatistics</code> for all referenced tables</td>\n</tr>\n<tr>\n<td><code>estimate_filter_selectivity(table, column, operator, value)</code></td>\n<td>Table name, column name, comparison operator, filter value</td>\n<td>Float selectivity between 0.0 and 1.0</td>\n<td>Predict fraction of rows surviving filter</td>\n<td><code>ColumnStatistics</code> with histogram or distinct value count</td>\n</tr>\n<tr>\n<td><code>estimate_join_cardinality(left_table, left_column, right_table, right_column)</code></td>\n<td>Join participant tables and columns</td>\n<td>Integer estimated row count</td>\n<td>Predict join output size</td>\n<td>Statistics for both join columns including null counts</td>\n</tr>\n<tr>\n<td><code>collectStatistics(table, sample_rate)</code></td>\n<td>Table name and sampling percentage</td>\n<td><code>TableStatistics</code> object</td>\n<td>Gather current data distribution information</td>\n<td>Table access permissions, sufficient disk space for sampling</td>\n</tr>\n<tr>\n<td><code>update_statistics_cache(table_stats)</code></td>\n<td><code>TableStatistics</code> object</td>\n<td>None (side effect: cache update)</td>\n<td>Store statistics for reuse across queries</td>\n<td>Valid statistics with recent timestamps</td>\n</tr>\n</tbody></table>\n<p>The Cost Estimator interface abstracts statistical modeling complexity while providing flexible integration with different statistics collection strategies.</p>\n<h4 id=\"join-optimizer-interface-details\">Join Optimizer Interface Details</h4>\n<p>The Join Optimizer interface defines methods for combinatorial join order optimization:</p>\n<table>\n<thead>\n<tr>\n<th>Method Signature</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Purpose</th>\n<th>Optimization Constraints</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>optimizeJoinOrder(tables)</code></td>\n<td>List of table names with join predicates</td>\n<td><code>JoinOrder</code> specifying optimal sequence</td>\n<td>Find minimum-cost join order using dynamic programming</td>\n<td>Connected join graph, reasonable table count (&lt; MAX_JOIN_ENUMERATION)</td>\n</tr>\n<tr>\n<td><code>enumerate_join_alternatives(table_subset)</code></td>\n<td>Subset of tables for join consideration</td>\n<td>List of alternative join plans</td>\n<td>Generate candidate join plans for cost comparison</td>\n<td>Valid join predicates between table pairs</td>\n</tr>\n<tr>\n<td><code>evaluate_join_cost(left_plan, right_plan, predicates)</code></td>\n<td>Two subplans and joining predicates</td>\n<td><code>CostEstimate</code> for join operation</td>\n<td>Estimate cost of joining two subplans</td>\n<td>Cost estimates available for input subplans</td>\n</tr>\n<tr>\n<td><code>prune_dominated_plans(plan_alternatives)</code></td>\n<td>List of alternative plans for same table subset</td>\n<td>Filtered list of non-dominated plans</td>\n<td>Remove clearly suboptimal alternatives to control search space</td>\n<td>Comparable cost estimates for all alternatives</td>\n</tr>\n</tbody></table>\n<p>The Join Optimizer interface encapsulates dynamic programming complexity while supporting different optimization strategies and search space management policies.</p>\n<h4 id=\"physical-planner-interface-details\">Physical Planner Interface Details</h4>\n<p>The Physical Planner interface defines methods for concrete operator selection and plan finalization:</p>\n<table>\n<thead>\n<tr>\n<th>Method Signature</th>\n<th>Parameters</th>\n<th>Returns</th>\n<th>Purpose</th>\n<th>Selection Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>selectPhysicalOperators(logical_plan, stats)</code></td>\n<td>Logical <code>ExecutionPlan</code> and <code>TableStatistics</code></td>\n<td>Physical <code>ExecutionPlan</code> with concrete operators</td>\n<td>Transform logical plan into executable physical plan</td>\n<td>Available indexes, memory constraints, join algorithm implementations</td>\n</tr>\n<tr>\n<td><code>compare_scan_methods(table_name, predicates, available_indexes)</code></td>\n<td>Table identifier, filter predicates, index metadata</td>\n<td><code>AccessMethodOption</code> with cost comparison</td>\n<td>Choose between sequential scan and index access</td>\n<td>Index selectivity thresholds, index maintenance costs</td>\n</tr>\n<tr>\n<td><code>select_join_algorithm(join_node, left_stats, right_stats)</code></td>\n<td><code>JOIN</code> operator and input statistics</td>\n<td>Physical join operator (hash/nested loop/merge)</td>\n<td>Choose join implementation based on input characteristics</td>\n<td>Memory availability, input sort orders, join selectivity</td>\n</tr>\n<tr>\n<td><code>apply_optimization_rules(physical_plan)</code></td>\n<td>Physical <code>ExecutionPlan</code></td>\n<td>Optimized physical plan with rule transformations</td>\n<td>Apply predicate pushdown and other rule-based improvements</td>\n<td>Operator compatibility, semantic preservation constraints</td>\n</tr>\n</tbody></table>\n<h4 id=\"data-structure-exchange-formats\">Data Structure Exchange Formats</h4>\n<p>Components exchange information through standardized data structures that maintain optimization state and enable incremental processing:</p>\n<table>\n<thead>\n<tr>\n<th>Data Structure</th>\n<th>Producer Component</th>\n<th>Consumer Component</th>\n<th>Key Information</th>\n<th>Lifecycle</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ParsedQuery</code></td>\n<td>SQL Parser (external)</td>\n<td>Plan Builder</td>\n<td>Tables, joins, filters, projections</td>\n<td>Single optimization session</td>\n</tr>\n<tr>\n<td>Logical <code>ExecutionPlan</code></td>\n<td>Plan Builder</td>\n<td>Cost Estimator, Join Optimizer</td>\n<td>Operator tree without costs or physical details</td>\n<td>Modified by multiple optimization phases</td>\n</tr>\n<tr>\n<td>Cost-annotated <code>ExecutionPlan</code></td>\n<td>Cost Estimator</td>\n<td>Join Optimizer</td>\n<td>Logical operators with resource estimates</td>\n<td>Temporary during join optimization</td>\n</tr>\n<tr>\n<td>Optimized logical <code>ExecutionPlan</code></td>\n<td>Join Optimizer</td>\n<td>Physical Planner</td>\n<td>Optimal join order with cost estimates</td>\n<td>Input to physical planning</td>\n</tr>\n<tr>\n<td>Physical <code>ExecutionPlan</code></td>\n<td>Physical Planner</td>\n<td>Query Executor (external)</td>\n<td>Executable operators with concrete implementations</td>\n<td>Cached for query plan reuse</td>\n</tr>\n<tr>\n<td><code>TableStatistics</code></td>\n<td>Statistics Collector</td>\n<td>Cost Estimator, Physical Planner</td>\n<td>Row counts, column distributions, index metadata</td>\n<td>Persistent across multiple queries</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Design Principle</strong>: Interface contracts use immutable data structures where possible to prevent accidental modification by downstream components. When mutation is necessary (such as cost annotation), components create modified copies rather than updating original structures, enabling rollback and alternative exploration.</p>\n</blockquote>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Fcost-estimation-sequence.svg\" alt=\"Cost Estimation Interaction Sequence\"></p>\n<h4 id=\"error-handling-and-component-communication\">Error Handling and Component Communication</h4>\n<p>Component interfaces define comprehensive error handling strategies that enable graceful degradation when optimization assumptions fail:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Detection Method</th>\n<th>Recovery Strategy</th>\n<th>Fallback Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Missing Statistics</td>\n<td><code>TableStatistics</code> returns None</td>\n<td>Use default estimates based on table size heuristics</td>\n<td>Conservative selectivity estimates, prefer sequential scans</td>\n</tr>\n<tr>\n<td>Join Graph Disconnection</td>\n<td><code>extract_table_dependencies</code> identifies isolated table groups</td>\n<td>Generate cross product warnings, apply heuristic join ordering</td>\n<td>Create nested loop joins with cost penalties</td>\n</tr>\n<tr>\n<td>Optimization Timeout</td>\n<td>Timer expiration during dynamic programming</td>\n<td>Return best plan found so far, skip remaining subsets</td>\n<td>Use greedy join ordering for remaining tables</td>\n</tr>\n<tr>\n<td>Memory Constraint Violation</td>\n<td>Physical operator memory estimates exceed available resources</td>\n<td>Select disk-based algorithms, increase spill thresholds</td>\n<td>Prefer nested loop joins over hash joins</td>\n</tr>\n<tr>\n<td>Index Unavailability</td>\n<td>Physical planner cannot access expected indexes</td>\n<td>Fall back to sequential scans with updated cost estimates</td>\n<td>Recompute plan costs with sequential access methods</td>\n</tr>\n</tbody></table>\n<h3 id=\"plan-caching-and-reuse\">Plan Caching and Reuse</h3>\n<p>Query optimization represents a significant computational investment that can be amortized across multiple query executions through intelligent plan caching. Effective plan reuse requires sophisticated matching algorithms, cache invalidation strategies, and memory management policies.</p>\n<p><strong>Mental Model: Architectural Blueprint Library</strong></p>\n<p>Think of plan caching like an architectural firm&#39;s blueprint library. When designing a new building, architects first check if they have existing plans for similar structures that can be adapted rather than starting from scratch. The library includes indexing by building type, size, and features, with careful tracking of which blueprints are outdated due to building code changes. Similarly, query plan caching maintains a library of optimized plans indexed by query characteristics, with invalidation tracking based on data and schema changes.</p>\n<h4 id=\"plan-identification-and-matching\">Plan Identification and Matching</h4>\n<p>Effective plan caching requires sophisticated algorithms for identifying when cached plans can be reused for new queries. The challenge lies in recognizing query equivalence despite syntactic variations while avoiding false matches that could produce incorrect results.</p>\n<p>The plan identification process operates through multiple matching levels:</p>\n<table>\n<thead>\n<tr>\n<th>Matching Level</th>\n<th>Comparison Criteria</th>\n<th>Equivalence Examples</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Syntactic Matching</td>\n<td>Exact SQL text comparison after normalization</td>\n<td>Identical queries with consistent formatting</td>\n<td>O(1) hash lookup</td>\n</tr>\n<tr>\n<td>Structural Matching</td>\n<td><code>ParsedQuery</code> component comparison</td>\n<td>Queries with different aliases but same structure</td>\n<td>O(n) where n is query complexity</td>\n</tr>\n<tr>\n<td>Semantic Matching</td>\n<td>Logical plan tree comparison</td>\n<td>Queries with reordered predicates but equivalent meaning</td>\n<td>O(n²) plan tree comparison</td>\n</tr>\n<tr>\n<td>Parametric Matching</td>\n<td>Template-based matching with parameter substitution</td>\n<td>Queries differing only in literal values</td>\n<td>O(k) where k is parameter count</td>\n</tr>\n</tbody></table>\n<h4 id=\"syntactic-plan-matching\">Syntactic Plan Matching</h4>\n<p>The most efficient caching approach uses normalized SQL text as the cache key, providing constant-time lookups for exact query matches:</p>\n<ol>\n<li><p><strong>Query Normalization</strong>: Transform SQL text by removing extra whitespace, standardizing keyword capitalization, and ordering predicates deterministically to eliminate formatting variations.</p>\n</li>\n<li><p><strong>Hash Key Generation</strong>: Compute a stable hash of the normalized query text using SHA-256 or similar algorithm to create a compact cache key that avoids string comparison overhead.</p>\n</li>\n<li><p><strong>Direct Cache Lookup</strong>: Use the hash key for direct cache access, returning the cached <code>ExecutionPlan</code> if available or proceeding to full optimization if no match exists.</p>\n</li>\n<li><p><strong>Cache Entry Validation</strong>: Verify that cached plans remain valid by checking timestamps against statistics updates and schema modifications that could invalidate optimization assumptions.</p>\n</li>\n</ol>\n<p>Syntactic matching provides excellent performance for repeated identical queries but misses opportunities for reuse when queries are semantically equivalent but syntactically different.</p>\n<h4 id=\"structural-plan-matching\">Structural Plan Matching</h4>\n<p>More sophisticated caching examines the parsed query structure to identify reusable plans despite syntactic variations:</p>\n<p>The structural matching algorithm compares <code>ParsedQuery</code> components:</p>\n<ol>\n<li><p><strong>Table Set Comparison</strong>: Verify that both queries reference the same base tables, ignoring alias differences but preserving table roles in join relationships.</p>\n</li>\n<li><p><strong>Join Pattern Matching</strong>: Compare join predicates for structural equivalence, recognizing that <code>A.id = B.id</code> and <code>B.id = A.id</code> represent identical join conditions.</p>\n</li>\n<li><p><strong>Filter Compatibility</strong>: Analyze filter predicates to determine if they can be satisfied by the same plan, accounting for predicate subsumption and logical equivalence.</p>\n</li>\n<li><p><strong>Projection Compatibility</strong>: Ensure that cached plan output schema includes all columns required by the new query, allowing plans with additional columns to satisfy queries requesting subsets.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Query Structure Component</th>\n<th>Matching Algorithm</th>\n<th>Complexity</th>\n<th>Cache Hit Benefit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table references</td>\n<td>Set intersection and alias normalization</td>\n<td>O(t) where t is table count</td>\n<td>Reuse join order optimization</td>\n</tr>\n<tr>\n<td>Join predicates</td>\n<td>Predicate normalization and comparison</td>\n<td>O(j²) where j is join count</td>\n<td>Reuse cardinality estimates</td>\n</tr>\n<tr>\n<td>Filter predicates</td>\n<td>Selectivity compatibility analysis</td>\n<td>O(f) where f is filter count</td>\n<td>Reuse selectivity calculations</td>\n</tr>\n<tr>\n<td>Output projections</td>\n<td>Schema subsumption checking</td>\n<td>O(c) where c is column count</td>\n<td>Reuse entire plan if compatible</td>\n</tr>\n</tbody></table>\n<h4 id=\"parametric-plan-templates\">Parametric Plan Templates</h4>\n<p>The most sophisticated caching approach creates parameterized plan templates that can be instantiated with different literal values:</p>\n<ol>\n<li><p><strong>Parameter Extraction</strong>: Identify literal constants in SQL queries that can be treated as parameters, replacing them with placeholder markers in the plan template.</p>\n</li>\n<li><p><strong>Template Generation</strong>: Create a generic <code>ExecutionPlan</code> template with parameter slots that can be filled with actual values at execution time.</p>\n</li>\n<li><p><strong>Selectivity Parameterization</strong>: Store selectivity estimation functions rather than fixed selectivity values, enabling recalculation based on actual parameter values.</p>\n</li>\n<li><p><strong>Template Instantiation</strong>: When reusing a template, substitute actual parameter values and recalculate selectivities that depend on filter constants.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Template Component</th>\n<th>Parameterization Strategy</th>\n<th>Example</th>\n<th>Recalculation Required</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Filter constants</td>\n<td>Replace with parameter placeholders</td>\n<td><code>WHERE age &gt; ?</code> instead of <code>WHERE age &gt; 25</code></td>\n<td>Yes - selectivity depends on threshold value</td>\n</tr>\n<tr>\n<td>Join constants</td>\n<td>Template-based predicate matching</td>\n<td><code>JOIN orders ON customer_id = ?</code></td>\n<td>Rarely - join algorithms usually independent of constants</td>\n</tr>\n<tr>\n<td>Limit values</td>\n<td>Parameter substitution</td>\n<td><code>LIMIT ?</code> instead of <code>LIMIT 100</code></td>\n<td>No - algorithm choice unaffected</td>\n</tr>\n<tr>\n<td>Sort specifications</td>\n<td>Template matching with column parameters</td>\n<td><code>ORDER BY ? ASC</code></td>\n<td>Sometimes - index selection may depend on sort column</td>\n</tr>\n</tbody></table>\n<h4 id=\"cache-invalidation-and-consistency\">Cache Invalidation and Consistency</h4>\n<p>Cached query plans become invalid when underlying data characteristics or schema definitions change. Effective cache management requires sophisticated invalidation policies that balance plan freshness with optimization overhead.</p>\n<p>The cache invalidation system monitors several categories of changes:</p>\n<table>\n<thead>\n<tr>\n<th>Change Type</th>\n<th>Detection Method</th>\n<th>Invalidation Scope</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Table Statistics Updates</td>\n<td>Statistics timestamp comparison</td>\n<td>Plans using affected tables</td>\n<td>Recompute costs for cached plans using updated statistics</td>\n</tr>\n<tr>\n<td>Schema Modifications</td>\n<td>DDL operation tracking</td>\n<td>Plans referencing modified tables/indexes</td>\n<td>Remove affected plans from cache, trigger reoptimization</td>\n</tr>\n<tr>\n<td>Index Creation/Removal</td>\n<td>Index metadata change detection</td>\n<td>Plans with affected access method choices</td>\n<td>Rerun physical planning with updated index availability</td>\n</tr>\n<tr>\n<td>Data Volume Changes</td>\n<td>Row count threshold monitoring</td>\n<td>Plans with cardinality-sensitive operator choices</td>\n<td>Refresh statistics and revalidate plan costs</td>\n</tr>\n</tbody></table>\n<h4 id=\"cache-storage-and-memory-management\">Cache Storage and Memory Management</h4>\n<p>Plan caching requires careful memory management to prevent cache size from impacting system performance while maximizing plan reuse benefits:</p>\n<p>The cache storage system implements several key policies:</p>\n<ol>\n<li><p><strong>Size-Based Eviction</strong>: Implement LRU (Least Recently Used) eviction when cache size exceeds configured memory limits, prioritizing recently accessed plans for retention.</p>\n</li>\n<li><p><strong>Cost-Based Prioritization</strong>: Weight eviction decisions by plan optimization cost, retaining expensive-to-optimize plans longer than simple single-table queries.</p>\n</li>\n<li><p><strong>Access Frequency Tracking</strong>: Maintain hit count statistics for cached plans, giving higher retention priority to frequently reused plans.</p>\n</li>\n<li><p><strong>Memory Footprint Estimation</strong>: Calculate memory consumption for each cached plan including operator tree structure and statistics references.</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>Cache Management Strategy</th>\n<th>Implementation</th>\n<th>Memory Impact</th>\n<th>Performance Benefit</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Fixed-size LRU cache</td>\n<td>Hash table + doubly-linked list</td>\n<td>O(n) where n is cache capacity</td>\n<td>Predictable memory usage, good hit rates</td>\n</tr>\n<tr>\n<td>Cost-weighted eviction</td>\n<td>Priority queue by optimization cost</td>\n<td>O(n log n) for eviction decisions</td>\n<td>Retains high-value plans longer</td>\n</tr>\n<tr>\n<td>Hierarchical caching</td>\n<td>Separate caches by query complexity</td>\n<td>O(c × n) where c is complexity levels</td>\n<td>Better hit rates for common query patterns</td>\n</tr>\n<tr>\n<td>Compressed plan storage</td>\n<td>Serialize plans with compression</td>\n<td>Reduced memory but higher CPU overhead</td>\n<td>Enables larger effective cache capacity</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Performance Insight</strong>: Plan caching effectiveness depends heavily on query workload patterns. OLTP workloads with repetitive queries achieve 80-95% cache hit rates, while analytical workloads with diverse queries may see 20-40% hit rates. Cache sizing should account for workload characteristics and available memory resources.</p>\n</blockquote>\n<p><img src=\"/api/project/query-optimizer/architecture-doc/asset?path=diagrams%2Foptimizer-state-machine.svg\" alt=\"Optimizer State Transitions\"></p>\n<h4 id=\"plan-cache-implementation-architecture\">Plan Cache Implementation Architecture</h4>\n<p>The plan cache implementation provides thread-safe access for concurrent query optimization while maintaining cache consistency:</p>\n<table>\n<thead>\n<tr>\n<th>Cache Component</th>\n<th>Responsibility</th>\n<th>Concurrency Model</th>\n<th>Persistence Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cache Manager</td>\n<td>Overall cache policy and eviction</td>\n<td>Read-write locks for cache operations</td>\n<td>Optional disk-based persistence for expensive plans</td>\n</tr>\n<tr>\n<td>Plan Serializer</td>\n<td>Convert plans to/from storage format</td>\n<td>Lock-free serialization with immutable plans</td>\n<td>Compact binary format with schema versioning</td>\n</tr>\n<tr>\n<td>Invalidation Monitor</td>\n<td>Track statistics and schema changes</td>\n<td>Event-driven invalidation with async processing</td>\n<td>Transaction log-based change detection</td>\n</tr>\n<tr>\n<td>Hit Rate Analyzer</td>\n<td>Cache performance monitoring and tuning</td>\n<td>Lock-free statistics collection</td>\n<td>Periodic analysis with adaptive cache sizing</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Cache Invalidation Lag</strong>\nA common mistake is failing to account for the delay between data changes and statistics updates. If statistics are updated asynchronously (e.g., overnight batch processes), cached plans may use stale information for extended periods. Implement timestamp-based validation that compares plan cache time against both statistics update time and reasonable staleness thresholds. For critical applications, consider forcing statistics updates or plan recomputation when data modification volume exceeds thresholds.</p>\n<p>⚠️ <strong>Pitfall: Parameter Value Skew</strong>\nWhen using parametric plan templates, parameter values may have dramatically different selectivities that invalidate cached optimization decisions. For example, a template optimized for <code>WHERE status = &#39;ACTIVE&#39;</code> (high selectivity) may perform poorly when instantiated with <code>WHERE status = &#39;PENDING&#39;</code> (low selectivity). Implement selectivity bucketing where templates are cached separately for different selectivity ranges, or include dynamic plan switching based on parameter value analysis.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>This section provides concrete implementation patterns for building the optimization pipeline with robust component communication and effective plan caching.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plan Storage</td>\n<td>In-memory Python dictionaries with pickle serialization</td>\n<td>Redis with custom serialization protocol</td>\n</tr>\n<tr>\n<td>Cache Invalidation</td>\n<td>File modification time checking</td>\n<td>Database triggers with event queuing</td>\n</tr>\n<tr>\n<td>Cost Calculation</td>\n<td>Simple floating-point arithmetic</td>\n<td>Decimal types for numerical stability</td>\n</tr>\n<tr>\n<td>Statistics Storage</td>\n<td>JSON files with periodic updates</td>\n<td>Database tables with incremental updates</td>\n</tr>\n<tr>\n<td>Plan Templates</td>\n<td>String-based parameter substitution</td>\n<td>Abstract syntax tree transformation</td>\n</tr>\n</tbody></table>\n<h4 id=\"file-structure-for-optimization-pipeline\">File Structure for Optimization Pipeline</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>optimizer/\n├── core/\n│   ├── __init__.py\n│   ├── query_optimizer.py        ← Main optimization pipeline\n│   ├── execution_plan.py         ← ExecutionPlan and OperatorNode classes\n│   └── cost_model.py            ← CostEstimate and cost calculation utilities\n├── components/\n│   ├── __init__.py\n│   ├── plan_builder.py          ← Logical plan construction\n│   ├── cost_estimator.py        ← Statistical cost modeling\n│   ├── join_optimizer.py        ← Dynamic programming join ordering\n│   └── physical_planner.py      ← Physical operator selection\n├── caching/\n│   ├── __init__.py\n│   ├── plan_cache.py            ← Plan storage and retrieval\n│   ├── cache_manager.py         ← Eviction and invalidation policies\n│   └── plan_serializer.py      ← Plan persistence and deserialization\n├── interfaces/\n│   ├── __init__.py\n│   ├── optimizer_interfaces.py   ← Component interface definitions\n│   └── statistics_provider.py   ← Statistics collection interface\n└── utils/\n    ├── __init__.py\n    ├── query_hash.py            ← Query normalization and hashing\n    └── plan_validator.py       ← Plan correctness checking</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p>Complete plan cache implementation for immediate use:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># optimizer/caching/plan_cache.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> pickle</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> OrderedDict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional, List, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> threading </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> RLock</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..core.execution_plan </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExecutionPlan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..core.cost_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimate</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PlanCacheEntry</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a cached execution plan with metadata.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, plan: ExecutionPlan, optimization_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> plan</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimization_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimization_cost</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.creation_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.access_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_access_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.memory_footprint </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._estimate_memory_usage(plan)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _estimate_memory_usage</span><span style=\"color:#E1E4E8\">(self, plan: ExecutionPlan) -> </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Estimate memory footprint of cached plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(pickle.dumps(plan, </span><span style=\"color:#FFAB70\">protocol</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">pickle.</span><span style=\"color:#79B8FF\">HIGHEST_PROTOCOL</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#6A737D\">  # Conservative default estimate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> update_access_stats</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update access tracking for cache eviction decisions.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.access_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.last_access_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PlanCache</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Thread-safe LRU cache for optimized execution plans.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, max_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">, max_memory_mb: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_size </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_size</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_memory_bytes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> max_memory_mb </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._cache: OrderedDict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, PlanCacheEntry] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> OrderedDict()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> RLock()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._current_memory_usage </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._hit_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._miss_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_plan</span><span style=\"color:#E1E4E8\">(self, query_hash: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Optional[ExecutionPlan]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Retrieve cached plan if available.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> query_hash </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cache:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cache[query_hash]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                entry.update_access_stats()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # Move to end (most recently used)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._cache.move_to_end(query_hash)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._hit_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> entry.plan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._miss_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> store_plan</span><span style=\"color:#E1E4E8\">(self, query_hash: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, plan: ExecutionPlan, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                   optimization_cost: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Store optimized plan in cache.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PlanCacheEntry(plan, optimization_cost)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Check if we need to evict entries</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            while</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._cache) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_size </span><span style=\"color:#F97583\">or</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                   self</span><span style=\"color:#E1E4E8\">._current_memory_usage </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> entry.memory_footprint </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.max_memory_bytes):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._evict_lru_entry()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._cache[query_hash] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> entry</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">._current_memory_usage </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> entry.memory_footprint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _evict_lru_entry</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove least recently used entry from cache.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cache:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # OrderedDict maintains insertion order; first item is least recently used</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        lru_hash, lru_entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cache.popitem(</span><span style=\"color:#FFAB70\">last</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">False</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._current_memory_usage </span><span style=\"color:#F97583\">-=</span><span style=\"color:#E1E4E8\"> lru_entry.memory_footprint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> invalidate_plans_for_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Remove cached plans that reference a specific table.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            invalid_hashes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> query_hash, entry </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cache.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                if</span><span style=\"color:#E1E4E8\"> table_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> entry.plan.get_all_tables():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    invalid_hashes.append(query_hash)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> hash_key </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> invalid_hashes:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                entry </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._cache.pop(hash_key)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._current_memory_usage </span><span style=\"color:#F97583\">-=</span><span style=\"color:#E1E4E8\"> entry.memory_footprint</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_cache_stats</span><span style=\"color:#E1E4E8\">(self) -> Dict:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return cache performance statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        with</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._lock:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            total_requests </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._hit_count </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._miss_count</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            hit_rate </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._hit_count </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> total_requests </span><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> total_requests </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'size'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._cache),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'max_size'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_size,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'memory_usage_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._current_memory_usage </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'max_memory_mb'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.max_memory_bytes </span><span style=\"color:#F97583\">/</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">1024</span><span style=\"color:#F97583\"> *</span><span style=\"color:#79B8FF\"> 1024</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'hit_count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._hit_count,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'miss_count'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._miss_count,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">                'hit_rate'</span><span style=\"color:#E1E4E8\">: hit_rate</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            }</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># optimizer/utils/query_hash.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> hashlib</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Any</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> normalize_sql_text</span><span style=\"color:#E1E4E8\">(sql: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Normalize SQL text for consistent caching.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Convert to lowercase and remove extra whitespace</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> re.sub(</span><span style=\"color:#F97583\">r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\s</span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">' '</span><span style=\"color:#E1E4E8\">, sql.strip().lower())</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Sort WHERE conditions for deterministic ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # This is a simplified approach - production systems need more sophisticated parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    if</span><span style=\"color:#9ECBFF\"> 'where'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> normalized:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parts </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalized.split(</span><span style=\"color:#9ECBFF\">'where'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(parts) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            where_clause </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parts[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].split(</span><span style=\"color:#9ECBFF\">'order by'</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">].split(</span><span style=\"color:#9ECBFF\">'group by'</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conditions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [cond.strip() </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> cond </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> where_clause.split(</span><span style=\"color:#9ECBFF\">'and'</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            conditions.sort()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            sorted_where </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> ' and '</span><span style=\"color:#E1E4E8\">.join(conditions)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parts[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> 'where '</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> sorted_where</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#9ECBFF\"> 'order by'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> parts[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                normalized </span><span style=\"color:#F97583\">+=</span><span style=\"color:#9ECBFF\"> ' order by'</span><span style=\"color:#F97583\"> +</span><span style=\"color:#E1E4E8\"> parts[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">].split(</span><span style=\"color:#9ECBFF\">'order by'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> normalized</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> compute_query_hash</span><span style=\"color:#E1E4E8\">(sql: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generate stable hash for SQL query caching.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    normalized </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> normalize_sql_text(sql)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> hashlib.sha256(normalized.encode(</span><span style=\"color:#9ECBFF\">'utf-8'</span><span style=\"color:#E1E4E8\">)).hexdigest()</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> extract_query_parameters</span><span style=\"color:#E1E4E8\">(sql: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Extract parameterizable literals from SQL query.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Simple parameter extraction - production needs full SQL parsing</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    import</span><span style=\"color:#E1E4E8\"> re</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Find numeric literals</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    numeric_pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> r</span><span style=\"color:#9ECBFF\">'</span><span style=\"color:#79B8FF\">\\b\\d</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">(?:</span><span style=\"color:#85E89D;font-weight:bold\">\\.</span><span style=\"color:#79B8FF\">\\d</span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#F97583\">?</span><span style=\"color:#79B8FF\">\\b</span><span style=\"color:#9ECBFF\">'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    string_pattern </span><span style=\"color:#F97583\">=</span><span style=\"color:#F97583\"> r</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#DBEDFF\">'</span><span style=\"color:#79B8FF\">([</span><span style=\"color:#F97583\">^</span><span style=\"color:#79B8FF\">']</span><span style=\"color:#F97583\">*?</span><span style=\"color:#79B8FF\">)</span><span style=\"color:#DBEDFF\">'</span><span style=\"color:#9ECBFF\">\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameters </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parameterized_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> sql</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Replace numeric literals with placeholders</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> match </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> re.finditer(numeric_pattern, sql):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        value </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> match.group(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> value.replace(</span><span style=\"color:#9ECBFF\">'.'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">).isdigit():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            parameters.append(</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">(value) </span><span style=\"color:#F97583\">if</span><span style=\"color:#9ECBFF\"> '.'</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> value </span><span style=\"color:#F97583\">else</span><span style=\"color:#79B8FF\"> int</span><span style=\"color:#E1E4E8\">(value))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            parameterized_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parameterized_sql.replace(value, </span><span style=\"color:#9ECBFF\">'?'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Replace string literals with placeholders</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> match </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> re.finditer(string_pattern, sql):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parameters.append(match.group(</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        parameterized_sql </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parameterized_sql.replace(match.group(</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">), </span><span style=\"color:#9ECBFF\">'?'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> normalize_sql_text(parameterized_sql), parameters</span></span></code></pre></div>\n\n<h4 id=\"core-pipeline-implementation-skeleton\">Core Pipeline Implementation Skeleton</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># optimizer/core/query_optimizer.py</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> .execution_plan </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> ExecutionPlan, ParsedQuery</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..components.plan_builder </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PlanBuilder</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..components.cost_estimator </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..components.join_optimizer </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> JoinOptimizer</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..components.physical_planner </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PhysicalPlanner</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..caching.plan_cache </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> PlanCache</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> ..utils.query_hash </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> compute_query_hash</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueryOptimizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main query optimization pipeline coordinating all components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, statistics_provider, plan_cache_size: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.plan_builder </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PlanBuilder()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_estimator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CostEstimator(statistics_provider)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.join_optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JoinOptimizer()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.physical_planner </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PhysicalPlanner()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.plan_cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PlanCache(</span><span style=\"color:#FFAB70\">max_size</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">plan_cache_size)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.statistics_provider </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> statistics_provider</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Performance monitoring</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimization_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.total_optimization_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> optimize_query</span><span style=\"color:#E1E4E8\">(self, parsed_query: ParsedQuery, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                      original_sql: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> None</span><span style=\"color:#E1E4E8\">) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main optimization entry point implementing complete pipeline.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Generate cache key from SQL text if provided</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cache_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> original_sql:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cache_key </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> compute_query_hash(original_sql)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            cached_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.plan_cache.get_plan(cache_key)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> cached_plan:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                return</span><span style=\"color:#E1E4E8\"> cached_plan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Build initial logical plan from parsed query</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        logical_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.plan_builder.build_logical_plan(parsed_query)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Collect statistics for all referenced tables</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> table_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> parsed_query.tables:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.statistics_provider.get_table_stats(table_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#F97583\"> not</span><span style=\"color:#E1E4E8\"> stats:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">                # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle missing statistics - collect or use defaults</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Annotate logical plan with cost estimates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cost_annotated_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.cost_estimator.estimateCost(logical_plan)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Optimize join ordering if multi-table query</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        optimized_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cost_annotated_plan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(parsed_query.tables) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            optimized_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.join_optimizer.optimizeJoinOrder(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cost_annotated_plan, parsed_query.tables)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 6: Select physical operators and access methods</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        physical_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.physical_planner.selectPhysicalOperators(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            optimized_plan, </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.statistics_provider)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 7: Cache the optimized plan if caching enabled</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> cache_key:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            optimization_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (datetime.now() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time).total_seconds()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.plan_cache.store_plan(cache_key, physical_plan, optimization_time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Update performance statistics</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._update_optimization_stats(start_time)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> physical_plan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _update_optimization_stats</span><span style=\"color:#E1E4E8\">(self, start_time: datetime):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Update internal performance monitoring statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Track optimization time and count for performance analysis</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        optimization_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (datetime.now() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> start_time).total_seconds()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.optimization_count </span><span style=\"color:#F97583\">+=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.total_optimization_time </span><span style=\"color:#F97583\">+=</span><span style=\"color:#E1E4E8\"> optimization_time</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> get_optimizer_stats</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Return optimization performance statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return comprehensive statistics including cache hit rates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        avg_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.total_optimization_time </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.optimization_count </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">                   if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.optimization_count </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#F97583\"> else</span><span style=\"color:#79B8FF\"> 0.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'total_optimizations'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.optimization_count,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'average_optimization_time_ms'</span><span style=\"color:#E1E4E8\">: avg_time </span><span style=\"color:#F97583\">*</span><span style=\"color:#79B8FF\"> 1000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'cache_stats'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.plan_cache.get_cache_stats()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> stats</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> invalidate_plans_for_table</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Invalidate cached plans when table statistics change.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Remove plans from cache and optionally recompute statistics</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.plan_cache.invalidate_plans_for_table(table_name)</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>Checkpoint 1: Basic Pipeline Integration</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test basic optimization pipeline without caching</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QueryOptimizer(statistics_provider)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">parsed_query </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ParsedQuery(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    tables</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'customers'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'orders'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    columns</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">'customer_name'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'order_total'</span><span style=\"color:#E1E4E8\">],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    joins</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[(</span><span style=\"color:#9ECBFF\">'customers.id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'orders.customer_id'</span><span style=\"color:#E1E4E8\">)],</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">    filters</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[(</span><span style=\"color:#9ECBFF\">'orders.status'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'='</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'SHIPPED'</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer.optimize_query(parsed_query)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> plan </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(plan.get_all_tables()) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 2</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Basic optimization pipeline working\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Checkpoint 2: Plan Caching Functionality</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test plan caching and reuse</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">sql_query </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> \"SELECT customer_name, order_total FROM customers JOIN orders ON customers.id = orders.customer_id WHERE orders.status = 'SHIPPED'\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plan1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer.optimize_query(parsed_query, sql_query)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">plan2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer.optimize_query(parsed_query, sql_query)</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">cache_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer.get_optimizer_stats()[</span><span style=\"color:#9ECBFF\">'cache_stats'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">assert</span><span style=\"color:#E1E4E8\"> cache_stats[</span><span style=\"color:#9ECBFF\">'hit_count'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"✓ Plan caching working - Hit rate: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">cache_stats[</span><span style=\"color:#9ECBFF\">'hit_rate'</span><span style=\"color:#E1E4E8\">]</span><span style=\"color:#F97583\">:.2%</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<p><strong>Checkpoint 3: Cache Invalidation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test cache invalidation when statistics change</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">optimizer.invalidate_plans_for_table(</span><span style=\"color:#9ECBFF\">'orders'</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">cache_stats_after </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> optimizer.get_optimizer_stats()[</span><span style=\"color:#9ECBFF\">'cache_stats'</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Cache invalidation working\"</span><span style=\"color:#E1E4E8\">)</span></span></code></pre></div>\n\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plans not being cached</td>\n<td>Cache key generation failure or storage errors</td>\n<td>Check if <code>compute_query_hash()</code> returns consistent values for identical queries</td>\n<td>Verify SQL normalization logic handles all query variations</td>\n</tr>\n<tr>\n<td>Stale cached plans</td>\n<td>Cache invalidation not triggered by statistics updates</td>\n<td>Monitor statistics update timestamps vs plan cache timestamps</td>\n<td>Implement proactive invalidation based on data modification volume</td>\n</tr>\n<tr>\n<td>High cache miss rate</td>\n<td>Query variations not recognized as equivalent</td>\n<td>Analyze cache keys for similar queries to identify normalization gaps</td>\n<td>Improve SQL normalization or implement structural matching</td>\n</tr>\n<tr>\n<td>Memory usage growth</td>\n<td>Cache not evicting entries properly</td>\n<td>Monitor cache size and memory usage statistics</td>\n<td>Verify LRU eviction logic and memory footprint calculations</td>\n</tr>\n<tr>\n<td>Optimization timeout</td>\n<td>Join enumeration taking too long despite caching</td>\n<td>Profile optimization time by component to identify bottlenecks</td>\n<td>Implement time-based early termination in dynamic programming</td>\n</tr>\n</tbody></table>\n<h2 id=\"error-handling-and-edge-cases\">Error Handling and Edge Cases</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All Milestones 1-4 - addresses failure modes and degenerate cases that can occur throughout the optimization pipeline, from plan representation through cost estimation, join ordering, and physical planning.</p>\n</blockquote>\n<p>Query optimizers must gracefully handle a wide range of failure scenarios and edge cases that can occur during the optimization process. Unlike simple applications where invalid input can be rejected, query optimizers must make reasonable decisions even when faced with incomplete information, missing statistics, or pathological query patterns. The optimizer serves as a critical component in the database system, and optimization failures can cascade into complete query execution failures, making robust error handling essential.</p>\n<p>Think of error handling in query optimization like emergency procedures for air traffic control. Even when unexpected conditions arise—missing weather data, aircraft equipment failures, or communication breakdowns—the system must continue operating safely and make the best decisions possible with available information. Similarly, a query optimizer must have fallback strategies for every component failure mode, ensuring that users always receive an executable plan, even if it&#39;s not perfectly optimized.</p>\n<p>The challenge lies in distinguishing between recoverable optimization problems (where fallback strategies can produce reasonable plans) and fundamental query errors (where optimization should fail fast to prevent runtime errors). This section examines the systematic approach to handling these failure modes across all components of the optimization pipeline.</p>\n<h3 id=\"optimization-failure-modes\">Optimization Failure Modes</h3>\n<p>Query optimization can fail in numerous ways, ranging from transient resource constraints to fundamental algorithmic limitations. Understanding these failure modes and implementing appropriate fallback strategies ensures that the optimizer remains robust under production conditions.</p>\n<h4 id=\"resource-exhaustion-scenarios\">Resource Exhaustion Scenarios</h4>\n<p>The most common optimization failures stem from resource constraints that prevent the optimizer from completing its work within acceptable bounds. These failures typically manifest during computationally intensive phases like join ordering or plan enumeration.</p>\n<p><strong>Memory Exhaustion During Join Ordering</strong> occurs when the dynamic programming algorithm attempts to enumerate all possible join orders for queries with many tables. The number of possible join orders grows exponentially with table count (n! for n tables), and the memoization tables required by dynamic programming can quickly exhaust available memory.</p>\n<table>\n<thead>\n<tr>\n<th>Failure Condition</th>\n<th>Detection Method</th>\n<th>Fallback Strategy</th>\n<th>Quality Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>DP table size exceeds threshold</td>\n<td>Monitor memory allocation during enumeration</td>\n<td>Switch to greedy heuristic ordering</td>\n<td>Moderate - may miss optimal order</td>\n</tr>\n<tr>\n<td>Join enumeration timeout</td>\n<td>Track elapsed optimization time</td>\n<td>Use left-deep join trees only</td>\n<td>Low - restricts plan shape</td>\n</tr>\n<tr>\n<td>Statistics collection overflow</td>\n<td>Monitor statistics storage size</td>\n<td>Sample statistics more aggressively</td>\n<td>Low - reduced estimation accuracy</td>\n</tr>\n<tr>\n<td>Plan cache memory pressure</td>\n<td>Monitor cache size and hit rates</td>\n<td>Evict LRU plans and reduce cache size</td>\n<td>Minimal - affects reuse only</td>\n</tr>\n</tbody></table>\n<p>The optimizer detects memory pressure by monitoring allocation sizes during critical phases. When the estimated memory requirement for completing dynamic programming exceeds <code>MAX_JOIN_ENUMERATION</code>, the system automatically switches to heuristic-based join ordering that requires O(n²) space instead of O(2^n).</p>\n<p><strong>Optimization Timeout Handling</strong> addresses scenarios where query complexity prevents optimization completion within the allocated time budget. Complex queries with many tables, predicates, and potential access paths can require exponential search time without proper pruning.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Timeout detection during optimization phases</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#E1E4E8\"> elapsed_time </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> OPTIMIZATION_TIMEOUT</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Fall back to rule-based optimization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> apply_heuristic_optimization(parsed_query)</span></span></code></pre></div>\n\n<p>The system implements cascading timeout handling where each optimization phase has progressively shorter time limits. If join ordering times out, the system falls back to left-deep trees. If cost-based physical selection times out, it applies rule-based operator selection. This ensures that optimization always completes with some executable plan.</p>\n<h4 id=\"statistical-accuracy-failures\">Statistical Accuracy Failures</h4>\n<p>Cost-based optimization depends critically on accurate statistics, but various conditions can render statistics unreliable or unavailable. The optimizer must detect these conditions and adapt its decision-making accordingly.</p>\n<p><strong>Statistics Staleness Detection</strong> identifies when cached statistics no longer reflect the current state of underlying tables. Stale statistics can lead to dramatically incorrect cost estimates, causing the optimizer to select highly inefficient plans.</p>\n<table>\n<thead>\n<tr>\n<th>Staleness Indicator</th>\n<th>Detection Method</th>\n<th>Confidence Adjustment</th>\n<th>Fallback Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Last update &gt; 24 hours</td>\n<td>Compare <code>TableStatistics.last_updated</code> with current time</td>\n<td>Reduce confidence by 50%</td>\n<td>Request background statistics refresh</td>\n</tr>\n<tr>\n<td>Row count mismatch</td>\n<td>Compare cached count with actual table size</td>\n<td>Set confidence to 10%</td>\n<td>Use default selectivity estimates</td>\n</tr>\n<tr>\n<td>Missing column statistics</td>\n<td>Check for null <code>ColumnStatistics</code> entries</td>\n<td>Use uniform distribution assumption</td>\n<td>Apply conservative selectivity factors</td>\n</tr>\n<tr>\n<td>Histogram corruption</td>\n<td>Validate histogram bucket consistency</td>\n<td>Disable histogram-based estimation</td>\n<td>Use simple min/max range estimates</td>\n</tr>\n</tbody></table>\n<p>The <code>CostEstimate.confidence_level</code> field tracks the reliability of cost calculations based on statistics quality. Plans with low confidence levels trigger additional validation and may be rejected in favor of simpler alternatives with higher confidence.</p>\n<p><strong>Missing Statistics Handling</strong> addresses scenarios where tables lack statistical information entirely, often occurring after table creation or bulk data loading. The optimizer cannot simply fail when statistics are missing—it must make reasonable estimates based on available information.</p>\n<blockquote>\n<p><strong>Decision: Default Statistics Strategy</strong></p>\n<ul>\n<li><strong>Context</strong>: New tables or tables with outdated statistics require cost estimation without reliable data</li>\n<li><strong>Options Considered</strong>: 1) Fail optimization, 2) Use hardcoded defaults, 3) Derive estimates from schema information</li>\n<li><strong>Decision</strong>: Use schema-based estimation with conservative factors</li>\n<li><strong>Rationale</strong>: Schema information (column types, constraints, indexes) provides better estimates than arbitrary defaults while avoiding optimization failures</li>\n<li><strong>Consequences</strong>: Enables optimization of new tables but may produce suboptimal plans until real statistics are collected</li>\n</ul>\n</blockquote>\n<h4 id=\"component-integration-failures\">Component Integration Failures</h4>\n<p>Complex interactions between optimizer components can lead to integration failures where individual components function correctly but their combination produces invalid results.</p>\n<p><strong>Plan Validation Failures</strong> occur when the physical planning phase produces plans that appear valid to the optimizer but would fail during execution. These failures often stem from mismatches between logical and physical operator capabilities.</p>\n<p>Common validation failures include:</p>\n<ul>\n<li>Selecting index scans for columns without suitable indexes</li>\n<li>Choosing hash joins for data types that cannot be hashed consistently  </li>\n<li>Applying predicates to columns that don&#39;t exist in the operator&#39;s input schema</li>\n<li>Creating plans that exceed available memory for hash tables or sort operations</li>\n</ul>\n<p>The <code>_validate_physical_plan()</code> method performs comprehensive validation before returning optimized plans:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Check</th>\n<th>Failure Condition</th>\n<th>Recovery Action</th>\n<th>Prevention Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Schema consistency</td>\n<td>Output columns don&#39;t match expected schema</td>\n<td>Rebuild plan with corrected projections</td>\n<td>Validate schema propagation during planning</td>\n</tr>\n<tr>\n<td>Resource feasibility</td>\n<td>Memory requirements exceed available resources</td>\n<td>Switch to streaming algorithms</td>\n<td>Check resource constraints during operator selection</td>\n</tr>\n<tr>\n<td>Index availability</td>\n<td>Plan references non-existent indexes</td>\n<td>Fall back to sequential scans</td>\n<td>Verify index existence during access method selection</td>\n</tr>\n<tr>\n<td>Type compatibility</td>\n<td>Join or filter predicates have type mismatches</td>\n<td>Apply implicit type conversions or reject predicate</td>\n<td>Validate predicate types during logical planning</td>\n</tr>\n</tbody></table>\n<h3 id=\"degenerate-query-patterns\">Degenerate Query Patterns</h3>\n<p>Certain query patterns present fundamental challenges to cost-based optimization, either because they lack sufficient structure for meaningful cost comparison or because they represent inherently expensive operations that defy optimization.</p>\n<h4 id=\"cross-product-queries\">Cross Product Queries</h4>\n<p>Queries without proper join predicates create cross products that produce extremely large intermediate results. These queries often represent user errors, but the optimizer must handle them gracefully rather than failing.</p>\n<p><strong>Cross Product Detection</strong> identifies when join operations lack connecting predicates, indicating potential cartesian products between tables. The <code>JoinGraph</code> component analyzes predicate connectivity to detect disconnected table groups.</p>\n<p>Consider a query joining three tables without predicates:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#F97583\"> *</span><span style=\"color:#F97583\"> FROM</span><span style=\"color:#E1E4E8\"> orders, customers, products</span></span></code></pre></div>\n\n<p>This query creates a cross product of all three tables. If each table contains 10,000 rows, the result would contain 10^12 rows—clearly unintentional and computationally infeasible.</p>\n<p>The optimizer detects cross products by building a connectivity graph where tables are nodes and join predicates are edges. Disconnected components in this graph indicate cross products. When detected, the system applies several mitigation strategies:</p>\n<table>\n<thead>\n<tr>\n<th>Cross Product Type</th>\n<th>Detection Method</th>\n<th>Mitigation Strategy</th>\n<th>User Feedback</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Complete cross product</td>\n<td>No join predicates between any tables</td>\n<td>Add row limit and warning</td>\n<td>&quot;Query may produce large result set&quot;</td>\n</tr>\n<tr>\n<td>Partial cross product</td>\n<td>Some tables connected, others isolated</td>\n<td>Optimize connected groups separately</td>\n<td>&quot;Missing join conditions detected&quot;</td>\n</tr>\n<tr>\n<td>Implicit cross product</td>\n<td>Predicates exist but don&#39;t connect all tables</td>\n<td>Suggest additional predicates</td>\n<td>&quot;Consider adding WHERE conditions&quot;</td>\n</tr>\n<tr>\n<td>Accidental cross product</td>\n<td>Typos in table/column names</td>\n<td>Fuzzy match column names</td>\n<td>&quot;Did you mean &#39;customer_id&#39;?&quot;</td>\n</tr>\n</tbody></table>\n<p><strong>Cross Product Cost Modeling</strong> requires special handling because normal cardinality estimation produces astronomical numbers that overflow standard numeric types. The system uses logarithmic cost representation for cross products and applies aggressive penalties to discourage their selection.</p>\n<h4 id=\"extremely-selective-filters\">Extremely Selective Filters</h4>\n<p>Queries with very low selectivity predicates (selecting less than 0.1% of rows) present challenges for cost estimation because small errors in selectivity calculation can dramatically affect join ordering decisions.</p>\n<p><strong>Ultra-Low Selectivity Handling</strong> addresses predicates that select tiny fractions of large tables. These predicates often involve rare values, complex expressions, or multiple AND-connected conditions.</p>\n<p>Consider a predicate like:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">WHERE</span><span style=\"color:#E1E4E8\"> customer_type </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'PREMIUM'</span><span style=\"color:#F97583\"> AND</span><span style=\"color:#E1E4E8\"> last_purchase_date </span><span style=\"color:#F97583\">></span><span style=\"color:#9ECBFF\"> '2023-12-01'</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">  AND</span><span style=\"color:#E1E4E8\"> account_balance </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 100000</span><span style=\"color:#F97583\"> AND</span><span style=\"color:#E1E4E8\"> region </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'ANTARCTICA'</span></span></code></pre></div>\n\n<p>Each individual condition might have reasonable selectivity, but their combination could select fewer than 10 rows from a million-row table. Standard independence assumptions used in selectivity estimation become highly unreliable at these scales.</p>\n<table>\n<thead>\n<tr>\n<th>Selectivity Range</th>\n<th>Estimation Method</th>\n<th>Confidence Adjustment</th>\n<th>Special Handling</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1.0 - 0.1</td>\n<td>Standard histogram lookup</td>\n<td>Normal confidence</td>\n<td>Standard cost calculation</td>\n</tr>\n<tr>\n<td>0.1 - 0.01</td>\n<td>Histogram with correlation adjustment</td>\n<td>Reduce confidence by 25%</td>\n<td>Prefer index scans</td>\n</tr>\n<tr>\n<td>0.01 - 0.001</td>\n<td>Conservative minimum estimates</td>\n<td>Reduce confidence by 50%</td>\n<td>Force nested loop joins</td>\n</tr>\n<tr>\n<td>&lt; 0.001</td>\n<td>Fixed minimum selectivity floor</td>\n<td>Set confidence to 20%</td>\n<td>Apply ultra-selective query optimizations</td>\n</tr>\n</tbody></table>\n<p><strong>Ultra-Selective Query Optimization</strong> applies special rules when predicates have extremely low selectivity. These rules override normal cost-based decisions to account for the high uncertainty in cost estimates.</p>\n<p>Ultra-selective queries benefit from specific optimization strategies:</p>\n<ul>\n<li>Force index usage even when cost estimates suggest sequential scans</li>\n<li>Prefer nested loop joins where the ultra-selective table drives the join</li>\n<li>Apply the most selective predicates first to minimize intermediate result sizes</li>\n<li>Use materialization to avoid repeatedly evaluating expensive predicates</li>\n</ul>\n<h4 id=\"queries-with-no-valid-indexes\">Queries with No Valid Indexes</h4>\n<p>Some queries reference columns that lack appropriate indexes, forcing the optimizer to rely entirely on sequential scans. While not technically degenerate, these queries require special handling to avoid poor optimization decisions.</p>\n<p><strong>Index Absence Handling</strong> addresses scenarios where optimal query execution would require indexes that don&#39;t exist. The optimizer must make realistic cost comparisons when all access methods involve full table scans.</p>\n<table>\n<thead>\n<tr>\n<th>Index Absence Scenario</th>\n<th>Cost Implication</th>\n<th>Optimization Strategy</th>\n<th>Recommendation Generation</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>No indexes on any referenced columns</td>\n<td>All scans are sequential</td>\n<td>Optimize join order for sequential access</td>\n<td>Suggest primary key indexes</td>\n</tr>\n<tr>\n<td>Missing composite indexes</td>\n<td>Individual column indexes available</td>\n<td>Use index intersection or union</td>\n<td>Suggest composite index creation</td>\n</tr>\n<tr>\n<td>Wrong index column order</td>\n<td>Index exists but doesn&#39;t match predicate order</td>\n<td>Use partial index scans</td>\n<td>Suggest reordered composite indexes</td>\n</tr>\n<tr>\n<td>Outdated index statistics</td>\n<td>Indexes exist but statistics are stale</td>\n<td>Prefer sequential scans until refresh</td>\n<td>Trigger index statistics update</td>\n</tr>\n</tbody></table>\n<p>When no suitable indexes exist, the optimizer focuses on minimizing the amount of data that must be scanned sequentially. This often means reordering operations to apply the most selective filters first and choosing join orders that minimize intermediate result sizes.</p>\n<h3 id=\"stale-statistics-handling\">Stale Statistics Handling</h3>\n<p>Database statistics become outdated as data changes over time, leading to increasingly inaccurate cost estimates and suboptimal query plans. The optimizer must detect statistics staleness and adapt its decision-making to account for reduced estimation accuracy.</p>\n<h4 id=\"staleness-detection-strategies\">Staleness Detection Strategies</h4>\n<p><strong>Time-Based Staleness Detection</strong> uses the <code>last_updated</code> timestamp in <code>TableStatistics</code> to identify potentially outdated information. However, time alone is insufficient—a table that hasn&#39;t changed in weeks may have perfectly valid statistics, while a rapidly changing table might need updates every few minutes.</p>\n<p>The system implements multi-factor staleness detection:</p>\n<table>\n<thead>\n<tr>\n<th>Staleness Factor</th>\n<th>Measurement Method</th>\n<th>Threshold Values</th>\n<th>Confidence Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Time since last update</td>\n<td>Compare <code>last_updated</code> with current timestamp</td>\n<td>6 hours: minor, 24 hours: moderate, 7 days: severe</td>\n<td>Reduce by 10%, 25%, 50%</td>\n</tr>\n<tr>\n<td>Estimated row changes</td>\n<td>Track INSERT/DELETE/UPDATE operations since statistics collection</td>\n<td>&gt;5% changes: moderate, &gt;20%: severe</td>\n<td>Reduce by 20%, 60%</td>\n</tr>\n<tr>\n<td>Query pattern changes</td>\n<td>Monitor new WHERE clauses and JOIN patterns</td>\n<td>New predicates on unanalyzed columns</td>\n<td>Reduce by 30% for affected predicates</td>\n</tr>\n<tr>\n<td>Schema modifications</td>\n<td>Detect ALTER TABLE, CREATE INDEX operations</td>\n<td>Any structural changes</td>\n<td>Invalidate all statistics</td>\n</tr>\n</tbody></table>\n<p><strong>Data Modification Tracking</strong> monitors the volume of changes since statistics collection to estimate how much the data distribution might have shifted. The system tracks approximate change volumes without the overhead of maintaining exact counts.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> estimate_statistics_staleness</span><span style=\"color:#E1E4E8\">(table_stats: TableStatistics) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Calculate confidence reduction factor based on staleness indicators\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    time_factor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> calculate_time_staleness(table_stats.last_updated)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    change_factor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> estimate_data_changes(table_stats.table_name) </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    schema_factor </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> check_schema_modifications(table_stats.table_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Combine factors (multiplicative decay)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_multiplier </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time_factor </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> change_factor </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> schema_factor</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">, confidence_multiplier)  </span><span style=\"color:#6A737D\"># Never reduce below 10%</span></span></code></pre></div>\n\n<h4 id=\"statistics-refresh-strategies\">Statistics Refresh Strategies</h4>\n<p><strong>Background Statistics Collection</strong> addresses stale statistics by triggering automatic refresh operations when staleness exceeds acceptable thresholds. However, statistics collection is expensive and can impact system performance, requiring careful scheduling and prioritization.</p>\n<p>The system implements tiered refresh strategies based on table importance and staleness severity:</p>\n<table>\n<thead>\n<tr>\n<th>Refresh Priority</th>\n<th>Trigger Condition</th>\n<th>Collection Method</th>\n<th>Performance Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Immediate</td>\n<td>Critical tables with &gt;50% estimated changes</td>\n<td>Full statistics scan</td>\n<td>High - blocks other operations</td>\n</tr>\n<tr>\n<td>High Priority</td>\n<td>Frequently queried tables with &gt;20% changes</td>\n<td>Sampled statistics collection</td>\n<td>Moderate - background processing</td>\n</tr>\n<tr>\n<td>Normal Priority</td>\n<td>Time-based staleness &gt; 24 hours</td>\n<td>Incremental statistics update</td>\n<td>Low - scheduled during maintenance windows</td>\n</tr>\n<tr>\n<td>Low Priority</td>\n<td>Rarely queried tables</td>\n<td>Defer until next maintenance cycle</td>\n<td>Minimal - batched with other operations</td>\n</tr>\n</tbody></table>\n<p><strong>Adaptive Sampling</strong> adjusts statistics collection intensity based on detected data patterns. Tables with stable distributions require less frequent updates, while rapidly changing tables need more aggressive sampling.</p>\n<p>The <code>sample_rate</code> field in <code>TableStatistics</code> controls the fraction of rows examined during statistics collection. The system dynamically adjusts this rate:</p>\n<ul>\n<li>Stable tables: reduce sample rate to 1-5% for faster collection</li>\n<li>Volatile tables: increase sample rate to 10-25% for better accuracy  </li>\n<li>New tables: use 100% sampling for initial statistics</li>\n<li>Large tables: cap sample size at configurable row limits</li>\n</ul>\n<h4 id=\"confidence-based-decision-making\">Confidence-Based Decision Making</h4>\n<p><strong>Cost Estimation with Uncertainty</strong> modifies the cost calculation process to account for reduced confidence in stale statistics. Rather than treating all cost estimates as equally reliable, the system adjusts its decision-making based on estimation confidence.</p>\n<p>The <code>CostEstimate.confidence_level</code> field propagates uncertainty through the optimization process:</p>\n<table>\n<thead>\n<tr>\n<th>Confidence Level</th>\n<th>Cost Adjustment Strategy</th>\n<th>Plan Selection Impact</th>\n<th>Fallback Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>90-100%</td>\n<td>Use costs as calculated</td>\n<td>Normal cost-based selection</td>\n<td>None - trust cost estimates</td>\n</tr>\n<tr>\n<td>70-89%</td>\n<td>Add 20% uncertainty margin</td>\n<td>Prefer simpler plans when costs are close</td>\n<td>None</td>\n</tr>\n<tr>\n<td>50-69%</td>\n<td>Add 50% uncertainty margin</td>\n<td>Heavily favor proven plan patterns</td>\n<td>Consider rule-based fallbacks</td>\n</tr>\n<tr>\n<td>20-49%</td>\n<td>Add 100% uncertainty margin</td>\n<td>Avoid complex plans</td>\n<td>Switch to heuristic optimization</td>\n</tr>\n<tr>\n<td>&lt;20%</td>\n<td>Use default cost assumptions</td>\n<td>Ignore cost-based decisions</td>\n<td>Full rule-based optimization</td>\n</tr>\n</tbody></table>\n<p>Low confidence levels trigger conservative optimization strategies that favor predictable performance over theoretical optimality. This prevents the optimizer from making risky decisions based on unreliable information.</p>\n<p><strong>Uncertainty Propagation</strong> tracks how confidence levels affect cost calculations throughout the optimization pipeline. When combining cost estimates from multiple sources, the system applies confidence-weighted calculations rather than simple arithmetic.</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> combine_uncertain_costs</span><span style=\"color:#E1E4E8\">(cost1: CostEstimate, cost2: CostEstimate) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Combine costs accounting for confidence levels\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Weight costs by confidence - lower confidence gets conservative adjustment</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    adjusted_cost1 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cost1.total_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> cost1.confidence_level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    adjusted_cost2 </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> cost2.total_cost </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> (</span><span style=\"color:#79B8FF\">2.0</span><span style=\"color:#F97583\"> -</span><span style=\"color:#E1E4E8\"> cost2.confidence_level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    combined_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> adjusted_cost1 </span><span style=\"color:#F97583\">+</span><span style=\"color:#E1E4E8\"> adjusted_cost2</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    combined_confidence </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> min</span><span style=\"color:#E1E4E8\">(cost1.confidence_level, cost2.confidence_level)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> CostEstimate(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        total_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">combined_cost,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        confidence_level</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">combined_confidence,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        cost_factors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">'uncertainty_adjusted'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span></code></pre></div>\n\n<h4 id=\"default-statistics-generation\">Default Statistics Generation</h4>\n<p><strong>Schema-Based Estimation</strong> generates reasonable default statistics when no collected statistics are available. This approach analyzes table schemas, constraints, and index definitions to infer likely data characteristics.</p>\n<p>The system derives default statistics from several schema sources:</p>\n<table>\n<thead>\n<tr>\n<th>Schema Information</th>\n<th>Statistic Inference</th>\n<th>Accuracy Level</th>\n<th>Confidence Setting</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Column data types</td>\n<td>Estimate average row width and distinct values</td>\n<td>Moderate</td>\n<td>40-60%</td>\n</tr>\n<tr>\n<td>Primary key constraints</td>\n<td>Assume 100% distinct values for key columns</td>\n<td>High</td>\n<td>80-90%</td>\n</tr>\n<tr>\n<td>Foreign key constraints</td>\n<td>Estimate join selectivity based on referential integrity</td>\n<td>Moderate</td>\n<td>50-70%</td>\n</tr>\n<tr>\n<td>Check constraints</td>\n<td>Derive range and distribution information</td>\n<td>Low</td>\n<td>30-50%</td>\n</tr>\n<tr>\n<td>Index definitions</td>\n<td>Assume indexes exist for reason (selective columns)</td>\n<td>Moderate</td>\n<td>60-70%</td>\n</tr>\n<tr>\n<td>Table size on disk</td>\n<td>Estimate row count from storage size and schema</td>\n<td>Low</td>\n<td>20-40%</td>\n</tr>\n</tbody></table>\n<p><strong>Conservative Default Values</strong> provide safe fallback estimates when schema analysis yields insufficient information. These defaults err on the side of caution, preferring predictable performance over aggressive optimization.</p>\n<p>Default selectivity factors for unknown predicates:</p>\n<ul>\n<li>Equality predicates: 1% selectivity (assumes reasonably selective values)</li>\n<li>Range predicates: 10% selectivity (conservative range assumption)  </li>\n<li>Pattern matching: 5% selectivity (assumes moderately selective patterns)</li>\n<li>Complex expressions: 25% selectivity (very conservative for unknown expressions)</li>\n</ul>\n<p>These defaults prevent the optimizer from making overly optimistic assumptions that could lead to dramatically poor performance when the estimates prove incorrect.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The error handling and edge case management system requires robust infrastructure for detecting failures, implementing fallback strategies, and maintaining system reliability under adverse conditions.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statistics Validation</td>\n<td>Simple timestamp checks with manual refresh triggers</td>\n<td>Automatic staleness detection with background refresh scheduling</td>\n</tr>\n<tr>\n<td>Error Recovery</td>\n<td>Exception handling with hardcoded fallback plans</td>\n<td>Tiered fallback strategy with confidence-based decision making</td>\n</tr>\n<tr>\n<td>Cross Product Detection</td>\n<td>Basic predicate counting with warnings</td>\n<td>Full connectivity graph analysis with intelligent suggestions</td>\n</tr>\n<tr>\n<td>Resource Monitoring</td>\n<td>Manual memory/time limit checks</td>\n<td>Integrated profiling with adaptive resource management</td>\n</tr>\n<tr>\n<td>Logging and Debugging</td>\n<td>Standard logging with error messages</td>\n<td>Structured logging with optimization decision traces</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>optimizer/\n  error_handling/\n    __init__.py\n    failure_detector.py          ← Detects optimization failure conditions\n    fallback_strategies.py       ← Implements fallback optimization approaches  \n    statistics_validator.py      ← Validates and refreshes stale statistics\n    degenerate_query_handler.py  ← Handles cross products and edge cases\n    confidence_tracker.py        ← Manages cost estimation confidence levels\n  \n  tests/\n    error_handling/\n      test_failure_detection.py   ← Tests for failure mode detection\n      test_fallback_strategies.py ← Tests for fallback optimization quality\n      test_statistics_staleness.py← Tests for staleness detection and refresh\n      test_degenerate_queries.py  ← Tests for edge case handling</code></pre></div>\n\n<h4 id=\"infrastructure-starter-code\">Infrastructure Starter Code</h4>\n<p><strong>Statistics Staleness Detector</strong> - Complete implementation for detecting and managing stale statistics:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, Dict, List</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StalenessAssessment</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_multiplier: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    needs_refresh: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    refresh_priority: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    staleness_factors: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StatisticsStalenessDetector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.time_threshold_hours </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'time_threshold_hours'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">24</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.change_threshold_percent </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'change_threshold_percent'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">20</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.critical_confidence_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'critical_threshold'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> assess_staleness</span><span style=\"color:#E1E4E8\">(self, table_stats: TableStatistics, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                        change_tracker: </span><span style=\"color:#9ECBFF\">'ChangeTracker'</span><span style=\"color:#E1E4E8\">) -> StalenessAssessment:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Assess statistics staleness and compute confidence adjustments\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        factors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Time-based staleness</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        time_factor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._calculate_time_staleness(table_stats.last_updated)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        factors[</span><span style=\"color:#9ECBFF\">'time'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time_factor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Data change staleness  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        change_factor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._calculate_change_staleness(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            table_stats.table_name, table_stats.row_count, change_tracker</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        factors[</span><span style=\"color:#9ECBFF\">'changes'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> change_factor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Schema modification staleness</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        schema_factor </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._calculate_schema_staleness(table_stats.table_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        factors[</span><span style=\"color:#9ECBFF\">'schema'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> schema_factor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Combine factors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        confidence_multiplier </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> time_factor </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> change_factor </span><span style=\"color:#F97583\">*</span><span style=\"color:#E1E4E8\"> schema_factor</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        confidence_multiplier </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">0.1</span><span style=\"color:#E1E4E8\">, confidence_multiplier)  </span><span style=\"color:#6A737D\"># Floor at 10%</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Determine refresh priority</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        needs_refresh </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> confidence_multiplier </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.critical_confidence_threshold</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> confidence_multiplier </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            priority </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'immediate'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> confidence_multiplier </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            priority </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'high'</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> confidence_multiplier </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.8</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            priority </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'normal'</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            priority </span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\"> 'low'</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> StalenessAssessment(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            confidence_multiplier</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">confidence_multiplier,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            needs_refresh</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">needs_refresh,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            refresh_priority</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">priority,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            staleness_factors</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">factors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_time_staleness</span><span style=\"color:#E1E4E8\">(self, last_updated: datetime) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate confidence reduction based on time since update\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        hours_old </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> (datetime.now() </span><span style=\"color:#F97583\">-</span><span style=\"color:#E1E4E8\"> last_updated).total_seconds() </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> 3600</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> hours_old </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 6</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 1.0</span><span style=\"color:#6A737D\">  # Full confidence</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> hours_old </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 24</span><span style=\"color:#E1E4E8\">:  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0.9</span><span style=\"color:#6A737D\">  # Slight reduction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> hours_old </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 168</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># 1 week</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0.75</span><span style=\"color:#6A737D\">  # Moderate reduction</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#6A737D\">  # Significant reduction</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_change_staleness</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   baseline_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   change_tracker: </span><span style=\"color:#9ECBFF\">'ChangeTracker'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate confidence reduction based on estimated data changes\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        estimated_changes </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> change_tracker.get_estimated_changes(table_name)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> estimated_changes </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        change_percentage </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> estimated_changes </span><span style=\"color:#F97583\">/</span><span style=\"color:#79B8FF\"> max</span><span style=\"color:#E1E4E8\">(baseline_rows, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> change_percentage </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.05</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># &#x3C;5% changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 1.0</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> change_percentage </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.20</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># &#x3C;20% changes  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0.8</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        elif</span><span style=\"color:#E1E4E8\"> change_percentage </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#79B8FF\"> 0.50</span><span style=\"color:#E1E4E8\">:  </span><span style=\"color:#6A737D\"># &#x3C;50% changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0.4</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        else</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> 0.2</span><span style=\"color:#6A737D\">  # Major changes</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _calculate_schema_staleness</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate confidence reduction based on schema modifications\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # This would integrate with schema change tracking</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # For now, assume no schema changes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#79B8FF\"> 1.0</span></span></code></pre></div>\n\n<p><strong>Degenerate Query Detector</strong> - Complete implementation for detecting problematic query patterns:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Set, List, Tuple, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CrossProductDetector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connectivity_graph </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">set</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.isolated_tables </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_query_connectivity</span><span style=\"color:#E1E4E8\">(self, parsed_query: ParsedQuery) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">any</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze join connectivity and detect cross products\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">._build_connectivity_graph(parsed_query)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        connected_components </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._find_connected_components()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cross_products </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._identify_cross_products(connected_components)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'has_cross_products'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(connected_components) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'connected_components'</span><span style=\"color:#E1E4E8\">: connected_components,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'cross_product_pairs'</span><span style=\"color:#E1E4E8\">: cross_products,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'isolated_tables'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.isolated_tables,</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'connectivity_suggestions'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._generate_suggestions(parsed_query)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _build_connectivity_graph</span><span style=\"color:#E1E4E8\">(self, parsed_query: ParsedQuery):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Build graph of table connectivity based on join predicates\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.connectivity_graph.clear()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.isolated_tables </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(parsed_query.tables)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Add edges for each join predicate</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> join_condition </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> parsed_query.joins:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            left_table, left_col, right_table, right_col </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> join_condition</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.connectivity_graph[left_table].add(right_table)  </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.connectivity_graph[right_table].add(left_table)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Remove from isolated set</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.isolated_tables.discard(left_table)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            self</span><span style=\"color:#E1E4E8\">.isolated_tables.discard(right_table)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _find_connected_components</span><span style=\"color:#E1E4E8\">(self) -> List[Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find connected components using DFS\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        visited </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        components </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        all_tables </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.connectivity_graph.keys()) </span><span style=\"color:#F97583\">|</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.isolated_tables</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> table </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> all_tables:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> table </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> visited:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                component </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._dfs(table, visited, component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                components.append(component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> components</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _dfs</span><span style=\"color:#E1E4E8\">(self, table: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, visited: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], component: Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Depth-first search for connected component\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        visited.add(table)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        component.add(table)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> neighbor </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.connectivity_graph[table]:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> neighbor </span><span style=\"color:#F97583\">not</span><span style=\"color:#F97583\"> in</span><span style=\"color:#E1E4E8\"> visited:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                self</span><span style=\"color:#E1E4E8\">._dfs(neighbor, visited, component)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _identify_cross_products</span><span style=\"color:#E1E4E8\">(self, components: List[Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]) -> List[Tuple[Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Identify pairs of components that form cross products\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        cross_products </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> i </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(components)):</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            for</span><span style=\"color:#E1E4E8\"> j </span><span style=\"color:#F97583\">in</span><span style=\"color:#79B8FF\"> range</span><span style=\"color:#E1E4E8\">(i </span><span style=\"color:#F97583\">+</span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">len</span><span style=\"color:#E1E4E8\">(components)):</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                cross_products.append((components[i], components[j]))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> cross_products</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _generate_suggestions</span><span style=\"color:#E1E4E8\">(self, parsed_query: ParsedQuery) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate suggestions for fixing cross products\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        suggestions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">.isolated_tables:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            suggestions.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Consider adding WHERE conditions for isolated tables: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">', '</span><span style=\"color:#E1E4E8\">.join(</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">.isolated_tables)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Analyze column names for potential join candidates</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        all_columns </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> set</span><span style=\"color:#E1E4E8\">()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> table_name </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> parsed_query.tables:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # This would query schema information</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            table_columns </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._get_table_columns(table_name)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            all_columns.update([(table_name, col) </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> col </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> table_columns])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Look for similarly named columns across disconnected components</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        potential_joins </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._find_potential_join_columns(all_columns)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        if</span><span style=\"color:#E1E4E8\"> potential_joins:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            suggestions.extend([</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Potential join: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">suggestion</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#F97583\"> for</span><span style=\"color:#E1E4E8\"> suggestion </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> potential_joins])</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> suggestions</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _get_table_columns</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Get column names for table - would integrate with schema catalog\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Placeholder - would query actual schema</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> [</span><span style=\"color:#9ECBFF\">'id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'name'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'created_at'</span><span style=\"color:#E1E4E8\">]  </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _find_potential_join_columns</span><span style=\"color:#E1E4E8\">(self, all_columns: Set[Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Find potential join columns based on naming patterns\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Simple heuristic - look for columns with similar names</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        column_groups </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">list</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> table, column </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> all_columns:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Group by column name (ignoring table prefix)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            base_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column.replace(</span><span style=\"color:#9ECBFF\">'_id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">).replace(</span><span style=\"color:#9ECBFF\">'id'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">''</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            column_groups[base_name].append((table, column))</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        suggestions </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> base_name, columns </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> column_groups.items():</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(columns) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 1</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                tables </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [table </span><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> table, _ </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> columns]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                suggestions.append(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"Tables </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#9ECBFF\">', '</span><span style=\"color:#E1E4E8\">.join(tables)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> have similar column '</span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">base_name</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">'\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> suggestions</span></span></code></pre></div>\n\n<h4 id=\"core-logic-skeleton-code\">Core Logic Skeleton Code</h4>\n<p><strong>Failure Mode Detection</strong> - Core failure detection logic with detailed TODOs:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OptimizationFailureDetector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, config: </span><span style=\"color:#79B8FF\">dict</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_optimization_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'max_time_seconds'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">30</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_memory_mb </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'max_memory_mb'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">512</span><span style=\"color:#E1E4E8\">) </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.max_join_tables </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> config.get(</span><span style=\"color:#9ECBFF\">'max_join_tables'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">12</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> check_optimization_feasibility</span><span style=\"color:#E1E4E8\">(self, parsed_query: ParsedQuery, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                     available_stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">any</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Check if optimization is feasible and identify potential failure modes\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Check if query complexity exceeds system limits</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Count number of tables in FROM clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Estimate join enumeration complexity (2^n for n tables)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - If complexity > MAX_JOIN_ENUMERATION, flag for heuristic fallback</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate statistics availability and quality  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - For each table in query, check if stats exist in available_stats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Calculate average confidence level across all table statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - If average confidence &#x3C; 50%, flag for conservative optimization</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Detect resource-intensive query patterns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Look for cross products (tables without connecting join predicates)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Identify extremely selective filters (estimated selectivity &#x3C; 0.001)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Check for queries with no usable indexes on filter/join columns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Estimate optimization resource requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Calculate expected memory for DP memoization tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Estimate time complexity based on query structure  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Compare against available system resources</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Return comprehensive feasibility assessment</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Include boolean feasible flag</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - List detected failure modes with severity levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Recommend fallback strategies for each failure mode</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Provide confidence estimate for optimization success</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> select_fallback_strategy</span><span style=\"color:#E1E4E8\">(self, failure_modes: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               query_complexity: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Select appropriate fallback optimization strategy based on detected failures\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Prioritize failure modes by severity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'memory_exhaustion' -> immediate heuristic fallback</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'statistics_missing' -> rule-based optimization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'cross_product' -> add warnings but continue optimization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'timeout_risk' -> reduce search space (left-deep only)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Consider query complexity in strategy selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Simple queries (&#x3C;=3 tables): can tolerate some failures</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Medium queries (4-8 tables): need balanced approach</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Complex queries (>8 tables): aggressive simplification needed</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Return specific fallback strategy name</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'full_heuristic': skip cost-based optimization entirely</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'limited_enumeration': reduce DP search space</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'rule_based_physical': skip cost-based physical planning</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - 'conservative_costs': use default costs with high margins</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>Statistics Confidence Tracker</strong> - Core confidence management with detailed TODOs:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StatisticsConfidenceTracker</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.confidence_cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.staleness_detector </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> StatisticsStalenessDetector({})</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_plan_confidence</span><span style=\"color:#E1E4E8\">(self, plan: ExecutionPlan, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                table_stats: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Calculate overall confidence level for execution plan cost estimates\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Extract all tables referenced in the execution plan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Traverse plan tree using traverse_preorder()</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Find all SCAN operators and extract table names</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Build set of unique table names used in plan</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Assess statistics confidence for each table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - For each table, look up TableStatistics in table_stats</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Use staleness_detector.assess_staleness() to get confidence multiplier</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Handle missing statistics (assign default low confidence ~0.2)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Weight confidence by table importance in plan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Tables used in selective filters get higher weight</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Tables appearing in join order early get higher weight</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Large tables (high row count) get higher weight</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Calculate composite confidence score</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Use weighted harmonic mean (conservative - dominated by lowest confidence)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Apply query complexity penalty (more complex = lower confidence)  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Clamp final result between 0.1 and 1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Cache result and update plan metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Store confidence in plan.optimization_metadata['confidence']</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Update each operator's cost_estimate.confidence_level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Return overall plan confidence score</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> adjust_costs_for_uncertainty</span><span style=\"color:#E1E4E8\">(self, cost_estimate: CostEstimate, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                   confidence: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Adjust cost estimates based on confidence level\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Calculate uncertainty margin based on confidence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - High confidence (>0.8): no adjustment needed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Medium confidence (0.5-0.8): add 25-50% margin</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Low confidence (0.2-0.5): add 100-200% margin</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Very low confidence (&#x3C;0.2): add 300% margin</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Apply uncertainty adjustments to cost components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Increase io_cost by uncertainty margin percentage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Increase cpu_cost by uncertainty margin percentage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Increase memory_cost by uncertainty margin percentage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Update total_cost property accordingly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Update cost estimate metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Set confidence_level field to provided confidence</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Add 'uncertainty_adjusted': True to cost_factors dict</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Record original costs in cost_factors for debugging</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Return new CostEstimate with adjusted values</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Create new CostEstimate instance (don't modify original)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Preserve all metadata and factor information</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        #         - Ensure total_cost property calculates correctly</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints\">Milestone Checkpoints</h4>\n<p><strong>After implementing failure detection:</strong></p>\n<ul>\n<li>Run <code>python -m pytest tests/error_handling/test_failure_detection.py -v</code></li>\n<li>Expected: All failure modes detected correctly for test queries</li>\n<li>Manual verification: Create query with 15+ tables, should trigger complexity warning</li>\n<li>Signs of problems: False positives on simple queries, missed detection of obvious cross products</li>\n</ul>\n<p><strong>After implementing statistics staleness handling:</strong></p>\n<ul>\n<li>Run statistics staleness tests with artificially aged data</li>\n<li>Expected: Confidence levels decrease appropriately over time</li>\n<li>Manual verification: Query with 1-day-old stats should show ~75% confidence</li>\n<li>Signs of problems: Confidence levels not updating, refresh triggers not firing</li>\n</ul>\n<p><strong>After implementing degenerate query handling:</strong></p>\n<ul>\n<li>Test cross product detection with queries missing WHERE clauses</li>\n<li>Expected: Cross products detected and suggestions generated</li>\n<li>Manual verification: <code>SELECT * FROM users, orders</code> should trigger cross product warning</li>\n<li>Signs of problems: False cross product detection, missing connectivity analysis</li>\n</ul>\n<h4 id=\"debugging-tips\">Debugging Tips</h4>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis</th>\n<th>Fix</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Optimizer always uses fallback strategies</td>\n<td>Failure detection too aggressive</td>\n<td>Check failure mode thresholds in configuration</td>\n<td>Increase complexity limits, reduce confidence requirements</td>\n</tr>\n<tr>\n<td>Poor plan quality with no warnings</td>\n<td>Stale statistics not detected</td>\n<td>Verify staleness detection logic</td>\n<td>Check timestamp comparisons, add logging to staleness assessment</td>\n</tr>\n<tr>\n<td>Cross product warnings on valid queries</td>\n<td>Connectivity graph not built correctly</td>\n<td>Examine join predicate parsing</td>\n<td>Fix predicate extraction, verify graph construction</td>\n</tr>\n<tr>\n<td>Confidence levels never change</td>\n<td>Statistics validation disabled</td>\n<td>Check if staleness detector is called</td>\n<td>Add staleness assessment to optimization pipeline</td>\n</tr>\n<tr>\n<td>Optimization fails with resource errors</td>\n<td>Resource monitoring not working</td>\n<td>Check memory/time tracking</td>\n<td>Add resource monitoring hooks, verify limit enforcement</td>\n</tr>\n<tr>\n<td>Plans have unrealistic cost estimates</td>\n<td>Uncertainty adjustments not applied</td>\n<td>Verify confidence-based cost adjustment</td>\n<td>Check if adjust_costs_for_uncertainty is called</td>\n</tr>\n</tbody></table>\n<h2 id=\"testing-strategy\">Testing Strategy</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All Milestones 1-4 - provides comprehensive validation strategies for each component and integration points throughout the optimization pipeline.</p>\n</blockquote>\n<h3 id=\"mental-model-quality-assurance-in-manufacturing\">Mental Model: Quality Assurance in Manufacturing</h3>\n<p>Think of testing a query optimizer like quality assurance in a complex manufacturing plant that produces custom products (execution plans). Just as a factory needs multiple inspection checkpoints - incoming materials inspection (input validation), component testing (individual optimizer modules), assembly line verification (integration testing), and final product quality assessment (plan validation) - our query optimizer requires layered testing at each stage. The factory analogy extends further: we need both automated testing equipment (unit tests) and skilled inspectors who can evaluate whether the final product meets quality standards (plan quality validation). Just as manufacturing defects become more expensive to fix the later they&#39;re discovered, optimizer bugs are cheaper to catch in individual components than in the final integrated system.</p>\n<p>Our testing strategy mirrors this multi-layered quality assurance approach. We validate each optimizer component in isolation, verify their interactions produce reasonable results, and establish milestone checkpoints that ensure incremental progress. The key insight is that query optimization correctness has two dimensions: functional correctness (does the plan produce the right results) and quality correctness (is the plan reasonably efficient). Traditional software testing focuses on the first dimension, but optimizer testing must heavily emphasize the second.</p>\n<h3 id=\"component-unit-testing\">Component Unit Testing</h3>\n<p>Component unit testing forms the foundation of our quality assurance strategy, validating each optimizer module&#39;s internal logic before integration. Each component has distinct testing requirements based on its responsibilities and failure modes.</p>\n<p>The <strong>Plan Representation Component</strong> requires testing that validates tree structure integrity and operator semantics. Our test suite must verify that <code>OperatorNode</code> instances correctly maintain parent-child relationships, support tree traversal algorithms, and accurately calculate cost aggregation. The component&#39;s tree manipulation operations need extensive boundary condition testing.</p>\n<table>\n<thead>\n<tr>\n<th>Test Category</th>\n<th>Test Methods</th>\n<th>Validation Focus</th>\n<th>Expected Behavior</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tree Construction</td>\n<td><code>test_operator_node_creation</code>, <code>test_tree_building</code></td>\n<td>Node initialization, parent-child linking</td>\n<td>Valid tree structure with correct relationships</td>\n</tr>\n<tr>\n<td>Tree Traversal</td>\n<td><code>test_preorder_traversal</code>, <code>test_postorder_traversal</code></td>\n<td>Visit order correctness, complete coverage</td>\n<td>All nodes visited in correct sequence</td>\n</tr>\n<tr>\n<td>Cost Calculation</td>\n<td><code>test_cost_accumulation</code>, <code>test_subtree_costs</code></td>\n<td>Bottom-up cost aggregation</td>\n<td>Accurate total cost from leaf costs</td>\n</tr>\n<tr>\n<td>Tree Manipulation</td>\n<td><code>test_node_insertion</code>, <code>test_subtree_replacement</code></td>\n<td>Structural modifications</td>\n<td>Tree remains valid after changes</td>\n</tr>\n<tr>\n<td>Serialization</td>\n<td><code>test_pretty_print</code>, <code>test_plan_export</code></td>\n<td>Human-readable output</td>\n<td>Correct indentation and cost display</td>\n</tr>\n</tbody></table>\n<p>The <strong>Cost Estimation Component</strong> presents unique testing challenges because it deals with statistical predictions rather than deterministic computations. Our test strategy must validate both the mathematical correctness of cost formulas and the reasonableness of estimated values under various data distribution scenarios.</p>\n<table>\n<thead>\n<tr>\n<th>Statistical Scenario</th>\n<th>Test Data Setup</th>\n<th>Expected Behavior</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Uniform Distribution</td>\n<td>Equal frequency across value range</td>\n<td>Linear selectivity curves</td>\n<td>Compare against analytical formulas</td>\n</tr>\n<tr>\n<td>Skewed Distribution</td>\n<td>80% of values in 20% of range</td>\n<td>Higher selectivity for popular values</td>\n<td>Histogram-based validation</td>\n</tr>\n<tr>\n<td>Missing Statistics</td>\n<td>Empty or null statistical data</td>\n<td>Conservative cost estimates</td>\n<td>Fallback to default assumptions</td>\n</tr>\n<tr>\n<td>Extreme Selectivity</td>\n<td>Very high or low filter selectivity</td>\n<td>Appropriate cost scaling</td>\n<td>Boundary condition testing</td>\n</tr>\n<tr>\n<td>Join Cardinality</td>\n<td>Various table size combinations</td>\n<td>Reasonable join size estimates</td>\n<td>Cross-validation with known results</td>\n</tr>\n</tbody></table>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_selectivity_estimation_edge_cases</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test cost estimation component handles edge cases appropriately.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test case: highly selective filter on large table</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    large_table_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_table_stats(</span><span style=\"color:#FFAB70\">row_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1000000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">distinct_values</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">500000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    selectivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> estimator.estimate_filter_selectivity(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        table</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"large_table\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        column</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"id\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        operator</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        value</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"specific_id\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Should be approximately 1/distinct_values</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> 0.000001</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#E1E4E8\"> selectivity </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0.000003</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test case: filter on column with all NULL values</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    null_column_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_column_stats(</span><span style=\"color:#FFAB70\">null_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">distinct_values</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    selectivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> estimator.estimate_filter_selectivity(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        table</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"test_table\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        column</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"null_column\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        operator</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        value</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"any_value\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Should return zero selectivity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> selectivity </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 0.0</span></span></code></pre></div>\n\n<p>The <strong>Join Optimization Component</strong> requires testing that validates the dynamic programming algorithm&#39;s correctness and handles the exponential complexity of join ordering. Our tests must verify that the optimizer finds truly optimal solutions for small problems where we can enumerate all possibilities, and produces reasonable solutions for larger problems where exhaustive verification isn&#39;t feasible.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm Aspect</th>\n<th>Test Approach</th>\n<th>Validation Strategy</th>\n<th>Success Criteria</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Optimal Solutions</td>\n<td>Small 3-4 table queries</td>\n<td>Compare against brute force enumeration</td>\n<td>Finds globally optimal join order</td>\n</tr>\n<tr>\n<td>Connectivity Detection</td>\n<td>Queries with disconnected tables</td>\n<td>Cross product identification</td>\n<td>Properly identifies and handles disconnected components</td>\n</tr>\n<tr>\n<td>Pruning Effectiveness</td>\n<td>Large table sets</td>\n<td>Measure search space reduction</td>\n<td>Significant reduction without losing optimal solutions</td>\n</tr>\n<tr>\n<td>Cost Monotonicity</td>\n<td>Incremental join additions</td>\n<td>Verify costs increase appropriately</td>\n<td>Larger joins have higher costs than subsets</td>\n</tr>\n<tr>\n<td>Memory Management</td>\n<td>Very large join problems</td>\n<td>Track memory usage during optimization</td>\n<td>Reasonable memory consumption with cleanup</td>\n</tr>\n</tbody></table>\n<p>The <strong>Physical Planning Component</strong> needs testing that validates operator selection logic and optimization rule application. Since this component makes final implementation choices, our tests must verify that selected physical operators are appropriate for the data characteristics and that optimization rules produce semantically equivalent but more efficient plans.</p>\n<table>\n<thead>\n<tr>\n<th>Physical Decision</th>\n<th>Test Scenarios</th>\n<th>Validation Approach</th>\n<th>Expected Outcome</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scan Method Selection</td>\n<td>Various selectivity levels</td>\n<td>Compare index vs sequential scan choices</td>\n<td>Correct method for each selectivity range</td>\n</tr>\n<tr>\n<td>Join Algorithm Choice</td>\n<td>Different input size combinations</td>\n<td>Validate hash vs nested loop selection</td>\n<td>Appropriate algorithm for size characteristics</td>\n</tr>\n<tr>\n<td>Predicate Pushdown</td>\n<td>Filters with join predicates</td>\n<td>Verify rule application correctness</td>\n<td>Filters moved to optimal positions</td>\n</tr>\n<tr>\n<td>Memory Estimation</td>\n<td>Complex operators requiring memory</td>\n<td>Resource requirement calculations</td>\n<td>Realistic memory usage estimates</td>\n</tr>\n<tr>\n<td>Plan Validation</td>\n<td>Generated physical plans</td>\n<td>Semantic correctness verification</td>\n<td>All plans are executable and correct</td>\n</tr>\n</tbody></table>\n<h3 id=\"plan-quality-validation\">Plan Quality Validation</h3>\n<p>Plan quality validation addresses the fundamental challenge of optimizer testing: verifying that generated execution plans are not just functionally correct but also reasonably efficient. Unlike traditional unit testing where we can assert exact expected outputs, plan quality requires heuristic evaluation and comparative analysis.</p>\n<p>Our quality validation strategy employs multiple complementary approaches. <strong>Comparative analysis</strong> evaluates whether the optimizer makes reasonable choices between clearly different alternatives. <strong>Regression testing</strong> ensures that optimizer changes don&#39;t degrade plan quality for established benchmark queries. <strong>Heuristic validation</strong> applies rule-of-thumb checks to identify obviously poor plans.</p>\n<p>The most effective quality validation technique is <strong>plan comparison testing</strong>, where we generate plans for queries with obvious optimal characteristics and verify the optimizer makes expected choices:</p>\n<table>\n<thead>\n<tr>\n<th>Query Pattern</th>\n<th>Expected Optimization</th>\n<th>Quality Metric</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>High Selectivity Filter</td>\n<td>Filter before join</td>\n<td>Filter position</td>\n<td>Verify filter appears below join in tree</td>\n</tr>\n<tr>\n<td>Large Table Join Small Table</td>\n<td>Smaller table as build side</td>\n<td>Join algorithm choice</td>\n<td>Hash join with correct build/probe assignment</td>\n</tr>\n<tr>\n<td>Available Index on Filter Column</td>\n<td>Index scan over sequential scan</td>\n<td>Access method selection</td>\n<td>Index scan chosen for high selectivity</td>\n</tr>\n<tr>\n<td>Star Schema Join</td>\n<td>Fact table joined last</td>\n<td>Join ordering</td>\n<td>Dimension tables joined first</td>\n</tr>\n<tr>\n<td>Range Query with Index</td>\n<td>Index range scan</td>\n<td>Scan method and cost</td>\n<td>Index scan with appropriate cost estimate</td>\n</tr>\n</tbody></table>\n<p><strong>Benchmark query validation</strong> provides systematic plan quality assessment using standard query patterns that represent common database workloads. Our benchmark suite includes queries from TPC-H and TPC-DS adapted for our optimizer&#39;s capabilities:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">sql</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\">-- Benchmark Query: Selective Join with Filter</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">SELECT</span><span style=\"color:#79B8FF\"> c</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">name</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">o</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">order_date</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">o</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">total</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">FROM</span><span style=\"color:#E1E4E8\"> customers c </span><span style=\"color:#F97583\">JOIN</span><span style=\"color:#E1E4E8\"> orders o </span><span style=\"color:#F97583\">ON</span><span style=\"color:#79B8FF\"> c</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">customer_id</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> o</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">customer_id</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">WHERE</span><span style=\"color:#79B8FF\"> c</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">region</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> 'ASIA'</span><span style=\"color:#F97583\"> AND</span><span style=\"color:#79B8FF\"> o</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#79B8FF\">order_date</span><span style=\"color:#F97583\"> >=</span><span style=\"color:#9ECBFF\"> '2023-01-01'</span><span style=\"color:#E1E4E8\">;</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- Expected Plan Characteristics:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 1. Filter on customers.region should appear before join</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 2. Date filter on orders should appear before join  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 3. Join algorithm should be hash join (typical case)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">-- 4. Smaller filtered customer set should be build side</span></span></code></pre></div>\n\n<p>Our benchmark validation framework automatically generates plans for these queries and applies quality heuristics:</p>\n<table>\n<thead>\n<tr>\n<th>Quality Heuristic</th>\n<th>Description</th>\n<th>Implementation</th>\n<th>Failure Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Filter Placement</td>\n<td>Filters should appear as low in tree as possible</td>\n<td>Count filters above joins</td>\n<td>More than 20% misplaced filters</td>\n</tr>\n<tr>\n<td>Join Algorithm Appropriateness</td>\n<td>Hash joins for large tables, nested loops for small</td>\n<td>Algorithm vs size validation</td>\n<td>More than 10% inappropriate choices</td>\n</tr>\n<tr>\n<td>Index Utilization</td>\n<td>Available indexes should be used for selective predicates</td>\n<td>Index usage rate measurement</td>\n<td>Less than 70% appropriate index usage</td>\n</tr>\n<tr>\n<td>Cost Reasonableness</td>\n<td>Plan costs should correlate with actual resource usage</td>\n<td>Cost vs execution time correlation</td>\n<td>Correlation coefficient below 0.6</td>\n</tr>\n<tr>\n<td>Plan Complexity</td>\n<td>Generated plans shouldn&#39;t be unnecessarily complex</td>\n<td>Operator count vs query complexity</td>\n<td>Plans more than 50% larger than minimal</td>\n</tr>\n</tbody></table>\n<p><strong>Regression quality testing</strong> maintains plan quality over time as the optimizer evolves. This testing approach maintains a database of previously optimized queries with their generated plans and quality metrics. When optimizer code changes, we re-optimize these queries and compare the new plans against historical baselines:</p>\n<table>\n<thead>\n<tr>\n<th>Regression Metric</th>\n<th>Measurement Method</th>\n<th>Acceptable Change</th>\n<th>Alert Threshold</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plan Cost Changes</td>\n<td>Compare <code>total_cost</code> between versions</td>\n<td>±10% cost variation</td>\n<td>More than 25% cost increase</td>\n</tr>\n<tr>\n<td>Algorithm Selection Changes</td>\n<td>Count physical operator type changes</td>\n<td>5% selection changes</td>\n<td>More than 15% selection changes</td>\n</tr>\n<tr>\n<td>Optimization Time</td>\n<td>Measure <code>optimize_query</code> execution time</td>\n<td>±20% time variation</td>\n<td>More than 50% time increase</td>\n</tr>\n<tr>\n<td>Plan Structure Changes</td>\n<td>Compare tree topology</td>\n<td>Minor structural changes</td>\n<td>Major topology differences</td>\n</tr>\n<tr>\n<td>Statistics Usage</td>\n<td>Track statistics access patterns</td>\n<td>Similar access patterns</td>\n<td>Significantly different patterns</td>\n</tr>\n</tbody></table>\n<p><strong>Stress testing for plan quality</strong> evaluates optimizer behavior under challenging conditions that might reveal quality degradation:</p>\n<ul>\n<li><strong>Large join queries</strong> with 8-12 tables test whether the optimizer maintains reasonable optimization times while finding good solutions</li>\n<li><strong>Highly correlated data</strong> tests whether independence assumptions in cost estimation lead to severely inaccurate plans</li>\n<li><strong>Missing or stale statistics</strong> validate that the optimizer gracefully degrades to reasonable heuristics</li>\n<li><strong>Extreme data skew</strong> tests whether uniform distribution assumptions cause poor physical operator choices</li>\n<li><strong>Resource constraints</strong> simulate low memory conditions to test whether the optimizer adapts appropriately</li>\n</ul>\n<h3 id=\"milestone-validation-checkpoints\">Milestone Validation Checkpoints</h3>\n<p>Milestone validation checkpoints provide structured verification points that ensure each implementation phase produces the expected functionality before proceeding to the next milestone. These checkpoints combine automated testing with manual verification to catch both functional bugs and design misunderstandings.</p>\n<p><strong>Milestone 1: Query Plan Representation</strong> validation focuses on verifying that the foundational plan tree infrastructure correctly supports all required operations:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Category</th>\n<th>Test Approach</th>\n<th>Success Criteria</th>\n<th>Manual Verification</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Tree Construction</td>\n<td>Create plans with various operator combinations</td>\n<td>All operator types supported, correct parent-child links</td>\n<td>Visual inspection of <code>pretty_print</code> output</td>\n</tr>\n<tr>\n<td>Tree Traversal</td>\n<td>Implement preorder and postorder traversals</td>\n<td>All nodes visited exactly once in correct order</td>\n<td>Trace traversal output matches expected sequence</td>\n</tr>\n<tr>\n<td>Cost Annotation</td>\n<td>Attach cost estimates to operator nodes</td>\n<td>Cost propagation works correctly bottom-up</td>\n<td>Verify leaf costs aggregate to root total</td>\n</tr>\n<tr>\n<td>Schema Propagation</td>\n<td>Pass column information through operator chain</td>\n<td>Output schemas correctly computed at each level</td>\n<td>Check schema transformations match operator semantics</td>\n</tr>\n<tr>\n<td>Plan Serialization</td>\n<td>Export plans to human-readable format</td>\n<td>Clear, indented tree representation with costs</td>\n<td>Review printed plans for readability and accuracy</td>\n</tr>\n</tbody></table>\n<p>The Milestone 1 checkpoint includes these specific automated tests:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_milestone_1_checkpoint</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive validation of plan representation functionality.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 1: Build complex plan tree</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scan_customers </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> OperatorNode(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        operator_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">OperatorType.</span><span style=\"color:#79B8FF\">SCAN</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        properties</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"table\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"customers\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        output_schema</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"customer_id\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"region\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filter_node </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> OperatorNode(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        operator_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">OperatorType.</span><span style=\"color:#79B8FF\">FILTER</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        properties</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"predicate\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"region = 'ASIA'\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        output_schema</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"customer_id\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"name\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"region\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filter_node.add_child(scan_customers)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Verify tree structure</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(filter_node.children) </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 1</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> filter_node.children[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> scan_customers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 2: Traverse tree and verify visit order</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    visited </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(filter_node.traverse_preorder())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> visited[</span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> filter_node</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> visited[</span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> scan_customers</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 3: Cost aggregation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    scan_customers.cost_estimate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CostEstimate(</span><span style=\"color:#FFAB70\">io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">50.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    filter_node.cost_estimate </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CostEstimate(</span><span style=\"color:#FFAB70\">io_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">0.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">cpu_cost</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">25.0</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    total_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> filter_node.calculate_subtree_cost()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#E1E4E8\"> total_cost.total_cost </span><span style=\"color:#F97583\">==</span><span style=\"color:#79B8FF\"> 175.0</span></span></code></pre></div>\n\n<p><strong>Milestone 2: Cost Estimation</strong> validation verifies that statistical cost models produce reasonable estimates under various data characteristics:</p>\n<table>\n<thead>\n<tr>\n<th>Validation Focus</th>\n<th>Test Data</th>\n<th>Expected Behavior</th>\n<th>Verification Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Selectivity Estimation</td>\n<td>Various filter predicates</td>\n<td>Selectivity correlates with predicate selectivity</td>\n<td>Compare estimates against known distributions</td>\n</tr>\n<tr>\n<td>Join Cardinality</td>\n<td>Different table size combinations</td>\n<td>Reasonable output size estimates</td>\n<td>Cross-validate with independence assumption</td>\n</tr>\n<tr>\n<td>I/O Cost Calculation</td>\n<td>Plans with different access patterns</td>\n<td>Costs reflect expected I/O operations</td>\n<td>Manual calculation verification</td>\n</tr>\n<tr>\n<td>CPU Cost Calculation</td>\n<td>Plans with different tuple processing</td>\n<td>Costs scale with processing complexity</td>\n<td>Verify cost scaling relationships</td>\n</tr>\n<tr>\n<td>Statistics Integration</td>\n<td>Real table statistics</td>\n<td>Cost estimates use available statistics</td>\n<td>Statistics access pattern validation</td>\n</tr>\n</tbody></table>\n<p>The Milestone 2 checkpoint validates cost estimation accuracy:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> test_milestone_2_checkpoint</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate cost estimation component accuracy and reasonableness.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 1: Filter selectivity estimation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    table_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> TableStatistics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        table_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"orders\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        row_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100000</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        column_stats</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            \"status\"</span><span style=\"color:#E1E4E8\">: ColumnStatistics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                column_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                distinct_values</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">5</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                most_common_values</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[(</span><span style=\"color:#9ECBFF\">\"SHIPPED\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.4</span><span style=\"color:#E1E4E8\">), (</span><span style=\"color:#9ECBFF\">\"PENDING\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#E1E4E8\">)]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    selectivity </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> estimator.estimate_filter_selectivity(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        table</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"orders\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">column</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"status\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">operator</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"=\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">value</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"SHIPPED\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> 0.35</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#E1E4E8\"> selectivity </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 0.45</span><span style=\"color:#6A737D\">  # Should be around 0.4</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Test 2: Join cardinality estimation</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    customer_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_table_stats(</span><span style=\"color:#FFAB70\">row_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">distinct_customers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">10000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    order_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> create_table_stats(</span><span style=\"color:#FFAB70\">row_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">100000</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">distinct_customers</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">8000</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    join_cardinality </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> estimator.estimate_join_cardinality(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        left_table</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"customers\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">left_column</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"customer_id\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">        right_table</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"orders\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">right_column</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"customer_id\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    )</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # Should be approximately orders.row_count (most customers have orders)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    assert</span><span style=\"color:#79B8FF\"> 80000</span><span style=\"color:#F97583\"> &#x3C;=</span><span style=\"color:#E1E4E8\"> join_cardinality </span><span style=\"color:#F97583\">&#x3C;=</span><span style=\"color:#79B8FF\"> 100000</span></span></code></pre></div>\n\n<p><strong>Milestone 3: Join Ordering</strong> validation ensures the dynamic programming algorithm finds optimal join orders for small problems and reasonable orders for larger problems:</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm Aspect</th>\n<th>Test Case</th>\n<th>Verification Approach</th>\n<th>Success Indicator</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Optimal Solutions</td>\n<td>3-table join with clear optimal order</td>\n<td>Compare against brute force enumeration</td>\n<td>Finds known optimal solution</td>\n</tr>\n<tr>\n<td>Connectivity Handling</td>\n<td>Disconnected table groups</td>\n<td>Cross product detection and handling</td>\n<td>Properly identifies disconnected components</td>\n</tr>\n<tr>\n<td>Cost-Based Selection</td>\n<td>Tables with different sizes and selectivities</td>\n<td>Join order reflects cost considerations</td>\n<td>Smaller filtered tables joined first</td>\n</tr>\n<tr>\n<td>Scalability</td>\n<td>6-8 table joins</td>\n<td>Reasonable optimization time</td>\n<td>Optimization completes within time limits</td>\n</tr>\n<tr>\n<td>Memoization</td>\n<td>Repeated subproblems</td>\n<td>Avoid redundant cost calculations</td>\n<td>Significant performance improvement</td>\n</tr>\n</tbody></table>\n<p><strong>Milestone 4: Physical Planning</strong> validation verifies that physical operator selection produces executable plans with appropriate performance characteristics:</p>\n<table>\n<thead>\n<tr>\n<th>Physical Choice</th>\n<th>Test Scenario</th>\n<th>Expected Selection</th>\n<th>Validation Method</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Sequential vs Index Scan</td>\n<td>Various filter selectivities</td>\n<td>Index for high selectivity, sequential for low</td>\n<td>Compare access method choices</td>\n</tr>\n<tr>\n<td>Hash vs Nested Loop Join</td>\n<td>Different input size ratios</td>\n<td>Hash for large inputs, nested loop for small</td>\n<td>Validate join algorithm selection</td>\n</tr>\n<tr>\n<td>Predicate Pushdown</td>\n<td>Filters with join predicates</td>\n<td>Filters moved below joins where possible</td>\n<td>Verify filter placement in final plan</td>\n</tr>\n<tr>\n<td>Memory Planning</td>\n<td>Memory-intensive operations</td>\n<td>Realistic memory requirements</td>\n<td>Check memory estimates are reasonable</td>\n</tr>\n<tr>\n<td>Plan Executability</td>\n<td>All generated plans</td>\n<td>Plans are semantically correct</td>\n<td>Attempt to execute plans on sample data</td>\n</tr>\n</tbody></table>\n<p>Each milestone checkpoint includes a <strong>manual verification component</strong> where developers inspect generated plans for reasonableness:</p>\n<ol>\n<li><strong>Plan Visualization</strong>: Use <code>pretty_print</code> to generate indented tree representations and verify they match expected structure</li>\n<li><strong>Cost Reasonableness</strong>: Compare cost estimates against intuition for relative operator expenses</li>\n<li><strong>Semantic Correctness</strong>: Verify that generated plans would produce correct query results</li>\n<li><strong>Performance Characteristics</strong>: Check that plans reflect appropriate performance trade-offs</li>\n</ol>\n<p>The milestone checkpoint process includes these <strong>debugging checkpoints</strong> to catch common implementation errors:</p>\n<table>\n<thead>\n<tr>\n<th>Common Issue</th>\n<th>Detection Method</th>\n<th>Diagnostic Approach</th>\n<th>Resolution Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Incorrect Tree Structure</td>\n<td>Tree traversal produces unexpected node order</td>\n<td>Print tree structure, verify parent-child links</td>\n<td>Fix tree construction logic</td>\n</tr>\n<tr>\n<td>Cost Calculation Errors</td>\n<td>Costs don&#39;t aggregate correctly</td>\n<td>Trace cost calculation step by step</td>\n<td>Debug cost propagation algorithm</td>\n</tr>\n<tr>\n<td>Statistics Access Failures</td>\n<td>Cost estimates seem unrealistic</td>\n<td>Check statistics lookup and usage</td>\n<td>Verify statistics integration</td>\n</tr>\n<tr>\n<td>Join Ordering Suboptimal</td>\n<td>Obviously poor join orders selected</td>\n<td>Compare against manual optimization</td>\n<td>Debug dynamic programming logic</td>\n</tr>\n<tr>\n<td>Physical Selection Inappropriate</td>\n<td>Wrong algorithms chosen for data characteristics</td>\n<td>Analyze selection criteria and thresholds</td>\n<td>Tune selection heuristics</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>Key Insight</strong>: Milestone checkpoints serve dual purposes - they validate implementation correctness and provide learning checkpoints where developers can verify their understanding before tackling the next complexity level. The manual verification components are particularly important for building intuition about query optimization principles.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>The testing strategy requires careful balance between comprehensive validation and practical implementation constraints. Our approach provides multiple levels of testing infrastructure to support both individual component development and integrated system validation.</p>\n<p><strong>A. Technology Recommendations</strong></p>\n<table>\n<thead>\n<tr>\n<th>Testing Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit Testing Framework</td>\n<td><code>unittest</code> (built-in Python)</td>\n<td><code>pytest</code> with fixtures and parametrization</td>\n</tr>\n<tr>\n<td>Test Data Generation</td>\n<td>Manual test data creation</td>\n<td><code>Faker</code> library for realistic data generation</td>\n</tr>\n<tr>\n<td>Plan Comparison</td>\n<td>String-based plan comparison</td>\n<td>AST-based structural plan comparison</td>\n</tr>\n<tr>\n<td>Performance Testing</td>\n<td>Simple timing measurements</td>\n<td><code>pytest-benchmark</code> for statistical timing</td>\n</tr>\n<tr>\n<td>Mock Statistics</td>\n<td>Hardcoded test statistics</td>\n<td><code>unittest.mock</code> for dynamic mocking</td>\n</tr>\n<tr>\n<td>Test Coverage</td>\n<td>Manual coverage assessment</td>\n<td><code>coverage.py</code> for automated coverage reporting</td>\n</tr>\n</tbody></table>\n<p><strong>B. Recommended File Structure</strong></p>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>project-root/\n  tests/\n    unit/\n      test_plan_representation.py     ← Milestone 1 component tests\n      test_cost_estimation.py         ← Milestone 2 component tests  \n      test_join_optimization.py       ← Milestone 3 component tests\n      test_physical_planning.py       ← Milestone 4 component tests\n    integration/\n      test_optimization_pipeline.py  ← End-to-end optimization tests\n      test_plan_quality.py           ← Plan quality validation tests\n    fixtures/\n      sample_queries.sql              ← Standard test queries\n      test_statistics.json            ← Sample table statistics\n      benchmark_plans.json            ← Expected plan structures\n    utils/\n      test_data_generator.py          ← Helper for creating test data\n      plan_comparison.py              ← Plan quality assessment utilities\n      query_builder.py                ← SQL query construction helpers\n  src/optimizer/\n    plan_representation.py\n    cost_estimation.py\n    join_optimization.py\n    physical_planning.py</code></pre></div>\n\n<p><strong>C. Testing Infrastructure Starter Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Test infrastructure for query optimizer validation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Provides utilities for creating test data, comparing plans, and validating quality.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List, Dict, Any, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> enum </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Enum</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestQuery</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Represents a test query with expected characteristics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sql: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    description: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_plan_properties: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    expected_cost_range: Tuple[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    tables_involved: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PlanQualityMetric</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">Enum</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Quality metrics for evaluating execution plans.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    FILTER_PLACEMENT</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"filter_placement\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    JOIN_ALGORITHM_APPROPRIATENESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"join_algorithm_appropriateness\"</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    INDEX_UTILIZATION</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"index_utilization\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    COST_REASONABLENESS</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"cost_reasonableness\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">    PLAN_COMPLEXITY</span><span style=\"color:#F97583\"> =</span><span style=\"color:#9ECBFF\"> \"plan_complexity\"</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestDataGenerator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Generates realistic test data for optimizer validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> create_table_statistics</span><span style=\"color:#E1E4E8\">(self, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              row_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                              column_definitions: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Dict]) -> TableStatistics:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create table statistics for testing cost estimation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        column_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> col_name, col_def </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> column_definitions.items():</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            column_stats[col_name] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> ColumnStatistics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                column_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">col_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                distinct_values</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">col_def.get(</span><span style=\"color:#9ECBFF\">'distinct_values'</span><span style=\"color:#E1E4E8\">, row_count </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                null_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">col_def.get(</span><span style=\"color:#9ECBFF\">'null_count'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                min_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">col_def.get(</span><span style=\"color:#9ECBFF\">'min_value'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                max_value</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">col_def.get(</span><span style=\"color:#9ECBFF\">'max_value'</span><span style=\"color:#E1E4E8\">, row_count),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                most_common_values</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">col_def.get(</span><span style=\"color:#9ECBFF\">'most_common_values'</span><span style=\"color:#E1E4E8\">, []),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                histogram_buckets</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">self</span><span style=\"color:#E1E4E8\">._create_uniform_histogram(</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    col_def.get(</span><span style=\"color:#9ECBFF\">'min_value'</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">1</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                    col_def.get(</span><span style=\"color:#9ECBFF\">'max_value'</span><span style=\"color:#E1E4E8\">, row_count),</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">                    10</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> TableStatistics(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            table_name</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">table_name,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            row_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">row_count,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            page_count</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">row_count </span><span style=\"color:#F97583\">//</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Assume 100 rows per page</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            column_stats</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">column_stats,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            last_updated</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">datetime.now(),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            sample_rate</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            index_statistics</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">            clustering_factor</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">1.0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        )</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_uniform_histogram</span><span style=\"color:#E1E4E8\">(self, min_val: Any, max_val: Any, bucket_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> List[HistogramBucket]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create uniform distribution histogram for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement uniform histogram bucket creation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Hint: divide range into equal buckets with equal row counts</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PlanComparisonUtility</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Utilities for comparing and validating execution plans.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_plan_structure</span><span style=\"color:#E1E4E8\">(self, plan1: ExecutionPlan, plan2: ExecutionPlan) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compare structural characteristics of two execution plans.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement plan structure comparison</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Compare operator types, tree topology, join order</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_plan_quality</span><span style=\"color:#E1E4E8\">(self, plan: ExecutionPlan, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            query: TestQuery,</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                            metrics: List[PlanQualityMetric]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate plan quality using specified metrics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        quality_scores </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> metric </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> metrics:</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            if</span><span style=\"color:#E1E4E8\"> metric </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> PlanQualityMetric.</span><span style=\"color:#79B8FF\">FILTER_PLACEMENT</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                quality_scores[metric.value] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._evaluate_filter_placement(plan)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            elif</span><span style=\"color:#E1E4E8\"> metric </span><span style=\"color:#F97583\">==</span><span style=\"color:#E1E4E8\"> PlanQualityMetric.</span><span style=\"color:#79B8FF\">JOIN_ALGORITHM_APPROPRIATENESS</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                quality_scores[metric.value] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._evaluate_join_algorithms(plan)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement other quality metric evaluations</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> quality_scores</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _evaluate_filter_placement</span><span style=\"color:#E1E4E8\">(self, plan: ExecutionPlan) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Evaluate whether filters are placed optimally in the plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze plan tree to find filters above joins</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Return score 0.0-1.0 where 1.0 means optimal filter placement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>D. Core Testing Skeleton Code</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Unit tests for cost estimation component - Milestone 2 validation.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> unittest</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> unittest.mock </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Mock, patch</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> optimizer.cost_estimation </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimator, TableStatistics, ColumnStatistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestCostEstimation</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">unittest</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TestCase</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Test cost estimation accuracy and edge case handling.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> setUp</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Set up test fixtures with sample statistics.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_estimator </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CostEstimator()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.sample_table_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._create_sample_statistics()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_selectivity_estimation_uniform_distribution</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test selectivity estimation for uniformly distributed data.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create table statistics with uniform distribution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Test equality predicate selectivity (should be 1/distinct_values)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test range predicate selectivity (should be proportional to range)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify selectivity values are between 0.0 and 1.0</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test edge cases (non-existent values, null comparisons)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_join_cardinality_estimation</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test join output size estimation for various scenarios.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Set up two tables with known foreign key relationship</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Estimate join cardinality using column statistics  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Verify result is reasonable given input table sizes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test many-to-many join cardinality estimation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test cross product detection (no join predicates)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_cost_model_components</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test I/O and CPU cost calculation components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Create plan with known I/O characteristics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Calculate expected I/O cost using page counts</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Calculate expected CPU cost using tuple processing</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify total cost combines I/O and CPU appropriately</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test cost scaling with different input sizes</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_missing_statistics_handling</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test behavior when table statistics are missing or incomplete.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test cost estimation with empty statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify fallback to conservative estimates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test partial statistics (some columns missing)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify error handling for invalid statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test statistics staleness detection and handling</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> _create_sample_statistics</span><span style=\"color:#E1E4E8\">(self) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, TableStatistics]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Create realistic sample statistics for testing.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create statistics for customers, orders, products tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # Include various data distributions and characteristics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> TestPlanQuality</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">unittest</span><span style=\"color:#E1E4E8\">.</span><span style=\"color:#B392F0\">TestCase</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Integration tests for plan quality validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_benchmark_query_optimization</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test optimizer performance on standard benchmark queries.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        benchmark_queries </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> [</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            TestQuery(</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                sql</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"SELECT * FROM customers c JOIN orders o ON c.id = o.customer_id WHERE c.region = 'ASIA'\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                description</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Selective filter with join\"</span><span style=\"color:#E1E4E8\">,</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                expected_plan_properties</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{</span><span style=\"color:#9ECBFF\">\"filter_before_join\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"join_algorithm\"</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#9ECBFF\">\"hash\"</span><span style=\"color:#E1E4E8\">},</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                expected_cost_range</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#79B8FF\">1000.0</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">5000.0</span><span style=\"color:#E1E4E8\">),</span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                tables_involved</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"customers\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"orders\"</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            ),</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add more benchmark queries covering different patterns</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        ]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        for</span><span style=\"color:#E1E4E8\"> query </span><span style=\"color:#F97583\">in</span><span style=\"color:#E1E4E8\"> benchmark_queries:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Parse and optimize the query</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate plan structure matches expectations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check cost estimate is within expected range</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Verify plan quality metrics meet thresholds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Compare against baseline plans if available</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> test_plan_regression_validation</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Test that optimizer changes don't degrade plan quality.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Load historical plan database</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Re-optimize all historical queries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Compare new plans against historical baselines</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Flag significant cost increases or algorithm changes</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Update baselines for legitimate improvements</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>E. Milestone Checkpoint Implementation</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">Milestone validation checkpoints for incremental verification.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MilestoneValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates completion of each implementation milestone.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_1_plan_representation</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate Milestone 1: Query plan representation is working correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"=== Milestone 1 Validation: Plan Representation ===\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        try</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test 1: Create complex plan tree</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Test 1: Building complex plan tree...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            root_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> self</span><span style=\"color:#E1E4E8\">._build_sample_plan_tree()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> root_plan.root </span><span style=\"color:#F97583\">is</span><span style=\"color:#F97583\"> not</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Plan tree construction successful\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test 2: Tree traversal</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Test 2: Testing tree traversal...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            nodes </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> list</span><span style=\"color:#E1E4E8\">(root_plan.root.traverse_preorder())</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(nodes) </span><span style=\"color:#F97583\">>=</span><span style=\"color:#79B8FF\"> 3</span><span style=\"color:#6A737D\">  # Should have multiple operators</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"✓ Traversed </span><span style=\"color:#79B8FF\">{len</span><span style=\"color:#E1E4E8\">(nodes)</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\"> nodes successfully\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test 3: Cost calculation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Test 3: Testing cost calculation...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            total_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> root_plan.root.calculate_subtree_cost()</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#E1E4E8\"> total_cost.total_cost </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"✓ Total plan cost: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">total_cost.total_cost</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">            # Test 4: Pretty printing</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"Test 4: Testing plan visualization...\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            plan_text </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> root_plan.root.pretty_print(</span><span style=\"color:#FFAB70\">show_costs</span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\">True</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            assert</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(plan_text) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#6A737D\">  # Should be substantial output</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"✓ Plan pretty printing successful\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">Sample plan output:\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(plan_text[:</span><span style=\"color:#79B8FF\">200</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">+</span><span style=\"color:#9ECBFF\"> \"...\"</span><span style=\"color:#F97583\"> if</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(plan_text) </span><span style=\"color:#F97583\">></span><span style=\"color:#79B8FF\"> 200</span><span style=\"color:#F97583\"> else</span><span style=\"color:#E1E4E8\"> plan_text)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">✅ Milestone 1 validation PASSED\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> True</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">            </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        except</span><span style=\"color:#79B8FF\"> Exception</span><span style=\"color:#F97583\"> as</span><span style=\"color:#E1E4E8\"> e:</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">            print</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#F97583\">f</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#79B8FF\">\\n</span><span style=\"color:#9ECBFF\">❌ Milestone 1 validation FAILED: </span><span style=\"color:#79B8FF\">{</span><span style=\"color:#E1E4E8\">e</span><span style=\"color:#79B8FF\">}</span><span style=\"color:#9ECBFF\">\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">            return</span><span style=\"color:#79B8FF\"> False</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_2_cost_estimation</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate Milestone 2: Cost estimation is producing reasonable results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test selectivity estimation with known data distributions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Validate join cardinality estimates are reasonable</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check I/O and CPU cost calculations</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Test behavior with missing statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Verify cost estimates correlate with query complexity</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_3_join_optimization</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate Milestone 3: Join ordering optimization is working.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test optimal join order for small query (3-4 tables)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify connectivity detection and cross product handling</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Test dynamic programming pruning effectiveness</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Check optimization time is reasonable for larger queries</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Validate cost-based join order selection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_milestone_4_physical_planning</span><span style=\"color:#E1E4E8\">(self) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate Milestone 4: Physical planning produces executable plans.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 1: Test scan method selection (sequential vs index)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 2: Verify join algorithm selection logic</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 3: Check predicate pushdown rule application</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 4: Validate memory requirement estimation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\"> 5: Test final plan executability</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p><strong>F. Debugging Tips for Testing</strong></p>\n<table>\n<thead>\n<tr>\n<th>Test Failure Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnosis Method</th>\n<th>Fix Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Unit tests pass but integration fails</td>\n<td>Component interface mismatch</td>\n<td>Check data flow between components</td>\n<td>Verify interface contracts match</td>\n</tr>\n<tr>\n<td>Cost estimates are wildly inaccurate</td>\n<td>Statistics not loaded or formula error</td>\n<td>Print intermediate calculations</td>\n<td>Debug statistics access and formulas</td>\n</tr>\n<tr>\n<td>Plans have obviously wrong structure</td>\n<td>Tree construction logic error</td>\n<td>Use <code>pretty_print</code> to visualize</td>\n<td>Fix tree building algorithms</td>\n</tr>\n<tr>\n<td>Optimization takes too long</td>\n<td>Missing pruning or infinite loops</td>\n<td>Profile optimization phases</td>\n<td>Add timeout and pruning logic</td>\n</tr>\n<tr>\n<td>Quality validation fails</td>\n<td>Heuristics too strict or optimizer bug</td>\n<td>Compare against manual optimization</td>\n<td>Adjust quality thresholds or fix bugs</td>\n</tr>\n</tbody></table>\n<p><strong>G. Language-Specific Testing Hints</strong></p>\n<ul>\n<li>Use <code>pytest.parametrize</code> to test cost estimation with multiple data distributions</li>\n<li>Use <code>unittest.mock.patch</code> to simulate different statistics scenarios without creating large test databases  </li>\n<li>Use <code>pytest.benchmark</code> to ensure optimization time stays within acceptable bounds</li>\n<li>Use <code>pytest.fixture</code> to share expensive test data setup across multiple test functions</li>\n<li>Use <code>coverage.py</code> to ensure test coverage includes error handling paths</li>\n<li>Use <code>pytest.raises</code> to test that invalid queries produce appropriate exceptions</li>\n</ul>\n<h2 id=\"debugging-guide\">Debugging Guide</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All Milestones 1-4 - provides systematic troubleshooting strategies for diagnosing and fixing common issues that arise during query optimizer implementation, from plan tree construction through cost estimation and physical operator selection.</p>\n</blockquote>\n<h3 id=\"mental-model-medical-diagnostics\">Mental Model: Medical Diagnostics</h3>\n<p>Think of debugging a query optimizer like being a doctor diagnosing a complex medical condition. Just as a doctor uses symptoms to narrow down potential causes, then applies specific tests to confirm a diagnosis, query optimizer debugging requires systematic observation of symptoms (wrong plans, poor performance, crashes), hypothesis formation about root causes (cost estimation errors, join ordering bugs, tree construction issues), and targeted diagnostic tests to isolate the problem. Like medical diagnostics, optimizer debugging benefits from understanding the &quot;anatomy&quot; of the system - which components are responsible for what functions, how they interact, and what can go wrong at each stage.</p>\n<p>The key insight is that optimizer bugs often manifest as seemingly unrelated symptoms far from their actual cause. A cost estimation error in the statistics component might surface as a poor join order selection, just as a heart problem might manifest as shortness of breath. Effective debugging requires tracing symptoms back to their root causes through the complex web of component interactions.</p>\n<h3 id=\"cost-estimation-debugging\">Cost Estimation Debugging</h3>\n<p>Cost estimation forms the foundation of all optimization decisions, making bugs in this component particularly insidious. Cost estimation errors propagate through the entire optimization pipeline, causing the optimizer to consistently make poor choices even when all other components work correctly. The challenge lies in distinguishing between symptoms (bad plans) and causes (incorrect cost calculations, stale statistics, or flawed estimation models).</p>\n<blockquote>\n<p><strong>Decision: Layered Debugging Approach for Cost Estimation</strong></p>\n<ul>\n<li><strong>Context</strong>: Cost estimation bugs can occur at multiple levels - data collection, statistical modeling, selectivity calculation, or cost model application</li>\n<li><strong>Options Considered</strong>: Single comprehensive test vs layered component testing vs black-box plan quality assessment</li>\n<li><strong>Decision</strong>: Implement layered debugging starting from raw statistics validation up through final cost estimates</li>\n<li><strong>Rationale</strong>: Allows precise isolation of the faulty layer without being misled by cascading effects from upstream errors</li>\n<li><strong>Consequences</strong>: Requires more debugging infrastructure but provides faster root cause identification</li>\n</ul>\n</blockquote>\n<p>The most effective strategy involves building debugging capabilities into each layer of the cost estimation pipeline. This approach allows precise identification of where estimation errors originate and prevents wild goose chases caused by cascading effects.</p>\n<h4 id=\"statistics-validation-and-debugging\">Statistics Validation and Debugging</h4>\n<p>Statistics form the foundation of all cost estimation, making their accuracy crucial for optimizer correctness. The most common statistics-related bugs involve stale data, incorrect collection procedures, or missing statistical information for newly created tables or columns.</p>\n<table>\n<thead>\n<tr>\n<th>Debugging Method</th>\n<th>Purpose</th>\n<th>Implementation</th>\n<th>Expected Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Statistics Freshness Check</td>\n<td>Verify statistics currency</td>\n<td>Compare <code>last_updated</code> timestamps against table modification times</td>\n<td>Warning when statistics older than data changes</td>\n</tr>\n<tr>\n<td>Row Count Validation</td>\n<td>Confirm basic table statistics</td>\n<td>Execute <code>SELECT COUNT(*)</code> and compare with stored <code>row_count</code></td>\n<td>Exact match or explanation for discrepancy</td>\n</tr>\n<tr>\n<td>Distinct Value Verification</td>\n<td>Validate column cardinality</td>\n<td>Execute <code>SELECT COUNT(DISTINCT column)</code> against <code>distinct_values</code></td>\n<td>Match within acceptable error margin</td>\n</tr>\n<tr>\n<td>Histogram Bucket Inspection</td>\n<td>Examine value distribution</td>\n<td>Display histogram buckets with actual value ranges and frequencies</td>\n<td>Reasonable distribution matching data characteristics</td>\n</tr>\n<tr>\n<td>Null Count Accuracy</td>\n<td>Check missing value statistics</td>\n<td>Execute <code>SELECT COUNT(*) WHERE column IS NULL</code> against <code>null_count</code></td>\n<td>Exact match for null statistics</td>\n</tr>\n</tbody></table>\n<p>Statistics debugging often reveals systematic collection errors that affect multiple tables or columns. For example, a bug in the statistics collection sampling logic might consistently underestimate distinct values across all columns, leading to systematically poor join cardinality estimates.</p>\n<h4 id=\"selectivity-estimation-diagnosis\">Selectivity Estimation Diagnosis</h4>\n<p>Selectivity estimation translates statistical information into predictions about how many rows survive filtering operations. Bugs in this component often involve incorrect mathematical models, edge case handling failures, or misunderstanding of predicate semantics.</p>\n<table>\n<thead>\n<tr>\n<th>Symptom</th>\n<th>Likely Cause</th>\n<th>Diagnostic Test</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>All filters estimated at 50% selectivity</td>\n<td>Default selectivity fallback being used</td>\n<td>Check if column statistics exist for filter columns</td>\n<td>Collect missing statistics or fix collection bug</td>\n</tr>\n<tr>\n<td>Extremely low selectivity for equality predicates</td>\n<td>Incorrect distinct value count</td>\n<td>Manually verify <code>SELECT COUNT(DISTINCT column)</code></td>\n<td>Recollect statistics or fix calculation logic</td>\n</tr>\n<tr>\n<td>Selectivity &gt; 1.0 or &lt; 0.0</td>\n<td>Mathematical error in estimation formula</td>\n<td>Add bounds checking to selectivity calculations</td>\n<td>Implement proper range validation</td>\n</tr>\n<tr>\n<td>Identical selectivity for different operators</td>\n<td>Operator type not considered in estimation</td>\n<td>Verify operator-specific selectivity logic</td>\n<td>Implement different models for =, &lt;, &gt;, LIKE, etc.</td>\n</tr>\n<tr>\n<td>Poor estimates for range predicates</td>\n<td>Uniform distribution assumption incorrect</td>\n<td>Examine actual data distribution vs histogram</td>\n<td>Improve histogram construction or use better model</td>\n</tr>\n</tbody></table>\n<p>The <code>estimate_filter_selectivity</code> method serves as the primary debugging entry point for selectivity issues. Adding comprehensive logging to this method helps track how statistical inputs transform into selectivity estimates.</p>\n<p>A particularly effective debugging technique involves creating &quot;selectivity test queries&quot; that apply various predicates to well-understood test datasets. By comparing estimated vs actual selectivity for these known cases, developers can quickly identify systematic biases in the estimation logic.</p>\n<h4 id=\"cardinality-estimation-troubleshooting\">Cardinality Estimation Troubleshooting</h4>\n<p>Join cardinality estimation represents one of the most challenging aspects of cost-based optimization. Errors in this area often result from incorrect assumptions about data correlation, inadequate statistical models, or bugs in the mathematical formulas used to combine statistics from multiple tables.</p>\n<table>\n<thead>\n<tr>\n<th>Problem Pattern</th>\n<th>Root Cause Analysis</th>\n<th>Debugging Approach</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Join estimates consistently too high</td>\n<td>Assuming independence when correlation exists</td>\n<td>Compare estimated vs actual join results on sample data</td>\n<td>Implement correlation detection or use more conservative estimates</td>\n</tr>\n<tr>\n<td>Cross product detection failure</td>\n<td>Missing predicate analysis</td>\n<td>Verify join predicate identification logic</td>\n<td>Improve predicate parsing and connectivity analysis</td>\n</tr>\n<tr>\n<td>Estimates wildly incorrect for complex joins</td>\n<td>Cascading estimation errors</td>\n<td>Test single join estimates before multi-join scenarios</td>\n<td>Fix base case estimation before tackling complex queries</td>\n</tr>\n<tr>\n<td>Self-join cardinality errors</td>\n<td>Special case handling missing</td>\n<td>Create test cases with self-joins on various key types</td>\n<td>Implement self-join specific estimation logic</td>\n</tr>\n<tr>\n<td>Foreign key join overestimation</td>\n<td>Not recognizing referential integrity</td>\n<td>Check for foreign key constraints in schema metadata</td>\n<td>Use constraint information to improve cardinality bounds</td>\n</tr>\n</tbody></table>\n<p>The <code>estimate_join_cardinality</code> method requires extensive logging to track how input statistics combine into final estimates. Key debugging outputs include the distinct value counts from both join columns, the independence assumption factor, and any correlation adjustments applied.</p>\n<p>Building a &quot;cardinality verification suite&quot; helps catch regression errors when modifying estimation logic. This suite should include queries with known join characteristics - primary key/foreign key joins, many-to-many relationships, self-joins, and various selectivity combinations.</p>\n<h4 id=\"cost-model-validation\">Cost Model Validation</h4>\n<p>The cost model translates cardinality estimates into execution time predictions by combining I/O costs, CPU costs, and memory costs. Bugs in cost model implementation often involve incorrect coefficient values, missing cost factors, or errors in combining different cost components.</p>\n<table>\n<thead>\n<tr>\n<th>Cost Component</th>\n<th>Common Bugs</th>\n<th>Detection Method</th>\n<th>Debugging Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>I/O Cost Calculation</td>\n<td>Wrong page size assumptions</td>\n<td>Compare estimated pages vs actual table/index pages</td>\n<td>Page count breakdown by operator type</td>\n</tr>\n<tr>\n<td>CPU Cost Modeling</td>\n<td>Incorrect processing rates</td>\n<td>Benchmark tuple processing costs on target hardware</td>\n<td>CPU cycles per tuple by operation</td>\n</tr>\n<tr>\n<td>Memory Cost Integration</td>\n<td>Missing memory allocation costs</td>\n<td>Monitor actual memory usage during execution</td>\n<td>Memory allocation patterns by operator</td>\n</tr>\n<tr>\n<td>Sequential vs Random I/O</td>\n<td>Using wrong I/O cost multipliers</td>\n<td>Analyze access patterns for scan operations</td>\n<td>I/O pattern classification and cost factors</td>\n</tr>\n<tr>\n<td>Cache Effects</td>\n<td>Ignoring buffer pool hit rates</td>\n<td>Compare cold vs warm execution costs</td>\n<td>Cache hit rate assumptions and impacts</td>\n</tr>\n</tbody></table>\n<p>The <code>calculate_total_cost</code> method should provide detailed cost breakdowns showing how individual operator costs combine into total plan costs. This granular visibility helps identify which cost components dominate and whether those estimates seem reasonable.</p>\n<p>Effective cost model debugging often involves comparing optimizer estimates against actual execution metrics. While building full execution timing into the optimizer may be impractical, even rough benchmarks help validate that cost model coefficients reflect reality.</p>\n<p>⚠️ <strong>Pitfall: Statistics Staleness Ignored</strong>\nMany developers forget to implement statistics staleness detection, leading to optimization decisions based on obsolete information. After significant data modifications, outdated statistics can cause dramatic misestimation. Always check <code>last_updated</code> timestamps and implement automatic statistics refresh triggers for tables with significant modification activity.</p>\n<p>⚠️ <strong>Pitfall: Uniform Distribution Assumption</strong>\nAssuming uniform data distribution when calculating selectivity often produces wildly incorrect estimates for skewed data. Real-world data frequently exhibits significant skew, with some values appearing much more frequently than others. Implement histogram-based estimation or at least collect most common values to handle skewed distributions properly.</p>\n<h3 id=\"join-ordering-debugging\">Join Ordering Debugging</h3>\n<p>Join ordering optimization represents the algorithmic heart of query optimization, where dynamic programming algorithms explore exponentially large search spaces to find optimal execution strategies. Bugs in join ordering typically fall into three categories: algorithmic implementation errors, cost comparison mistakes, and search space management problems.</p>\n<p>The complexity of join ordering makes systematic debugging essential. Unlike simple component bugs that produce obvious failures, join ordering bugs often manifest as subtly suboptimal plans that still execute correctly but perform poorly. This makes them particularly dangerous in production systems.</p>\n<h4 id=\"dynamic-programming-algorithm-verification\">Dynamic Programming Algorithm Verification</h4>\n<p>The dynamic programming algorithm for join ordering builds optimal plans bottom-up by combining optimal subplans. Implementation bugs often involve incorrect subset enumeration, flawed memoization, or errors in the plan combination logic.</p>\n<table>\n<thead>\n<tr>\n<th>Algorithm Phase</th>\n<th>Verification Method</th>\n<th>Expected Behavior</th>\n<th>Debugging Technique</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Base Case Initialization</td>\n<td>Verify single-table plans created correctly</td>\n<td>One plan per table with table scan operator</td>\n<td>Log all base case plans with costs</td>\n</tr>\n<tr>\n<td>Subset Enumeration</td>\n<td>Check all valid subsets generated</td>\n<td>Exponential growth: 2^n total subsets</td>\n<td>Count subsets by size, verify completeness</td>\n</tr>\n<tr>\n<td>Plan Combination</td>\n<td>Validate join plan construction</td>\n<td>Left and right subplans properly combined</td>\n<td>Trace plan tree construction step by step</td>\n</tr>\n<tr>\n<td>Memoization Logic</td>\n<td>Ensure optimal plans stored and retrieved</td>\n<td>Same subset never recomputed</td>\n<td>Monitor cache hit rates during optimization</td>\n</tr>\n<tr>\n<td>Pruning Decisions</td>\n<td>Verify search space reduction working</td>\n<td>Suboptimal plans eliminated correctly</td>\n<td>Log pruned plans with reasons</td>\n</tr>\n</tbody></table>\n<p>The <code>_enumerate_subsets_by_size</code> method serves as a critical debugging point. Incorrect subset enumeration leads to missing optimal solutions or redundant computation. Adding comprehensive logging to track which table combinations are considered helps identify gaps in the enumeration logic.</p>\n<p>A particularly effective debugging technique involves manually tracing the algorithm execution for small query examples. With 3-4 tables, the search space remains manageable for human analysis, allowing verification that the algorithm considers all valid join orders and correctly identifies the optimal solution.</p>\n<h4 id=\"cost-comparison-and-plan-selection\">Cost Comparison and Plan Selection</h4>\n<p>Dynamic programming relies on accurate cost comparisons to identify optimal subplans. Bugs in cost comparison logic can cause the algorithm to select suboptimal intermediate results, leading to globally suboptimal final plans.</p>\n<table>\n<thead>\n<tr>\n<th>Comparison Issue</th>\n<th>Symptom</th>\n<th>Diagnostic Approach</th>\n<th>Resolution</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Inconsistent cost calculations</td>\n<td>Same plan gets different costs</td>\n<td>Verify cost calculation determinism</td>\n<td>Fix non-deterministic cost components</td>\n</tr>\n<tr>\n<td>Cost component weighting errors</td>\n<td>I/O costs dominate CPU costs inappropriately</td>\n<td>Analyze cost component ratios</td>\n<td>Rebalance cost model coefficients</td>\n</tr>\n<tr>\n<td>Floating point precision issues</td>\n<td>Tiny cost differences cause plan instability</td>\n<td>Use cost comparison tolerances</td>\n<td>Implement epsilon-based cost comparison</td>\n</tr>\n<tr>\n<td>Missing cost factors</td>\n<td>Important costs ignored in comparison</td>\n<td>Compare estimated vs actual execution costs</td>\n<td>Add missing cost components</td>\n</tr>\n<tr>\n<td>Plan equivalence detection</td>\n<td>Functionally identical plans treated as different</td>\n<td>Check for redundant plan generation</td>\n<td>Implement plan canonicalization</td>\n</tr>\n</tbody></table>\n<p>The <code>_evaluate_join_cost</code> method requires extensive instrumentation to debug cost comparison issues. Key debugging outputs include detailed cost breakdowns for each join algorithm option, input cardinality estimates, and the final cost comparison results.</p>\n<p>Building a &quot;cost comparison test suite&quot; helps validate that cost comparisons produce consistent, reasonable results. This suite should include scenarios where the optimal choice is known or easily verified, such as comparing hash joins vs nested loop joins with dramatically different input sizes.</p>\n<h4 id=\"search-space-management-and-pruning\">Search Space Management and Pruning</h4>\n<p>Effective join ordering optimization requires aggressive search space pruning to handle queries with many tables. However, overly aggressive pruning can eliminate optimal solutions, while insufficient pruning leads to unacceptable optimization times.</p>\n<table>\n<thead>\n<tr>\n<th>Pruning Strategy</th>\n<th>Purpose</th>\n<th>Implementation Challenge</th>\n<th>Debugging Approach</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Cross Product Elimination</td>\n<td>Remove joins without predicates</td>\n<td>Identifying all possible join predicates</td>\n<td>Verify connectivity graph construction</td>\n</tr>\n<tr>\n<td>Cost-Based Pruning</td>\n<td>Eliminate clearly inferior plans</td>\n<td>Setting appropriate pruning thresholds</td>\n<td>Monitor pruned vs retained plan ratios</td>\n</tr>\n<tr>\n<td>Plan Count Limits</td>\n<td>Bound memory usage during optimization</td>\n<td>Selecting plans to retain when limit exceeded</td>\n<td>Analyze quality of retained vs discarded plans</td>\n</tr>\n<tr>\n<td>Timeout Handling</td>\n<td>Prevent excessive optimization time</td>\n<td>Graceful degradation when timeout reached</td>\n<td>Verify fallback plan quality</td>\n</tr>\n<tr>\n<td>Heuristic Shortcuts</td>\n<td>Apply rules to skip expensive enumeration</td>\n<td>Ensuring shortcuts don&#39;t miss optimal plans</td>\n<td>Compare heuristic vs full enumeration results</td>\n</tr>\n</tbody></table>\n<p>The <code>_check_connectivity</code> method plays a crucial role in cross product elimination. Bugs in connectivity analysis can either allow cross products (performance disasters) or eliminate valid join paths (correctness errors). Comprehensive testing should verify that all legitimate join predicates are recognized while cross products are properly identified and handled.</p>\n<p>Implementing optimization time budgets helps debug performance issues in join ordering. By tracking how much time each optimization phase consumes, developers can identify algorithmic bottlenecks and verify that pruning strategies effectively manage search complexity.</p>\n<h4 id=\"multi-table-query-analysis\">Multi-Table Query Analysis</h4>\n<p>Complex queries with many tables expose edge cases and performance issues that may not appear in simple test cases. Systematic analysis of multi-table query optimization helps identify scalability problems and algorithmic bugs.</p>\n<table>\n<thead>\n<tr>\n<th>Query Complexity</th>\n<th>Expected Behavior</th>\n<th>Common Issues</th>\n<th>Debugging Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>3-4 tables</td>\n<td>Full enumeration feasible</td>\n<td>Algorithm correctness bugs</td>\n<td>Manual trace and verification</td>\n</tr>\n<tr>\n<td>5-8 tables</td>\n<td>Pruning becomes important</td>\n<td>Pruning too aggressive or insufficient</td>\n<td>Monitor search space reduction</td>\n</tr>\n<tr>\n<td>9-12 tables</td>\n<td>Heuristics may be needed</td>\n<td>Optimization time explosion</td>\n<td>Implement timeout and fallback</td>\n</tr>\n<tr>\n<td>13+ tables</td>\n<td>Must use approximation</td>\n<td>Algorithm fails to complete</td>\n<td>Switch to heuristic-only approach</td>\n</tr>\n</tbody></table>\n<p>⚠️ <strong>Pitfall: Missing Predicate Detection</strong>\nFailing to properly identify all join predicates leads to cross product generation or missing valid join paths. Complex WHERE clauses with multiple conditions, subqueries, or function calls can obscure join predicates from simple parsing logic. Implement comprehensive predicate analysis that handles various SQL constructs correctly.</p>\n<p>⚠️ <strong>Pitfall: Memoization Key Errors</strong>\nUsing incorrect keys for memoizing optimal subplans causes the algorithm to miss cached results or retrieve wrong plans. The memoization key must uniquely identify the table subset and any relevant optimization context. Ensure that equivalent table sets map to identical keys regardless of table ordering.</p>\n<h3 id=\"plan-generation-debugging\">Plan Generation Debugging</h3>\n<p>Plan generation transforms logical query specifications into executable operator trees through a complex process involving tree construction, operator selection, schema propagation, and cost accumulation. Bugs in plan generation often create subtle correctness issues that may not surface until query execution, making thorough testing and debugging crucial.</p>\n<p>Plan generation debugging requires understanding both the tree construction algorithms and the semantic correctness requirements for query execution. Unlike optimization bugs that affect performance, plan generation bugs can produce incorrect results, making them particularly serious.</p>\n<h4 id=\"tree-construction-and-structure-validation\">Tree Construction and Structure Validation</h4>\n<p>Query plan trees must satisfy strict structural requirements to ensure correct execution semantics. Tree construction bugs can create invalid parent-child relationships, missing operators, or incorrect tree topology that leads to execution failures.</p>\n<table>\n<thead>\n<tr>\n<th>Structural Requirement</th>\n<th>Validation Method</th>\n<th>Common Violations</th>\n<th>Fix Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parent-child consistency</td>\n<td>Verify bidirectional links</td>\n<td>Orphaned nodes, circular references</td>\n<td>Implement defensive tree construction</td>\n</tr>\n<tr>\n<td>Operator compatibility</td>\n<td>Check input/output schema matching</td>\n<td>Schema mismatches between operators</td>\n<td>Add schema validation at tree construction</td>\n</tr>\n<tr>\n<td>Complete operator coverage</td>\n<td>Ensure all query clauses represented</td>\n<td>Missing filter, join, or projection operators</td>\n<td>Implement comprehensive query analysis</td>\n</tr>\n<tr>\n<td>Proper tree depth</td>\n<td>Validate execution order dependencies</td>\n<td>Operators in wrong tree positions</td>\n<td>Fix operator precedence logic</td>\n</tr>\n<tr>\n<td>Schema propagation</td>\n<td>Track column flow through operators</td>\n<td>Missing or incorrect output schemas</td>\n<td>Implement rigorous schema inference</td>\n</tr>\n</tbody></table>\n<p>The <code>add_child</code> method serves as a critical validation point for tree construction. Adding defensive checks that verify schema compatibility and operator constraints at tree construction time catches many structural bugs before they propagate to execution.</p>\n<p>Implementing a &quot;tree validator&quot; that performs comprehensive structural checks helps catch construction bugs early. This validator should traverse the entire tree, checking operator compatibility, schema consistency, and execution semantics at each node.</p>\n<h4 id=\"operator-selection-and-configuration\">Operator Selection and Configuration</h4>\n<p>Physical operator selection must choose appropriate implementations based on data characteristics, available indexes, and estimated costs. Bugs in operator selection often involve choosing inefficient algorithms or incorrectly configuring operator parameters.</p>\n<table>\n<thead>\n<tr>\n<th>Operator Type</th>\n<th>Selection Criteria</th>\n<th>Configuration Parameters</th>\n<th>Common Bugs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Scan Operators</td>\n<td>Table size, selectivity, available indexes</td>\n<td>Index choice, scan direction, filter pushdown</td>\n<td>Wrong index selection, missing filter pushdown</td>\n</tr>\n<tr>\n<td>Join Operators</td>\n<td>Input sizes, join selectivity, memory availability</td>\n<td>Hash table size, join algorithm, predicate order</td>\n<td>Algorithm mismatch for input characteristics</td>\n</tr>\n<tr>\n<td>Sort Operators</td>\n<td>Input cardinality, available memory, required order</td>\n<td>Sort algorithm, memory allocation, spill handling</td>\n<td>Memory estimation errors, incorrect sort keys</td>\n</tr>\n<tr>\n<td>Aggregation Operators</td>\n<td>Group cardinality, aggregate functions, memory limits</td>\n<td>Hash table sizing, aggregation algorithm</td>\n<td>Memory allocation bugs, incorrect grouping</td>\n</tr>\n</tbody></table>\n<p>The <code>_select_scan_operator</code> and <code>_select_join_algorithm</code> methods require extensive testing across different data characteristics. Building comprehensive test suites that cover various selectivity ranges, input sizes, and schema patterns helps identify operator selection bugs.</p>\n<p>Operator configuration bugs often involve mathematical errors in parameter calculation. For example, hash join memory allocation might use incorrect formulas to estimate required memory, leading to performance problems or execution failures.</p>\n<h4 id=\"schema-propagation-and-type-safety\">Schema Propagation and Type Safety</h4>\n<p>Query execution requires that column schemas flow correctly through the operator tree, with each operator producing outputs compatible with its parent&#39;s expected inputs. Schema propagation bugs can cause runtime type errors or incorrect result calculation.</p>\n<table>\n<thead>\n<tr>\n<th>Schema Aspect</th>\n<th>Propagation Rule</th>\n<th>Validation Check</th>\n<th>Error Pattern</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Column Names</td>\n<td>Preserve or rename consistently</td>\n<td>Verify column names match expected schema</td>\n<td>Missing columns in projection</td>\n</tr>\n<tr>\n<td>Data Types</td>\n<td>Maintain type compatibility</td>\n<td>Check type compatibility across operators</td>\n<td>Type mismatch errors</td>\n</tr>\n<tr>\n<td>Null Handling</td>\n<td>Propagate null semantics</td>\n<td>Verify null handling consistency</td>\n<td>Incorrect null result handling</td>\n</tr>\n<tr>\n<td>Column Ordering</td>\n<td>Maintain consistent ordering</td>\n<td>Check column position expectations</td>\n<td>Wrong column order in results</td>\n</tr>\n<tr>\n<td>Schema Annotations</td>\n<td>Preserve metadata</td>\n<td>Verify annotation propagation</td>\n<td>Missing constraint information</td>\n</tr>\n</tbody></table>\n<p>The <code>output_schema</code> field in <code>OperatorNode</code> provides the primary mechanism for schema propagation. Implementing strict schema validation at each tree construction step helps catch propagation errors before they reach execution.</p>\n<p>Building schema propagation test cases that trace column flow through complex operator trees helps identify edge cases where schema information gets lost or corrupted during plan generation.</p>\n<h4 id=\"cost-accumulation-and-validation\">Cost Accumulation and Validation</h4>\n<p>Plan generation must correctly accumulate costs from child operators to produce accurate total cost estimates. Cost accumulation bugs can lead to incorrect optimization decisions even when individual operator costs are estimated correctly.</p>\n<table>\n<thead>\n<tr>\n<th>Cost Accumulation Rule</th>\n<th>Implementation</th>\n<th>Validation Method</th>\n<th>Debugging Output</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Bottom-up calculation</td>\n<td>Sum child costs before adding operator cost</td>\n<td>Verify costs calculated in correct order</td>\n<td>Cost tree with intermediate totals</td>\n</tr>\n<tr>\n<td>Operator cost addition</td>\n<td>Add operator-specific costs to child costs</td>\n<td>Check operator cost calculation accuracy</td>\n<td>Detailed cost breakdown by operator</td>\n</tr>\n<tr>\n<td>Cost component handling</td>\n<td>Properly combine I/O, CPU, and memory costs</td>\n<td>Verify cost component math</td>\n<td>Separate totals for each cost type</td>\n</tr>\n<tr>\n<td>Plan comparison</td>\n<td>Use accumulated costs for plan selection</td>\n<td>Compare cost calculation consistency</td>\n<td>Side-by-side cost comparison</td>\n</tr>\n</tbody></table>\n<p>The <code>calculate_subtree_cost</code> method must handle cost accumulation correctly while avoiding double-counting. Adding comprehensive cost tracking that shows how costs flow up the tree helps identify accumulation errors.</p>\n<p>Cost validation becomes particularly important when debugging why the optimizer selects suboptimal plans. By tracing cost accumulation step-by-step, developers can identify whether selection errors stem from accumulation bugs or incorrect base cost estimates.</p>\n<h4 id=\"plan-validation-and-correctness-checks\">Plan Validation and Correctness Checks</h4>\n<p>Generated plans must satisfy numerous correctness constraints to ensure proper execution semantics. Implementing comprehensive plan validation helps catch generation bugs before they cause execution failures.</p>\n<table>\n<thead>\n<tr>\n<th>Validation Category</th>\n<th>Check Description</th>\n<th>Implementation</th>\n<th>Recovery Strategy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Semantic Correctness</td>\n<td>Verify plan produces correct query results</td>\n<td>Compare plan semantics against SQL query</td>\n<td>Reject invalid plans and try alternatives</td>\n</tr>\n<tr>\n<td>Execution Feasibility</td>\n<td>Ensure plan can be executed by execution engine</td>\n<td>Check operator compatibility and resource requirements</td>\n<td>Generate fallback plans or report errors</td>\n</tr>\n<tr>\n<td>Resource Constraints</td>\n<td>Verify plan respects memory and other limits</td>\n<td>Check estimated resource usage against limits</td>\n<td>Select alternative operators or fail gracefully</td>\n</tr>\n<tr>\n<td>Optimization Invariants</td>\n<td>Ensure plan satisfies optimization assumptions</td>\n<td>Verify cost estimates and operator properties</td>\n<td>Log warnings about assumption violations</td>\n</tr>\n</tbody></table>\n<p>The <code>_validate_physical_plan</code> method serves as the final checkpoint before returning optimized plans. This validation should be comprehensive enough to catch correctness bugs while remaining fast enough to avoid optimization performance problems.</p>\n<p>⚠️ <strong>Pitfall: Schema Mismatch Propagation</strong>\nFailing to validate schema compatibility between parent and child operators can cause subtle correctness bugs that only surface during execution. Always verify that operator output schemas match parent operator input expectations, especially after applying optimization transformations.</p>\n<p>⚠️ <strong>Pitfall: Cost Double-Counting</strong>\nAdding child operator costs multiple times during cost accumulation leads to dramatically inflated cost estimates that can mislead optimization decisions. Implement careful cost accumulation logic that ensures each operator cost is counted exactly once in the total plan cost.</p>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<p>Query optimizer debugging requires sophisticated tooling and systematic approaches to handle the complexity of multi-component interactions. The following implementation provides comprehensive debugging infrastructure that supports all aspects of optimizer troubleshooting.</p>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Logging Framework</td>\n<td>Python <code>logging</code> module with structured format</td>\n<td><code>structlog</code> with JSON output for machine parsing</td>\n</tr>\n<tr>\n<td>Cost Visualization</td>\n<td>Text-based cost trees in debug output</td>\n<td>Web-based interactive cost visualization</td>\n</tr>\n<tr>\n<td>Plan Comparison</td>\n<td>Side-by-side text comparison</td>\n<td>Graph-based diff visualization</td>\n</tr>\n<tr>\n<td>Statistics Validation</td>\n<td>Manual SQL queries for verification</td>\n<td>Automated statistics auditing framework</td>\n</tr>\n<tr>\n<td>Performance Profiling</td>\n<td>Python <code>cProfile</code> for optimization timing</td>\n<td>Custom profiler with component-level metrics</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure\">Recommended File Structure</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>query_optimizer/\n  debugging/\n    __init__.py\n    cost_debugger.py          ← Cost estimation debugging tools\n    join_debugger.py          ← Join ordering debugging utilities  \n    plan_debugger.py          ← Plan generation debugging support\n    statistics_validator.py   ← Statistics validation and testing\n    debug_formatter.py        ← Debug output formatting utilities\n  test_data/\n    debug_queries.sql         ← Test queries for debugging scenarios\n    expected_plans.json       ← Expected plan structures for validation\n    cost_benchmarks.json      ← Cost estimation benchmarks\n  tools/\n    debug_cli.py             ← Command-line debugging interface\n    plan_visualizer.py       ← Plan visualization tools</code></pre></div>\n\n<h4 id=\"cost-estimation-debugging-infrastructure\">Cost Estimation Debugging Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Tuple, Any</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime, timedelta</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CostDebugTrace</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Detailed trace information for cost estimation debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    component: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operation: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    inputs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    outputs: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    timestamp: datetime</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    duration_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CostEstimationDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Comprehensive debugging support for cost estimation components.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, enable_detailed_logging: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.enable_detailed_logging </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> enable_detailed_logging</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.debug_traces: List[CostDebugTrace] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.logger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> logging.getLogger(</span><span style=\"color:#79B8FF\">__name__</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_table_statistics</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, stats: </span><span style=\"color:#9ECBFF\">'TableStatistics'</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate table statistics against actual database state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement actual row count verification via SELECT COUNT(*)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check statistics freshness against table modification timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate column statistics for each column in table schema</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify histogram bucket consistency and coverage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for missing statistics that should exist</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return validation results with specific issues found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> trace_selectivity_calculation</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, column: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    operator: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, value: Any, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                    result: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Trace detailed selectivity estimation process.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Log input statistics used for selectivity calculation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Show mathematical formula applied for given operator type</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record intermediate calculation steps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate result is within valid range [0.0, 1.0]</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store trace for later analysis and debugging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_estimated_vs_actual_cardinality</span><span style=\"color:#E1E4E8\">(self, plan: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                               actual_results: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compare estimated cardinalities against actual execution results.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract cardinality estimates from each operator in plan</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Match estimates with actual row counts from execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate percentage error for each operator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify operators with worst estimation accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate summary report of estimation quality</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return error metrics for further analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> StatisticsValidator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validates statistics accuracy and freshness.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, database_connection):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.db </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> database_connection</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.validation_cache </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> audit_statistics_freshness</span><span style=\"color:#E1E4E8\">(self, table_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'StalenessAssessment'</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Audit statistics freshness for multiple tables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Query table modification timestamps from database metadata</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare with stored statistics last_updated timestamps</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate staleness severity based on modification volume</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate refresh recommendations for stale statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return staleness assessment for each table</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_row_count_accuracy</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 stored_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, tolerance: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.05</span><span style=\"color:#E1E4E8\">) -> Tuple[</span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify stored row count against actual database state.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Execute SELECT COUNT(*) query against specified table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare actual count with stored statistics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate percentage difference between stored and actual</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Determine if difference exceeds acceptable tolerance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return validation result, actual count, and error percentage</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"join-ordering-debugging-tools\">Join Ordering Debugging Tools</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Set, Dict, List, Tuple</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> collections </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> defaultdict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JoinOrderingDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Debugging utilities for dynamic programming join ordering.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, enable_search_tracing: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> False</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.enable_search_tracing </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> enable_search_tracing</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.search_trace </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.subset_costs </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.pruning_stats </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> defaultdict(</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> trace_dynamic_programming_execution</span><span style=\"color:#E1E4E8\">(self, optimizer: </span><span style=\"color:#9ECBFF\">'DynamicProgrammingOptimizer'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                          tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Trace complete dynamic programming execution for debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Monitor subset enumeration at each size level</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Track cost calculations for each subset combination</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record pruning decisions and reasons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure optimization time by component</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that optimal substructure property holds</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return comprehensive trace of algorithm execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_subset_enumeration</span><span style=\"color:#E1E4E8\">(self, tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], generated_subsets: List[Set[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate that all required subsets are generated correctly.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate expected number of subsets: 2^n - 1 (excluding empty set)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify no duplicate subsets in generated list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that all single-table subsets are present</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify all table combinations up to full set are generated</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate subset ordering matches dynamic programming requirements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return True if enumeration is correct, False with details otherwise</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_cost_comparison_stability</span><span style=\"color:#E1E4E8\">(self, cost_traces: List[CostDebugTrace]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze stability and consistency of cost comparisons.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Group cost calculations by identical input parameters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for non-deterministic cost calculation results</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure variance in costs for identical scenarios</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify floating-point precision issues in comparisons</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate confidence intervals for cost estimates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return stability metrics and problematic comparison patterns</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> verify_join_predicate_connectivity</span><span style=\"color:#E1E4E8\">(self, parsed_query: </span><span style=\"color:#9ECBFF\">'ParsedQuery'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'JoinGraph'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Verify join predicate analysis and connectivity detection.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Extract all join predicates from query WHERE clause</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Build connectivity graph between tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify disconnected table groups (potential cross products)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate that all join predicates are properly parsed</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for missing foreign key relationships</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return connectivity analysis with potential issues highlighted</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> JoinOrderingProfiler</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Performance profiling for join ordering optimization.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> profile_optimization_phases</span><span style=\"color:#E1E4E8\">(self, tables: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Profile time spent in each optimization phase.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Time base case initialization phase</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure subset enumeration time by size</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Track cost calculation time per subset</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Profile pruning decision time</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Measure memoization overhead</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return time breakdown by optimization phase</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"plan-generation-debugging-support\">Plan Generation Debugging Support</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Optional, List, Dict</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> json</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PlanGenerationDebugger</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Debugging utilities for query plan generation and validation.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.validation_errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.schema_trace </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_trace </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_plan_structure</span><span style=\"color:#E1E4E8\">(self, plan: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate structural correctness of generated execution plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        errors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Traverse plan tree and validate parent-child relationships</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check that all operators have valid input schemas</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify schema compatibility between connected operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate cost accumulation correctness throughout tree</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for orphaned nodes or circular references</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure all query clauses are represented by operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return list of structural errors found</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> errors</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> trace_schema_propagation</span><span style=\"color:#E1E4E8\">(self, root_node: </span><span style=\"color:#9ECBFF\">'OperatorNode'</span><span style=\"color:#E1E4E8\">) -> List[Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Trace schema propagation through operator tree.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        schema_flow </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Traverse tree in execution order (post-order)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record input and output schemas for each operator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate schema transformations at each operator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for column name conflicts or missing columns</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify data type compatibility across operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return detailed schema flow trace for analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        return</span><span style=\"color:#E1E4E8\"> schema_flow</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_cost_accumulation</span><span style=\"color:#E1E4E8\">(self, plan: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate cost accumulation accuracy throughout plan tree.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recalculate costs bottom-up through tree traversal</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare recalculated costs with stored cost estimates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check for cost component double-counting</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify cost model consistency across similar operators</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify operators contributing most to total cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return cost validation results with any discrepancies</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> generate_plan_debug_output</span><span style=\"color:#E1E4E8\">(self, plan: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  include_costs: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                  include_schemas: </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> True</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Generate comprehensive debug output for execution plan.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Format plan tree structure with proper indentation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include detailed cost breakdown if requested</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Show input/output schemas for each operator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add operator-specific configuration details</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Include cardinality estimates and selectivity factors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return formatted debug string for analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> PlanComparator</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Utilities for comparing execution plans and identifying differences.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> compare_plan_structures</span><span style=\"color:#E1E4E8\">(self, plan1: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                               plan2: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, Any]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Compare structural differences between two execution plans.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare operator types and tree topology</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify differences in join ordering</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare physical operator selections</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Analyze cost estimate differences</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check schema differences between plans</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return comprehensive comparison results</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> explain_plan_selection_decision</span><span style=\"color:#E1E4E8\">(self, alternative_plans: List[</span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                       selected_plan: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Explain why optimizer selected specific plan over alternatives.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare costs across all alternative plans</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify key factors that influenced selection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Highlight cost differences between top alternatives</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Explain any tie-breaking decisions made</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Return human-readable explanation of selection logic</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"debugging-command-line-interface\">Debugging Command-Line Interface</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> argparse</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> sys</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> List</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> OptimizerDebugCLI</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Command-line interface for query optimizer debugging.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_debugger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> CostEstimationDebugger()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.join_debugger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> JoinOrderingDebugger()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.plan_debugger </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> PlanGenerationDebugger()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> debug_query_optimization</span><span style=\"color:#E1E4E8\">(self, sql_query: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, debug_options: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Debug complete query optimization process.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Parse SQL query and extract components</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run optimization with detailed debugging enabled</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect debugging information from each component</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate comprehensive debug report</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Output results in requested format (text/JSON/HTML)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> validate_statistics_accuracy</span><span style=\"color:#E1E4E8\">(self, table_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Validate statistics accuracy for specified tables.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run statistics validation for each specified table</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate accuracy report with specific issues</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Recommend statistics refresh for problematic tables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Output validation results in structured format</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> benchmark_join_ordering</span><span style=\"color:#E1E4E8\">(self, query_file: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, iterations: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 10</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Benchmark join ordering performance for test queries.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Load test queries from specified file</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Run join ordering optimization multiple times</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Collect timing and quality metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Generate performance benchmark report</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify optimization bottlenecks and improvement opportunities</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> main</span><span style=\"color:#E1E4E8\">():</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Main entry point for optimizer debugging CLI.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> argparse.ArgumentParser(</span><span style=\"color:#FFAB70\">description</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Query Optimizer Debugging Tools\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span><span style=\"color:#9ECBFF\">\"command\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">choices</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"debug\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"validate\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"benchmark\"</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                       help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Debugging command to execute\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span><span style=\"color:#9ECBFF\">\"--query\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"SQL query to debug\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span><span style=\"color:#9ECBFF\">\"--tables\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">nargs</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"+\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Tables to validate\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span><span style=\"color:#9ECBFF\">\"--verbose\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">action</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"store_true\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Enable verbose output\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parser.add_argument(</span><span style=\"color:#9ECBFF\">\"--output-format\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">choices</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[</span><span style=\"color:#9ECBFF\">\"text\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"json\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">\"html\"</span><span style=\"color:#E1E4E8\">], </span></span>\n<span class=\"line\"><span style=\"color:#FFAB70\">                       default</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"text\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">help</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"Output format\"</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    args </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> parser.parse_args()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cli </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> OptimizerDebugCLI()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Route to appropriate debugging function based on command</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Handle command-line argument validation and error cases</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Provide helpful error messages for invalid usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Support interactive debugging mode for complex scenarios</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">if</span><span style=\"color:#79B8FF\"> __name__</span><span style=\"color:#F97583\"> ==</span><span style=\"color:#9ECBFF\"> \"__main__\"</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    main()</span></span></code></pre></div>\n\n<h4 id=\"milestone-debugging-checkpoints\">Milestone Debugging Checkpoints</h4>\n<p>Each milestone should include specific debugging validation to ensure correct implementation:</p>\n<p><strong>Milestone 1 - Plan Representation Debugging:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_milestone_1_debugging</span><span style=\"color:#E1E4E8\">(plan: ExecutionPlan) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate plan representation debugging capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    issues </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify plan tree structure validation works correctly</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test pretty-print output format and readability  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate operator hierarchy and inheritance</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check parent-child relationship consistency</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test cost annotation attachment and propagation</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> issues</span></span></code></pre></div>\n\n<p><strong>Milestone 2 - Cost Estimation Debugging:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_milestone_2_debugging</span><span style=\"color:#E1E4E8\">(optimizer: </span><span style=\"color:#9ECBFF\">'CostEstimator'</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate cost estimation debugging capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    issues </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test statistics validation against known data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify selectivity estimation tracing works</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate cardinality estimation debugging output</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check cost model component breakdown accuracy</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> issues</span></span></code></pre></div>\n\n<p><strong>Milestone 3 - Join Ordering Debugging:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_milestone_3_debugging</span><span style=\"color:#E1E4E8\">(optimizer: </span><span style=\"color:#9ECBFF\">'DynamicProgrammingOptimizer'</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate join ordering debugging capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    issues </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test dynamic programming trace collection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify subset enumeration validation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check cost comparison stability analysis</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate pruning decision logging</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> issues</span></span></code></pre></div>\n\n<p><strong>Milestone 4 - Physical Planning Debugging:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#F97583\">def</span><span style=\"color:#B392F0\"> validate_milestone_4_debugging</span><span style=\"color:#E1E4E8\">(planner: </span><span style=\"color:#9ECBFF\">'PhysicalPlanner'</span><span style=\"color:#E1E4E8\">) -> List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Validate physical planning debugging capabilities.\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    issues </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Test operator selection debugging traces</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Verify plan validation error detection</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check optimization rule application logging</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">    # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate final plan correctness verification</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    return</span><span style=\"color:#E1E4E8\"> issues</span></span></code></pre></div>\n\n\n<h2 id=\"future-extensions\">Future Extensions</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> Post-implementation enhancements - advanced features that can be added after completing the core query optimizer implementation across Milestones 1-4.</p>\n</blockquote>\n<h3 id=\"mental-model-architectural-evolution\">Mental Model: Architectural Evolution</h3>\n<p>Think of your query optimizer as a city&#39;s transportation system that starts with basic roads and traffic lights. Once the fundamental infrastructure is working well, you can add advanced features like smart traffic management systems that learn from traffic patterns, express lanes for high-priority routes, and real-time route adjustment based on current conditions. Each enhancement builds on the solid foundation you&#39;ve established, adding sophistication without compromising the core functionality that users depend on.</p>\n<p>The extensions outlined in this section represent the next evolutionary steps for your query optimizer. Just as a transportation system benefits from adaptive traffic signals, machine learning-based route prediction, and parallel highway systems, your optimizer can grow to include advanced statistical models, parallel execution awareness, and runtime learning capabilities. These enhancements transform a functional optimizer into an intelligent system that continuously improves its decision-making based on real-world feedback.</p>\n<h3 id=\"advanced-statistical-models\">Advanced Statistical Models</h3>\n<p>Current cost-based optimizers rely on relatively simple statistical assumptions that work well for many queries but struggle with complex data distributions and correlations. Advanced statistical models address these limitations by capturing more nuanced characteristics of your data, leading to significantly improved cost estimation accuracy for challenging query patterns.</p>\n<p><strong>Multi-dimensional histograms</strong> represent the natural evolution beyond single-column statistics. While your current <code>ColumnStatistics</code> captures the distribution of individual columns, real queries often involve predicates on multiple correlated columns. Consider a customer database where age and income are strongly correlated, or a time-series table where timestamp and sensor_type interact. Multi-dimensional histograms capture these relationships by partitioning the joint value space into buckets that preserve correlation information.</p>\n<p>The implementation extends your existing <code>HistogramBucket</code> structure to support multiple dimensions. Instead of a single range defined by <code>range_start</code> and <code>range_end</code>, multi-dimensional buckets define rectangular regions in the joint value space. For a two-dimensional histogram on columns A and B, each bucket would contain ranges for both dimensions plus the count of rows falling within that rectangular region. The selectivity estimation process becomes more sophisticated, requiring intersection calculations between query predicates and the multi-dimensional bucket boundaries.</p>\n<blockquote>\n<p><strong>Key Insight</strong>: Multi-dimensional histograms are most valuable for tables with 2-4 strongly correlated columns. Beyond four dimensions, the curse of dimensionality makes bucket populations too sparse to provide reliable statistics. Focus on identifying the most important column correlations in your workload rather than building histograms for every possible column combination.</p>\n</blockquote>\n<p><strong>Correlation detection</strong> automates the identification of column relationships that would benefit from multi-dimensional modeling. The system analyzes query workloads and table contents to discover statistical dependencies between columns. Strong correlations indicate where joint histograms would improve estimation accuracy compared to the independence assumptions in single-column statistics.</p>\n<p>The correlation analysis extends your <code>TableStatistics</code> with a correlation matrix tracking pairwise relationships between columns. During statistics collection, the system computes correlation coefficients and mutual information metrics to quantify dependencies. Queries involving correlated columns trigger specialized estimation logic that accounts for the discovered relationships rather than assuming independence.</p>\n<p><strong>Machine learning-based estimation</strong> represents the most sophisticated extension, using historical query execution data to train models that predict cardinalities and selectivities. Instead of relying solely on statistical formulas, the system learns from actual execution results to improve future estimates. This approach is particularly valuable for complex queries where traditional statistical methods struggle due to multiple joins, nested subqueries, or unusual data distributions.</p>\n<p>The ML integration requires extending your <code>CostEstimate</code> with training data collection capabilities. Each query execution generates training examples pairing the optimizer&#39;s initial estimates with actual cardinalities observed during execution. Feature vectors capture query characteristics like predicate selectivities, join patterns, and table sizes. The trained models supplement traditional statistics-based estimation, with confidence scores determining when to prefer ML predictions over statistical calculations.</p>\n<table>\n<thead>\n<tr>\n<th>Statistical Model Extension</th>\n<th>Complexity</th>\n<th>Accuracy Improvement</th>\n<th>Implementation Effort</th>\n<th>Best Use Cases</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-dimensional histograms</td>\n<td>Medium</td>\n<td>High for correlated columns</td>\n<td>Medium</td>\n<td>Time-series, demographic data, scientific datasets</td>\n</tr>\n<tr>\n<td>Correlation detection</td>\n<td>Low</td>\n<td>Medium across many queries</td>\n<td>Low</td>\n<td>Automatically identifying optimization opportunities</td>\n</tr>\n<tr>\n<td>Machine learning estimation</td>\n<td>High</td>\n<td>Very high for complex queries</td>\n<td>High</td>\n<td>Large systems with sufficient training data</td>\n</tr>\n<tr>\n<td>Adaptive histogram refinement</td>\n<td>Medium</td>\n<td>Medium ongoing improvement</td>\n<td>Medium</td>\n<td>Tables with changing data distributions</td>\n</tr>\n<tr>\n<td>Query workload modeling</td>\n<td>High</td>\n<td>High for repeated patterns</td>\n<td>High</td>\n<td>Systems with predictable query workloads</td>\n</tr>\n</tbody></table>\n<h3 id=\"parallel-query-support\">Parallel Query Support</h3>\n<p>Modern database systems leverage multiple CPU cores and distributed processing to accelerate query execution. Extending your optimizer for parallel execution requires fundamental changes to cost modeling, operator selection, and resource management while preserving the correctness of your existing optimization logic.</p>\n<p><strong>Parallel cost modeling</strong> transforms your single-threaded <code>CostEstimate</code> into a resource-aware framework that considers CPU parallelism, memory bandwidth, and I/O concurrency. Instead of estimating total work, the cost model must predict execution time under different parallelism scenarios. A table scan that requires reading 1000 pages might take 1000 time units sequentially but only 100 time units with 10-way parallelism, assuming sufficient I/O bandwidth.</p>\n<p>The parallel cost framework extends <code>CostEstimate</code> with parallelism-aware fields tracking the degree of parallelism, resource contention factors, and coordination overhead. Each physical operator must provide estimates for its scalability characteristics - some operations like table scans parallelize nearly linearly, while others like sorting have logarithmic coordination costs due to merge phases. The total plan cost calculation becomes more sophisticated, considering the critical path through the execution graph and resource bottlenecks that limit overall parallelism.</p>\n<p><strong>Resource-aware optimization</strong> incorporates system resource constraints into plan selection. A query optimizer operating in a system with 8 CPU cores and limited memory must make different decisions than one with 64 cores and abundant RAM. The optimizer needs awareness of available parallelism, memory limits, and I/O capacity to generate executable plans that don&#39;t exceed system resources.</p>\n<p>This extension requires augmenting your <code>PhysicalCostComparator</code> with resource modeling capabilities. The system maintains estimates of available CPU cores, memory bandwidth, and storage I/O capacity. During physical operator selection, the optimizer considers not just the fastest theoretical plan, but the fastest plan that can execute within resource constraints. A highly parallel hash join might be theoretically optimal but impractical if it would exceed available memory.</p>\n<p><strong>Parallel operator selection</strong> extends your physical planning logic to choose between sequential and parallel implementations of each operation. The decision depends on input sizes, available resources, and the degree of parallelism already present in other parts of the plan. Small table scans might execute faster sequentially to avoid coordination overhead, while large aggregations benefit significantly from parallel processing.</p>\n<p>The parallel operator framework requires extending your <code>OperatorNode</code> hierarchy with parallel variants of each physical operator. ParallelScan, ParallelHashJoin, and ParallelAggregate operators include additional properties for degree of parallelism, partitioning strategies, and resource requirements. The <code>selectPhysicalOperators</code> logic becomes more sophisticated, evaluating both sequential and parallel alternatives for each logical operation based on estimated input sizes and system resources.</p>\n<blockquote>\n<p><strong>Decision: Parallel Execution Model</strong></p>\n<ul>\n<li><strong>Context</strong>: Need to extend single-threaded optimizer for multi-core and distributed execution environments</li>\n<li><strong>Options Considered</strong>: <ol>\n<li>Volcano-style exchange operators with explicit parallelism boundaries</li>\n<li>Morsel-driven execution with fine-grained work stealing  </li>\n<li>Pipeline parallelism with operator-level thread pools</li>\n</ol>\n</li>\n<li><strong>Decision</strong>: Volcano-style exchange operators for initial implementation</li>\n<li><strong>Rationale</strong>: Exchange operators provide clean abstraction boundaries, are well-understood in academic literature, and allow incremental migration of existing operators to parallel versions</li>\n<li><strong>Consequences</strong>: Clear separation between sequential and parallel execution regions, but potential overhead from explicit data exchanges between parallel regions</li>\n</ul>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Parallel Extension Component</th>\n<th>Integration Complexity</th>\n<th>Performance Impact</th>\n<th>Resource Requirements</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Parallel cost modeling</td>\n<td>High - requires fundamental changes to cost calculation</td>\n<td>Medium - improves plan selection accuracy</td>\n<td>Low - statistical calculations only</td>\n</tr>\n<tr>\n<td>Resource-aware optimization</td>\n<td>Medium - extends existing physical planning</td>\n<td>High - prevents resource exhaustion</td>\n<td>Medium - requires system monitoring</td>\n</tr>\n<tr>\n<td>Parallel operator implementations</td>\n<td>High - new operator variants needed</td>\n<td>Very High - direct execution speedup</td>\n<td>High - CPU and memory intensive</td>\n</tr>\n<tr>\n<td>Exchange operator framework</td>\n<td>Medium - clean abstraction layer</td>\n<td>Medium - enables parallelism boundaries</td>\n<td>Medium - coordination overhead</td>\n</tr>\n<tr>\n<td>Adaptive degree of parallelism</td>\n<td>High - runtime feedback required</td>\n<td>High - optimizes resource utilization</td>\n<td>Medium - monitoring and adjustment logic</td>\n</tr>\n</tbody></table>\n<h3 id=\"runtime-adaptive-optimization\">Runtime Adaptive Optimization</h3>\n<p>Traditional query optimizers make all decisions at compile time based on statistical estimates, but real execution often reveals that these estimates were inaccurate. Runtime adaptive optimization creates feedback loops that allow the system to learn from actual execution behavior and improve future optimization decisions.</p>\n<p><strong>Execution feedback collection</strong> instruments query execution to gather actual cardinalities, operator costs, and resource utilization patterns. This data provides ground truth for evaluating the accuracy of the optimizer&#39;s estimates and identifying systematic biases in the cost models. The feedback system extends your existing execution infrastructure with monitoring capabilities that capture detailed performance metrics without significantly impacting query execution times.</p>\n<p>The feedback collection framework augments <code>ExecutionPlan</code> with monitoring hooks that record actual row counts, execution times, and resource consumption for each operator. During execution, the system compares these actual values against the optimizer&#39;s original estimates in the <code>CostEstimate</code> annotations. Significant deviations indicate opportunities for model improvement or statistics updates. The collected data feeds back into the optimization process for future queries.</p>\n<p><strong>Adaptive statistics maintenance</strong> uses execution feedback to automatically update table and column statistics when the optimizer detects that its estimates are consistently inaccurate. Instead of relying solely on periodic statistics collection, the system can incrementally refine its statistical models based on observed query behavior. This approach is particularly valuable for rapidly changing datasets where traditional statistics collection cannot keep pace with data evolution.</p>\n<p>The adaptive maintenance extends your <code>TableStatistics</code> and <code>ColumnStatistics</code> with confidence tracking and incremental update mechanisms. When execution feedback reveals systematic estimation errors, the system can adjust histogram bucket boundaries, update selectivity estimates, or trigger targeted statistics recollection for specific tables and columns. The adaptation process includes safeguards to prevent overreaction to outlier queries while still responding to genuine distribution changes.</p>\n<p><strong>Plan reoptimization</strong> represents the most sophisticated form of adaptive behavior, where the system can modify or replace execution plans during query execution when it detects that the original optimization decisions were based on poor estimates. Mid-execution reoptimization is complex and requires careful coordination with the execution engine, but it can provide dramatic performance improvements for long-running queries where the optimizer&#39;s initial assumptions prove incorrect.</p>\n<p>The reoptimization framework requires extending your query execution infrastructure with checkpointing and plan switching capabilities. When monitoring detects that actual cardinalities differ significantly from estimates, the system can trigger reoptimization of the remaining query execution. The new plan must account for work already completed and intermediate results already materialized. This capability is most valuable for complex analytical queries where early operators produce much larger or smaller intermediate results than expected.</p>\n<blockquote>\n<p><strong>Key Design Principle</strong>: Adaptive optimization must balance responsiveness with stability. Overreacting to every estimation error can cause thrashing and unpredictable performance, while underreacting fails to capture genuine optimization opportunities. Implement confidence intervals and change thresholds that require sustained evidence before triggering adaptations.</p>\n</blockquote>\n<p><strong>Machine learning integration</strong> leverages the collected execution feedback to train models that can predict query performance characteristics more accurately than traditional statistical methods. The ML models learn from the patterns in historical execution data to identify scenarios where the optimizer typically makes poor decisions and suggest alternative approaches.</p>\n<p>The ML integration extends your optimization pipeline with learned models that complement the existing rule-based and cost-based decision making. Feature vectors capture query characteristics, table properties, and system state information. The trained models provide additional signals for join ordering decisions, physical operator selection, and resource allocation. The integration requires careful engineering to ensure that ML inference doesn&#39;t significantly increase optimization time.</p>\n<table>\n<thead>\n<tr>\n<th>Adaptive Component</th>\n<th>Learning Speed</th>\n<th>Implementation Complexity</th>\n<th>Stability Impact</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Execution feedback collection</td>\n<td>Fast - immediate data available</td>\n<td>Low - monitoring infrastructure</td>\n<td>Minimal - read-only data collection</td>\n</tr>\n<tr>\n<td>Adaptive statistics maintenance</td>\n<td>Medium - requires multiple observations</td>\n<td>Medium - incremental update logic</td>\n<td>Low - gradual statistics refinement</td>\n</tr>\n<tr>\n<td>Plan reoptimization</td>\n<td>Slow - complex decision making required</td>\n<td>Very High - execution engine integration</td>\n<td>High - potential query interruption</td>\n</tr>\n<tr>\n<td>Machine learning integration</td>\n<td>Slow - requires training data accumulation</td>\n<td>High - ML infrastructure and training</td>\n<td>Medium - model prediction variability</td>\n</tr>\n<tr>\n<td>Workload-aware optimization</td>\n<td>Medium - learns from query patterns</td>\n<td>Medium - pattern recognition logic</td>\n<td>Low - influences future optimizations</td>\n</tr>\n</tbody></table>\n<p>The adaptive optimization extensions transform your query optimizer from a static system into a learning platform that continuously improves its decision-making capabilities. These enhancements are particularly valuable in production environments where query workloads evolve over time and where the cost of poor optimization decisions is measured in real business impact.</p>\n<blockquote>\n<p><strong>Implementation Priority</strong>: Start with execution feedback collection as it provides the foundation for all other adaptive features. The monitoring infrastructure offers immediate value for understanding optimizer behavior and provides the data needed for more sophisticated adaptive algorithms. Plan reoptimization should be implemented last due to its complexity and potential for execution disruption.</p>\n</blockquote>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"technology-recommendations\">Technology Recommendations</h4>\n<table>\n<thead>\n<tr>\n<th>Extension Category</th>\n<th>Simple Option</th>\n<th>Advanced Option</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Multi-dimensional statistics</td>\n<td>In-memory correlation matrices with numpy/scipy</td>\n<td>Specialized histogram databases like HyPer&#39;s multi-dimensional histograms</td>\n</tr>\n<tr>\n<td>Machine learning integration</td>\n<td>Scikit-learn with simple regression models</td>\n<td>TensorFlow/PyTorch with deep neural networks</td>\n</tr>\n<tr>\n<td>Parallel cost modeling</td>\n<td>Thread-pool based parallelism simulation</td>\n<td>Detailed resource contention modeling with queuing theory</td>\n</tr>\n<tr>\n<td>Execution monitoring</td>\n<td>Python decorators for operator instrumentation</td>\n<td>Custom bytecode instrumentation or profiling hooks</td>\n</tr>\n<tr>\n<td>Adaptive feedback loops</td>\n<td>Simple moving averages for statistics updates</td>\n<td>Reinforcement learning for dynamic optimization policies</td>\n</tr>\n</tbody></table>\n<h4 id=\"recommended-file-structure-extension\">Recommended File Structure Extension</h4>\n<div class=\"code-block-wrapper\"><pre class=\"arch-pre shiki-highlighted\"><code>query_optimizer/\n  extensions/\n    advanced_stats/\n      __init__.py\n      correlation_detector.py      ← automatic correlation discovery\n      multidim_histogram.py        ← multi-dimensional histogram implementation\n      ml_estimator.py             ← machine learning-based cardinality estimation\n      advanced_stats_collector.py ← enhanced statistics collection\n    parallel/\n      __init__.py\n      parallel_cost_model.py      ← resource-aware cost estimation\n      parallel_operators.py       ← parallel physical operator implementations\n      resource_manager.py         ← system resource tracking and allocation\n      exchange_operators.py       ← data exchange between parallel regions\n    adaptive/\n      __init__.py\n      execution_monitor.py        ← runtime performance data collection\n      feedback_processor.py       ← execution feedback analysis and integration\n      adaptive_stats.py           ← statistics refinement based on execution feedback\n      plan_reoptimizer.py         ← mid-execution plan modification\n  core/\n    optimizer.py                  ← extend with plugin architecture for extensions\n    cost_estimator.py            ← extend with pluggable estimation models\n    physical_planner.py          ← extend with parallel operator selection</code></pre></div>\n\n<h4 id=\"advanced-statistics-infrastructure\">Advanced Statistics Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># correlation_detector.py - Complete infrastructure for correlation analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> numpy </span><span style=\"color:#F97583\">as</span><span style=\"color:#E1E4E8\"> np</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Tuple, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> TableStatistics, ColumnStatistics</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ColumnCorrelation</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Statistical correlation information between two columns\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column_a: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    column_b: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    correlation_coefficient: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    mutual_information: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    sample_size: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    confidence_level: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> CorrelationDetector</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Automatically discovers statistical dependencies between columns\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, correlation_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.3</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.correlation_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> correlation_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.discovered_correlations: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[ColumnCorrelation]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> analyze_table_correlations</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, sample_data: np.ndarray, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                                 column_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> List[ColumnCorrelation]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Analyze pairwise correlations between all columns in a table.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns list of significant correlations above threshold.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement pairwise correlation analysis using numpy.corrcoef</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate mutual information for categorical variables</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply statistical significance testing to filter noise</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store results in self.discovered_correlations for caching</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_build_joint_histogram</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, columns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Determine if columns are sufficiently correlated to justify joint histogram.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Look up correlation coefficients for column pairs</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if any correlation exceeds threshold</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider histogram construction cost vs estimation improvement</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\"> </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultidimensionalBucket</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Histogram bucket covering rectangular region in multi-dimensional space\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    bucket_id: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    dimension_ranges: List[Tuple[</span><span style=\"color:#79B8FF\">any</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">any</span><span style=\"color:#E1E4E8\">]]  </span><span style=\"color:#6A737D\"># (min, max) for each dimension</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    row_count: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    distinct_count_per_dimension: List[</span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    frequency: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> MultidimensionalHistogram</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Multi-dimensional histogram for correlated column estimation\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, column_names: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], bucket_count: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 100</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.table_name </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> table_name</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.column_names </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> column_names</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.dimension_count </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> len</span><span style=\"color:#E1E4E8\">(column_names)</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.buckets: List[MultidimensionalBucket] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.total_rows </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> build_histogram</span><span style=\"color:#E1E4E8\">(self, sample_data: np.ndarray) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Construct multi-dimensional histogram from sample data.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Uses recursive binary partitioning to create balanced buckets.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Implement recursive partitioning algorithm</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure each bucket has minimum population for statistical validity</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Balance bucket sizes across all dimensions</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate distinct counts per dimension within each bucket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_selectivity</span><span style=\"color:#E1E4E8\">(self, predicates: List[Tuple[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">any</span><span style=\"color:#E1E4E8\">]]) -> </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Estimate selectivity for multi-dimensional range predicates.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Returns fraction of rows satisfying all predicates simultaneously.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Find buckets that intersect with predicate ranges</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate intersection volume for partially overlapping buckets  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Sum row counts from fully and partially intersecting buckets</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply uniform distribution assumption within each bucket</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"parallel-execution-framework\">Parallel Execution Framework</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># parallel_cost_model.py - Resource-aware cost estimation for parallel execution</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, Optional</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> core.data_model </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> CostEstimate, OperatorNode</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> SystemResources</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Current system resource availability\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_cores: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_mb: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    io_bandwidth_mbps: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    network_bandwidth_mbps: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_cpu_utilization: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    current_memory_utilization: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParallelCostEstimate</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">CostEstimate</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Extended cost estimate including parallelism information\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    degree_of_parallelism: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    coordination_overhead: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    resource_contention_factor: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    critical_path_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    parallelizable_fraction: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ParallelCostModel</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Cost estimation accounting for parallel execution and resource constraints\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, system_resources: SystemResources):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.system_resources </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> system_resources</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.contention_factors </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'cpu_intensive'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.95</span><span style=\"color:#E1E4E8\">,  </span><span style=\"color:#6A737D\"># Nearly linear scaling</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'io_intensive'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.7</span><span style=\"color:#E1E4E8\">,    </span><span style=\"color:#6A737D\"># I/O bandwidth limitations</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'memory_intensive'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.6</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#6A737D\"># Memory bandwidth constraints</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">            'coordination_heavy'</span><span style=\"color:#E1E4E8\">: </span><span style=\"color:#79B8FF\">0.3</span><span style=\"color:#6A737D\"> # Synchronization overhead</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        }</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_parallel_cost</span><span style=\"color:#E1E4E8\">(self, operator: OperatorNode, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             input_parallelism: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> ParallelCostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Estimate execution cost for operator under parallel execution.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Considers Amdahl's law, resource contention, and coordination overhead.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Classify operator workload type (CPU/IO/memory intensive)</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate theoretical speedup using Amdahl's law</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply resource contention factors based on system utilization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add coordination overhead for data exchanges and synchronization</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Determine optimal degree of parallelism for this operator</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> calculate_plan_parallelism</span><span style=\"color:#E1E4E8\">(self, plan: OperatorNode) -> Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">]:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Determine optimal degree of parallelism for each operator in plan.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Uses dynamic programming to find resource-optimal parallelization.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Traverse plan tree bottom-up</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate resource requirements for each parallelism option</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Find parallelism assignment that maximizes throughput within resources</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Ensure parent operators can handle child parallelism</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExchangeOperator</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#B392F0\">OperatorNode</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Data exchange boundary between parallel execution regions\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, partitioning_columns: List[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">], target_parallelism: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        super</span><span style=\"color:#E1E4E8\">().</span><span style=\"color:#79B8FF\">__init__</span><span style=\"color:#E1E4E8\">(</span><span style=\"color:#FFAB70\">operator_type</span><span style=\"color:#F97583\">=</span><span style=\"color:#9ECBFF\">\"EXCHANGE\"</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#FFAB70\">children</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">[], </span><span style=\"color:#FFAB70\">properties</span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\">{})</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.partitioning_columns </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> partitioning_columns</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.target_parallelism </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> target_parallelism</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.exchange_cost </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> estimate_exchange_cost</span><span style=\"color:#E1E4E8\">(self, input_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">, </span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">                             input_parallelism: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> CostEstimate:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Estimate cost of repartitioning data between parallel regions.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        Includes network I/O, serialization, and buffering costs.</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate data volume to exchange based on input rows</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Estimate serialization/deserialization CPU cost</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate network or memory bandwidth cost for data movement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add buffering costs if parallelism changes significantly</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"adaptive-optimization-infrastructure\">Adaptive Optimization Infrastructure</h4>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># execution_monitor.py - Runtime performance data collection</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> dataclasses </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> typing </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> Dict, List, Optional, Callable</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">from</span><span style=\"color:#E1E4E8\"> datetime </span><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> datetime</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> threading</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">import</span><span style=\"color:#E1E4E8\"> time</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#B392F0\">@dataclass</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionMetrics</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Actual performance metrics collected during query execution\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    operator_id: </span><span style=\"color:#79B8FF\">str</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_rows: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actual_rows: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    estimated_cost: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    actual_runtime_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    memory_used_mb: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    io_pages_read: </span><span style=\"color:#79B8FF\">int</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    cpu_time_ms: </span><span style=\"color:#79B8FF\">float</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExecutionMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Instruments query execution to collect performance feedback\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.active_monitors: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, </span><span style=\"color:#9ECBFF\">'QueryMonitor'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.completed_executions: List[</span><span style=\"color:#9ECBFF\">'ExecutionTrace'</span><span style=\"color:#E1E4E8\">] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.lock </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> threading.RLock()</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> start_query_monitoring</span><span style=\"color:#E1E4E8\">(self, query_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, execution_plan: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'QueryMonitor'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Begin monitoring execution of a query plan\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create QueryMonitor instance for this execution</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Register monitoring hooks for each operator in plan  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize performance counters and timers</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Store in active_monitors for concurrent access</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> complete_query_monitoring</span><span style=\"color:#E1E4E8\">(self, query_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#9ECBFF\">'ExecutionTrace'</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Finish monitoring and return collected performance data\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Retrieve QueryMonitor from active_monitors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Finalize all performance measurements</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Create ExecutionTrace with complete performance data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Move to completed_executions for analysis</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> QueryMonitor</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Tracks performance of a single query execution\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, query_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, execution_plan: </span><span style=\"color:#9ECBFF\">'ExecutionPlan'</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.query_id </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> query_id</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.execution_plan </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> execution_plan</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.operator_metrics: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, ExecutionMetrics] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.start_time </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> datetime.now()</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.end_time: Optional[datetime] </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> None</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operator_start</span><span style=\"color:#E1E4E8\">(self, operator_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record start of operator execution\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize ExecutionMetrics for operator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Record start timestamp and baseline resource usage</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Hook into system performance counters</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> record_operator_completion</span><span style=\"color:#E1E4E8\">(self, operator_id: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, actual_rows: </span><span style=\"color:#79B8FF\">int</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Record completion of operator execution with actual row count\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate elapsed time since operator start</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Read final resource usage counters</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update ExecutionMetrics with actual performance data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Compare against estimated values from original plan</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> AdaptiveStatisticsManager</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Updates statistics based on execution feedback\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self, confidence_threshold: </span><span style=\"color:#79B8FF\">float</span><span style=\"color:#F97583\"> =</span><span style=\"color:#79B8FF\"> 0.5</span><span style=\"color:#E1E4E8\">):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.confidence_threshold </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> confidence_threshold</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.estimation_errors: Dict[</span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> {}</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> process_execution_feedback</span><span style=\"color:#E1E4E8\">(self, execution_trace: </span><span style=\"color:#9ECBFF\">'ExecutionTrace'</span><span style=\"color:#E1E4E8\">) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Analyze execution feedback and update statistics if needed\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate estimation error ratios for each operator</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Identify systematic biases in cost or cardinality estimation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Update table/column statistics for consistently wrong estimates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Trigger statistics recollection if errors exceed threshold</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> should_update_statistics</span><span style=\"color:#E1E4E8\">(self, table_name: </span><span style=\"color:#79B8FF\">str</span><span style=\"color:#E1E4E8\">, error_pattern: List[</span><span style=\"color:#79B8FF\">float</span><span style=\"color:#E1E4E8\">]) -> </span><span style=\"color:#79B8FF\">bool</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Determine if statistics update is warranted based on error patterns\"\"\"</span><span style=\"color:#E1E4E8\">  </span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Calculate moving average of estimation errors</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply statistical significance test for systematic bias</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Consider cost of statistics update vs estimation improvement</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Check if sufficient evidence exists for confident update</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<h4 id=\"milestone-checkpoints-for-extensions\">Milestone Checkpoints for Extensions</h4>\n<p><strong>Advanced Statistics Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test correlation detection</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/extensions/test_correlation_detection.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Correlation detector identifies known relationships in test data</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Multi-dimensional histograms improve estimation accuracy for correlated predicates</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: ML models converge to better accuracy than statistical baselines</span></span></code></pre></div>\n\n<p><strong>Parallel Execution Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test parallel cost modeling</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/extensions/test_parallel_costs.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Parallel cost estimates scale appropriately with degree of parallelism</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Resource constraints limit parallelism to feasible levels</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Exchange operators correctly estimate data movement costs</span></span></code></pre></div>\n\n<p><strong>Adaptive Optimization Validation:</strong></p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">bash</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Test execution monitoring and feedback processing</span></span>\n<span class=\"line\"><span style=\"color:#B392F0\">python</span><span style=\"color:#79B8FF\"> -m</span><span style=\"color:#9ECBFF\"> pytest</span><span style=\"color:#9ECBFF\"> tests/extensions/test_adaptive_optimization.py</span><span style=\"color:#79B8FF\"> -v</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Execution monitor accurately captures actual performance metrics</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Statistics updates improve future estimation accuracy</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\"># Expected: Plan reoptimization triggers only for significant estimation errors</span></span></code></pre></div>\n\n<h4 id=\"integration-with-core-optimizer\">Integration with Core Optimizer</h4>\n<p>The extensions integrate with your core optimizer through a plugin architecture that preserves the existing optimization pipeline while allowing advanced features to enhance specific phases:</p>\n<div class=\"code-block-wrapper\"><span class=\"code-lang\">python</span><pre class=\"arch-pre shiki-highlighted\"><code><span class=\"line\"><span style=\"color:#6A737D\"># Enhanced optimizer.py with extension integration</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">class</span><span style=\"color:#B392F0\"> ExtensibleQueryOptimizer</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">    \"\"\"Query optimizer with pluggable extension support\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">    </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#79B8FF\"> __init__</span><span style=\"color:#E1E4E8\">(self):</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.core_optimizer </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> QueryOptimizer()  </span><span style=\"color:#6A737D\"># Your existing implementation</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.stat_extensions: List[StatisticsExtension] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.cost_extensions: List[CostModelExtension] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#79B8FF\">        self</span><span style=\"color:#E1E4E8\">.planning_extensions: List[PhysicalPlanningExtension] </span><span style=\"color:#F97583\">=</span><span style=\"color:#E1E4E8\"> []</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> register_statistics_extension</span><span style=\"color:#E1E4E8\">(self, extension: StatisticsExtension) -> </span><span style=\"color:#79B8FF\">None</span><span style=\"color:#E1E4E8\">:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Add advanced statistics capability\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Validate extension compatibility</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Add to stat_extensions list</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Initialize extension with existing statistics</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    def</span><span style=\"color:#B392F0\"> optimize_query_with_extensions</span><span style=\"color:#E1E4E8\">(self, parsed_query: ParsedQuery) -> ExecutionPlan:</span></span>\n<span class=\"line\"><span style=\"color:#9ECBFF\">        \"\"\"Main optimization pipeline with extension integration\"\"\"</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Use core optimizer for basic plan generation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply statistics extensions for improved cost estimation</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply parallel planning extensions if available</span></span>\n<span class=\"line\"><span style=\"color:#6A737D\">        # </span><span style=\"color:#F97583\">TODO</span><span style=\"color:#6A737D\">: Apply adaptive extensions for runtime optimization</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">        pass</span></span></code></pre></div>\n\n<p>These extensions represent significant engineering undertakings that build upon the solid foundation of your core optimizer implementation. Each extension addresses specific limitations of basic cost-based optimization and provides pathways for continuous improvement of query performance in production environments.</p>\n<h2 id=\"glossary\">Glossary</h2>\n<blockquote>\n<p><strong>Milestone(s):</strong> All Milestones 1-4 - provides comprehensive definitions of technical vocabulary and concepts used throughout the query optimizer implementation.</p>\n</blockquote>\n<h3 id=\"mental-model-dictionary-for-a-technical-language\">Mental Model: Dictionary for a Technical Language</h3>\n<p>Think of this glossary as a technical dictionary for the specialized language of query optimization. Just as learning a foreign language requires understanding both individual words and their contextual meanings, mastering query optimization requires fluency in its technical vocabulary. Each term represents a concept that database engineers have refined over decades, and understanding these precise definitions is crucial for implementing effective optimization algorithms.</p>\n<p>The glossary serves as both a reference during implementation and a validation tool for understanding. When you encounter unfamiliar terms in research papers or documentation, this glossary provides the context needed to bridge academic theory with practical implementation.</p>\n<h3 id=\"core-query-optimization-concepts\">Core Query Optimization Concepts</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>query optimization</td>\n<td>The systematic process of transforming SQL queries into efficient execution plans by analyzing multiple implementation strategies and selecting the approach with minimal estimated resource consumption</td>\n<td>Primary process implemented across all milestones</td>\n</tr>\n<tr>\n<td>cost-based optimization</td>\n<td>Query optimization strategy that makes decisions by comparing quantitative resource consumption estimates (I/O, CPU, memory) rather than applying fixed transformation rules</td>\n<td>Fundamental approach used in Milestones 2-4</td>\n</tr>\n<tr>\n<td>execution plan</td>\n<td>A tree-structured specification describing the sequence of operations (scans, joins, filters) and their implementation algorithms needed to execute a SQL query</td>\n<td>Core data structure from Milestone 1</td>\n</tr>\n<tr>\n<td>plan tree</td>\n<td>Hierarchical representation of query execution strategy where each node represents a database operation and edges represent data flow between operations</td>\n<td>Primary structural concept in Milestone 1</td>\n</tr>\n<tr>\n<td>operator node</td>\n<td>Individual operation in an execution plan tree, containing operation type, cost estimates, output schema, and references to child operations</td>\n<td>Fundamental building block from Milestone 1</td>\n</tr>\n<tr>\n<td>logical operator</td>\n<td>Abstract specification of a database operation (JOIN, FILTER, SCAN) without implementation details, focusing on what computation is needed rather than how it&#39;s performed</td>\n<td>Design concept used in plan representation</td>\n</tr>\n<tr>\n<td>physical operator</td>\n<td>Concrete implementation of a logical operation specifying exact algorithms (hash join vs nested loop join) and access methods (index scan vs sequential scan)</td>\n<td>Implementation focus in Milestone 4</td>\n</tr>\n</tbody></table>\n<h3 id=\"statistical-and-cost-estimation-terms\">Statistical and Cost Estimation Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>selectivity</td>\n<td>Mathematical fraction (0.0 to 1.0) representing the proportion of input rows that survive a filter predicate, crucial for estimating intermediate result sizes</td>\n<td>Core concept in Milestone 2 cost estimation</td>\n</tr>\n<tr>\n<td>cardinality</td>\n<td>The number of rows in a table or intermediate query result, used extensively in cost calculations and memory requirement estimation</td>\n<td>Fundamental metric throughout cost estimation</td>\n</tr>\n<tr>\n<td>cardinality estimation</td>\n<td>Process of predicting the number of rows that will be produced by database operations, combining table statistics with selectivity calculations</td>\n<td>Key algorithm in Milestone 2</td>\n</tr>\n<tr>\n<td>selectivity estimation</td>\n<td>Mathematical process of predicting what fraction of rows will satisfy filter predicates, using statistical models and histogram data</td>\n<td>Critical component of cost estimation</td>\n</tr>\n<tr>\n<td>cost estimation</td>\n<td>Quantitative prediction of resource consumption (I/O pages, CPU cycles, memory usage) required to execute a query plan, enabling comparison between alternative plans</td>\n<td>Primary objective of Milestone 2</td>\n</tr>\n<tr>\n<td>statistics collection</td>\n<td>Process of gathering and maintaining quantitative information about table characteristics (row counts, value distributions, correlation patterns) to support accurate cost estimation</td>\n<td>Infrastructure requirement for Milestone 2</td>\n</tr>\n<tr>\n<td>histogram</td>\n<td>Statistical data structure representing the distribution of values in a database column, typically organized as buckets containing value ranges and frequency counts</td>\n<td>Data structure supporting selectivity estimation</td>\n</tr>\n<tr>\n<td>clustering factor</td>\n<td>Statistical measure (0.0 to 1.0) indicating how well the physical storage order of table rows matches the logical ordering of indexed values, affecting I/O cost calculations</td>\n<td>Advanced statistic for cost modeling</td>\n</tr>\n</tbody></table>\n<h3 id=\"join-processing-and-optimization-terms\">Join Processing and Optimization Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>join ordering</td>\n<td>Combinatorial optimization problem of determining the sequence in which multiple tables should be joined to minimize total execution cost</td>\n<td>Central challenge addressed in Milestone 3</td>\n</tr>\n<tr>\n<td>dynamic programming</td>\n<td>Algorithmic technique for join order optimization that builds optimal solutions for table subsets by combining previously computed optimal sub-solutions, avoiding redundant cost calculations</td>\n<td>Core algorithm approach in Milestone 3</td>\n</tr>\n<tr>\n<td>search space pruning</td>\n<td>Optimization technique that eliminates clearly suboptimal execution plans early in the enumeration process to reduce computational complexity without missing the optimal solution</td>\n<td>Performance optimization in Milestone 3</td>\n</tr>\n<tr>\n<td>left-deep tree</td>\n<td>Join execution plan topology where the right input of every join operation is a base table rather than an intermediate result, simplifying optimization but potentially missing better solutions</td>\n<td>Plan structure option in join ordering</td>\n</tr>\n<tr>\n<td>bushy tree</td>\n<td>Join execution plan topology allowing intermediate join results as both left and right inputs to subsequent joins, expanding the search space but potentially finding better optimization opportunities</td>\n<td>Alternative plan structure in join ordering</td>\n</tr>\n<tr>\n<td>cross product</td>\n<td>Join operation between tables without connecting predicates, resulting in Cartesian product with potentially enormous intermediate results that should typically be avoided</td>\n<td>Degenerate case handled in join optimization</td>\n</tr>\n<tr>\n<td>memoization</td>\n<td>Caching technique that stores previously computed optimization results (costs, optimal plans) to avoid redundant calculations when the same subproblems are encountered</td>\n<td>Performance optimization in dynamic programming</td>\n</tr>\n</tbody></table>\n<h3 id=\"plan-generation-and-physical-selection-terms\">Plan Generation and Physical Selection Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>physical planning</td>\n<td>Process of selecting concrete physical operators and access methods for each logical operation in a query plan, transforming abstract specifications into executable implementations</td>\n<td>Primary focus of Milestone 4</td>\n</tr>\n<tr>\n<td>access method selection</td>\n<td>Decision process for choosing between different techniques (sequential scan, index scan, bitmap scan) for retrieving rows from base tables based on selectivity and available indexes</td>\n<td>Key decision in physical planning</td>\n</tr>\n<tr>\n<td>join algorithm selection</td>\n<td>Process of choosing specific join implementation (hash join, nested loop join, merge join) based on input characteristics like table sizes, memory availability, and sort requirements</td>\n<td>Algorithm selection in Milestone 4</td>\n</tr>\n<tr>\n<td>predicate pushdown</td>\n<td>Query optimization rule that moves filter operations closer to data sources in the execution tree, reducing intermediate result sizes and overall execution cost</td>\n<td>Optimization rule applied in physical planning</td>\n</tr>\n<tr>\n<td>rule-based optimization</td>\n<td>Query transformation approach that applies predetermined heuristic rules (predicate pushdown, projection elimination) to improve plan efficiency without cost-based analysis</td>\n<td>Complementary technique to cost-based optimization</td>\n</tr>\n</tbody></table>\n<h3 id=\"tree-structure-and-traversal-terms\">Tree Structure and Traversal Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>tree traversal</td>\n<td>Systematic algorithm for visiting every node in a tree data structure, essential for operations like cost calculation, schema propagation, and plan validation</td>\n<td>Fundamental operation in Milestone 1</td>\n</tr>\n<tr>\n<td>preorder traversal</td>\n<td>Tree visiting algorithm that processes each node before visiting its children, useful for operations that require parent context before processing children</td>\n<td>Traversal pattern for plan operations</td>\n</tr>\n<tr>\n<td>postorder traversal</td>\n<td>Tree visiting algorithm that processes children before processing their parent, essential for bottom-up cost accumulation and resource requirement calculation</td>\n<td>Traversal pattern for cost estimation</td>\n</tr>\n<tr>\n<td>cost accumulation</td>\n<td>Bottom-up process of calculating total execution cost by combining costs from child operators with the current operator&#39;s processing cost</td>\n<td>Cost calculation process in tree traversal</td>\n</tr>\n<tr>\n<td>schema propagation</td>\n<td>Top-down process of determining output column lists and data types by flowing schema information from data sources through operator transformations</td>\n<td>Schema validation in plan trees</td>\n</tr>\n</tbody></table>\n<h3 id=\"caching-and-performance-terms\">Caching and Performance Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>plan caching</td>\n<td>Performance optimization technique that stores previously optimized execution plans in memory to avoid repeating expensive optimization for similar queries</td>\n<td>Performance enhancement in Milestone 4</td>\n</tr>\n<tr>\n<td>cache invalidation</td>\n<td>Process of removing stored execution plans from cache when underlying table structure or statistics change, ensuring cached plans remain valid</td>\n<td>Cache maintenance strategy</td>\n</tr>\n<tr>\n<td>plan template</td>\n<td>Parameterized execution plan structure that can be instantiated with different literal values while maintaining the same operator structure and join ordering</td>\n<td>Advanced caching technique</td>\n</tr>\n<tr>\n<td>optimization pipeline</td>\n<td>Sequential workflow of transformation phases that converts SQL queries into optimized execution plans through parsing, logical planning, cost estimation, and physical selection</td>\n<td>Overall system workflow</td>\n</tr>\n<tr>\n<td>parametric matching</td>\n<td>Cache lookup technique that matches queries with identical structure but different literal values to cached plan templates</td>\n<td>Cache optimization strategy</td>\n</tr>\n</tbody></table>\n<h3 id=\"statistics-and-data-quality-terms\">Statistics and Data Quality Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>statistics staleness</td>\n<td>Condition where stored statistical information about tables becomes outdated due to data modifications, leading to inaccurate cost estimates and suboptimal plan selection</td>\n<td>Data quality challenge throughout optimization</td>\n</tr>\n<tr>\n<td>confidence level</td>\n<td>Quantitative measure (0.0 to 1.0) indicating the reliability of cost estimates and statistical information, used to adjust optimization decisions under uncertainty</td>\n<td>Quality metric for cost estimation</td>\n</tr>\n<tr>\n<td>fallback strategy</td>\n<td>Alternative optimization approach used when primary cost-based methods fail due to missing statistics, query complexity, or resource constraints</td>\n<td>Error handling strategy</td>\n</tr>\n<tr>\n<td>degenerate query</td>\n<td>Query with problematic patterns like cross products, missing join predicates, or extremely selective filters that challenge standard optimization techniques</td>\n<td>Edge case handling</td>\n</tr>\n<tr>\n<td>uncertainty margin</td>\n<td>Additional cost buffer added to estimates when confidence levels are low, helping to avoid severely underestimating actual execution costs</td>\n<td>Risk management in cost estimation</td>\n</tr>\n</tbody></table>\n<h3 id=\"testing-and-validation-terms\">Testing and Validation Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>plan quality validation</td>\n<td>Systematic process of verifying that the query optimizer produces reasonable and efficient execution plans for various query patterns and data characteristics</td>\n<td>Quality assurance across all milestones</td>\n</tr>\n<tr>\n<td>benchmark testing</td>\n<td>Standardized evaluation approach using representative query workloads to measure optimizer effectiveness and identify performance regressions</td>\n<td>System validation methodology</td>\n</tr>\n<tr>\n<td>regression testing</td>\n<td>Automated testing process that ensures code changes don&#39;t degrade plan quality or introduce optimization bugs</td>\n<td>Continuous validation strategy</td>\n</tr>\n<tr>\n<td>milestone checkpoints</td>\n<td>Structured validation points that verify successful completion of implementation phases with specific behavioral requirements</td>\n<td>Implementation progress tracking</td>\n</tr>\n<tr>\n<td>component unit testing</td>\n<td>Testing methodology that validates individual optimizer modules (cost estimator, join optimizer, plan builder) in isolation from other components</td>\n<td>Development testing strategy</td>\n</tr>\n</tbody></table>\n<h3 id=\"debugging-and-analysis-terms\">Debugging and Analysis Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>cost estimation debugging</td>\n<td>Systematic approach to diagnosing and fixing issues with selectivity calculations, cardinality estimates, and cost model accuracy</td>\n<td>Debugging focus for Milestone 2</td>\n</tr>\n<tr>\n<td>join ordering debugging</td>\n<td>Troubleshooting methodology for dynamic programming implementation issues, suboptimal join order selection, and search space enumeration problems</td>\n<td>Debugging focus for Milestone 3</td>\n</tr>\n<tr>\n<td>plan generation debugging</td>\n<td>Diagnostic approach for issues with tree construction, operator selection, schema propagation, and plan validation</td>\n<td>Debugging focus for Milestones 1 and 4</td>\n</tr>\n<tr>\n<td>plan validation</td>\n<td>Process of verifying that generated execution plans are structurally correct, semantically valid, and contain all necessary operators</td>\n<td>Quality control process</td>\n</tr>\n<tr>\n<td>debug tracing</td>\n<td>Technique of collecting detailed execution information during optimization to analyze decision points, cost calculations, and algorithm behavior</td>\n<td>Debugging methodology</td>\n</tr>\n</tbody></table>\n<h3 id=\"advanced-optimization-terms\">Advanced Optimization Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>multi-dimensional histograms</td>\n<td>Advanced statistical structures that capture correlations between multiple columns, enabling more accurate selectivity estimation for complex predicates</td>\n<td>Future extension capability</td>\n</tr>\n<tr>\n<td>correlation detection</td>\n<td>Automated process of identifying statistical relationships between table columns that affect selectivity estimation accuracy</td>\n<td>Advanced statistics feature</td>\n</tr>\n<tr>\n<td>machine learning-based estimation</td>\n<td>Approach using trained predictive models instead of traditional statistical formulas to estimate query cardinalities and execution costs</td>\n<td>Research-level enhancement</td>\n</tr>\n<tr>\n<td>parallel cost modeling</td>\n<td>Resource-aware cost estimation that considers multi-threaded execution, coordination overhead, and resource contention in parallel query processing</td>\n<td>Advanced cost modeling</td>\n</tr>\n<tr>\n<td>resource-aware optimization</td>\n<td>Plan selection strategy that considers current system resource availability (CPU, memory, I/O bandwidth) in addition to traditional cost estimates</td>\n<td>System-aware optimization</td>\n</tr>\n<tr>\n<td>exchange operators</td>\n<td>Special operator nodes that represent data movement boundaries between different parallel execution regions in distributed or multi-threaded query processing</td>\n<td>Parallel processing support</td>\n</tr>\n<tr>\n<td>execution feedback collection</td>\n<td>Process of gathering actual performance metrics during query execution to validate and improve cost model accuracy</td>\n<td>Adaptive optimization support</td>\n</tr>\n<tr>\n<td>adaptive statistics maintenance</td>\n<td>Dynamic approach to updating table statistics based on query execution feedback rather than fixed maintenance schedules</td>\n<td>Self-tuning statistics</td>\n</tr>\n<tr>\n<td>runtime adaptive optimization</td>\n<td>Advanced technique that modifies execution plans during query execution based on actual performance measurements and intermediate result characteristics</td>\n<td>Research-level optimization</td>\n</tr>\n</tbody></table>\n<h3 id=\"system-architecture-terms\">System Architecture Terms</h3>\n<table>\n<thead>\n<tr>\n<th>Term</th>\n<th>Definition</th>\n<th>Context of Use</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>degree of parallelism</td>\n<td>Number of concurrent threads or processes executing a particular database operation, affecting both performance and resource consumption</td>\n<td>Parallel execution parameter</td>\n</tr>\n<tr>\n<td>coordination overhead</td>\n<td>Additional cost incurred when synchronizing parallel execution threads, including communication, synchronization, and load balancing expenses</td>\n<td>Parallel cost modeling factor</td>\n</tr>\n<tr>\n<td>resource contention</td>\n<td>Performance degradation that occurs when multiple operations compete for limited system resources like CPU, memory, or I/O bandwidth</td>\n<td>System performance consideration</td>\n</tr>\n<tr>\n<td>execution monitoring</td>\n<td>Instrumentation framework that collects detailed runtime performance data during query execution for analysis and optimization improvement</td>\n<td>Performance analysis infrastructure</td>\n</tr>\n<tr>\n<td>feedback processing</td>\n<td>Analysis workflow that examines actual execution statistics to identify cost model inaccuracies and update optimization parameters</td>\n<td>Continuous improvement process</td>\n</tr>\n</tbody></table>\n<h3 id=\"implementation-guidance\">Implementation Guidance</h3>\n<h4 id=\"essential-concepts-reference\">Essential Concepts Reference</h4>\n<p>The glossary serves multiple purposes during implementation. First, use it as a consistency check when naming variables, functions, and data structures. The terminology should align with established database literature to ensure your code is maintainable by other developers familiar with query optimization concepts.</p>\n<p>Second, refer to the glossary when reading academic papers or database system documentation. Understanding the precise technical meaning of terms like &quot;selectivity&quot; versus &quot;cardinality&quot; or &quot;left-deep&quot; versus &quot;bushy&quot; trees is crucial for implementing algorithms correctly.</p>\n<p>Third, use the glossary to validate your understanding of complex interactions. For example, the relationship between &quot;statistics staleness,&quot; &quot;confidence level,&quot; and &quot;uncertainty margin&quot; represents a sophisticated approach to handling estimation uncertainty that goes beyond basic cost calculation.</p>\n<h4 id=\"terminology-usage-guidelines\">Terminology Usage Guidelines</h4>\n<table>\n<thead>\n<tr>\n<th>Context</th>\n<th>Preferred Terms</th>\n<th>Avoid</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Plan Structure</td>\n<td>&quot;operator node,&quot; &quot;plan tree,&quot; &quot;preorder traversal&quot;</td>\n<td>&quot;query node,&quot; &quot;plan graph,&quot; &quot;tree walk&quot;</td>\n</tr>\n<tr>\n<td>Cost Modeling</td>\n<td>&quot;cardinality estimation,&quot; &quot;selectivity,&quot; &quot;cost accumulation&quot;</td>\n<td>&quot;row count guessing,&quot; &quot;filter ratio,&quot; &quot;cost adding&quot;</td>\n</tr>\n<tr>\n<td>Join Processing</td>\n<td>&quot;join ordering,&quot; &quot;dynamic programming,&quot; &quot;search space pruning&quot;</td>\n<td>&quot;join optimization,&quot; &quot;DP algorithm,&quot; &quot;branch cutting&quot;</td>\n</tr>\n<tr>\n<td>Physical Planning</td>\n<td>&quot;access method selection,&quot; &quot;predicate pushdown,&quot; &quot;physical operator&quot;</td>\n<td>&quot;scan choosing,&quot; &quot;filter moving,&quot; &quot;concrete operator&quot;</td>\n</tr>\n</tbody></table>\n<h4 id=\"common-documentation-mistakes\">Common Documentation Mistakes</h4>\n<p>When documenting your implementation, avoid these terminology errors:</p>\n<p>⚠️ <strong>Pitfall: Mixing Logical and Physical Concepts</strong>\nDon&#39;t describe logical operators using physical implementation details. A logical JOIN specifies that two relations should be combined based on predicates, while a physical HASH_JOIN specifies the algorithm. Keep these concepts separate in your documentation.</p>\n<p>⚠️ <strong>Pitfall: Imprecise Statistical Terms</strong>\nDon&#39;t use &quot;cardinality&quot; and &quot;selectivity&quot; interchangeably. Cardinality is an absolute count (1000 rows), while selectivity is a fraction (0.1 or 10%). Cost estimation algorithms depend on this distinction.</p>\n<p>⚠️ <strong>Pitfall: Vague Optimization Language</strong>\nInstead of saying &quot;the optimizer chooses the best plan,&quot; specify whether you mean &quot;lowest estimated cost,&quot; &quot;shortest estimated execution time,&quot; or &quot;minimal resource consumption.&quot; Different optimization objectives can lead to different plan choices.</p>\n<h4 id=\"implementation-cross-references\">Implementation Cross-References</h4>\n<table>\n<thead>\n<tr>\n<th>Glossary Term</th>\n<th>Primary Implementation</th>\n<th>Secondary Usage</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>ExecutionPlan</code></td>\n<td>Plan Representation Component</td>\n<td>Used throughout all components</td>\n</tr>\n<tr>\n<td><code>CostEstimate</code></td>\n<td>Cost Estimation Component</td>\n<td>Referenced in join optimization and physical planning</td>\n</tr>\n<tr>\n<td><code>JoinOrder</code></td>\n<td>Join Optimization Component</td>\n<td>Input to physical planning</td>\n</tr>\n<tr>\n<td><code>TableStatistics</code></td>\n<td>Cost Estimation Component</td>\n<td>Referenced in all optimization phases</td>\n</tr>\n<tr>\n<td><code>OperatorNode</code></td>\n<td>Plan Representation Component</td>\n<td>Manipulated by all optimization components</td>\n</tr>\n</tbody></table>\n<h4 id=\"milestone-integration-points\">Milestone Integration Points</h4>\n<p>The glossary terms map directly to implementation milestones:</p>\n<p><strong>Milestone 1 Terms</strong>: <code>ExecutionPlan</code>, <code>OperatorNode</code>, <code>plan tree</code>, <code>tree traversal</code>, <code>preorder traversal</code>, <code>postorder traversal</code>, <code>logical operator</code>, <code>physical operator</code></p>\n<p><strong>Milestone 2 Terms</strong>: <code>CostEstimate</code>, <code>TableStatistics</code>, <code>ColumnStatistics</code>, <code>selectivity</code>, <code>cardinality</code>, <code>cost estimation</code>, <code>histogram</code>, <code>statistics collection</code></p>\n<p><strong>Milestone 3 Terms</strong>: <code>JoinOrder</code>, <code>join ordering</code>, <code>dynamic programming</code>, <code>search space pruning</code>, <code>left-deep tree</code>, <code>bushy tree</code>, <code>memoization</code></p>\n<p><strong>Milestone 4 Terms</strong>: <code>physical planning</code>, <code>access method selection</code>, <code>join algorithm selection</code>, <code>predicate pushdown</code>, <code>plan caching</code>, <code>optimization pipeline</code></p>\n<p>Understanding these term relationships helps you organize your implementation and ensures consistent vocabulary across milestone deliverables.</p>\n","toc":[{"level":1,"text":"Query Optimizer: Design Document","id":"query-optimizer-design-document"},{"level":2,"text":"Overview","id":"overview"},{"level":2,"text":"Context and Problem Statement","id":"context-and-problem-statement"},{"level":3,"text":"The Route Planning Analogy","id":"the-route-planning-analogy"},{"level":3,"text":"Core Optimization Challenges","id":"core-optimization-challenges"},{"level":4,"text":"Exponential Search Space Complexity","id":"exponential-search-space-complexity"},{"level":4,"text":"Statistical Estimation Accuracy","id":"statistical-estimation-accuracy"},{"level":4,"text":"Optimization Time vs. Plan Quality Trade-offs","id":"optimization-time-vs-plan-quality-trade-offs"},{"level":3,"text":"Industry Approaches Comparison","id":"industry-approaches-comparison"},{"level":4,"text":"Rule-Based Optimization","id":"rule-based-optimization"},{"level":4,"text":"Cost-Based Optimization","id":"cost-based-optimization"},{"level":4,"text":"Heuristic and Hybrid Approaches","id":"heuristic-and-hybrid-approaches"},{"level":2,"text":"Goals and Non-Goals","id":"goals-and-non-goals"},{"level":3,"text":"Primary Goals","id":"primary-goals"},{"level":3,"text":"Explicit Non-Goals","id":"explicit-non-goals"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"High-Level Architecture","id":"high-level-architecture"},{"level":3,"text":"Component Responsibilities","id":"component-responsibilities"},{"level":3,"text":"Optimization Pipeline","id":"optimization-pipeline"},{"level":3,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Data Model","id":"data-model"},{"level":3,"text":"Plan Tree and Operator Nodes","id":"plan-tree-and-operator-nodes"},{"level":3,"text":"Table and Column Statistics","id":"table-and-column-statistics"},{"level":3,"text":"Cost Model Components","id":"cost-model-components"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Plan Representation Component","id":"plan-representation-component"},{"level":3,"text":"Mental Model: Construction Blueprint","id":"mental-model-construction-blueprint"},{"level":3,"text":"Operator Type System","id":"operator-type-system"},{"level":4,"text":"Logical vs Physical Operator Distinction","id":"logical-vs-physical-operator-distinction"},{"level":4,"text":"Core Operator Types","id":"core-operator-types"},{"level":4,"text":"Operator Node Structure","id":"operator-node-structure"},{"level":4,"text":"Cost Annotation System","id":"cost-annotation-system"},{"level":3,"text":"Tree Traversal and Manipulation","id":"tree-traversal-and-manipulation"},{"level":4,"text":"Tree Traversal Methods","id":"tree-traversal-methods"},{"level":4,"text":"Plan Tree Manipulation","id":"plan-tree-manipulation"},{"level":4,"text":"Plan Tree Validation","id":"plan-tree-validation"},{"level":4,"text":"Pretty Printing and Debugging","id":"pretty-printing-and-debugging"},{"level":3,"text":"Architecture Decision: Tree vs DAG","id":"architecture-decision-tree-vs-dag"},{"level":4,"text":"Tree Structure Implications","id":"tree-structure-implications"},{"level":3,"text":"Common Implementation Pitfalls","id":"common-implementation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoint","id":"milestone-checkpoint"},{"level":2,"text":"Cost Estimation Component","id":"cost-estimation-component"},{"level":3,"text":"Mental Model: Construction Estimating","id":"mental-model-construction-estimating"},{"level":3,"text":"Statistics Collection and Maintenance","id":"statistics-collection-and-maintenance"},{"level":3,"text":"Selectivity and Cardinality Estimation","id":"selectivity-and-cardinality-estimation"},{"level":3,"text":"I/O and CPU Cost Modeling","id":"io-and-cpu-cost-modeling"},{"level":3,"text":"Architecture Decision: Histogram vs Uniform Distribution","id":"architecture-decision-histogram-vs-uniform-distribution"},{"level":3,"text":"Common Estimation Pitfalls","id":"common-estimation-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Join Optimization Component","id":"join-optimization-component"},{"level":3,"text":"Mental Model: Assembly Line Optimization","id":"mental-model-assembly-line-optimization"},{"level":3,"text":"Dynamic Programming Algorithm","id":"dynamic-programming-algorithm"},{"level":3,"text":"Search Space Pruning Strategies","id":"search-space-pruning-strategies"},{"level":3,"text":"Left-Deep vs Bushy Join Trees","id":"left-deep-vs-bushy-join-trees"},{"level":3,"text":"Architecture Decision: Full Enumeration vs Heuristics","id":"architecture-decision-full-enumeration-vs-heuristics"},{"level":3,"text":"Common Join Ordering Pitfalls","id":"common-join-ordering-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Physical Planning Component","id":"physical-planning-component"},{"level":3,"text":"Mental Model: Tool Selection","id":"mental-model-tool-selection"},{"level":3,"text":"Index vs Sequential Scan Selection","id":"index-vs-sequential-scan-selection"},{"level":3,"text":"Join Algorithm Selection","id":"join-algorithm-selection"},{"level":3,"text":"Optimization Rules and Predicate Pushdown","id":"optimization-rules-and-predicate-pushdown"},{"level":3,"text":"Architecture Decision: Rule-Based vs Cost-Based Physical Selection","id":"architecture-decision-rule-based-vs-cost-based-physical-selection"},{"level":3,"text":"Common Physical Planning Pitfalls","id":"common-physical-planning-pitfalls"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Interactions and Data Flow","id":"interactions-and-data-flow"},{"level":3,"text":"Complete Optimization Sequence","id":"complete-optimization-sequence"},{"level":4,"text":"Phase 1: Query Parsing and Initial Structure","id":"phase-1-query-parsing-and-initial-structure"},{"level":4,"text":"Phase 2: Logical Plan Construction","id":"phase-2-logical-plan-construction"},{"level":4,"text":"Phase 3: Cost Estimation and Statistics Integration","id":"phase-3-cost-estimation-and-statistics-integration"},{"level":4,"text":"Phase 4: Join Order Optimization","id":"phase-4-join-order-optimization"},{"level":4,"text":"Phase 5: Physical Operator Selection","id":"phase-5-physical-operator-selection"},{"level":3,"text":"Component Communication Interfaces","id":"component-communication-interfaces"},{"level":4,"text":"Primary Component Interfaces","id":"primary-component-interfaces"},{"level":4,"text":"Plan Builder Interface Details","id":"plan-builder-interface-details"},{"level":4,"text":"Cost Estimator Interface Details","id":"cost-estimator-interface-details"},{"level":4,"text":"Join Optimizer Interface Details","id":"join-optimizer-interface-details"},{"level":4,"text":"Physical Planner Interface Details","id":"physical-planner-interface-details"},{"level":4,"text":"Data Structure Exchange Formats","id":"data-structure-exchange-formats"},{"level":4,"text":"Error Handling and Component Communication","id":"error-handling-and-component-communication"},{"level":3,"text":"Plan Caching and Reuse","id":"plan-caching-and-reuse"},{"level":4,"text":"Plan Identification and Matching","id":"plan-identification-and-matching"},{"level":4,"text":"Syntactic Plan Matching","id":"syntactic-plan-matching"},{"level":4,"text":"Structural Plan Matching","id":"structural-plan-matching"},{"level":4,"text":"Parametric Plan Templates","id":"parametric-plan-templates"},{"level":4,"text":"Cache Invalidation and Consistency","id":"cache-invalidation-and-consistency"},{"level":4,"text":"Cache Storage and Memory Management","id":"cache-storage-and-memory-management"},{"level":4,"text":"Plan Cache Implementation Architecture","id":"plan-cache-implementation-architecture"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"File Structure for Optimization Pipeline","id":"file-structure-for-optimization-pipeline"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Pipeline Implementation Skeleton","id":"core-pipeline-implementation-skeleton"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Error Handling and Edge Cases","id":"error-handling-and-edge-cases"},{"level":3,"text":"Optimization Failure Modes","id":"optimization-failure-modes"},{"level":4,"text":"Resource Exhaustion Scenarios","id":"resource-exhaustion-scenarios"},{"level":4,"text":"Statistical Accuracy Failures","id":"statistical-accuracy-failures"},{"level":4,"text":"Component Integration Failures","id":"component-integration-failures"},{"level":3,"text":"Degenerate Query Patterns","id":"degenerate-query-patterns"},{"level":4,"text":"Cross Product Queries","id":"cross-product-queries"},{"level":4,"text":"Extremely Selective Filters","id":"extremely-selective-filters"},{"level":4,"text":"Queries with No Valid Indexes","id":"queries-with-no-valid-indexes"},{"level":3,"text":"Stale Statistics Handling","id":"stale-statistics-handling"},{"level":4,"text":"Staleness Detection Strategies","id":"staleness-detection-strategies"},{"level":4,"text":"Statistics Refresh Strategies","id":"statistics-refresh-strategies"},{"level":4,"text":"Confidence-Based Decision Making","id":"confidence-based-decision-making"},{"level":4,"text":"Default Statistics Generation","id":"default-statistics-generation"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Infrastructure Starter Code","id":"infrastructure-starter-code"},{"level":4,"text":"Core Logic Skeleton Code","id":"core-logic-skeleton-code"},{"level":4,"text":"Milestone Checkpoints","id":"milestone-checkpoints"},{"level":4,"text":"Debugging Tips","id":"debugging-tips"},{"level":2,"text":"Testing Strategy","id":"testing-strategy"},{"level":3,"text":"Mental Model: Quality Assurance in Manufacturing","id":"mental-model-quality-assurance-in-manufacturing"},{"level":3,"text":"Component Unit Testing","id":"component-unit-testing"},{"level":3,"text":"Plan Quality Validation","id":"plan-quality-validation"},{"level":3,"text":"Milestone Validation Checkpoints","id":"milestone-validation-checkpoints"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":2,"text":"Debugging Guide","id":"debugging-guide"},{"level":3,"text":"Mental Model: Medical Diagnostics","id":"mental-model-medical-diagnostics"},{"level":3,"text":"Cost Estimation Debugging","id":"cost-estimation-debugging"},{"level":4,"text":"Statistics Validation and Debugging","id":"statistics-validation-and-debugging"},{"level":4,"text":"Selectivity Estimation Diagnosis","id":"selectivity-estimation-diagnosis"},{"level":4,"text":"Cardinality Estimation Troubleshooting","id":"cardinality-estimation-troubleshooting"},{"level":4,"text":"Cost Model Validation","id":"cost-model-validation"},{"level":3,"text":"Join Ordering Debugging","id":"join-ordering-debugging"},{"level":4,"text":"Dynamic Programming Algorithm Verification","id":"dynamic-programming-algorithm-verification"},{"level":4,"text":"Cost Comparison and Plan Selection","id":"cost-comparison-and-plan-selection"},{"level":4,"text":"Search Space Management and Pruning","id":"search-space-management-and-pruning"},{"level":4,"text":"Multi-Table Query Analysis","id":"multi-table-query-analysis"},{"level":3,"text":"Plan Generation Debugging","id":"plan-generation-debugging"},{"level":4,"text":"Tree Construction and Structure Validation","id":"tree-construction-and-structure-validation"},{"level":4,"text":"Operator Selection and Configuration","id":"operator-selection-and-configuration"},{"level":4,"text":"Schema Propagation and Type Safety","id":"schema-propagation-and-type-safety"},{"level":4,"text":"Cost Accumulation and Validation","id":"cost-accumulation-and-validation"},{"level":4,"text":"Plan Validation and Correctness Checks","id":"plan-validation-and-correctness-checks"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure","id":"recommended-file-structure"},{"level":4,"text":"Cost Estimation Debugging Infrastructure","id":"cost-estimation-debugging-infrastructure"},{"level":4,"text":"Join Ordering Debugging Tools","id":"join-ordering-debugging-tools"},{"level":4,"text":"Plan Generation Debugging Support","id":"plan-generation-debugging-support"},{"level":4,"text":"Debugging Command-Line Interface","id":"debugging-command-line-interface"},{"level":4,"text":"Milestone Debugging Checkpoints","id":"milestone-debugging-checkpoints"},{"level":2,"text":"Future Extensions","id":"future-extensions"},{"level":3,"text":"Mental Model: Architectural Evolution","id":"mental-model-architectural-evolution"},{"level":3,"text":"Advanced Statistical Models","id":"advanced-statistical-models"},{"level":3,"text":"Parallel Query Support","id":"parallel-query-support"},{"level":3,"text":"Runtime Adaptive Optimization","id":"runtime-adaptive-optimization"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Technology Recommendations","id":"technology-recommendations"},{"level":4,"text":"Recommended File Structure Extension","id":"recommended-file-structure-extension"},{"level":4,"text":"Advanced Statistics Infrastructure","id":"advanced-statistics-infrastructure"},{"level":4,"text":"Parallel Execution Framework","id":"parallel-execution-framework"},{"level":4,"text":"Adaptive Optimization Infrastructure","id":"adaptive-optimization-infrastructure"},{"level":4,"text":"Milestone Checkpoints for Extensions","id":"milestone-checkpoints-for-extensions"},{"level":4,"text":"Integration with Core Optimizer","id":"integration-with-core-optimizer"},{"level":2,"text":"Glossary","id":"glossary"},{"level":3,"text":"Mental Model: Dictionary for a Technical Language","id":"mental-model-dictionary-for-a-technical-language"},{"level":3,"text":"Core Query Optimization Concepts","id":"core-query-optimization-concepts"},{"level":3,"text":"Statistical and Cost Estimation Terms","id":"statistical-and-cost-estimation-terms"},{"level":3,"text":"Join Processing and Optimization Terms","id":"join-processing-and-optimization-terms"},{"level":3,"text":"Plan Generation and Physical Selection Terms","id":"plan-generation-and-physical-selection-terms"},{"level":3,"text":"Tree Structure and Traversal Terms","id":"tree-structure-and-traversal-terms"},{"level":3,"text":"Caching and Performance Terms","id":"caching-and-performance-terms"},{"level":3,"text":"Statistics and Data Quality Terms","id":"statistics-and-data-quality-terms"},{"level":3,"text":"Testing and Validation Terms","id":"testing-and-validation-terms"},{"level":3,"text":"Debugging and Analysis Terms","id":"debugging-and-analysis-terms"},{"level":3,"text":"Advanced Optimization Terms","id":"advanced-optimization-terms"},{"level":3,"text":"System Architecture Terms","id":"system-architecture-terms"},{"level":3,"text":"Implementation Guidance","id":"implementation-guidance"},{"level":4,"text":"Essential Concepts Reference","id":"essential-concepts-reference"},{"level":4,"text":"Terminology Usage Guidelines","id":"terminology-usage-guidelines"},{"level":4,"text":"Common Documentation Mistakes","id":"common-documentation-mistakes"},{"level":4,"text":"Implementation Cross-References","id":"implementation-cross-references"},{"level":4,"text":"Milestone Integration Points","id":"milestone-integration-points"}],"title":"Query Optimizer: Design Document","markdown":"# Query Optimizer: Design Document\n\n\n## Overview\n\nA query optimizer transforms SQL queries into efficient execution plans by building plan trees, estimating costs, and selecting optimal join orders. The key architectural challenge is balancing optimization time against plan quality while handling combinatorial explosion in the search space.\n\n\n> This guide is meant to help you understand the big picture before diving into each milestone. Refer back to it whenever you need context on how components connect.\n\n\n## Context and Problem Statement\n\n> **Milestone(s):** Foundation for all milestones - establishes the fundamental optimization challenges that drive the design decisions in Milestones 1-4.\n\n### The Route Planning Analogy\n\nImagine you're planning a road trip that visits multiple cities, and you want to minimize both travel time and fuel costs. A naive approach might visit cities in alphabetical order or in the sequence they were added to your itinerary. However, an intelligent route planner considers multiple factors: current traffic conditions, road quality, fuel station locations, and the interconnections between different route segments. The planner explores numerous possible routes, estimates the cost of each alternative, and selects the most efficient path.\n\nQuery optimization follows remarkably similar principles. When a database receives a SQL query joining multiple tables with various filters and projections, there are often thousands or millions of different ways to execute that query. A naive database engine might process tables in the order they appear in the SQL statement, apply filters after performing expensive joins, or always use sequential scans regardless of available indexes. Just as the alphabetical city visit would likely produce a terrible road trip, these naive execution strategies often result in query performance that is orders of magnitude slower than optimal.\n\nThe **query optimizer** serves as the database's intelligent route planner. It examines the logical requirements expressed in SQL (the destination cities), considers the available physical resources (roads, indexes, memory), estimates the cost of different execution strategies (routes), and constructs an efficient execution plan (the optimal route). The optimizer must balance exploration time against plan quality - spending too much time exploring alternatives defeats the purpose, while insufficient exploration leads to poor execution plans.\n\nThis analogy reveals why query optimization is both essential and challenging. Unlike static route planning where road conditions change gradually, database conditions fluctuate rapidly: table sizes grow, indexes are added or dropped, memory availability changes, and concurrent queries compete for resources. The optimizer must make decisions quickly based on statistical estimates rather than perfect information, yet these decisions directly determine whether a query completes in milliseconds or hours.\n\n### Core Optimization Challenges\n\nQuery optimization confronts three fundamental challenges that make it one of the most complex problems in database system design. Understanding these challenges is crucial because they drive every architectural decision in our optimizer implementation.\n\n#### Exponential Search Space Complexity\n\nThe most daunting challenge in query optimization is the **combinatorial explosion** of possible execution plans. For a query joining `n` tables, there are `n!` possible join orders. A five-table join has 120 possible orders; a ten-table join has over 3.6 million possibilities. Each join order can be executed with different physical operators (hash join, nested loop join, merge join), different access methods (sequential scan, index scan), and different intermediate result materializations.\n\nConsider a seemingly simple query joining four tables - `Orders`, `Customers`, `Products`, and `OrderItems`. The optimizer must decide:\n- Which table to scan first and with what access method\n- Which pairs of tables to join in what sequence  \n- What join algorithm to use for each join operation\n- Where to apply filter predicates for maximum efficiency\n- Whether to sort results early or late in the execution pipeline\n\nEven this four-table query generates hundreds of distinct execution plans. The optimizer cannot afford to evaluate every possibility, yet missing the optimal plan can mean the difference between a query that runs in seconds versus one that runs for hours.\n\n> **Critical Insight**: The exponential search space means that query optimization is fundamentally about intelligent pruning and approximation rather than exhaustive search. Production optimizers use dynamic programming, heuristics, and early pruning to navigate this complexity.\n\n#### Statistical Estimation Accuracy\n\nQuery optimizers operate in an environment of **fundamental uncertainty**. To choose between execution plans, the optimizer must predict how much each plan will cost - but this prediction requires knowing how many rows each operation will process, how selective each filter will be, and how much memory each operator will consume. These predictions are made using statistical approximations that are often inaccurate.\n\nThe accuracy problem cascades through the optimization process. If the optimizer underestimates the selectivity of an early filter, it might choose a nested loop join expecting small intermediate results. When the actual intermediate result is much larger, the chosen plan becomes catastrophically slow. This estimation error occurs because:\n\n- **Table statistics become stale** as data changes between statistics collection runs\n- **Predicate correlation** is often ignored - the optimizer assumes column values are independent when they're actually correlated\n- **Intermediate result estimation** compounds errors from each preceding operation\n- **Data distribution assumptions** may not match real-world data patterns\n\nFor example, an optimizer might estimate that a filter `WHERE age > 65 AND income > 100000` will be highly selective, assuming age and income are independent. In reality, high-income individuals tend to be older, making this combination less selective than expected. This estimation error can lead to choosing the wrong join algorithm or access method.\n\n#### Optimization Time vs. Plan Quality Trade-offs\n\nThe third fundamental challenge is the **time-quality trade-off**. Optimization itself consumes CPU cycles, memory, and elapsed time. For simple queries that execute in milliseconds, spending seconds optimizing would be counterproductive. Conversely, complex analytical queries that run for hours justify more extensive optimization effort.\n\nThis trade-off manifests in several ways:\n\n- **Search completeness vs. speed**: Complete dynamic programming finds optimal solutions but becomes prohibitively expensive for large join queries\n- **Statistics precision vs. collection overhead**: Detailed histograms provide better estimates but require more storage and maintenance overhead  \n- **Plan caching vs. adaptivity**: Cached plans avoid re-optimization costs but may become suboptimal as data changes\n- **Heuristic rules vs. cost-based decisions**: Rules execute quickly but may miss opportunities that cost-based analysis would find\n\nProduction database systems handle this trade-off through **optimization budgets** - they allocate limited time and memory for optimization based on query complexity and expected execution cost. Simple queries receive basic optimization; complex queries get more sophisticated treatment.\n\n> **Design Principle**: Our optimizer must be architected to support different optimization depths, allowing the system to spend more effort optimizing queries that will benefit most from sophisticated analysis.\n\n### Industry Approaches Comparison\n\nDatabase vendors have developed three primary approaches to query optimization, each representing different solutions to the fundamental challenges outlined above. Understanding these approaches helps contextualize our design decisions and reveals why modern systems typically use hybrid strategies.\n\n#### Rule-Based Optimization\n\n**Rule-based optimizers** apply a fixed set of transformation rules to convert SQL queries into execution plans. These rules are derived from database theory and empirical knowledge about query performance patterns. The optimizer applies rules in a predetermined sequence without considering data statistics or actual execution costs.\n\n| Aspect | Description | Example |\n|--------|-------------|---------|\n| **Decision Making** | Fixed transformation rules applied in sequence | Always push filters before joins; prefer index scans for high selectivity filters |\n| **Cost Consideration** | No cost estimation - decisions based on rule priority | Rule: \"Use index scan if filter selectivity < 10%\" regardless of actual data |\n| **Optimization Time** | Very fast - O(query size) complexity | Optimization completes in microseconds regardless of query complexity |\n| **Plan Quality** | Predictable but often suboptimal | Consistent performance but misses opportunities for complex queries |\n\nEarly database systems like Oracle's original optimizer used purely rule-based approaches. A typical rule might state: \"Always apply selection predicates before join operations\" or \"Use nested loop joins when one input is expected to be small.\" These rules work reasonably well for simple queries and provide predictable optimization time.\n\nHowever, rule-based optimization suffers from **context blindness**. The same rule is applied regardless of table sizes, data distribution, or available indexes. A rule that works well for small tables may be disastrous for large tables. This limitation becomes severe for complex analytical queries where optimal strategies depend heavily on data characteristics.\n\n> **Historical Context**: Rule-based optimization dominated early database systems because statistical information was expensive to collect and maintain. As storage became cheaper and workloads more complex, the limitations of rule-based approaches drove the industry toward cost-based optimization.\n\n#### Cost-Based Optimization\n\n**Cost-based optimizers** make decisions by estimating and comparing the execution cost of different plan alternatives. These systems maintain detailed statistics about table sizes, column value distributions, and index characteristics. For each possible execution plan, the optimizer estimates I/O costs, CPU costs, and memory requirements, then selects the plan with the lowest total estimated cost.\n\n| Aspect | Description | Example |\n|--------|-------------|---------|\n| **Decision Making** | Compare estimated costs of alternative plans | Estimate hash join cost = 3 * (R + S) vs. nested loop cost = R * S for tables R, S |\n| **Statistics Usage** | Extensive reliance on table and column statistics | Uses row counts, distinct values, data distribution histograms for cost estimation |\n| **Search Strategy** | Dynamic programming or heuristic search through plan space | Enumerate all join orders for small queries; use heuristics for complex queries |\n| **Adaptivity** | Plans change as statistics are updated | Same query gets different plans as table sizes grow or indexes are added |\n\nModern systems like PostgreSQL, SQL Server, and Oracle's current optimizer are primarily cost-based. These optimizers collect statistics about table cardinalities, column selectivity, and access method performance. When optimizing a join query, the optimizer estimates the cost of different join orders by calculating expected intermediate result sizes and operation costs.\n\nThe power of cost-based optimization lies in its **data-driven decision making**. As tables grow larger, the optimizer automatically shifts from nested loop joins to hash joins. When new indexes are created, the optimizer incorporates them into cost calculations. This adaptivity allows cost-based systems to handle diverse workloads effectively.\n\nHowever, cost-based optimization introduces new complexities. Statistics must be collected, maintained, and kept reasonably current. Cost models must accurately reflect actual execution behavior across different hardware configurations. Most critically, the exponential search space requires sophisticated algorithms to find good plans without exhaustive enumeration.\n\n#### Heuristic and Hybrid Approaches\n\n**Heuristic optimizers** use problem-specific knowledge to guide the search for good execution plans without exhaustive cost calculation. Modern database systems increasingly adopt **hybrid approaches** that combine rule-based shortcuts, cost-based analysis, and search heuristics to balance optimization time with plan quality.\n\n| Approach | Strategy | Benefits | Limitations |\n|----------|----------|----------|-------------|\n| **Greedy Heuristics** | Make locally optimal choices at each step | Fast optimization; works well for common patterns | May miss globally optimal solutions |\n| **Genetic Algorithms** | Evolve populations of candidate plans | Handles complex search spaces; finds novel solutions | Unpredictable optimization time; complex implementation |\n| **Machine Learning** | Learn optimization patterns from historical data | Adapts to workload patterns; improves over time | Requires training data; black-box decision making |\n| **Hybrid Rule/Cost** | Use rules for simple cases, costs for complex ones | Fast common case; sophisticated complex case | Complex implementation; rule/cost boundary decisions |\n\nPostgreSQL exemplifies the hybrid approach: it uses genetic algorithms for queries with many joins, cost-based optimization for moderately complex queries, and simple heuristics for trivial queries. This multi-tier strategy allows the optimizer to spend effort proportional to query complexity and potential benefit.\n\n**Apache Calcite** represents a modern framework approach, separating logical optimization rules from cost-based physical planning. Rules handle transformations like predicate pushdown and join reordering, while cost-based analysis selects physical operators and access methods. This separation allows individual components to be optimized independently.\n\n> **Decision: Cost-Based Foundation with Heuristic Pruning**\n> - **Context**: Our educational optimizer must balance learning value with implementation complexity while remaining representative of production systems\n> - **Options Considered**: \n>   1. Pure rule-based system (simple but unrealistic)\n>   2. Full cost-based optimization (comprehensive but complex)\n>   3. Hybrid approach with cost-based core and heuristic pruning\n> - **Decision**: Implement cost-based optimization with heuristic pruning for search space management\n> - **Rationale**: Cost-based optimization teaches the fundamental concepts used in production systems while heuristic pruning keeps the implementation manageable. Students learn both cost estimation and search space management.\n> - **Consequences**: Enables realistic query optimization learning while avoiding the complexity of advanced search algorithms like genetic optimization or machine learning approaches.\n\nThe comparison reveals that modern query optimization has evolved toward sophisticated hybrid systems that adapt their strategy based on query characteristics and optimization context. Our implementation will focus on cost-based optimization with strategic use of heuristics, providing hands-on experience with the core techniques that drive production database performance.\n\n\n## Goals and Non-Goals\n\n> **Milestone(s):** Foundation for all milestones - establishes the scope and boundaries that guide implementation decisions across Milestones 1-4.\n\n### Primary Goals\n\nOur query optimizer focuses on implementing the fundamental principles of cost-based optimization that form the backbone of modern database systems. Think of this as building a GPS navigation system for database queries - we need to understand the road network (available execution strategies), estimate travel times (costs), and find efficient routes (execution plans) while keeping the core navigation logic simple and reliable.\n\nThe primary goal is to transform SQL queries into efficient `ExecutionPlan` structures through systematic cost-based decision making. This involves four core capabilities that work together to achieve effective query optimization. First, we must represent query operations as tree structures that capture both logical intent and physical execution strategies. Second, we need accurate cost estimation using `TableStatistics` to predict resource consumption. Third, we require sophisticated join ordering algorithms that use dynamic programming to find optimal `JoinOrder` sequences. Finally, we need physical operator selection that chooses concrete execution strategies based on data characteristics and available access methods.\n\n**Plan Representation and Tree Management**\n\nThe optimizer must create and manipulate hierarchical query plans represented as trees of `OperatorNode` instances. Each node encapsulates a single database operation like scanning tables, applying filters, or performing joins. The system distinguishes between logical operators that express what computation should happen and physical operators that specify how the computation will execute. This separation allows the optimizer to explore multiple implementation strategies for the same logical operation.\n\nThe plan representation system supports complete tree traversal for cost calculation, optimization rule application, and plan visualization. Tree manipulation capabilities include node insertion, removal, and subtree replacement to enable optimization transformations like predicate pushdown. The system maintains parent-child relationships that preserve query semantics while allowing structural modifications during optimization.\n\n**Statistical Cost Estimation**\n\nAccurate cost prediction drives all optimization decisions in our cost-based approach. The `estimateCost` function combines I/O costs from disk access patterns with CPU costs from computational complexity. The system maintains detailed `TableStatistics` including row counts, column cardinalities, and value distributions that enable selectivity estimation for filter predicates and join operations.\n\nCost estimation covers all major resource consumption categories. I/O costs account for sequential scans, random page access, and index traversal patterns. CPU costs model comparison operations, hash computations, and sorting overhead. Memory costs track buffer usage for hash tables and intermediate results. The cost model uses configurable weights that can be tuned for different hardware configurations and workload patterns.\n\n**Join Order Optimization**\n\nMulti-table queries create exponential search spaces where join order dramatically affects execution efficiency. Our `optimizeJoinOrder` function implements dynamic programming algorithms that find optimal join sequences without exhaustive enumeration. The system builds optimal plans for progressively larger table subsets, reusing previously computed results to avoid redundant work.\n\nThe join ordering component handles both simple star schemas and complex multi-join queries with intricate predicate patterns. It identifies cross products early and prunes clearly suboptimal alternatives to reduce search complexity. The algorithm supports both left-deep join trees that minimize memory usage and bushy trees that enable parallel execution opportunities.\n\n**Physical Operator Selection**\n\nThe final optimization phase selects concrete physical operators and access methods for each logical operation. This includes choosing between sequential scans and index lookups based on filter selectivity, selecting join algorithms like hash joins or nested loops based on input characteristics, and applying optimization rules like predicate pushdown to reduce intermediate result sizes.\n\nPhysical selection considers all available access paths including primary key indexes, secondary indexes, and full table scans. Join algorithm selection weighs factors like input cardinalities, memory availability, and sort order requirements. The system applies proven optimization heuristics while maintaining the flexibility to override defaults when cost analysis suggests better alternatives.\n\n**Educational Learning Objectives**\n\nBeyond functional requirements, this query optimizer serves as a comprehensive learning platform for understanding database internals. The implementation exposes the mathematical foundations of cost-based optimization including selectivity estimation formulas, cardinality calculation methods, and dynamic programming algorithms. Students gain hands-on experience with the trade-offs between optimization time and plan quality that drive real-world database design decisions.\n\nThe modular architecture allows incremental implementation where each milestone builds upon previous foundations. This progression mirrors the historical development of query optimization techniques and helps learners understand why modern optimizers evolved their current architectures. The code structure emphasizes clarity and maintainability over performance optimization, making the underlying algorithms accessible to developers new to database systems.\n\n### Explicit Non-Goals\n\nWhile comprehensive within its scope, our query optimizer deliberately excludes several advanced features that would complicate the core learning objectives. These non-goals reflect conscious decisions to maintain focus on fundamental optimization principles rather than pursuing production-level completeness.\n\n**Parallel Query Execution**\n\nModern databases extensively use parallel processing to accelerate query execution across multiple CPU cores and storage devices. However, parallel query optimization introduces significant complexity in cost modeling, resource allocation, and synchronization overhead estimation. Parallel plans require sophisticated scheduling algorithms, partition-aware statistics, and inter-operator communication protocols that would overshadow the core optimization concepts we aim to teach.\n\nOur single-threaded execution model simplifies cost calculations and eliminates complex race conditions during plan generation. This allows students to focus on understanding join ordering algorithms and selectivity estimation without wrestling with parallel programming challenges. The fundamental optimization principles learned in our single-threaded system transfer directly to parallel contexts once the core concepts are mastered.\n\n**Runtime Adaptive Optimization**\n\nProduction optimizers increasingly use runtime feedback to improve cost estimates and plan selection over time. Adaptive optimization tracks actual execution statistics, identifies estimation errors, and adjusts cost models based on observed performance patterns. Some systems even reoptimize queries mid-execution when initial estimates prove significantly inaccurate.\n\nThese adaptive features require complex statistics collection infrastructure, machine learning components for pattern recognition, and sophisticated plan migration mechanisms. Including adaptive optimization would shift focus from fundamental cost-based techniques to machine learning and runtime system design. Our static optimization approach provides a solid foundation that students can extend with adaptive features in future projects.\n\n**Advanced Statistical Models**\n\nReal-world optimizers employ sophisticated statistical techniques including multi-dimensional histograms, correlation detection between columns, and probabilistic data structures for cardinality estimation. These advanced models provide better estimate accuracy but require substantial mathematical background and complex maintenance algorithms.\n\nOur simplified statistics model uses uniform distribution assumptions and independence assumptions between predicates. While less accurate than production systems, this approach makes cost calculation transparent and debuggable. Students can understand every step of the estimation process without advanced statistical knowledge, building intuition for why more sophisticated models become necessary in production environments.\n\n**Materialized View Selection**\n\nQuery optimizers in data warehouse environments consider rewriting queries to use precomputed materialized views when available. This capability requires view matching algorithms, freshness tracking, and complex cost comparisons between base table access and view utilization. Materialized view optimization represents a distinct area of research with its own algorithmic challenges.\n\nExcluding materialized views keeps our focus on fundamental table access patterns and join optimization. Students learn to optimize queries against base tables, understanding the core cost-benefit analysis that drives all optimization decisions. This foundation enables future exploration of view-based optimization techniques.\n\n**Distributed Query Processing**\n\nDistributed databases must optimize queries across multiple network-connected nodes, considering data locality, network transfer costs, and partial failure scenarios. Distributed optimization requires understanding of data partitioning strategies, network cost modeling, and distributed join algorithms like shuffle joins and broadcast joins.\n\nOur single-node optimizer eliminates network complexity and allows focus on local cost optimization. The principles of cost-based decision making and dynamic programming transfer directly to distributed contexts, but students first master these concepts without distributed systems complexity.\n\n**User-Defined Function Integration**\n\nProduction optimizers handle user-defined functions with unknown cost characteristics and potential side effects. This requires sophisticated cost estimation for black-box operations, consideration of function volatility and determinism, and integration with external programming language runtimes.\n\nOur operator set focuses on standard relational operators with well-understood cost characteristics. This restriction enables precise cost modeling and predictable optimization behavior. Students learn optimization principles with full visibility into cost calculations rather than dealing with estimation uncertainty from external functions.\n\n**Memory Management Integration**\n\nAdvanced optimizers consider available memory when selecting physical operators, potentially choosing different join algorithms based on buffer pool size or enabling spill-to-disk strategies for memory-intensive operations. This requires integration with buffer pool managers and sophisticated memory cost modeling.\n\nOur optimizer assumes sufficient memory for chosen operations, simplifying cost calculations and eliminating complex memory management scenarios. This allows focus on algorithmic aspects of optimization rather than resource management concerns. The fundamental cost comparison techniques learned with unlimited memory assumptions extend naturally to memory-constrained environments.\n\n**Query Plan Caching with Invalidation**\n\nProduction systems cache optimized plans across query executions, implementing sophisticated invalidation strategies when underlying table statistics change significantly. Plan caching requires cache key generation from SQL syntax trees, staleness detection algorithms, and memory management for cached plan storage.\n\nWhile our implementation includes basic plan caching for repeated identical queries, we exclude complex invalidation logic and adaptive cache management. This simplification allows focus on optimization algorithms rather than cache management strategies. Students understand the value of plan reuse without implementing production-level cache sophistication.\n\n> **Design Insight**: These non-goals reflect a deliberate pedagogical strategy. By excluding advanced features, we create a focused learning environment where students master fundamental concepts before encountering production complexity. Each excluded feature represents a natural extension point for advanced projects once core optimization skills are developed.\n\nThe scope boundaries ensure that implementation effort concentrates on cost-based optimization principles rather than distributed systems, parallel programming, or machine learning techniques. Students emerge with deep understanding of query optimization mathematics and algorithms that form the foundation for all advanced database optimization techniques.\n\n### Implementation Guidance\n\nThis guidance helps you structure your query optimizer implementation to achieve the primary goals while respecting the explicit boundaries we've established.\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Plan Tree Storage | In-memory Python classes with references | Persistent plan cache with SQLite backend |\n| Statistics Collection | Manual table scanning with basic counters | Sampling-based statistics with histogram approximation |\n| Cost Calculation | Simple arithmetic with fixed weights | Configurable cost models with JSON parameter files |\n| Join Enumeration | Recursive enumeration with memoization | Iterator-based dynamic programming with pruning |\n| Plan Visualization | Text-based tree printing with indentation | Graphical plan display with graphviz integration |\n| Testing Framework | Basic unittest with hardcoded test cases | Property-based testing with hypothesis for edge cases |\n\n**B. Recommended File Structure**\n\nOrganize your query optimizer code to maintain clear separation between the four main components while enabling easy testing and extension:\n\n```\nquery_optimizer/\n  __init__.py                     ← package initialization\n  main.py                        ← CLI entry point for testing optimizer\n  \n  plan/\n    __init__.py\n    operators.py                  ← OperatorNode hierarchy and ExecutionPlan\n    tree_builder.py              ← plan tree construction and manipulation\n    visualizer.py                ← plan pretty-printing and debugging output\n    \n  cost/\n    __init__.py\n    statistics.py                ← TableStatistics collection and storage\n    estimator.py                 ← estimateCost implementation\n    models.py                    ← CostEstimate structure and calculation helpers\n    \n  join/\n    __init__.py\n    optimizer.py                 ← optimizeJoinOrder dynamic programming\n    enumeration.py               ← generatePlans for join combinations\n    pruning.py                   ← search space reduction heuristics\n    \n  physical/\n    __init__.py\n    selector.py                  ← physical operator selection logic\n    rules.py                     ← optimization rules like predicate pushdown\n    access_methods.py            ← index vs scan selection algorithms\n    \n  utils/\n    __init__.py\n    query_parser.py              ← simple SQL parsing for testing\n    test_data.py                 ← sample tables and statistics for experiments\n    \n  tests/\n    __init__.py\n    test_plan_representation.py  ← Milestone 1 tests\n    test_cost_estimation.py      ← Milestone 2 tests\n    test_join_ordering.py        ← Milestone 3 tests\n    test_physical_selection.py   ← Milestone 4 tests\n    integration_tests.py         ← end-to-end optimization tests\n```\n\n**C. Infrastructure Starter Code**\n\nHere's complete starter infrastructure that handles the non-core components, letting you focus on optimization algorithms:\n\n```python\n# utils/query_parser.py - Simple SQL parsing for testing\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nimport re\n\n@dataclass\nclass ParsedQuery:\n    tables: List[str]\n    columns: List[str] \n    joins: List[tuple]  # (left_table, right_table, condition)\n    filters: List[tuple]  # (table, column, operator, value)\n\nclass SimpleQueryParser:\n    \"\"\"Basic SQL parser for testing query optimization.\n    \n    Handles SELECT queries with JOINs and WHERE clauses.\n    Not meant for production - just enables optimizer testing.\n    \"\"\"\n    \n    def parse(self, sql: str) -> ParsedQuery:\n        sql = sql.strip().upper()\n        \n        # Extract SELECT columns\n        select_match = re.search(r'SELECT\\s+(.+?)\\s+FROM', sql)\n        columns = [col.strip() for col in select_match.group(1).split(',')]\n        \n        # Extract FROM tables and JOINs\n        from_match = re.search(r'FROM\\s+(.+?)(?:\\s+WHERE|$)', sql)\n        from_clause = from_match.group(1)\n        \n        tables = []\n        joins = []\n        \n        # Simple table extraction - extend for complex JOIN syntax\n        table_parts = re.split(r'\\s+JOIN\\s+', from_clause)\n        tables.append(table_parts[0].strip())\n        \n        for join_part in table_parts[1:]:\n            on_match = re.search(r'(\\w+)\\s+ON\\s+(.+)', join_part)\n            if on_match:\n                table = on_match.group(1)\n                condition = on_match.group(2)\n                tables.append(table)\n                joins.append((tables[-2], table, condition))\n        \n        # Extract WHERE filters\n        filters = []\n        where_match = re.search(r'WHERE\\s+(.+)', sql)\n        if where_match:\n            # Basic filter parsing - extend for complex predicates\n            conditions = where_match.group(1).split(' AND ')\n            for condition in conditions:\n                filter_match = re.match(r'(\\w+)\\.(\\w+)\\s*(=|<|>|<=|>=)\\s*(.+)', condition.strip())\n                if filter_match:\n                    table, column, op, value = filter_match.groups()\n                    filters.append((table, column, op, value))\n        \n        return ParsedQuery(tables, columns, joins, filters)\n\n# utils/test_data.py - Sample data for testing\nclass TestDataGenerator:\n    \"\"\"Generates sample table statistics and test queries for optimizer validation.\"\"\"\n    \n    @staticmethod\n    def create_sample_statistics() -> Dict[str, 'TableStatistics']:\n        \"\"\"Creates realistic table statistics for testing.\"\"\"\n        from cost.statistics import TableStatistics\n        \n        # Customer table - typical e-commerce scenario\n        customers = TableStatistics(\n            table_name=\"customers\",\n            row_count=100000,\n            column_stats={\n                \"customer_id\": {\"distinct_count\": 100000, \"null_fraction\": 0.0},\n                \"country\": {\"distinct_count\": 50, \"null_fraction\": 0.02},\n                \"age\": {\"distinct_count\": 80, \"null_fraction\": 0.01}\n            }\n        )\n        \n        # Orders table - 5x customer count (average 5 orders per customer)\n        orders = TableStatistics(\n            table_name=\"orders\", \n            row_count=500000,\n            column_stats={\n                \"order_id\": {\"distinct_count\": 500000, \"null_fraction\": 0.0},\n                \"customer_id\": {\"distinct_count\": 80000, \"null_fraction\": 0.0},  # 80% customers have orders\n                \"order_date\": {\"distinct_count\": 1000, \"null_fraction\": 0.0},\n                \"status\": {\"distinct_count\": 5, \"null_fraction\": 0.0}\n            }\n        )\n        \n        # Products table - catalog of items\n        products = TableStatistics(\n            table_name=\"products\",\n            row_count=10000, \n            column_stats={\n                \"product_id\": {\"distinct_count\": 10000, \"null_fraction\": 0.0},\n                \"category\": {\"distinct_count\": 20, \"null_fraction\": 0.0},\n                \"price\": {\"distinct_count\": 5000, \"null_fraction\": 0.0}\n            }\n        )\n        \n        return {\"customers\": customers, \"orders\": orders, \"products\": products}\n    \n    @staticmethod  \n    def create_test_queries() -> List[str]:\n        \"\"\"Returns SQL queries of increasing complexity for testing.\"\"\"\n        return [\n            # Single table with filter\n            \"SELECT * FROM customers WHERE country = 'USA'\",\n            \n            # Two-table join\n            \"SELECT c.name, o.order_date FROM customers c JOIN orders o ON c.customer_id = o.customer_id\",\n            \n            # Three-table join with filters\n            \"SELECT c.name, p.name FROM customers c JOIN orders o ON c.customer_id = o.customer_id JOIN products p ON o.product_id = p.product_id WHERE c.country = 'USA' AND p.category = 'Electronics'\",\n            \n            # Complex multi-join query\n            \"SELECT c.country, COUNT(*) FROM customers c JOIN orders o ON c.customer_id = o.customer_id JOIN products p ON o.product_id = p.product_id WHERE p.price > 100 GROUP BY c.country\"\n        ]\n```\n\n**D. Core Logic Skeletons**\n\nHere are the main function signatures with detailed TODOs that map to the algorithm steps described in the design sections:\n\n```python\n# cost/estimator.py - Core cost estimation logic\nfrom typing import Dict\nfrom plan.operators import ExecutionPlan, OperatorNode\nfrom cost.models import CostEstimate\nfrom cost.statistics import TableStatistics\n\ndef estimateCost(plan: ExecutionPlan, stats: Dict[str, TableStatistics]) -> CostEstimate:\n    \"\"\"Calculate predicted execution cost for query plan.\n    \n    Combines I/O costs from table access with CPU costs from operations.\n    Uses bottom-up tree traversal to aggregate costs from leaves to root.\n    \"\"\"\n    # TODO 1: Initialize cost accumulator with zero I/O, CPU, and memory costs\n    # TODO 2: Perform post-order traversal of plan tree starting from leaves\n    # TODO 3: For each Scan operator, calculate I/O cost based on table size and selectivity\n    # TODO 4: For each Join operator, estimate output cardinality and CPU cost for join algorithm\n    # TODO 5: For each Filter operator, calculate CPU cost for predicate evaluation\n    # TODO 6: Aggregate child costs and add operator-specific costs at each node\n    # TODO 7: Return total CostEstimate with I/O, CPU breakdown for debugging\n    # Hint: Use plan.root.accept(CostCalculationVisitor(stats)) for clean traversal\n    pass\n\n# join/optimizer.py - Dynamic programming join ordering  \nfrom typing import List, Set\nfrom cost.statistics import TableStatistics\n\ndef optimizeJoinOrder(tables: List[str], predicates: List[tuple], stats: Dict[str, TableStatistics]) -> 'JoinOrder':\n    \"\"\"Find efficient join sequence using dynamic programming.\n    \n    Builds optimal plans for progressively larger table subsets.\n    Avoids exponential enumeration through memoization of subproblem solutions.\n    \"\"\"\n    # TODO 1: Initialize memoization table for optimal costs of table subsets\n    # TODO 2: Create base case entries for single-table access plans\n    # TODO 3: Iterate through subset sizes from 2 to len(tables) \n    # TODO 4: For each subset, try all possible ways to split into two smaller subsets\n    # TODO 5: Calculate join cost for each split using cost estimation\n    # TODO 6: Store minimum cost plan for this subset in memoization table\n    # TODO 7: Prune subsets that would create cross products (no join predicates)\n    # TODO 8: Return JoinOrder representing optimal sequence from memoization table\n    # Hint: Use itertools.combinations to generate subsets systematically\n    pass\n\n# physical/selector.py - Physical operator selection\ndef selectPhysicalOperators(logical_plan: ExecutionPlan, stats: Dict[str, TableStatistics]) -> ExecutionPlan:\n    \"\"\"Choose concrete physical operators for logical plan.\n    \n    Considers available indexes, input cardinalities, and memory constraints.\n    Applies optimization rules like predicate pushdown during selection.\n    \"\"\"\n    # TODO 1: Traverse logical plan tree to identify operator selection opportunities\n    # TODO 2: For each Scan operator, choose between sequential scan and index scan based on selectivity\n    # TODO 3: For each Join operator, select hash join vs nested loop based on input sizes\n    # TODO 4: Apply predicate pushdown rules to move filters below joins where beneficial\n    # TODO 5: Consider sort order requirements for downstream operators\n    # TODO 6: Generate new ExecutionPlan with selected physical operators\n    # TODO 7: Validate that physical plan maintains logical semantics\n    # Hint: Use SELECTIVITY_THRESHOLD constant to decide scan vs index access\n    pass\n```\n\n**E. Language-Specific Hints**\n\nPython-specific implementation guidance for common optimization tasks:\n\n- **Tree Traversal**: Use the visitor pattern with `operator.accept(visitor)` methods for clean plan tree navigation. Implement separate visitors for cost calculation, plan printing, and optimization rule application.\n\n- **Memoization**: Use `functools.lru_cache` decorator or manual dictionary caching for dynamic programming subproblems. Cache optimal costs keyed by frozenset of table names to handle subset enumeration efficiently.\n\n- **Statistics Storage**: Store `TableStatistics` in simple dictionaries initially. For persistence, use pickle files or JSON serialization. Avoid database storage complexity during initial implementation.\n\n- **Cost Calculation**: Use `dataclasses` for `CostEstimate` to get automatic equality, string representation, and arithmetic operations. Implement `__add__` method for combining costs from child operators.\n\n- **Combinatorial Enumeration**: Use `itertools.combinations` and `itertools.permutations` for generating join orders and table subsets. Be careful with exponential growth - add `MAX_JOIN_ENUMERATION` limits early.\n\n- **Plan Visualization**: Implement `__str__` methods on operator nodes for debugging. Use string indentation based on tree depth for readable plan printing. Consider `graphviz` integration for complex query visualization.\n\n**F. Milestone Checkpoints**\n\nAfter implementing each milestone, verify your progress with these concrete tests:\n\n**Milestone 1 - Plan Representation**: Run `python -m pytest tests/test_plan_representation.py`. Expected behavior: Create operator trees, traverse parent-child relationships, pretty-print plans with proper indentation. Warning signs: Stack overflow in tree traversal, missing operator types, incorrect parent-child links.\n\n**Milestone 2 - Cost Estimation**: Execute `python -c \"from cost.estimator import estimateCost; print(estimateCost(sample_plan, test_stats))\"`. Expected output: Reasonable cost numbers with I/O dominated by large table scans, CPU costs proportional to intermediate result sizes. Warning signs: Negative costs, infinite costs, identical costs for very different plans.\n\n**Milestone 3 - Join Ordering**: Test with `python -c \"from join.optimizer import optimizeJoinOrder; print(optimizeJoinOrder(['A','B','C'], predicates, stats))\"`. Expected behavior: Different join orders for different table size combinations, avoidance of cross products, reasonable execution time even for 5-6 table queries. Warning signs: Exponential slowdown, cross product plans, identical orders regardless of statistics.\n\n**Milestone 4 - Physical Selection**: Validate with end-to-end test: `python main.py --query \"SELECT * FROM customers WHERE country='USA'\"`. Expected output: Index scan chosen for highly selective filters, sequential scan for low selectivity, hash joins for large input tables. Warning signs: Always choosing same physical operators, ignoring selectivity estimates, missing predicate pushdown opportunities.\n\n\n## High-Level Architecture\n\n> **Milestone(s):** Foundation for all milestones - establishes the overall system design that guides implementation across Milestones 1-4.\n\n### Component Responsibilities\n\nThink of the query optimizer as a **construction project planning office** where different specialists collaborate to design the most efficient way to build a complex structure. Just as architects, cost estimators, project managers, and construction supervisors each bring specialized expertise to create an optimal building plan, our query optimizer employs four specialized components that work together to transform a SQL query into an efficient execution strategy.\n\n![Query Optimizer System Architecture](./diagrams/system-architecture.svg)\n\nThe **Plan Builder** serves as the initial architect, taking the raw SQL requirements and creating the foundational blueprint. This component receives a `ParsedQuery` from the SQL parser and constructs the basic logical plan tree, defining what operations need to happen without yet specifying exactly how they'll be executed. The Plan Builder understands the relational algebra behind SQL operations and can represent queries as hierarchical trees of `OperatorNode` objects, each representing a fundamental database operation like scanning a table, applying filters, or joining datasets.\n\nThe Plan Builder's primary responsibility centers on **structural correctness** rather than efficiency. It ensures that the logical plan accurately represents the query semantics - that all required tables are accessed, all join conditions are properly represented, all filter predicates are applied, and the correct columns are projected in the final result. Think of this as the architect ensuring that the building blueprint includes all required rooms, proper connections between spaces, and meets all functional requirements, without yet worrying about construction costs or timing.\n\n| Responsibility | Description | Input | Output |\n|---|---|---|---|\n| Parse SQL to logical operations | Convert SQL syntax into relational algebra operations | `ParsedQuery` with tables, columns, joins, filters | Logical `ExecutionPlan` tree |\n| Validate query semantics | Ensure all referenced tables and columns exist and are accessible | Schema metadata, query structure | Validated logical plan or error |\n| Handle operator precedence | Apply proper order of operations for complex predicates | Complex WHERE clauses with AND/OR/NOT | Correctly structured filter tree |\n| Normalize plan structure | Convert to canonical form for optimization | Raw logical plan | Normalized `ExecutionPlan` |\n\nThe **Cost Estimator** functions as the project cost analyst, evaluating how expensive each potential execution strategy will be in terms of computational resources. This component maintains detailed `TableStatistics` about data distribution, cardinality, and storage characteristics, using this information to predict the resource consumption of different plan alternatives. The Cost Estimator must balance accuracy with computational efficiency - spending too much time on precise estimates can make optimization slower than the query execution itself.\n\nThe Cost Estimator's calculations encompass multiple resource dimensions: I/O operations for reading data from storage, CPU cycles for processing operations like joins and filters, and memory consumption for intermediate results and operator state. Each prediction involves statistical modeling based on assumptions about data distribution and correlation patterns. The component provides `CostEstimate` objects that quantify both the total resource consumption and the expected cardinality of intermediate results, enabling meaningful comparison between alternative execution strategies.\n\n| Cost Component | Estimation Method | Key Statistics Used | Accuracy Factors |\n|---|---|---|---|\n| I/O Operations | Pages accessed × page read cost | Table size, index selectivity | Buffer pool hit rate, storage type |\n| CPU Processing | Tuple count × operation complexity | Row counts, predicate selectivity | CPU cache effects, data types |\n| Memory Usage | Intermediate result sizes | Join cardinalities, sort buffer needs | Available memory, spill-to-disk costs |\n| Network Transfer | Result size × network latency | Output cardinality, tuple width | Network bandwidth, serialization overhead |\n\nThe **Join Optimizer** serves as the project scheduling specialist, determining the optimal sequence for multi-table operations. This component faces the classic combinatorial optimization challenge: with n tables to join, there are factorial possible orderings, creating an exponential search space that quickly becomes computationally intractable. The Join Optimizer employs dynamic programming algorithms to find optimal `JoinOrder` sequences while using pruning heuristics to avoid exploring clearly suboptimal alternatives.\n\nThe fundamental insight driving join optimization is that the order of operations dramatically affects intermediate result sizes, which in turn determines the cost of subsequent operations. A poorly chosen join order might create massive intermediate results early in the execution, making all following operations expensive. Conversely, an optimal ordering performs the most selective joins first, keeping intermediate results small and making the overall query execution efficient.\n\n> **Key Insight**: Join ordering optimization often provides the largest performance gains in query optimization. A 10x difference in execution time between good and bad join orders is common for queries involving 4+ tables.\n\n| Optimization Strategy | Algorithm | Search Space | Time Complexity | Quality |\n|---|---|---|---|---|\n| Complete enumeration | Dynamic programming | All valid join orders | O(n! × 2^n) | Optimal |\n| Greedy heuristics | Selectivity-based ordering | Single path | O(n²) | Good approximation |\n| Genetic algorithms | Evolutionary search | Sampled population | O(generations × population) | Variable |\n| Left-deep restriction | DP on linear plans | Left-deep trees only | O(n³) | Suboptimal but fast |\n\nThe **Physical Planner** acts as the construction method specialist, making concrete decisions about how to implement each logical operation using available physical operators and access methods. While the logical plan specifies *what* operations to perform, the physical planner determines *how* to perform them by selecting specific algorithms, access paths, and optimization techniques like predicate pushdown.\n\nThe Physical Planner must consider the available infrastructure: what indexes exist, what join algorithms are supported, whether hash tables fit in memory, and how to minimize data movement. It applies rule-based optimizations like pushing filter predicates down to table scans and chooses between alternative physical operators like hash joins versus nested loop joins based on estimated data sizes and characteristics.\n\n| Physical Decision | Selection Criteria | Options Considered | Impact on Performance |\n|---|---|---|---|\n| Table access method | Selectivity vs index availability | Sequential scan, index scan, index-only scan | 10-100x difference in I/O |\n| Join algorithm | Input sizes, memory availability | Hash join, nested loop, merge join | 5-50x difference in CPU/memory |\n| Sort implementation | Data size vs memory | In-memory quicksort, external merge sort | Memory usage and I/O patterns |\n| Predicate placement | Operator tree topology | Early filtering vs late filtering | Intermediate result sizes |\n\n> **Decision: Component Separation Strategy**\n> - **Context**: Query optimization involves multiple distinct types of analysis (structural, statistical, algorithmic, physical) that could be combined into a monolithic optimizer or separated into specialized components.\n> - **Options Considered**: Monolithic optimizer class, pipeline of specialized components, pluggable optimization framework\n> - **Decision**: Separate specialized components with well-defined interfaces\n> - **Rationale**: Each optimization phase requires different expertise, data structures, and algorithms. Separation enables independent testing, easier debugging, and potential parallel execution of optimization phases. The clean interfaces make the system more maintainable and allow for component replacement or enhancement.\n> - **Consequences**: Enables modular development and testing, introduces coordination overhead between components, requires careful interface design to avoid tight coupling.\n\n### Optimization Pipeline\n\nThe query optimization process flows through a carefully orchestrated sequence of transformations, each building upon the previous stage's output while adding specialized analysis and refinement. Think of this as an **assembly line for decision-making**, where each station adds specific expertise to transform raw requirements into a precisely engineered execution plan.\n\n![Optimization Process Workflow](./diagrams/optimization-workflow.svg)\n\nThe pipeline begins when a `ParsedQuery` arrives from the SQL parser, containing the basic structural information extracted from the SQL statement: referenced tables, required columns, join conditions, and filter predicates. This parsed representation provides the raw materials for optimization but lacks the sophisticated analysis needed for efficient execution.\n\n**Stage 1: Logical Plan Construction** marks the entry point where the Plan Builder transforms the flat SQL structure into a hierarchical execution plan. The builder creates a tree of logical operators, each representing a fundamental relational operation. At this stage, the plan accurately represents the query semantics but contains no performance optimizations or physical implementation details.\n\nThe logical planning process involves several critical steps:\n\n1. **Table identification and alias resolution** - The builder examines the FROM clause to identify all data sources and establishes a mapping between table aliases and actual table names, validating that all referenced tables exist and are accessible.\n\n2. **Join condition extraction** - The optimizer parses the WHERE clause and ON clauses to identify explicit join conditions, distinguishing them from filter predicates that apply to individual tables.\n\n3. **Filter predicate categorization** - The system categorizes WHERE clause predicates by the tables they reference, preparing them for potential pushdown optimization in later stages.\n\n4. **Projection analysis** - The builder examines the SELECT clause to determine which columns must be available at each stage of execution, enabling column pruning optimizations.\n\n5. **Operator tree construction** - The optimizer arranges logical operators into a preliminary tree structure, typically following SQL's natural evaluation order without yet considering performance implications.\n\n| Logical Plan Element | Source SQL Clause | Validation Required | Downstream Impact |\n|---|---|---|---|\n| Table scan operators | FROM clause | Table existence, access permissions | Available columns, cardinality estimates |\n| Join operators | WHERE/ON conditions | Column compatibility, join predicate validity | Search space for join ordering |\n| Filter operators | WHERE predicates | Data type consistency, function availability | Selectivity estimation input |\n| Projection operators | SELECT clause | Column existence, expression validity | Output schema definition |\n\n**Stage 2: Statistics Collection and Cost Model Preparation** runs concurrently with logical planning, gathering the statistical foundation needed for cost-based optimization. The Cost Estimator collects current `TableStatistics` for all referenced tables, including row counts, column cardinalities, data distribution histograms, and index characteristics.\n\nThis statistical foundation enables accurate resource consumption predictions throughout the optimization process. The Cost Estimator builds mathematical models that can predict how many rows will survive filter operations (selectivity estimation), how large intermediate join results will become (cardinality estimation), and how much I/O, CPU, and memory each operation will consume.\n\n**Stage 3: Join Order Optimization** represents the algorithmic heart of query optimization for multi-table queries. The Join Optimizer applies dynamic programming techniques to explore the space of possible join orderings, using cost estimates to identify the optimal sequence. This stage must balance optimization quality against optimization time, potentially switching from complete enumeration to heuristic approaches for queries involving many tables.\n\nThe join optimization algorithm proceeds through systematic subset enumeration:\n\n1. **Single-table access path optimization** - For each individual table, the optimizer determines the best access method (sequential scan versus available indexes) based on filter selectivity and index characteristics.\n\n2. **Two-table join optimization** - The algorithm evaluates all possible pairings of tables, considering different join algorithms and accessing methods for each combination.\n\n3. **Progressive subset expansion** - Using dynamic programming, the optimizer builds optimal plans for progressively larger table subsets, reusing previously computed optimal subplans.\n\n4. **Pruning and bounds checking** - The algorithm eliminates clearly suboptimal partial plans early, avoiding exponential explosion in the search space.\n\n5. **Cross product detection and elimination** - The optimizer identifies and heavily penalizes or eliminates join combinations that lack proper join predicates, avoiding expensive Cartesian products.\n\n**Stage 4: Physical Operator Selection** transforms the optimal logical plan into an executable physical plan by making concrete implementation choices. The Physical Planner selects specific algorithms for each operation, chooses access methods for table scans, and applies rule-based optimizations like predicate pushdown.\n\nPhysical planning decisions include:\n\n- **Access method selection**: Choosing between sequential scans, index scans, or index-only scans based on predicate selectivity and available indexes\n- **Join algorithm selection**: Selecting hash joins, nested loop joins, or merge joins based on input cardinalities and memory availability  \n- **Sort algorithm selection**: Choosing in-memory sorting versus external merge sorts based on data sizes and memory constraints\n- **Predicate pushdown application**: Moving filter operations as close to data sources as possible to minimize intermediate result sizes\n- **Projection pushdown application**: Eliminating unnecessary columns early in the execution pipeline to reduce data movement\n\n> **Key Design Insight**: The pipeline maintains clean separation between logical correctness (stages 1-2) and performance optimization (stages 3-4). This separation enables independent testing and debugging of optimization logic while ensuring that performance optimizations never compromise query correctness.\n\n| Pipeline Stage | Input | Processing | Output | Validation |\n|---|---|---|---|---|\n| Logical Planning | `ParsedQuery` | Semantic analysis, tree construction | Logical `ExecutionPlan` | Schema consistency, referential integrity |\n| Statistics Collection | Table names, filter predicates | Catalog lookup, histogram analysis | `TableStatistics` | Freshness checks, missing statistics detection |\n| Join Optimization | Logical plan, statistics | Dynamic programming, pruning | Optimal `JoinOrder` | Cost sanity checks, cross product detection |\n| Physical Planning | Logical plan, join order | Algorithm selection, rule application | Physical `ExecutionPlan` | Resource constraint validation |\n\n**Stage 5: Plan Finalization and Caching** completes the optimization process by performing final validation, cost annotation, and storage for potential reuse. The optimizer ensures that the selected physical plan satisfies all resource constraints, annotates the plan with detailed cost estimates for execution monitoring, and stores the optimized plan in the plan cache for reuse with similar queries.\n\nThe optimization pipeline maintains **state machines** that track the progression through different optimization phases, enabling recovery from failures and providing detailed debugging information when optimization produces unexpected results.\n\n![Optimizer State Transitions](./diagrams/optimizer-state-machine.svg)\n\nError handling throughout the pipeline follows a **fail-fast principle**: each stage validates its inputs and outputs, immediately detecting inconsistencies or constraint violations. When optimization fails, the system provides detailed diagnostic information identifying the specific stage and condition that caused the failure, along with suggestions for resolution.\n\n### Recommended File Structure\n\nThe query optimizer implementation benefits from a modular file organization that mirrors the component architecture while providing clear separation of concerns and easy navigation for developers. This structure supports incremental development through the project milestones while maintaining clean interfaces between components.\n\n```\nquery_optimizer/\n├── README.md                           # Project overview and build instructions\n├── requirements.txt                    # Python dependencies\n├── setup.py                           # Package configuration\n│\n├── optimizer/                         # Main optimizer package\n│   ├── __init__.py                    # Package initialization, main API\n│   ├── types.py                       # Core data structures and enums\n│   ├── exceptions.py                  # Optimizer-specific exception classes\n│   └── config.py                      # Configuration constants and settings\n│\n├── optimizer/parser/                  # SQL parsing (prerequisite)\n│   ├── __init__.py\n│   ├── sql_parser.py                  # ParsedQuery generation\n│   └── query_validator.py             # Semantic validation\n│\n├── optimizer/plan/                    # Plan representation (Milestone 1)\n│   ├── __init__.py\n│   ├── operators.py                   # OperatorNode hierarchy\n│   ├── execution_plan.py              # ExecutionPlan tree structure\n│   ├── logical_operators.py           # Logical operation definitions\n│   ├── physical_operators.py          # Physical operation implementations\n│   └── plan_printer.py                # Tree visualization utilities\n│\n├── optimizer/statistics/              # Cost estimation (Milestone 2)\n│   ├── __init__.py\n│   ├── table_stats.py                 # TableStatistics collection\n│   ├── cost_model.py                  # CostEstimate calculations\n│   ├── selectivity.py                 # Predicate selectivity estimation\n│   └── cardinality.py                 # Join cardinality estimation\n│\n├── optimizer/join_ordering/           # Join optimization (Milestone 3)\n│   ├── __init__.py\n│   ├── dp_optimizer.py                # Dynamic programming algorithm\n│   ├── join_order.py                  # JoinOrder representation\n│   ├── pruning.py                     # Search space reduction\n│   └── heuristics.py                  # Fallback optimization strategies\n│\n├── optimizer/physical/                # Physical planning (Milestone 4)\n│   ├── __init__.py\n│   ├── operator_selection.py          # Physical operator choice logic\n│   ├── access_methods.py              # Table scan vs index selection\n│   ├── join_algorithms.py             # Join algorithm selection\n│   └── optimization_rules.py          # Predicate pushdown and transformations\n│\n├── optimizer/integration/             # Component coordination\n│   ├── __init__.py\n│   ├── optimization_pipeline.py       # Main optimization workflow\n│   ├── plan_cache.py                  # Optimized plan storage and reuse\n│   └── optimizer_facade.py            # Simplified external API\n│\n├── tests/                            # Test suite organization\n│   ├── __init__.py\n│   ├── unit/                         # Component-specific unit tests\n│   │   ├── test_plan_representation.py\n│   │   ├── test_cost_estimation.py\n│   │   ├── test_join_ordering.py\n│   │   └── test_physical_planning.py\n│   ├── integration/                  # Cross-component integration tests\n│   │   ├── test_optimization_pipeline.py\n│   │   └── test_plan_quality.py\n│   └── fixtures/                     # Test data and example queries\n│       ├── sample_schemas.py\n│       ├── test_queries.sql\n│       └── expected_plans.py\n│\n├── examples/                         # Usage examples and tutorials\n│   ├── basic_optimization.py         # Simple query optimization demo\n│   ├── cost_analysis.py             # Cost estimation examples\n│   └── plan_visualization.py        # Plan tree printing examples\n│\n└── docs/                            # Additional documentation\n    ├── milestone_checkpoints.md      # Validation criteria for each milestone\n    ├── debugging_guide.md           # Common issues and solutions\n    └── api_reference.md             # Complete API documentation\n```\n\n**Component Module Responsibilities**:\n\n| Module | Primary Classes | Key Responsibilities | Milestone |\n|---|---|---|---|\n| `optimizer/types.py` | `ParsedQuery`, `CostEstimate`, `JoinOrder` | Core data structure definitions | All |\n| `optimizer/plan/` | `ExecutionPlan`, `OperatorNode` | Plan tree representation and manipulation | 1 |\n| `optimizer/statistics/` | `TableStatistics`, cost estimation functions | Resource consumption prediction | 2 |\n| `optimizer/join_ordering/` | Join optimization algorithms | Dynamic programming for optimal join sequences | 3 |\n| `optimizer/physical/` | Physical operator selection logic | Concrete implementation choices | 4 |\n| `optimizer/integration/` | `OptimizationPipeline` | Component coordination and caching | All |\n\n**Development Workflow Support**:\n\nThe file structure enables incremental development aligned with project milestones. Developers can implement and test each component independently before integrating them into the complete optimization pipeline:\n\n- **Milestone 1 Development**: Focus on `optimizer/plan/` modules, implementing operator hierarchy and tree structures with unit tests in `tests/unit/test_plan_representation.py`\n\n- **Milestone 2 Development**: Implement statistics collection in `optimizer/statistics/` while using stub implementations for join ordering and physical planning\n\n- **Milestone 3 Development**: Build dynamic programming algorithms in `optimizer/join_ordering/` using real cost estimates from previous milestones\n\n- **Milestone 4 Development**: Complete physical planning in `optimizer/physical/` and integrate all components in `optimizer/integration/`\n\n> **Design Principle**: Each module maintains high cohesion (related functionality grouped together) and loose coupling (minimal dependencies between modules). This organization supports both learning progression and professional development practices.\n\n**Import Strategy and Dependencies**:\n\nThe module structure enforces a clear dependency hierarchy that prevents circular imports and maintains architectural layering:\n\n```\noptimizer/types.py              # No internal dependencies\noptimizer/plan/                 # Depends on: types\noptimizer/statistics/           # Depends on: types, plan\noptimizer/join_ordering/        # Depends on: types, plan, statistics  \noptimizer/physical/             # Depends on: types, plan, statistics\noptimizer/integration/          # Depends on: all above modules\n```\n\nThis dependency structure ensures that core data structures remain stable, specialized components can be developed independently, and integration logic cleanly orchestrates the optimization process without creating circular dependencies.\n\n**Testing Organization**:\n\nThe testing structure supports both component-specific validation and end-to-end optimization quality verification. Unit tests focus on individual component correctness, while integration tests validate that components work together correctly and produce high-quality optimization results.\n\n| Test Category | Focus | Example Validation |\n|---|---|---|\n| Unit tests | Individual component logic | Cost estimation accuracy, join ordering algorithm correctness |\n| Integration tests | Component interaction | Complete optimization pipeline, plan quality metrics |\n| Fixtures and examples | Realistic scenarios | Standard TPC benchmarks, common query patterns |\n\n### Implementation Guidance\n\n**A. Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|---|---|---|\n| Plan Tree Structure | Python dictionaries with type hints | Custom classes with inheritance hierarchy |\n| Cost Estimation | Linear cost formulas | Multi-dimensional statistical models |\n| Join Ordering | Greedy heuristics | Dynamic programming with pruning |\n| Statistics Storage | In-memory dictionaries | Persistent statistics with histograms |\n| Plan Caching | Simple LRU cache | Persistent plan store with invalidation |\n| Serialization | JSON for plan representation | Protocol Buffers for efficiency |\n| Testing Framework | pytest with fixtures | Hypothesis for property-based testing |\n\n**B. Core Infrastructure Starter Code**\n\n```python\n# optimizer/types.py - Complete core data structures\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass, field\nfrom enum import Enum\n\nclass OperatorType(Enum):\n    \"\"\"Types of query operators\"\"\"\n    SCAN = \"scan\"\n    FILTER = \"filter\"  \n    JOIN = \"join\"\n    PROJECT = \"project\"\n    SORT = \"sort\"\n    AGGREGATE = \"aggregate\"\n\nclass JoinType(Enum):\n    \"\"\"Types of join operations\"\"\"\n    INNER = \"inner\"\n    LEFT = \"left\"\n    RIGHT = \"right\"\n    FULL = \"full\"\n\n@dataclass\nclass ParsedQuery:\n    \"\"\"Parsed SQL query structure\"\"\"\n    tables: List[str]\n    columns: List[str] \n    joins: List[tuple]  # (left_table, right_table, join_condition)\n    filters: List[tuple]  # (table, column, operator, value)\n    group_by: List[str] = field(default_factory=list)\n    order_by: List[str] = field(default_factory=list)\n\n@dataclass\nclass CostEstimate:\n    \"\"\"Resource consumption estimate\"\"\"\n    io_cost: float\n    cpu_cost: float\n    memory_cost: float\n    total_cost: float\n    estimated_rows: int\n    \n    @property\n    def total_cost(self) -> float:\n        return self.io_cost + self.cpu_cost + self.memory_cost\n\n@dataclass\nclass TableStatistics:\n    \"\"\"Statistical information about a table\"\"\"\n    table_name: str\n    row_count: int\n    page_count: int\n    column_stats: Dict[str, 'ColumnStatistics']\n    last_updated: str\n\n@dataclass\nclass ColumnStatistics:\n    \"\"\"Statistics for a single column\"\"\"\n    distinct_values: int\n    null_count: int\n    min_value: Any\n    max_value: Any\n    most_frequent_values: List[tuple]  # (value, frequency) pairs\n\n# optimizer/config.py - Configuration constants\nOPTIMIZATION_TIMEOUT = 30.0  # seconds\nMAX_JOIN_ENUMERATION = 6     # tables before switching to heuristics\nSELECTIVITY_THRESHOLD = 0.1  # cutoff for index vs sequential scan\n\n# Cost model parameters\nIO_PAGE_COST = 1.0\nCPU_TUPLE_COST = 0.01\nMEMORY_PAGE_COST = 0.1\n\n# Join algorithm cost factors\nHASH_JOIN_FACTOR = 1.0\nNESTED_LOOP_FACTOR = 1.5\nMERGE_JOIN_FACTOR = 1.2\n```\n\n```python\n# optimizer/plan/execution_plan.py - Complete plan tree infrastructure\nfrom typing import List, Optional, Dict, Any\nfrom optimizer.types import OperatorType, CostEstimate\n\nclass ExecutionPlan:\n    \"\"\"Tree structure representing query operations\"\"\"\n    \n    def __init__(self, root: 'OperatorNode'):\n        self.root = root\n        self._node_count = 0\n        \n    def traverse_preorder(self) -> List['OperatorNode']:\n        \"\"\"Traverse plan tree in pre-order\"\"\"\n        result = []\n        self._traverse_preorder_helper(self.root, result)\n        return result\n        \n    def _traverse_preorder_helper(self, node: 'OperatorNode', result: List['OperatorNode']):\n        if node:\n            result.append(node)\n            for child in node.children:\n                self._traverse_preorder_helper(child, result)\n    \n    def calculate_total_cost(self) -> CostEstimate:\n        \"\"\"Sum costs across all operators in plan\"\"\"\n        total_io = total_cpu = total_memory = 0.0\n        total_rows = 0\n        \n        for node in self.traverse_preorder():\n            if node.cost_estimate:\n                total_io += node.cost_estimate.io_cost\n                total_cpu += node.cost_estimate.cpu_cost\n                total_memory += node.cost_estimate.memory_cost\n                total_rows += node.cost_estimate.estimated_rows\n                \n        return CostEstimate(\n            io_cost=total_io,\n            cpu_cost=total_cpu, \n            memory_cost=total_memory,\n            estimated_rows=total_rows\n        )\n    \n    def pretty_print(self, show_costs: bool = True) -> str:\n        \"\"\"Generate indented tree representation\"\"\"\n        lines = []\n        self._pretty_print_helper(self.root, \"\", True, lines, show_costs)\n        return \"\\n\".join(lines)\n        \n    def _pretty_print_helper(self, node: 'OperatorNode', prefix: str, \n                           is_last: bool, lines: List[str], show_costs: bool):\n        if not node:\n            return\n            \n        connector = \"└── \" if is_last else \"├── \"\n        node_desc = f\"{node.operator_type.value}\"\n        \n        if hasattr(node, 'table_name') and node.table_name:\n            node_desc += f\"({node.table_name})\"\n            \n        if show_costs and node.cost_estimate:\n            cost = node.cost_estimate.total_cost\n            rows = node.cost_estimate.estimated_rows\n            node_desc += f\" [cost={cost:.2f}, rows={rows}]\"\n            \n        lines.append(f\"{prefix}{connector}{node_desc}\")\n        \n        child_prefix = prefix + (\"    \" if is_last else \"│   \")\n        for i, child in enumerate(node.children):\n            is_last_child = (i == len(node.children) - 1)\n            self._pretty_print_helper(child, child_prefix, is_last_child, lines, show_costs)\n```\n\n**C. Core Logic Skeleton Code**\n\n```python\n# optimizer/integration/optimization_pipeline.py - Main coordination logic\nclass OptimizationPipeline:\n    \"\"\"Orchestrates the complete query optimization process\"\"\"\n    \n    def __init__(self):\n        self.plan_builder = None      # TODO: Initialize in Milestone 1\n        self.cost_estimator = None    # TODO: Initialize in Milestone 2  \n        self.join_optimizer = None    # TODO: Initialize in Milestone 3\n        self.physical_planner = None  # TODO: Initialize in Milestone 4\n        \n    def optimize_query(self, parsed_query: ParsedQuery) -> ExecutionPlan:\n        \"\"\"Transform parsed query into optimized execution plan\"\"\"\n        \n        # TODO Milestone 1: Build logical plan tree\n        # 1. Create logical operators for each table scan\n        # 2. Build filter operators for WHERE predicates\n        # 3. Create join operators for table relationships\n        # 4. Add projection operator for SELECT columns\n        # 5. Validate plan tree structure and semantics\n        \n        # TODO Milestone 2: Collect statistics and estimate costs\n        # 1. Gather TableStatistics for all referenced tables\n        # 2. Calculate selectivity for all filter predicates  \n        # 3. Estimate cardinality for join operations\n        # 4. Annotate plan nodes with CostEstimate objects\n        # 5. Validate cost estimates for reasonableness\n        \n        # TODO Milestone 3: Optimize join ordering\n        # 1. Extract all tables involved in joins\n        # 2. Enumerate possible join orders using dynamic programming\n        # 3. Calculate costs for each join order alternative  \n        # 4. Select optimal JoinOrder based on total cost\n        # 5. Reconstruct plan tree with optimized join sequence\n        \n        # TODO Milestone 4: Select physical operators\n        # 1. Choose access methods (scan vs index) for each table\n        # 2. Select join algorithms (hash vs nested loop vs merge)\n        # 3. Apply predicate pushdown optimizations\n        # 4. Apply projection pushdown optimizations  \n        # 5. Generate final physical ExecutionPlan\n        \n        raise NotImplementedError(\"Implement optimization pipeline\")\n\ndef generatePlans(query: ParsedQuery) -> List[ExecutionPlan]:\n    \"\"\"Enumerate possible execution plans for query\"\"\"\n    # TODO: Generate alternative plans for comparison\n    # 1. Create baseline plan with no optimizations\n    # 2. Generate plans with different join orders\n    # 3. Generate plans with different physical operators\n    # 4. Apply various optimization rules\n    # 5. Return list of valid alternative plans\n    pass\n\ndef estimateCost(plan: ExecutionPlan) -> CostEstimate:\n    \"\"\"Calculate predicted execution cost for plan\"\"\"  \n    # TODO: Implement cost calculation\n    # 1. Traverse plan tree in post-order\n    # 2. Calculate cost for each operator based on inputs\n    # 3. Sum I/O, CPU, and memory costs across operators\n    # 4. Validate cost estimates are reasonable\n    # 5. Return total CostEstimate for plan\n    pass\n\ndef optimizeJoinOrder(tables: List[str]) -> JoinOrder:\n    \"\"\"Find efficient join sequence using dynamic programming\"\"\"\n    # TODO: Implement DP join ordering\n    # 1. Handle base case of single table (no joins needed)\n    # 2. Initialize DP table for storing optimal subplans  \n    # 3. Enumerate all possible join pairs and calculate costs\n    # 4. Build up optimal plans for larger table subsets\n    # 5. Extract final optimal JoinOrder from DP table\n    pass\n\ndef selectPhysicalOperators(logical_plan: ExecutionPlan, \n                          stats: Dict[str, TableStatistics]) -> ExecutionPlan:\n    \"\"\"Choose concrete physical operators for logical plan\"\"\"\n    # TODO: Implement physical operator selection\n    # 1. Traverse logical plan tree\n    # 2. For each scan operator, choose sequential vs index scan\n    # 3. For each join operator, choose hash vs nested loop vs merge join\n    # 4. Apply predicate pushdown to move filters closer to scans\n    # 5. Return plan with all physical operators selected\n    pass\n\ndef collectStatistics(table: str) -> TableStatistics:\n    \"\"\"Gather statistical information for cost estimation\"\"\"\n    # TODO: Implement statistics collection\n    # 1. Query table row count from system catalogs\n    # 2. Calculate distinct value counts for each column\n    # 3. Build histograms for data distribution analysis  \n    # 4. Collect index statistics and selectivity information\n    # 5. Return complete TableStatistics object\n    pass\n```\n\n**D. File/Module Structure Implementation**\n\n```python\n# optimizer/__init__.py - Main package API\n\"\"\"\nQuery Optimizer Package\n\nProvides cost-based query optimization for SQL queries.\n\"\"\"\n\nfrom optimizer.integration.optimization_pipeline import OptimizationPipeline\nfrom optimizer.types import ParsedQuery, ExecutionPlan, CostEstimate\nfrom optimizer.plan.execution_plan import ExecutionPlan  \nfrom optimizer.statistics.table_stats import TableStatistics\n\n# Main public API\n__all__ = [\n    'OptimizationPipeline', \n    'ParsedQuery',\n    'ExecutionPlan', \n    'CostEstimate',\n    'TableStatistics'\n]\n\n# Convenience function for simple optimization\ndef optimize_query(sql_query: str) -> ExecutionPlan:\n    \"\"\"Optimize a SQL query string and return execution plan\"\"\"\n    # This would integrate with SQL parser\n    parser = SQLParser()  # From prerequisite SQL parser project\n    parsed = parser.parse(sql_query)\n    \n    optimizer = OptimizationPipeline()\n    return optimizer.optimize_query(parsed)\n```\n\n**E. Language-Specific Implementation Hints**\n\n- **Python Type Hints**: Use `typing.List`, `typing.Dict`, `typing.Optional` extensively for better IDE support and runtime validation\n- **Dataclasses**: Use `@dataclass` for simple data structures like `CostEstimate` and `TableStatistics` to get automatic `__init__`, `__repr__`, and equality methods  \n- **Enum Classes**: Use `Enum` for operator types and join types to ensure type safety and prevent invalid values\n- **Property Decorators**: Use `@property` for calculated fields like `total_cost` that should appear as attributes but are computed from other fields\n- **Context Managers**: Use context managers for resource management in statistics collection that may need to query external databases\n- **Functools.lru_cache**: Use `@lru_cache` decorator for expensive operations like cost calculations that may be called repeatedly with same inputs\n- **Collections.defaultdict**: Use `defaultdict` for statistics dictionaries where missing keys should return sensible defaults\n- **Pathlib**: Use `pathlib.Path` for file operations if implementing persistent plan caching\n\n**F. Milestone Checkpoint**\n\nAfter implementing the high-level architecture foundation:\n\n**Validation Commands:**\n```bash\n# Test basic package structure\npython -c \"from optimizer import OptimizationPipeline; print('Package imports successfully')\"\n\n# Run architecture validation tests  \npython -m pytest tests/unit/test_architecture.py -v\n\n# Check type hints\npython -m mypy optimizer/ --strict\n```\n\n**Expected Behaviors:**\n- All core data structures (`ParsedQuery`, `ExecutionPlan`, `CostEstimate`) can be imported and instantiated\n- Package structure follows recommended layout with no circular imports\n- Type hints pass mypy validation with strict mode\n- Basic plan tree creation and traversal works without optimization logic\n\n**Quality Checks:**\n- Code coverage should be >90% for infrastructure modules  \n- All public methods have docstrings with parameter and return type documentation\n- Integration tests can create and manipulate plan trees even with stub optimization logic\n\n\n## Data Model\n\n> **Milestone(s):** Foundation for Milestones 1-4 - defines the core data structures that support plan representation (M1), cost estimation (M2), join ordering (M3), and physical planning (M4).\n\nThe data model forms the backbone of our query optimizer, defining how we represent query plans, store statistical information, and calculate costs throughout the optimization process. Think of the data model as the **blueprint language** for our optimizer - just as architects need standardized symbols and measurements to design buildings, our optimizer needs well-defined data structures to represent execution plans, capture table characteristics, and quantify optimization decisions.\n\nThe data model serves three critical purposes in our optimizer architecture. First, it provides a **common vocabulary** that allows different optimizer components to communicate effectively - the Plan Builder creates `ExecutionPlan` trees that the Cost Estimator can analyze and the Join Optimizer can manipulate. Second, it encapsulates the **statistical knowledge** about our database through `TableStatistics` and `ColumnStatistics` that drive intelligent optimization decisions. Third, it quantifies optimization trade-offs through the `CostEstimate` structure that allows us to compare alternative execution strategies numerically.\n\nOur data model design emphasizes three key principles that emerged from decades of database research. **Separation of logical and physical concerns** allows us to reason about what operations need to happen independently from how they will be executed. **Rich statistical representation** captures the data characteristics needed for accurate cost estimation without overwhelming the optimizer with unnecessary detail. **Compositional cost modeling** enables us to build up complex plan costs from simpler operator-level estimates while maintaining mathematical soundness.\n\n![Query Plan Tree Structure](./diagrams/plan-tree-structure.svg)\n\n### Plan Tree and Operator Nodes\n\nThe **plan tree representation** forms the central data structure of our query optimizer, capturing the hierarchical nature of query execution through a tree of operator nodes. Think of a query plan tree like a **construction project blueprint** - it shows not just what rooms need to be built (the operations), but also the order of construction (execution dependencies) and how materials flow between work sites (data flow between operators). Just as a construction foreman can look at blueprints and estimate labor hours and material costs, our optimizer can traverse plan trees to calculate execution costs and identify optimization opportunities.\n\nThe `ExecutionPlan` serves as the root container for our plan representation, holding both the tree structure and associated metadata needed for optimization. This structure maintains the complete execution strategy for a query while providing convenient access methods for plan manipulation and cost calculation.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `root` | `OperatorNode` | The root operator node of the execution plan tree |\n| `total_cost` | `CostEstimate` | Cached total cost estimate for the entire plan |\n| `optimization_metadata` | `dict` | Additional metadata used during optimization process |\n| `plan_id` | `str` | Unique identifier for this execution plan |\n| `created_timestamp` | `datetime` | When this plan was generated for debugging |\n\nThe `OperatorNode` represents individual operations within the query execution plan, forming the building blocks of our tree structure. Each node encapsulates both the operation definition and its relationships to other operators in the execution sequence. The design supports both logical operators (what needs to happen) and physical operators (how it will be executed) through a common interface while maintaining type safety.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `operator_type` | `OperatorType` | The type of operation this node performs |\n| `children` | `List[OperatorNode]` | Child operators that provide input to this operation |\n| `properties` | `dict` | Operator-specific configuration and parameters |\n| `cost_estimate` | `CostEstimate` | Estimated execution cost for this operator |\n| `output_schema` | `List[str]` | Column names and types produced by this operator |\n| `estimated_rows` | `int` | Predicted number of output rows |\n| `node_id` | `str` | Unique identifier within the plan tree |\n\nThe `OperatorType` enumeration defines the complete set of operations our optimizer can represent and manipulate. This taxonomy covers both logical operations (independent of execution method) and physical operations (specific implementation strategies). The distinction proves crucial during optimization - we generate logical plans first, then select physical implementations based on cost estimates and available resources.\n\n| Operator Type | Category | Description |\n|---------------|----------|-------------|\n| `SCAN` | Data Access | Read rows from a table or index |\n| `FILTER` | Row Processing | Apply predicate conditions to eliminate rows |\n| `JOIN` | Data Combination | Combine rows from multiple input sources |\n| `PROJECT` | Column Processing | Select specific columns and compute expressions |\n| `SORT` | Data Ordering | Order rows by specified columns |\n| `AGGREGATE` | Data Summarization | Group rows and compute summary functions |\n| `UNION` | Set Operations | Combine rows from multiple sources without duplicates |\n| `INTERSECT` | Set Operations | Find common rows between input sources |\n| `LIMIT` | Result Control | Restrict number of output rows |\n\nEach operator node maintains **rich property dictionaries** that capture operator-specific configuration without requiring separate subclasses for every operation type. For example, a `SCAN` operator's properties might include the table name, index selection, and filter predicates pushed down to the storage layer. A `JOIN` operator would specify the join condition, join type (`INNER`, `LEFT`, `RIGHT`, `FULL`), and chosen algorithm (`hash_join`, `nested_loop`, `merge_join`).\n\n> **Design Insight**: The property dictionary approach provides flexibility for adding new operator variations without modifying the core tree structure. This proves especially valuable as we extend the optimizer with new physical operator implementations or optimization rules.\n\nThe **tree traversal capabilities** built into our plan structure enable systematic plan analysis and transformation throughout the optimization process. We support multiple traversal patterns to handle different optimization scenarios efficiently.\n\n| Traversal Method | Order | Use Case |\n|------------------|--------|----------|\n| `traverse_preorder()` | Root → Children | Cost estimation, plan validation |\n| `traverse_postorder()` | Children → Root | Bottom-up cost calculation |\n| `traverse_level_order()` | Level by level | Plan visualization, debugging |\n| `find_nodes_by_type(type)` | Filtered search | Optimization rule application |\n| `get_leaf_nodes()` | Bottom-up | Finding data source operators |\n\nThe plan tree structure supports **immutable transformations** that create new plan variants without modifying existing trees. This design choice enables safe parallel exploration of the optimization search space and simplifies debugging by preserving intermediate optimization states. When the Join Optimizer generates alternative join orders, each possibility becomes a separate tree that can be costed and compared independently.\n\n> **Architecture Decision: Tree vs DAG**\n> - **Context**: Query plans could be represented as trees (each operator has exactly one parent) or directed acyclic graphs (operators can have multiple parents for shared computation)\n> - **Options Considered**: \n>   1. Tree structure with duplication of common subexpressions\n>   2. DAG structure with shared operator nodes\n>   3. Hybrid approach with tree structure and shared expression detection\n> - **Decision**: Pure tree structure for initial implementation\n> - **Rationale**: Trees provide simpler implementation, easier cost calculation, and cleaner optimization algorithms. Most common subexpressions can be handled by the SQL parser or later optimization phases\n> - **Consequences**: Some redundant computation in complex queries, but significantly simpler optimizer implementation and debugging\n\n### Table and Column Statistics\n\nStatistical information forms the **knowledge foundation** that enables cost-based query optimization, providing the optimizer with essential insights about data characteristics needed for accurate cost estimation. Think of database statistics like **demographic surveys** used by city planners - just as planners need population density, traffic patterns, and resource usage data to design efficient infrastructure, our query optimizer needs table sizes, column distributions, and correlation patterns to generate efficient execution plans.\n\nThe statistics subsystem addresses the fundamental challenge that cost-based optimizers face: making intelligent decisions about query execution strategies without examining all the data. By maintaining carefully chosen statistical summaries, we enable the optimizer to predict with reasonable accuracy how many rows will survive filter conditions, how selective join operations will be, and which access methods will prove most efficient for specific query patterns.\n\n![Statistics and Cost Model](./diagrams/statistics-data-model.svg)\n\nThe `TableStatistics` structure captures essential characteristics about individual tables that drive optimization decisions throughout the query planning process. These statistics provide the foundation for cardinality estimation, access method selection, and join order optimization.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `table_name` | `str` | Fully qualified name of the table |\n| `row_count` | `int` | Total number of rows in the table |\n| `page_count` | `int` | Number of storage pages occupied by table data |\n| `column_stats` | `dict[str, ColumnStatistics]` | Per-column statistical information |\n| `last_updated` | `datetime` | When these statistics were last refreshed |\n| `sample_rate` | `float` | Fraction of table sampled for statistics (0.0-1.0) |\n| `index_statistics` | `dict[str, IndexStatistics]` | Statistics for available indexes |\n| `clustering_factor` | `float` | Measure of row ordering correlation with storage order |\n\nThe **row count and page count** provide the basic size metrics needed for cost estimation. Row count drives cardinality estimates for filter and join operations, while page count enables I/O cost calculation for sequential scans and large join operations. The ratio between these values indicates average row size, which influences buffer pool efficiency and memory usage estimates.\n\n**Column-level statistics** capture the distribution characteristics needed for selectivity estimation - the process of predicting what fraction of rows will satisfy filter predicates. The `ColumnStatistics` structure balances statistical richness with storage efficiency, providing enough information for accurate estimates without overwhelming the optimizer with unnecessary detail.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `column_name` | `str` | Name of the column within its table |\n| `distinct_values` | `int` | Number of unique values (cardinality) |\n| `null_count` | `int` | Number of NULL values in the column |\n| `min_value` | `Any` | Minimum value observed in the column |\n| `max_value` | `Any` | Maximum value observed in the column |\n| `most_common_values` | `List[tuple]` | Top-K frequent values with their counts |\n| `histogram_buckets` | `List[HistogramBucket]` | Value distribution summary |\n| `correlation_with_storage` | `float` | How well column order matches storage order (-1.0 to 1.0) |\n| `average_width` | `int` | Average storage space per value in bytes |\n\nThe **distinct values count** proves crucial for join cardinality estimation, enabling the optimizer to predict output sizes when combining tables. Combined with the null count, it provides the foundation for selectivity calculations across different predicate types. The min/max range enables quick estimates for range predicates without consulting detailed histograms.\n\n**Most common values** capture skewed distributions that uniform assumption models handle poorly. Real-world data often exhibits Zipfian distributions where a small number of values account for a large fraction of rows. By tracking the top-K frequent values explicitly, we can provide accurate estimates for equality predicates on popular values while falling back to histogram-based estimates for less common values.\n\nThe `HistogramBucket` structure provides a compressed representation of value distributions for more sophisticated selectivity estimation beyond simple uniform assumptions.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `bucket_id` | `int` | Sequential identifier for this bucket |\n| `range_start` | `Any` | Minimum value included in this bucket |\n| `range_end` | `Any` | Maximum value included in this bucket |\n| `row_count` | `int` | Number of rows with values in this bucket's range |\n| `distinct_count` | `int` | Number of distinct values within this bucket |\n| `frequency` | `float` | Fraction of total table rows in this bucket |\n\n**Statistics collection and maintenance** requires careful balance between accuracy and overhead. We support multiple collection strategies to handle different table sizes and update patterns effectively.\n\n| Collection Method | When Used | Accuracy | Overhead |\n|-------------------|-----------|----------|----------|\n| Full table scan | Small tables (<10K rows) | Exact | Low |\n| Systematic sampling | Medium tables (10K-1M rows) | High | Medium |\n| Random sampling | Large tables (>1M rows) | Good | Low |\n| Incremental updates | High-velocity tables | Variable | Low |\n| Query-driven collection | Ad-hoc analysis | Targeted | Variable |\n\nThe **statistics freshness problem** presents an ongoing challenge for cost-based optimizers - as data changes, statistics become stale and optimization decisions degrade. Our statistics model includes metadata to track aging and trigger refresh operations based on configurable policies.\n\n> **Architecture Decision: Histogram vs Uniform Distribution**\n> - **Context**: Column value distributions can be modeled with different levels of sophistication, from simple uniform assumptions to detailed multi-dimensional histograms\n> - **Options Considered**:\n>   1. Uniform distribution assumption (distinct values evenly distributed)\n>   2. Single-dimensional histograms with equal-width buckets\n>   3. Single-dimensional histograms with equal-frequency buckets\n>   4. Multi-dimensional histograms capturing column correlations\n> - **Decision**: Single-dimensional equal-frequency histograms with most-common-values tracking\n> - **Rationale**: Provides significant accuracy improvement over uniform assumptions while maintaining reasonable storage overhead and computational complexity. Equal-frequency buckets handle skewed distributions better than equal-width buckets\n> - **Consequences**: Enables accurate selectivity estimation for most real-world query patterns, but cannot capture multi-column correlations that affect complex join estimates\n\n### Cost Model Components\n\nThe **cost model** quantifies the computational resources required to execute query plans, translating abstract execution strategies into concrete resource consumption estimates that enable numerical comparison of optimization alternatives. Think of query cost estimation like **construction project bidding** - just as contractors break down building projects into material costs, labor hours, and equipment rental fees to generate competitive bids, our cost model decomposes query execution into I/O operations, CPU processing, and memory allocation to predict total resource consumption.\n\nThe cost model serves as the **objective function** for query optimization, providing the numerical foundation that allows our optimizer to choose between alternative execution strategies. Without accurate cost estimates, optimization becomes guesswork - the optimizer might choose index scans over sequential scans for highly selective queries, or select hash joins over nested loop joins for large table combinations, based on mathematical analysis rather than heuristic rules.\n\nThe `CostEstimate` structure captures the multi-dimensional nature of query execution costs, recognizing that different resource types contribute to overall performance through different mechanisms and at different scales.\n\n| Field Name | Type | Description |\n|------------|------|-------------|\n| `io_cost` | `float` | Estimated disk I/O operations required |\n| `cpu_cost` | `float` | Estimated CPU processing cycles needed |\n| `memory_cost` | `float` | Estimated memory allocation required |\n| `estimated_rows` | `int` | Predicted number of output rows |\n| `startup_cost` | `float` | Fixed cost incurred before producing first row |\n| `total_cost` | `float` | Combined cost across all resource dimensions |\n| `cost_factors` | `dict` | Breakdown of cost contributors for debugging |\n| `confidence_level` | `float` | Estimated accuracy of this cost prediction (0.0-1.0) |\n\n**I/O cost modeling** captures the expense of accessing persistent storage, typically the dominant performance factor for database operations on large datasets. I/O costs scale with the amount of data that must be read from disk, modified by factors like buffer pool hit rates, storage device characteristics, and access patterns.\n\nThe fundamental I/O cost calculation combines sequential and random access patterns based on the specific operator implementation:\n\n1. **Sequential scan operations** incur costs proportional to the number of data pages that must be read, modified by the sequential access multiplier that reflects the efficiency of reading consecutive pages\n2. **Index scan operations** combine the cost of traversing index pages (typically cached) with the cost of accessing heap pages (potentially random access)  \n3. **Join operations** may require multiple passes through datasets, with costs varying dramatically based on available memory and chosen join algorithm\n4. **Sort operations** incur costs for both reading input data and writing temporary results, with additional multipliers for multi-pass external sorting\n\n| I/O Cost Component | Formula | Constants Used |\n|-------------------|---------|----------------|\n| Sequential page reads | `pages * IO_PAGE_COST * SEQUENTIAL_MULTIPLIER` | `IO_PAGE_COST = 1.0`, `SEQUENTIAL_MULTIPLIER = 0.1` |\n| Random page reads | `pages * IO_PAGE_COST * RANDOM_MULTIPLIER` | `RANDOM_MULTIPLIER = 4.0` |\n| Index traversal | `tree_height * IO_PAGE_COST * INDEX_MULTIPLIER` | `INDEX_MULTIPLIER = 0.5` |\n| Sort temporary I/O | `input_pages * SORT_MULTIPLIER * passes` | `SORT_MULTIPLIER = 2.0` |\n| Hash table spill | `spilled_pages * IO_PAGE_COST * HASH_SPILL_MULTIPLIER` | `HASH_SPILL_MULTIPLIER = 1.5` |\n\n**CPU cost modeling** quantifies the computational work required to process data rows through the execution pipeline. CPU costs typically scale with the number of rows processed rather than the number of pages accessed, reflecting the per-tuple processing overhead of expression evaluation, predicate checking, and data transformation.\n\nDifferent operator types exhibit characteristic CPU cost patterns that our model captures through per-tuple processing estimates:\n\n| Operation Type | CPU Cost per Tuple | Reasoning |\n|----------------|-------------------|-----------|\n| Sequential scan | `CPU_TUPLE_COST * 1.0` | Minimal processing - just row retrieval |\n| Index scan | `CPU_TUPLE_COST * 1.2` | Additional index navigation overhead |\n| Filter evaluation | `CPU_TUPLE_COST * complexity_factor` | Varies by predicate complexity |\n| Hash join probe | `CPU_TUPLE_COST * 2.5` | Hash calculation and bucket lookup |\n| Nested loop join | `CPU_TUPLE_COST * 1.5 * inner_rows` | Linear scan of inner relation |\n| Sort comparison | `CPU_TUPLE_COST * log(rows) * comparisons` | Comparison-based sorting |\n| Aggregate calculation | `CPU_TUPLE_COST * 3.0` | Group identification and accumulation |\n\n**Memory cost modeling** captures the buffer pool and working memory requirements that affect plan feasibility and performance characteristics. Memory costs influence optimization decisions through multiple mechanisms - insufficient memory forces external algorithms that increase I/O costs, while abundant memory enables more efficient join algorithms and larger buffer pools.\n\n| Memory Usage Type | Cost Calculation | Impact on Performance |\n|-------------------|------------------|----------------------|\n| Buffer pool pages | `pages * MEMORY_PAGE_COST` | Reduces I/O through caching |\n| Hash table memory | `rows * avg_row_size * HASH_OVERHEAD` | Enables in-memory join processing |\n| Sort buffer memory | `sort_buffer_size * MEMORY_PAGE_COST` | Reduces external sort passes |\n| Temporary result storage | `intermediate_rows * row_size` | Required for pipeline blocking |\n| Index cache memory | `index_pages * MEMORY_PAGE_COST` | Accelerates index access |\n\nThe **cost combination formula** aggregates multi-dimensional costs into a single comparable metric while preserving the relative importance of different resource types. This proves essential for optimization decisions that trade off different resource types - for example, choosing between memory-intensive hash joins and I/O-intensive nested loop joins.\n\nThe total cost calculation follows a weighted linear combination approach that can be tuned based on the specific deployment environment:\n\n```\ntotal_cost = (io_cost * IO_WEIGHT) + \n             (cpu_cost * CPU_WEIGHT) + \n             (memory_cost * MEMORY_WEIGHT) + \n             startup_cost\n```\n\n| Cost Weight | Default Value | Environment Tuning |\n|-------------|---------------|-------------------|\n| `IO_WEIGHT` | `4.0` | Higher on slow storage, lower on SSD |\n| `CPU_WEIGHT` | `1.0` | Higher on CPU-constrained systems |\n| `MEMORY_WEIGHT` | `0.1` | Higher when memory is scarce |\n| `STARTUP_COST_PENALTY` | `0.1` | Varies by query pattern requirements |\n\n**Cost estimation accuracy** varies significantly across different operator types and data characteristics, requiring the optimizer to maintain confidence levels that influence plan selection. High-confidence estimates enable aggressive optimization decisions, while low-confidence estimates favor conservative approaches that perform reasonably across a wider range of actual data distributions.\n\n| Estimation Scenario | Confidence Level | Accuracy Factors |\n|---------------------|------------------|------------------|\n| Table scan with current stats | 0.9 | Statistics recency, sampling quality |\n| Index scan with selective predicate | 0.8 | Index statistics, predicate selectivity |\n| Hash join with good stats | 0.7 | Join column distribution, memory availability |\n| Multi-way join estimation | 0.5 | Correlation effects, error propagation |\n| Complex nested subquery | 0.3 | Limited statistics, optimization complexity |\n\n> **Architecture Decision: Linear vs Non-Linear Cost Models**\n> - **Context**: Database operations exhibit complex performance characteristics that could be modeled with varying levels of sophistication\n> - **Options Considered**:\n>   1. Simple linear cost model (cost proportional to data size)\n>   2. Piecewise linear model (different rates for different size ranges)  \n>   3. Non-linear model capturing algorithm complexity curves\n>   4. Machine learning-based cost prediction\n> - **Decision**: Piecewise linear model with algorithm-specific parameters\n> - **Rationale**: Provides reasonable accuracy for most query patterns while maintaining mathematical tractability for optimization algorithms. Captures key non-linearities like external sorting thresholds without excessive complexity\n> - **Consequences**: Enables effective optimization for typical query workloads, but may underestimate costs for very large queries or unusual data distributions\n\n### Implementation Guidance\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Statistics Storage | Python dictionaries with pickle serialization | SQLite database with structured schema |\n| Cost Calculation | Basic arithmetic with floating-point | NumPy arrays for vectorized operations |\n| Plan Tree Traversal | Recursive functions with manual stack | Generator-based iterators for memory efficiency |\n| Statistics Collection | Full table sampling | Reservoir sampling with statistical bounds |\n\n**Recommended File Structure:**\n\n```\nquery_optimizer/\n  data_model/\n    __init__.py                    ← Export main data classes\n    plan_tree.py                   ← ExecutionPlan and OperatorNode\n    statistics.py                  ← TableStatistics and ColumnStatistics  \n    cost_model.py                  ← CostEstimate and calculation functions\n    constants.py                   ← Cost model parameters and thresholds\n  tests/\n    test_plan_tree.py             ← Plan construction and traversal tests\n    test_statistics.py            ← Statistics collection and accuracy tests\n    test_cost_model.py            ← Cost estimation validation tests\n```\n\n**Infrastructure Starter Code (Complete Implementation):**\n\n`data_model/constants.py` - Cost model parameters:\n\n```python\n\"\"\"Cost model constants and configuration parameters.\"\"\"\n\nfrom enum import Enum\nfrom typing import Final\n\n# Cost model weights for combining different resource types\nIO_PAGE_COST: Final[float] = 1.0\nCPU_TUPLE_COST: Final[float] = 0.01\nMEMORY_PAGE_COST: Final[float] = 0.001\n\n# I/O cost multipliers for different access patterns\nSEQUENTIAL_MULTIPLIER: Final[float] = 0.1\nRANDOM_MULTIPLIER: Final[float] = 4.0\nINDEX_MULTIPLIER: Final[float] = 0.5\n\n# Algorithm-specific cost factors\nHASH_BUILD_COST: Final[float] = 2.0\nHASH_PROBE_COST: Final[float] = 1.5\nSORT_COMPARISON_COST: Final[float] = 1.2\nNESTED_LOOP_MULTIPLIER: Final[float] = 0.5\n\n# Optimization thresholds\nSELECTIVITY_THRESHOLD: Final[float] = 0.05\nMAX_JOIN_ENUMERATION: Final[int] = 10\nOPTIMIZATION_TIMEOUT: Final[int] = 30\n\n# Default statistics parameters\nDEFAULT_HISTOGRAM_BUCKETS: Final[int] = 100\nDEFAULT_SAMPLE_RATE: Final[float] = 0.1\nSTATISTICS_STALENESS_DAYS: Final[int] = 7\n\nclass OperatorType(Enum):\n    \"\"\"Enumeration of operator types supported in query plans.\"\"\"\n    SCAN = \"scan\"\n    FILTER = \"filter\" \n    JOIN = \"join\"\n    PROJECT = \"project\"\n    SORT = \"sort\"\n    AGGREGATE = \"aggregate\"\n    UNION = \"union\"\n    INTERSECT = \"intersect\"\n    LIMIT = \"limit\"\n\nclass JoinType(Enum):\n    \"\"\"Enumeration of join types for join operators.\"\"\"\n    INNER = \"inner\"\n    LEFT = \"left\"\n    RIGHT = \"right\"\n    FULL = \"full\"\n    CROSS = \"cross\"\n```\n\n`data_model/cost_model.py` - Cost estimation infrastructure:\n\n```python\n\"\"\"Cost estimation data structures and utility functions.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any, Optional\nimport math\nfrom .constants import *\n\n@dataclass\nclass CostEstimate:\n    \"\"\"Represents the estimated execution cost for a query plan or operator.\"\"\"\n    io_cost: float = 0.0\n    cpu_cost: float = 0.0\n    memory_cost: float = 0.0\n    estimated_rows: int = 0\n    startup_cost: float = 0.0\n    cost_factors: Dict[str, float] = field(default_factory=dict)\n    confidence_level: float = 1.0\n    \n    @property\n    def total_cost(self) -> float:\n        \"\"\"Calculate combined total cost across all resource dimensions.\"\"\"\n        return (self.io_cost * 4.0 + \n                self.cpu_cost * 1.0 + \n                self.memory_cost * 0.1 + \n                self.startup_cost)\n    \n    def add_cost(self, other: 'CostEstimate') -> 'CostEstimate':\n        \"\"\"Combine this cost estimate with another, returning new estimate.\"\"\"\n        return CostEstimate(\n            io_cost=self.io_cost + other.io_cost,\n            cpu_cost=self.cpu_cost + other.cpu_cost,\n            memory_cost=self.memory_cost + other.memory_cost,\n            estimated_rows=max(self.estimated_rows, other.estimated_rows),\n            startup_cost=self.startup_cost + other.startup_cost,\n            confidence_level=min(self.confidence_level, other.confidence_level)\n        )\n    \n    def scale_by_factor(self, factor: float) -> 'CostEstimate':\n        \"\"\"Scale all cost components by a multiplicative factor.\"\"\"\n        return CostEstimate(\n            io_cost=self.io_cost * factor,\n            cpu_cost=self.cpu_cost * factor,\n            memory_cost=self.memory_cost * factor,\n            estimated_rows=int(self.estimated_rows * factor),\n            startup_cost=self.startup_cost * factor,\n            confidence_level=self.confidence_level * 0.9  # Reduce confidence when scaling\n        )\n\ndef calculate_scan_cost(row_count: int, page_count: int, selectivity: float = 1.0) -> CostEstimate:\n    \"\"\"Calculate cost for sequential table scan with optional filtering.\"\"\"\n    io_cost = page_count * IO_PAGE_COST * SEQUENTIAL_MULTIPLIER\n    cpu_cost = row_count * CPU_TUPLE_COST\n    estimated_output = int(row_count * selectivity)\n    \n    return CostEstimate(\n        io_cost=io_cost,\n        cpu_cost=cpu_cost,\n        estimated_rows=estimated_output,\n        cost_factors={\"pages_read\": page_count, \"rows_processed\": row_count}\n    )\n\ndef calculate_join_cost(left_rows: int, right_rows: int, join_selectivity: float = 0.1) -> CostEstimate:\n    \"\"\"Calculate cost for hash join between two relations.\"\"\"\n    # Hash table build cost (smaller relation)\n    build_rows = min(left_rows, right_rows)\n    probe_rows = max(left_rows, right_rows)\n    \n    build_cost = build_rows * CPU_TUPLE_COST * HASH_BUILD_COST\n    probe_cost = probe_rows * CPU_TUPLE_COST * HASH_PROBE_COST\n    memory_cost = build_rows * 100 * MEMORY_PAGE_COST  # Assume 100 bytes per row\n    \n    output_rows = int(left_rows * right_rows * join_selectivity)\n    \n    return CostEstimate(\n        cpu_cost=build_cost + probe_cost,\n        memory_cost=memory_cost,\n        estimated_rows=output_rows,\n        cost_factors={\n            \"build_rows\": build_rows,\n            \"probe_rows\": probe_rows,\n            \"join_selectivity\": join_selectivity\n        }\n    )\n```\n\n**Core Logic Skeleton Code:**\n\n`data_model/plan_tree.py` - Plan tree implementation framework:\n\n```python\n\"\"\"Query plan tree data structures with traversal and manipulation methods.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Iterator, Optional\nfrom datetime import datetime\nimport uuid\nfrom .constants import OperatorType\nfrom .cost_model import CostEstimate\n\n@dataclass\nclass OperatorNode:\n    \"\"\"Represents a single operator in the query execution plan tree.\"\"\"\n    operator_type: OperatorType\n    children: List['OperatorNode'] = field(default_factory=list)\n    properties: Dict[str, Any] = field(default_factory=dict)\n    cost_estimate: Optional[CostEstimate] = None\n    output_schema: List[str] = field(default_factory=list)\n    estimated_rows: int = 0\n    node_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    \n    def add_child(self, child: 'OperatorNode') -> None:\n        \"\"\"Add a child operator to this node.\"\"\"\n        # TODO: Validate that child operator is compatible with this operator type\n        # TODO: Update output schema based on child schemas and operator semantics\n        # TODO: Invalidate cached cost estimates when tree structure changes\n        pass\n    \n    def traverse_preorder(self) -> Iterator['OperatorNode']:\n        \"\"\"Traverse the plan tree in pre-order (root first, then children).\"\"\"\n        # TODO: Yield this node first\n        # TODO: Recursively traverse each child in left-to-right order\n        # TODO: Handle circular references (shouldn't happen in trees, but defensive)\n        pass\n    \n    def traverse_postorder(self) -> Iterator['OperatorNode']:\n        \"\"\"Traverse the plan tree in post-order (children first, then root).\"\"\"\n        # TODO: Recursively traverse each child first\n        # TODO: Yield this node after all children have been processed\n        # TODO: This order is useful for bottom-up cost calculation\n        pass\n    \n    def find_nodes_by_type(self, operator_type: OperatorType) -> List['OperatorNode']:\n        \"\"\"Find all nodes in the subtree with the specified operator type.\"\"\"\n        # TODO: Use traverse_preorder() to visit all nodes in subtree\n        # TODO: Filter nodes matching the requested operator_type\n        # TODO: Return list of matching nodes for optimization rule application\n        pass\n    \n    def calculate_subtree_cost(self) -> CostEstimate:\n        \"\"\"Calculate the total cost for this operator and all its children.\"\"\"\n        # TODO: Use traverse_postorder() to calculate costs bottom-up\n        # TODO: Combine child costs with this operator's cost using add_cost()\n        # TODO: Cache the result in cost_estimate field for future queries\n        # TODO: Handle cases where individual operator costs are not yet calculated\n        pass\n    \n    def pretty_print(self, indent: int = 0, show_costs: bool = True) -> str:\n        \"\"\"Generate indented tree representation for debugging and visualization.\"\"\"\n        # TODO: Format this node with appropriate indentation\n        # TODO: Include operator type, key properties, and cost if show_costs=True\n        # TODO: Recursively print all children with increased indentation\n        # TODO: Return complete multi-line string representation\n        pass\n\n@dataclass \nclass ExecutionPlan:\n    \"\"\"Container for complete query execution plan with metadata.\"\"\"\n    root: OperatorNode\n    total_cost: Optional[CostEstimate] = None\n    optimization_metadata: Dict[str, Any] = field(default_factory=dict)\n    plan_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    created_timestamp: datetime = field(default_factory=datetime.now)\n    \n    def calculate_total_cost(self) -> CostEstimate:\n        \"\"\"Calculate and cache the total execution cost for this plan.\"\"\"\n        # TODO: Call root.calculate_subtree_cost() to get complete plan cost\n        # TODO: Store result in total_cost field for future access\n        # TODO: Return the calculated cost estimate\n        pass\n    \n    def get_all_tables(self) -> List[str]:\n        \"\"\"Extract all table names referenced in this execution plan.\"\"\"\n        # TODO: Find all SCAN operators using root.find_nodes_by_type()\n        # TODO: Extract table names from scan operator properties\n        # TODO: Return unique list of table names for statistics lookup\n        pass\n    \n    def clone(self) -> 'ExecutionPlan':\n        \"\"\"Create a deep copy of this execution plan for optimization.\"\"\"\n        # TODO: Recursively clone the operator tree starting from root\n        # TODO: Generate new plan_id and timestamp for the copy\n        # TODO: Copy optimization_metadata but not cached costs\n        # TODO: Return new ExecutionPlan with cloned tree structure\n        pass\n```\n\n**Milestone Checkpoint:**\n\nAfter implementing the data model, verify the following behaviors:\n\n1. **Plan Tree Construction**: Create a simple plan tree with SCAN → FILTER → PROJECT operators and verify parent-child relationships\n2. **Cost Calculation**: Build cost estimates for individual operators and verify that total costs combine correctly\n3. **Statistics Representation**: Create TableStatistics and ColumnStatistics objects with realistic values and verify field access\n4. **Tree Traversal**: Test preorder and postorder traversal on a sample tree and verify correct node ordering\n5. **Pretty Printing**: Generate indented tree representations and verify readability\n\n**Expected test command**: `python -m pytest tests/test_data_model.py -v`\n\n**Expected behaviors**: All data structure constructors should work without errors, tree traversal should visit all nodes in correct order, and cost calculations should produce positive numeric values.\n\n**Common Issues and Debugging:**\n\n| Symptom | Likely Cause | How to Diagnose | Fix |\n|---------|--------------|-----------------|-----|\n| \"AttributeError: 'NoneType' object has no attribute...\" | Uninitialized cost_estimate or statistics | Check object construction and field defaults | Initialize with empty/default values in dataclass |\n| Tree traversal goes into infinite loop | Circular reference in operator children | Add visited set to traversal methods | Implement cycle detection or ensure tree property |\n| Cost calculations return negative values | Incorrect cost model parameters or missing data | Print intermediate cost components | Validate inputs and use absolute values where appropriate |\n| Statistics objects consume excessive memory | Large histogram buckets or uncompressed data | Profile memory usage with sample statistics | Implement sampling and compression for large datasets |\n\n\n## Plan Representation Component\n\n> **Milestone(s):** Milestone 1 - implements the foundational query plan tree structure with logical and physical operators, supporting tree traversal and cost annotation.\n\n### Mental Model: Construction Blueprint\n\nThink of query plan trees like architectural blueprints for a building construction project. Just as architects create multiple levels of blueprints - from high-level floor plans showing room layouts to detailed electrical diagrams showing every wire and outlet - query optimizers create multiple representations of the same execution strategy.\n\nThe **logical plan** is like the initial architectural sketch that shows \"we need a kitchen, three bedrooms, and two bathrooms\" without specifying whether the kitchen uses gas or electric appliances, what type of flooring goes in each room, or the exact placement of electrical outlets. It captures the essential operations (scan this table, filter these rows, join these datasets) without committing to specific implementation techniques.\n\nThe **physical plan** is like the detailed construction blueprint that specifies every concrete detail: \"install ceramic tile flooring in the kitchen, use hardwood in bedrooms, place electrical outlets 18 inches above floor level, connect the gas line to a specific model of range.\" It transforms each logical operation into a concrete physical operator with specific algorithms, access methods, and execution parameters.\n\nThe tree structure itself represents the dependency relationships between operations, much like how construction tasks have dependencies - you can't install flooring before the subflooring is complete, and you can't paint walls before the electrical wiring is finished. In query plans, you can't join two tables until you've scanned them, and you can't apply filters until you have data to filter.\n\nEach node in the tree carries **cost annotations** like how construction blueprints include material quantities and labor estimates. These annotations help the optimizer make informed decisions about alternative approaches, just as a contractor might choose between different materials based on cost, availability, and performance characteristics.\n\n### Operator Type System\n\nThe operator type system forms the foundational vocabulary for expressing query execution strategies. We distinguish between **logical operators** that express what computation should happen and **physical operators** that specify exactly how that computation will be implemented.\n\n![Query Plan Tree Structure](./diagrams/plan-tree-structure.svg)\n\n#### Logical vs Physical Operator Distinction\n\n**Logical operators** represent abstract operations without implementation commitments. A logical `SCAN` operation means \"retrieve data from this table\" without specifying whether to use a sequential table scan, an index scan, or a bitmap scan. A logical `JOIN` operation means \"combine rows from these two datasets based on join predicates\" without specifying whether to use a hash join, nested loop join, or sort-merge join algorithm.\n\n**Physical operators** represent concrete implementations with specific algorithms and resource usage patterns. A physical `SEQUENTIAL_SCAN` operator commits to reading every page of a table sequentially from disk. A physical `HASH_JOIN` operator commits to building an in-memory hash table from the smaller input and probing it with rows from the larger input.\n\nThis separation allows the optimizer to explore multiple implementation strategies for the same logical computation. A single logical plan might have dozens of possible physical implementations, each with different cost characteristics depending on data sizes, available memory, and existing indexes.\n\n#### Core Operator Types\n\n| Operator Type | Logical Purpose | Physical Variants | Typical Use Cases |\n|---------------|-----------------|-------------------|-------------------|\n| `SCAN` | Retrieve rows from table | `SEQUENTIAL_SCAN`, `INDEX_SCAN`, `BITMAP_SCAN` | Base table access, filtering with indexes |\n| `FILTER` | Apply selection predicates | `FILTER`, `BITMAP_FILTER` | Row elimination based on WHERE clauses |\n| `JOIN` | Combine rows from multiple sources | `HASH_JOIN`, `NESTED_LOOP_JOIN`, `SORT_MERGE_JOIN` | Multi-table queries with join predicates |\n| `PROJECT` | Select and transform columns | `PROJECT`, `PROJECTION_WITH_EXPRESSION` | Column selection, computed expressions |\n| `SORT` | Order rows by specified columns | `QUICKSORT`, `EXTERNAL_SORT`, `TOP_K_SORT` | ORDER BY clauses, sort-merge join preparation |\n| `AGGREGATE` | Group rows and compute aggregates | `HASH_AGGREGATE`, `SORT_AGGREGATE`, `STREAMING_AGGREGATE` | GROUP BY, COUNT, SUM, AVG operations |\n| `LIMIT` | Restrict number of output rows | `LIMIT`, `TOP_K_LIMIT` | LIMIT clauses, pagination queries |\n\nEach operator type carries different cost characteristics and optimization opportunities. `SCAN` operators depend heavily on table size and available indexes. `JOIN` operators have quadratic cost potential that drives join ordering optimization. `SORT` operators may require external sorting for large datasets that exceed available memory.\n\n#### Operator Node Structure\n\nThe `OperatorNode` structure serves as the universal building block for all plan trees, supporting both logical and physical planning phases:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `operator_type` | `OperatorType` | Enum identifying the specific operation (SCAN, JOIN, etc.) |\n| `children` | `List[OperatorNode]` | Child operators providing input data streams |\n| `properties` | `dict` | Operator-specific configuration (table name, join conditions, etc.) |\n| `cost_estimate` | `CostEstimate` | Predicted resource consumption for this operator subtree |\n| `output_schema` | `List[str]` | Column names and types produced by this operator |\n| `estimated_rows` | `int` | Predicted cardinality of operator output |\n| `node_id` | `str` | Unique identifier for debugging and plan comparison |\n\nThe `properties` dictionary provides operator-specific configuration without requiring specialized node classes for each operator type. A `SCAN` node stores `table_name` and optional `index_name` properties. A `JOIN` node stores `join_type`, `join_conditions`, and `join_algorithm` properties. A `FILTER` node stores `predicate_expressions` and `selectivity_hint` properties.\n\nThis flexible design allows the same node structure to represent both logical operations (where `join_algorithm` might be unspecified) and physical operations (where `join_algorithm` specifies `HASH_JOIN` or `NESTED_LOOP_JOIN`). The optimizer can gradually add more specific properties as it transforms logical plans into physical plans.\n\n#### Cost Annotation System\n\nEvery operator node carries cost annotations that accumulate bottom-up through the plan tree. The `CostEstimate` structure captures multiple cost dimensions that affect execution performance:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `io_cost` | `float` | Estimated disk I/O operations (page reads/writes) |\n| `cpu_cost` | `float` | Estimated CPU operations (tuple processing, comparisons) |\n| `memory_cost` | `float` | Estimated memory consumption (hash tables, sort buffers) |\n| `estimated_rows` | `int` | Predicted output cardinality for this subtree |\n| `startup_cost` | `float` | One-time initialization cost before producing first row |\n| `cost_factors` | `dict` | Operator-specific cost components for debugging |\n| `confidence_level` | `float` | Reliability estimate for cost prediction (0.0 to 1.0) |\n\nThe `total_cost` property combines `io_cost`, `cpu_cost`, and `memory_cost` using configurable weight factors:\n\n```\ntotal_cost = (io_cost * IO_PAGE_COST) + \n             (cpu_cost * CPU_TUPLE_COST) + \n             (memory_cost * MEMORY_PAGE_COST)\n```\n\nCost accumulation follows standard tree traversal patterns. Leaf nodes (typically `SCAN` operators) have costs determined by table statistics and access method selection. Internal nodes accumulate costs from their children plus their own processing costs. This bottom-up cost propagation allows the optimizer to compare alternative plans by examining their root node costs.\n\n### Tree Traversal and Manipulation\n\nQuery plan trees support standard tree traversal algorithms plus specialized operations for plan analysis and transformation. These operations form the foundation for cost calculation, plan comparison, and optimization rule application.\n\n#### Tree Traversal Methods\n\n| Method | Traversal Order | Returns | Primary Use Cases |\n|--------|----------------|---------|-------------------|\n| `traverse_preorder()` | Parent before children | `Iterator[OperatorNode]` | Top-down plan analysis, rule application |\n| `traverse_postorder()` | Children before parent | `Iterator[OperatorNode]` | Bottom-up cost calculation, tree validation |\n| `find_nodes_by_type(type)` | Breadth-first search | `List[OperatorNode]` | Finding all operators of specific type |\n| `get_all_tables()` | Depth-first search | `List[str]` | Extracting table dependencies for statistics |\n\n**Preorder traversal** visits each node before visiting its children, making it ideal for top-down transformations like predicate pushdown where parent operators propagate constraints to child operators. The algorithm maintains a stack of nodes to visit, processing each node and then adding its children to the stack for later processing.\n\n**Postorder traversal** visits children before their parent, making it essential for bottom-up cost calculation where each node's cost depends on its children's costs. The algorithm uses a modified depth-first search that marks nodes as visited only after processing all their children.\n\nThe traversal iterators yield `OperatorNode` objects rather than returning complete lists, allowing memory-efficient processing of large plan trees without materializing the entire traversal sequence in memory.\n\n#### Plan Tree Manipulation\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `add_child(child)` | `child: OperatorNode` | `None` | Append child operator to current node |\n| `replace_child(old, new)` | `old: OperatorNode, new: OperatorNode` | `bool` | Replace existing child with new subtree |\n| `remove_child(child)` | `child: OperatorNode` | `bool` | Remove child operator from current node |\n| `clone()` | None | `OperatorNode` | Create deep copy of subtree |\n| `validate_tree()` | None | `List[str]` | Check tree structure invariants |\n\nTree manipulation methods maintain structural invariants and update cost annotations automatically. When adding a child operator, the parent recalculates its cost estimate to include the new child's contribution. When replacing a subtree, the parent updates its output schema and estimated row count based on the new child's properties.\n\nThe `clone()` method creates deep copies of plan subtrees for exploring alternative optimizations without modifying the original plan. This supports optimization algorithms that need to try multiple transformations and compare their costs before committing to a specific approach.\n\n#### Plan Tree Validation\n\nPlan tree validation ensures structural correctness and semantic consistency throughout the optimization process:\n\n1. **Structural validation** confirms that the tree has proper parent-child relationships, no circular references, and appropriate operator arities (unary operators have one child, binary operators have two children).\n\n2. **Schema validation** verifies that each operator's input schema matches its children's output schemas and that column references in properties (like join conditions) refer to columns that actually exist in the input schemas.\n\n3. **Cost consistency validation** checks that each node's cost estimate properly incorporates its children's costs and that estimated row counts decrease monotonically through filter operations.\n\n4. **Logical consistency validation** ensures that operators appear in valid sequences - for example, that join operations have appropriate join conditions and that aggregation operations group by columns that exist in their input schemas.\n\n#### Pretty Printing and Debugging\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `pretty_print(indent, show_costs)` | `indent: int = 0, show_costs: bool = True` | `str` | Generate indented tree representation |\n| `explain_plan(verbose)` | `verbose: bool = False` | `str` | Generate human-readable execution plan explanation |\n| `cost_breakdown()` | None | `dict` | Detailed cost analysis by operator type |\n\nThe pretty printing system generates indented tree representations that visualize plan structure and cost information:\n\n```\nHASH_JOIN (cost=1250.0, rows=1000)\n├── SEQUENTIAL_SCAN customers (cost=100.0, rows=10000)\n│   └── FILTER age > 25 (cost=50.0, rows=5000)\n└── INDEX_SCAN orders(customer_id) (cost=200.0, rows=2000)\n    └── FILTER order_date > '2023-01-01' (cost=25.0, rows=1000)\n```\n\nEach line shows the operator type, key properties, estimated cost, and predicted row count. Indentation levels indicate parent-child relationships, with children indented more deeply than their parents. Optional cost details show the breakdown between I/O, CPU, and memory costs for detailed performance analysis.\n\n### Architecture Decision: Tree vs DAG\n\n> **Decision: Use Tree Structure Rather Than Directed Acyclic Graph (DAG)**\n> - **Context**: Query plans can potentially share common subexpressions where the same subtree appears multiple times. DAGs could eliminate duplicate computation, but trees are simpler to implement and reason about.\n> - **Options Considered**: Tree structure, DAG with shared nodes, hybrid approach with subquery materialization\n> - **Decision**: Implement pure tree structure without shared subtrees\n> - **Rationale**: Tree structure significantly simplifies traversal algorithms, cost calculation, and plan manipulation. Most queries have limited subexpression sharing opportunities, and the added complexity of DAG management outweighs performance benefits for our learning-focused implementation.\n> - **Consequences**: Some queries with repeated subexpressions may have suboptimal plans, but implementation complexity remains manageable and debugging is much easier.\n\n| Option | Pros | Cons | Implementation Complexity |\n|--------|------|------|---------------------------|\n| **Tree Structure** | Simple traversal, easy cost calculation, clear ownership | Potential duplicate computation, larger memory usage | Low - standard tree algorithms |\n| **DAG with Shared Nodes** | Eliminates duplicate subexpressions, optimal for complex queries | Complex traversal, reference counting, cycle detection | High - requires graph algorithms |\n| **Hybrid with Materialization** | Benefits of DAG where needed, tree simplicity elsewhere | Complex decision logic, mixed paradigms | Medium - selective optimization |\n\nThe tree structure decision prioritizes implementation clarity over theoretical optimality. In production database systems, DAG-based optimizers can achieve better performance on queries with significant subexpression sharing, but they require sophisticated graph management algorithms for traversal, cost calculation, and memory management.\n\nOur tree-based approach uses standard algorithms that any developer can understand and debug. Cost calculation follows simple bottom-up accumulation. Tree traversal uses familiar recursive patterns. Plan manipulation operations like node replacement and subtree cloning work predictably without complex reference management.\n\nThe trade-off means some queries might generate suboptimal plans where the same computation appears multiple times, but this aligns with our learning objectives. Understanding how to build a robust tree-based optimizer provides the foundation for later exploring more advanced DAG-based approaches.\n\n#### Tree Structure Implications\n\nTree structure shapes several key aspects of plan representation and manipulation:\n\n1. **Unique Parent Relationship**: Each operator node has exactly one parent (except the root), eliminating ambiguity about data flow direction and cost responsibility. This simplifies cost accumulation because each subtree's cost contributes to exactly one parent.\n\n2. **Simple Memory Management**: Tree nodes can use straightforward ownership semantics where each parent owns its children. Cleanup becomes simple recursive deallocation without reference counting or garbage collection complexity.\n\n3. **Predictable Traversal**: Tree traversal algorithms have well-defined termination conditions and visit each node exactly once. This makes plan analysis and transformation algorithms more predictable and easier to debug.\n\n4. **Clear Optimization Boundaries**: Each subtree represents an independent optimization unit that can be analyzed and transformed without affecting other parts of the plan. This modularity supports incremental optimization approaches.\n\n### Common Implementation Pitfalls\n\nUnderstanding common mistakes helps avoid implementation problems that can derail query optimizer development. These pitfalls often stem from misunderstanding the relationship between logical and physical operators or incorrectly implementing tree traversal algorithms.\n\n⚠️ **Pitfall: Mixing Logical and Physical Properties in Same Node**\n\nBeginning developers often store both logical information (like \"join these tables\") and physical implementation details (like \"use hash join algorithm\") in the same operator node from the beginning. This makes it impossible to explore different physical implementations for the same logical operation.\n\n**Why it's wrong**: The optimizer needs to generate multiple physical plans for each logical plan to compare their costs. If logical and physical properties are mixed, the optimizer can't represent alternative implementations.\n\n**How to fix**: Keep logical and physical properties separate. During logical planning, store only high-level operation descriptions. During physical planning, add implementation-specific properties while preserving logical information for debugging.\n\n⚠️ **Pitfall: Incorrect Cost Accumulation Direction**\n\nA common mistake is trying to calculate costs top-down (from parent to children) instead of bottom-up (from children to parent). This leads to circular dependencies where a parent's cost depends on children that haven't been costed yet.\n\n**Why it's wrong**: Each operator's cost depends on the amount of data it receives from its children. You can't determine how much work a join operator will do until you know how many rows its child operators will produce.\n\n**How to fix**: Always use postorder traversal for cost calculation. Calculate each node's cost only after all its children have been costed. Store intermediate results to avoid recalculation.\n\n⚠️ **Pitfall: Modifying Trees During Traversal**\n\nModifying a tree structure while traversing it can cause traversal algorithms to skip nodes, visit nodes multiple times, or encounter dangling references to deleted nodes.\n\n**Why it's wrong**: Tree modification changes parent-child relationships that the traversal algorithm depends on. Adding or removing nodes can invalidate iterators or recursive function call stacks.\n\n**How to fix**: Separate traversal from modification. Collect nodes that need modification during traversal, then apply modifications after traversal completes. Alternatively, use immutable tree operations that create new trees instead of modifying existing ones.\n\n⚠️ **Pitfall: Forgetting Schema Propagation**\n\nFailing to properly propagate column schemas through the plan tree leads to errors when operators reference columns that don't exist in their input streams.\n\n**Why it's wrong**: Each operator transforms its input schema into an output schema. Child operators must provide columns that parent operators expect to consume. Schema mismatches cause runtime errors during plan execution.\n\n**How to fix**: Implement schema calculation as part of plan construction. Each operator calculates its output schema based on its input schemas and its specific operation. Validate schema consistency during plan validation.\n\n⚠️ **Pitfall: Inconsistent Node Identification**\n\nUsing inconsistent or non-unique node identifiers makes plan debugging and comparison nearly impossible. Without reliable node identification, it's difficult to track which optimizations were applied or compare alternative plans.\n\n**Why it's wrong**: Plan optimization involves generating many alternative plans and comparing their structures. Without consistent identification, the optimizer can't determine whether two plans are equivalent or track the source of performance differences.\n\n**How to fix**: Generate unique, stable identifiers for each node using a combination of operator type, properties hash, and creation sequence. Include enough information to uniquely identify each node while keeping identifiers stable across optimization runs.\n\n⚠️ **Pitfall: Inefficient Tree Cloning**\n\nImplementing tree cloning with shallow copying instead of deep copying causes multiple plans to share the same node objects, leading to unexpected mutations when optimizing alternative plans.\n\n**Why it's wrong**: Query optimization explores many alternative plans by cloning and modifying existing plans. If clones share node objects, modifications to one plan affect other plans, making cost comparisons invalid.\n\n**How to fix**: Implement proper deep cloning that creates independent copies of all nodes, properties, and cost estimates. Consider using immutable data structures to avoid cloning overhead while preventing accidental mutations.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Tree Structure | Python dataclass with recursive references | Immutable tree with persistent data structures |\n| Cost Calculation | Recursive function with memoization | Visitor pattern with cost accumulation |\n| Tree Traversal | Generator functions with yield | Iterator protocol with custom traversal state |\n| Operator Properties | Python dict with string keys | Typed property classes with validation |\n| Plan Validation | Simple assertion checks | Comprehensive validation framework |\n| Pretty Printing | String concatenation with recursion | Template-based formatting with customization |\n\n#### Recommended File Structure\n\n```\nquery_optimizer/\n├── plan/\n│   ├── __init__.py\n│   ├── operator_node.py          ← OperatorNode class and tree operations\n│   ├── operator_types.py         ← OperatorType enum and constants\n│   ├── execution_plan.py         ← ExecutionPlan wrapper with metadata\n│   ├── cost_estimate.py          ← CostEstimate class with arithmetic operations\n│   └── plan_printer.py           ← Pretty printing and plan explanation\n├── statistics/\n│   ├── table_statistics.py       ← TableStatistics and ColumnStatistics\n│   └── histogram.py              ← HistogramBucket for advanced statistics\n└── tests/\n    ├── test_operator_node.py     ← Unit tests for tree operations\n    ├── test_execution_plan.py    ← Integration tests for complete plans\n    └── test_plan_traversal.py    ← Tests for traversal algorithms\n```\n\n#### Infrastructure Starter Code\n\n**operator_types.py** - Complete enumeration and constants:\n\n```python\nfrom enum import Enum\nfrom typing import Dict, Any\n\nclass OperatorType(Enum):\n    \"\"\"Enumeration of logical and physical operator types.\"\"\"\n    # Logical operators\n    SCAN = \"SCAN\"\n    FILTER = \"FILTER\" \n    JOIN = \"JOIN\"\n    PROJECT = \"PROJECT\"\n    SORT = \"SORT\"\n    AGGREGATE = \"AGGREGATE\"\n    UNION = \"UNION\"\n    INTERSECT = \"INTERSECT\"\n    LIMIT = \"LIMIT\"\n    \n    # Physical scan operators\n    SEQUENTIAL_SCAN = \"SEQUENTIAL_SCAN\"\n    INDEX_SCAN = \"INDEX_SCAN\"\n    BITMAP_SCAN = \"BITMAP_SCAN\"\n    \n    # Physical join operators\n    HASH_JOIN = \"HASH_JOIN\"\n    NESTED_LOOP_JOIN = \"NESTED_LOOP_JOIN\"\n    SORT_MERGE_JOIN = \"SORT_MERGE_JOIN\"\n\nclass JoinType(Enum):\n    \"\"\"Join algorithm types.\"\"\"\n    INNER = \"INNER\"\n    LEFT = \"LEFT\"\n    RIGHT = \"RIGHT\"\n    FULL = \"FULL\"\n    CROSS = \"CROSS\"\n\n# Cost model constants\nIO_PAGE_COST = 1.0\nCPU_TUPLE_COST = 0.01\nMEMORY_PAGE_COST = 0.05\nSEQUENTIAL_MULTIPLIER = 1.0\nRANDOM_MULTIPLIER = 4.0\n\n# Optimization limits\nOPTIMIZATION_TIMEOUT = 30.0  # seconds\nMAX_JOIN_ENUMERATION = 12    # tables\nSELECTIVITY_THRESHOLD = 0.1  # for index vs sequential scan\n\ndef get_physical_operators(logical_type: OperatorType) -> List[OperatorType]:\n    \"\"\"Return physical operator variants for logical operator type.\"\"\"\n    physical_mapping = {\n        OperatorType.SCAN: [OperatorType.SEQUENTIAL_SCAN, OperatorType.INDEX_SCAN],\n        OperatorType.JOIN: [OperatorType.HASH_JOIN, OperatorType.NESTED_LOOP_JOIN, \n                           OperatorType.SORT_MERGE_JOIN],\n        # Other operators map to themselves\n    }\n    return physical_mapping.get(logical_type, [logical_type])\n```\n\n**cost_estimate.py** - Complete cost estimation with arithmetic:\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any\nimport operator_types as opt\n\n@dataclass\nclass CostEstimate:\n    \"\"\"Represents estimated execution cost for query plan operator.\"\"\"\n    io_cost: float = 0.0\n    cpu_cost: float = 0.0  \n    memory_cost: float = 0.0\n    estimated_rows: int = 0\n    startup_cost: float = 0.0\n    cost_factors: Dict[str, float] = field(default_factory=dict)\n    confidence_level: float = 1.0\n    \n    @property\n    def total_cost(self) -> float:\n        \"\"\"Calculate total weighted cost across all dimensions.\"\"\"\n        return (self.io_cost * opt.IO_PAGE_COST + \n                self.cpu_cost * opt.CPU_TUPLE_COST + \n                self.memory_cost * opt.MEMORY_PAGE_COST + \n                self.startup_cost)\n    \n    def add_cost(self, other: 'CostEstimate') -> 'CostEstimate':\n        \"\"\"Combine two cost estimates by adding components.\"\"\"\n        return CostEstimate(\n            io_cost=self.io_cost + other.io_cost,\n            cpu_cost=self.cpu_cost + other.cpu_cost,\n            memory_cost=self.memory_cost + other.memory_cost,\n            estimated_rows=max(self.estimated_rows, other.estimated_rows),\n            startup_cost=self.startup_cost + other.startup_cost,\n            cost_factors={**self.cost_factors, **other.cost_factors},\n            confidence_level=min(self.confidence_level, other.confidence_level)\n        )\n    \n    def scale_by_factor(self, factor: float) -> 'CostEstimate':\n        \"\"\"Scale all cost components by multiplicative factor.\"\"\"\n        return CostEstimate(\n            io_cost=self.io_cost * factor,\n            cpu_cost=self.cpu_cost * factor,\n            memory_cost=self.memory_cost * factor,\n            estimated_rows=int(self.estimated_rows * factor),\n            startup_cost=self.startup_cost * factor,\n            cost_factors={k: v * factor for k, v in self.cost_factors.items()},\n            confidence_level=self.confidence_level\n        )\n```\n\n#### Core Logic Skeleton Code\n\n**operator_node.py** - Node structure with traversal methods:\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Iterator, Optional\nimport uuid\nfrom operator_types import OperatorType\nfrom cost_estimate import CostEstimate\n\n@dataclass\nclass OperatorNode:\n    \"\"\"Represents single operator in query execution plan tree.\"\"\"\n    operator_type: OperatorType\n    children: List['OperatorNode'] = field(default_factory=list)\n    properties: Dict[str, Any] = field(default_factory=dict)\n    cost_estimate: CostEstimate = field(default_factory=CostEstimate)\n    output_schema: List[str] = field(default_factory=list)\n    estimated_rows: int = 0\n    node_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    \n    def add_child(self, child: 'OperatorNode') -> None:\n        \"\"\"Add child operator and update cost estimates.\"\"\"\n        # TODO 1: Append child to children list\n        # TODO 2: Recalculate this node's cost_estimate to include child's cost\n        # TODO 3: Update estimated_rows based on operator semantics\n        # TODO 4: Update output_schema based on operator type and child schemas\n        # Hint: Different operator types combine child costs differently\n        pass\n    \n    def traverse_preorder(self) -> Iterator['OperatorNode']:\n        \"\"\"Traverse tree in preorder (parent before children).\"\"\"\n        # TODO 1: Yield self first\n        # TODO 2: Recursively yield all nodes from each child subtree\n        # Hint: Use 'yield from' to flatten nested iterators\n        pass\n    \n    def traverse_postorder(self) -> Iterator['OperatorNode']:\n        \"\"\"Traverse tree in postorder (children before parent).\"\"\"\n        # TODO 1: Recursively yield all nodes from each child subtree first\n        # TODO 2: Yield self after all children have been yielded\n        # Hint: This is essential for bottom-up cost calculation\n        pass\n    \n    def find_nodes_by_type(self, target_type: OperatorType) -> List['OperatorNode']:\n        \"\"\"Find all nodes in subtree matching specified operator type.\"\"\"\n        # TODO 1: Initialize empty result list\n        # TODO 2: Use preorder traversal to visit all nodes\n        # TODO 3: Add nodes with matching operator_type to result\n        # TODO 4: Return complete result list\n        # Hint: This is useful for finding all scan nodes or all join nodes\n        pass\n    \n    def calculate_subtree_cost(self) -> CostEstimate:\n        \"\"\"Calculate total cost for this operator and all its children.\"\"\"\n        # TODO 1: Start with this node's base cost (before adding children)\n        # TODO 2: Use postorder traversal to ensure children are costed first\n        # TODO 3: Add each child's subtree cost to running total\n        # TODO 4: Store result in cost_estimate field and return it\n        # Hint: Different operators combine child costs differently\n        pass\n    \n    def pretty_print(self, indent: int = 0, show_costs: bool = True) -> str:\n        \"\"\"Generate indented tree representation for debugging.\"\"\"\n        # TODO 1: Create indentation string based on depth level\n        # TODO 2: Format operator type and key properties on first line\n        # TODO 3: Optionally include cost and row count information\n        # TODO 4: Recursively print all children with increased indentation\n        # TODO 5: Combine all lines and return complete tree representation\n        # Hint: Use \"├──\" and \"└──\" characters for tree structure visualization\n        pass\n    \n    def get_all_tables(self) -> List[str]:\n        \"\"\"Extract all table names referenced in this subtree.\"\"\"\n        # TODO 1: Initialize empty table name set to avoid duplicates\n        # TODO 2: Traverse all nodes in subtree\n        # TODO 3: For SCAN operators, extract table name from properties\n        # TODO 4: Return sorted list of unique table names\n        # Hint: Table names are typically stored in properties['table_name']\n        pass\n    \n    def clone(self) -> 'OperatorNode':\n        \"\"\"Create deep copy of this subtree for plan exploration.\"\"\"\n        # TODO 1: Create new node with same operator_type and properties copy\n        # TODO 2: Recursively clone all children\n        # TODO 3: Copy cost_estimate, output_schema, and estimated_rows\n        # TODO 4: Generate new unique node_id for the clone\n        # Hint: Use copy.deepcopy for properties dict to avoid shared references\n        pass\n```\n\n**execution_plan.py** - Plan wrapper with metadata:\n\n```python\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Dict, Any\nfrom operator_node import OperatorNode\nfrom cost_estimate import CostEstimate\n\n@dataclass  \nclass ExecutionPlan:\n    \"\"\"Complete query execution plan with metadata and cost information.\"\"\"\n    root: OperatorNode\n    total_cost: CostEstimate = field(default_factory=CostEstimate) \n    optimization_metadata: Dict[str, Any] = field(default_factory=dict)\n    plan_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    created_timestamp: datetime = field(default_factory=datetime.now)\n    \n    def calculate_total_cost(self) -> CostEstimate:\n        \"\"\"Calculate and cache total execution cost for entire plan.\"\"\"\n        # TODO 1: Use root node's calculate_subtree_cost() method\n        # TODO 2: Store result in total_cost field\n        # TODO 3: Add plan-level metadata to optimization_metadata\n        # TODO 4: Return the calculated cost estimate\n        # Hint: This should trigger bottom-up cost calculation through entire tree\n        pass\n    \n    def pretty_print(self, show_costs: bool = True) -> str:\n        \"\"\"Generate formatted plan representation with header information.\"\"\"\n        # TODO 1: Create header with plan_id, timestamp, and total cost\n        # TODO 2: Call root.pretty_print() for tree structure  \n        # TODO 3: Add footer with summary statistics (total nodes, etc.)\n        # TODO 4: Return complete formatted representation\n        # Hint: Include optimization metadata for debugging optimization decisions\n        pass\n```\n\n#### Milestone Checkpoint\n\nAfter implementing plan representation, verify your implementation with these tests:\n\n**Basic Tree Construction Test:**\n```python\n# Create simple plan: SELECT * FROM users WHERE age > 25\nscan_node = OperatorNode(\n    operator_type=OperatorType.SEQUENTIAL_SCAN,\n    properties={'table_name': 'users'},\n    output_schema=['id', 'name', 'age', 'email'],\n    estimated_rows=10000\n)\n\nfilter_node = OperatorNode(\n    operator_type=OperatorType.FILTER,\n    properties={'predicate': 'age > 25'},\n    estimated_rows=5000\n)\nfilter_node.add_child(scan_node)\n\nplan = ExecutionPlan(root=filter_node)\n```\n\n**Expected Behavior:**\n- Tree structure should have filter node as root with scan node as child\n- `traverse_preorder()` should yield filter node first, then scan node\n- `traverse_postorder()` should yield scan node first, then filter node  \n- `pretty_print()` should show indented tree with costs\n- `get_all_tables()` should return ['users']\n\n**Common Issues to Check:**\n- Node IDs should be unique across all nodes\n- Cost estimates should accumulate from children to parents\n- Schema propagation should work correctly through operator chain\n- Tree traversal should visit each node exactly once\n\n\n## Cost Estimation Component\n\n> **Milestone(s):** Milestone 2 - implements statistical cost models for predicting query execution costs, including selectivity estimation and cardinality calculation.\n\n### Mental Model: Construction Estimating\n\nThink of query cost estimation like estimating a construction project. When a general contractor bids on building a house, they don't just guess at the total cost. Instead, they break down the project into measurable components: materials (concrete, lumber, fixtures), labor (hours for different skilled trades), and equipment (crane rental, specialized tools). Each component has a different cost structure - some are fixed costs regardless of project size, others scale linearly with square footage, and some have economies of scale that reduce per-unit costs as volume increases.\n\nQuery cost estimation follows this same principle. Just as a contractor estimates concrete costs based on square footage and local material prices, our cost estimator predicts I/O costs based on table size and storage characteristics. Labor costs in construction depend on the complexity of work and available workforce - similarly, our CPU costs depend on the complexity of operations (joins vs scans) and the processing power available. A contractor also considers the sequence of work - you can't install drywall before framing - just as we must account for data dependencies where join costs depend on the results of earlier filter operations.\n\nThe key insight from construction estimating is that **accuracy improves with better data about actual conditions**. A contractor who has built similar houses in the same neighborhood with the same subcontractors will provide more accurate estimates than one working from generic industry averages. Similarly, our cost estimator becomes more accurate when it has current statistics about table sizes, data distributions, and actual selectivity patterns rather than relying on outdated or generic assumptions.\n\nHowever, just as construction projects face uncertainties (weather delays, material price changes, hidden structural problems), query cost estimation must handle inherent unpredictability in data access patterns, cache behavior, and concurrent workload interference. The goal is not perfect prediction but rather sufficient accuracy to distinguish between genuinely good and poor plan choices.\n\n![Cost Estimation Interaction Sequence](./diagrams/cost-estimation-sequence.svg)\n\n### Statistics Collection and Maintenance\n\nThe foundation of accurate cost estimation lies in comprehensive statistics collection. Without current data about table characteristics, our cost estimates become as unreliable as a contractor bidding on a house they've never seen. The `TableStatistics` structure captures the essential information needed for cost modeling.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `table_name` | str | Unique identifier for the table being analyzed |\n| `row_count` | int | Total number of rows currently in the table |\n| `page_count` | int | Number of storage pages occupied by table data |\n| `column_stats` | dict | Mapping from column names to `ColumnStatistics` objects |\n| `last_updated` | datetime | Timestamp when statistics were last refreshed |\n| `sample_rate` | float | Fraction of table sampled for statistics (1.0 = full scan) |\n| `index_statistics` | dict | Mapping from index names to index-specific statistics |\n| `clustering_factor` | float | Measure of how well table storage aligns with query access patterns |\n\nThe `ColumnStatistics` structure provides detailed information about individual columns that drives selectivity estimation. This granular data allows the cost estimator to make informed predictions about how many rows will survive filter predicates.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `column_name` | str | Name of the column being analyzed |\n| `distinct_values` | int | Number of unique values in the column (cardinality) |\n| `null_count` | int | Number of rows where this column contains NULL |\n| `min_value` | Any | Minimum value found in the column (for range estimates) |\n| `max_value` | Any | Maximum value found in the column (for range estimates) |\n| `most_common_values` | List[tuple] | Most frequent values with their occurrence counts |\n| `histogram_buckets` | List[HistogramBucket] | Distribution information for selectivity estimation |\n| `correlation_with_storage` | float | How well column values correlate with physical storage order |\n| `average_width` | int | Average size in bytes for variable-length columns |\n\nStatistics collection operates through the `collectStatistics` method, which can run in different modes depending on performance requirements and accuracy needs. For large tables, full table scans for statistics collection can be prohibitively expensive, so the system supports sampling-based approaches where accuracy is traded for collection speed.\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `collectStatistics` | table: str, sample_rate: float | `TableStatistics` | Gather comprehensive statistics for cost estimation |\n| `refreshColumnStats` | table: str, column: str | `ColumnStatistics` | Update statistics for a specific column |\n| `estimateStalenessCost` | stats: `TableStatistics` | float | Predict accuracy degradation since last update |\n| `shouldRefreshStats` | stats: `TableStatistics`, query_pattern: str | bool | Determine if statistics update is needed |\n\nThe statistics collection process follows a systematic approach that balances accuracy with performance impact:\n\n1. The system determines whether full table analysis or sampling is appropriate based on table size and available maintenance windows\n2. For sampled statistics, it uses systematic sampling to ensure representative coverage across the entire table rather than clustering samples in one region\n3. Column analysis computes basic statistics (min, max, distinct count) along with frequency distributions for the most common values\n4. Histogram construction divides the value range into buckets with approximately equal frequency, enabling accurate range query selectivity estimation\n5. Index statistics collection analyzes the correlation between index order and table storage order, which affects the cost of index-driven table access\n6. The system records collection metadata including sample rates and timestamps to enable staleness detection during query optimization\n\nStatistics maintenance requires careful scheduling to avoid impacting production query performance while ensuring cost estimation accuracy. Stale statistics can lead to dramatically poor plan choices, particularly for tables with rapidly changing data distributions.\n\n> **Key Insight**: The cost of statistics collection must be weighed against the benefit of improved plan quality. For frequently queried tables, the performance gain from better optimization easily justifies regular statistics updates. For rarely accessed tables, infrequent updates based on data modification thresholds provide better resource allocation.\n\n![Statistics and Cost Model](./diagrams/statistics-data-model.svg)\n\n### Selectivity and Cardinality Estimation\n\nSelectivity estimation predicts what fraction of rows will survive filter predicates, while cardinality estimation calculates the absolute number of rows in intermediate results. These predictions drive the entire cost estimation process, as they determine how much data flows between operators and influences both I/O and CPU costs.\n\nThe `CostEstimate` structure captures the multi-dimensional nature of query execution costs, distinguishing between different resource types that contribute to total execution time.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `io_cost` | float | Estimated disk I/O operations required |\n| `cpu_cost` | float | Estimated CPU cycles for processing |\n| `memory_cost` | float | Estimated memory allocation and access overhead |\n| `estimated_rows` | int | Predicted number of output rows |\n| `startup_cost` | float | Fixed cost paid before first row is produced |\n| `total_cost` | property | Computed total combining all cost components |\n| `cost_factors` | dict | Breakdown of contributing factors for debugging |\n| `confidence_level` | float | Estimated accuracy of the cost prediction |\n\nSelectivity estimation algorithms vary based on the type of predicate and available statistical information. The system handles several common predicate patterns with specialized estimation techniques.\n\n**Equality Predicates** (`column = value`): For equality predicates, selectivity depends on whether the queried value is among the most common values tracked in statistics. If the value appears in the `most_common_values` list, its exact frequency provides precise selectivity. For values not in the common list, the estimator assumes uniform distribution among the remaining values: `selectivity = (1 - sum_of_common_frequencies) / (distinct_values - num_common_values)`.\n\n**Range Predicates** (`column > value`, `column BETWEEN low AND high`): Range selectivity uses histogram buckets to estimate what fraction of values fall within the specified range. The algorithm identifies which histogram buckets intersect the query range and uses linear interpolation within partially overlapping buckets. This approach provides reasonable accuracy for uniformly distributed data but can be misleading for skewed distributions within buckets.\n\n**Join Selectivity**: Join cardinality estimation uses the fundamental principle that the result size depends on the degree of correlation between join columns. For an inner join between tables with `R` and `S` rows on columns with `D_R` and `D_S` distinct values respectively, the estimated result size is: `(R * S) / max(D_R, D_S)`. This formula assumes that join keys are foreign key relationships where one side has unique values.\n\n| Predicate Type | Estimation Method | Accuracy Factors | Fallback Strategy |\n|----------------|------------------|------------------|-------------------|\n| Equality | Most common values or uniform distribution | Frequency of queried value, statistics staleness | Default selectivity of 0.1% |\n| Range | Histogram bucket interpolation | Distribution uniformity, bucket granularity | Linear interpolation between min/max |\n| LIKE patterns | Pattern complexity analysis | Prefix selectivity, wildcard positions | Conservative estimate of 25% |\n| IN lists | Sum of individual equality estimates | Length of IN list, value frequency | Treat as multiple OR conditions |\n| IS NULL | Direct from null_count statistics | Statistics accuracy | Default 5% selectivity |\n\nThe cardinality estimation process builds upon selectivity calculations to predict intermediate result sizes throughout the query plan:\n\n1. **Base Table Cardinality**: Start with table row counts from statistics, adjusted for any concurrent modifications detected since statistics collection\n2. **Filter Application**: Apply selectivity estimates for each filter predicate, assuming independence between predicates (which often underestimates real selectivity due to correlation)\n3. **Join Cardinality**: Estimate join result size using the join selectivity formulas, considering the cardinality of input relations after their filter predicates\n4. **Projection Effects**: Account for duplicate elimination in DISTINCT projections using distinct value statistics\n5. **Aggregation Cardinality**: Estimate GROUP BY result size based on the distinct value counts of grouping columns\n\nCommon complications arise when predicates are correlated or when statistics don't capture important data characteristics. For example, in a table of orders with both `order_date` and `ship_date` columns, filtering on `order_date > '2023-01-01'` and `ship_date < '2023-02-01'` involves strong correlation that independence assumptions ignore.\n\n> **Critical Consideration**: Selectivity estimation errors compound throughout the query plan tree. A 2x error in early filter selectivity can become an 8x error in final join cardinality, leading to dramatically wrong plan choices. Conservative estimation that slightly overestimates intermediate result sizes often produces more robust plans than aggressive estimates that risk severe underestimation.\n\n### I/O and CPU Cost Modeling\n\nThe cost model translates estimated cardinalities and selectivities into predicted resource consumption, providing a unified metric for comparing different execution plans. The model must account for the fundamental difference between I/O-bound and CPU-bound operations while handling the complex interactions between memory usage, cache effects, and concurrent workload interference.\n\nI/O cost modeling centers on the observation that disk access patterns dramatically affect performance. Sequential scans that read contiguous pages benefit from operating system prefetching and storage device optimizations, while random access patterns incur significant seek overhead on traditional spinning disks and reduced parallelism on SSDs.\n\n| Cost Component | Base Unit | Scaling Factors | Description |\n|----------------|-----------|-----------------|-------------|\n| Sequential Scan I/O | `IO_PAGE_COST * page_count` | `SEQUENTIAL_MULTIPLIER = 0.3` | Reading table pages in storage order |\n| Random I/O | `IO_PAGE_COST * page_count` | `RANDOM_MULTIPLIER = 2.0` | Scattered reads following index pointers |\n| Index Scan I/O | `IO_PAGE_COST * (index_pages + table_pages)` | Clustering factor adjustment | Index traversal plus table access |\n| Sort I/O | `IO_PAGE_COST * pages * log(pages)` | Memory buffer consideration | External sort with disk-based merge phases |\n\nCPU cost modeling focuses on the computational work required to process individual tuples through various operators. Different operators have vastly different per-tuple costs - a simple projection may involve only memory copying, while a hash join requires hash computation, probe operations, and potentially complex result construction.\n\n| Operation Type | Per-Tuple Cost | Scaling Factors | Additional Considerations |\n|----------------|----------------|-----------------|--------------------------|\n| Sequential Scan | `CPU_TUPLE_COST * 1.0` | Filter complexity multiplier | Base cost for reading and basic processing |\n| Hash Join Build | `CPU_TUPLE_COST * 3.0` | Hash function complexity | Building hash table for smaller relation |\n| Hash Join Probe | `CPU_TUPLE_COST * 2.0` | Hash collision rate | Probing hash table with larger relation |\n| Nested Loop Join | `CPU_TUPLE_COST * 1.5` | Inner loop optimization | Cost per inner tuple access |\n| Sort Operation | `CPU_TUPLE_COST * log(tuples)` | Comparison function cost | Per-tuple cost increases with sort size |\n\nMemory cost modeling addresses the resource consumption and performance impact of memory allocation for operator-specific data structures. Hash joins require memory for hash tables, sorts need memory for buffering, and all operations benefit from larger buffer pools that reduce I/O.\n\nThe `estimateCost` method combines these cost components into a unified prediction that enables plan comparison:\n\n| Method | Parameters | Returns | Description |\n|--------|------------|---------|-------------|\n| `estimateCost` | plan: `ExecutionPlan` | `CostEstimate` | Calculate total predicted execution cost |\n| `estimateOperatorCost` | node: `OperatorNode`, input_stats: dict | `CostEstimate` | Cost for individual operator |\n| `estimateIOCost` | access_method: str, pages: int, selectivity: float | float | I/O cost based on access pattern |\n| `estimateCPUCost` | operation: str, tuples: int, complexity: float | float | CPU cost for computational work |\n| `estimateMemoryCost` | algorithm: str, input_size: int, available_memory: int | float | Memory allocation and pressure costs |\n\nThe cost estimation algorithm traverses the query plan tree in post-order, computing costs from leaves to root:\n\n1. **Leaf Node Costs**: Table scan operators use table statistics to compute base I/O costs, applying sequential or random access multipliers based on scan type and available indexes\n2. **Filter Costs**: Add CPU costs for predicate evaluation and adjust output cardinality based on selectivity estimates\n3. **Join Costs**: Combine input cardinalities with join algorithms to compute both CPU costs (for join processing) and I/O costs (for any required sorting or temporary storage)\n4. **Memory Pressure**: Adjust costs based on available memory - operations that exceed memory capacity incur additional I/O for spilling to disk\n5. **Startup vs Runtime**: Distinguish between fixed startup costs (building hash tables, sorting) and per-tuple runtime costs that scale with data volume\n\n> **Architectural Insight**: The cost model must be calibrated to the target deployment environment. Default costs tuned for spinning disks will make poor decisions on SSD storage, while models optimized for single-user workloads may not account for resource contention in concurrent environments. Parameterizing cost constants enables adaptation to different hardware and workload characteristics.\n\n### Architecture Decision: Histogram vs Uniform Distribution\n\nThe choice between histogram-based statistics and simple uniform distribution assumptions represents a fundamental trade-off between estimation accuracy and system complexity. This decision affects not only the cost estimation component but also statistics collection overhead and query optimization performance.\n\n> **Decision: Use Simplified Histograms with Uniform Distribution Fallback**\n> - **Context**: Selectivity estimation accuracy directly impacts plan quality, but complex statistical models increase collection overhead and optimization time. Production systems show that moderate histogram complexity provides most benefits while avoiding the maintenance burden of sophisticated models.\n> - **Options Considered**: \n>   1. Uniform distribution only (simple, fast, inaccurate for skewed data)\n>   2. Full equi-depth histograms (accurate, complex, expensive to maintain)\n>   3. Simplified histograms with fallbacks (balanced approach)\n> - **Decision**: Implement simplified histograms with 10-20 buckets per column, falling back to uniform distribution when histograms are unavailable or stale\n> - **Rationale**: Analysis of real query workloads shows that even simple histograms provide 80% of the benefit of complex models. The remaining accuracy improvement rarely changes plan selection for typical queries, while the maintenance overhead of detailed histograms impacts system performance.\n> - **Consequences**: Enables reasonable selectivity estimation for most query patterns while keeping statistics collection and optimization overhead manageable. May produce suboptimal plans for queries with highly skewed data access patterns.\n\n| Approach | Accuracy | Collection Overhead | Optimization Speed | Maintenance Complexity |\n|----------|----------|-------------------|-------------------|----------------------|\n| Uniform Distribution | Low for skewed data | Minimal (just min/max/distinct) | Very fast | Simple - just row counts |\n| Simplified Histograms | Good for most queries | Moderate (single table pass) | Fast | Medium - periodic refresh needed |\n| Detailed Histograms | High accuracy | High (multiple passes, sampling) | Slower (complex calculations) | Complex - staleness detection crucial |\n\nThe `HistogramBucket` structure supports the simplified approach while maintaining extensibility for future enhancements:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `bucket_id` | int | Sequential identifier for ordering buckets |\n| `range_start` | Any | Minimum value included in this bucket |\n| `range_end` | Any | Maximum value included in this bucket |\n| `row_count` | int | Number of rows with values in this bucket's range |\n| `distinct_count` | int | Number of unique values within this bucket |\n| `frequency` | float | Fraction of total table rows represented by this bucket |\n\nThe histogram construction algorithm balances accuracy with simplicity:\n\n1. **Sample Selection**: For large tables, use systematic sampling to select a representative subset (typically 10,000-100,000 rows) that spans the entire table\n2. **Bucket Boundary Calculation**: Sort sample values and divide into equal-frequency buckets, ensuring each bucket represents approximately the same number of rows\n3. **Boundary Adjustment**: Adjust bucket boundaries to align with natural value breaks when possible (e.g., month boundaries for dates) to improve estimation for common query patterns\n4. **Distinct Value Estimation**: Within each bucket, estimate distinct values using sampling techniques or assume uniform distribution if detailed analysis is too expensive\n5. **Validation**: Compare histogram estimates against known queries to detect systematic biases and adjust bucket strategies\n\nThe fallback strategy handles cases where histogram information is unavailable or unreliable:\n\n- **Missing Histograms**: Use uniform distribution between min and max values, with conservative selectivity estimates for edge cases\n- **Stale Histograms**: Apply staleness penalties to account for potential distribution changes, gradually falling back to uniform distribution as staleness increases\n- **Insufficient Sample Size**: For small tables or columns with few distinct values, skip histogram construction and use exact frequency counts where possible\n\n> **Implementation Note**: The histogram approach chosen here provides a solid foundation that can be enhanced incrementally. Starting with uniform distribution assumptions, adding basic histograms, and then refining bucket strategies allows the system to evolve without fundamental architectural changes.\n\n### Common Estimation Pitfalls\n\nCost estimation involves numerous subtle sources of error that can dramatically impact plan quality. Understanding these pitfalls helps both implementers avoid common mistakes and users diagnose optimization problems in production systems.\n\n⚠️ **Pitfall: Independence Assumption for Correlated Predicates**\n\nMany cost estimators assume that filter predicates are independent, multiplying individual selectivities to estimate combined selectivity. For a query with `WHERE age > 65 AND status = 'retired'`, the estimator might calculate `selectivity = 0.15 * 0.12 = 0.018`. However, these predicates are strongly correlated - nearly all retired people are over 65. The actual selectivity might be closer to 0.12, making the estimate off by a factor of 6.\n\nThis error compounds in complex queries with many correlated predicates. The fix involves detecting common correlation patterns (age/status, date/season, geographic coordinates) and maintaining correlation statistics for the most important column combinations. When correlation statistics aren't available, using the most selective predicate alone often produces better estimates than independence assumptions.\n\n⚠️ **Pitfall: Stale Statistics Causing Plan Regression**\n\nStatistics become stale as data changes, but the degradation isn't uniform across all table characteristics. Row counts may remain accurate while data distributions shift dramatically. Consider a table that grows from 1 million to 1.5 million rows (50% increase) while a new application feature causes 90% of queries to target recently added data. Statistics showing uniform distribution will dramatically underestimate filter selectivity for these queries.\n\nThe solution requires staleness detection based on both time and data modification patterns. Track not just when statistics were last updated, but also the volume of insertions, deletions, and updates since collection. Implement automatic re-optimization triggers when plan performance deviates significantly from cost estimates.\n\n⚠️ **Pitfall: Ignoring Memory Constraints in Cost Models**\n\nMany cost models assume unlimited memory availability, leading to severe underestimation of costs for operations that exceed available memory. A hash join estimated to cost 1000 units with unlimited memory might actually cost 10,000 units when hash table construction forces frequent disk spills.\n\nAddress this by incorporating memory pressure into cost calculations. Monitor actual memory usage during query execution and adjust cost constants based on observed spill behavior. For operations with variable memory requirements, estimate costs for both in-memory and disk-spill scenarios, weighting by the probability of each outcome.\n\n⚠️ **Pitfall: Linear Cost Scaling Assumptions**\n\nAssuming that costs scale linearly with data size ignores important threshold effects and algorithmic complexity changes. Sorting 1,000 rows might cost 10 units, but sorting 100,000 rows doesn't cost 1,000 units - it might cost 1,500 units due to cache misses and external sort algorithms.\n\nModel these effects by using algorithmic complexity formulas rather than simple linear scaling. For sorting, use `O(n log n)` scaling. For hash operations, account for increased collision rates and memory pressure as hash tables grow. Calibrate these models against actual execution data to ensure accuracy across different data sizes.\n\n⚠️ **Pitfall: Ignoring Concurrent Workload Impact**\n\nCost models developed for single-user systems often fail in production environments with concurrent queries competing for resources. I/O costs increase when multiple queries access storage simultaneously, and CPU costs vary with system load.\n\nIncorporate workload-aware cost adjustments that modify base costs based on current system utilization. This requires monitoring concurrent query activity and adjusting cost models dynamically. Simple approaches include multiplying base costs by load factors, while more sophisticated systems maintain separate cost models for different concurrency levels.\n\n⚠️ **Pitfall: Inadequate Error Propagation Through Plan Trees**\n\nSmall estimation errors at leaf nodes can become large errors at the root of complex query plans. A 20% error in base table cardinality can become a 300% error after multiple joins, completely changing the optimal plan choice.\n\nImplement confidence tracking that propagates estimation uncertainty through the plan tree. Maintain confidence bounds on cardinality estimates and use these bounds to evaluate plan robustness. When confidence is low, prefer plans that perform reasonably across a wide range of actual cardinalities rather than plans that are optimal only for specific estimates.\n\n| Pitfall | Detection Method | Prevention Strategy | Recovery Approach |\n|---------|------------------|-------------------|------------------|\n| Correlated Predicates | Monitor queries with multiple filters on same table | Collect correlation statistics for common column pairs | Use most selective predicate when correlation unknown |\n| Stale Statistics | Compare actual vs predicted cardinalities | Automatic re-collection based on data change volume | Gradual fallback to conservative estimates |\n| Memory Pressure | Monitor query memory usage and spill events | Include memory constraints in cost model | Dynamic re-optimization when spills detected |\n| Nonlinear Scaling | Profile actual costs across different data sizes | Use algorithmic complexity formulas instead of linear | Calibrate cost constants from execution feedback |\n| Concurrency Impact | Track query performance during peak load periods | Load-aware cost model adjustments | Separate cost models for different load levels |\n| Error Propagation | Confidence interval tracking through plan tree | Prefer robust plans when uncertainty is high | Plan re-evaluation when estimates prove inaccurate |\n\n### Implementation Guidance\n\nThe cost estimation component bridges statistical analysis with practical resource modeling. This implementation provides the foundation for effective cost-based optimization while maintaining the flexibility to evolve as understanding of workload patterns improves.\n\n**Technology Recommendations:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Statistics Storage | JSON files with periodic updates | Embedded SQLite database for statistics |\n| Histogram Implementation | Fixed-size arrays with linear search | B-tree structures for efficient range queries |\n| Cost Model Calibration | Hard-coded constants from literature | Machine learning-based cost model training |\n| Staleness Detection | Time-based refresh schedules | Trigger-based statistics invalidation |\n\n**Recommended File Structure:**\n```\noptimizer/\n  cost_estimation/\n    __init__.py                    ← Component interface\n    statistics_collector.py        ← Statistics gathering and maintenance\n    selectivity_estimator.py       ← Predicate selectivity calculation\n    cost_model.py                  ← I/O and CPU cost modeling\n    histogram.py                   ← Distribution modeling utilities\n    cost_estimation_test.py        ← Component test suite\n  data/\n    table_statistics.json          ← Cached statistics storage\n    cost_calibration.json          ← Environment-specific cost constants\n```\n\n**Statistics Collection Infrastructure:**\n```python\n# Complete implementation for statistics gathering\nimport json\nimport time\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass HistogramBucket:\n    bucket_id: int\n    range_start: Any\n    range_end: Any\n    row_count: int\n    distinct_count: int\n    frequency: float\n\n@dataclass\nclass ColumnStatistics:\n    column_name: str\n    distinct_values: int\n    null_count: int\n    min_value: Any\n    max_value: Any\n    most_common_values: List[tuple]\n    histogram_buckets: List[HistogramBucket] = field(default_factory=list)\n    correlation_with_storage: float = 0.0\n    average_width: int = 0\n\n@dataclass \nclass TableStatistics:\n    table_name: str\n    row_count: int\n    page_count: int\n    column_stats: Dict[str, ColumnStatistics] = field(default_factory=dict)\n    last_updated: datetime = field(default_factory=datetime.now)\n    sample_rate: float = 1.0\n    index_statistics: Dict[str, dict] = field(default_factory=dict)\n    clustering_factor: float = 1.0\n\nclass StatisticsCollector:\n    def __init__(self, storage_path: str = \"data/table_statistics.json\"):\n        self.storage_path = storage_path\n        self.cached_stats: Dict[str, TableStatistics] = {}\n        self.load_cached_statistics()\n    \n    def load_cached_statistics(self):\n        \"\"\"Load previously collected statistics from storage.\"\"\"\n        try:\n            with open(self.storage_path, 'r') as f:\n                data = json.load(f)\n                # TODO: Deserialize JSON data into TableStatistics objects\n                # TODO: Handle version compatibility for statistics format changes\n        except FileNotFoundError:\n            self.cached_stats = {}\n    \n    def save_statistics(self):\n        \"\"\"Persist current statistics to storage.\"\"\"\n        # TODO: Serialize TableStatistics objects to JSON format\n        # TODO: Include metadata about collection time and sample rates\n        # TODO: Implement atomic write to prevent corruption during updates\n        pass\n```\n\n**Cost Model Core Logic:**\n```python\n@dataclass\nclass CostEstimate:\n    io_cost: float\n    cpu_cost: float  \n    memory_cost: float\n    estimated_rows: int\n    startup_cost: float\n    cost_factors: Dict[str, float] = field(default_factory=dict)\n    confidence_level: float = 0.8\n    \n    @property\n    def total_cost(self) -> float:\n        return self.io_cost + self.cpu_cost + self.memory_cost + self.startup_cost\n    \n    def add_cost(self, other: 'CostEstimate') -> 'CostEstimate':\n        \"\"\"Combine two cost estimates for sequential operations.\"\"\"\n        # TODO: Add all cost components together\n        # TODO: Handle estimated_rows appropriately based on operation type\n        # TODO: Combine confidence levels using uncertainty propagation formulas\n        # TODO: Merge cost_factors dictionaries for detailed cost breakdown\n        pass\n    \n    def scale_by_factor(self, factor: float) -> 'CostEstimate':\n        \"\"\"Scale cost estimate by a multiplier factor.\"\"\"\n        # TODO: Scale all cost components by the factor\n        # TODO: Keep estimated_rows unchanged (scaling doesn't change output size)  \n        # TODO: Adjust confidence_level based on scaling magnitude\n        pass\n\n# Cost model constants - calibrate these for your environment\nIO_PAGE_COST = 1.0          # Base cost per I/O page read\nCPU_TUPLE_COST = 0.01       # Base cost per tuple processed\nMEMORY_PAGE_COST = 0.001    # Cost per memory page allocated\nSEQUENTIAL_MULTIPLIER = 0.3  # Efficiency factor for sequential I/O\nRANDOM_MULTIPLIER = 2.0     # Penalty factor for random I/O\n\nclass CostModel:\n    def __init__(self):\n        self.io_page_cost = IO_PAGE_COST\n        self.cpu_tuple_cost = CPU_TUPLE_COST\n        self.memory_page_cost = MEMORY_PAGE_COST\n    \n    def estimateCost(self, plan: 'ExecutionPlan') -> CostEstimate:\n        \"\"\"Calculate predicted execution cost for complete plan.\"\"\"\n        # TODO: Traverse plan tree in post-order to calculate costs bottom-up\n        # TODO: Start with leaf nodes (table scans) and work toward root\n        # TODO: For each operator, combine input costs with operator-specific costs\n        # TODO: Accumulate startup costs and per-tuple costs separately\n        # TODO: Return total cost estimate with confidence bounds\n        pass\n    \n    def estimate_scan_cost(self, table_stats: TableStatistics, \n                          selectivity: float = 1.0) -> CostEstimate:\n        \"\"\"Estimate cost for table scan with optional filtering.\"\"\"\n        # TODO: Calculate I/O cost based on page_count and access pattern\n        # TODO: Apply sequential multiplier for full table scans  \n        # TODO: Calculate CPU cost for tuple processing and filter evaluation\n        # TODO: Estimate output rows using table row_count and selectivity\n        # TODO: Set startup_cost to account for query setup overhead\n        pass\n    \n    def estimate_join_cost(self, left_input: CostEstimate, \n                          right_input: CostEstimate, \n                          join_selectivity: float) -> CostEstimate:\n        \"\"\"Estimate cost for join operation between two inputs.\"\"\"\n        # TODO: Choose join algorithm (hash vs nested loop) based on input sizes\n        # TODO: For hash join: build cost + probe cost + memory allocation\n        # TODO: For nested loop: outer loop cost * inner loop iterations\n        # TODO: Calculate output cardinality using join selectivity formula\n        # TODO: Add I/O costs if join exceeds available memory (spill to disk)\n        pass\n```\n\n**Selectivity Estimation Skeleton:**\n```python\nclass SelectivityEstimator:\n    def __init__(self, stats_collector: StatisticsCollector):\n        self.stats = stats_collector\n    \n    def estimate_filter_selectivity(self, table_name: str, \n                                   column_name: str, \n                                   operator: str, \n                                   value: Any) -> float:\n        \"\"\"Estimate fraction of rows surviving a filter predicate.\"\"\"\n        # TODO: Get column statistics from stats collector\n        # TODO: Handle case where statistics are missing (return conservative default)\n        # TODO: For equality: check most_common_values or use uniform distribution\n        # TODO: For range operators: use histogram buckets for estimation\n        # TODO: For LIKE patterns: analyze pattern complexity and estimate\n        # TODO: Return selectivity between 0.0 and 1.0\n        pass\n    \n    def estimate_join_cardinality(self, left_table: str, left_column: str,\n                                 right_table: str, right_column: str) -> int:\n        \"\"\"Estimate number of rows produced by join operation.\"\"\"\n        # TODO: Get statistics for both tables and join columns\n        # TODO: Apply standard join cardinality formula: (R * S) / max(D_R, D_S)\n        # TODO: Adjust for foreign key relationships if detected\n        # TODO: Handle case where one side has much higher cardinality\n        # TODO: Apply conservative bounds to prevent extreme over/underestimation\n        pass\n    \n    def detect_predicate_correlation(self, table_name: str, \n                                   predicates: List[tuple]) -> float:\n        \"\"\"Detect correlation between multiple predicates on same table.\"\"\"\n        # TODO: Check if predicates involve known correlated column pairs\n        # TODO: For uncorrelated predicates: multiply individual selectivities  \n        # TODO: For correlated predicates: use most selective predicate only\n        # TODO: Return combined selectivity estimate accounting for correlation\n        pass\n```\n\n**Milestone Checkpoint:**\nAfter implementing the cost estimation component, verify correct behavior:\n\n1. **Statistics Collection Test**: Run `python -m pytest cost_estimation/statistics_collector_test.py -v`\n   - Should collect row counts, distinct values, and basic histograms\n   - Verify statistics serialization and loading from storage\n   - Test sampling-based collection for large tables\n\n2. **Selectivity Estimation Test**: Create test tables with known data distributions\n   - Execute `collectStatistics(\"test_table\")` and verify histogram accuracy  \n   - Test selectivity estimation: `estimate_filter_selectivity(\"test_table\", \"status\", \"=\", \"active\")` should return reasonable values\n   - Verify join cardinality estimates match expected results for known foreign key relationships\n\n3. **Cost Model Validation**: Compare cost estimates with actual execution measurements\n   - Run identical queries with different plans and measure actual execution time\n   - Verify that cost model ranks plans in same order as actual performance\n   - Check that cost estimates are within 2-3x of actual costs (closer is better, but exact accuracy isn't required)\n\n**Expected Output:**\n```\nStatistics Collection: PASS\n- Collected stats for 3 tables in 1.2 seconds\n- Histogram accuracy: 95% for uniform data, 78% for skewed data\n- Storage size: 15KB for typical table statistics\n\nCost Estimation: PASS  \n- Selectivity estimates within 50% of actual for 85% of test cases\n- Join cardinality estimates within 2x of actual for 92% of test cases\n- Plan ranking correlation with actual performance: 88%\n```\n\n**Debugging Tips:**\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| All selectivity estimates return 0.1 | Missing or corrupt statistics | Check statistics loading, verify table exists | Recollect statistics with `collectStatistics()` |\n| Join cardinality estimates are wildly high | Missing join predicates detected as cross product | Verify join conditions in parsed query | Add cross-product detection and warnings |\n| Cost estimates don't correlate with performance | Cost constants not calibrated for environment | Run benchmark queries and measure actual costs | Adjust `IO_PAGE_COST` and `CPU_TUPLE_COST` constants |\n| Statistics collection takes too long | Full table scan on large tables | Check sampling rate configuration | Set `sample_rate=0.1` for tables over 1M rows |\n| Histogram-based estimates worse than uniform | Insufficient bucket count or poor boundaries | Examine histogram bucket distribution | Increase bucket count or improve boundary selection |\n\n\n## Join Optimization Component\n\n> **Milestone(s):** Milestone 3 - implements dynamic programming algorithms for finding optimal join orders in multi-table queries while managing exponential search complexity.\n\n### Mental Model: Assembly Line Optimization\n\nThink of join ordering as optimizing a manufacturing assembly line that builds complex products from multiple components. Just as a factory must decide the sequence for assembling parts into a final product, the query optimizer must determine the order for combining tables into the final result set.\n\nIn a car manufacturing plant, you might have engines, transmissions, chassis, and electronics arriving from different suppliers. The assembly sequence matters enormously - you can't install the engine after the hood is welded shut, and certain combinations are more efficient than others. If you attach the transmission first, you might need specialized lifting equipment. If you install the electronics early, they might get damaged during heavy welding operations.\n\nSimilarly, in query optimization, each table join is like an assembly operation that combines two intermediate results. The order determines both the cost and feasibility of subsequent operations. Joining two large tables early might create a massive intermediate result that makes all subsequent joins expensive. Conversely, applying selective joins first can dramatically reduce the working set size, making later operations much cheaper.\n\nThe assembly line analogy extends to resource constraints and dependencies. Just as some manufacturing steps require specific equipment or create bottlenecks, different join orders have varying memory requirements and I/O patterns. A hash join needs sufficient memory to build hash tables, while a nested loop join might perform better with smaller inner relations. The optimizer must consider these physical constraints when determining the optimal assembly sequence.\n\n### Dynamic Programming Algorithm\n\nThe core challenge in join ordering is the combinatorial explosion of possibilities. For n tables, there are (2n-2)!/(n-1)! possible join orders when considering all tree shapes. Dynamic programming solves this by building optimal solutions incrementally, reusing previously computed results for subproblems.\n\n**Bottom-Up Subset Enumeration**\n\nThe dynamic programming approach works by considering all possible subsets of tables and computing the cheapest way to join them. We start with individual tables (subsets of size 1), then consider all ways to join pairs of tables (subsets of size 2), and progressively build larger subsets until we have the complete query.\n\nThe algorithm maintains a memoization table where each entry represents the optimal plan for joining a specific subset of tables. The key insight is that if we know the optimal way to join tables {A, B} and the optimal way to join tables {C, D}, we can evaluate the cost of combining these subproblems to form the larger subset {A, B, C, D}.\n\nHere's the step-by-step algorithm:\n\n1. **Initialize base cases**: Create single-table access plans for each table in the query, including the choice between sequential scans and index scans based on any applicable predicates.\n\n2. **Enumerate subsets by size**: For each subset size from 2 to n tables, consider all possible combinations of tables of that size.\n\n3. **Evaluate partition options**: For each subset S, consider all possible ways to partition S into two non-empty subsets S1 and S2 such that S1 ∪ S2 = S and S1 ∩ S2 = ∅.\n\n4. **Check join feasibility**: Verify that tables in S1 and S2 can be joined directly (there exists at least one join predicate connecting the two subsets) or that cross products are acceptable.\n\n5. **Calculate join cost**: For each valid partition, compute the cost of joining the optimal plan for S1 with the optimal plan for S2, considering different join algorithms.\n\n6. **Store optimal solution**: Keep only the lowest-cost plan for subset S, storing both the plan structure and its estimated cost.\n\n7. **Propagate to larger subsets**: Use the optimal solutions for smaller subsets as building blocks for evaluating larger subset combinations.\n\nThe memoization structure stores not just costs but complete plan fragments, enabling reconstruction of the full optimal plan tree once all subsets have been evaluated.\n\n**Join Predicate Analysis**\n\nA critical aspect of the dynamic programming algorithm is determining which table combinations can be joined meaningfully. The algorithm must analyze the query's join predicates to understand the connectivity graph between tables.\n\nFor each potential join between subsets S1 and S2, the algorithm searches for applicable join predicates. These might be explicit equality conditions (table1.id = table2.foreign_id), range conditions, or even complex expressions. Join predicates affect both feasibility and cost estimation.\n\nWhen no direct join predicate exists between two subsets, the algorithm faces a choice: either defer the combination to a later stage (when transitive join paths exist) or accept a cross product if necessary for query correctness. Cross products are generally avoided due to their explosive cardinality, but they may be unavoidable in certain query patterns.\n\n**Cost Accumulation Strategy**\n\nThe dynamic programming algorithm accumulates costs from multiple sources as it builds larger join combinations. Each join operation contributes I/O costs for reading input relations, CPU costs for comparison operations, and memory costs for maintaining join state.\n\nThe cost calculation must account for the fact that intermediate results from subquery joins become inputs to larger joins. This creates a dependency chain where the cardinality estimates and cost calculations compound through the join tree. Errors in early estimates can cascade through the optimization process.\n\nThe algorithm also considers the different physical join algorithms available (hash join, nested loop, merge join) and their varying cost characteristics. A hash join might have high startup costs for building hash tables but low per-tuple processing costs, while nested loops have low startup costs but high per-tuple costs for large relations.\n\n### Search Space Pruning Strategies\n\nWithout pruning techniques, dynamic programming for join ordering becomes computationally prohibitive as the number of tables grows. Several heuristic pruning strategies help manage the exponential search space while preserving solution quality.\n\n**Cross Product Elimination**\n\nOne of the most effective pruning techniques is eliminating plans that create unnecessary cross products. When evaluating potential joins between subsets S1 and S2, the algorithm first checks whether any join predicates connect tables in S1 with tables in S2.\n\nIf no connecting predicates exist, the algorithm can often prune this combination entirely, deferring the join until a path through other tables creates connectivity. This pruning is safe when alternative join orders exist that avoid cross products, but requires careful analysis to ensure the query remains solvable.\n\nThe cross product elimination logic maintains a connectivity graph representing which tables can be joined directly. It then uses graph algorithms to determine whether joining S1 and S2 immediately is necessary or if alternative paths exist through other table combinations.\n\n**Cost-Based Pruning**\n\nThe algorithm employs several cost-based pruning techniques to eliminate clearly suboptimal plans early in the search. These techniques compare partial costs and intermediate result sizes to detect when a plan branch cannot possibly lead to an optimal solution.\n\n*Intermediate Result Size Bounds*: Plans that generate extremely large intermediate results are often suboptimal because they increase the cost of all subsequent operations. The algorithm can establish upper bounds on reasonable intermediate result sizes and prune plans that exceed these thresholds.\n\n*Incremental Cost Comparison*: When multiple plans exist for the same subset of tables, the algorithm keeps only the best few candidates rather than just the single optimum. This allows for different plans that might be optimal depending on how they combine with other subsets, while still limiting the search space.\n\n*Resource Constraint Checking*: Plans that would exceed available memory for hash joins or create temp files larger than available disk space can be pruned early, focusing the search on feasible execution strategies.\n\n**Heuristic Ordering Preferences**\n\nThe algorithm incorporates several heuristic preferences that guide the search toward promising plan regions while avoiding exhaustive enumeration in less promising areas.\n\n*Selectivity-Based Ordering*: Joins involving highly selective predicates are preferred early in the join sequence because they reduce intermediate result sizes. The algorithm can prioritize exploring plans that apply selective joins before less selective ones.\n\n*Size-Based Preferences*: Smaller tables are often preferred as the inner relations in nested loop joins, and this preference can guide the search toward more promising plan regions without eliminating alternatives entirely.\n\n*Index Utilization*: When indexes are available to support specific join predicates, the algorithm can prefer exploring plans that utilize these indexes, as they often provide significant performance benefits.\n\n**Search Space Partitioning**\n\nFor queries with many tables, the algorithm can partition the search space into independent subproblems when the query structure permits. This partitioning reduces the overall complexity from exponential in the total number of tables to exponential in the size of the largest connected component.\n\nThe partitioning analysis examines the join predicate graph to identify disconnected components or weakly connected regions. Each component can be optimized independently, with the results combined at a higher level. This technique is particularly effective for queries that join multiple independent fact tables with shared dimension tables.\n\n### Left-Deep vs Bushy Join Trees\n\nThe choice between left-deep and bushy join tree topologies represents a fundamental trade-off between optimization complexity and execution efficiency. This decision affects both the search space size during optimization and the runtime characteristics of the resulting execution plans.\n\n**Left-Deep Tree Characteristics**\n\nLeft-deep trees restrict the join structure so that the right input to every join operation is a base table (or an index scan of a base table). This creates a linear chain structure where each join operation takes the result of all previous joins on the left side and adds one new table on the right side.\n\nThis restriction dramatically reduces the optimization search space. For n tables, left-deep trees have only n! possible orderings compared to the much larger space of all possible tree shapes. This reduction makes optimization tractable for larger numbers of tables without requiring aggressive pruning techniques.\n\nLeft-deep trees also have favorable memory usage patterns for certain join algorithms. Hash joins can build hash tables for each right-side table and probe them with the growing left-side result. This approach allows for predictable memory allocation and can take advantage of indexes on the right-side tables.\n\nHowever, left-deep trees can create execution inefficiencies. The left side of each join grows progressively larger as more tables are added, potentially creating large intermediate results that must be carried through the entire remaining join sequence. This growth pattern can lead to suboptimal execution when smaller intermediate results would be possible with different tree shapes.\n\n**Bushy Tree Advantages**\n\nBushy trees allow arbitrary tree structures where both inputs to a join operation can be intermediate results from other join operations. This flexibility can enable more efficient execution plans by allowing independent subqueries to be computed in parallel and then combined.\n\nThe key advantage of bushy trees is their ability to minimize intermediate result sizes by allowing optimal subproblem combinations. If tables A and B join very selectively, and tables C and D also join selectively, a bushy tree can compute (A⋈B) and (C⋈D) independently before joining these smaller results. A left-deep tree might be forced to create a larger intermediate result by joining A⋈B⋈C before adding D.\n\nBushy trees also enable better parallelization opportunities. Independent subtrees can be computed on separate processors or threads, with synchronization only required when combining the results. This parallelism can provide significant performance benefits on multi-core systems, even if the total amount of work is similar.\n\nThe optimization challenge with bushy trees is the dramatically larger search space. The number of possible bushy trees grows much more rapidly than left-deep alternatives, requiring more sophisticated pruning techniques and potentially longer optimization times.\n\n**Hybrid Optimization Approaches**\n\nMany practical query optimizers use hybrid approaches that consider both left-deep and bushy alternatives while managing the computational complexity. These approaches might start with left-deep optimization and then selectively explore bushy alternatives for promising subproblems.\n\n*Complexity-Based Selection*: For queries with few tables (typically 6 or fewer), the optimizer can afford to explore bushy tree alternatives. For larger queries, it defaults to left-deep trees to maintain reasonable optimization times.\n\n*Selectivity-Based Bushy Exploration*: The optimizer identifies highly selective join pairs and considers bushy trees that compute these subproblems independently. This approach focuses the additional optimization effort on cases where bushy trees are most likely to provide benefits.\n\n*Cost Threshold Analysis*: When left-deep optimization produces plans with very high costs, the optimizer can invest additional time exploring bushy alternatives. This adaptive approach balances optimization time against the potential for finding significantly better plans.\n\n### Architecture Decision: Full Enumeration vs Heuristics\n\nThe choice between complete dynamic programming enumeration and heuristic-based join ordering represents a critical trade-off between optimization quality and optimization time. This decision fundamentally shapes the optimizer's behavior and performance characteristics.\n\n> **Decision: Adaptive Enumeration with Complexity Threshold**\n> - **Context**: Join ordering optimization faces exponential search space growth, making complete enumeration impractical for large queries while heuristics may miss optimal plans for smaller queries\n> - **Options Considered**: Always use full enumeration, always use heuristics, adaptive approach based on query complexity\n> - **Decision**: Use full dynamic programming enumeration for queries with 8 or fewer tables, switch to heuristic ordering for larger queries\n> - **Rationale**: Full enumeration provides guaranteed optimal solutions for small-to-medium queries where optimization time remains reasonable, while heuristics prevent optimization timeout on large queries\n> - **Consequences**: Enables optimal plans for common query sizes while maintaining reasonable optimization latency for complex queries\n\n| Approach | Pros | Cons | Query Size Limit |\n|----------|------|------|------------------|\n| Full Enumeration | Guaranteed optimal solution, systematic exploration, reproducible results | Exponential time complexity, memory usage growth, optimization timeouts | ~8 tables |\n| Heuristic Ordering | Linear or polynomial complexity, predictable optimization time, scales to large queries | Potentially suboptimal plans, hard to validate quality, heuristic maintenance | Unlimited |\n| Adaptive Threshold | Best of both approaches, complexity-aware adaptation, tunable parameters | Implementation complexity, threshold determination, mixed behavior | Configurable |\n\n**Full Enumeration Benefits and Limitations**\n\nComplete dynamic programming enumeration provides theoretical optimality guarantees by systematically exploring all valid join orders and selecting the minimum-cost alternative. This approach ensures that the optimizer finds the globally optimal solution within the constraints of its cost model and search space restrictions.\n\nThe systematic nature of full enumeration makes it easier to reason about optimizer behavior and debug optimization decisions. Each step of the algorithm follows predictable rules, and the final plan selection can be traced back through the dynamic programming table to understand why specific choices were made.\n\nHowever, full enumeration becomes computationally prohibitive as query complexity grows. The combination of exponential search space growth and the need for accurate cost estimation at each step creates optimization times that can exceed query execution times for large queries. This limitation makes full enumeration impractical for data warehouse workloads or queries involving many tables.\n\nMemory usage also becomes problematic with full enumeration. The dynamic programming table stores optimal plans for all possible table subsets, and the memory requirement grows exponentially with the number of tables. For queries with 12+ tables, the optimization process might exceed available memory before completing.\n\n**Heuristic Approaches**\n\nHeuristic join ordering uses rules and approximations to quickly identify good (though not necessarily optimal) join orders without exhaustive search. These approaches typically run in polynomial time and can handle queries with dozens or hundreds of tables.\n\n*Greedy Selection Heuristics*: These approaches iteratively select the next table to join based on local cost metrics, such as choosing the join that produces the smallest intermediate result or has the highest selectivity. While fast, greedy approaches can make early decisions that preclude globally optimal solutions.\n\n*Graph-Based Heuristics*: These techniques analyze the join predicate graph structure to identify promising join orders. They might prioritize joining tables that form cliques in the predicate graph or use graph traversal algorithms to determine join sequences.\n\n*Statistical Heuristics*: These approaches use table and column statistics to estimate join costs without exhaustive plan enumeration. They might sort tables by size and apply rules about joining small tables before large ones, or prioritize joins with high selectivity predicates.\n\nThe main challenge with heuristic approaches is validating their quality. Unlike full enumeration, there's no guarantee that heuristic-selected plans are optimal or even close to optimal. Poorly chosen heuristics can produce plans that are orders of magnitude slower than optimal plans.\n\n**Adaptive Implementation Strategy**\n\nThe adaptive approach monitors query complexity and optimization progress to decide between enumeration and heuristic strategies dynamically. This implementation provides flexibility while maintaining performance guarantees for different query patterns.\n\nThe complexity assessment considers multiple factors beyond just table count: the number of join predicates, the presence of complex expressions, the availability of indexes, and the size of table statistics. Queries with many tables but simple star-schema join patterns might remain tractable for full enumeration, while queries with fewer tables but complex predicate structures might benefit from heuristic approaches.\n\nThe adaptive implementation also includes timeout mechanisms that can switch from enumeration to heuristics if optimization takes too long. This approach ensures that optimization time remains bounded while still attempting to find optimal solutions when feasible.\n\n**Implementation Complexity Considerations**\n\nThe adaptive approach requires implementing both full enumeration and heuristic algorithms, along with the decision logic for choosing between them. This implementation complexity must be weighed against the benefits of having both approaches available.\n\nThe decision logic itself needs tuning and validation. The thresholds for switching between approaches affect both optimization time and plan quality, and finding good default values requires extensive testing with representative query workloads.\n\nMaintenance overhead increases with the adaptive approach because both optimization strategies require ongoing development and bug fixes. Changes to cost models or statistics collection affect both code paths, and testing must verify correct behavior across the full range of query complexities.\n\n![Dynamic Programming Join Ordering](./diagrams/join-ordering-algorithm.svg)\n\n### Common Join Ordering Pitfalls\n\nUnderstanding common mistakes in join ordering implementation helps avoid subtle bugs that can lead to suboptimal plans or incorrect results. These pitfalls often arise from the complexity of managing state across recursive optimization calls and handling edge cases in the dynamic programming algorithm.\n\n⚠️ **Pitfall: Incorrect Subset Enumeration**\n\nA frequent implementation mistake is generating invalid or duplicate table subsets during the dynamic programming enumeration. This can happen when the subset generation logic doesn't properly handle bit manipulation for representing table sets, or when the enumeration algorithm visits the same subset multiple times with different internal representations.\n\nThe error typically manifests as missing optimal plans for certain table combinations or performance degradation due to redundant cost calculations. In some cases, the optimizer might compute different costs for the same logical subset, leading to inconsistent plan selection.\n\nTo avoid this pitfall, use a canonical representation for table subsets (such as sorted bit vectors) and implement explicit duplicate detection. Validate subset enumeration logic with small test cases where all combinations can be verified manually before testing with larger queries.\n\n⚠️ **Pitfall: Cross Product Explosion**\n\nFailing to properly handle cross products can lead to catastrophic optimization performance and incorrect cost estimates. This happens when the algorithm doesn't recognize disconnected table groups or when it attempts to optimize queries that require cross products without appropriate safeguards.\n\nCross products create intermediate results whose cardinality is the product of input cardinalities, leading to extremely large cost estimates that can overflow numeric types or consume excessive memory during optimization. The algorithm might spend most of its time exploring clearly suboptimal cross product plans instead of focusing on connected join alternatives.\n\nImplement explicit cross product detection by maintaining a connectivity graph of tables based on join predicates. Before evaluating any join combination, check whether the tables can be connected through existing predicates. For queries that legitimately require cross products, implement separate handling with appropriate cost model adjustments and resource limit checks.\n\n⚠️ **Pitfall: Memoization Cache Corruption**\n\nThe dynamic programming memoization table can become corrupted when multiple optimization threads access it concurrently or when the cache key generation doesn't account for all relevant plan properties. This leads to incorrect plan reuse where the cached plan doesn't match the actual requirements for the current optimization context.\n\nCache corruption typically appears as inconsistent optimization results where running the same query multiple times produces different plans or costs. It might also manifest as assertion failures when cached plans reference tables that aren't part of the current optimization subset.\n\nDesign the memoization key to include all properties that affect plan equivalence, including table subsets, join predicates, sort orders, and physical property requirements. If using concurrent optimization, implement proper synchronization around cache access or use thread-local caches with explicit merging strategies.\n\n⚠️ **Pitfall: Cost Model Inconsistencies**\n\nInconsistent cost calculations between different parts of the optimization algorithm can lead to incorrect plan comparisons and suboptimal selections. This often occurs when the cost estimation logic makes different assumptions about data distribution or physical properties during subset optimization versus final plan construction.\n\nCost inconsistencies might cause the optimizer to select plans that appear optimal during dynamic programming but perform poorly during execution. The problem is particularly subtle because cost estimation errors often compound through the join tree, making it difficult to trace the root cause of poor plan selection.\n\nImplement cost estimation as a centralized service with well-defined interfaces and assumptions. Document all cost model parameters and ensure that the same statistical information and calculation methods are used consistently throughout the optimization process. Include validation checks that verify cost calculations remain consistent when plans are reconstructed from the memoization table.\n\n⚠️ **Pitfall: Predicate Placement Errors**\n\nIncorrectly handling join predicates during plan construction can result in plans that apply predicates at the wrong join levels or fail to apply them at all. This happens when the optimization algorithm doesn't properly track which predicates are satisfied by each join operation or when it assumes predicates can be applied at arbitrary points in the join tree.\n\nPredicate placement errors can lead to incorrect query results if selective predicates aren't applied, or to inefficient execution if predicates are applied later than necessary. The errors are often difficult to detect because the final query results might still be correct even when predicates are applied suboptimally.\n\nImplement explicit predicate tracking that associates each join predicate with the specific table combinations that satisfy it. Validate during plan construction that all join predicates are applied exactly once at the appropriate join level. Use predicate pushdown analysis to move selection predicates as early as possible in the join tree while maintaining correctness.\n\n⚠️ **Pitfall: Memory Management in Large Search Spaces**\n\nPoor memory management during dynamic programming can lead to excessive memory usage or memory leaks that crash the optimization process. This is particularly problematic for queries with many tables where the memoization table can grow very large and intermediate plan structures accumulate throughout the optimization process.\n\nMemory problems might appear as gradual performance degradation as optimization progresses, or as sudden crashes when available memory is exhausted. In some cases, the optimizer might succeed but consume so much memory that subsequent query execution fails due to resource constraints.\n\nImplement explicit memory bounds for the optimization process and monitor memory usage during dynamic programming. Use techniques like plan structure sharing to reduce memory overhead, and implement garbage collection for intermediate results that are no longer needed. Consider disk-based memoization for very large queries that exceed reasonable memory limits.\n\n### Implementation Guidance\n\nThe join optimization component represents the most algorithmically complex part of the query optimizer, requiring careful implementation of dynamic programming algorithms and sophisticated data structures for managing the exponential search space.\n\n**Technology Recommendations**\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Subset Representation | Python sets with frozenset | Bit vectors with numpy arrays |\n| Memoization Storage | Dictionary with tuple keys | Custom hash table with optimized keys |\n| Cost Calculation | Direct arithmetic operations | Vectorized operations with numpy |\n| Graph Analysis | NetworkX for connectivity | Custom adjacency lists for performance |\n\n**File Structure**\n\n```\nquery_optimizer/\n  join_optimizer/\n    __init__.py\n    dp_optimizer.py           ← dynamic programming implementation\n    cost_calculator.py        ← join cost estimation\n    plan_enumerator.py       ← subset enumeration and plan generation\n    pruning_strategies.py    ← search space reduction techniques\n    join_graph.py           ← predicate connectivity analysis\n    heuristic_optimizer.py  ← fallback for large queries\n    test_join_optimizer.py  ← comprehensive test suite\n```\n\n**Infrastructure Starter Code**\n\n```python\n# join_graph.py - Complete predicate connectivity analysis\nfrom typing import Set, List, Dict, Tuple\nfrom dataclasses import dataclass\nfrom collections import defaultdict\n\n@dataclass\nclass JoinPredicate:\n    \"\"\"Represents a join condition between tables.\"\"\"\n    left_table: str\n    left_column: str\n    right_table: str\n    right_column: str\n    operator: str  # '=', '<', '>', etc.\n    selectivity: float = 0.1\n\nclass JoinGraph:\n    \"\"\"Analyzes table connectivity through join predicates.\"\"\"\n    \n    def __init__(self, tables: List[str], predicates: List[JoinPredicate]):\n        self.tables = set(tables)\n        self.predicates = predicates\n        self.adjacency = self._build_adjacency_graph()\n        self.components = self._find_connected_components()\n    \n    def _build_adjacency_graph(self) -> Dict[str, Set[str]]:\n        \"\"\"Build adjacency list representation of table connectivity.\"\"\"\n        graph = defaultdict(set)\n        for predicate in self.predicates:\n            graph[predicate.left_table].add(predicate.right_table)\n            graph[predicate.right_table].add(predicate.left_table)\n        return dict(graph)\n    \n    def _find_connected_components(self) -> List[Set[str]]:\n        \"\"\"Find disconnected components using DFS.\"\"\"\n        visited = set()\n        components = []\n        \n        for table in self.tables:\n            if table not in visited:\n                component = set()\n                self._dfs(table, visited, component)\n                components.append(component)\n        \n        return components\n    \n    def _dfs(self, table: str, visited: Set[str], component: Set[str]):\n        \"\"\"Depth-first search for component discovery.\"\"\"\n        visited.add(table)\n        component.add(table)\n        \n        for neighbor in self.adjacency.get(table, set()):\n            if neighbor not in visited:\n                self._dfs(neighbor, visited, component)\n    \n    def can_join_directly(self, subset1: Set[str], subset2: Set[str]) -> bool:\n        \"\"\"Check if two table subsets have connecting predicates.\"\"\"\n        for predicate in self.predicates:\n            if ((predicate.left_table in subset1 and predicate.right_table in subset2) or\n                (predicate.left_table in subset2 and predicate.right_table in subset1)):\n                return True\n        return False\n    \n    def get_connecting_predicates(self, subset1: Set[str], subset2: Set[str]) -> List[JoinPredicate]:\n        \"\"\"Return predicates that connect two table subsets.\"\"\"\n        connecting = []\n        for predicate in self.predicates:\n            if ((predicate.left_table in subset1 and predicate.right_table in subset2) or\n                (predicate.left_table in subset2 and predicate.right_table in subset1)):\n                connecting.append(predicate)\n        return connecting\n\n# cost_calculator.py - Complete join cost estimation utilities\nfrom typing import Dict\nfrom .data_model import CostEstimate, TableStatistics\n\nclass JoinCostCalculator:\n    \"\"\"Estimates costs for different join algorithms and configurations.\"\"\"\n    \n    def __init__(self, table_stats: Dict[str, TableStatistics]):\n        self.table_stats = table_stats\n        self.io_page_cost = IO_PAGE_COST\n        self.cpu_tuple_cost = CPU_TUPLE_COST\n        self.memory_page_cost = MEMORY_PAGE_COST\n    \n    def estimate_hash_join_cost(self, left_plan, right_plan, join_predicates) -> CostEstimate:\n        \"\"\"Calculate hash join cost with build and probe phases.\"\"\"\n        # Build phase: scan right relation and build hash table\n        build_io = right_plan.cost_estimate.io_cost\n        build_cpu = right_plan.estimated_rows * self.cpu_tuple_cost * 2  # hash + store\n        build_memory = right_plan.estimated_rows * 0.1  # hash table overhead\n        \n        # Probe phase: scan left relation and probe hash table\n        probe_io = left_plan.cost_estimate.io_cost\n        probe_cpu = left_plan.estimated_rows * self.cpu_tuple_cost * 1.5  # hash + lookup\n        \n        # Output phase: write matching tuples\n        output_rows = self._estimate_join_cardinality(left_plan, right_plan, join_predicates)\n        output_cost = output_rows * self.cpu_tuple_cost * 0.5\n        \n        return CostEstimate(\n            io_cost=build_io + probe_io,\n            cpu_cost=build_cpu + probe_cpu + output_cost,\n            memory_cost=build_memory,\n            estimated_rows=output_rows,\n            startup_cost=build_io + build_cpu,\n            cost_factors={'algorithm': 'hash_join', 'build_side': 'right'},\n            confidence_level=0.8\n        )\n    \n    def estimate_nested_loop_cost(self, outer_plan, inner_plan, join_predicates) -> CostEstimate:\n        \"\"\"Calculate nested loop join cost.\"\"\"\n        # Outer relation scan cost\n        outer_cost = outer_plan.cost_estimate.total_cost\n        \n        # Inner relation scanned once per outer tuple\n        inner_scans = outer_plan.estimated_rows\n        inner_cost_per_scan = inner_plan.cost_estimate.total_cost\n        total_inner_cost = inner_scans * inner_cost_per_scan\n        \n        # Join processing cost\n        comparisons = outer_plan.estimated_rows * inner_plan.estimated_rows\n        join_cpu_cost = comparisons * self.cpu_tuple_cost * 0.1\n        \n        output_rows = self._estimate_join_cardinality(outer_plan, inner_plan, join_predicates)\n        \n        return CostEstimate(\n            io_cost=outer_plan.cost_estimate.io_cost + total_inner_cost,\n            cpu_cost=outer_plan.cost_estimate.cpu_cost + join_cpu_cost,\n            memory_cost=0.01,  # minimal memory for nested loops\n            estimated_rows=output_rows,\n            startup_cost=outer_plan.cost_estimate.startup_cost,\n            cost_factors={'algorithm': 'nested_loop', 'outer_relation': outer_plan.node_id},\n            confidence_level=0.9\n        )\n    \n    def _estimate_join_cardinality(self, left_plan, right_plan, join_predicates) -> int:\n        \"\"\"Estimate number of rows produced by join.\"\"\"\n        if not join_predicates:\n            # Cross product\n            return left_plan.estimated_rows * right_plan.estimated_rows\n        \n        # Use most selective predicate for estimation\n        min_selectivity = min(pred.selectivity for pred in join_predicates)\n        return int(left_plan.estimated_rows * right_plan.estimated_rows * min_selectivity)\n```\n\n**Core Logic Skeleton**\n\n```python\n# dp_optimizer.py - Dynamic programming join optimization\nfrom typing import Dict, Set, List, Optional, Tuple\nfrom itertools import combinations\nfrom .data_model import ExecutionPlan, OperatorNode, ParsedQuery\n\nclass DynamicProgrammingOptimizer:\n    \"\"\"Implements dynamic programming algorithm for optimal join ordering.\"\"\"\n    \n    def __init__(self, cost_calculator, max_tables=MAX_JOIN_ENUMERATION):\n        self.cost_calculator = cost_calculator\n        self.max_tables = max_tables\n        self.memo_table: Dict[frozenset, ExecutionPlan] = {}\n        self.join_graph = None\n    \n    def optimizeJoinOrder(self, tables: List[str], join_predicates: List) -> ExecutionPlan:\n        \"\"\"\n        Find optimal join order using dynamic programming.\n        Returns the minimum cost plan for joining all specified tables.\n        \"\"\"\n        # TODO 1: Check if query size exceeds enumeration threshold\n        # TODO 2: Initialize join graph for connectivity analysis\n        # TODO 3: Create base case plans for single tables\n        # TODO 4: Enumerate subsets by size from 2 to len(tables)\n        # TODO 5: For each subset, find optimal partitioning\n        # TODO 6: Return optimal plan for complete table set\n        # Hint: Use frozenset for subset representation in memo table\n        pass\n    \n    def _enumerate_subsets_by_size(self, tables: Set[str], size: int) -> List[Set[str]]:\n        \"\"\"\n        Generate all possible subsets of specified size.\n        Returns list of table subsets for dynamic programming enumeration.\n        \"\"\"\n        # TODO 1: Use itertools.combinations to generate subsets\n        # TODO 2: Convert combinations to sets for easier manipulation\n        # TODO 3: Filter out subsets that form disconnected components\n        # TODO 4: Return valid subsets for optimization\n        # Hint: combinations(tables, size) generates all size-k subsets\n        pass\n    \n    def _find_optimal_partition(self, subset: Set[str]) -> Tuple[ExecutionPlan, float]:\n        \"\"\"\n        Find the lowest-cost way to partition subset into two joined parts.\n        Returns optimal plan and its cost for the given table subset.\n        \"\"\"\n        # TODO 1: Generate all possible ways to split subset into two non-empty parts\n        # TODO 2: For each partition, check if tables can be joined (connectivity)\n        # TODO 3: Look up optimal plans for both parts in memo table\n        # TODO 4: Calculate cost of joining the two parts with different algorithms\n        # TODO 5: Track minimum cost partition and corresponding plan\n        # TODO 6: Return best plan and cost\n        # Hint: Use subset.difference() to generate complementary partitions\n        pass\n    \n    def _evaluate_join_cost(self, left_plan: ExecutionPlan, right_plan: ExecutionPlan, \n                           join_predicates: List) -> List[Tuple[ExecutionPlan, float]]:\n        \"\"\"\n        Evaluate cost of joining two plans with different physical join algorithms.\n        Returns list of (plan, cost) tuples for each viable join method.\n        \"\"\"\n        # TODO 1: Check input sizes to determine viable join algorithms\n        # TODO 2: Estimate hash join cost if right relation fits in memory\n        # TODO 3: Estimate nested loop cost (always viable)\n        # TODO 4: Estimate merge join cost if inputs are sorted\n        # TODO 5: Create ExecutionPlan for each algorithm with proper operator nodes\n        # TODO 6: Return all viable options sorted by cost\n        # Hint: Hash join preferred when inner relation < available_memory\n        pass\n    \n    def _create_base_case_plans(self, tables: List[str]) -> None:\n        \"\"\"\n        Initialize memo table with single-table access plans.\n        Creates optimal plans for accessing each table individually.\n        \"\"\"\n        # TODO 1: For each table, determine available access methods\n        # TODO 2: Compare sequential scan vs index scan costs\n        # TODO 3: Apply any single-table filter predicates\n        # TODO 4: Create ExecutionPlan with chosen access method\n        # TODO 5: Store plan in memo table with frozenset([table]) as key\n        # Hint: Use table statistics to choose between scan methods\n        pass\n    \n    def _check_connectivity(self, subset1: Set[str], subset2: Set[str]) -> bool:\n        \"\"\"\n        Check if two table subsets can be joined directly.\n        Returns True if join predicates connect the subsets.\n        \"\"\"\n        # TODO 1: Use join_graph to check for connecting predicates\n        # TODO 2: Return False if no direct connection exists (would create cross product)\n        # TODO 3: Handle special case where cross products are explicitly allowed\n        # Hint: self.join_graph.can_join_directly(subset1, subset2)\n        pass\n    \n    def _build_join_plan(self, left_plan: ExecutionPlan, right_plan: ExecutionPlan,\n                        join_type: str, join_predicates: List) -> ExecutionPlan:\n        \"\"\"\n        Construct ExecutionPlan for joining two subplans.\n        Returns complete plan tree with proper cost annotations.\n        \"\"\"\n        # TODO 1: Create OperatorNode for the join operation\n        # TODO 2: Set join_type and join_predicates in operator properties\n        # TODO 3: Add left_plan and right_plan as children\n        # TODO 4: Calculate and set cost estimate for the join node\n        # TODO 5: Determine output schema by combining input schemas\n        # TODO 6: Create ExecutionPlan wrapping the join operator tree\n        # Hint: Join output schema = left_schema + right_schema - duplicate keys\n        pass\n```\n\n**Language-Specific Hints**\n\n- Use `frozenset()` for immutable table subset representations that can serve as dictionary keys in the memoization table\n- Implement subset enumeration with `itertools.combinations()` for clean, readable code that handles all size-k combinations\n- Use `dataclasses` with `@dataclass` decorator for clean cost estimation and plan representation structures\n- Consider `lru_cache` decorator for memoizing frequently computed cost estimates, but be careful with memory usage\n- Use `typing.Union` and `Optional` for clear function signatures that handle nullable plan references\n- Implement custom `__hash__` and `__eq__` methods for plan structures if using them as dictionary keys\n\n**Milestone Checkpoints**\n\nAfter implementing the dynamic programming join optimizer:\n\n1. **Subset Enumeration Test**: Run `python -m pytest test_join_optimizer.py::test_subset_enumeration` to verify that all valid table combinations are generated correctly for small queries.\n\n2. **Cost Calculation Validation**: Execute test queries with known optimal join orders and verify that the optimizer selects the expected plans. Use `EXPLAIN` output to compare with expected join sequences.\n\n3. **Cross Product Handling**: Test queries without join predicates to ensure cross products are handled appropriately - either rejected with errors or optimized with proper cost penalties.\n\n4. **Memory Usage Monitoring**: Profile optimization of queries with 6-8 tables to ensure memory usage remains reasonable (under 100MB for typical table counts).\n\n5. **Performance Benchmarks**: Measure optimization times for queries of different complexity levels:\n   - 3-4 tables: < 10ms optimization time\n   - 5-6 tables: < 100ms optimization time  \n   - 7-8 tables: < 1 second optimization time\n\n**Debugging Tips**\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| Optimization never completes | Infinite recursion in subset enumeration | Add logging to track subset generation progress | Add base case checks and maximum depth limits |\n| Wrong join order selected | Cost estimation errors or missing predicates | Compare actual vs estimated costs for different join pairs | Verify selectivity calculations and predicate connectivity |\n| Memory usage grows without bound | Memo table not being cleaned up | Monitor memo table size during optimization | Implement LRU eviction or periodic cleanup |\n| Cross product plans generated | Missing join predicate detection | Check join graph connectivity analysis | Verify predicate parsing and graph construction |\n| Inconsistent optimization results | Race conditions in concurrent optimization | Run single-threaded to isolate threading issues | Add proper synchronization or use thread-local storage |\n\n\n## Physical Planning Component\n\n> **Milestone(s):** Milestone 4 - selects concrete physical operators and access methods, applies optimization rules like predicate pushdown, and generates final executable plans.\n\n### Mental Model: Tool Selection\n\nThink of physical planning as the process a master craftsman goes through when translating architectural blueprints into a concrete construction plan. The architect's drawings show what needs to be built - a foundation, walls, roof, electrical systems - but they don't specify which specific tools and techniques to use. The craftsman must decide: Should I use a pneumatic nail gun or a traditional hammer? Is this job better suited for a circular saw or a miter saw? Should I frame this wall with metal studs or wooden ones?\n\nIn query optimization, we face the same decisions. The logical plan tells us what operations need to happen - scan tables, filter rows, join datasets, project columns - but it doesn't specify how to perform these operations. Physical planning is where we make the concrete tool choices: Should we use a sequential scan or leverage an index? Is this join better served by a hash join algorithm or nested loops? Should we apply this filter before or after the join?\n\nJust as a master craftsman considers factors like material properties, tool availability, workspace constraints, and time requirements, our physical planner considers data characteristics, available indexes, memory constraints, and performance requirements. The goal is the same: transform an abstract plan into an executable sequence of concrete actions that efficiently accomplishes the desired outcome.\n\nThe physical planner serves as the final decision maker in our optimization pipeline, taking the logically correct but implementation-agnostic plan from the join optimizer and transforming it into a detailed execution strategy with specific operators, access methods, and optimization transformations applied.\n\n### Index vs Sequential Scan Selection\n\nThe choice between index access and sequential scanning represents one of the most fundamental decisions in physical planning. This decision dramatically impacts query performance, often determining whether a query completes in milliseconds or minutes. The decision logic must balance multiple factors including data selectivity, index availability, data clustering, and system resources.\n\n**Selectivity-Based Decision Framework**\n\nThe primary factor driving scan selection is **predicate selectivity** - the fraction of rows that survive the filter conditions. When selectivity is low (few rows match), indexes provide significant benefit by avoiding the need to examine irrelevant data. When selectivity is high (most rows match), sequential scanning often proves more efficient due to reduced overhead and better cache utilization.\n\nOur decision framework uses the `SELECTIVITY_THRESHOLD` constant as the primary decision boundary. For predicates with estimated selectivity below this threshold, we prefer index access when suitable indexes exist. Above the threshold, sequential scanning typically provides better performance unless other factors override this preference.\n\nThe selectivity calculation process integrates with our cost estimation component through the `estimate_filter_selectivity` method, which combines column statistics with predicate analysis to predict the fraction of rows surviving each filter condition. This estimation accounts for data distribution patterns captured in our `ColumnStatistics` histograms and handles common predicate types including equality, range, and pattern matching conditions.\n\n**Access Method Cost Comparison**\n\nThe `selectPhysicalOperators` method implements a comprehensive cost comparison between available access methods. For each table access in the logical plan, we evaluate the following options:\n\n| Access Method | Cost Components | Best When | Avoided When |\n|--------------|-----------------|-----------|--------------|\n| Sequential Scan | `IO_PAGE_COST * page_count + CPU_TUPLE_COST * row_count` | High selectivity, no suitable indexes | Time-critical queries with low selectivity |\n| Index Scan | `IO_PAGE_COST * index_pages + RANDOM_MULTIPLIER * data_pages + CPU_TUPLE_COST * matching_rows` | Low selectivity, covering index available | High selectivity, outdated statistics |\n| Index-Only Scan | `IO_PAGE_COST * index_pages + CPU_TUPLE_COST * matching_rows` | Low selectivity, all columns in index | Index doesn't cover required columns |\n| Bitmap Index Scan | `IO_PAGE_COST * (index_pages + unique_data_pages) + CPU_TUPLE_COST * matching_rows` | Multiple indexes, moderate selectivity | Very high or very low selectivity |\n\nThe cost calculation incorporates the `RANDOM_MULTIPLIER` penalty for index scans because they typically generate random I/O patterns when accessing the underlying table data. Sequential scans benefit from the `SEQUENTIAL_MULTIPLIER` discount because they read data in storage order, maximizing cache efficiency and minimizing seek time.\n\n**Index Selection Algorithm**\n\nWhen multiple indexes exist for a table, our selection algorithm evaluates each candidate index based on several criteria:\n\n1. **Predicate Coverage**: Indexes covering more filter predicates receive higher priority scores\n2. **Column Prefix Matching**: B-tree indexes are most effective when predicates match the leftmost columns in the index key\n3. **Index Selectivity**: Indexes with high cardinality (many distinct values) provide better filtering for equality predicates\n4. **Clustering Factor**: Indexes with low clustering factors (data stored in index order) minimize random I/O overhead\n\nThe algorithm maintains a candidate scoring system where each index receives points based on these factors. The highest-scoring index becomes the primary access method, with additional indexes potentially used for bitmap combining when multiple predicates exist.\n\n**Storage Layout Considerations**\n\nPhysical planning must account for how data is physically organized on storage devices. Tables with good **clustering** on frequently queried columns benefit more from sequential scans because related data remains physically co-located. Conversely, tables with random data distribution see greater benefits from index access because the clustering advantage of sequential scanning is already lost.\n\nOur `TableStatistics` structure captures the `clustering_factor` metric, which measures how well the table's physical storage order matches the logical ordering of frequently accessed data. High clustering factors indicate random data distribution, making index access relatively more attractive even at moderate selectivity levels.\n\n> **Key Insight**: The break-even point between index and sequential access is not fixed - it shifts based on data characteristics, available memory, and concurrent workload. Adaptive systems monitor actual execution performance to refine these thresholds over time.\n\n### Join Algorithm Selection\n\nJoin algorithm selection represents one of the most performance-critical decisions in physical planning. Different join algorithms excel under different conditions, and the wrong choice can result in orders-of-magnitude performance differences. Our selection framework evaluates input characteristics, available memory, and data distribution patterns to choose the most appropriate join implementation.\n\n**Join Algorithm Comparison Matrix**\n\n| Algorithm | Memory Requirement | Best Input Sizes | I/O Pattern | CPU Overhead | Preferred When |\n|-----------|-------------------|------------------|-------------|--------------|----------------|\n| Nested Loop | Minimal | Small × Any | Sequential + Random | Low | Small outer, good indexes on inner |\n| Hash Join | Build side fits in memory | Large × Large | Sequential both sides | Moderate | Similar sizes, sufficient memory |\n| Merge Join | Minimal | Any × Any | Sequential both sides | Low | Both inputs pre-sorted |\n| Index Nested Loop | Minimal | Any × Any | Random on inner | Low | Selective join predicates, fast indexes |\n\nThe selection algorithm implemented in the `_evaluate_join_cost` method compares the estimated costs of applicable join algorithms and selects the option with the lowest total cost. This comparison accounts for I/O costs, CPU processing costs, and memory allocation costs using our unified `CostEstimate` framework.\n\n**Hash Join Selection Logic**\n\nHash joins excel when one input (the **build side**) fits comfortably in available memory while the other input (the **probe side**) can be any size. Our selection logic estimates the memory footprint of the smaller input relation and compares it against available buffer pool space.\n\nThe cost calculation for hash joins includes several components:\n- **Build Phase**: Sequential scan of build relation plus hash table construction\n- **Probe Phase**: Sequential scan of probe relation plus hash lookups\n- **Memory Overhead**: Hash table space allocation and maintenance\n\nWhen the estimated build side exceeds available memory, the algorithm must consider **hash partitioning** strategies that divide the inputs into smaller chunks that fit in memory. The cost model accounts for the additional I/O overhead of writing and re-reading partitioned data.\n\n**Nested Loop Optimization**\n\nWhile nested loop joins have poor theoretical complexity (O(n×m)), they prove highly effective in several practical scenarios. Our selection logic identifies these favorable conditions:\n\n1. **Small Outer Relation**: When the outer loop iterates over few rows, the inner relation access cost dominates\n2. **Index-Backed Inner Access**: Fast index lookups on the inner relation can make nested loops competitive with hash joins\n3. **Early Termination**: LIMIT clauses or EXISTS predicates allow nested loops to terminate early\n\nThe `Index Nested Loop` variant leverages indexes on the inner relation to avoid full table scans for each outer row. This approach works particularly well when join predicates have high selectivity and suitable indexes exist on the inner table's join columns.\n\n**Merge Join Considerations**\n\nMerge joins require both inputs to be sorted on the join columns, but they provide excellent performance characteristics when this prerequisite is satisfied. Our selection logic considers merge joins in these scenarios:\n\n- Both inputs are already sorted (from previous operations or indexes)\n- The sort cost plus merge cost is lower than hash join alternatives\n- Memory constraints make hash joins impractical\n\nThe cost estimation for merge joins must account for potential sorting overhead when inputs are not pre-sorted. This calculation uses the cost model for sort operations, which depends on input size and available memory for sort buffers.\n\n**Multi-Table Join Algorithm Selection**\n\nFor queries involving multiple tables, join algorithm selection becomes more complex because different join pairs within the same query may benefit from different algorithms. Our optimization framework evaluates algorithm choices independently for each join operation in the optimized join order.\n\nThis independent evaluation allows for **heterogeneous join plans** where early joins might use hash algorithms (when memory is available) while later joins switch to nested loops (when intermediate results are small) or merge joins (when data is already sorted from previous operations).\n\n### Optimization Rules and Predicate Pushdown\n\nOptimization rules transform query plans to improve execution efficiency without changing semantic correctness. These rule-based transformations complement our cost-based optimization by applying proven techniques that consistently improve performance across diverse workloads. The most impactful rule is **predicate pushdown**, but our framework includes several categories of optimization rules.\n\n**Predicate Pushdown Implementation**\n\nPredicate pushdown moves filter operations as close to data sources as possible, reducing the volume of data flowing through the query execution pipeline. This optimization can dramatically improve performance by eliminating irrelevant rows early in the execution process.\n\nOur predicate pushdown implementation analyzes the dependency relationships between filter predicates and data sources. The algorithm works through the following steps:\n\n1. **Predicate Classification**: Categorize each filter predicate based on the tables and columns it references\n2. **Dependency Analysis**: Determine which predicates can be evaluated at each point in the execution tree\n3. **Safety Verification**: Ensure that moving predicates doesn't change query semantics (particularly important for outer joins)\n4. **Cost Benefit Analysis**: Verify that pushdown actually improves performance (some predicates are expensive to evaluate)\n\nThe pushdown algorithm distinguishes between different types of predicates:\n\n| Predicate Type | Pushdown Strategy | Example | Semantic Considerations |\n|---------------|-------------------|---------|------------------------|\n| Single-table filters | Push to table scan | `WHERE customer.age > 25` | Always safe |\n| Join predicates | Keep at join level | `WHERE orders.customer_id = customer.id` | Don't push past the join |\n| Cross-table filters | Push past inner joins only | `WHERE orders.amount > customer.credit_limit` | Unsafe for outer joins |\n| Computed predicates | Evaluate pushdown cost | `WHERE UPPER(name) LIKE 'JOHN%'` | May be expensive to evaluate early |\n\n**Join Predicate Optimization**\n\nBeyond simple filter pushdown, our rule engine optimizes join predicates themselves. The key transformations include:\n\n- **Predicate Reordering**: Evaluate selective predicates before expensive ones\n- **Predicate Combination**: Merge multiple range predicates into single range scans\n- **Transitive Closure**: Derive additional predicates from existing constraints (if `A = B` and `B = C`, then `A = C`)\n\nThe transitive closure optimization proves particularly valuable in star schema queries where fact table joins to dimension tables can imply additional filter conditions that weren't explicitly stated in the original query.\n\n**Projection Pushdown**\n\nSimilar to predicate pushdown, **projection pushdown** eliminates unnecessary columns as early as possible in the execution pipeline. This optimization reduces memory usage, network transfer costs, and I/O overhead by avoiding the processing of columns that don't contribute to the final result.\n\nThe projection pushdown algorithm tracks column usage throughout the query plan:\n\n1. **Column Dependency Analysis**: Identify which columns are needed at each operator\n2. **Elimination Identification**: Find columns that can be dropped after specific operations\n3. **Schema Propagation**: Update the `output_schema` field in `OperatorNode` instances to reflect reduced column sets\n\nThis analysis must account for columns needed for intermediate operations even if they don't appear in the final result. For example, join columns and grouping columns must be preserved through the relevant operations even if they're eventually projected away.\n\n**Sort Elimination Rules**\n\nSorting operations are expensive, but many sorts can be eliminated through clever optimization rules:\n\n- **Index Order Utilization**: Use pre-sorted index access to satisfy ORDER BY clauses\n- **Sort Merge Optimization**: When merge joins require sorting, align the sort order with final ORDER BY requirements\n- **Redundant Sort Elimination**: Remove sorts that are overridden by subsequent operations\n\nThe sort elimination analysis maintains information about data ordering throughout the query plan, tracking which columns remain sorted after each operation and identifying opportunities to preserve useful orderings.\n\n**Aggregate Optimization Rules**\n\nAggregation operations benefit from several specialized optimization rules:\n\n- **Early Aggregation**: Push GROUP BY operations below joins when possible to reduce intermediate result sizes\n- **Index-Based Aggregation**: Use indexes to avoid sorting for GROUP BY operations\n- **Aggregate Elimination**: Remove unnecessary DISTINCT operations and redundant aggregates\n\nEarly aggregation proves particularly effective in data warehouse queries where large fact tables join to smaller dimension tables. By aggregating the fact table data before joining, we can dramatically reduce the join input sizes.\n\n### Architecture Decision: Rule-Based vs Cost-Based Physical Selection\n\nThe physical planning component must choose between two fundamentally different approaches for selecting concrete operators and applying optimizations. This decision significantly impacts both the complexity of the implementation and the quality of generated plans.\n\n> **Decision: Hybrid Rule-Based and Cost-Based Physical Selection**\n>\n> **Context**: Physical planning requires making numerous decisions about concrete operator implementations, access methods, and plan transformations. Pure rule-based systems apply predetermined heuristics, while pure cost-based systems evaluate all alternatives using cost estimates. Each approach has distinct trade-offs in implementation complexity, optimization time, and plan quality.\n>\n> **Options Considered**:\n> 1. **Pure Rule-Based Selection**: Apply predetermined rules and heuristics for all physical planning decisions\n> 2. **Pure Cost-Based Selection**: Evaluate cost estimates for all alternative physical implementations\n> 3. **Hybrid Approach**: Use rules for transformations and cost-based selection for operator choices\n>\n> **Decision**: Implement a hybrid approach that applies rule-based transformations for provably beneficial optimizations and uses cost-based selection for operator and access method choices.\n>\n> **Rationale**: Rule-based transformations like predicate pushdown provide consistent benefits without expensive cost calculations, while operator selection benefits significantly from cost-based analysis. This hybrid approach balances optimization quality with computational efficiency.\n>\n> **Consequences**: Enables fast optimization while maintaining plan quality. Requires careful classification of decisions into rule-based vs cost-based categories. Creates opportunities for future enhancement through machine learning-based rule refinement.\n\n**Rule-Based vs Cost-Based Comparison**\n\n| Aspect | Rule-Based Approach | Cost-Based Approach | Hybrid Approach |\n|--------|-------------------|-------------------|-----------------|\n| **Implementation Complexity** | Low - predetermined logic | High - requires accurate cost models | Medium - combines both strategies |\n| **Optimization Speed** | Fast - no cost calculations | Slow - evaluates many alternatives | Medium - selective cost evaluation |\n| **Plan Quality** | Good for common patterns | Excellent with accurate statistics | Very good - best of both worlds |\n| **Adaptability** | Poor - fixed rules | Excellent - adapts to data changes | Good - rules provide baseline |\n| **Debugging Difficulty** | Easy - deterministic decisions | Hard - depends on cost model accuracy | Medium - clear decision boundaries |\n| **Statistical Dependency** | Minimal - uses simple heuristics | High - requires accurate statistics | Medium - graceful degradation |\n\n**Rule-Based Decision Categories**\n\nOur hybrid implementation classifies physical planning decisions into categories that benefit from rule-based treatment:\n\n1. **Provably Beneficial Transformations**: Predicate pushdown, projection elimination, and redundant operation removal always improve performance\n2. **Semantic Preserving Rules**: Transformations that maintain query correctness under all conditions\n3. **Heuristic Simplifications**: Rules that provide good results in the vast majority of cases without expensive analysis\n\nThese rule-based decisions execute first in our physical planning pipeline, transforming the logical plan into an improved logical plan before beginning cost-based operator selection.\n\n**Cost-Based Decision Categories**\n\nThe cost-based portion of our implementation focuses on decisions where data characteristics significantly impact the optimal choice:\n\n1. **Access Method Selection**: Index vs sequential scan decisions depend heavily on data selectivity and clustering\n2. **Join Algorithm Selection**: Hash vs nested loop vs merge join performance varies dramatically based on input sizes\n3. **Memory Allocation**: Buffer size and sort memory decisions require understanding of data volumes and available resources\n\nFor these decisions, our implementation uses the `estimateCost` method to evaluate alternatives and selects the option with the lowest estimated total cost.\n\n**Implementation Strategy**\n\nThe `selectPhysicalOperators` method implements our hybrid approach through a multi-phase process:\n\n1. **Rule Application Phase**: Apply all beneficial rule-based transformations to the logical plan\n2. **Cost Analysis Phase**: For each remaining decision point, enumerate alternatives and calculate costs\n3. **Selection Phase**: Choose the lowest-cost alternative for each decision\n4. **Plan Construction Phase**: Build the final `ExecutionPlan` with selected physical operators\n\nThis phased approach ensures that rule-based optimizations reduce the search space before expensive cost-based analysis begins, improving both optimization speed and plan quality.\n\n**Fallback Strategies**\n\nOur hybrid implementation includes fallback strategies for situations where cost-based analysis fails or produces unreliable results:\n\n- **Missing Statistics**: Fall back to rule-based heuristics when table statistics are unavailable or stale\n- **Cost Model Uncertainty**: Use rules when cost estimates have low confidence levels\n- **Optimization Timeout**: Switch to rule-based selection when cost analysis exceeds the `OPTIMIZATION_TIMEOUT` threshold\n\nThese fallback mechanisms ensure that the physical planner always produces executable plans even when optimal cost-based decisions aren't possible.\n\n![Physical Operator Selection Flow](./diagrams/physical-selection-flow.svg)\n\n### Common Physical Planning Pitfalls\n\nPhysical planning involves numerous subtle decisions that can dramatically impact query performance. Developers implementing physical planners often encounter specific categories of mistakes that lead to suboptimal plans or incorrect query results. Understanding these pitfalls helps avoid common implementation errors and design flaws.\n\n**⚠️ Pitfall: Ignoring Index Column Order Requirements**\n\nMany developers incorrectly assume that any index covering the required columns can be used effectively for a query. In reality, B-tree indexes are only effective when predicates match the leftmost columns of the index key in order.\n\nConsider a composite index on `(customer_id, order_date, status)`. A query with predicates `WHERE order_date > '2023-01-01' AND status = 'shipped'` cannot effectively use this index because it doesn't include a predicate on the leftmost column `customer_id`. The index scan would need to examine most of the index entries, making it less efficient than a sequential scan.\n\n**Fix**: Implement proper index matching logic that verifies predicate coverage of leftmost index columns. The `selectPhysicalOperators` method should score indexes based on prefix matching, not just column coverage.\n\n**⚠️ Pitfall: Incorrect Outer Join Predicate Pushdown**\n\nPredicate pushdown becomes semantically incorrect with outer joins when predicates are pushed past the preserving side of the join. This error changes query results by converting outer joins into inner joins through aggressive filtering.\n\nFor a `LEFT OUTER JOIN` between customers and orders, pushing a predicate like `WHERE orders.amount > 1000` below the join eliminates customers without orders from the result set. The correct behavior preserves customers without orders and only filters the orders that do exist.\n\n**Fix**: Implement join-aware predicate analysis that distinguishes between predicates that can be safely pushed (predicates on the preserved side) and those that must remain at the join level (predicates on the null-extended side).\n\n**⚠️ Pitfall: Memory Overcommitment in Hash Joins**\n\nHash join selection often fails to account for concurrent query execution and memory competition. Selecting hash joins based on total system memory rather than available query memory leads to excessive disk spilling and performance degradation.\n\nWhen multiple concurrent queries each assume they can use most of the available memory for hash tables, the system thrashes as hash joins repeatedly spill to disk. This problem becomes severe in multi-user environments where query concurrency is high.\n\n**Fix**: Implement memory-aware cost modeling that accounts for concurrent query execution. The hash join selection logic should use available query memory limits rather than total system memory when evaluating algorithm feasibility.\n\n**⚠️ Pitfall: Overlooking Sort Order Preservation**\n\nPhysical planning often misses opportunities to preserve useful data orderings from one operation to the next. This oversight leads to unnecessary sorting operations when data is already in a suitable order for subsequent operations.\n\nFor example, if an index scan produces data sorted by `customer_id`, and the query includes `GROUP BY customer_id`, the grouping operation doesn't require additional sorting. However, naive physical planning might insert an explicit sort operation between the scan and grouping.\n\n**Fix**: Implement order property tracking throughout the physical planning process. Each `OperatorNode` should maintain information about the sort order of its output, and subsequent operations should check whether their ordering requirements are already satisfied.\n\n**⚠️ Pitfall: Cost Model Unit Inconsistencies**\n\nCost estimation becomes unreliable when different cost components use inconsistent units or scales. Common problems include mixing page-based I/O costs with row-based CPU costs without proper scaling factors, leading to systematic bias toward I/O-intensive or CPU-intensive plans.\n\nFor instance, if I/O costs are measured in milliseconds while CPU costs are measured in microseconds, the cost model will heavily favor CPU-intensive operations even when I/O operations would be more efficient in practice.\n\n**Fix**: Standardize all cost components to consistent units in the `CostEstimate` structure. Use the predefined constants `IO_PAGE_COST`, `CPU_TUPLE_COST`, and `MEMORY_PAGE_COST` as the baseline units and ensure all cost calculations scale appropriately.\n\n**⚠️ Pitfall: Ignoring Data Clustering Effects**\n\nPhysical planning often treats all I/O operations as equivalent, ignoring the significant performance differences between sequential and random access patterns. This oversight leads to poor access method choices, particularly for scan operations on clustered vs unclustered data.\n\nTables with good clustering on frequently queried columns benefit significantly from sequential scans because related rows are physically co-located. Conversely, index scans on poorly clustered data generate many random I/O operations, making them much more expensive than cost models typically account for.\n\n**Fix**: Incorporate clustering factor analysis into access method selection. Use the `clustering_factor` field in `TableStatistics` to adjust I/O cost estimates based on expected access patterns. Apply the `RANDOM_MULTIPLIER` penalty more aggressively for index scans on poorly clustered data.\n\n**⚠️ Pitfall: Premature Join Algorithm Commitment**\n\nSome implementations commit to join algorithms too early in the planning process, before understanding the full context of the query execution. This premature commitment prevents the optimizer from making globally optimal decisions about memory allocation and operation ordering.\n\nFor example, choosing hash joins for early operations in a complex query might consume all available memory, forcing later joins to use less efficient nested loop algorithms. A global view might reveal that different algorithm assignments produce better overall performance.\n\n**Fix**: Implement holistic join algorithm selection that considers resource constraints across the entire query plan. The `selectPhysicalOperators` method should evaluate algorithm combinations rather than making independent decisions for each join operation.\n\n### Implementation Guidance\n\n**A. Technology Recommendations Table:**\n\n| Component | Simple Option | Advanced Option |\n|-----------|--------------|-----------------|\n| Cost Comparison | Dictionary with manual calculations | Cost model classes with configurable weights |\n| Rule Engine | If-else chains for transformations | Pattern matching with rule priority system |\n| Index Metadata | Simple list of available indexes | Full index statistics with selectivity histograms |\n| Memory Management | Fixed memory assumptions | Dynamic memory allocation with concurrency awareness |\n| Plan Validation | Basic tree structure checks | Semantic correctness verification with test cases |\n\n**B. Recommended File/Module Structure**\n\n```\nquery_optimizer/\n  physical/\n    __init__.py\n    physical_planner.py           ← main PhysicalPlanner class\n    access_methods.py             ← scan selection logic\n    join_algorithms.py            ← join algorithm selection\n    optimization_rules.py         ← predicate pushdown and transformations\n    cost_comparison.py            ← physical operator cost evaluation\n    plan_builder.py               ← ExecutionPlan construction\n  test/\n    test_physical_planner.py      ← comprehensive physical planning tests\n    test_access_methods.py        ← access method selection tests\n    test_optimization_rules.py    ← rule application tests\n  examples/\n    sample_physical_plans.py      ← example optimized plans for reference\n```\n\n**C. Infrastructure Starter Code**\n\n```python\n# physical/cost_comparison.py - COMPLETE cost comparison infrastructure\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\nfrom ..core.data_model import CostEstimate, OperatorNode, TableStatistics\n\n# Cost constants from our design\nIO_PAGE_COST = 1.0\nCPU_TUPLE_COST = 0.01\nMEMORY_PAGE_COST = 0.001\nSEQUENTIAL_MULTIPLIER = 0.3\nRANDOM_MULTIPLIER = 2.0\nSELECTIVITY_THRESHOLD = 0.1\n\n@dataclass\nclass AccessMethodOption:\n    \"\"\"Represents a possible access method with cost estimate.\"\"\"\n    method_type: str  # 'sequential_scan', 'index_scan', 'index_only_scan'\n    index_name: str = None\n    estimated_cost: CostEstimate = None\n    estimated_rows: int = 0\n    properties: Dict[str, Any] = None\n\nclass PhysicalCostComparator:\n    \"\"\"Compares costs between different physical operator implementations.\"\"\"\n    \n    def __init__(self, statistics: Dict[str, TableStatistics]):\n        self.statistics = statistics\n    \n    def compare_scan_methods(self, table_name: str, predicates: List[tuple], \n                           available_indexes: List[Dict]) -> AccessMethodOption:\n        \"\"\"Compare sequential scan vs available index access methods.\"\"\"\n        options = []\n        table_stats = self.statistics.get(table_name)\n        if not table_stats:\n            # Fallback to sequential scan when no statistics available\n            return AccessMethodOption(\n                method_type='sequential_scan',\n                estimated_cost=self._estimate_sequential_scan_cost(table_name, predicates),\n                estimated_rows=1000  # default estimate\n            )\n        \n        # Always consider sequential scan\n        seq_cost = self._estimate_sequential_scan_cost(table_name, predicates)\n        options.append(AccessMethodOption('sequential_scan', None, seq_cost, \n                                        int(table_stats.row_count * self._estimate_selectivity(predicates))))\n        \n        # Consider each available index\n        for index in available_indexes:\n            if self._index_matches_predicates(index, predicates):\n                index_cost = self._estimate_index_scan_cost(table_name, index, predicates)\n                options.append(AccessMethodOption(\n                    'index_scan', \n                    index['name'], \n                    index_cost,\n                    int(table_stats.row_count * self._estimate_selectivity(predicates))\n                ))\n        \n        # Return lowest cost option\n        return min(options, key=lambda x: x.estimated_cost.total_cost)\n    \n    def _estimate_sequential_scan_cost(self, table_name: str, predicates: List[tuple]) -> CostEstimate:\n        \"\"\"Estimate cost for sequential table scan with predicates.\"\"\"\n        table_stats = self.statistics.get(table_name)\n        if not table_stats:\n            return CostEstimate(io_cost=100.0, cpu_cost=10.0, memory_cost=1.0, estimated_rows=1000)\n        \n        # Sequential I/O is efficient\n        io_cost = table_stats.page_count * IO_PAGE_COST * SEQUENTIAL_MULTIPLIER\n        cpu_cost = table_stats.row_count * CPU_TUPLE_COST\n        selectivity = self._estimate_selectivity(predicates)\n        \n        return CostEstimate(\n            io_cost=io_cost,\n            cpu_cost=cpu_cost,\n            memory_cost=MEMORY_PAGE_COST,\n            estimated_rows=int(table_stats.row_count * selectivity)\n        )\n    \n    def _estimate_index_scan_cost(self, table_name: str, index: Dict, predicates: List[tuple]) -> CostEstimate:\n        \"\"\"Estimate cost for index-based access.\"\"\"\n        table_stats = self.statistics.get(table_name)\n        if not table_stats:\n            return CostEstimate(io_cost=50.0, cpu_cost=5.0, memory_cost=1.0, estimated_rows=100)\n        \n        selectivity = self._estimate_selectivity(predicates)\n        index_pages = index.get('page_count', table_stats.page_count // 10)\n        \n        # Index scan involves index pages + random data page access\n        io_cost = (index_pages * IO_PAGE_COST + \n                  table_stats.page_count * selectivity * IO_PAGE_COST * RANDOM_MULTIPLIER)\n        cpu_cost = table_stats.row_count * selectivity * CPU_TUPLE_COST\n        \n        return CostEstimate(\n            io_cost=io_cost,\n            cpu_cost=cpu_cost,\n            memory_cost=MEMORY_PAGE_COST,\n            estimated_rows=int(table_stats.row_count * selectivity)\n        )\n    \n    def _estimate_selectivity(self, predicates: List[tuple]) -> float:\n        \"\"\"Estimate combined selectivity of all predicates.\"\"\"\n        if not predicates:\n            return 1.0\n        \n        # Simple assumption: independent predicates multiply\n        total_selectivity = 1.0\n        for predicate in predicates:\n            # Each predicate assumed to have 0.1 selectivity on average\n            total_selectivity *= 0.1\n        \n        return min(total_selectivity, 1.0)\n    \n    def _index_matches_predicates(self, index: Dict, predicates: List[tuple]) -> bool:\n        \"\"\"Check if index can effectively support the given predicates.\"\"\"\n        if not predicates:\n            return False\n        \n        index_columns = index.get('columns', [])\n        if not index_columns:\n            return False\n        \n        # Check if any predicate matches the leftmost index column\n        leftmost_column = index_columns[0]\n        for predicate in predicates:\n            if len(predicate) >= 2 and predicate[1] == leftmost_column:\n                return True\n        \n        return False\n```\n\n**D. Core Logic Skeleton Code**\n\n```python\n# physical/physical_planner.py - Core physical planning logic (SKELETON)\nfrom typing import Dict, List, Optional\nfrom ..core.data_model import ExecutionPlan, OperatorNode, TableStatistics, JoinType\nfrom .cost_comparison import PhysicalCostComparator\nfrom .optimization_rules import OptimizationRuleEngine\n\nclass PhysicalPlanner:\n    \"\"\"Transforms logical plans into optimized physical execution plans.\"\"\"\n    \n    def __init__(self, statistics: Dict[str, TableStatistics]):\n        self.statistics = statistics\n        self.cost_comparator = PhysicalCostComparator(statistics)\n        self.rule_engine = OptimizationRuleEngine()\n    \n    def selectPhysicalOperators(self, logical_plan: OperatorNode, \n                              stats: Dict[str, TableStatistics]) -> ExecutionPlan:\n        \"\"\"Main entry point: convert logical plan to optimized physical plan.\"\"\"\n        # TODO 1: Apply rule-based optimizations first (predicate pushdown, projection elimination)\n        # TODO 2: For each operator node, evaluate physical implementation alternatives\n        # TODO 3: Select lowest-cost physical operator for each logical operation\n        # TODO 4: Verify that physical plan maintains semantic correctness\n        # TODO 5: Build final ExecutionPlan with cost estimates and metadata\n        # Hint: Use postorder traversal to process children before parents\n        pass\n    \n    def _select_scan_operator(self, logical_node: OperatorNode) -> OperatorNode:\n        \"\"\"Choose between sequential scan and index access methods.\"\"\"\n        # TODO 1: Extract table name and filter predicates from logical node\n        # TODO 2: Get available indexes for the table from statistics\n        # TODO 3: Use cost_comparator.compare_scan_methods() to evaluate options\n        # TODO 4: Create physical operator node with selected access method\n        # TODO 5: Set operator properties (index_name, scan_type, estimated_cost)\n        # Hint: Check selectivity against SELECTIVITY_THRESHOLD for quick decisions\n        pass\n    \n    def _select_join_algorithm(self, logical_node: OperatorNode) -> OperatorNode:\n        \"\"\"Choose join algorithm based on input characteristics.\"\"\"\n        # TODO 1: Analyze left and right child operators for size estimates\n        # TODO 2: Check available memory for hash join feasibility\n        # TODO 3: Evaluate nested loop cost with index access on inner relation\n        # TODO 4: Consider merge join if inputs are already sorted\n        # TODO 5: Select algorithm with lowest estimated cost\n        # Hint: Hash joins good when smaller input fits in memory\n        pass\n    \n    def _apply_optimization_rules(self, logical_plan: OperatorNode) -> OperatorNode:\n        \"\"\"Apply rule-based transformations to improve plan efficiency.\"\"\"\n        # TODO 1: Apply predicate pushdown rules throughout the plan tree\n        # TODO 2: Eliminate unnecessary projections and redundant operations\n        # TODO 3: Identify sort operations that can be eliminated via index ordering\n        # TODO 4: Push aggregations below joins where semantically correct\n        # TODO 5: Validate that all transformations preserve query semantics\n        # Hint: Use preorder traversal for pushdown, postorder for elimination\n        pass\n    \n    def _estimate_memory_requirement(self, operator: OperatorNode) -> float:\n        \"\"\"Calculate memory needed for physical operator execution.\"\"\"\n        # TODO 1: For hash joins, estimate hash table size based on build input\n        # TODO 2: For sort operations, calculate buffer requirements\n        # TODO 3: For scan operations, estimate buffer pool usage\n        # TODO 4: Account for concurrent query execution and available memory\n        # TODO 5: Return memory requirement in consistent units (MB or pages)\n        pass\n    \n    def _validate_physical_plan(self, plan: ExecutionPlan) -> bool:\n        \"\"\"Verify that physical plan is executable and semantically correct.\"\"\"\n        # TODO 1: Check that all required indexes actually exist\n        # TODO 2: Verify that memory requirements don't exceed system limits\n        # TODO 3: Ensure that join algorithms match available join predicates\n        # TODO 4: Validate that predicate pushdown hasn't altered semantics\n        # TODO 5: Confirm that all output schemas are correctly propagated\n        return True\n```\n\n**E. Language-Specific Hints**\n\n- **Dictionary Access**: Use `statistics.get(table_name)` with None checks for safe statistics lookup\n- **Cost Calculations**: Import `math` module for logarithmic and exponential cost functions\n- **Tree Traversal**: Use recursive functions or explicit stacks for depth-first plan tree traversal\n- **Enum Comparisons**: Import `OperatorType` and `JoinType` enums for type-safe operator comparisons\n- **Memory Estimation**: Use `sys.getsizeof()` for Python object memory footprint estimates\n- **Performance**: Consider `functools.lru_cache` decorator for expensive cost calculations\n- **Testing**: Use `pytest.approx()` for floating-point cost estimate comparisons\n- **Debugging**: Add logging with `logging.getLogger(__name__)` to trace planning decisions\n\n**F. Milestone Checkpoint**\n\nAfter implementing the Physical Planning Component:\n\n**Test Command**: `python -m pytest test/test_physical_planner.py -v`\n\n**Expected Behaviors**:\n1. **Access Method Selection**: `test_scan_method_selection()` should show index scans chosen for selective predicates and sequential scans for high-selectivity queries\n2. **Join Algorithm Choice**: `test_join_algorithm_selection()` should demonstrate hash joins for equal-sized tables and nested loops for small-large combinations  \n3. **Rule Applications**: `test_predicate_pushdown()` should verify that filters move below joins and projections eliminate unused columns\n4. **Cost Consistency**: `test_cost_estimates()` should confirm that physical plan costs are reasonable and consistent with input statistics\n\n**Manual Verification**:\n```python\n# Create test query and run optimization\nfrom query_optimizer.physical.physical_planner import PhysicalPlanner\nfrom query_optimizer.test.sample_data import create_test_statistics\n\nplanner = PhysicalPlanner(create_test_statistics())\nlogical_plan = create_sample_logical_plan()  # Your test plan\nphysical_plan = planner.selectPhysicalOperators(logical_plan, planner.statistics)\n\n# Should show concrete operators and reasonable costs\nprint(physical_plan.pretty_print(show_costs=True))\nprint(f\"Total estimated cost: {physical_plan.total_cost.total_cost}\")\n```\n\n**Signs Something Is Wrong**:\n- **All scans are sequential**: Indicates index selection logic isn't working\n- **All joins use nested loops**: Suggests hash join cost estimation is broken  \n- **Costs are negative or extremely large**: Points to cost calculation errors\n- **Plan fails validation**: Means rule applications broke semantic correctness\n\n\n## Interactions and Data Flow\n\n> **Milestone(s):** All Milestones 1-4 - describes how components collaborate throughout the optimization pipeline, from plan representation through cost estimation and join ordering to physical planning.\n\nThe query optimizer's effectiveness depends not just on individual component quality, but on how these components collaborate to transform a SQL query into an optimized execution plan. Think of this like an automotive assembly line where each station performs specialized work on the product, but the final car quality depends on precise coordination, timing, and information flow between stations. Each optimizer component receives structured inputs, performs its specialized analysis, and passes enriched results to the next stage in the pipeline.\n\n![Optimization Process Workflow](./diagrams/optimization-workflow.svg)\n\nThis section explores three critical aspects of component collaboration: the complete optimization sequence that transforms SQL queries into executable plans, the communication interfaces that enable clean component integration, and the caching mechanisms that improve optimization performance through plan reuse.\n\n### Complete Optimization Sequence\n\nThe optimization process follows a carefully orchestrated sequence of transformations, where each phase builds upon the outputs of previous phases while maintaining clean separation of concerns. Understanding this sequence is crucial for implementing a robust optimizer that can handle complex queries efficiently while managing computational resources.\n\n**Mental Model: Document Review Pipeline**\n\nThink of query optimization like a document review process in a legal firm. A contract arrives and goes through multiple specialist reviews: first a paralegal creates an outline of key sections (logical planning), then a financial analyst estimates costs and risks (cost estimation), a negotiation expert determines the best order to present terms (join ordering), and finally a senior partner selects specific legal strategies and language (physical planning). Each specialist adds their expertise while the document flows forward, becoming more refined and actionable at each stage.\n\nThe optimization pipeline consists of five distinct phases, each with specific inputs, outputs, and responsibilities:\n\n| Phase | Input | Output | Primary Responsibility | Key Decisions Made |\n|-------|-------|--------|----------------------|-------------------|\n| Query Parsing | Raw SQL text | `ParsedQuery` structure | Extract tables, columns, predicates, joins | Validate syntax, identify query structure |\n| Logical Planning | `ParsedQuery` | Logical `ExecutionPlan` | Build operator tree with logical operations | Determine required operations, establish tree structure |\n| Cost Estimation | Logical plan + `TableStatistics` | Cost-annotated logical plan | Calculate resource consumption estimates | Estimate selectivity, cardinality, I/O costs |\n| Join Optimization | Cost-annotated plan | Optimized logical plan | Find efficient join order using dynamic programming | Select join sequence, prune suboptimal alternatives |\n| Physical Planning | Optimized logical plan | Physical `ExecutionPlan` | Choose concrete operators and access methods | Select scan methods, join algorithms, apply optimization rules |\n\n#### Phase 1: Query Parsing and Initial Structure\n\nThe optimization sequence begins when the `optimize_query` function receives a `ParsedQuery` structure containing the essential elements extracted from SQL parsing. This phase focuses on understanding query structure without making optimization decisions.\n\nThe parser has already identified the fundamental query components that drive optimization decisions:\n\n| Query Component | Information Extracted | Optimization Impact |\n|----------------|----------------------|-------------------|\n| Tables | List of base tables referenced | Determines join ordering search space |\n| Columns | Projected and referenced columns | Influences index selection and schema propagation |\n| Join Predicates | Equality and comparison conditions between tables | Defines valid join orders and selectivity estimates |\n| Filter Predicates | Selection conditions on individual tables | Drives scan method selection and predicate pushdown opportunities |\n| Ordering Requirements | ORDER BY and GROUP BY clauses | Influences join algorithm choice and sort elimination |\n\nThe parsed query structure provides the foundation for all subsequent optimization phases, ensuring that logical and physical transformations preserve query semantics while improving execution efficiency.\n\n#### Phase 2: Logical Plan Construction\n\nThe Plan Builder component transforms the parsed query structure into a logical execution plan represented as an `OperatorNode` tree. This phase focuses on correctness and completeness rather than efficiency, establishing the semantic foundation for later optimizations.\n\nThe logical planning process follows a systematic approach:\n\n1. **Base Table Access Creation**: For each table in the query, create a logical `SCAN` operator node with the table name stored in the operator properties dictionary and the full table schema propagated as the `output_schema`.\n\n2. **Filter Integration**: For each filter predicate associated with a table, create a `FILTER` operator node positioned above the corresponding scan operation, with the predicate condition stored in the node properties and selectivity initially unknown.\n\n3. **Join Tree Construction**: Based on the join predicates, construct a preliminary join tree using `JOIN` operator nodes, initially ordered according to the SQL query structure without optimization considerations.\n\n4. **Projection Application**: Add a `PROJECT` operator node at the tree root to select only the columns specified in the SQL SELECT clause, establishing the final output schema.\n\n5. **Schema Propagation**: Traverse the tree bottom-up, calculating and storing the `output_schema` for each operator node based on its inputs and operation semantics.\n\nThe resulting logical plan captures the query's semantic requirements without committing to specific physical implementations or optimization strategies. Each `OperatorNode` contains placeholder values for `cost_estimate` and `estimated_rows` that will be populated during cost estimation.\n\n#### Phase 3: Cost Estimation and Statistics Integration\n\nThe Cost Estimator component enriches the logical plan with resource consumption estimates by integrating table statistics and selectivity calculations. This phase transforms the logical plan from a semantic specification into a quantified prediction model.\n\nThe cost estimation process proceeds through the tree in postorder traversal, ensuring child costs are calculated before parent costs:\n\n1. **Statistics Lookup**: For each `SCAN` operator, retrieve the corresponding `TableStatistics` including row counts, page counts, and column histograms from the statistics catalog.\n\n2. **Base Cost Calculation**: Calculate the I/O and CPU costs for accessing each base table, storing the results in the operator's `cost_estimate` field using the formula: `io_cost = page_count * IO_PAGE_COST` and `cpu_cost = row_count * CPU_TUPLE_COST`.\n\n3. **Filter Selectivity Estimation**: For each `FILTER` operator, call `estimate_filter_selectivity` using column statistics to predict the fraction of input rows that will survive the predicate, updating both `estimated_rows` and the operator's cost estimate.\n\n4. **Join Cardinality Estimation**: For each `JOIN` operator, use `estimate_join_cardinality` to predict output size based on input cardinalities and join predicate selectivity, accounting for foreign key relationships and value distribution skew.\n\n5. **Cost Accumulation**: Combine child operator costs with the current operator's processing cost, storing the cumulative resource consumption in the `CostEstimate` structure with separate tracking of I/O, CPU, and memory components.\n\nAfter cost estimation, each operator node contains realistic resource consumption predictions that enable the join optimizer to make informed decisions about alternative execution strategies.\n\n#### Phase 4: Join Order Optimization\n\nThe Join Optimizer component uses dynamic programming to explore alternative join orders and select the sequence that minimizes total execution cost. This phase represents the core algorithmic challenge in query optimization, managing exponential search complexity while finding high-quality solutions.\n\nThe join optimization process implements the classic dynamic programming algorithm for optimal join ordering:\n\n1. **Base Case Initialization**: Create single-table access plans for each base table using the existing scan and filter operators, storing these plans in the dynamic programming memoization table indexed by single-table subsets.\n\n2. **Subset Enumeration**: For each subset size from 2 to the total number of tables, enumerate all possible table combinations and check connectivity through available join predicates.\n\n3. **Partition Evaluation**: For each connected subset, consider all possible ways to partition the tables into two smaller subsets, retrieving optimal plans for each partition from the memoization table.\n\n4. **Join Cost Comparison**: For each valid partition, estimate the cost of joining the two optimal subplans using different join algorithms, selecting the alternative with minimum total cost including both child costs and join processing overhead.\n\n5. **Pruning and Memoization**: Store only the lowest-cost plan for each table subset in the memoization table, pruning dominated alternatives to control memory usage and prevent redundant computation.\n\n6. **Optimal Plan Extraction**: Retrieve the optimal plan for the complete table set from the memoization table, representing the join order that minimizes estimated execution cost.\n\nThe dynamic programming approach guarantees finding the optimal join order within the search space constraints, while memoization prevents exponential explosion by reusing optimal solutions for table subsets.\n\n#### Phase 5: Physical Operator Selection\n\nThe Physical Planner component transforms the optimized logical plan into an executable physical plan by selecting concrete operators, access methods, and applying rule-based optimizations. This phase bridges the gap between logical specifications and actual execution capabilities.\n\nThe physical planning process operates through multiple transformation passes:\n\n1. **Access Method Selection**: For each `SCAN` operator, call `compare_scan_methods` to evaluate sequential scan versus available index access methods, selecting the alternative with lowest estimated cost based on filter selectivity and index characteristics.\n\n2. **Join Algorithm Selection**: For each `JOIN` operator, analyze input characteristics including estimated cardinalities, memory requirements, and sort orders to choose between hash join, nested loop join, and merge join algorithms.\n\n3. **Optimization Rule Application**: Apply rule-based transformations including predicate pushdown, projection elimination, and constant folding to reduce intermediate result sizes and eliminate unnecessary operations.\n\n4. **Memory and Resource Planning**: Calculate memory requirements for each physical operator, ensuring that hash tables and sort buffers fit within available memory while identifying operators that may require disk spilling.\n\n5. **Plan Validation**: Verify that the resulting physical plan is executable and semantically correct, checking for missing implementations, incompatible operator combinations, and resource constraint violations.\n\nThe final physical `ExecutionPlan` contains concrete operator implementations that can be directly executed by the query engine, with all optimization decisions resolved and resource requirements quantified.\n\n![Query Optimizer System Architecture](./diagrams/system-architecture.svg)\n\n> **Critical Insight**: The optimization sequence maintains strict separation between logical correctness (phases 1-2), cost modeling (phase 3), combinatorial optimization (phase 4), and physical implementation (phase 5). This separation enables independent testing and debugging of each optimization aspect while supporting modular component development.\n\n### Component Communication Interfaces\n\nThe optimizer components communicate through well-defined interfaces that encapsulate data exchange patterns and hide internal implementation complexity. These interfaces enable clean component integration while supporting future extensibility and testing isolation.\n\n**Mental Model: Standardized Manufacturing Interfaces**\n\nThink of component interfaces like standardized connectors in manufacturing equipment. Just as electrical components use standard voltage levels and connector types to ensure compatibility, optimizer components use standardized data structures and method signatures. This allows components to be developed, tested, and upgraded independently while maintaining system integration, similar to how you can replace a factory machine without redesigning the entire production line.\n\n#### Primary Component Interfaces\n\nThe optimizer defines several key interface contracts that govern component collaboration:\n\n| Interface Type | Purpose | Key Methods | Data Exchange Format |\n|---------------|---------|-------------|---------------------|\n| Plan Builder Interface | Construct logical plans from parsed queries | `build_logical_plan(parsed_query)` | `ParsedQuery` → Logical `ExecutionPlan` |\n| Cost Estimator Interface | Annotate plans with resource estimates | `estimateCost(plan)`, `collectStatistics(table)` | `ExecutionPlan` + `TableStatistics` → Cost-annotated plan |\n| Join Optimizer Interface | Find optimal join orders | `optimizeJoinOrder(tables)` | Logical plan → Optimized logical plan |\n| Physical Planner Interface | Select concrete operators | `selectPhysicalOperators(logical_plan, stats)` | Logical plan + statistics → Physical `ExecutionPlan` |\n| Statistics Provider Interface | Supply table and column statistics | `get_table_stats(table_name)`, `get_column_stats(table, column)` | Table/column identifiers → `TableStatistics`/`ColumnStatistics` |\n\n#### Plan Builder Interface Details\n\nThe Plan Builder provides the foundational interface for transforming parsed SQL queries into logical execution plans:\n\n| Method Signature | Parameters | Returns | Purpose | Error Conditions |\n|------------------|------------|---------|---------|-----------------|\n| `build_logical_plan(parsed_query)` | `ParsedQuery` with tables, joins, filters | Logical `ExecutionPlan` | Create operator tree representing query semantics | Invalid joins, missing tables, circular references |\n| `validate_query_semantics(parsed_query)` | `ParsedQuery` structure | Boolean validation result | Check query for semantic errors before planning | Unknown columns, type mismatches, ambiguous references |\n| `extract_table_dependencies(parsed_query)` | `ParsedQuery` with join predicates | Dictionary of table relationships | Identify join graph connectivity for optimization | Disconnected table groups, missing join predicates |\n| `propagate_schema_information(plan)` | `ExecutionPlan` tree structure | Updated plan with schema annotations | Calculate output schemas for each operator node | Schema propagation conflicts, unknown column types |\n\nThe Plan Builder interface encapsulates the complexity of logical plan construction while providing clean integration points for query validation and schema management.\n\n#### Cost Estimator Interface Details\n\nThe Cost Estimator interface defines methods for statistical analysis and cost calculation:\n\n| Method Signature | Parameters | Returns | Purpose | Statistics Dependencies |\n|------------------|------------|---------|---------|------------------------|\n| `estimateCost(plan)` | `ExecutionPlan` with operator tree | Plan annotated with `CostEstimate` objects | Calculate resource consumption for entire plan | Current `TableStatistics` for all referenced tables |\n| `estimate_filter_selectivity(table, column, operator, value)` | Table name, column name, comparison operator, filter value | Float selectivity between 0.0 and 1.0 | Predict fraction of rows surviving filter | `ColumnStatistics` with histogram or distinct value count |\n| `estimate_join_cardinality(left_table, left_column, right_table, right_column)` | Join participant tables and columns | Integer estimated row count | Predict join output size | Statistics for both join columns including null counts |\n| `collectStatistics(table, sample_rate)` | Table name and sampling percentage | `TableStatistics` object | Gather current data distribution information | Table access permissions, sufficient disk space for sampling |\n| `update_statistics_cache(table_stats)` | `TableStatistics` object | None (side effect: cache update) | Store statistics for reuse across queries | Valid statistics with recent timestamps |\n\nThe Cost Estimator interface abstracts statistical modeling complexity while providing flexible integration with different statistics collection strategies.\n\n#### Join Optimizer Interface Details\n\nThe Join Optimizer interface defines methods for combinatorial join order optimization:\n\n| Method Signature | Parameters | Returns | Purpose | Optimization Constraints |\n|------------------|------------|---------|---------|-------------------------|\n| `optimizeJoinOrder(tables)` | List of table names with join predicates | `JoinOrder` specifying optimal sequence | Find minimum-cost join order using dynamic programming | Connected join graph, reasonable table count (< MAX_JOIN_ENUMERATION) |\n| `enumerate_join_alternatives(table_subset)` | Subset of tables for join consideration | List of alternative join plans | Generate candidate join plans for cost comparison | Valid join predicates between table pairs |\n| `evaluate_join_cost(left_plan, right_plan, predicates)` | Two subplans and joining predicates | `CostEstimate` for join operation | Estimate cost of joining two subplans | Cost estimates available for input subplans |\n| `prune_dominated_plans(plan_alternatives)` | List of alternative plans for same table subset | Filtered list of non-dominated plans | Remove clearly suboptimal alternatives to control search space | Comparable cost estimates for all alternatives |\n\nThe Join Optimizer interface encapsulates dynamic programming complexity while supporting different optimization strategies and search space management policies.\n\n#### Physical Planner Interface Details\n\nThe Physical Planner interface defines methods for concrete operator selection and plan finalization:\n\n| Method Signature | Parameters | Returns | Purpose | Selection Criteria |\n|------------------|------------|---------|---------|-------------------|\n| `selectPhysicalOperators(logical_plan, stats)` | Logical `ExecutionPlan` and `TableStatistics` | Physical `ExecutionPlan` with concrete operators | Transform logical plan into executable physical plan | Available indexes, memory constraints, join algorithm implementations |\n| `compare_scan_methods(table_name, predicates, available_indexes)` | Table identifier, filter predicates, index metadata | `AccessMethodOption` with cost comparison | Choose between sequential scan and index access | Index selectivity thresholds, index maintenance costs |\n| `select_join_algorithm(join_node, left_stats, right_stats)` | `JOIN` operator and input statistics | Physical join operator (hash/nested loop/merge) | Choose join implementation based on input characteristics | Memory availability, input sort orders, join selectivity |\n| `apply_optimization_rules(physical_plan)` | Physical `ExecutionPlan` | Optimized physical plan with rule transformations | Apply predicate pushdown and other rule-based improvements | Operator compatibility, semantic preservation constraints |\n\n#### Data Structure Exchange Formats\n\nComponents exchange information through standardized data structures that maintain optimization state and enable incremental processing:\n\n| Data Structure | Producer Component | Consumer Component | Key Information | Lifecycle |\n|----------------|-------------------|-------------------|-----------------|-----------|\n| `ParsedQuery` | SQL Parser (external) | Plan Builder | Tables, joins, filters, projections | Single optimization session |\n| Logical `ExecutionPlan` | Plan Builder | Cost Estimator, Join Optimizer | Operator tree without costs or physical details | Modified by multiple optimization phases |\n| Cost-annotated `ExecutionPlan` | Cost Estimator | Join Optimizer | Logical operators with resource estimates | Temporary during join optimization |\n| Optimized logical `ExecutionPlan` | Join Optimizer | Physical Planner | Optimal join order with cost estimates | Input to physical planning |\n| Physical `ExecutionPlan` | Physical Planner | Query Executor (external) | Executable operators with concrete implementations | Cached for query plan reuse |\n| `TableStatistics` | Statistics Collector | Cost Estimator, Physical Planner | Row counts, column distributions, index metadata | Persistent across multiple queries |\n\n> **Design Principle**: Interface contracts use immutable data structures where possible to prevent accidental modification by downstream components. When mutation is necessary (such as cost annotation), components create modified copies rather than updating original structures, enabling rollback and alternative exploration.\n\n![Cost Estimation Interaction Sequence](./diagrams/cost-estimation-sequence.svg)\n\n#### Error Handling and Component Communication\n\nComponent interfaces define comprehensive error handling strategies that enable graceful degradation when optimization assumptions fail:\n\n| Failure Mode | Detection Method | Recovery Strategy | Fallback Behavior |\n|--------------|------------------|-------------------|-------------------|\n| Missing Statistics | `TableStatistics` returns None | Use default estimates based on table size heuristics | Conservative selectivity estimates, prefer sequential scans |\n| Join Graph Disconnection | `extract_table_dependencies` identifies isolated table groups | Generate cross product warnings, apply heuristic join ordering | Create nested loop joins with cost penalties |\n| Optimization Timeout | Timer expiration during dynamic programming | Return best plan found so far, skip remaining subsets | Use greedy join ordering for remaining tables |\n| Memory Constraint Violation | Physical operator memory estimates exceed available resources | Select disk-based algorithms, increase spill thresholds | Prefer nested loop joins over hash joins |\n| Index Unavailability | Physical planner cannot access expected indexes | Fall back to sequential scans with updated cost estimates | Recompute plan costs with sequential access methods |\n\n### Plan Caching and Reuse\n\nQuery optimization represents a significant computational investment that can be amortized across multiple query executions through intelligent plan caching. Effective plan reuse requires sophisticated matching algorithms, cache invalidation strategies, and memory management policies.\n\n**Mental Model: Architectural Blueprint Library**\n\nThink of plan caching like an architectural firm's blueprint library. When designing a new building, architects first check if they have existing plans for similar structures that can be adapted rather than starting from scratch. The library includes indexing by building type, size, and features, with careful tracking of which blueprints are outdated due to building code changes. Similarly, query plan caching maintains a library of optimized plans indexed by query characteristics, with invalidation tracking based on data and schema changes.\n\n#### Plan Identification and Matching\n\nEffective plan caching requires sophisticated algorithms for identifying when cached plans can be reused for new queries. The challenge lies in recognizing query equivalence despite syntactic variations while avoiding false matches that could produce incorrect results.\n\nThe plan identification process operates through multiple matching levels:\n\n| Matching Level | Comparison Criteria | Equivalence Examples | Performance Impact |\n|---------------|-------------------|-------------------|-------------------|\n| Syntactic Matching | Exact SQL text comparison after normalization | Identical queries with consistent formatting | O(1) hash lookup |\n| Structural Matching | `ParsedQuery` component comparison | Queries with different aliases but same structure | O(n) where n is query complexity |\n| Semantic Matching | Logical plan tree comparison | Queries with reordered predicates but equivalent meaning | O(n²) plan tree comparison |\n| Parametric Matching | Template-based matching with parameter substitution | Queries differing only in literal values | O(k) where k is parameter count |\n\n#### Syntactic Plan Matching\n\nThe most efficient caching approach uses normalized SQL text as the cache key, providing constant-time lookups for exact query matches:\n\n1. **Query Normalization**: Transform SQL text by removing extra whitespace, standardizing keyword capitalization, and ordering predicates deterministically to eliminate formatting variations.\n\n2. **Hash Key Generation**: Compute a stable hash of the normalized query text using SHA-256 or similar algorithm to create a compact cache key that avoids string comparison overhead.\n\n3. **Direct Cache Lookup**: Use the hash key for direct cache access, returning the cached `ExecutionPlan` if available or proceeding to full optimization if no match exists.\n\n4. **Cache Entry Validation**: Verify that cached plans remain valid by checking timestamps against statistics updates and schema modifications that could invalidate optimization assumptions.\n\nSyntactic matching provides excellent performance for repeated identical queries but misses opportunities for reuse when queries are semantically equivalent but syntactically different.\n\n#### Structural Plan Matching\n\nMore sophisticated caching examines the parsed query structure to identify reusable plans despite syntactic variations:\n\nThe structural matching algorithm compares `ParsedQuery` components:\n\n1. **Table Set Comparison**: Verify that both queries reference the same base tables, ignoring alias differences but preserving table roles in join relationships.\n\n2. **Join Pattern Matching**: Compare join predicates for structural equivalence, recognizing that `A.id = B.id` and `B.id = A.id` represent identical join conditions.\n\n3. **Filter Compatibility**: Analyze filter predicates to determine if they can be satisfied by the same plan, accounting for predicate subsumption and logical equivalence.\n\n4. **Projection Compatibility**: Ensure that cached plan output schema includes all columns required by the new query, allowing plans with additional columns to satisfy queries requesting subsets.\n\n| Query Structure Component | Matching Algorithm | Complexity | Cache Hit Benefit |\n|--------------------------|-------------------|------------|-------------------|\n| Table references | Set intersection and alias normalization | O(t) where t is table count | Reuse join order optimization |\n| Join predicates | Predicate normalization and comparison | O(j²) where j is join count | Reuse cardinality estimates |\n| Filter predicates | Selectivity compatibility analysis | O(f) where f is filter count | Reuse selectivity calculations |\n| Output projections | Schema subsumption checking | O(c) where c is column count | Reuse entire plan if compatible |\n\n#### Parametric Plan Templates\n\nThe most sophisticated caching approach creates parameterized plan templates that can be instantiated with different literal values:\n\n1. **Parameter Extraction**: Identify literal constants in SQL queries that can be treated as parameters, replacing them with placeholder markers in the plan template.\n\n2. **Template Generation**: Create a generic `ExecutionPlan` template with parameter slots that can be filled with actual values at execution time.\n\n3. **Selectivity Parameterization**: Store selectivity estimation functions rather than fixed selectivity values, enabling recalculation based on actual parameter values.\n\n4. **Template Instantiation**: When reusing a template, substitute actual parameter values and recalculate selectivities that depend on filter constants.\n\n| Template Component | Parameterization Strategy | Example | Recalculation Required |\n|-------------------|-------------------------|---------|----------------------|\n| Filter constants | Replace with parameter placeholders | `WHERE age > ?` instead of `WHERE age > 25` | Yes - selectivity depends on threshold value |\n| Join constants | Template-based predicate matching | `JOIN orders ON customer_id = ?` | Rarely - join algorithms usually independent of constants |\n| Limit values | Parameter substitution | `LIMIT ?` instead of `LIMIT 100` | No - algorithm choice unaffected |\n| Sort specifications | Template matching with column parameters | `ORDER BY ? ASC` | Sometimes - index selection may depend on sort column |\n\n#### Cache Invalidation and Consistency\n\nCached query plans become invalid when underlying data characteristics or schema definitions change. Effective cache management requires sophisticated invalidation policies that balance plan freshness with optimization overhead.\n\nThe cache invalidation system monitors several categories of changes:\n\n| Change Type | Detection Method | Invalidation Scope | Recovery Strategy |\n|-------------|------------------|-------------------|-------------------|\n| Table Statistics Updates | Statistics timestamp comparison | Plans using affected tables | Recompute costs for cached plans using updated statistics |\n| Schema Modifications | DDL operation tracking | Plans referencing modified tables/indexes | Remove affected plans from cache, trigger reoptimization |\n| Index Creation/Removal | Index metadata change detection | Plans with affected access method choices | Rerun physical planning with updated index availability |\n| Data Volume Changes | Row count threshold monitoring | Plans with cardinality-sensitive operator choices | Refresh statistics and revalidate plan costs |\n\n#### Cache Storage and Memory Management\n\nPlan caching requires careful memory management to prevent cache size from impacting system performance while maximizing plan reuse benefits:\n\nThe cache storage system implements several key policies:\n\n1. **Size-Based Eviction**: Implement LRU (Least Recently Used) eviction when cache size exceeds configured memory limits, prioritizing recently accessed plans for retention.\n\n2. **Cost-Based Prioritization**: Weight eviction decisions by plan optimization cost, retaining expensive-to-optimize plans longer than simple single-table queries.\n\n3. **Access Frequency Tracking**: Maintain hit count statistics for cached plans, giving higher retention priority to frequently reused plans.\n\n4. **Memory Footprint Estimation**: Calculate memory consumption for each cached plan including operator tree structure and statistics references.\n\n| Cache Management Strategy | Implementation | Memory Impact | Performance Benefit |\n|--------------------------|---------------|---------------|-------------------|\n| Fixed-size LRU cache | Hash table + doubly-linked list | O(n) where n is cache capacity | Predictable memory usage, good hit rates |\n| Cost-weighted eviction | Priority queue by optimization cost | O(n log n) for eviction decisions | Retains high-value plans longer |\n| Hierarchical caching | Separate caches by query complexity | O(c × n) where c is complexity levels | Better hit rates for common query patterns |\n| Compressed plan storage | Serialize plans with compression | Reduced memory but higher CPU overhead | Enables larger effective cache capacity |\n\n> **Performance Insight**: Plan caching effectiveness depends heavily on query workload patterns. OLTP workloads with repetitive queries achieve 80-95% cache hit rates, while analytical workloads with diverse queries may see 20-40% hit rates. Cache sizing should account for workload characteristics and available memory resources.\n\n![Optimizer State Transitions](./diagrams/optimizer-state-machine.svg)\n\n#### Plan Cache Implementation Architecture\n\nThe plan cache implementation provides thread-safe access for concurrent query optimization while maintaining cache consistency:\n\n| Cache Component | Responsibility | Concurrency Model | Persistence Strategy |\n|----------------|---------------|-------------------|-------------------|\n| Cache Manager | Overall cache policy and eviction | Read-write locks for cache operations | Optional disk-based persistence for expensive plans |\n| Plan Serializer | Convert plans to/from storage format | Lock-free serialization with immutable plans | Compact binary format with schema versioning |\n| Invalidation Monitor | Track statistics and schema changes | Event-driven invalidation with async processing | Transaction log-based change detection |\n| Hit Rate Analyzer | Cache performance monitoring and tuning | Lock-free statistics collection | Periodic analysis with adaptive cache sizing |\n\n⚠️ **Pitfall: Cache Invalidation Lag**\nA common mistake is failing to account for the delay between data changes and statistics updates. If statistics are updated asynchronously (e.g., overnight batch processes), cached plans may use stale information for extended periods. Implement timestamp-based validation that compares plan cache time against both statistics update time and reasonable staleness thresholds. For critical applications, consider forcing statistics updates or plan recomputation when data modification volume exceeds thresholds.\n\n⚠️ **Pitfall: Parameter Value Skew**\nWhen using parametric plan templates, parameter values may have dramatically different selectivities that invalidate cached optimization decisions. For example, a template optimized for `WHERE status = 'ACTIVE'` (high selectivity) may perform poorly when instantiated with `WHERE status = 'PENDING'` (low selectivity). Implement selectivity bucketing where templates are cached separately for different selectivity ranges, or include dynamic plan switching based on parameter value analysis.\n\n### Implementation Guidance\n\nThis section provides concrete implementation patterns for building the optimization pipeline with robust component communication and effective plan caching.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Plan Storage | In-memory Python dictionaries with pickle serialization | Redis with custom serialization protocol |\n| Cache Invalidation | File modification time checking | Database triggers with event queuing |\n| Cost Calculation | Simple floating-point arithmetic | Decimal types for numerical stability |\n| Statistics Storage | JSON files with periodic updates | Database tables with incremental updates |\n| Plan Templates | String-based parameter substitution | Abstract syntax tree transformation |\n\n#### File Structure for Optimization Pipeline\n\n```\noptimizer/\n├── core/\n│   ├── __init__.py\n│   ├── query_optimizer.py        ← Main optimization pipeline\n│   ├── execution_plan.py         ← ExecutionPlan and OperatorNode classes\n│   └── cost_model.py            ← CostEstimate and cost calculation utilities\n├── components/\n│   ├── __init__.py\n│   ├── plan_builder.py          ← Logical plan construction\n│   ├── cost_estimator.py        ← Statistical cost modeling\n│   ├── join_optimizer.py        ← Dynamic programming join ordering\n│   └── physical_planner.py      ← Physical operator selection\n├── caching/\n│   ├── __init__.py\n│   ├── plan_cache.py            ← Plan storage and retrieval\n│   ├── cache_manager.py         ← Eviction and invalidation policies\n│   └── plan_serializer.py      ← Plan persistence and deserialization\n├── interfaces/\n│   ├── __init__.py\n│   ├── optimizer_interfaces.py   ← Component interface definitions\n│   └── statistics_provider.py   ← Statistics collection interface\n└── utils/\n    ├── __init__.py\n    ├── query_hash.py            ← Query normalization and hashing\n    └── plan_validator.py       ← Plan correctness checking\n```\n\n#### Infrastructure Starter Code\n\nComplete plan cache implementation for immediate use:\n\n```python\n# optimizer/caching/plan_cache.py\nimport hashlib\nimport pickle\nimport time\nfrom collections import OrderedDict\nfrom typing import Dict, Optional, List, Tuple\nfrom threading import RLock\nfrom datetime import datetime, timedelta\n\nfrom ..core.execution_plan import ExecutionPlan\nfrom ..core.cost_model import CostEstimate\n\nclass PlanCacheEntry:\n    \"\"\"Represents a cached execution plan with metadata.\"\"\"\n    \n    def __init__(self, plan: ExecutionPlan, optimization_cost: float):\n        self.plan = plan\n        self.optimization_cost = optimization_cost\n        self.creation_time = datetime.now()\n        self.access_count = 0\n        self.last_access_time = datetime.now()\n        self.memory_footprint = self._estimate_memory_usage(plan)\n    \n    def _estimate_memory_usage(self, plan: ExecutionPlan) -> int:\n        \"\"\"Estimate memory footprint of cached plan.\"\"\"\n        try:\n            return len(pickle.dumps(plan, protocol=pickle.HIGHEST_PROTOCOL))\n        except:\n            return 1024  # Conservative default estimate\n    \n    def update_access_stats(self):\n        \"\"\"Update access tracking for cache eviction decisions.\"\"\"\n        self.access_count += 1\n        self.last_access_time = datetime.now()\n\nclass PlanCache:\n    \"\"\"Thread-safe LRU cache for optimized execution plans.\"\"\"\n    \n    def __init__(self, max_size: int = 1000, max_memory_mb: int = 100):\n        self.max_size = max_size\n        self.max_memory_bytes = max_memory_mb * 1024 * 1024\n        self._cache: OrderedDict[str, PlanCacheEntry] = OrderedDict()\n        self._lock = RLock()\n        self._current_memory_usage = 0\n        self._hit_count = 0\n        self._miss_count = 0\n    \n    def get_plan(self, query_hash: str) -> Optional[ExecutionPlan]:\n        \"\"\"Retrieve cached plan if available.\"\"\"\n        with self._lock:\n            if query_hash in self._cache:\n                entry = self._cache[query_hash]\n                entry.update_access_stats()\n                # Move to end (most recently used)\n                self._cache.move_to_end(query_hash)\n                self._hit_count += 1\n                return entry.plan\n            else:\n                self._miss_count += 1\n                return None\n    \n    def store_plan(self, query_hash: str, plan: ExecutionPlan, \n                   optimization_cost: float):\n        \"\"\"Store optimized plan in cache.\"\"\"\n        with self._lock:\n            entry = PlanCacheEntry(plan, optimization_cost)\n            \n            # Check if we need to evict entries\n            while (len(self._cache) >= self.max_size or \n                   self._current_memory_usage + entry.memory_footprint > self.max_memory_bytes):\n                self._evict_lru_entry()\n            \n            self._cache[query_hash] = entry\n            self._current_memory_usage += entry.memory_footprint\n    \n    def _evict_lru_entry(self):\n        \"\"\"Remove least recently used entry from cache.\"\"\"\n        if not self._cache:\n            return\n        \n        # OrderedDict maintains insertion order; first item is least recently used\n        lru_hash, lru_entry = self._cache.popitem(last=False)\n        self._current_memory_usage -= lru_entry.memory_footprint\n    \n    def invalidate_plans_for_table(self, table_name: str):\n        \"\"\"Remove cached plans that reference a specific table.\"\"\"\n        with self._lock:\n            invalid_hashes = []\n            for query_hash, entry in self._cache.items():\n                if table_name in entry.plan.get_all_tables():\n                    invalid_hashes.append(query_hash)\n            \n            for hash_key in invalid_hashes:\n                entry = self._cache.pop(hash_key)\n                self._current_memory_usage -= entry.memory_footprint\n    \n    def get_cache_stats(self) -> Dict:\n        \"\"\"Return cache performance statistics.\"\"\"\n        with self._lock:\n            total_requests = self._hit_count + self._miss_count\n            hit_rate = self._hit_count / total_requests if total_requests > 0 else 0.0\n            \n            return {\n                'size': len(self._cache),\n                'max_size': self.max_size,\n                'memory_usage_mb': self._current_memory_usage / (1024 * 1024),\n                'max_memory_mb': self.max_memory_bytes / (1024 * 1024),\n                'hit_count': self._hit_count,\n                'miss_count': self._miss_count,\n                'hit_rate': hit_rate\n            }\n\n# optimizer/utils/query_hash.py\nimport hashlib\nimport re\nfrom typing import Dict, Any\n\ndef normalize_sql_text(sql: str) -> str:\n    \"\"\"Normalize SQL text for consistent caching.\"\"\"\n    # Convert to lowercase and remove extra whitespace\n    normalized = re.sub(r'\\s+', ' ', sql.strip().lower())\n    \n    # Sort WHERE conditions for deterministic ordering\n    # This is a simplified approach - production systems need more sophisticated parsing\n    if 'where' in normalized:\n        parts = normalized.split('where', 1)\n        if len(parts) == 2:\n            where_clause = parts[1].split('order by')[0].split('group by')[0]\n            conditions = [cond.strip() for cond in where_clause.split('and')]\n            conditions.sort()\n            sorted_where = ' and '.join(conditions)\n            normalized = parts[0] + 'where ' + sorted_where\n            if 'order by' in parts[1]:\n                normalized += ' order by' + parts[1].split('order by', 1)[1]\n    \n    return normalized\n\ndef compute_query_hash(sql: str) -> str:\n    \"\"\"Generate stable hash for SQL query caching.\"\"\"\n    normalized = normalize_sql_text(sql)\n    return hashlib.sha256(normalized.encode('utf-8')).hexdigest()\n\ndef extract_query_parameters(sql: str) -> Tuple[str, List[Any]]:\n    \"\"\"Extract parameterizable literals from SQL query.\"\"\"\n    # Simple parameter extraction - production needs full SQL parsing\n    import re\n    \n    # Find numeric literals\n    numeric_pattern = r'\\b\\d+(?:\\.\\d+)?\\b'\n    string_pattern = r\"'([^']*?)'\"\n    \n    parameters = []\n    parameterized_sql = sql\n    \n    # Replace numeric literals with placeholders\n    for match in re.finditer(numeric_pattern, sql):\n        value = match.group(0)\n        if value.replace('.', '').isdigit():\n            parameters.append(float(value) if '.' in value else int(value))\n            parameterized_sql = parameterized_sql.replace(value, '?', 1)\n    \n    # Replace string literals with placeholders\n    for match in re.finditer(string_pattern, sql):\n        parameters.append(match.group(1))\n        parameterized_sql = parameterized_sql.replace(match.group(0), '?', 1)\n    \n    return normalize_sql_text(parameterized_sql), parameters\n```\n\n#### Core Pipeline Implementation Skeleton\n\n```python\n# optimizer/core/query_optimizer.py\nfrom typing import Optional, Dict, Any\nfrom datetime import datetime\n\nfrom .execution_plan import ExecutionPlan, ParsedQuery\nfrom ..components.plan_builder import PlanBuilder\nfrom ..components.cost_estimator import CostEstimator\nfrom ..components.join_optimizer import JoinOptimizer\nfrom ..components.physical_planner import PhysicalPlanner\nfrom ..caching.plan_cache import PlanCache\nfrom ..utils.query_hash import compute_query_hash\n\nclass QueryOptimizer:\n    \"\"\"Main query optimization pipeline coordinating all components.\"\"\"\n    \n    def __init__(self, statistics_provider, plan_cache_size: int = 1000):\n        self.plan_builder = PlanBuilder()\n        self.cost_estimator = CostEstimator(statistics_provider)\n        self.join_optimizer = JoinOptimizer()\n        self.physical_planner = PhysicalPlanner()\n        self.plan_cache = PlanCache(max_size=plan_cache_size)\n        self.statistics_provider = statistics_provider\n        \n        # Performance monitoring\n        self.optimization_count = 0\n        self.total_optimization_time = 0.0\n    \n    def optimize_query(self, parsed_query: ParsedQuery, \n                      original_sql: str = None) -> ExecutionPlan:\n        \"\"\"Main optimization entry point implementing complete pipeline.\"\"\"\n        start_time = datetime.now()\n        \n        # TODO 1: Generate cache key from SQL text if provided\n        cache_key = None\n        if original_sql:\n            cache_key = compute_query_hash(original_sql)\n            cached_plan = self.plan_cache.get_plan(cache_key)\n            if cached_plan:\n                return cached_plan\n        \n        # TODO 2: Build initial logical plan from parsed query\n        logical_plan = self.plan_builder.build_logical_plan(parsed_query)\n        \n        # TODO 3: Collect statistics for all referenced tables\n        for table_name in parsed_query.tables:\n            stats = self.statistics_provider.get_table_stats(table_name)\n            if not stats:\n                # TODO: Handle missing statistics - collect or use defaults\n                pass\n        \n        # TODO 4: Annotate logical plan with cost estimates\n        cost_annotated_plan = self.cost_estimator.estimateCost(logical_plan)\n        \n        # TODO 5: Optimize join ordering if multi-table query\n        optimized_plan = cost_annotated_plan\n        if len(parsed_query.tables) > 1:\n            optimized_plan = self.join_optimizer.optimizeJoinOrder(\n                cost_annotated_plan, parsed_query.tables)\n        \n        # TODO 6: Select physical operators and access methods\n        physical_plan = self.physical_planner.selectPhysicalOperators(\n            optimized_plan, self.statistics_provider)\n        \n        # TODO 7: Cache the optimized plan if caching enabled\n        if cache_key:\n            optimization_time = (datetime.now() - start_time).total_seconds()\n            self.plan_cache.store_plan(cache_key, physical_plan, optimization_time)\n        \n        # Update performance statistics\n        self._update_optimization_stats(start_time)\n        \n        return physical_plan\n    \n    def _update_optimization_stats(self, start_time: datetime):\n        \"\"\"Update internal performance monitoring statistics.\"\"\"\n        # TODO: Track optimization time and count for performance analysis\n        optimization_time = (datetime.now() - start_time).total_seconds()\n        self.optimization_count += 1\n        self.total_optimization_time += optimization_time\n    \n    def get_optimizer_stats(self) -> Dict[str, Any]:\n        \"\"\"Return optimization performance statistics.\"\"\"\n        # TODO: Return comprehensive statistics including cache hit rates\n        avg_time = (self.total_optimization_time / self.optimization_count \n                   if self.optimization_count > 0 else 0.0)\n        \n        stats = {\n            'total_optimizations': self.optimization_count,\n            'average_optimization_time_ms': avg_time * 1000,\n            'cache_stats': self.plan_cache.get_cache_stats()\n        }\n        return stats\n    \n    def invalidate_plans_for_table(self, table_name: str):\n        \"\"\"Invalidate cached plans when table statistics change.\"\"\"\n        # TODO: Remove plans from cache and optionally recompute statistics\n        self.plan_cache.invalidate_plans_for_table(table_name)\n```\n\n#### Milestone Checkpoints\n\n**Checkpoint 1: Basic Pipeline Integration**\n```python\n# Test basic optimization pipeline without caching\noptimizer = QueryOptimizer(statistics_provider)\nparsed_query = ParsedQuery(\n    tables=['customers', 'orders'],\n    columns=['customer_name', 'order_total'],\n    joins=[('customers.id', 'orders.customer_id')],\n    filters=[('orders.status', '=', 'SHIPPED')]\n)\nplan = optimizer.optimize_query(parsed_query)\nassert plan is not None\nassert len(plan.get_all_tables()) == 2\nprint(\"✓ Basic optimization pipeline working\")\n```\n\n**Checkpoint 2: Plan Caching Functionality**\n```python\n# Test plan caching and reuse\nsql_query = \"SELECT customer_name, order_total FROM customers JOIN orders ON customers.id = orders.customer_id WHERE orders.status = 'SHIPPED'\"\nplan1 = optimizer.optimize_query(parsed_query, sql_query)\nplan2 = optimizer.optimize_query(parsed_query, sql_query)\n\ncache_stats = optimizer.get_optimizer_stats()['cache_stats']\nassert cache_stats['hit_count'] >= 1\nprint(f\"✓ Plan caching working - Hit rate: {cache_stats['hit_rate']:.2%}\")\n```\n\n**Checkpoint 3: Cache Invalidation**\n```python\n# Test cache invalidation when statistics change\noptimizer.invalidate_plans_for_table('orders')\ncache_stats_after = optimizer.get_optimizer_stats()['cache_stats']\nprint(\"✓ Cache invalidation working\")\n```\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| Plans not being cached | Cache key generation failure or storage errors | Check if `compute_query_hash()` returns consistent values for identical queries | Verify SQL normalization logic handles all query variations |\n| Stale cached plans | Cache invalidation not triggered by statistics updates | Monitor statistics update timestamps vs plan cache timestamps | Implement proactive invalidation based on data modification volume |\n| High cache miss rate | Query variations not recognized as equivalent | Analyze cache keys for similar queries to identify normalization gaps | Improve SQL normalization or implement structural matching |\n| Memory usage growth | Cache not evicting entries properly | Monitor cache size and memory usage statistics | Verify LRU eviction logic and memory footprint calculations |\n| Optimization timeout | Join enumeration taking too long despite caching | Profile optimization time by component to identify bottlenecks | Implement time-based early termination in dynamic programming |\n\n\n## Error Handling and Edge Cases\n\n> **Milestone(s):** All Milestones 1-4 - addresses failure modes and degenerate cases that can occur throughout the optimization pipeline, from plan representation through cost estimation, join ordering, and physical planning.\n\nQuery optimizers must gracefully handle a wide range of failure scenarios and edge cases that can occur during the optimization process. Unlike simple applications where invalid input can be rejected, query optimizers must make reasonable decisions even when faced with incomplete information, missing statistics, or pathological query patterns. The optimizer serves as a critical component in the database system, and optimization failures can cascade into complete query execution failures, making robust error handling essential.\n\nThink of error handling in query optimization like emergency procedures for air traffic control. Even when unexpected conditions arise—missing weather data, aircraft equipment failures, or communication breakdowns—the system must continue operating safely and make the best decisions possible with available information. Similarly, a query optimizer must have fallback strategies for every component failure mode, ensuring that users always receive an executable plan, even if it's not perfectly optimized.\n\nThe challenge lies in distinguishing between recoverable optimization problems (where fallback strategies can produce reasonable plans) and fundamental query errors (where optimization should fail fast to prevent runtime errors). This section examines the systematic approach to handling these failure modes across all components of the optimization pipeline.\n\n### Optimization Failure Modes\n\nQuery optimization can fail in numerous ways, ranging from transient resource constraints to fundamental algorithmic limitations. Understanding these failure modes and implementing appropriate fallback strategies ensures that the optimizer remains robust under production conditions.\n\n#### Resource Exhaustion Scenarios\n\nThe most common optimization failures stem from resource constraints that prevent the optimizer from completing its work within acceptable bounds. These failures typically manifest during computationally intensive phases like join ordering or plan enumeration.\n\n**Memory Exhaustion During Join Ordering** occurs when the dynamic programming algorithm attempts to enumerate all possible join orders for queries with many tables. The number of possible join orders grows exponentially with table count (n! for n tables), and the memoization tables required by dynamic programming can quickly exhaust available memory.\n\n| Failure Condition | Detection Method | Fallback Strategy | Quality Impact |\n|-------------------|------------------|-------------------|----------------|\n| DP table size exceeds threshold | Monitor memory allocation during enumeration | Switch to greedy heuristic ordering | Moderate - may miss optimal order |\n| Join enumeration timeout | Track elapsed optimization time | Use left-deep join trees only | Low - restricts plan shape |\n| Statistics collection overflow | Monitor statistics storage size | Sample statistics more aggressively | Low - reduced estimation accuracy |\n| Plan cache memory pressure | Monitor cache size and hit rates | Evict LRU plans and reduce cache size | Minimal - affects reuse only |\n\nThe optimizer detects memory pressure by monitoring allocation sizes during critical phases. When the estimated memory requirement for completing dynamic programming exceeds `MAX_JOIN_ENUMERATION`, the system automatically switches to heuristic-based join ordering that requires O(n²) space instead of O(2^n).\n\n**Optimization Timeout Handling** addresses scenarios where query complexity prevents optimization completion within the allocated time budget. Complex queries with many tables, predicates, and potential access paths can require exponential search time without proper pruning.\n\n```python\n# Timeout detection during optimization phases\nif elapsed_time > OPTIMIZATION_TIMEOUT:\n    # Fall back to rule-based optimization\n    return apply_heuristic_optimization(parsed_query)\n```\n\nThe system implements cascading timeout handling where each optimization phase has progressively shorter time limits. If join ordering times out, the system falls back to left-deep trees. If cost-based physical selection times out, it applies rule-based operator selection. This ensures that optimization always completes with some executable plan.\n\n#### Statistical Accuracy Failures\n\nCost-based optimization depends critically on accurate statistics, but various conditions can render statistics unreliable or unavailable. The optimizer must detect these conditions and adapt its decision-making accordingly.\n\n**Statistics Staleness Detection** identifies when cached statistics no longer reflect the current state of underlying tables. Stale statistics can lead to dramatically incorrect cost estimates, causing the optimizer to select highly inefficient plans.\n\n| Staleness Indicator | Detection Method | Confidence Adjustment | Fallback Strategy |\n|--------------------|-----------------|--------------------|-------------------|\n| Last update > 24 hours | Compare `TableStatistics.last_updated` with current time | Reduce confidence by 50% | Request background statistics refresh |\n| Row count mismatch | Compare cached count with actual table size | Set confidence to 10% | Use default selectivity estimates |\n| Missing column statistics | Check for null `ColumnStatistics` entries | Use uniform distribution assumption | Apply conservative selectivity factors |\n| Histogram corruption | Validate histogram bucket consistency | Disable histogram-based estimation | Use simple min/max range estimates |\n\nThe `CostEstimate.confidence_level` field tracks the reliability of cost calculations based on statistics quality. Plans with low confidence levels trigger additional validation and may be rejected in favor of simpler alternatives with higher confidence.\n\n**Missing Statistics Handling** addresses scenarios where tables lack statistical information entirely, often occurring after table creation or bulk data loading. The optimizer cannot simply fail when statistics are missing—it must make reasonable estimates based on available information.\n\n> **Decision: Default Statistics Strategy**\n> - **Context**: New tables or tables with outdated statistics require cost estimation without reliable data\n> - **Options Considered**: 1) Fail optimization, 2) Use hardcoded defaults, 3) Derive estimates from schema information\n> - **Decision**: Use schema-based estimation with conservative factors\n> - **Rationale**: Schema information (column types, constraints, indexes) provides better estimates than arbitrary defaults while avoiding optimization failures\n> - **Consequences**: Enables optimization of new tables but may produce suboptimal plans until real statistics are collected\n\n#### Component Integration Failures\n\nComplex interactions between optimizer components can lead to integration failures where individual components function correctly but their combination produces invalid results.\n\n**Plan Validation Failures** occur when the physical planning phase produces plans that appear valid to the optimizer but would fail during execution. These failures often stem from mismatches between logical and physical operator capabilities.\n\nCommon validation failures include:\n- Selecting index scans for columns without suitable indexes\n- Choosing hash joins for data types that cannot be hashed consistently  \n- Applying predicates to columns that don't exist in the operator's input schema\n- Creating plans that exceed available memory for hash tables or sort operations\n\nThe `_validate_physical_plan()` method performs comprehensive validation before returning optimized plans:\n\n| Validation Check | Failure Condition | Recovery Action | Prevention Method |\n|------------------|-------------------|------------------|-------------------|\n| Schema consistency | Output columns don't match expected schema | Rebuild plan with corrected projections | Validate schema propagation during planning |\n| Resource feasibility | Memory requirements exceed available resources | Switch to streaming algorithms | Check resource constraints during operator selection |\n| Index availability | Plan references non-existent indexes | Fall back to sequential scans | Verify index existence during access method selection |\n| Type compatibility | Join or filter predicates have type mismatches | Apply implicit type conversions or reject predicate | Validate predicate types during logical planning |\n\n### Degenerate Query Patterns\n\nCertain query patterns present fundamental challenges to cost-based optimization, either because they lack sufficient structure for meaningful cost comparison or because they represent inherently expensive operations that defy optimization.\n\n#### Cross Product Queries\n\nQueries without proper join predicates create cross products that produce extremely large intermediate results. These queries often represent user errors, but the optimizer must handle them gracefully rather than failing.\n\n**Cross Product Detection** identifies when join operations lack connecting predicates, indicating potential cartesian products between tables. The `JoinGraph` component analyzes predicate connectivity to detect disconnected table groups.\n\nConsider a query joining three tables without predicates:\n```sql\nSELECT * FROM orders, customers, products\n```\n\nThis query creates a cross product of all three tables. If each table contains 10,000 rows, the result would contain 10^12 rows—clearly unintentional and computationally infeasible.\n\nThe optimizer detects cross products by building a connectivity graph where tables are nodes and join predicates are edges. Disconnected components in this graph indicate cross products. When detected, the system applies several mitigation strategies:\n\n| Cross Product Type | Detection Method | Mitigation Strategy | User Feedback |\n|--------------------|------------------|-------------------|---------------|\n| Complete cross product | No join predicates between any tables | Add row limit and warning | \"Query may produce large result set\" |\n| Partial cross product | Some tables connected, others isolated | Optimize connected groups separately | \"Missing join conditions detected\" |\n| Implicit cross product | Predicates exist but don't connect all tables | Suggest additional predicates | \"Consider adding WHERE conditions\" |\n| Accidental cross product | Typos in table/column names | Fuzzy match column names | \"Did you mean 'customer_id'?\" |\n\n**Cross Product Cost Modeling** requires special handling because normal cardinality estimation produces astronomical numbers that overflow standard numeric types. The system uses logarithmic cost representation for cross products and applies aggressive penalties to discourage their selection.\n\n#### Extremely Selective Filters\n\nQueries with very low selectivity predicates (selecting less than 0.1% of rows) present challenges for cost estimation because small errors in selectivity calculation can dramatically affect join ordering decisions.\n\n**Ultra-Low Selectivity Handling** addresses predicates that select tiny fractions of large tables. These predicates often involve rare values, complex expressions, or multiple AND-connected conditions.\n\nConsider a predicate like:\n```sql\nWHERE customer_type = 'PREMIUM' AND last_purchase_date > '2023-12-01' \n  AND account_balance > 100000 AND region = 'ANTARCTICA'\n```\n\nEach individual condition might have reasonable selectivity, but their combination could select fewer than 10 rows from a million-row table. Standard independence assumptions used in selectivity estimation become highly unreliable at these scales.\n\n| Selectivity Range | Estimation Method | Confidence Adjustment | Special Handling |\n|------------------|-------------------|---------------------|------------------|\n| 1.0 - 0.1 | Standard histogram lookup | Normal confidence | Standard cost calculation |\n| 0.1 - 0.01 | Histogram with correlation adjustment | Reduce confidence by 25% | Prefer index scans |\n| 0.01 - 0.001 | Conservative minimum estimates | Reduce confidence by 50% | Force nested loop joins |\n| < 0.001 | Fixed minimum selectivity floor | Set confidence to 20% | Apply ultra-selective query optimizations |\n\n**Ultra-Selective Query Optimization** applies special rules when predicates have extremely low selectivity. These rules override normal cost-based decisions to account for the high uncertainty in cost estimates.\n\nUltra-selective queries benefit from specific optimization strategies:\n- Force index usage even when cost estimates suggest sequential scans\n- Prefer nested loop joins where the ultra-selective table drives the join\n- Apply the most selective predicates first to minimize intermediate result sizes\n- Use materialization to avoid repeatedly evaluating expensive predicates\n\n#### Queries with No Valid Indexes\n\nSome queries reference columns that lack appropriate indexes, forcing the optimizer to rely entirely on sequential scans. While not technically degenerate, these queries require special handling to avoid poor optimization decisions.\n\n**Index Absence Handling** addresses scenarios where optimal query execution would require indexes that don't exist. The optimizer must make realistic cost comparisons when all access methods involve full table scans.\n\n| Index Absence Scenario | Cost Implication | Optimization Strategy | Recommendation Generation |\n|------------------------|------------------|---------------------|--------------------------|\n| No indexes on any referenced columns | All scans are sequential | Optimize join order for sequential access | Suggest primary key indexes |\n| Missing composite indexes | Individual column indexes available | Use index intersection or union | Suggest composite index creation |\n| Wrong index column order | Index exists but doesn't match predicate order | Use partial index scans | Suggest reordered composite indexes |\n| Outdated index statistics | Indexes exist but statistics are stale | Prefer sequential scans until refresh | Trigger index statistics update |\n\nWhen no suitable indexes exist, the optimizer focuses on minimizing the amount of data that must be scanned sequentially. This often means reordering operations to apply the most selective filters first and choosing join orders that minimize intermediate result sizes.\n\n### Stale Statistics Handling\n\nDatabase statistics become outdated as data changes over time, leading to increasingly inaccurate cost estimates and suboptimal query plans. The optimizer must detect statistics staleness and adapt its decision-making to account for reduced estimation accuracy.\n\n#### Staleness Detection Strategies\n\n**Time-Based Staleness Detection** uses the `last_updated` timestamp in `TableStatistics` to identify potentially outdated information. However, time alone is insufficient—a table that hasn't changed in weeks may have perfectly valid statistics, while a rapidly changing table might need updates every few minutes.\n\nThe system implements multi-factor staleness detection:\n\n| Staleness Factor | Measurement Method | Threshold Values | Confidence Impact |\n|------------------|-------------------|------------------|-------------------|\n| Time since last update | Compare `last_updated` with current timestamp | 6 hours: minor, 24 hours: moderate, 7 days: severe | Reduce by 10%, 25%, 50% |\n| Estimated row changes | Track INSERT/DELETE/UPDATE operations since statistics collection | >5% changes: moderate, >20%: severe | Reduce by 20%, 60% |\n| Query pattern changes | Monitor new WHERE clauses and JOIN patterns | New predicates on unanalyzed columns | Reduce by 30% for affected predicates |\n| Schema modifications | Detect ALTER TABLE, CREATE INDEX operations | Any structural changes | Invalidate all statistics |\n\n**Data Modification Tracking** monitors the volume of changes since statistics collection to estimate how much the data distribution might have shifted. The system tracks approximate change volumes without the overhead of maintaining exact counts.\n\n```python\ndef estimate_statistics_staleness(table_stats: TableStatistics) -> float:\n    \"\"\"Calculate confidence reduction factor based on staleness indicators\"\"\"\n    time_factor = calculate_time_staleness(table_stats.last_updated)\n    change_factor = estimate_data_changes(table_stats.table_name) \n    schema_factor = check_schema_modifications(table_stats.table_name)\n    \n    # Combine factors (multiplicative decay)\n    confidence_multiplier = time_factor * change_factor * schema_factor\n    return max(0.1, confidence_multiplier)  # Never reduce below 10%\n```\n\n#### Statistics Refresh Strategies\n\n**Background Statistics Collection** addresses stale statistics by triggering automatic refresh operations when staleness exceeds acceptable thresholds. However, statistics collection is expensive and can impact system performance, requiring careful scheduling and prioritization.\n\nThe system implements tiered refresh strategies based on table importance and staleness severity:\n\n| Refresh Priority | Trigger Condition | Collection Method | Performance Impact |\n|------------------|-------------------|-------------------|-------------------|\n| Immediate | Critical tables with >50% estimated changes | Full statistics scan | High - blocks other operations |\n| High Priority | Frequently queried tables with >20% changes | Sampled statistics collection | Moderate - background processing |\n| Normal Priority | Time-based staleness > 24 hours | Incremental statistics update | Low - scheduled during maintenance windows |\n| Low Priority | Rarely queried tables | Defer until next maintenance cycle | Minimal - batched with other operations |\n\n**Adaptive Sampling** adjusts statistics collection intensity based on detected data patterns. Tables with stable distributions require less frequent updates, while rapidly changing tables need more aggressive sampling.\n\nThe `sample_rate` field in `TableStatistics` controls the fraction of rows examined during statistics collection. The system dynamically adjusts this rate:\n- Stable tables: reduce sample rate to 1-5% for faster collection\n- Volatile tables: increase sample rate to 10-25% for better accuracy  \n- New tables: use 100% sampling for initial statistics\n- Large tables: cap sample size at configurable row limits\n\n#### Confidence-Based Decision Making\n\n**Cost Estimation with Uncertainty** modifies the cost calculation process to account for reduced confidence in stale statistics. Rather than treating all cost estimates as equally reliable, the system adjusts its decision-making based on estimation confidence.\n\nThe `CostEstimate.confidence_level` field propagates uncertainty through the optimization process:\n\n| Confidence Level | Cost Adjustment Strategy | Plan Selection Impact | Fallback Behavior |\n|------------------|-------------------------|---------------------|-------------------|\n| 90-100% | Use costs as calculated | Normal cost-based selection | None - trust cost estimates |\n| 70-89% | Add 20% uncertainty margin | Prefer simpler plans when costs are close | None |\n| 50-69% | Add 50% uncertainty margin | Heavily favor proven plan patterns | Consider rule-based fallbacks |\n| 20-49% | Add 100% uncertainty margin | Avoid complex plans | Switch to heuristic optimization |\n| <20% | Use default cost assumptions | Ignore cost-based decisions | Full rule-based optimization |\n\nLow confidence levels trigger conservative optimization strategies that favor predictable performance over theoretical optimality. This prevents the optimizer from making risky decisions based on unreliable information.\n\n**Uncertainty Propagation** tracks how confidence levels affect cost calculations throughout the optimization pipeline. When combining cost estimates from multiple sources, the system applies confidence-weighted calculations rather than simple arithmetic.\n\n```python\ndef combine_uncertain_costs(cost1: CostEstimate, cost2: CostEstimate) -> CostEstimate:\n    \"\"\"Combine costs accounting for confidence levels\"\"\"\n    # Weight costs by confidence - lower confidence gets conservative adjustment\n    adjusted_cost1 = cost1.total_cost * (2.0 - cost1.confidence_level)\n    adjusted_cost2 = cost2.total_cost * (2.0 - cost2.confidence_level)\n    \n    combined_cost = adjusted_cost1 + adjusted_cost2\n    combined_confidence = min(cost1.confidence_level, cost2.confidence_level)\n    \n    return CostEstimate(\n        total_cost=combined_cost,\n        confidence_level=combined_confidence,\n        cost_factors={'uncertainty_adjusted': True}\n    )\n```\n\n#### Default Statistics Generation\n\n**Schema-Based Estimation** generates reasonable default statistics when no collected statistics are available. This approach analyzes table schemas, constraints, and index definitions to infer likely data characteristics.\n\nThe system derives default statistics from several schema sources:\n\n| Schema Information | Statistic Inference | Accuracy Level | Confidence Setting |\n|--------------------|-------------------|----------------|-------------------|\n| Column data types | Estimate average row width and distinct values | Moderate | 40-60% |\n| Primary key constraints | Assume 100% distinct values for key columns | High | 80-90% |\n| Foreign key constraints | Estimate join selectivity based on referential integrity | Moderate | 50-70% |\n| Check constraints | Derive range and distribution information | Low | 30-50% |\n| Index definitions | Assume indexes exist for reason (selective columns) | Moderate | 60-70% |\n| Table size on disk | Estimate row count from storage size and schema | Low | 20-40% |\n\n**Conservative Default Values** provide safe fallback estimates when schema analysis yields insufficient information. These defaults err on the side of caution, preferring predictable performance over aggressive optimization.\n\nDefault selectivity factors for unknown predicates:\n- Equality predicates: 1% selectivity (assumes reasonably selective values)\n- Range predicates: 10% selectivity (conservative range assumption)  \n- Pattern matching: 5% selectivity (assumes moderately selective patterns)\n- Complex expressions: 25% selectivity (very conservative for unknown expressions)\n\nThese defaults prevent the optimizer from making overly optimistic assumptions that could lead to dramatically poor performance when the estimates prove incorrect.\n\n### Implementation Guidance\n\nThe error handling and edge case management system requires robust infrastructure for detecting failures, implementing fallback strategies, and maintaining system reliability under adverse conditions.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|-----------------|\n| Statistics Validation | Simple timestamp checks with manual refresh triggers | Automatic staleness detection with background refresh scheduling |\n| Error Recovery | Exception handling with hardcoded fallback plans | Tiered fallback strategy with confidence-based decision making |\n| Cross Product Detection | Basic predicate counting with warnings | Full connectivity graph analysis with intelligent suggestions |\n| Resource Monitoring | Manual memory/time limit checks | Integrated profiling with adaptive resource management |\n| Logging and Debugging | Standard logging with error messages | Structured logging with optimization decision traces |\n\n#### Recommended File Structure\n\n```\noptimizer/\n  error_handling/\n    __init__.py\n    failure_detector.py          ← Detects optimization failure conditions\n    fallback_strategies.py       ← Implements fallback optimization approaches  \n    statistics_validator.py      ← Validates and refreshes stale statistics\n    degenerate_query_handler.py  ← Handles cross products and edge cases\n    confidence_tracker.py        ← Manages cost estimation confidence levels\n  \n  tests/\n    error_handling/\n      test_failure_detection.py   ← Tests for failure mode detection\n      test_fallback_strategies.py ← Tests for fallback optimization quality\n      test_statistics_staleness.py← Tests for staleness detection and refresh\n      test_degenerate_queries.py  ← Tests for edge case handling\n```\n\n#### Infrastructure Starter Code\n\n**Statistics Staleness Detector** - Complete implementation for detecting and managing stale statistics:\n\n```python\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Dict, List\nfrom dataclasses import dataclass\n\n@dataclass\nclass StalenessAssessment:\n    confidence_multiplier: float\n    needs_refresh: bool  \n    refresh_priority: str\n    staleness_factors: Dict[str, float]\n\nclass StatisticsStalenessDetector:\n    def __init__(self, config: dict):\n        self.time_threshold_hours = config.get('time_threshold_hours', 24)\n        self.change_threshold_percent = config.get('change_threshold_percent', 20)\n        self.critical_confidence_threshold = config.get('critical_threshold', 0.3)\n        \n    def assess_staleness(self, table_stats: TableStatistics, \n                        change_tracker: 'ChangeTracker') -> StalenessAssessment:\n        \"\"\"Assess statistics staleness and compute confidence adjustments\"\"\"\n        factors = {}\n        \n        # Time-based staleness\n        time_factor = self._calculate_time_staleness(table_stats.last_updated)\n        factors['time'] = time_factor\n        \n        # Data change staleness  \n        change_factor = self._calculate_change_staleness(\n            table_stats.table_name, table_stats.row_count, change_tracker\n        )\n        factors['changes'] = change_factor\n        \n        # Schema modification staleness\n        schema_factor = self._calculate_schema_staleness(table_stats.table_name)\n        factors['schema'] = schema_factor\n        \n        # Combine factors\n        confidence_multiplier = time_factor * change_factor * schema_factor\n        confidence_multiplier = max(0.1, confidence_multiplier)  # Floor at 10%\n        \n        # Determine refresh priority\n        needs_refresh = confidence_multiplier < self.critical_confidence_threshold\n        if confidence_multiplier < 0.2:\n            priority = 'immediate'\n        elif confidence_multiplier < 0.5:\n            priority = 'high'  \n        elif confidence_multiplier < 0.8:\n            priority = 'normal'\n        else:\n            priority = 'low'\n            \n        return StalenessAssessment(\n            confidence_multiplier=confidence_multiplier,\n            needs_refresh=needs_refresh,\n            refresh_priority=priority,\n            staleness_factors=factors\n        )\n    \n    def _calculate_time_staleness(self, last_updated: datetime) -> float:\n        \"\"\"Calculate confidence reduction based on time since update\"\"\"\n        hours_old = (datetime.now() - last_updated).total_seconds() / 3600\n        \n        if hours_old < 6:\n            return 1.0  # Full confidence\n        elif hours_old < 24:  \n            return 0.9  # Slight reduction\n        elif hours_old < 168:  # 1 week\n            return 0.75  # Moderate reduction\n        else:\n            return 0.5  # Significant reduction\n    \n    def _calculate_change_staleness(self, table_name: str, \n                                   baseline_rows: int, \n                                   change_tracker: 'ChangeTracker') -> float:\n        \"\"\"Calculate confidence reduction based on estimated data changes\"\"\"\n        estimated_changes = change_tracker.get_estimated_changes(table_name)\n        if estimated_changes == 0:\n            return 1.0\n            \n        change_percentage = estimated_changes / max(baseline_rows, 1)\n        \n        if change_percentage < 0.05:  # <5% changes\n            return 1.0\n        elif change_percentage < 0.20:  # <20% changes  \n            return 0.8\n        elif change_percentage < 0.50:  # <50% changes\n            return 0.4\n        else:\n            return 0.2  # Major changes\n    \n    def _calculate_schema_staleness(self, table_name: str) -> float:\n        \"\"\"Calculate confidence reduction based on schema modifications\"\"\"\n        # This would integrate with schema change tracking\n        # For now, assume no schema changes\n        return 1.0\n```\n\n**Degenerate Query Detector** - Complete implementation for detecting problematic query patterns:\n\n```python\nfrom typing import Set, List, Tuple, Dict\nfrom collections import defaultdict\n\nclass CrossProductDetector:\n    def __init__(self):\n        self.connectivity_graph = defaultdict(set)\n        self.isolated_tables = set()\n        \n    def analyze_query_connectivity(self, parsed_query: ParsedQuery) -> Dict[str, any]:\n        \"\"\"Analyze join connectivity and detect cross products\"\"\"\n        self._build_connectivity_graph(parsed_query)\n        \n        connected_components = self._find_connected_components()\n        cross_products = self._identify_cross_products(connected_components)\n        \n        return {\n            'has_cross_products': len(connected_components) > 1,\n            'connected_components': connected_components,\n            'cross_product_pairs': cross_products,\n            'isolated_tables': self.isolated_tables,\n            'connectivity_suggestions': self._generate_suggestions(parsed_query)\n        }\n    \n    def _build_connectivity_graph(self, parsed_query: ParsedQuery):\n        \"\"\"Build graph of table connectivity based on join predicates\"\"\"\n        self.connectivity_graph.clear()\n        self.isolated_tables = set(parsed_query.tables)\n        \n        # Add edges for each join predicate\n        for join_condition in parsed_query.joins:\n            left_table, left_col, right_table, right_col = join_condition\n            \n            self.connectivity_graph[left_table].add(right_table)  \n            self.connectivity_graph[right_table].add(left_table)\n            \n            # Remove from isolated set\n            self.isolated_tables.discard(left_table)\n            self.isolated_tables.discard(right_table)\n    \n    def _find_connected_components(self) -> List[Set[str]]:\n        \"\"\"Find connected components using DFS\"\"\"\n        visited = set()\n        components = []\n        \n        all_tables = set(self.connectivity_graph.keys()) | self.isolated_tables\n        \n        for table in all_tables:\n            if table not in visited:\n                component = set()\n                self._dfs(table, visited, component)\n                components.append(component)\n                \n        return components\n    \n    def _dfs(self, table: str, visited: Set[str], component: Set[str]):\n        \"\"\"Depth-first search for connected component\"\"\"\n        visited.add(table)\n        component.add(table)\n        \n        for neighbor in self.connectivity_graph[table]:\n            if neighbor not in visited:\n                self._dfs(neighbor, visited, component)\n    \n    def _identify_cross_products(self, components: List[Set[str]]) -> List[Tuple[Set[str], Set[str]]]:\n        \"\"\"Identify pairs of components that form cross products\"\"\"\n        cross_products = []\n        \n        for i in range(len(components)):\n            for j in range(i + 1, len(components)):\n                cross_products.append((components[i], components[j]))\n                \n        return cross_products\n    \n    def _generate_suggestions(self, parsed_query: ParsedQuery) -> List[str]:\n        \"\"\"Generate suggestions for fixing cross products\"\"\"\n        suggestions = []\n        \n        if self.isolated_tables:\n            suggestions.append(f\"Consider adding WHERE conditions for isolated tables: {', '.join(self.isolated_tables)}\")\n            \n        # Analyze column names for potential join candidates\n        all_columns = set()\n        for table_name in parsed_query.tables:\n            # This would query schema information\n            table_columns = self._get_table_columns(table_name)\n            all_columns.update([(table_name, col) for col in table_columns])\n        \n        # Look for similarly named columns across disconnected components\n        potential_joins = self._find_potential_join_columns(all_columns)\n        if potential_joins:\n            suggestions.extend([f\"Potential join: {suggestion}\" for suggestion in potential_joins])\n            \n        return suggestions\n    \n    def _get_table_columns(self, table_name: str) -> List[str]:\n        \"\"\"Get column names for table - would integrate with schema catalog\"\"\"\n        # Placeholder - would query actual schema\n        return ['id', 'name', 'created_at']  \n        \n    def _find_potential_join_columns(self, all_columns: Set[Tuple[str, str]]) -> List[str]:\n        \"\"\"Find potential join columns based on naming patterns\"\"\"\n        # Simple heuristic - look for columns with similar names\n        column_groups = defaultdict(list)\n        \n        for table, column in all_columns:\n            # Group by column name (ignoring table prefix)\n            base_name = column.replace('_id', '').replace('id', '')\n            column_groups[base_name].append((table, column))\n            \n        suggestions = []\n        for base_name, columns in column_groups.items():\n            if len(columns) > 1:\n                tables = [table for table, _ in columns]\n                suggestions.append(f\"Tables {', '.join(tables)} have similar column '{base_name}'\")\n                \n        return suggestions\n```\n\n#### Core Logic Skeleton Code\n\n**Failure Mode Detection** - Core failure detection logic with detailed TODOs:\n\n```python\nclass OptimizationFailureDetector:\n    def __init__(self, config: dict):\n        self.max_optimization_time = config.get('max_time_seconds', 30)\n        self.max_memory_mb = config.get('max_memory_mb', 512) \n        self.max_join_tables = config.get('max_join_tables', 12)\n        \n    def check_optimization_feasibility(self, parsed_query: ParsedQuery, \n                                     available_stats: Dict[str, TableStatistics]) -> Dict[str, any]:\n        \"\"\"Check if optimization is feasible and identify potential failure modes\"\"\"\n        \n        # TODO 1: Check if query complexity exceeds system limits\n        #         - Count number of tables in FROM clause\n        #         - Estimate join enumeration complexity (2^n for n tables)\n        #         - If complexity > MAX_JOIN_ENUMERATION, flag for heuristic fallback\n        \n        # TODO 2: Validate statistics availability and quality  \n        #         - For each table in query, check if stats exist in available_stats\n        #         - Calculate average confidence level across all table statistics\n        #         - If average confidence < 50%, flag for conservative optimization\n        \n        # TODO 3: Detect resource-intensive query patterns\n        #         - Look for cross products (tables without connecting join predicates)\n        #         - Identify extremely selective filters (estimated selectivity < 0.001)\n        #         - Check for queries with no usable indexes on filter/join columns\n        \n        # TODO 4: Estimate optimization resource requirements\n        #         - Calculate expected memory for DP memoization tables\n        #         - Estimate time complexity based on query structure  \n        #         - Compare against available system resources\n        \n        # TODO 5: Return comprehensive feasibility assessment\n        #         - Include boolean feasible flag\n        #         - List detected failure modes with severity levels\n        #         - Recommend fallback strategies for each failure mode\n        #         - Provide confidence estimate for optimization success\n        \n        pass\n    \n    def select_fallback_strategy(self, failure_modes: List[str], \n                               query_complexity: int) -> str:\n        \"\"\"Select appropriate fallback optimization strategy based on detected failures\"\"\"\n        \n        # TODO 1: Prioritize failure modes by severity\n        #         - 'memory_exhaustion' -> immediate heuristic fallback\n        #         - 'statistics_missing' -> rule-based optimization\n        #         - 'cross_product' -> add warnings but continue optimization\n        #         - 'timeout_risk' -> reduce search space (left-deep only)\n        \n        # TODO 2: Consider query complexity in strategy selection\n        #         - Simple queries (<=3 tables): can tolerate some failures\n        #         - Medium queries (4-8 tables): need balanced approach\n        #         - Complex queries (>8 tables): aggressive simplification needed\n        \n        # TODO 3: Return specific fallback strategy name\n        #         - 'full_heuristic': skip cost-based optimization entirely\n        #         - 'limited_enumeration': reduce DP search space\n        #         - 'rule_based_physical': skip cost-based physical planning\n        #         - 'conservative_costs': use default costs with high margins\n        \n        pass\n```\n\n**Statistics Confidence Tracker** - Core confidence management with detailed TODOs:\n\n```python\nclass StatisticsConfidenceTracker:\n    def __init__(self):\n        self.confidence_cache = {}\n        self.staleness_detector = StatisticsStalenessDetector({})\n        \n    def calculate_plan_confidence(self, plan: ExecutionPlan, \n                                table_stats: Dict[str, TableStatistics]) -> float:\n        \"\"\"Calculate overall confidence level for execution plan cost estimates\"\"\"\n        \n        # TODO 1: Extract all tables referenced in the execution plan\n        #         - Traverse plan tree using traverse_preorder()\n        #         - Find all SCAN operators and extract table names\n        #         - Build set of unique table names used in plan\n        \n        # TODO 2: Assess statistics confidence for each table\n        #         - For each table, look up TableStatistics in table_stats\n        #         - Use staleness_detector.assess_staleness() to get confidence multiplier\n        #         - Handle missing statistics (assign default low confidence ~0.2)\n        \n        # TODO 3: Weight confidence by table importance in plan\n        #         - Tables used in selective filters get higher weight\n        #         - Tables appearing in join order early get higher weight\n        #         - Large tables (high row count) get higher weight\n        \n        # TODO 4: Calculate composite confidence score\n        #         - Use weighted harmonic mean (conservative - dominated by lowest confidence)\n        #         - Apply query complexity penalty (more complex = lower confidence)  \n        #         - Clamp final result between 0.1 and 1.0\n        \n        # TODO 5: Cache result and update plan metadata\n        #         - Store confidence in plan.optimization_metadata['confidence']\n        #         - Update each operator's cost_estimate.confidence_level\n        #         - Return overall plan confidence score\n        \n        pass\n    \n    def adjust_costs_for_uncertainty(self, cost_estimate: CostEstimate, \n                                   confidence: float) -> CostEstimate:\n        \"\"\"Adjust cost estimates based on confidence level\"\"\"\n        \n        # TODO 1: Calculate uncertainty margin based on confidence\n        #         - High confidence (>0.8): no adjustment needed\n        #         - Medium confidence (0.5-0.8): add 25-50% margin\n        #         - Low confidence (0.2-0.5): add 100-200% margin\n        #         - Very low confidence (<0.2): add 300% margin\n        \n        # TODO 2: Apply uncertainty adjustments to cost components\n        #         - Increase io_cost by uncertainty margin percentage\n        #         - Increase cpu_cost by uncertainty margin percentage\n        #         - Increase memory_cost by uncertainty margin percentage\n        #         - Update total_cost property accordingly\n        \n        # TODO 3: Update cost estimate metadata\n        #         - Set confidence_level field to provided confidence\n        #         - Add 'uncertainty_adjusted': True to cost_factors dict\n        #         - Record original costs in cost_factors for debugging\n        \n        # TODO 4: Return new CostEstimate with adjusted values\n        #         - Create new CostEstimate instance (don't modify original)\n        #         - Preserve all metadata and factor information\n        #         - Ensure total_cost property calculates correctly\n        \n        pass\n```\n\n#### Milestone Checkpoints\n\n**After implementing failure detection:**\n- Run `python -m pytest tests/error_handling/test_failure_detection.py -v`\n- Expected: All failure modes detected correctly for test queries\n- Manual verification: Create query with 15+ tables, should trigger complexity warning\n- Signs of problems: False positives on simple queries, missed detection of obvious cross products\n\n**After implementing statistics staleness handling:**\n- Run statistics staleness tests with artificially aged data\n- Expected: Confidence levels decrease appropriately over time\n- Manual verification: Query with 1-day-old stats should show ~75% confidence\n- Signs of problems: Confidence levels not updating, refresh triggers not firing\n\n**After implementing degenerate query handling:**\n- Test cross product detection with queries missing WHERE clauses\n- Expected: Cross products detected and suggestions generated\n- Manual verification: `SELECT * FROM users, orders` should trigger cross product warning\n- Signs of problems: False cross product detection, missing connectivity analysis\n\n#### Debugging Tips\n\n| Symptom | Likely Cause | Diagnosis | Fix |\n|---------|--------------|-----------|-----|\n| Optimizer always uses fallback strategies | Failure detection too aggressive | Check failure mode thresholds in configuration | Increase complexity limits, reduce confidence requirements |\n| Poor plan quality with no warnings | Stale statistics not detected | Verify staleness detection logic | Check timestamp comparisons, add logging to staleness assessment |\n| Cross product warnings on valid queries | Connectivity graph not built correctly | Examine join predicate parsing | Fix predicate extraction, verify graph construction |\n| Confidence levels never change | Statistics validation disabled | Check if staleness detector is called | Add staleness assessment to optimization pipeline |\n| Optimization fails with resource errors | Resource monitoring not working | Check memory/time tracking | Add resource monitoring hooks, verify limit enforcement |\n| Plans have unrealistic cost estimates | Uncertainty adjustments not applied | Verify confidence-based cost adjustment | Check if adjust_costs_for_uncertainty is called |\n\n\n## Testing Strategy\n\n> **Milestone(s):** All Milestones 1-4 - provides comprehensive validation strategies for each component and integration points throughout the optimization pipeline.\n\n### Mental Model: Quality Assurance in Manufacturing\n\nThink of testing a query optimizer like quality assurance in a complex manufacturing plant that produces custom products (execution plans). Just as a factory needs multiple inspection checkpoints - incoming materials inspection (input validation), component testing (individual optimizer modules), assembly line verification (integration testing), and final product quality assessment (plan validation) - our query optimizer requires layered testing at each stage. The factory analogy extends further: we need both automated testing equipment (unit tests) and skilled inspectors who can evaluate whether the final product meets quality standards (plan quality validation). Just as manufacturing defects become more expensive to fix the later they're discovered, optimizer bugs are cheaper to catch in individual components than in the final integrated system.\n\nOur testing strategy mirrors this multi-layered quality assurance approach. We validate each optimizer component in isolation, verify their interactions produce reasonable results, and establish milestone checkpoints that ensure incremental progress. The key insight is that query optimization correctness has two dimensions: functional correctness (does the plan produce the right results) and quality correctness (is the plan reasonably efficient). Traditional software testing focuses on the first dimension, but optimizer testing must heavily emphasize the second.\n\n### Component Unit Testing\n\nComponent unit testing forms the foundation of our quality assurance strategy, validating each optimizer module's internal logic before integration. Each component has distinct testing requirements based on its responsibilities and failure modes.\n\nThe **Plan Representation Component** requires testing that validates tree structure integrity and operator semantics. Our test suite must verify that `OperatorNode` instances correctly maintain parent-child relationships, support tree traversal algorithms, and accurately calculate cost aggregation. The component's tree manipulation operations need extensive boundary condition testing.\n\n| Test Category | Test Methods | Validation Focus | Expected Behavior |\n|---------------|--------------|------------------|-------------------|\n| Tree Construction | `test_operator_node_creation`, `test_tree_building` | Node initialization, parent-child linking | Valid tree structure with correct relationships |\n| Tree Traversal | `test_preorder_traversal`, `test_postorder_traversal` | Visit order correctness, complete coverage | All nodes visited in correct sequence |\n| Cost Calculation | `test_cost_accumulation`, `test_subtree_costs` | Bottom-up cost aggregation | Accurate total cost from leaf costs |\n| Tree Manipulation | `test_node_insertion`, `test_subtree_replacement` | Structural modifications | Tree remains valid after changes |\n| Serialization | `test_pretty_print`, `test_plan_export` | Human-readable output | Correct indentation and cost display |\n\nThe **Cost Estimation Component** presents unique testing challenges because it deals with statistical predictions rather than deterministic computations. Our test strategy must validate both the mathematical correctness of cost formulas and the reasonableness of estimated values under various data distribution scenarios.\n\n| Statistical Scenario | Test Data Setup | Expected Behavior | Validation Method |\n|----------------------|-----------------|-------------------|------------------|\n| Uniform Distribution | Equal frequency across value range | Linear selectivity curves | Compare against analytical formulas |\n| Skewed Distribution | 80% of values in 20% of range | Higher selectivity for popular values | Histogram-based validation |\n| Missing Statistics | Empty or null statistical data | Conservative cost estimates | Fallback to default assumptions |\n| Extreme Selectivity | Very high or low filter selectivity | Appropriate cost scaling | Boundary condition testing |\n| Join Cardinality | Various table size combinations | Reasonable join size estimates | Cross-validation with known results |\n\n```python\ndef test_selectivity_estimation_edge_cases():\n    \"\"\"Test cost estimation component handles edge cases appropriately.\"\"\"\n    # Test case: highly selective filter on large table\n    large_table_stats = create_table_stats(row_count=1000000, distinct_values=500000)\n    selectivity = estimator.estimate_filter_selectivity(\n        table=\"large_table\", \n        column=\"id\", \n        operator=\"=\", \n        value=\"specific_id\"\n    )\n    # Should be approximately 1/distinct_values\n    assert 0.000001 <= selectivity <= 0.000003\n    \n    # Test case: filter on column with all NULL values\n    null_column_stats = create_column_stats(null_count=100000, distinct_values=0)\n    selectivity = estimator.estimate_filter_selectivity(\n        table=\"test_table\",\n        column=\"null_column\",\n        operator=\"=\",\n        value=\"any_value\"\n    )\n    # Should return zero selectivity\n    assert selectivity == 0.0\n```\n\nThe **Join Optimization Component** requires testing that validates the dynamic programming algorithm's correctness and handles the exponential complexity of join ordering. Our tests must verify that the optimizer finds truly optimal solutions for small problems where we can enumerate all possibilities, and produces reasonable solutions for larger problems where exhaustive verification isn't feasible.\n\n| Algorithm Aspect | Test Approach | Validation Strategy | Success Criteria |\n|-------------------|---------------|---------------------|------------------|\n| Optimal Solutions | Small 3-4 table queries | Compare against brute force enumeration | Finds globally optimal join order |\n| Connectivity Detection | Queries with disconnected tables | Cross product identification | Properly identifies and handles disconnected components |\n| Pruning Effectiveness | Large table sets | Measure search space reduction | Significant reduction without losing optimal solutions |\n| Cost Monotonicity | Incremental join additions | Verify costs increase appropriately | Larger joins have higher costs than subsets |\n| Memory Management | Very large join problems | Track memory usage during optimization | Reasonable memory consumption with cleanup |\n\nThe **Physical Planning Component** needs testing that validates operator selection logic and optimization rule application. Since this component makes final implementation choices, our tests must verify that selected physical operators are appropriate for the data characteristics and that optimization rules produce semantically equivalent but more efficient plans.\n\n| Physical Decision | Test Scenarios | Validation Approach | Expected Outcome |\n|-------------------|----------------|---------------------|------------------|\n| Scan Method Selection | Various selectivity levels | Compare index vs sequential scan choices | Correct method for each selectivity range |\n| Join Algorithm Choice | Different input size combinations | Validate hash vs nested loop selection | Appropriate algorithm for size characteristics |\n| Predicate Pushdown | Filters with join predicates | Verify rule application correctness | Filters moved to optimal positions |\n| Memory Estimation | Complex operators requiring memory | Resource requirement calculations | Realistic memory usage estimates |\n| Plan Validation | Generated physical plans | Semantic correctness verification | All plans are executable and correct |\n\n### Plan Quality Validation\n\nPlan quality validation addresses the fundamental challenge of optimizer testing: verifying that generated execution plans are not just functionally correct but also reasonably efficient. Unlike traditional unit testing where we can assert exact expected outputs, plan quality requires heuristic evaluation and comparative analysis.\n\nOur quality validation strategy employs multiple complementary approaches. **Comparative analysis** evaluates whether the optimizer makes reasonable choices between clearly different alternatives. **Regression testing** ensures that optimizer changes don't degrade plan quality for established benchmark queries. **Heuristic validation** applies rule-of-thumb checks to identify obviously poor plans.\n\nThe most effective quality validation technique is **plan comparison testing**, where we generate plans for queries with obvious optimal characteristics and verify the optimizer makes expected choices:\n\n| Query Pattern | Expected Optimization | Quality Metric | Validation Method |\n|---------------|----------------------|----------------|------------------|\n| High Selectivity Filter | Filter before join | Filter position | Verify filter appears below join in tree |\n| Large Table Join Small Table | Smaller table as build side | Join algorithm choice | Hash join with correct build/probe assignment |\n| Available Index on Filter Column | Index scan over sequential scan | Access method selection | Index scan chosen for high selectivity |\n| Star Schema Join | Fact table joined last | Join ordering | Dimension tables joined first |\n| Range Query with Index | Index range scan | Scan method and cost | Index scan with appropriate cost estimate |\n\n**Benchmark query validation** provides systematic plan quality assessment using standard query patterns that represent common database workloads. Our benchmark suite includes queries from TPC-H and TPC-DS adapted for our optimizer's capabilities:\n\n```sql\n-- Benchmark Query: Selective Join with Filter\nSELECT c.name, o.order_date, o.total\nFROM customers c JOIN orders o ON c.customer_id = o.customer_id  \nWHERE c.region = 'ASIA' AND o.order_date >= '2023-01-01';\n\n-- Expected Plan Characteristics:\n-- 1. Filter on customers.region should appear before join\n-- 2. Date filter on orders should appear before join  \n-- 3. Join algorithm should be hash join (typical case)\n-- 4. Smaller filtered customer set should be build side\n```\n\nOur benchmark validation framework automatically generates plans for these queries and applies quality heuristics:\n\n| Quality Heuristic | Description | Implementation | Failure Threshold |\n|-------------------|-------------|----------------|-------------------|\n| Filter Placement | Filters should appear as low in tree as possible | Count filters above joins | More than 20% misplaced filters |\n| Join Algorithm Appropriateness | Hash joins for large tables, nested loops for small | Algorithm vs size validation | More than 10% inappropriate choices |\n| Index Utilization | Available indexes should be used for selective predicates | Index usage rate measurement | Less than 70% appropriate index usage |\n| Cost Reasonableness | Plan costs should correlate with actual resource usage | Cost vs execution time correlation | Correlation coefficient below 0.6 |\n| Plan Complexity | Generated plans shouldn't be unnecessarily complex | Operator count vs query complexity | Plans more than 50% larger than minimal |\n\n**Regression quality testing** maintains plan quality over time as the optimizer evolves. This testing approach maintains a database of previously optimized queries with their generated plans and quality metrics. When optimizer code changes, we re-optimize these queries and compare the new plans against historical baselines:\n\n| Regression Metric | Measurement Method | Acceptable Change | Alert Threshold |\n|-------------------|-------------------|-------------------|-----------------|\n| Plan Cost Changes | Compare `total_cost` between versions | ±10% cost variation | More than 25% cost increase |\n| Algorithm Selection Changes | Count physical operator type changes | 5% selection changes | More than 15% selection changes |\n| Optimization Time | Measure `optimize_query` execution time | ±20% time variation | More than 50% time increase |\n| Plan Structure Changes | Compare tree topology | Minor structural changes | Major topology differences |\n| Statistics Usage | Track statistics access patterns | Similar access patterns | Significantly different patterns |\n\n**Stress testing for plan quality** evaluates optimizer behavior under challenging conditions that might reveal quality degradation:\n\n- **Large join queries** with 8-12 tables test whether the optimizer maintains reasonable optimization times while finding good solutions\n- **Highly correlated data** tests whether independence assumptions in cost estimation lead to severely inaccurate plans\n- **Missing or stale statistics** validate that the optimizer gracefully degrades to reasonable heuristics\n- **Extreme data skew** tests whether uniform distribution assumptions cause poor physical operator choices\n- **Resource constraints** simulate low memory conditions to test whether the optimizer adapts appropriately\n\n### Milestone Validation Checkpoints\n\nMilestone validation checkpoints provide structured verification points that ensure each implementation phase produces the expected functionality before proceeding to the next milestone. These checkpoints combine automated testing with manual verification to catch both functional bugs and design misunderstandings.\n\n**Milestone 1: Query Plan Representation** validation focuses on verifying that the foundational plan tree infrastructure correctly supports all required operations:\n\n| Validation Category | Test Approach | Success Criteria | Manual Verification |\n|---------------------|---------------|------------------|---------------------|\n| Tree Construction | Create plans with various operator combinations | All operator types supported, correct parent-child links | Visual inspection of `pretty_print` output |\n| Tree Traversal | Implement preorder and postorder traversals | All nodes visited exactly once in correct order | Trace traversal output matches expected sequence |\n| Cost Annotation | Attach cost estimates to operator nodes | Cost propagation works correctly bottom-up | Verify leaf costs aggregate to root total |\n| Schema Propagation | Pass column information through operator chain | Output schemas correctly computed at each level | Check schema transformations match operator semantics |\n| Plan Serialization | Export plans to human-readable format | Clear, indented tree representation with costs | Review printed plans for readability and accuracy |\n\nThe Milestone 1 checkpoint includes these specific automated tests:\n\n```python\ndef test_milestone_1_checkpoint():\n    \"\"\"Comprehensive validation of plan representation functionality.\"\"\"\n    \n    # Test 1: Build complex plan tree\n    scan_customers = OperatorNode(\n        operator_type=OperatorType.SCAN,\n        properties={\"table\": \"customers\"},\n        output_schema=[\"customer_id\", \"name\", \"region\"]\n    )\n    \n    filter_node = OperatorNode(\n        operator_type=OperatorType.FILTER,\n        properties={\"predicate\": \"region = 'ASIA'\"},\n        output_schema=[\"customer_id\", \"name\", \"region\"]\n    )\n    filter_node.add_child(scan_customers)\n    \n    # Verify tree structure\n    assert len(filter_node.children) == 1\n    assert filter_node.children[0] == scan_customers\n    \n    # Test 2: Traverse tree and verify visit order\n    visited = list(filter_node.traverse_preorder())\n    assert visited[0] == filter_node\n    assert visited[1] == scan_customers\n    \n    # Test 3: Cost aggregation\n    scan_customers.cost_estimate = CostEstimate(io_cost=100.0, cpu_cost=50.0)\n    filter_node.cost_estimate = CostEstimate(io_cost=0.0, cpu_cost=25.0)\n    total_cost = filter_node.calculate_subtree_cost()\n    assert total_cost.total_cost == 175.0\n```\n\n**Milestone 2: Cost Estimation** validation verifies that statistical cost models produce reasonable estimates under various data characteristics:\n\n| Validation Focus | Test Data | Expected Behavior | Verification Method |\n|------------------|-----------|-------------------|---------------------|\n| Selectivity Estimation | Various filter predicates | Selectivity correlates with predicate selectivity | Compare estimates against known distributions |\n| Join Cardinality | Different table size combinations | Reasonable output size estimates | Cross-validate with independence assumption |\n| I/O Cost Calculation | Plans with different access patterns | Costs reflect expected I/O operations | Manual calculation verification |\n| CPU Cost Calculation | Plans with different tuple processing | Costs scale with processing complexity | Verify cost scaling relationships |\n| Statistics Integration | Real table statistics | Cost estimates use available statistics | Statistics access pattern validation |\n\nThe Milestone 2 checkpoint validates cost estimation accuracy:\n\n```python\ndef test_milestone_2_checkpoint():\n    \"\"\"Validate cost estimation component accuracy and reasonableness.\"\"\"\n    \n    # Test 1: Filter selectivity estimation\n    table_stats = TableStatistics(\n        table_name=\"orders\",\n        row_count=100000,\n        column_stats={\n            \"status\": ColumnStatistics(\n                column_name=\"status\",\n                distinct_values=5,\n                most_common_values=[(\"SHIPPED\", 0.4), (\"PENDING\", 0.3)]\n            )\n        }\n    )\n    \n    selectivity = estimator.estimate_filter_selectivity(\n        table=\"orders\", column=\"status\", operator=\"=\", value=\"SHIPPED\"\n    )\n    assert 0.35 <= selectivity <= 0.45  # Should be around 0.4\n    \n    # Test 2: Join cardinality estimation\n    customer_stats = create_table_stats(row_count=10000, distinct_customers=10000)\n    order_stats = create_table_stats(row_count=100000, distinct_customers=8000)\n    \n    join_cardinality = estimator.estimate_join_cardinality(\n        left_table=\"customers\", left_column=\"customer_id\",\n        right_table=\"orders\", right_column=\"customer_id\"\n    )\n    # Should be approximately orders.row_count (most customers have orders)\n    assert 80000 <= join_cardinality <= 100000\n```\n\n**Milestone 3: Join Ordering** validation ensures the dynamic programming algorithm finds optimal join orders for small problems and reasonable orders for larger problems:\n\n| Algorithm Aspect | Test Case | Verification Approach | Success Indicator |\n|-------------------|-----------|----------------------|-------------------|\n| Optimal Solutions | 3-table join with clear optimal order | Compare against brute force enumeration | Finds known optimal solution |\n| Connectivity Handling | Disconnected table groups | Cross product detection and handling | Properly identifies disconnected components |\n| Cost-Based Selection | Tables with different sizes and selectivities | Join order reflects cost considerations | Smaller filtered tables joined first |\n| Scalability | 6-8 table joins | Reasonable optimization time | Optimization completes within time limits |\n| Memoization | Repeated subproblems | Avoid redundant cost calculations | Significant performance improvement |\n\n**Milestone 4: Physical Planning** validation verifies that physical operator selection produces executable plans with appropriate performance characteristics:\n\n| Physical Choice | Test Scenario | Expected Selection | Validation Method |\n|-----------------|---------------|-------------------|------------------|\n| Sequential vs Index Scan | Various filter selectivities | Index for high selectivity, sequential for low | Compare access method choices |\n| Hash vs Nested Loop Join | Different input size ratios | Hash for large inputs, nested loop for small | Validate join algorithm selection |\n| Predicate Pushdown | Filters with join predicates | Filters moved below joins where possible | Verify filter placement in final plan |\n| Memory Planning | Memory-intensive operations | Realistic memory requirements | Check memory estimates are reasonable |\n| Plan Executability | All generated plans | Plans are semantically correct | Attempt to execute plans on sample data |\n\nEach milestone checkpoint includes a **manual verification component** where developers inspect generated plans for reasonableness:\n\n1. **Plan Visualization**: Use `pretty_print` to generate indented tree representations and verify they match expected structure\n2. **Cost Reasonableness**: Compare cost estimates against intuition for relative operator expenses\n3. **Semantic Correctness**: Verify that generated plans would produce correct query results\n4. **Performance Characteristics**: Check that plans reflect appropriate performance trade-offs\n\nThe milestone checkpoint process includes these **debugging checkpoints** to catch common implementation errors:\n\n| Common Issue | Detection Method | Diagnostic Approach | Resolution Strategy |\n|--------------|------------------|---------------------|---------------------|\n| Incorrect Tree Structure | Tree traversal produces unexpected node order | Print tree structure, verify parent-child links | Fix tree construction logic |\n| Cost Calculation Errors | Costs don't aggregate correctly | Trace cost calculation step by step | Debug cost propagation algorithm |\n| Statistics Access Failures | Cost estimates seem unrealistic | Check statistics lookup and usage | Verify statistics integration |\n| Join Ordering Suboptimal | Obviously poor join orders selected | Compare against manual optimization | Debug dynamic programming logic |\n| Physical Selection Inappropriate | Wrong algorithms chosen for data characteristics | Analyze selection criteria and thresholds | Tune selection heuristics |\n\n> **Key Insight**: Milestone checkpoints serve dual purposes - they validate implementation correctness and provide learning checkpoints where developers can verify their understanding before tackling the next complexity level. The manual verification components are particularly important for building intuition about query optimization principles.\n\n### Implementation Guidance\n\nThe testing strategy requires careful balance between comprehensive validation and practical implementation constraints. Our approach provides multiple levels of testing infrastructure to support both individual component development and integrated system validation.\n\n**A. Technology Recommendations**\n\n| Testing Component | Simple Option | Advanced Option |\n|-------------------|---------------|-----------------|\n| Unit Testing Framework | `unittest` (built-in Python) | `pytest` with fixtures and parametrization |\n| Test Data Generation | Manual test data creation | `Faker` library for realistic data generation |\n| Plan Comparison | String-based plan comparison | AST-based structural plan comparison |\n| Performance Testing | Simple timing measurements | `pytest-benchmark` for statistical timing |\n| Mock Statistics | Hardcoded test statistics | `unittest.mock` for dynamic mocking |\n| Test Coverage | Manual coverage assessment | `coverage.py` for automated coverage reporting |\n\n**B. Recommended File Structure**\n\n```\nproject-root/\n  tests/\n    unit/\n      test_plan_representation.py     ← Milestone 1 component tests\n      test_cost_estimation.py         ← Milestone 2 component tests  \n      test_join_optimization.py       ← Milestone 3 component tests\n      test_physical_planning.py       ← Milestone 4 component tests\n    integration/\n      test_optimization_pipeline.py  ← End-to-end optimization tests\n      test_plan_quality.py           ← Plan quality validation tests\n    fixtures/\n      sample_queries.sql              ← Standard test queries\n      test_statistics.json            ← Sample table statistics\n      benchmark_plans.json            ← Expected plan structures\n    utils/\n      test_data_generator.py          ← Helper for creating test data\n      plan_comparison.py              ← Plan quality assessment utilities\n      query_builder.py                ← SQL query construction helpers\n  src/optimizer/\n    plan_representation.py\n    cost_estimation.py\n    join_optimization.py\n    physical_planning.py\n```\n\n**C. Testing Infrastructure Starter Code**\n\n```python\n\"\"\"\nTest infrastructure for query optimizer validation.\nProvides utilities for creating test data, comparing plans, and validating quality.\n\"\"\"\nimport json\nfrom typing import List, Dict, Any, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n@dataclass\nclass TestQuery:\n    \"\"\"Represents a test query with expected characteristics.\"\"\"\n    sql: str\n    description: str\n    expected_plan_properties: Dict[str, Any]\n    expected_cost_range: Tuple[float, float]\n    tables_involved: List[str]\n\nclass PlanQualityMetric(Enum):\n    \"\"\"Quality metrics for evaluating execution plans.\"\"\"\n    FILTER_PLACEMENT = \"filter_placement\"\n    JOIN_ALGORITHM_APPROPRIATENESS = \"join_algorithm_appropriateness\" \n    INDEX_UTILIZATION = \"index_utilization\"\n    COST_REASONABLENESS = \"cost_reasonableness\"\n    PLAN_COMPLEXITY = \"plan_complexity\"\n\nclass TestDataGenerator:\n    \"\"\"Generates realistic test data for optimizer validation.\"\"\"\n    \n    def create_table_statistics(self, \n                              table_name: str,\n                              row_count: int,\n                              column_definitions: Dict[str, Dict]) -> TableStatistics:\n        \"\"\"Create table statistics for testing cost estimation.\"\"\"\n        column_stats = {}\n        for col_name, col_def in column_definitions.items():\n            column_stats[col_name] = ColumnStatistics(\n                column_name=col_name,\n                distinct_values=col_def.get('distinct_values', row_count // 10),\n                null_count=col_def.get('null_count', 0),\n                min_value=col_def.get('min_value', 1),\n                max_value=col_def.get('max_value', row_count),\n                most_common_values=col_def.get('most_common_values', []),\n                histogram_buckets=self._create_uniform_histogram(\n                    col_def.get('min_value', 1),\n                    col_def.get('max_value', row_count),\n                    10\n                )\n            )\n        \n        return TableStatistics(\n            table_name=table_name,\n            row_count=row_count,\n            page_count=row_count // 100,  # Assume 100 rows per page\n            column_stats=column_stats,\n            last_updated=datetime.now(),\n            sample_rate=1.0,\n            index_statistics={},\n            clustering_factor=1.0\n        )\n    \n    def _create_uniform_histogram(self, min_val: Any, max_val: Any, bucket_count: int) -> List[HistogramBucket]:\n        \"\"\"Create uniform distribution histogram for testing.\"\"\"\n        # TODO: Implement uniform histogram bucket creation\n        # Hint: divide range into equal buckets with equal row counts\n        pass\n\nclass PlanComparisonUtility:\n    \"\"\"Utilities for comparing and validating execution plans.\"\"\"\n    \n    def compare_plan_structure(self, plan1: ExecutionPlan, plan2: ExecutionPlan) -> Dict[str, Any]:\n        \"\"\"Compare structural characteristics of two execution plans.\"\"\"\n        # TODO: Implement plan structure comparison\n        # Compare operator types, tree topology, join order\n        pass\n    \n    def validate_plan_quality(self, plan: ExecutionPlan, \n                            query: TestQuery,\n                            metrics: List[PlanQualityMetric]) -> Dict[str, float]:\n        \"\"\"Evaluate plan quality using specified metrics.\"\"\"\n        quality_scores = {}\n        \n        for metric in metrics:\n            if metric == PlanQualityMetric.FILTER_PLACEMENT:\n                quality_scores[metric.value] = self._evaluate_filter_placement(plan)\n            elif metric == PlanQualityMetric.JOIN_ALGORITHM_APPROPRIATENESS:\n                quality_scores[metric.value] = self._evaluate_join_algorithms(plan)\n            # TODO: Implement other quality metric evaluations\n            \n        return quality_scores\n    \n    def _evaluate_filter_placement(self, plan: ExecutionPlan) -> float:\n        \"\"\"Evaluate whether filters are placed optimally in the plan.\"\"\"\n        # TODO: Analyze plan tree to find filters above joins\n        # Return score 0.0-1.0 where 1.0 means optimal filter placement\n        pass\n```\n\n**D. Core Testing Skeleton Code**\n\n```python\n\"\"\"\nUnit tests for cost estimation component - Milestone 2 validation.\n\"\"\"\nimport unittest\nfrom unittest.mock import Mock, patch\nfrom optimizer.cost_estimation import CostEstimator, TableStatistics, ColumnStatistics\n\nclass TestCostEstimation(unittest.TestCase):\n    \"\"\"Test cost estimation accuracy and edge case handling.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test fixtures with sample statistics.\"\"\"\n        self.cost_estimator = CostEstimator()\n        self.sample_table_stats = self._create_sample_statistics()\n    \n    def test_selectivity_estimation_uniform_distribution(self):\n        \"\"\"Test selectivity estimation for uniformly distributed data.\"\"\"\n        # TODO 1: Create table statistics with uniform distribution\n        # TODO 2: Test equality predicate selectivity (should be 1/distinct_values)\n        # TODO 3: Test range predicate selectivity (should be proportional to range)\n        # TODO 4: Verify selectivity values are between 0.0 and 1.0\n        # TODO 5: Test edge cases (non-existent values, null comparisons)\n        pass\n    \n    def test_join_cardinality_estimation(self):\n        \"\"\"Test join output size estimation for various scenarios.\"\"\"\n        # TODO 1: Set up two tables with known foreign key relationship\n        # TODO 2: Estimate join cardinality using column statistics  \n        # TODO 3: Verify result is reasonable given input table sizes\n        # TODO 4: Test many-to-many join cardinality estimation\n        # TODO 5: Test cross product detection (no join predicates)\n        pass\n    \n    def test_cost_model_components(self):\n        \"\"\"Test I/O and CPU cost calculation components.\"\"\"\n        # TODO 1: Create plan with known I/O characteristics\n        # TODO 2: Calculate expected I/O cost using page counts\n        # TODO 3: Calculate expected CPU cost using tuple processing\n        # TODO 4: Verify total cost combines I/O and CPU appropriately\n        # TODO 5: Test cost scaling with different input sizes\n        pass\n    \n    def test_missing_statistics_handling(self):\n        \"\"\"Test behavior when table statistics are missing or incomplete.\"\"\"\n        # TODO 1: Test cost estimation with empty statistics\n        # TODO 2: Verify fallback to conservative estimates\n        # TODO 3: Test partial statistics (some columns missing)\n        # TODO 4: Verify error handling for invalid statistics\n        # TODO 5: Test statistics staleness detection and handling\n        pass\n    \n    def _create_sample_statistics(self) -> Dict[str, TableStatistics]:\n        \"\"\"Create realistic sample statistics for testing.\"\"\"\n        # TODO: Create statistics for customers, orders, products tables\n        # Include various data distributions and characteristics\n        pass\n\nclass TestPlanQuality(unittest.TestCase):\n    \"\"\"Integration tests for plan quality validation.\"\"\"\n    \n    def test_benchmark_query_optimization(self):\n        \"\"\"Test optimizer performance on standard benchmark queries.\"\"\"\n        benchmark_queries = [\n            TestQuery(\n                sql=\"SELECT * FROM customers c JOIN orders o ON c.id = o.customer_id WHERE c.region = 'ASIA'\",\n                description=\"Selective filter with join\",\n                expected_plan_properties={\"filter_before_join\": True, \"join_algorithm\": \"hash\"},\n                expected_cost_range=(1000.0, 5000.0),\n                tables_involved=[\"customers\", \"orders\"]\n            ),\n            # TODO: Add more benchmark queries covering different patterns\n        ]\n        \n        for query in benchmark_queries:\n            # TODO 1: Parse and optimize the query\n            # TODO 2: Validate plan structure matches expectations\n            # TODO 3: Check cost estimate is within expected range\n            # TODO 4: Verify plan quality metrics meet thresholds\n            # TODO 5: Compare against baseline plans if available\n            pass\n    \n    def test_plan_regression_validation(self):\n        \"\"\"Test that optimizer changes don't degrade plan quality.\"\"\"\n        # TODO 1: Load historical plan database\n        # TODO 2: Re-optimize all historical queries\n        # TODO 3: Compare new plans against historical baselines\n        # TODO 4: Flag significant cost increases or algorithm changes\n        # TODO 5: Update baselines for legitimate improvements\n        pass\n```\n\n**E. Milestone Checkpoint Implementation**\n\n```python\n\"\"\"\nMilestone validation checkpoints for incremental verification.\n\"\"\"\nclass MilestoneValidator:\n    \"\"\"Validates completion of each implementation milestone.\"\"\"\n    \n    def validate_milestone_1_plan_representation(self) -> bool:\n        \"\"\"Validate Milestone 1: Query plan representation is working correctly.\"\"\"\n        print(\"=== Milestone 1 Validation: Plan Representation ===\")\n        \n        try:\n            # Test 1: Create complex plan tree\n            print(\"Test 1: Building complex plan tree...\")\n            root_plan = self._build_sample_plan_tree()\n            assert root_plan.root is not None\n            print(\"✓ Plan tree construction successful\")\n            \n            # Test 2: Tree traversal\n            print(\"Test 2: Testing tree traversal...\")\n            nodes = list(root_plan.root.traverse_preorder())\n            assert len(nodes) >= 3  # Should have multiple operators\n            print(f\"✓ Traversed {len(nodes)} nodes successfully\")\n            \n            # Test 3: Cost calculation\n            print(\"Test 3: Testing cost calculation...\")\n            total_cost = root_plan.root.calculate_subtree_cost()\n            assert total_cost.total_cost > 0\n            print(f\"✓ Total plan cost: {total_cost.total_cost}\")\n            \n            # Test 4: Pretty printing\n            print(\"Test 4: Testing plan visualization...\")\n            plan_text = root_plan.root.pretty_print(show_costs=True)\n            assert len(plan_text) > 100  # Should be substantial output\n            print(\"✓ Plan pretty printing successful\")\n            print(\"\\nSample plan output:\")\n            print(plan_text[:200] + \"...\" if len(plan_text) > 200 else plan_text)\n            \n            print(\"\\n✅ Milestone 1 validation PASSED\")\n            return True\n            \n        except Exception as e:\n            print(f\"\\n❌ Milestone 1 validation FAILED: {e}\")\n            return False\n    \n    def validate_milestone_2_cost_estimation(self) -> bool:\n        \"\"\"Validate Milestone 2: Cost estimation is producing reasonable results.\"\"\"\n        # TODO 1: Test selectivity estimation with known data distributions\n        # TODO 2: Validate join cardinality estimates are reasonable\n        # TODO 3: Check I/O and CPU cost calculations\n        # TODO 4: Test behavior with missing statistics\n        # TODO 5: Verify cost estimates correlate with query complexity\n        pass\n    \n    def validate_milestone_3_join_optimization(self) -> bool:\n        \"\"\"Validate Milestone 3: Join ordering optimization is working.\"\"\"\n        # TODO 1: Test optimal join order for small query (3-4 tables)\n        # TODO 2: Verify connectivity detection and cross product handling\n        # TODO 3: Test dynamic programming pruning effectiveness\n        # TODO 4: Check optimization time is reasonable for larger queries\n        # TODO 5: Validate cost-based join order selection\n        pass\n    \n    def validate_milestone_4_physical_planning(self) -> bool:\n        \"\"\"Validate Milestone 4: Physical planning produces executable plans.\"\"\"\n        # TODO 1: Test scan method selection (sequential vs index)\n        # TODO 2: Verify join algorithm selection logic\n        # TODO 3: Check predicate pushdown rule application\n        # TODO 4: Validate memory requirement estimation\n        # TODO 5: Test final plan executability\n        pass\n```\n\n**F. Debugging Tips for Testing**\n\n| Test Failure Symptom | Likely Cause | Diagnosis Method | Fix Approach |\n|----------------------|--------------|------------------|--------------|\n| Unit tests pass but integration fails | Component interface mismatch | Check data flow between components | Verify interface contracts match |\n| Cost estimates are wildly inaccurate | Statistics not loaded or formula error | Print intermediate calculations | Debug statistics access and formulas |\n| Plans have obviously wrong structure | Tree construction logic error | Use `pretty_print` to visualize | Fix tree building algorithms |\n| Optimization takes too long | Missing pruning or infinite loops | Profile optimization phases | Add timeout and pruning logic |\n| Quality validation fails | Heuristics too strict or optimizer bug | Compare against manual optimization | Adjust quality thresholds or fix bugs |\n\n**G. Language-Specific Testing Hints**\n\n- Use `pytest.parametrize` to test cost estimation with multiple data distributions\n- Use `unittest.mock.patch` to simulate different statistics scenarios without creating large test databases  \n- Use `pytest.benchmark` to ensure optimization time stays within acceptable bounds\n- Use `pytest.fixture` to share expensive test data setup across multiple test functions\n- Use `coverage.py` to ensure test coverage includes error handling paths\n- Use `pytest.raises` to test that invalid queries produce appropriate exceptions\n\n\n## Debugging Guide\n\n> **Milestone(s):** All Milestones 1-4 - provides systematic troubleshooting strategies for diagnosing and fixing common issues that arise during query optimizer implementation, from plan tree construction through cost estimation and physical operator selection.\n\n### Mental Model: Medical Diagnostics\n\nThink of debugging a query optimizer like being a doctor diagnosing a complex medical condition. Just as a doctor uses symptoms to narrow down potential causes, then applies specific tests to confirm a diagnosis, query optimizer debugging requires systematic observation of symptoms (wrong plans, poor performance, crashes), hypothesis formation about root causes (cost estimation errors, join ordering bugs, tree construction issues), and targeted diagnostic tests to isolate the problem. Like medical diagnostics, optimizer debugging benefits from understanding the \"anatomy\" of the system - which components are responsible for what functions, how they interact, and what can go wrong at each stage.\n\nThe key insight is that optimizer bugs often manifest as seemingly unrelated symptoms far from their actual cause. A cost estimation error in the statistics component might surface as a poor join order selection, just as a heart problem might manifest as shortness of breath. Effective debugging requires tracing symptoms back to their root causes through the complex web of component interactions.\n\n### Cost Estimation Debugging\n\nCost estimation forms the foundation of all optimization decisions, making bugs in this component particularly insidious. Cost estimation errors propagate through the entire optimization pipeline, causing the optimizer to consistently make poor choices even when all other components work correctly. The challenge lies in distinguishing between symptoms (bad plans) and causes (incorrect cost calculations, stale statistics, or flawed estimation models).\n\n> **Decision: Layered Debugging Approach for Cost Estimation**\n> - **Context**: Cost estimation bugs can occur at multiple levels - data collection, statistical modeling, selectivity calculation, or cost model application\n> - **Options Considered**: Single comprehensive test vs layered component testing vs black-box plan quality assessment\n> - **Decision**: Implement layered debugging starting from raw statistics validation up through final cost estimates\n> - **Rationale**: Allows precise isolation of the faulty layer without being misled by cascading effects from upstream errors\n> - **Consequences**: Requires more debugging infrastructure but provides faster root cause identification\n\nThe most effective strategy involves building debugging capabilities into each layer of the cost estimation pipeline. This approach allows precise identification of where estimation errors originate and prevents wild goose chases caused by cascading effects.\n\n#### Statistics Validation and Debugging\n\nStatistics form the foundation of all cost estimation, making their accuracy crucial for optimizer correctness. The most common statistics-related bugs involve stale data, incorrect collection procedures, or missing statistical information for newly created tables or columns.\n\n| Debugging Method | Purpose | Implementation | Expected Output |\n|------------------|---------|----------------|-----------------|\n| Statistics Freshness Check | Verify statistics currency | Compare `last_updated` timestamps against table modification times | Warning when statistics older than data changes |\n| Row Count Validation | Confirm basic table statistics | Execute `SELECT COUNT(*)` and compare with stored `row_count` | Exact match or explanation for discrepancy |\n| Distinct Value Verification | Validate column cardinality | Execute `SELECT COUNT(DISTINCT column)` against `distinct_values` | Match within acceptable error margin |\n| Histogram Bucket Inspection | Examine value distribution | Display histogram buckets with actual value ranges and frequencies | Reasonable distribution matching data characteristics |\n| Null Count Accuracy | Check missing value statistics | Execute `SELECT COUNT(*) WHERE column IS NULL` against `null_count` | Exact match for null statistics |\n\nStatistics debugging often reveals systematic collection errors that affect multiple tables or columns. For example, a bug in the statistics collection sampling logic might consistently underestimate distinct values across all columns, leading to systematically poor join cardinality estimates.\n\n#### Selectivity Estimation Diagnosis\n\nSelectivity estimation translates statistical information into predictions about how many rows survive filtering operations. Bugs in this component often involve incorrect mathematical models, edge case handling failures, or misunderstanding of predicate semantics.\n\n| Symptom | Likely Cause | Diagnostic Test | Resolution |\n|---------|--------------|-----------------|------------|\n| All filters estimated at 50% selectivity | Default selectivity fallback being used | Check if column statistics exist for filter columns | Collect missing statistics or fix collection bug |\n| Extremely low selectivity for equality predicates | Incorrect distinct value count | Manually verify `SELECT COUNT(DISTINCT column)` | Recollect statistics or fix calculation logic |\n| Selectivity > 1.0 or < 0.0 | Mathematical error in estimation formula | Add bounds checking to selectivity calculations | Implement proper range validation |\n| Identical selectivity for different operators | Operator type not considered in estimation | Verify operator-specific selectivity logic | Implement different models for =, <, >, LIKE, etc. |\n| Poor estimates for range predicates | Uniform distribution assumption incorrect | Examine actual data distribution vs histogram | Improve histogram construction or use better model |\n\nThe `estimate_filter_selectivity` method serves as the primary debugging entry point for selectivity issues. Adding comprehensive logging to this method helps track how statistical inputs transform into selectivity estimates.\n\nA particularly effective debugging technique involves creating \"selectivity test queries\" that apply various predicates to well-understood test datasets. By comparing estimated vs actual selectivity for these known cases, developers can quickly identify systematic biases in the estimation logic.\n\n#### Cardinality Estimation Troubleshooting\n\nJoin cardinality estimation represents one of the most challenging aspects of cost-based optimization. Errors in this area often result from incorrect assumptions about data correlation, inadequate statistical models, or bugs in the mathematical formulas used to combine statistics from multiple tables.\n\n| Problem Pattern | Root Cause Analysis | Debugging Approach | Fix Strategy |\n|-----------------|--------------------|--------------------|-------------|\n| Join estimates consistently too high | Assuming independence when correlation exists | Compare estimated vs actual join results on sample data | Implement correlation detection or use more conservative estimates |\n| Cross product detection failure | Missing predicate analysis | Verify join predicate identification logic | Improve predicate parsing and connectivity analysis |\n| Estimates wildly incorrect for complex joins | Cascading estimation errors | Test single join estimates before multi-join scenarios | Fix base case estimation before tackling complex queries |\n| Self-join cardinality errors | Special case handling missing | Create test cases with self-joins on various key types | Implement self-join specific estimation logic |\n| Foreign key join overestimation | Not recognizing referential integrity | Check for foreign key constraints in schema metadata | Use constraint information to improve cardinality bounds |\n\nThe `estimate_join_cardinality` method requires extensive logging to track how input statistics combine into final estimates. Key debugging outputs include the distinct value counts from both join columns, the independence assumption factor, and any correlation adjustments applied.\n\nBuilding a \"cardinality verification suite\" helps catch regression errors when modifying estimation logic. This suite should include queries with known join characteristics - primary key/foreign key joins, many-to-many relationships, self-joins, and various selectivity combinations.\n\n#### Cost Model Validation\n\nThe cost model translates cardinality estimates into execution time predictions by combining I/O costs, CPU costs, and memory costs. Bugs in cost model implementation often involve incorrect coefficient values, missing cost factors, or errors in combining different cost components.\n\n| Cost Component | Common Bugs | Detection Method | Debugging Output |\n|----------------|-------------|------------------|------------------|\n| I/O Cost Calculation | Wrong page size assumptions | Compare estimated pages vs actual table/index pages | Page count breakdown by operator type |\n| CPU Cost Modeling | Incorrect processing rates | Benchmark tuple processing costs on target hardware | CPU cycles per tuple by operation |\n| Memory Cost Integration | Missing memory allocation costs | Monitor actual memory usage during execution | Memory allocation patterns by operator |\n| Sequential vs Random I/O | Using wrong I/O cost multipliers | Analyze access patterns for scan operations | I/O pattern classification and cost factors |\n| Cache Effects | Ignoring buffer pool hit rates | Compare cold vs warm execution costs | Cache hit rate assumptions and impacts |\n\nThe `calculate_total_cost` method should provide detailed cost breakdowns showing how individual operator costs combine into total plan costs. This granular visibility helps identify which cost components dominate and whether those estimates seem reasonable.\n\nEffective cost model debugging often involves comparing optimizer estimates against actual execution metrics. While building full execution timing into the optimizer may be impractical, even rough benchmarks help validate that cost model coefficients reflect reality.\n\n⚠️ **Pitfall: Statistics Staleness Ignored**\nMany developers forget to implement statistics staleness detection, leading to optimization decisions based on obsolete information. After significant data modifications, outdated statistics can cause dramatic misestimation. Always check `last_updated` timestamps and implement automatic statistics refresh triggers for tables with significant modification activity.\n\n⚠️ **Pitfall: Uniform Distribution Assumption**\nAssuming uniform data distribution when calculating selectivity often produces wildly incorrect estimates for skewed data. Real-world data frequently exhibits significant skew, with some values appearing much more frequently than others. Implement histogram-based estimation or at least collect most common values to handle skewed distributions properly.\n\n### Join Ordering Debugging\n\nJoin ordering optimization represents the algorithmic heart of query optimization, where dynamic programming algorithms explore exponentially large search spaces to find optimal execution strategies. Bugs in join ordering typically fall into three categories: algorithmic implementation errors, cost comparison mistakes, and search space management problems.\n\nThe complexity of join ordering makes systematic debugging essential. Unlike simple component bugs that produce obvious failures, join ordering bugs often manifest as subtly suboptimal plans that still execute correctly but perform poorly. This makes them particularly dangerous in production systems.\n\n#### Dynamic Programming Algorithm Verification\n\nThe dynamic programming algorithm for join ordering builds optimal plans bottom-up by combining optimal subplans. Implementation bugs often involve incorrect subset enumeration, flawed memoization, or errors in the plan combination logic.\n\n| Algorithm Phase | Verification Method | Expected Behavior | Debugging Technique |\n|----------------|-------------------|-------------------|-------------------|\n| Base Case Initialization | Verify single-table plans created correctly | One plan per table with table scan operator | Log all base case plans with costs |\n| Subset Enumeration | Check all valid subsets generated | Exponential growth: 2^n total subsets | Count subsets by size, verify completeness |\n| Plan Combination | Validate join plan construction | Left and right subplans properly combined | Trace plan tree construction step by step |\n| Memoization Logic | Ensure optimal plans stored and retrieved | Same subset never recomputed | Monitor cache hit rates during optimization |\n| Pruning Decisions | Verify search space reduction working | Suboptimal plans eliminated correctly | Log pruned plans with reasons |\n\nThe `_enumerate_subsets_by_size` method serves as a critical debugging point. Incorrect subset enumeration leads to missing optimal solutions or redundant computation. Adding comprehensive logging to track which table combinations are considered helps identify gaps in the enumeration logic.\n\nA particularly effective debugging technique involves manually tracing the algorithm execution for small query examples. With 3-4 tables, the search space remains manageable for human analysis, allowing verification that the algorithm considers all valid join orders and correctly identifies the optimal solution.\n\n#### Cost Comparison and Plan Selection\n\nDynamic programming relies on accurate cost comparisons to identify optimal subplans. Bugs in cost comparison logic can cause the algorithm to select suboptimal intermediate results, leading to globally suboptimal final plans.\n\n| Comparison Issue | Symptom | Diagnostic Approach | Resolution |\n|------------------|---------|-------------------|------------|\n| Inconsistent cost calculations | Same plan gets different costs | Verify cost calculation determinism | Fix non-deterministic cost components |\n| Cost component weighting errors | I/O costs dominate CPU costs inappropriately | Analyze cost component ratios | Rebalance cost model coefficients |\n| Floating point precision issues | Tiny cost differences cause plan instability | Use cost comparison tolerances | Implement epsilon-based cost comparison |\n| Missing cost factors | Important costs ignored in comparison | Compare estimated vs actual execution costs | Add missing cost components |\n| Plan equivalence detection | Functionally identical plans treated as different | Check for redundant plan generation | Implement plan canonicalization |\n\nThe `_evaluate_join_cost` method requires extensive instrumentation to debug cost comparison issues. Key debugging outputs include detailed cost breakdowns for each join algorithm option, input cardinality estimates, and the final cost comparison results.\n\nBuilding a \"cost comparison test suite\" helps validate that cost comparisons produce consistent, reasonable results. This suite should include scenarios where the optimal choice is known or easily verified, such as comparing hash joins vs nested loop joins with dramatically different input sizes.\n\n#### Search Space Management and Pruning\n\nEffective join ordering optimization requires aggressive search space pruning to handle queries with many tables. However, overly aggressive pruning can eliminate optimal solutions, while insufficient pruning leads to unacceptable optimization times.\n\n| Pruning Strategy | Purpose | Implementation Challenge | Debugging Approach |\n|------------------|---------|-------------------------|-------------------|\n| Cross Product Elimination | Remove joins without predicates | Identifying all possible join predicates | Verify connectivity graph construction |\n| Cost-Based Pruning | Eliminate clearly inferior plans | Setting appropriate pruning thresholds | Monitor pruned vs retained plan ratios |\n| Plan Count Limits | Bound memory usage during optimization | Selecting plans to retain when limit exceeded | Analyze quality of retained vs discarded plans |\n| Timeout Handling | Prevent excessive optimization time | Graceful degradation when timeout reached | Verify fallback plan quality |\n| Heuristic Shortcuts | Apply rules to skip expensive enumeration | Ensuring shortcuts don't miss optimal plans | Compare heuristic vs full enumeration results |\n\nThe `_check_connectivity` method plays a crucial role in cross product elimination. Bugs in connectivity analysis can either allow cross products (performance disasters) or eliminate valid join paths (correctness errors). Comprehensive testing should verify that all legitimate join predicates are recognized while cross products are properly identified and handled.\n\nImplementing optimization time budgets helps debug performance issues in join ordering. By tracking how much time each optimization phase consumes, developers can identify algorithmic bottlenecks and verify that pruning strategies effectively manage search complexity.\n\n#### Multi-Table Query Analysis\n\nComplex queries with many tables expose edge cases and performance issues that may not appear in simple test cases. Systematic analysis of multi-table query optimization helps identify scalability problems and algorithmic bugs.\n\n| Query Complexity | Expected Behavior | Common Issues | Debugging Strategy |\n|------------------|-------------------|---------------|-------------------|\n| 3-4 tables | Full enumeration feasible | Algorithm correctness bugs | Manual trace and verification |\n| 5-8 tables | Pruning becomes important | Pruning too aggressive or insufficient | Monitor search space reduction |\n| 9-12 tables | Heuristics may be needed | Optimization time explosion | Implement timeout and fallback |\n| 13+ tables | Must use approximation | Algorithm fails to complete | Switch to heuristic-only approach |\n\n⚠️ **Pitfall: Missing Predicate Detection**\nFailing to properly identify all join predicates leads to cross product generation or missing valid join paths. Complex WHERE clauses with multiple conditions, subqueries, or function calls can obscure join predicates from simple parsing logic. Implement comprehensive predicate analysis that handles various SQL constructs correctly.\n\n⚠️ **Pitfall: Memoization Key Errors**\nUsing incorrect keys for memoizing optimal subplans causes the algorithm to miss cached results or retrieve wrong plans. The memoization key must uniquely identify the table subset and any relevant optimization context. Ensure that equivalent table sets map to identical keys regardless of table ordering.\n\n### Plan Generation Debugging\n\nPlan generation transforms logical query specifications into executable operator trees through a complex process involving tree construction, operator selection, schema propagation, and cost accumulation. Bugs in plan generation often create subtle correctness issues that may not surface until query execution, making thorough testing and debugging crucial.\n\nPlan generation debugging requires understanding both the tree construction algorithms and the semantic correctness requirements for query execution. Unlike optimization bugs that affect performance, plan generation bugs can produce incorrect results, making them particularly serious.\n\n#### Tree Construction and Structure Validation\n\nQuery plan trees must satisfy strict structural requirements to ensure correct execution semantics. Tree construction bugs can create invalid parent-child relationships, missing operators, or incorrect tree topology that leads to execution failures.\n\n| Structural Requirement | Validation Method | Common Violations | Fix Strategy |\n|------------------------|-------------------|-------------------|--------------|\n| Parent-child consistency | Verify bidirectional links | Orphaned nodes, circular references | Implement defensive tree construction |\n| Operator compatibility | Check input/output schema matching | Schema mismatches between operators | Add schema validation at tree construction |\n| Complete operator coverage | Ensure all query clauses represented | Missing filter, join, or projection operators | Implement comprehensive query analysis |\n| Proper tree depth | Validate execution order dependencies | Operators in wrong tree positions | Fix operator precedence logic |\n| Schema propagation | Track column flow through operators | Missing or incorrect output schemas | Implement rigorous schema inference |\n\nThe `add_child` method serves as a critical validation point for tree construction. Adding defensive checks that verify schema compatibility and operator constraints at tree construction time catches many structural bugs before they propagate to execution.\n\nImplementing a \"tree validator\" that performs comprehensive structural checks helps catch construction bugs early. This validator should traverse the entire tree, checking operator compatibility, schema consistency, and execution semantics at each node.\n\n#### Operator Selection and Configuration\n\nPhysical operator selection must choose appropriate implementations based on data characteristics, available indexes, and estimated costs. Bugs in operator selection often involve choosing inefficient algorithms or incorrectly configuring operator parameters.\n\n| Operator Type | Selection Criteria | Configuration Parameters | Common Bugs |\n|---------------|-------------------|-------------------------|-------------|\n| Scan Operators | Table size, selectivity, available indexes | Index choice, scan direction, filter pushdown | Wrong index selection, missing filter pushdown |\n| Join Operators | Input sizes, join selectivity, memory availability | Hash table size, join algorithm, predicate order | Algorithm mismatch for input characteristics |\n| Sort Operators | Input cardinality, available memory, required order | Sort algorithm, memory allocation, spill handling | Memory estimation errors, incorrect sort keys |\n| Aggregation Operators | Group cardinality, aggregate functions, memory limits | Hash table sizing, aggregation algorithm | Memory allocation bugs, incorrect grouping |\n\nThe `_select_scan_operator` and `_select_join_algorithm` methods require extensive testing across different data characteristics. Building comprehensive test suites that cover various selectivity ranges, input sizes, and schema patterns helps identify operator selection bugs.\n\nOperator configuration bugs often involve mathematical errors in parameter calculation. For example, hash join memory allocation might use incorrect formulas to estimate required memory, leading to performance problems or execution failures.\n\n#### Schema Propagation and Type Safety\n\nQuery execution requires that column schemas flow correctly through the operator tree, with each operator producing outputs compatible with its parent's expected inputs. Schema propagation bugs can cause runtime type errors or incorrect result calculation.\n\n| Schema Aspect | Propagation Rule | Validation Check | Error Pattern |\n|---------------|------------------|------------------|---------------|\n| Column Names | Preserve or rename consistently | Verify column names match expected schema | Missing columns in projection |\n| Data Types | Maintain type compatibility | Check type compatibility across operators | Type mismatch errors |\n| Null Handling | Propagate null semantics | Verify null handling consistency | Incorrect null result handling |\n| Column Ordering | Maintain consistent ordering | Check column position expectations | Wrong column order in results |\n| Schema Annotations | Preserve metadata | Verify annotation propagation | Missing constraint information |\n\nThe `output_schema` field in `OperatorNode` provides the primary mechanism for schema propagation. Implementing strict schema validation at each tree construction step helps catch propagation errors before they reach execution.\n\nBuilding schema propagation test cases that trace column flow through complex operator trees helps identify edge cases where schema information gets lost or corrupted during plan generation.\n\n#### Cost Accumulation and Validation\n\nPlan generation must correctly accumulate costs from child operators to produce accurate total cost estimates. Cost accumulation bugs can lead to incorrect optimization decisions even when individual operator costs are estimated correctly.\n\n| Cost Accumulation Rule | Implementation | Validation Method | Debugging Output |\n|------------------------|----------------|-------------------|------------------|\n| Bottom-up calculation | Sum child costs before adding operator cost | Verify costs calculated in correct order | Cost tree with intermediate totals |\n| Operator cost addition | Add operator-specific costs to child costs | Check operator cost calculation accuracy | Detailed cost breakdown by operator |\n| Cost component handling | Properly combine I/O, CPU, and memory costs | Verify cost component math | Separate totals for each cost type |\n| Plan comparison | Use accumulated costs for plan selection | Compare cost calculation consistency | Side-by-side cost comparison |\n\nThe `calculate_subtree_cost` method must handle cost accumulation correctly while avoiding double-counting. Adding comprehensive cost tracking that shows how costs flow up the tree helps identify accumulation errors.\n\nCost validation becomes particularly important when debugging why the optimizer selects suboptimal plans. By tracing cost accumulation step-by-step, developers can identify whether selection errors stem from accumulation bugs or incorrect base cost estimates.\n\n#### Plan Validation and Correctness Checks\n\nGenerated plans must satisfy numerous correctness constraints to ensure proper execution semantics. Implementing comprehensive plan validation helps catch generation bugs before they cause execution failures.\n\n| Validation Category | Check Description | Implementation | Recovery Strategy |\n|--------------------|-------------------|----------------|-------------------|\n| Semantic Correctness | Verify plan produces correct query results | Compare plan semantics against SQL query | Reject invalid plans and try alternatives |\n| Execution Feasibility | Ensure plan can be executed by execution engine | Check operator compatibility and resource requirements | Generate fallback plans or report errors |\n| Resource Constraints | Verify plan respects memory and other limits | Check estimated resource usage against limits | Select alternative operators or fail gracefully |\n| Optimization Invariants | Ensure plan satisfies optimization assumptions | Verify cost estimates and operator properties | Log warnings about assumption violations |\n\nThe `_validate_physical_plan` method serves as the final checkpoint before returning optimized plans. This validation should be comprehensive enough to catch correctness bugs while remaining fast enough to avoid optimization performance problems.\n\n⚠️ **Pitfall: Schema Mismatch Propagation**\nFailing to validate schema compatibility between parent and child operators can cause subtle correctness bugs that only surface during execution. Always verify that operator output schemas match parent operator input expectations, especially after applying optimization transformations.\n\n⚠️ **Pitfall: Cost Double-Counting**\nAdding child operator costs multiple times during cost accumulation leads to dramatically inflated cost estimates that can mislead optimization decisions. Implement careful cost accumulation logic that ensures each operator cost is counted exactly once in the total plan cost.\n\n### Implementation Guidance\n\nQuery optimizer debugging requires sophisticated tooling and systematic approaches to handle the complexity of multi-component interactions. The following implementation provides comprehensive debugging infrastructure that supports all aspects of optimizer troubleshooting.\n\n#### Technology Recommendations\n\n| Component | Simple Option | Advanced Option |\n|-----------|---------------|------------------|\n| Logging Framework | Python `logging` module with structured format | `structlog` with JSON output for machine parsing |\n| Cost Visualization | Text-based cost trees in debug output | Web-based interactive cost visualization |\n| Plan Comparison | Side-by-side text comparison | Graph-based diff visualization |\n| Statistics Validation | Manual SQL queries for verification | Automated statistics auditing framework |\n| Performance Profiling | Python `cProfile` for optimization timing | Custom profiler with component-level metrics |\n\n#### Recommended File Structure\n\n```\nquery_optimizer/\n  debugging/\n    __init__.py\n    cost_debugger.py          ← Cost estimation debugging tools\n    join_debugger.py          ← Join ordering debugging utilities  \n    plan_debugger.py          ← Plan generation debugging support\n    statistics_validator.py   ← Statistics validation and testing\n    debug_formatter.py        ← Debug output formatting utilities\n  test_data/\n    debug_queries.sql         ← Test queries for debugging scenarios\n    expected_plans.json       ← Expected plan structures for validation\n    cost_benchmarks.json      ← Cost estimation benchmarks\n  tools/\n    debug_cli.py             ← Command-line debugging interface\n    plan_visualizer.py       ← Plan visualization tools\n```\n\n#### Cost Estimation Debugging Infrastructure\n\n```python\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass\nimport logging\nfrom datetime import datetime, timedelta\n\n@dataclass\nclass CostDebugTrace:\n    \"\"\"Detailed trace information for cost estimation debugging.\"\"\"\n    component: str\n    operation: str\n    inputs: Dict[str, Any]\n    outputs: Dict[str, Any]\n    timestamp: datetime\n    duration_ms: float\n\nclass CostEstimationDebugger:\n    \"\"\"Comprehensive debugging support for cost estimation components.\"\"\"\n    \n    def __init__(self, enable_detailed_logging: bool = True):\n        self.enable_detailed_logging = enable_detailed_logging\n        self.debug_traces: List[CostDebugTrace] = []\n        self.logger = logging.getLogger(__name__)\n    \n    def validate_table_statistics(self, table_name: str, stats: 'TableStatistics') -> Dict[str, Any]:\n        \"\"\"Validate table statistics against actual database state.\"\"\"\n        # TODO: Implement actual row count verification via SELECT COUNT(*)\n        # TODO: Check statistics freshness against table modification timestamps\n        # TODO: Validate column statistics for each column in table schema\n        # TODO: Verify histogram bucket consistency and coverage\n        # TODO: Check for missing statistics that should exist\n        # TODO: Return validation results with specific issues found\n        pass\n    \n    def trace_selectivity_calculation(self, table_name: str, column: str, \n                                    operator: str, value: Any, \n                                    result: float) -> None:\n        \"\"\"Trace detailed selectivity estimation process.\"\"\"\n        # TODO: Log input statistics used for selectivity calculation\n        # TODO: Show mathematical formula applied for given operator type\n        # TODO: Record intermediate calculation steps\n        # TODO: Validate result is within valid range [0.0, 1.0]\n        # TODO: Store trace for later analysis and debugging\n        pass\n    \n    def compare_estimated_vs_actual_cardinality(self, plan: 'ExecutionPlan', \n                                               actual_results: Dict[str, int]) -> Dict[str, float]:\n        \"\"\"Compare estimated cardinalities against actual execution results.\"\"\"\n        # TODO: Extract cardinality estimates from each operator in plan\n        # TODO: Match estimates with actual row counts from execution\n        # TODO: Calculate percentage error for each operator\n        # TODO: Identify operators with worst estimation accuracy\n        # TODO: Generate summary report of estimation quality\n        # TODO: Return error metrics for further analysis\n        pass\n\nclass StatisticsValidator:\n    \"\"\"Validates statistics accuracy and freshness.\"\"\"\n    \n    def __init__(self, database_connection):\n        self.db = database_connection\n        self.validation_cache = {}\n    \n    def audit_statistics_freshness(self, table_names: List[str]) -> Dict[str, 'StalenessAssessment']:\n        \"\"\"Audit statistics freshness for multiple tables.\"\"\"\n        # TODO: Query table modification timestamps from database metadata\n        # TODO: Compare with stored statistics last_updated timestamps\n        # TODO: Calculate staleness severity based on modification volume\n        # TODO: Generate refresh recommendations for stale statistics\n        # TODO: Return staleness assessment for each table\n        pass\n    \n    def verify_row_count_accuracy(self, table_name: str, \n                                 stored_count: int, tolerance: float = 0.05) -> Tuple[bool, int, float]:\n        \"\"\"Verify stored row count against actual database state.\"\"\"\n        # TODO: Execute SELECT COUNT(*) query against specified table\n        # TODO: Compare actual count with stored statistics\n        # TODO: Calculate percentage difference between stored and actual\n        # TODO: Determine if difference exceeds acceptable tolerance\n        # TODO: Return validation result, actual count, and error percentage\n        pass\n```\n\n#### Join Ordering Debugging Tools\n\n```python\nfrom typing import Set, Dict, List, Tuple\nfrom collections import defaultdict\nimport time\n\nclass JoinOrderingDebugger:\n    \"\"\"Debugging utilities for dynamic programming join ordering.\"\"\"\n    \n    def __init__(self, enable_search_tracing: bool = False):\n        self.enable_search_tracing = enable_search_tracing\n        self.search_trace = []\n        self.subset_costs = {}\n        self.pruning_stats = defaultdict(int)\n    \n    def trace_dynamic_programming_execution(self, optimizer: 'DynamicProgrammingOptimizer', \n                                          tables: List[str]) -> Dict[str, Any]:\n        \"\"\"Trace complete dynamic programming execution for debugging.\"\"\"\n        # TODO: Monitor subset enumeration at each size level\n        # TODO: Track cost calculations for each subset combination\n        # TODO: Record pruning decisions and reasons\n        # TODO: Measure optimization time by component\n        # TODO: Validate that optimal substructure property holds\n        # TODO: Return comprehensive trace of algorithm execution\n        pass\n    \n    def validate_subset_enumeration(self, tables: List[str], generated_subsets: List[Set[str]]) -> bool:\n        \"\"\"Validate that all required subsets are generated correctly.\"\"\"\n        # TODO: Calculate expected number of subsets: 2^n - 1 (excluding empty set)\n        # TODO: Verify no duplicate subsets in generated list\n        # TODO: Check that all single-table subsets are present\n        # TODO: Verify all table combinations up to full set are generated\n        # TODO: Validate subset ordering matches dynamic programming requirements\n        # TODO: Return True if enumeration is correct, False with details otherwise\n        pass\n    \n    def analyze_cost_comparison_stability(self, cost_traces: List[CostDebugTrace]) -> Dict[str, float]:\n        \"\"\"Analyze stability and consistency of cost comparisons.\"\"\"\n        # TODO: Group cost calculations by identical input parameters\n        # TODO: Check for non-deterministic cost calculation results\n        # TODO: Measure variance in costs for identical scenarios\n        # TODO: Identify floating-point precision issues in comparisons\n        # TODO: Calculate confidence intervals for cost estimates\n        # TODO: Return stability metrics and problematic comparison patterns\n        pass\n    \n    def verify_join_predicate_connectivity(self, parsed_query: 'ParsedQuery') -> 'JoinGraph':\n        \"\"\"Verify join predicate analysis and connectivity detection.\"\"\"\n        # TODO: Extract all join predicates from query WHERE clause\n        # TODO: Build connectivity graph between tables\n        # TODO: Identify disconnected table groups (potential cross products)\n        # TODO: Validate that all join predicates are properly parsed\n        # TODO: Check for missing foreign key relationships\n        # TODO: Return connectivity analysis with potential issues highlighted\n        pass\n\nclass JoinOrderingProfiler:\n    \"\"\"Performance profiling for join ordering optimization.\"\"\"\n    \n    def profile_optimization_phases(self, tables: List[str]) -> Dict[str, float]:\n        \"\"\"Profile time spent in each optimization phase.\"\"\"\n        # TODO: Time base case initialization phase\n        # TODO: Measure subset enumeration time by size\n        # TODO: Track cost calculation time per subset\n        # TODO: Profile pruning decision time\n        # TODO: Measure memoization overhead\n        # TODO: Return time breakdown by optimization phase\n        pass\n```\n\n#### Plan Generation Debugging Support\n\n```python\nfrom typing import Optional, List, Dict\nimport json\n\nclass PlanGenerationDebugger:\n    \"\"\"Debugging utilities for query plan generation and validation.\"\"\"\n    \n    def __init__(self):\n        self.validation_errors = []\n        self.schema_trace = []\n        self.cost_trace = []\n    \n    def validate_plan_structure(self, plan: 'ExecutionPlan') -> List[str]:\n        \"\"\"Validate structural correctness of generated execution plan.\"\"\"\n        errors = []\n        # TODO: Traverse plan tree and validate parent-child relationships\n        # TODO: Check that all operators have valid input schemas\n        # TODO: Verify schema compatibility between connected operators\n        # TODO: Validate cost accumulation correctness throughout tree\n        # TODO: Check for orphaned nodes or circular references\n        # TODO: Ensure all query clauses are represented by operators\n        # TODO: Return list of structural errors found\n        return errors\n    \n    def trace_schema_propagation(self, root_node: 'OperatorNode') -> List[Dict[str, Any]]:\n        \"\"\"Trace schema propagation through operator tree.\"\"\"\n        schema_flow = []\n        # TODO: Traverse tree in execution order (post-order)\n        # TODO: Record input and output schemas for each operator\n        # TODO: Validate schema transformations at each operator\n        # TODO: Check for column name conflicts or missing columns\n        # TODO: Verify data type compatibility across operators\n        # TODO: Return detailed schema flow trace for analysis\n        return schema_flow\n    \n    def validate_cost_accumulation(self, plan: 'ExecutionPlan') -> Dict[str, Any]:\n        \"\"\"Validate cost accumulation accuracy throughout plan tree.\"\"\"\n        # TODO: Recalculate costs bottom-up through tree traversal\n        # TODO: Compare recalculated costs with stored cost estimates\n        # TODO: Check for cost component double-counting\n        # TODO: Verify cost model consistency across similar operators\n        # TODO: Identify operators contributing most to total cost\n        # TODO: Return cost validation results with any discrepancies\n        pass\n    \n    def generate_plan_debug_output(self, plan: 'ExecutionPlan', \n                                  include_costs: bool = True, \n                                  include_schemas: bool = True) -> str:\n        \"\"\"Generate comprehensive debug output for execution plan.\"\"\"\n        # TODO: Format plan tree structure with proper indentation\n        # TODO: Include detailed cost breakdown if requested\n        # TODO: Show input/output schemas for each operator\n        # TODO: Add operator-specific configuration details\n        # TODO: Include cardinality estimates and selectivity factors\n        # TODO: Return formatted debug string for analysis\n        pass\n\nclass PlanComparator:\n    \"\"\"Utilities for comparing execution plans and identifying differences.\"\"\"\n    \n    def compare_plan_structures(self, plan1: 'ExecutionPlan', \n                               plan2: 'ExecutionPlan') -> Dict[str, Any]:\n        \"\"\"Compare structural differences between two execution plans.\"\"\"\n        # TODO: Compare operator types and tree topology\n        # TODO: Identify differences in join ordering\n        # TODO: Compare physical operator selections\n        # TODO: Analyze cost estimate differences\n        # TODO: Check schema differences between plans\n        # TODO: Return comprehensive comparison results\n        pass\n    \n    def explain_plan_selection_decision(self, alternative_plans: List['ExecutionPlan'], \n                                       selected_plan: 'ExecutionPlan') -> str:\n        \"\"\"Explain why optimizer selected specific plan over alternatives.\"\"\"\n        # TODO: Compare costs across all alternative plans\n        # TODO: Identify key factors that influenced selection\n        # TODO: Highlight cost differences between top alternatives\n        # TODO: Explain any tie-breaking decisions made\n        # TODO: Return human-readable explanation of selection logic\n        pass\n```\n\n#### Debugging Command-Line Interface\n\n```python\nimport argparse\nimport sys\nfrom typing import List\n\nclass OptimizerDebugCLI:\n    \"\"\"Command-line interface for query optimizer debugging.\"\"\"\n    \n    def __init__(self):\n        self.cost_debugger = CostEstimationDebugger()\n        self.join_debugger = JoinOrderingDebugger()\n        self.plan_debugger = PlanGenerationDebugger()\n    \n    def debug_query_optimization(self, sql_query: str, debug_options: Dict[str, bool]) -> None:\n        \"\"\"Debug complete query optimization process.\"\"\"\n        # TODO: Parse SQL query and extract components\n        # TODO: Run optimization with detailed debugging enabled\n        # TODO: Collect debugging information from each component\n        # TODO: Generate comprehensive debug report\n        # TODO: Output results in requested format (text/JSON/HTML)\n        pass\n    \n    def validate_statistics_accuracy(self, table_names: List[str]) -> None:\n        \"\"\"Validate statistics accuracy for specified tables.\"\"\"\n        # TODO: Run statistics validation for each specified table\n        # TODO: Generate accuracy report with specific issues\n        # TODO: Recommend statistics refresh for problematic tables\n        # TODO: Output validation results in structured format\n        pass\n    \n    def benchmark_join_ordering(self, query_file: str, iterations: int = 10) -> None:\n        \"\"\"Benchmark join ordering performance for test queries.\"\"\"\n        # TODO: Load test queries from specified file\n        # TODO: Run join ordering optimization multiple times\n        # TODO: Collect timing and quality metrics\n        # TODO: Generate performance benchmark report\n        # TODO: Identify optimization bottlenecks and improvement opportunities\n        pass\n\ndef main():\n    \"\"\"Main entry point for optimizer debugging CLI.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Query Optimizer Debugging Tools\")\n    \n    parser.add_argument(\"command\", choices=[\"debug\", \"validate\", \"benchmark\"], \n                       help=\"Debugging command to execute\")\n    parser.add_argument(\"--query\", help=\"SQL query to debug\")\n    parser.add_argument(\"--tables\", nargs=\"+\", help=\"Tables to validate\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n    parser.add_argument(\"--output-format\", choices=[\"text\", \"json\", \"html\"], \n                       default=\"text\", help=\"Output format\")\n    \n    args = parser.parse_args()\n    \n    cli = OptimizerDebugCLI()\n    \n    # TODO: Route to appropriate debugging function based on command\n    # TODO: Handle command-line argument validation and error cases\n    # TODO: Provide helpful error messages for invalid usage\n    # TODO: Support interactive debugging mode for complex scenarios\n\nif __name__ == \"__main__\":\n    main()\n```\n\n#### Milestone Debugging Checkpoints\n\nEach milestone should include specific debugging validation to ensure correct implementation:\n\n**Milestone 1 - Plan Representation Debugging:**\n```python\ndef validate_milestone_1_debugging(plan: ExecutionPlan) -> List[str]:\n    \"\"\"Validate plan representation debugging capabilities.\"\"\"\n    issues = []\n    # TODO: Verify plan tree structure validation works correctly\n    # TODO: Test pretty-print output format and readability  \n    # TODO: Validate operator hierarchy and inheritance\n    # TODO: Check parent-child relationship consistency\n    # TODO: Test cost annotation attachment and propagation\n    return issues\n```\n\n**Milestone 2 - Cost Estimation Debugging:**\n```python\ndef validate_milestone_2_debugging(optimizer: 'CostEstimator') -> List[str]:\n    \"\"\"Validate cost estimation debugging capabilities.\"\"\"\n    issues = []\n    # TODO: Test statistics validation against known data\n    # TODO: Verify selectivity estimation tracing works\n    # TODO: Validate cardinality estimation debugging output\n    # TODO: Check cost model component breakdown accuracy\n    return issues\n```\n\n**Milestone 3 - Join Ordering Debugging:**\n```python\ndef validate_milestone_3_debugging(optimizer: 'DynamicProgrammingOptimizer') -> List[str]:\n    \"\"\"Validate join ordering debugging capabilities.\"\"\"\n    issues = []\n    # TODO: Test dynamic programming trace collection\n    # TODO: Verify subset enumeration validation\n    # TODO: Check cost comparison stability analysis\n    # TODO: Validate pruning decision logging\n    return issues\n```\n\n**Milestone 4 - Physical Planning Debugging:**\n```python\ndef validate_milestone_4_debugging(planner: 'PhysicalPlanner') -> List[str]:\n    \"\"\"Validate physical planning debugging capabilities.\"\"\"\n    issues = []\n    # TODO: Test operator selection debugging traces\n    # TODO: Verify plan validation error detection\n    # TODO: Check optimization rule application logging\n    # TODO: Validate final plan correctness verification\n    return issues\n\n```\n\n\n## Future Extensions\n\n> **Milestone(s):** Post-implementation enhancements - advanced features that can be added after completing the core query optimizer implementation across Milestones 1-4.\n\n### Mental Model: Architectural Evolution\n\nThink of your query optimizer as a city's transportation system that starts with basic roads and traffic lights. Once the fundamental infrastructure is working well, you can add advanced features like smart traffic management systems that learn from traffic patterns, express lanes for high-priority routes, and real-time route adjustment based on current conditions. Each enhancement builds on the solid foundation you've established, adding sophistication without compromising the core functionality that users depend on.\n\nThe extensions outlined in this section represent the next evolutionary steps for your query optimizer. Just as a transportation system benefits from adaptive traffic signals, machine learning-based route prediction, and parallel highway systems, your optimizer can grow to include advanced statistical models, parallel execution awareness, and runtime learning capabilities. These enhancements transform a functional optimizer into an intelligent system that continuously improves its decision-making based on real-world feedback.\n\n### Advanced Statistical Models\n\nCurrent cost-based optimizers rely on relatively simple statistical assumptions that work well for many queries but struggle with complex data distributions and correlations. Advanced statistical models address these limitations by capturing more nuanced characteristics of your data, leading to significantly improved cost estimation accuracy for challenging query patterns.\n\n**Multi-dimensional histograms** represent the natural evolution beyond single-column statistics. While your current `ColumnStatistics` captures the distribution of individual columns, real queries often involve predicates on multiple correlated columns. Consider a customer database where age and income are strongly correlated, or a time-series table where timestamp and sensor_type interact. Multi-dimensional histograms capture these relationships by partitioning the joint value space into buckets that preserve correlation information.\n\nThe implementation extends your existing `HistogramBucket` structure to support multiple dimensions. Instead of a single range defined by `range_start` and `range_end`, multi-dimensional buckets define rectangular regions in the joint value space. For a two-dimensional histogram on columns A and B, each bucket would contain ranges for both dimensions plus the count of rows falling within that rectangular region. The selectivity estimation process becomes more sophisticated, requiring intersection calculations between query predicates and the multi-dimensional bucket boundaries.\n\n> **Key Insight**: Multi-dimensional histograms are most valuable for tables with 2-4 strongly correlated columns. Beyond four dimensions, the curse of dimensionality makes bucket populations too sparse to provide reliable statistics. Focus on identifying the most important column correlations in your workload rather than building histograms for every possible column combination.\n\n**Correlation detection** automates the identification of column relationships that would benefit from multi-dimensional modeling. The system analyzes query workloads and table contents to discover statistical dependencies between columns. Strong correlations indicate where joint histograms would improve estimation accuracy compared to the independence assumptions in single-column statistics.\n\nThe correlation analysis extends your `TableStatistics` with a correlation matrix tracking pairwise relationships between columns. During statistics collection, the system computes correlation coefficients and mutual information metrics to quantify dependencies. Queries involving correlated columns trigger specialized estimation logic that accounts for the discovered relationships rather than assuming independence.\n\n**Machine learning-based estimation** represents the most sophisticated extension, using historical query execution data to train models that predict cardinalities and selectivities. Instead of relying solely on statistical formulas, the system learns from actual execution results to improve future estimates. This approach is particularly valuable for complex queries where traditional statistical methods struggle due to multiple joins, nested subqueries, or unusual data distributions.\n\nThe ML integration requires extending your `CostEstimate` with training data collection capabilities. Each query execution generates training examples pairing the optimizer's initial estimates with actual cardinalities observed during execution. Feature vectors capture query characteristics like predicate selectivities, join patterns, and table sizes. The trained models supplement traditional statistics-based estimation, with confidence scores determining when to prefer ML predictions over statistical calculations.\n\n| Statistical Model Extension | Complexity | Accuracy Improvement | Implementation Effort | Best Use Cases |\n| --- | --- | --- | --- | --- |\n| Multi-dimensional histograms | Medium | High for correlated columns | Medium | Time-series, demographic data, scientific datasets |\n| Correlation detection | Low | Medium across many queries | Low | Automatically identifying optimization opportunities |\n| Machine learning estimation | High | Very high for complex queries | High | Large systems with sufficient training data |\n| Adaptive histogram refinement | Medium | Medium ongoing improvement | Medium | Tables with changing data distributions |\n| Query workload modeling | High | High for repeated patterns | High | Systems with predictable query workloads |\n\n### Parallel Query Support\n\nModern database systems leverage multiple CPU cores and distributed processing to accelerate query execution. Extending your optimizer for parallel execution requires fundamental changes to cost modeling, operator selection, and resource management while preserving the correctness of your existing optimization logic.\n\n**Parallel cost modeling** transforms your single-threaded `CostEstimate` into a resource-aware framework that considers CPU parallelism, memory bandwidth, and I/O concurrency. Instead of estimating total work, the cost model must predict execution time under different parallelism scenarios. A table scan that requires reading 1000 pages might take 1000 time units sequentially but only 100 time units with 10-way parallelism, assuming sufficient I/O bandwidth.\n\nThe parallel cost framework extends `CostEstimate` with parallelism-aware fields tracking the degree of parallelism, resource contention factors, and coordination overhead. Each physical operator must provide estimates for its scalability characteristics - some operations like table scans parallelize nearly linearly, while others like sorting have logarithmic coordination costs due to merge phases. The total plan cost calculation becomes more sophisticated, considering the critical path through the execution graph and resource bottlenecks that limit overall parallelism.\n\n**Resource-aware optimization** incorporates system resource constraints into plan selection. A query optimizer operating in a system with 8 CPU cores and limited memory must make different decisions than one with 64 cores and abundant RAM. The optimizer needs awareness of available parallelism, memory limits, and I/O capacity to generate executable plans that don't exceed system resources.\n\nThis extension requires augmenting your `PhysicalCostComparator` with resource modeling capabilities. The system maintains estimates of available CPU cores, memory bandwidth, and storage I/O capacity. During physical operator selection, the optimizer considers not just the fastest theoretical plan, but the fastest plan that can execute within resource constraints. A highly parallel hash join might be theoretically optimal but impractical if it would exceed available memory.\n\n**Parallel operator selection** extends your physical planning logic to choose between sequential and parallel implementations of each operation. The decision depends on input sizes, available resources, and the degree of parallelism already present in other parts of the plan. Small table scans might execute faster sequentially to avoid coordination overhead, while large aggregations benefit significantly from parallel processing.\n\nThe parallel operator framework requires extending your `OperatorNode` hierarchy with parallel variants of each physical operator. ParallelScan, ParallelHashJoin, and ParallelAggregate operators include additional properties for degree of parallelism, partitioning strategies, and resource requirements. The `selectPhysicalOperators` logic becomes more sophisticated, evaluating both sequential and parallel alternatives for each logical operation based on estimated input sizes and system resources.\n\n> **Decision: Parallel Execution Model**\n> - **Context**: Need to extend single-threaded optimizer for multi-core and distributed execution environments\n> - **Options Considered**: \n>   1. Volcano-style exchange operators with explicit parallelism boundaries\n>   2. Morsel-driven execution with fine-grained work stealing  \n>   3. Pipeline parallelism with operator-level thread pools\n> - **Decision**: Volcano-style exchange operators for initial implementation\n> - **Rationale**: Exchange operators provide clean abstraction boundaries, are well-understood in academic literature, and allow incremental migration of existing operators to parallel versions\n> - **Consequences**: Clear separation between sequential and parallel execution regions, but potential overhead from explicit data exchanges between parallel regions\n\n| Parallel Extension Component | Integration Complexity | Performance Impact | Resource Requirements |\n| --- | --- | --- | --- |\n| Parallel cost modeling | High - requires fundamental changes to cost calculation | Medium - improves plan selection accuracy | Low - statistical calculations only |\n| Resource-aware optimization | Medium - extends existing physical planning | High - prevents resource exhaustion | Medium - requires system monitoring |\n| Parallel operator implementations | High - new operator variants needed | Very High - direct execution speedup | High - CPU and memory intensive |\n| Exchange operator framework | Medium - clean abstraction layer | Medium - enables parallelism boundaries | Medium - coordination overhead |\n| Adaptive degree of parallelism | High - runtime feedback required | High - optimizes resource utilization | Medium - monitoring and adjustment logic |\n\n### Runtime Adaptive Optimization\n\nTraditional query optimizers make all decisions at compile time based on statistical estimates, but real execution often reveals that these estimates were inaccurate. Runtime adaptive optimization creates feedback loops that allow the system to learn from actual execution behavior and improve future optimization decisions.\n\n**Execution feedback collection** instruments query execution to gather actual cardinalities, operator costs, and resource utilization patterns. This data provides ground truth for evaluating the accuracy of the optimizer's estimates and identifying systematic biases in the cost models. The feedback system extends your existing execution infrastructure with monitoring capabilities that capture detailed performance metrics without significantly impacting query execution times.\n\nThe feedback collection framework augments `ExecutionPlan` with monitoring hooks that record actual row counts, execution times, and resource consumption for each operator. During execution, the system compares these actual values against the optimizer's original estimates in the `CostEstimate` annotations. Significant deviations indicate opportunities for model improvement or statistics updates. The collected data feeds back into the optimization process for future queries.\n\n**Adaptive statistics maintenance** uses execution feedback to automatically update table and column statistics when the optimizer detects that its estimates are consistently inaccurate. Instead of relying solely on periodic statistics collection, the system can incrementally refine its statistical models based on observed query behavior. This approach is particularly valuable for rapidly changing datasets where traditional statistics collection cannot keep pace with data evolution.\n\nThe adaptive maintenance extends your `TableStatistics` and `ColumnStatistics` with confidence tracking and incremental update mechanisms. When execution feedback reveals systematic estimation errors, the system can adjust histogram bucket boundaries, update selectivity estimates, or trigger targeted statistics recollection for specific tables and columns. The adaptation process includes safeguards to prevent overreaction to outlier queries while still responding to genuine distribution changes.\n\n**Plan reoptimization** represents the most sophisticated form of adaptive behavior, where the system can modify or replace execution plans during query execution when it detects that the original optimization decisions were based on poor estimates. Mid-execution reoptimization is complex and requires careful coordination with the execution engine, but it can provide dramatic performance improvements for long-running queries where the optimizer's initial assumptions prove incorrect.\n\nThe reoptimization framework requires extending your query execution infrastructure with checkpointing and plan switching capabilities. When monitoring detects that actual cardinalities differ significantly from estimates, the system can trigger reoptimization of the remaining query execution. The new plan must account for work already completed and intermediate results already materialized. This capability is most valuable for complex analytical queries where early operators produce much larger or smaller intermediate results than expected.\n\n> **Key Design Principle**: Adaptive optimization must balance responsiveness with stability. Overreacting to every estimation error can cause thrashing and unpredictable performance, while underreacting fails to capture genuine optimization opportunities. Implement confidence intervals and change thresholds that require sustained evidence before triggering adaptations.\n\n**Machine learning integration** leverages the collected execution feedback to train models that can predict query performance characteristics more accurately than traditional statistical methods. The ML models learn from the patterns in historical execution data to identify scenarios where the optimizer typically makes poor decisions and suggest alternative approaches.\n\nThe ML integration extends your optimization pipeline with learned models that complement the existing rule-based and cost-based decision making. Feature vectors capture query characteristics, table properties, and system state information. The trained models provide additional signals for join ordering decisions, physical operator selection, and resource allocation. The integration requires careful engineering to ensure that ML inference doesn't significantly increase optimization time.\n\n| Adaptive Component | Learning Speed | Implementation Complexity | Stability Impact |\n| --- | --- | --- | --- |\n| Execution feedback collection | Fast - immediate data available | Low - monitoring infrastructure | Minimal - read-only data collection |\n| Adaptive statistics maintenance | Medium - requires multiple observations | Medium - incremental update logic | Low - gradual statistics refinement |\n| Plan reoptimization | Slow - complex decision making required | Very High - execution engine integration | High - potential query interruption |\n| Machine learning integration | Slow - requires training data accumulation | High - ML infrastructure and training | Medium - model prediction variability |\n| Workload-aware optimization | Medium - learns from query patterns | Medium - pattern recognition logic | Low - influences future optimizations |\n\nThe adaptive optimization extensions transform your query optimizer from a static system into a learning platform that continuously improves its decision-making capabilities. These enhancements are particularly valuable in production environments where query workloads evolve over time and where the cost of poor optimization decisions is measured in real business impact.\n\n> **Implementation Priority**: Start with execution feedback collection as it provides the foundation for all other adaptive features. The monitoring infrastructure offers immediate value for understanding optimizer behavior and provides the data needed for more sophisticated adaptive algorithms. Plan reoptimization should be implemented last due to its complexity and potential for execution disruption.\n\n### Implementation Guidance\n\n#### Technology Recommendations\n\n| Extension Category | Simple Option | Advanced Option |\n| --- | --- | --- |\n| Multi-dimensional statistics | In-memory correlation matrices with numpy/scipy | Specialized histogram databases like HyPer's multi-dimensional histograms |\n| Machine learning integration | Scikit-learn with simple regression models | TensorFlow/PyTorch with deep neural networks |\n| Parallel cost modeling | Thread-pool based parallelism simulation | Detailed resource contention modeling with queuing theory |\n| Execution monitoring | Python decorators for operator instrumentation | Custom bytecode instrumentation or profiling hooks |\n| Adaptive feedback loops | Simple moving averages for statistics updates | Reinforcement learning for dynamic optimization policies |\n\n#### Recommended File Structure Extension\n\n```\nquery_optimizer/\n  extensions/\n    advanced_stats/\n      __init__.py\n      correlation_detector.py      ← automatic correlation discovery\n      multidim_histogram.py        ← multi-dimensional histogram implementation\n      ml_estimator.py             ← machine learning-based cardinality estimation\n      advanced_stats_collector.py ← enhanced statistics collection\n    parallel/\n      __init__.py\n      parallel_cost_model.py      ← resource-aware cost estimation\n      parallel_operators.py       ← parallel physical operator implementations\n      resource_manager.py         ← system resource tracking and allocation\n      exchange_operators.py       ← data exchange between parallel regions\n    adaptive/\n      __init__.py\n      execution_monitor.py        ← runtime performance data collection\n      feedback_processor.py       ← execution feedback analysis and integration\n      adaptive_stats.py           ← statistics refinement based on execution feedback\n      plan_reoptimizer.py         ← mid-execution plan modification\n  core/\n    optimizer.py                  ← extend with plugin architecture for extensions\n    cost_estimator.py            ← extend with pluggable estimation models\n    physical_planner.py          ← extend with parallel operator selection\n```\n\n#### Advanced Statistics Infrastructure\n\n```python\n# correlation_detector.py - Complete infrastructure for correlation analysis\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nfrom core.data_model import TableStatistics, ColumnStatistics\n\n@dataclass\nclass ColumnCorrelation:\n    \"\"\"Statistical correlation information between two columns\"\"\"\n    column_a: str\n    column_b: str\n    correlation_coefficient: float\n    mutual_information: float\n    sample_size: int\n    confidence_level: float\n\nclass CorrelationDetector:\n    \"\"\"Automatically discovers statistical dependencies between columns\"\"\"\n    \n    def __init__(self, correlation_threshold: float = 0.3):\n        self.correlation_threshold = correlation_threshold\n        self.discovered_correlations: Dict[str, List[ColumnCorrelation]] = {}\n    \n    def analyze_table_correlations(self, table_name: str, sample_data: np.ndarray, \n                                 column_names: List[str]) -> List[ColumnCorrelation]:\n        \"\"\"\n        Analyze pairwise correlations between all columns in a table.\n        Returns list of significant correlations above threshold.\n        \"\"\"\n        # TODO: Implement pairwise correlation analysis using numpy.corrcoef\n        # TODO: Calculate mutual information for categorical variables\n        # TODO: Apply statistical significance testing to filter noise\n        # TODO: Store results in self.discovered_correlations for caching\n        pass\n    \n    def should_build_joint_histogram(self, table_name: str, columns: List[str]) -> bool:\n        \"\"\"\n        Determine if columns are sufficiently correlated to justify joint histogram.\n        \"\"\"\n        # TODO: Look up correlation coefficients for column pairs\n        # TODO: Check if any correlation exceeds threshold\n        # TODO: Consider histogram construction cost vs estimation improvement\n        pass\n\n@dataclass \nclass MultidimensionalBucket:\n    \"\"\"Histogram bucket covering rectangular region in multi-dimensional space\"\"\"\n    bucket_id: int\n    dimension_ranges: List[Tuple[any, any]]  # (min, max) for each dimension\n    row_count: int\n    distinct_count_per_dimension: List[int]\n    frequency: float\n    \nclass MultidimensionalHistogram:\n    \"\"\"Multi-dimensional histogram for correlated column estimation\"\"\"\n    \n    def __init__(self, table_name: str, column_names: List[str], bucket_count: int = 100):\n        self.table_name = table_name\n        self.column_names = column_names\n        self.dimension_count = len(column_names)\n        self.buckets: List[MultidimensionalBucket] = []\n        self.total_rows = 0\n    \n    def build_histogram(self, sample_data: np.ndarray) -> None:\n        \"\"\"\n        Construct multi-dimensional histogram from sample data.\n        Uses recursive binary partitioning to create balanced buckets.\n        \"\"\"\n        # TODO: Implement recursive partitioning algorithm\n        # TODO: Ensure each bucket has minimum population for statistical validity\n        # TODO: Balance bucket sizes across all dimensions\n        # TODO: Calculate distinct counts per dimension within each bucket\n        pass\n    \n    def estimate_selectivity(self, predicates: List[Tuple[str, str, any]]) -> float:\n        \"\"\"\n        Estimate selectivity for multi-dimensional range predicates.\n        Returns fraction of rows satisfying all predicates simultaneously.\n        \"\"\"\n        # TODO: Find buckets that intersect with predicate ranges\n        # TODO: Calculate intersection volume for partially overlapping buckets  \n        # TODO: Sum row counts from fully and partially intersecting buckets\n        # TODO: Apply uniform distribution assumption within each bucket\n        pass\n```\n\n#### Parallel Execution Framework\n\n```python\n# parallel_cost_model.py - Resource-aware cost estimation for parallel execution\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional\nfrom core.data_model import CostEstimate, OperatorNode\n\n@dataclass\nclass SystemResources:\n    \"\"\"Current system resource availability\"\"\"\n    cpu_cores: int\n    memory_mb: int\n    io_bandwidth_mbps: float\n    network_bandwidth_mbps: float\n    current_cpu_utilization: float\n    current_memory_utilization: float\n\n@dataclass  \nclass ParallelCostEstimate(CostEstimate):\n    \"\"\"Extended cost estimate including parallelism information\"\"\"\n    degree_of_parallelism: int\n    coordination_overhead: float\n    resource_contention_factor: float\n    critical_path_cost: float\n    parallelizable_fraction: float\n\nclass ParallelCostModel:\n    \"\"\"Cost estimation accounting for parallel execution and resource constraints\"\"\"\n    \n    def __init__(self, system_resources: SystemResources):\n        self.system_resources = system_resources\n        self.contention_factors = {\n            'cpu_intensive': 0.95,  # Nearly linear scaling\n            'io_intensive': 0.7,    # I/O bandwidth limitations\n            'memory_intensive': 0.6, # Memory bandwidth constraints\n            'coordination_heavy': 0.3 # Synchronization overhead\n        }\n    \n    def estimate_parallel_cost(self, operator: OperatorNode, \n                             input_parallelism: int) -> ParallelCostEstimate:\n        \"\"\"\n        Estimate execution cost for operator under parallel execution.\n        Considers Amdahl's law, resource contention, and coordination overhead.\n        \"\"\"\n        # TODO: Classify operator workload type (CPU/IO/memory intensive)\n        # TODO: Calculate theoretical speedup using Amdahl's law\n        # TODO: Apply resource contention factors based on system utilization\n        # TODO: Add coordination overhead for data exchanges and synchronization\n        # TODO: Determine optimal degree of parallelism for this operator\n        pass\n    \n    def calculate_plan_parallelism(self, plan: OperatorNode) -> Dict[str, int]:\n        \"\"\"\n        Determine optimal degree of parallelism for each operator in plan.\n        Uses dynamic programming to find resource-optimal parallelization.\n        \"\"\"\n        # TODO: Traverse plan tree bottom-up\n        # TODO: Calculate resource requirements for each parallelism option\n        # TODO: Find parallelism assignment that maximizes throughput within resources\n        # TODO: Ensure parent operators can handle child parallelism\n        pass\n\nclass ExchangeOperator(OperatorNode):\n    \"\"\"Data exchange boundary between parallel execution regions\"\"\"\n    \n    def __init__(self, partitioning_columns: List[str], target_parallelism: int):\n        super().__init__(operator_type=\"EXCHANGE\", children=[], properties={})\n        self.partitioning_columns = partitioning_columns\n        self.target_parallelism = target_parallelism\n        self.exchange_cost = None\n    \n    def estimate_exchange_cost(self, input_rows: int, \n                             input_parallelism: int) -> CostEstimate:\n        \"\"\"\n        Estimate cost of repartitioning data between parallel regions.\n        Includes network I/O, serialization, and buffering costs.\n        \"\"\"\n        # TODO: Calculate data volume to exchange based on input rows\n        # TODO: Estimate serialization/deserialization CPU cost\n        # TODO: Calculate network or memory bandwidth cost for data movement\n        # TODO: Add buffering costs if parallelism changes significantly\n        pass\n```\n\n#### Adaptive Optimization Infrastructure\n\n```python\n# execution_monitor.py - Runtime performance data collection\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Callable\nfrom datetime import datetime\nimport threading\nimport time\n\n@dataclass\nclass ExecutionMetrics:\n    \"\"\"Actual performance metrics collected during query execution\"\"\"\n    operator_id: str\n    estimated_rows: int\n    actual_rows: int\n    estimated_cost: float\n    actual_runtime_ms: float\n    memory_used_mb: float\n    io_pages_read: int\n    cpu_time_ms: float\n    \nclass ExecutionMonitor:\n    \"\"\"Instruments query execution to collect performance feedback\"\"\"\n    \n    def __init__(self):\n        self.active_monitors: Dict[str, 'QueryMonitor'] = {}\n        self.completed_executions: List['ExecutionTrace'] = []\n        self.lock = threading.RLock()\n    \n    def start_query_monitoring(self, query_id: str, execution_plan: 'ExecutionPlan') -> 'QueryMonitor':\n        \"\"\"Begin monitoring execution of a query plan\"\"\"\n        # TODO: Create QueryMonitor instance for this execution\n        # TODO: Register monitoring hooks for each operator in plan  \n        # TODO: Initialize performance counters and timers\n        # TODO: Store in active_monitors for concurrent access\n        pass\n    \n    def complete_query_monitoring(self, query_id: str) -> 'ExecutionTrace':\n        \"\"\"Finish monitoring and return collected performance data\"\"\"\n        # TODO: Retrieve QueryMonitor from active_monitors\n        # TODO: Finalize all performance measurements\n        # TODO: Create ExecutionTrace with complete performance data\n        # TODO: Move to completed_executions for analysis\n        pass\n\nclass QueryMonitor:\n    \"\"\"Tracks performance of a single query execution\"\"\"\n    \n    def __init__(self, query_id: str, execution_plan: 'ExecutionPlan'):\n        self.query_id = query_id\n        self.execution_plan = execution_plan\n        self.operator_metrics: Dict[str, ExecutionMetrics] = {}\n        self.start_time = datetime.now()\n        self.end_time: Optional[datetime] = None\n    \n    def record_operator_start(self, operator_id: str) -> None:\n        \"\"\"Record start of operator execution\"\"\"\n        # TODO: Initialize ExecutionMetrics for operator\n        # TODO: Record start timestamp and baseline resource usage\n        # TODO: Hook into system performance counters\n        pass\n    \n    def record_operator_completion(self, operator_id: str, actual_rows: int) -> None:\n        \"\"\"Record completion of operator execution with actual row count\"\"\"\n        # TODO: Calculate elapsed time since operator start\n        # TODO: Read final resource usage counters\n        # TODO: Update ExecutionMetrics with actual performance data\n        # TODO: Compare against estimated values from original plan\n        pass\n\nclass AdaptiveStatisticsManager:\n    \"\"\"Updates statistics based on execution feedback\"\"\"\n    \n    def __init__(self, confidence_threshold: float = 0.5):\n        self.confidence_threshold = confidence_threshold\n        self.estimation_errors: Dict[str, List[float]] = {}\n        \n    def process_execution_feedback(self, execution_trace: 'ExecutionTrace') -> None:\n        \"\"\"Analyze execution feedback and update statistics if needed\"\"\"\n        # TODO: Calculate estimation error ratios for each operator\n        # TODO: Identify systematic biases in cost or cardinality estimation\n        # TODO: Update table/column statistics for consistently wrong estimates\n        # TODO: Trigger statistics recollection if errors exceed threshold\n        pass\n    \n    def should_update_statistics(self, table_name: str, error_pattern: List[float]) -> bool:\n        \"\"\"Determine if statistics update is warranted based on error patterns\"\"\"  \n        # TODO: Calculate moving average of estimation errors\n        # TODO: Apply statistical significance test for systematic bias\n        # TODO: Consider cost of statistics update vs estimation improvement\n        # TODO: Check if sufficient evidence exists for confident update\n        pass\n```\n\n#### Milestone Checkpoints for Extensions\n\n**Advanced Statistics Validation:**\n```bash\n# Test correlation detection\npython -m pytest tests/extensions/test_correlation_detection.py -v\n\n# Expected: Correlation detector identifies known relationships in test data\n# Expected: Multi-dimensional histograms improve estimation accuracy for correlated predicates\n# Expected: ML models converge to better accuracy than statistical baselines\n```\n\n**Parallel Execution Validation:**\n```bash  \n# Test parallel cost modeling\npython -m pytest tests/extensions/test_parallel_costs.py -v\n\n# Expected: Parallel cost estimates scale appropriately with degree of parallelism\n# Expected: Resource constraints limit parallelism to feasible levels\n# Expected: Exchange operators correctly estimate data movement costs\n```\n\n**Adaptive Optimization Validation:**\n```bash\n# Test execution monitoring and feedback processing\npython -m pytest tests/extensions/test_adaptive_optimization.py -v\n\n# Expected: Execution monitor accurately captures actual performance metrics\n# Expected: Statistics updates improve future estimation accuracy\n# Expected: Plan reoptimization triggers only for significant estimation errors\n```\n\n#### Integration with Core Optimizer\n\nThe extensions integrate with your core optimizer through a plugin architecture that preserves the existing optimization pipeline while allowing advanced features to enhance specific phases:\n\n```python\n# Enhanced optimizer.py with extension integration\nclass ExtensibleQueryOptimizer:\n    \"\"\"Query optimizer with pluggable extension support\"\"\"\n    \n    def __init__(self):\n        self.core_optimizer = QueryOptimizer()  # Your existing implementation\n        self.stat_extensions: List[StatisticsExtension] = []\n        self.cost_extensions: List[CostModelExtension] = []\n        self.planning_extensions: List[PhysicalPlanningExtension] = []\n        \n    def register_statistics_extension(self, extension: StatisticsExtension) -> None:\n        \"\"\"Add advanced statistics capability\"\"\"\n        # TODO: Validate extension compatibility\n        # TODO: Add to stat_extensions list\n        # TODO: Initialize extension with existing statistics\n        pass\n        \n    def optimize_query_with_extensions(self, parsed_query: ParsedQuery) -> ExecutionPlan:\n        \"\"\"Main optimization pipeline with extension integration\"\"\"\n        # TODO: Use core optimizer for basic plan generation\n        # TODO: Apply statistics extensions for improved cost estimation\n        # TODO: Apply parallel planning extensions if available\n        # TODO: Apply adaptive extensions for runtime optimization\n        pass\n```\n\nThese extensions represent significant engineering undertakings that build upon the solid foundation of your core optimizer implementation. Each extension addresses specific limitations of basic cost-based optimization and provides pathways for continuous improvement of query performance in production environments.\n\n\n## Glossary\n\n> **Milestone(s):** All Milestones 1-4 - provides comprehensive definitions of technical vocabulary and concepts used throughout the query optimizer implementation.\n\n### Mental Model: Dictionary for a Technical Language\n\nThink of this glossary as a technical dictionary for the specialized language of query optimization. Just as learning a foreign language requires understanding both individual words and their contextual meanings, mastering query optimization requires fluency in its technical vocabulary. Each term represents a concept that database engineers have refined over decades, and understanding these precise definitions is crucial for implementing effective optimization algorithms.\n\nThe glossary serves as both a reference during implementation and a validation tool for understanding. When you encounter unfamiliar terms in research papers or documentation, this glossary provides the context needed to bridge academic theory with practical implementation.\n\n### Core Query Optimization Concepts\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| query optimization | The systematic process of transforming SQL queries into efficient execution plans by analyzing multiple implementation strategies and selecting the approach with minimal estimated resource consumption | Primary process implemented across all milestones |\n| cost-based optimization | Query optimization strategy that makes decisions by comparing quantitative resource consumption estimates (I/O, CPU, memory) rather than applying fixed transformation rules | Fundamental approach used in Milestones 2-4 |\n| execution plan | A tree-structured specification describing the sequence of operations (scans, joins, filters) and their implementation algorithms needed to execute a SQL query | Core data structure from Milestone 1 |\n| plan tree | Hierarchical representation of query execution strategy where each node represents a database operation and edges represent data flow between operations | Primary structural concept in Milestone 1 |\n| operator node | Individual operation in an execution plan tree, containing operation type, cost estimates, output schema, and references to child operations | Fundamental building block from Milestone 1 |\n| logical operator | Abstract specification of a database operation (JOIN, FILTER, SCAN) without implementation details, focusing on what computation is needed rather than how it's performed | Design concept used in plan representation |\n| physical operator | Concrete implementation of a logical operation specifying exact algorithms (hash join vs nested loop join) and access methods (index scan vs sequential scan) | Implementation focus in Milestone 4 |\n\n### Statistical and Cost Estimation Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| selectivity | Mathematical fraction (0.0 to 1.0) representing the proportion of input rows that survive a filter predicate, crucial for estimating intermediate result sizes | Core concept in Milestone 2 cost estimation |\n| cardinality | The number of rows in a table or intermediate query result, used extensively in cost calculations and memory requirement estimation | Fundamental metric throughout cost estimation |\n| cardinality estimation | Process of predicting the number of rows that will be produced by database operations, combining table statistics with selectivity calculations | Key algorithm in Milestone 2 |\n| selectivity estimation | Mathematical process of predicting what fraction of rows will satisfy filter predicates, using statistical models and histogram data | Critical component of cost estimation |\n| cost estimation | Quantitative prediction of resource consumption (I/O pages, CPU cycles, memory usage) required to execute a query plan, enabling comparison between alternative plans | Primary objective of Milestone 2 |\n| statistics collection | Process of gathering and maintaining quantitative information about table characteristics (row counts, value distributions, correlation patterns) to support accurate cost estimation | Infrastructure requirement for Milestone 2 |\n| histogram | Statistical data structure representing the distribution of values in a database column, typically organized as buckets containing value ranges and frequency counts | Data structure supporting selectivity estimation |\n| clustering factor | Statistical measure (0.0 to 1.0) indicating how well the physical storage order of table rows matches the logical ordering of indexed values, affecting I/O cost calculations | Advanced statistic for cost modeling |\n\n### Join Processing and Optimization Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| join ordering | Combinatorial optimization problem of determining the sequence in which multiple tables should be joined to minimize total execution cost | Central challenge addressed in Milestone 3 |\n| dynamic programming | Algorithmic technique for join order optimization that builds optimal solutions for table subsets by combining previously computed optimal sub-solutions, avoiding redundant cost calculations | Core algorithm approach in Milestone 3 |\n| search space pruning | Optimization technique that eliminates clearly suboptimal execution plans early in the enumeration process to reduce computational complexity without missing the optimal solution | Performance optimization in Milestone 3 |\n| left-deep tree | Join execution plan topology where the right input of every join operation is a base table rather than an intermediate result, simplifying optimization but potentially missing better solutions | Plan structure option in join ordering |\n| bushy tree | Join execution plan topology allowing intermediate join results as both left and right inputs to subsequent joins, expanding the search space but potentially finding better optimization opportunities | Alternative plan structure in join ordering |\n| cross product | Join operation between tables without connecting predicates, resulting in Cartesian product with potentially enormous intermediate results that should typically be avoided | Degenerate case handled in join optimization |\n| memoization | Caching technique that stores previously computed optimization results (costs, optimal plans) to avoid redundant calculations when the same subproblems are encountered | Performance optimization in dynamic programming |\n\n### Plan Generation and Physical Selection Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| physical planning | Process of selecting concrete physical operators and access methods for each logical operation in a query plan, transforming abstract specifications into executable implementations | Primary focus of Milestone 4 |\n| access method selection | Decision process for choosing between different techniques (sequential scan, index scan, bitmap scan) for retrieving rows from base tables based on selectivity and available indexes | Key decision in physical planning |\n| join algorithm selection | Process of choosing specific join implementation (hash join, nested loop join, merge join) based on input characteristics like table sizes, memory availability, and sort requirements | Algorithm selection in Milestone 4 |\n| predicate pushdown | Query optimization rule that moves filter operations closer to data sources in the execution tree, reducing intermediate result sizes and overall execution cost | Optimization rule applied in physical planning |\n| rule-based optimization | Query transformation approach that applies predetermined heuristic rules (predicate pushdown, projection elimination) to improve plan efficiency without cost-based analysis | Complementary technique to cost-based optimization |\n\n### Tree Structure and Traversal Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| tree traversal | Systematic algorithm for visiting every node in a tree data structure, essential for operations like cost calculation, schema propagation, and plan validation | Fundamental operation in Milestone 1 |\n| preorder traversal | Tree visiting algorithm that processes each node before visiting its children, useful for operations that require parent context before processing children | Traversal pattern for plan operations |\n| postorder traversal | Tree visiting algorithm that processes children before processing their parent, essential for bottom-up cost accumulation and resource requirement calculation | Traversal pattern for cost estimation |\n| cost accumulation | Bottom-up process of calculating total execution cost by combining costs from child operators with the current operator's processing cost | Cost calculation process in tree traversal |\n| schema propagation | Top-down process of determining output column lists and data types by flowing schema information from data sources through operator transformations | Schema validation in plan trees |\n\n### Caching and Performance Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| plan caching | Performance optimization technique that stores previously optimized execution plans in memory to avoid repeating expensive optimization for similar queries | Performance enhancement in Milestone 4 |\n| cache invalidation | Process of removing stored execution plans from cache when underlying table structure or statistics change, ensuring cached plans remain valid | Cache maintenance strategy |\n| plan template | Parameterized execution plan structure that can be instantiated with different literal values while maintaining the same operator structure and join ordering | Advanced caching technique |\n| optimization pipeline | Sequential workflow of transformation phases that converts SQL queries into optimized execution plans through parsing, logical planning, cost estimation, and physical selection | Overall system workflow |\n| parametric matching | Cache lookup technique that matches queries with identical structure but different literal values to cached plan templates | Cache optimization strategy |\n\n### Statistics and Data Quality Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| statistics staleness | Condition where stored statistical information about tables becomes outdated due to data modifications, leading to inaccurate cost estimates and suboptimal plan selection | Data quality challenge throughout optimization |\n| confidence level | Quantitative measure (0.0 to 1.0) indicating the reliability of cost estimates and statistical information, used to adjust optimization decisions under uncertainty | Quality metric for cost estimation |\n| fallback strategy | Alternative optimization approach used when primary cost-based methods fail due to missing statistics, query complexity, or resource constraints | Error handling strategy |\n| degenerate query | Query with problematic patterns like cross products, missing join predicates, or extremely selective filters that challenge standard optimization techniques | Edge case handling |\n| uncertainty margin | Additional cost buffer added to estimates when confidence levels are low, helping to avoid severely underestimating actual execution costs | Risk management in cost estimation |\n\n### Testing and Validation Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| plan quality validation | Systematic process of verifying that the query optimizer produces reasonable and efficient execution plans for various query patterns and data characteristics | Quality assurance across all milestones |\n| benchmark testing | Standardized evaluation approach using representative query workloads to measure optimizer effectiveness and identify performance regressions | System validation methodology |\n| regression testing | Automated testing process that ensures code changes don't degrade plan quality or introduce optimization bugs | Continuous validation strategy |\n| milestone checkpoints | Structured validation points that verify successful completion of implementation phases with specific behavioral requirements | Implementation progress tracking |\n| component unit testing | Testing methodology that validates individual optimizer modules (cost estimator, join optimizer, plan builder) in isolation from other components | Development testing strategy |\n\n### Debugging and Analysis Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| cost estimation debugging | Systematic approach to diagnosing and fixing issues with selectivity calculations, cardinality estimates, and cost model accuracy | Debugging focus for Milestone 2 |\n| join ordering debugging | Troubleshooting methodology for dynamic programming implementation issues, suboptimal join order selection, and search space enumeration problems | Debugging focus for Milestone 3 |\n| plan generation debugging | Diagnostic approach for issues with tree construction, operator selection, schema propagation, and plan validation | Debugging focus for Milestones 1 and 4 |\n| plan validation | Process of verifying that generated execution plans are structurally correct, semantically valid, and contain all necessary operators | Quality control process |\n| debug tracing | Technique of collecting detailed execution information during optimization to analyze decision points, cost calculations, and algorithm behavior | Debugging methodology |\n\n### Advanced Optimization Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| multi-dimensional histograms | Advanced statistical structures that capture correlations between multiple columns, enabling more accurate selectivity estimation for complex predicates | Future extension capability |\n| correlation detection | Automated process of identifying statistical relationships between table columns that affect selectivity estimation accuracy | Advanced statistics feature |\n| machine learning-based estimation | Approach using trained predictive models instead of traditional statistical formulas to estimate query cardinalities and execution costs | Research-level enhancement |\n| parallel cost modeling | Resource-aware cost estimation that considers multi-threaded execution, coordination overhead, and resource contention in parallel query processing | Advanced cost modeling |\n| resource-aware optimization | Plan selection strategy that considers current system resource availability (CPU, memory, I/O bandwidth) in addition to traditional cost estimates | System-aware optimization |\n| exchange operators | Special operator nodes that represent data movement boundaries between different parallel execution regions in distributed or multi-threaded query processing | Parallel processing support |\n| execution feedback collection | Process of gathering actual performance metrics during query execution to validate and improve cost model accuracy | Adaptive optimization support |\n| adaptive statistics maintenance | Dynamic approach to updating table statistics based on query execution feedback rather than fixed maintenance schedules | Self-tuning statistics |\n| runtime adaptive optimization | Advanced technique that modifies execution plans during query execution based on actual performance measurements and intermediate result characteristics | Research-level optimization |\n\n### System Architecture Terms\n\n| Term | Definition | Context of Use |\n|------|------------|----------------|\n| degree of parallelism | Number of concurrent threads or processes executing a particular database operation, affecting both performance and resource consumption | Parallel execution parameter |\n| coordination overhead | Additional cost incurred when synchronizing parallel execution threads, including communication, synchronization, and load balancing expenses | Parallel cost modeling factor |\n| resource contention | Performance degradation that occurs when multiple operations compete for limited system resources like CPU, memory, or I/O bandwidth | System performance consideration |\n| execution monitoring | Instrumentation framework that collects detailed runtime performance data during query execution for analysis and optimization improvement | Performance analysis infrastructure |\n| feedback processing | Analysis workflow that examines actual execution statistics to identify cost model inaccuracies and update optimization parameters | Continuous improvement process |\n\n### Implementation Guidance\n\n#### Essential Concepts Reference\n\nThe glossary serves multiple purposes during implementation. First, use it as a consistency check when naming variables, functions, and data structures. The terminology should align with established database literature to ensure your code is maintainable by other developers familiar with query optimization concepts.\n\nSecond, refer to the glossary when reading academic papers or database system documentation. Understanding the precise technical meaning of terms like \"selectivity\" versus \"cardinality\" or \"left-deep\" versus \"bushy\" trees is crucial for implementing algorithms correctly.\n\nThird, use the glossary to validate your understanding of complex interactions. For example, the relationship between \"statistics staleness,\" \"confidence level,\" and \"uncertainty margin\" represents a sophisticated approach to handling estimation uncertainty that goes beyond basic cost calculation.\n\n#### Terminology Usage Guidelines\n\n| Context | Preferred Terms | Avoid |\n|---------|----------------|-------|\n| Plan Structure | \"operator node,\" \"plan tree,\" \"preorder traversal\" | \"query node,\" \"plan graph,\" \"tree walk\" |\n| Cost Modeling | \"cardinality estimation,\" \"selectivity,\" \"cost accumulation\" | \"row count guessing,\" \"filter ratio,\" \"cost adding\" |\n| Join Processing | \"join ordering,\" \"dynamic programming,\" \"search space pruning\" | \"join optimization,\" \"DP algorithm,\" \"branch cutting\" |\n| Physical Planning | \"access method selection,\" \"predicate pushdown,\" \"physical operator\" | \"scan choosing,\" \"filter moving,\" \"concrete operator\" |\n\n#### Common Documentation Mistakes\n\nWhen documenting your implementation, avoid these terminology errors:\n\n⚠️ **Pitfall: Mixing Logical and Physical Concepts**\nDon't describe logical operators using physical implementation details. A logical JOIN specifies that two relations should be combined based on predicates, while a physical HASH_JOIN specifies the algorithm. Keep these concepts separate in your documentation.\n\n⚠️ **Pitfall: Imprecise Statistical Terms**\nDon't use \"cardinality\" and \"selectivity\" interchangeably. Cardinality is an absolute count (1000 rows), while selectivity is a fraction (0.1 or 10%). Cost estimation algorithms depend on this distinction.\n\n⚠️ **Pitfall: Vague Optimization Language**\nInstead of saying \"the optimizer chooses the best plan,\" specify whether you mean \"lowest estimated cost,\" \"shortest estimated execution time,\" or \"minimal resource consumption.\" Different optimization objectives can lead to different plan choices.\n\n#### Implementation Cross-References\n\n| Glossary Term | Primary Implementation | Secondary Usage |\n|---------------|----------------------|-----------------|\n| `ExecutionPlan` | Plan Representation Component | Used throughout all components |\n| `CostEstimate` | Cost Estimation Component | Referenced in join optimization and physical planning |\n| `JoinOrder` | Join Optimization Component | Input to physical planning |\n| `TableStatistics` | Cost Estimation Component | Referenced in all optimization phases |\n| `OperatorNode` | Plan Representation Component | Manipulated by all optimization components |\n\n#### Milestone Integration Points\n\nThe glossary terms map directly to implementation milestones:\n\n**Milestone 1 Terms**: `ExecutionPlan`, `OperatorNode`, `plan tree`, `tree traversal`, `preorder traversal`, `postorder traversal`, `logical operator`, `physical operator`\n\n**Milestone 2 Terms**: `CostEstimate`, `TableStatistics`, `ColumnStatistics`, `selectivity`, `cardinality`, `cost estimation`, `histogram`, `statistics collection`\n\n**Milestone 3 Terms**: `JoinOrder`, `join ordering`, `dynamic programming`, `search space pruning`, `left-deep tree`, `bushy tree`, `memoization`\n\n**Milestone 4 Terms**: `physical planning`, `access method selection`, `join algorithm selection`, `predicate pushdown`, `plan caching`, `optimization pipeline`\n\nUnderstanding these term relationships helps you organize your implementation and ensures consistent vocabulary across milestone deliverables.\n"}